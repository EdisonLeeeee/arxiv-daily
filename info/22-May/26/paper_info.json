[
  {
    "id": "arXiv:2205.12260",
    "title": "Releasing survey microdata with exact cluster locations and additional  privacy safeguards",
    "abstract": "Household survey programs around the world publish fine-granular\ngeoreferenced microdata to support research on the interdependence of human\nlivelihoods and their surrounding environment. To safeguard the respondents'\nprivacy, micro-level survey data is usually (pseudo)-anonymized through\ndeletion or perturbation procedures such as obfuscating the true location of\ndata collection. This, however, poses a challenge to emerging approaches that\naugment survey data with auxiliary information on a local level. Here, we\npropose an alternative microdata dissemination strategy that leverages the\nutility of the original microdata with additional privacy safeguards through\nsynthetically generated data using generative models. We back our proposal with\nexperiments using data from the 2011 Costa Rican census and satellite-derived\nauxiliary information. Our strategy reduces the respondents' re-identification\nrisk for any number of disclosed attributes by 60-80\\% even under\nre-identification attempts.",
    "descriptor": "",
    "authors": [
      "Till Koebe",
      "Alejandra Arias-Salazar"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.12260"
  },
  {
    "id": "arXiv:2205.12261",
    "title": "Action Recognition for American Sign Language",
    "abstract": "In this research, we present our findings to recognize American Sign Language\nfrom series of hand gestures. While most researches in literature focus only on\nstatic handshapes, our work target dynamic hand gestures. Since dynamic signs\ndataset are very few, we collect an initial dataset of 150 videos for 10 signs\nand an extension of 225 videos for 15 signs. We apply transfer learning models\nin combination with deep neural networks and background subtraction for videos\nin different temporal settings. Our primarily results show that we can get an\naccuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12\nframes accordingly.",
    "descriptor": "\nComments: 2 pages\n",
    "authors": [
      "Nguyen Huu Phong",
      "Bernardete Ribeiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12261"
  },
  {
    "id": "arXiv:2205.12262",
    "title": "PINO-MBD: Physics-informed Neural Operator for Solving Coupled ODEs in  Multi-body Dynamics",
    "abstract": "In multi-body dynamics, the motion of a complicated physical object is\ndescribed as a coupled ordinary differential equation system with multiple\nunknown solutions. Engineers need to constantly adjust the object to meet\nrequirements at the design stage, where a highly efficient solver is needed.\nThe rise of machine learning-based partial differential equation solvers can\nmeet this need. These solvers can be classified into two categories:\napproximating the solution function (Physics-informed neural network) and\nlearning the solution operator (Neural operator). The recently proposed\nphysics-informed neural operator (PINO) gains advantages from both categories\nby embedding physics equations into the loss function of a neural operator.\nFollowing this state-of-art concept, we propose the physics-informed neural\noperator for coupled ODEs in multi-body dynamics (PINO-MBD), which learns the\nmapping between parameter spaces and solution spaces. Once PINO-MBD is trained,\nonly one forward pass of the network is required to obtain the solutions for a\nnew instance with different parameters. To handle the difficulty that coupled\nODEs contain multiple solutions (instead of only one in normal PDE problems),\ntwo new physics embedding methods are also proposed. The experimental results\non classic vehicle-track coupled dynamics problem show state-of-art performance\nnot only on solutions but also the first and second derivatives of solutions.",
    "descriptor": "\nComments: 9 pages, 9 figures\n",
    "authors": [
      "Wenhao Ding",
      "Qing He",
      "Hanghang Tong",
      "Ping Wang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.12262"
  },
  {
    "id": "arXiv:2205.12263",
    "title": "An Artificial Bee Colony optimization-based approach for sizing and  composition of Arctic offshore drilling support fleets considering  cost-efficiency",
    "abstract": "This article presents an optimization-based approach for sizing and\ncomposition of an Arctic offshore drilling support fleet considering\ncost-efficiency. The approach studies the main types of duties related to\nArctic offshore drillings: supply, towing, anchor handling, standby, oil spill\nresponse, firefighting, and ice management. The approach considers the combined\neffect of the expected costs of accidental events, the versatility of\nindividual support vessels, and ice management. The approach applies an\nArtificial Bee Colony algorithm-based optimization procedure. As demonstrated\nthrough case studies, the approach may help to find a range of cost-efficient\nfleet compositions. Some of the obtained solutions are similar to corresponding\nreal-life fleets, indicating that the approach works in principle. Sensitivity\nanalyses indicate that the consideration of the expected costs from accidental\nevents significantly impacts the obtained solution, and that investments to\nreduce these costs may improve the overall cost-efficiency of an Arctic\noffshore drilling support fleet.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Aleksander A. Kondratenko",
      "Martin Bergstr\u00f6m",
      "Mikko Suominen",
      "Pentti Kujala"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12263"
  },
  {
    "id": "arXiv:2205.12264",
    "title": "Efficient Update of Redundancy Matrices for Truss and Frame Structures",
    "abstract": "Redundancy matrices provide insights into the load carrying behavior of\nstatically indeterminate structures. This information can be employed for the\ndesign and analysis of structures with regard to certain objectives, for\nexample reliability, robustness, or adaptability. In this context, the\nstructure is often iteratively examined with the help of slight adjustments.\nHowever, this procedure generally requires a high computational effort for the\nrecalculation of the redundancy matrix due to the necessity of costly matrix\noperations. This paper addresses this problem by providing generic algebraic\nformulations for efficiently updating the redundancy matrix (and related\nmatrices). The formulations include various modifications like adding,\nremoving, and exchanging elements and are applicable to truss and frame\nstructures. With several examples, we demonstrate the interaction between the\nformulas and their mechanical interpretation. Finally, a performance test for a\nscaleable structure is presented.",
    "descriptor": "",
    "authors": [
      "Tim Krake",
      "Malte von Scheven",
      "Jan Gade",
      "Moataz Abdelaal",
      "Daniel Weiskopf",
      "Manfred Bischoff"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.12264"
  },
  {
    "id": "arXiv:2205.12265",
    "title": "The Economic Viability of an In-Home Monitoring System in the context of  an Aged Care Setting",
    "abstract": "The aged care sector in Australia faces significant challenges. While many of\nthese issues have been clearly identified, their urgency has been further\nhighlighted during the COVID-19 pandemic. Technology such as in-home monitoring\nis one way to address some of these challenges. However, the efficacy of\ntechnology must be considered together with its implementation and running\ncosts to ensure that there is a return on investment, and it is economically\nviable as a solution. A pilot program was run in-home monitoring system to test\nthe efficacy of this system. This paper focuses on an economic analysis to\nbetter understand the financial viability of such systems. Using a secondary\nanalysis approach, the findings identified that revenue could be generated by\nproviding carers with additional services such as real-time monitoring of the\nclient, which can foster deeper relationships with the customer, along with\nsavings of healthcare costs to carers, service providers and Government.\nSavings are related to the earlier intervention of critical events that are\nidentified by the system, as delays in treatment of some critical events can\ncreate much more severe and costly health outcomes. Further health costs\nsavings can be made via trend analysis which can show more nuanced health\ndeterioration that is often missed. The implementation of preventative measures\nvia this identification can reduce the chances of critical events occurring\nwhich have much higher costs. Overall, monitoring systems lead to a transition\nfrom a reactive to a preventative services offering, delivering more targeted\nand personalised care.",
    "descriptor": "\nComments: The Paper is accepted to publish in a journal\n",
    "authors": [
      "Frank Perri",
      "Shah J Miah",
      "Steve Zanon",
      "Keis Ohtsuka"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.12265"
  },
  {
    "id": "arXiv:2205.12267",
    "title": "DRL-based Resource Allocation in Remote State Estimation",
    "abstract": "Remote state estimation, where sensors send their measurements of distributed\ndynamic plants to a remote estimator over shared wireless resources, is\nessential for mission-critical applications of Industry 4.0. Existing\nalgorithms on dynamic radio resource allocation for remote estimation systems\nassumed oversimplified wireless communications models and can only work for\nsmall-scale settings. In this work, we consider remote estimation systems with\npractical wireless models over the orthogonal multiple-access and\nnon-orthogonal multiple-access schemes. We derive necessary and sufficient\nconditions under which remote estimation systems can be stabilized. The\nconditions are described in terms of the transmission power budget, channel\nstatistics, and plants' parameters. For each multiple-access scheme, we\nformulate a novel dynamic resource allocation problem as a decision-making\nproblem for achieving the minimum overall long-term average estimation\nmean-square error. Both the estimation quality and the channel quality states\nare taken into account for decision making. We systematically investigated the\nproblems under different multiple-access schemes with large discrete, hybrid\ndiscrete-and-continuous, and continuous action spaces, respectively. We propose\nnovel action-space compression methods and develop advanced deep reinforcement\nlearning algorithms to solve the problems. Numerical results show that our\nalgorithms solve the resource allocation problems effectively and provide much\nbetter scalability than the literature.",
    "descriptor": "\nComments: Paper submitted to IEEE for possible publication. arXiv admin note: text overlap with arXiv:2205.11861\n",
    "authors": [
      "Gaoyang Pang",
      "Wanchun Liu",
      "Yonghui Li",
      "Branka Vucetic"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12267"
  },
  {
    "id": "arXiv:2205.12268",
    "title": "Wavelet Feature Maps Compression for Image-to-Image CNNs",
    "abstract": "Convolutional Neural Networks (CNNs) are known for requiring extensive\ncomputational resources, and quantization is among the best and most common\nmethods for compressing them. While aggressive quantization (i.e., less than\n4-bits) performs well for classification, it may cause severe performance\ndegradation in image-to-image tasks such as semantic segmentation and depth\nestimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a\nnovel approach for high-resolution activation maps compression integrated with\npoint-wise convolutions, which are the main computational cost of modern\narchitectures. To this end, we use an efficient and hardware-friendly\nHaar-wavelet transform, known for its effectiveness in image compression, and\ndefine the convolution on the compressed activation map. We experiment on\nvarious tasks, that benefit from high-resolution input, and by combining WCC\nwith light quantization, we achieve compression rates equivalent to 1-4bit\nactivation quantization with relatively small and much more graceful\ndegradation in performance.",
    "descriptor": "",
    "authors": [
      "Shahaf E. Finder",
      "Yair Zohav",
      "Maor Ashkenazi",
      "Eran Treister"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12268"
  },
  {
    "id": "arXiv:2205.12270",
    "title": "The Next-Generation OS Process Abstraction",
    "abstract": "Operating Systems are built upon a set of abstractions to provide resource\nmanagement and programming APIs for common functionality, such as\nsynchronization, communication, protection, and I/O. The process abstraction is\nthe bridge across these two aspects; unsurprisingly, research efforts pay\nparticular attention to the process abstraction, aiming at enhancing security,\nimproving performance, and supporting hardware innovations. However, given the\nintrinsic difficulties to implement modifications at the OS level, recent\nendeavors have not yet been widely adopted in production-oriented OSes. Still,\nwe believe the current hardware evolution and new application requirements\nprovide favorable conditions to change this trend. This paper evaluates recent\nresearch on OS process features identifying potential evolution paths. We\nderive a set of relevant process characteristics, and propose how to extend\nthem as to benefit OSes and applications.",
    "descriptor": "",
    "authors": [
      "Rodrigo Siqueira",
      "Nelson Lago",
      "Fabio Kon",
      "Dejan Miloji\u010di\u0107"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.12270"
  },
  {
    "id": "arXiv:2205.12292",
    "title": "Trajectory Optimization for Physics-Based Reconstruction of 3d Human  Pose from Monocular Video",
    "abstract": "We focus on the task of estimating a physically plausible articulated human\nmotion from monocular video. Existing approaches that do not consider physics\noften produce temporally inconsistent output with motion artifacts, while\nstate-of-the-art physics-based approaches have either been shown to work only\nin controlled laboratory conditions or consider simplified body-ground contact\nlimited to feet. This paper explores how these shortcomings can be addressed by\ndirectly incorporating a fully-featured physics engine into the pose estimation\nprocess. Given an uncontrolled, real-world scene as input, our approach\nestimates the ground-plane location and the dimensions of the physical body\nmodel. It then recovers the physical motion by performing trajectory\noptimization. The advantage of our formulation is that it readily generalizes\nto a variety of scenes that might have diverse ground properties and supports\nany form of self-contact and contact between the articulated body and scene\ngeometry. We show that our approach achieves competitive results with respect\nto existing physics-based methods on the Human3.6M benchmark, while being\ndirectly applicable without re-training to more complex dynamic motions from\nthe AIST benchmark and to uncontrolled internet videos.",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Erik G\u00e4rtner",
      "Mykhaylo Andriluka",
      "Hongyi Xu",
      "Cristian Sminchisescu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12292"
  },
  {
    "id": "arXiv:2205.12295",
    "title": "lpSpikeCon: Enabling Low-Precision Spiking Neural Network Processing for  Efficient Unsupervised Continual Learning on Autonomous Agents",
    "abstract": "Recent advances have shown that SNN-based systems can efficiently perform\nunsupervised continual learning due to their bio-plausible learning rule, e.g.,\nSpike-Timing-Dependent Plasticity (STDP). Such learning capabilities are\nespecially beneficial for use cases like autonomous agents (e.g., robots and\nUAVs) that need to continuously adapt to dynamically changing\nscenarios/environments, where new data gathered directly from the environment\nmay have novel features that should be learned online. Current state-of-the-art\nworks employ high-precision weights (i.e., 32 bit) for both training and\ninference phases, which pose high memory and energy costs thereby hindering\nefficient embedded implementations of such systems for battery-driven mobile\nautonomous systems. On the other hand, precision reduction may jeopardize the\nquality of unsupervised continual learning due to information loss. Towards\nthis, we propose lpSpikeCon, a novel methodology to enable low-precision SNN\nprocessing for efficient unsupervised continual learning on\nresource-constrained autonomous agents/systems. Our lpSpikeCon methodology\nemploys the following key steps: (1) analyzing the impacts of training the SNN\nmodel under unsupervised continual learning settings with reduced weight\nprecision on the inference accuracy; (2) leveraging this study to identify SNN\nparameters that have a significant impact on the inference accuracy; and (3)\ndeveloping an algorithm for searching the respective SNN parameter values that\nimprove the quality of unsupervised continual learning. The experimental\nresults show that our lpSpikeCon can reduce weight memory of the SNN model by\n8x (i.e., by judiciously employing 4-bit weights) for performing online\ntraining with unsupervised continual learning and achieve no accuracy loss in\nthe inference phase, as compared to the baseline model with 32-bit weights\nacross different network sizes.",
    "descriptor": "\nComments: To appear at the 2022 International Joint Conference on Neural Networks (IJCNN), the 2022 IEEE World Congress on Computational Intelligence (WCCI), July 2022, Padova, Italy\n",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12295"
  },
  {
    "id": "arXiv:2205.12301",
    "title": "FreDo: Frequency Domain-based Long-Term Time Series Forecasting",
    "abstract": "The ability to forecast far into the future is highly beneficial to many\napplications, including but not limited to climatology, energy consumption, and\nlogistics. However, due to noise or measurement error, it is questionable how\nfar into the future one can reasonably predict. In this paper, we first\nmathematically show that due to error accumulation, sophisticated models might\nnot outperform baseline models for long-term forecasting. To demonstrate, we\nshow that a non-parametric baseline model based on periodicity can actually\nachieve comparable performance to a state-of-the-art Transformer-based model on\nvarious datasets. We further propose FreDo, a frequency domain-based neural\nnetwork model that is built on top of the baseline model to enhance its\nperformance and which greatly outperforms the state-of-the-art model. Finally,\nwe validate that the frequency domain is indeed better by comparing univariate\nmodels trained in the frequency v.s. time domain.",
    "descriptor": "",
    "authors": [
      "Fan-Keng Sun",
      "Duane S. Boning"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12301"
  },
  {
    "id": "arXiv:2205.12302",
    "title": "Garden-Path Traversal within GPT-2",
    "abstract": "In recent years, massive language models consisting exclusively of\ntransformer decoders, led by the GPT-x family, have become increasingly\npopular. While studies have examined the behavior of these models, they tend to\nonly focus on the output of the language model, avoiding analyzing their\ninternal states despite such analyses being popular tools used within BERTology\nto study transformer encoders. We present a collection of methods for analyzing\nGPT-2's hidden states, and use the model's navigation of garden path sentences\nas a case study to demonstrate the utility of studying this model's behavior\nbeyond its output alone. To support this analysis, we introduce a novel dataset\nconsisting of 3 different types of garden path sentences, along with scripts to\nmanipulate them. We find that measuring Manhattan distances and cosine\nsimilarities between hidden states shows that GPT-2 navigates these sentences\nmore intuitively than conventional methods that predict from the model's output\nalone.",
    "descriptor": "\nComments: 7 pages, 6 figures, preprint\n",
    "authors": [
      "William Jurayj",
      "William Rudman",
      "Carsten Eickhoff"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12302"
  },
  {
    "id": "arXiv:2205.12304",
    "title": "Adaptive multilingual speech recognition with pretrained models",
    "abstract": "Multilingual speech recognition with supervised learning has achieved great\nresults as reflected in recent research. With the development of pretraining\nmethods on audio and text data, it is imperative to transfer the knowledge from\nunsupervised multilingual models to facilitate recognition, especially in many\nlanguages with limited data. Our work investigated the effectiveness of using\ntwo pretrained models for two modalities: wav2vec 2.0 for audio and MBART50 for\ntext, together with the adaptive weight techniques to massively improve the\nrecognition quality on the public datasets containing CommonVoice and Europarl.\nOverall, we noticed an 44% improvement over purely supervised learning, and\nmore importantly, each technique provides a different reinforcement in\ndifferent languages. We also explore other possibilities to potentially obtain\nthe best model by slightly adding either depth or relative attention to the\narchitecture.",
    "descriptor": "\nComments: Submitted to INTERSPEECH 2022\n",
    "authors": [
      "Ngoc-Quan Pham",
      "Alex Waibel",
      "Jan Niehues"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12304"
  },
  {
    "id": "arXiv:2205.12307",
    "title": "Approximate Euclidean lengths and distances beyond Johnson-Lindenstrauss",
    "abstract": "A classical result of Johnson and Lindenstrauss states that a set of $n$ high\ndimensional data points can be projected down to $O(\\log n/\\epsilon^2)$\ndimensions such that the square of their pairwise distances is preserved up to\na small distortion $\\epsilon\\in(0,1)$. It has been proved that the JL lemma is\noptimal for the general case, therefore, improvements can only be explored for\nspecial cases. This work aims to improve the $\\epsilon^{-2}$ dependency based\non techniques inspired by the Hutch++ Algorithm , which reduces $\\epsilon^{-2}$\nto $\\epsilon^{-1}$ for the related problem of implicit matrix trace estimation.\nFor $\\epsilon=0.01$, for example, this translates to $100$ times less\nmatrix-vector products in the matrix-vector query model to achieve the same\naccuracy as other previous estimators. We first present an algorithm to\nestimate the Euclidean lengths of the rows of a matrix. We prove element-wise\nprobabilistic bounds that are at least as good as standard JL approximations in\nthe worst-case, but are asymptotically better for matrices with decaying\nspectrum. Moreover, for any matrix, regardless of its spectrum, the algorithm\nachieves $\\epsilon$-accuracy for the total, Frobenius norm-wise relative error\nusing only $O(\\epsilon^{-1})$ queries. This is a quadratic improvement over the\nnorm-wise error of standard JL approximations. We finally show how these\nresults can be extended to estimate the Euclidean distances between data points\nand to approximate the statistical leverage scores of a tall-and-skinny data\nmatrix, which are ubiquitous for many applications. Proof-of-concept numerical\nexperiments are presented to validate the theoretical analysis.",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Aleksandros Sobczyk",
      "Mathieu Luisier"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12307"
  },
  {
    "id": "arXiv:2205.12309",
    "title": "Structured Prompt Tuning",
    "abstract": "We propose structured prompt tuning, a simple and effective method to improve\nprompt tuning. Instead of prepending a sequence of tunable embeddings to the\ninput, we generate the soft prompt embeddings through a hypernetwork. Our\napproach subsumes the standard prompt tuning, allows more flexibility in model\ndesign and can be applied to both single-task and multi-task training settings.\nEmpirically, structured prompt tuning shows a gain of +1.2$~1.5 points on the\nGLUE benchmark and is less sensitive to the change of learning rate, compared\nto standard prompt tuning.",
    "descriptor": "",
    "authors": [
      "Chi-Liang Liu",
      "Hung-yi Lee",
      "Wen-tau Yih"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12309"
  },
  {
    "id": "arXiv:2205.12311",
    "title": "Fast & Furious: Modelling Malware Detection as Evolving Data Streams",
    "abstract": "Malware is a major threat to computer systems and imposes many challenges to\ncyber security. Targeted threats, such as ransomware, cause millions of dollars\nin losses every year. The constant increase of malware infections has been\nmotivating popular antiviruses (AVs) to develop dedicated detection strategies,\nwhich include meticulously crafted machine learning (ML) pipelines. However,\nmalware developers unceasingly change their samples features to bypass\ndetection. This constant evolution of malware samples causes changes to the\ndata distribution (i.e., concept drifts) that directly affect ML model\ndetection rates. In this work, we evaluate the impact of concept drift on\nmalware classifiers for two Android datasets: DREBIN (~130K apps) and AndroZoo\n(~350K apps). Android is a ubiquitous operating system for smartphones, which\nstimulates attackers to regularly create and update malware to the platform. We\nconducted a longitudinal evaluation by (i) classifying malware samples\ncollected over nine years (2009-2018), (ii) reviewing concept drift detection\nalgorithms to attest its pervasiveness, (iii) comparing distinct ML approaches\nto mitigate the issue, and (iv) proposing an ML data stream pipeline that\noutperformed literature approaches. As a result, we observed that updating\nevery component of the pipeline in response to concept drifts allows the\nclassification model to achieve increasing detection rates as the data\nrepresentation (extracted features) is updated. Furthermore, we discuss the\nimpact of the changes on the classification models by comparing the variations\nin the extracted features.",
    "descriptor": "",
    "authors": [
      "Fabr\u00edcio Ceschin",
      "Marcus Botacin",
      "Heitor Murilo Gomes",
      "Felipe Pinag\u00e9",
      "Luiz S. Oliveira",
      "Andr\u00e9 Gr\u00e9gio"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12311"
  },
  {
    "id": "arXiv:2205.12318",
    "title": "ColdGuess: A General and Effective Relational Graph Convolutional  Network to Tackle Cold Start Cases",
    "abstract": "Low-quality listings and bad actor behavior in online retail websites\nthreatens e-commerce business as these result in sub-optimal buying experience\nand erode customer trust. When a new listing is created, how to tell it has\ngood-quality? Is the method effective, fast, and scalable? Previous approaches\noften have three limitations/challenges: (1) unable to handle cold start\nproblems where new sellers/listings lack sufficient selling histories. (2)\ninability of scoring hundreds of millions of listings at scale, or compromise\nperformance for scalability. (3) has space challenges from large-scale graph\nwith giant e-commerce business size. To overcome these limitations/challenges,\nwe proposed ColdGuess, an inductive graph-based risk predictor built upon a\nheterogeneous seller product graph, which effectively identifies risky\nseller/product/listings at scale. ColdGuess tackles the large-scale graph by\nconsolidated nodes, and addresses the cold start problems using homogeneous\ninfluence1. The evaluation on real data demonstrates that ColdGuess has stable\nperformance as the number of unknown features increases. It outperforms the\nlightgbm2 by up to 34 pcp ROC-AUC in a cold start case when a new seller sells\na new product . The resulting system, ColdGuess, is effective, adaptable to\nchanging risky seller behavior, and is already in production",
    "descriptor": "",
    "authors": [
      "Bo He",
      "Xiang Song",
      "Vincent Gao",
      "Christos Faloutsos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12318"
  },
  {
    "id": "arXiv:2205.12323",
    "title": "Scoring Coreference Chains with Split-Antecedent Anaphors",
    "abstract": "Anaphoric reference is an aspect of language interpretation covering a\nvariety of types of interpretation beyond the simple case of identity reference\nto entities introduced via nominal expressions covered by the traditional\ncoreference task in its most recent incarnation in ONTONOTES and similar\ndatasets. One of these cases that go beyond simple coreference is anaphoric\nreference to entities that must be added to the discourse model via\naccommodation, and in particular split-antecedent references to entities\nconstructed out of other entities, as in split-antecedent plurals and in some\ncases of discourse deixis. Although this type of anaphoric reference is now\nannotated in many datasets, systems interpreting such references cannot be\nevaluated using the Reference coreference scorer Pradhan et al. (2014). As part\nof the work towards a new scorer for anaphoric reference able to evaluate all\naspects of anaphoric interpretation in the coverage of the Universal Anaphora\ninitiative, we propose in this paper a solution to the technical problem of\ngeneralizing existing metrics for identity anaphora so that they can also be\nused to score cases of split-antecedents. This is the first such proposal in\nthe literature on anaphora or coreference, and has been successfully used to\nscore both split-antecedent plural references and discourse deixis in the\nrecent CODI/CRAC anaphora resolution in dialogue shared tasks.",
    "descriptor": "",
    "authors": [
      "Silviu Paun",
      "Juntao Yu",
      "Nafise Sadat Moosavi",
      "Massimo Poesio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12323"
  },
  {
    "id": "arXiv:2205.12324",
    "title": "Linearly representable games and pseudo-polynomial calculation of the  Shapley value",
    "abstract": "We introduce the notion of linearly representable games. Broadly speaking,\nthese are TU games that can be described by as many parameters as the number of\nplayers, like weighted voting games, airport games, or bankruptcy games. We\nshow that the Shapley value calculation is pseudo-polynomial for linearly\nrepresentable games. This is a generalization of many classical and recent\nresults in the literature. Our method naturally turns into a strictly\npolynomial algorithm when the parameters are polynomial in the number of\nplayers.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Ferenc Ill\u00e9s"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.12324"
  },
  {
    "id": "arXiv:2205.12327",
    "title": "Beyond Impossibility: Balancing Sufficiency, Separation and Accuracy",
    "abstract": "Among the various aspects of algorithmic fairness studied in recent years,\nthe tension between satisfying both \\textit{sufficiency} and\n\\textit{separation} -- e.g. the ratios of positive or negative predictive\nvalues, and false positive or false negative rates across groups -- has\nreceived much attention. Following a debate sparked by COMPAS, a criminal\njustice predictive system, the academic community has responded by laying out\nimportant theoretical understanding, showing that one cannot achieve both with\nan imperfect predictor when there is no equal distribution of labels across the\ngroups. In this paper, we shed more light on what might be still possible\nbeyond the impossibility -- the existence of a trade-off means we should aim to\nfind a good balance within it. After refining the existing theoretical result,\nwe propose an objective that aims to balance \\textit{sufficiency} and\n\\textit{separation} measures, while maintaining similar accuracy levels. We\nshow the use of such an objective in two empirical case studies, one involving\na multi-objective framework, and the other fine-tuning of a model pre-trained\nfor accuracy. We show promising results, where better trade-offs are achieved\ncompared to existing alternatives.",
    "descriptor": "",
    "authors": [
      "Limor Gultchin",
      "Vincent Cohen-Addad",
      "Sophie Giffard-Roisin",
      "Varun Kanade",
      "Frederik Mallmann-Trenn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.12327"
  },
  {
    "id": "arXiv:2205.12328",
    "title": "Multilevel sentiment analysis in arabic",
    "abstract": "In this study, we aimed to improve the performance results of Arabic\nsentiment analysis. This can be achieved by investigating the most successful\nmachine learning method and the most useful feature vector to classify\nsentiments in both term and document levels into two (positive or negative)\ncategories. Moreover, specification of one polarity degree for the term that\nhas more than one is investigated. Also to handle the negations and\nintensifications, some rules are developed. According to the obtained results,\nArtificial Neural Network classifier is nominated as the best classifier in\nboth term and document level sentiment analysis (SA) for Arabic Language.\nFurthermore, the average F-score achieved in the term level SA for both\npositive and negative testing classes is 0.92. In the document level SA, the\naverage F-score for positive testing classes is 0.94, while for negative\nclasses is 0.93.",
    "descriptor": "\nComments: 10 pages, 3 figures, Published in: 2019 IEEE 7th Palestinian International Conference on Electrical and Computer Engineering (PICECE), Date of Conference: 26-27 March 2019\n",
    "authors": [
      "Ahmed Nassar",
      "Ebru Sezer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12328"
  },
  {
    "id": "arXiv:2205.12331",
    "title": "Certified Robustness Against Natural Language Attacks by Causal  Intervention",
    "abstract": "Deep learning models have achieved great success in many fields, yet they are\nvulnerable to adversarial examples. This paper follows a causal perspective to\nlook into the adversarial vulnerability and proposes Causal Intervention by\nSemantic Smoothing (CISS), a novel framework towards robustness against natural\nlanguage attacks. Instead of merely fitting observational data, CISS learns\ncausal effects p(y|do(x)) by smoothing in the latent semantic space to make\nrobust predictions, which scales to deep architectures and avoids tedious\nconstruction of noise customized for specific attacks. CISS is provably robust\nagainst word substitution attacks, as well as empirically robust even when\nperturbations are strengthened by unknown attack algorithms. For example, on\nYELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness\nagainst word substitutions, and achieves 79.4% empirical robustness when\nsyntactic attacks are integrated.",
    "descriptor": "",
    "authors": [
      "Haiteng Zhao",
      "Chang Ma",
      "Xinshuai Dong",
      "Anh Tuan Luu",
      "Zhi-Hong Deng",
      "Hanwang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12331"
  },
  {
    "id": "arXiv:2205.12332",
    "title": "Constant Curvature Curve Tube Codes for Low-Latency Analog Error  Correction",
    "abstract": "Recent research in ultra-reliable and low latency communications (URLLC) for\nfuture wireless systems has spurred interest in short block-length codes. In\nthis context, we introduce a new class of high-dimension constant curvature\ncurves codes for analog error correction of independent continuous-alphabet\nuniform sources. In particular, we employ the circumradius function from knot\ntheory to prescribe insulating tubes about the centerline of constant curvature\ncurves. We then use tube packing density within a hypersphere to optimize the\ncurve parameters. The resulting constant curvature curve tube (C3T) codes\npossess the smallest possible latency -- block-length is unity under bandwidth\nexpansion mapping. Further, the codes provide within $5$ dB of Shannon's\noptimal performance theoretically achievable at the lower range of\nsignal-to-noise ratios and BW expansion factors. We exploit the fact that the\nC3T encoder locus is a geodesic on a flat torus in even dimensions and a\ngeneralized helix in odd dimensions to obtain useful code properties and\nprovide noise-reducing projections at the decoder stage. We validate the\nperformance of these codes using fully connected multi-layer perceptrons that\napproximate maximum likelihood decoders. For the case of independent and\nidentically distributed uniform sources, we show that analog error correction\nis advantageous over digital coding in terms of required block-lengths needed\nto match {signal-to-noise ratio, source-to-distortion ratio} tuples. The best\npossible digital codes require two to three orders of magnitude higher latency\ncompared to C3T codes, thereby demonstrating the latter's utility for URLLC.",
    "descriptor": "\nComments: 11 pages, 2 tables, 6 figures\n",
    "authors": [
      "Robert M. Taylor Jr.",
      "Anders M. Buvarp",
      "Kumar Vijay Mishra",
      "Lamine M. Mili",
      "Amir I. Zaghloul"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12332"
  },
  {
    "id": "arXiv:2205.12335",
    "title": "K-12BERT: BERT for K-12 education",
    "abstract": "Online education platforms are powered by various NLP pipelines, which\nutilize models like BERT to aid in content curation. Since the inception of the\npre-trained language models like BERT, there have also been many efforts toward\nadapting these pre-trained models to specific domains. However, there has not\nbeen a model specifically adapted for the education domain (particularly K-12)\nacross subjects to the best of our knowledge. In this work, we propose to train\na language model on a corpus of data curated by us across multiple subjects\nfrom various sources for K-12 education. We also evaluate our model, K12-BERT,\non downstream tasks like hierarchical taxonomy tagging.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Vasu Goel",
      "Dhruv Sahnan",
      "Venktesh V",
      "Gaurav Sharma",
      "Deep Dwivedi",
      "Mukesh Mohania"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12335"
  },
  {
    "id": "arXiv:2205.12339",
    "title": "Women, artificial intelligence, and key positions in collaboration  networks: Towards a more equal scientific ecosystem",
    "abstract": "Scientific collaboration in almost every discipline is mainly driven by the\nneed of sharing knowledge, expertise, and pooled resources. Science is becoming\nmore complex which has encouraged scientists to involve more in collaborative\nresearch projects in order to better address the challenges. As a highly\ninterdisciplinary field with a rapidly evolving scientific landscape,\nartificial intelligence calls for researchers with special profiles covering a\ndiverse set of skills and expertise. Understanding gender aspects of scientific\ncollaboration is of paramount importance, especially in a field such as\nartificial intelligence that has been attracting large investments. Using\nsocial network analysis, natural language processing, and machine learning and\nfocusing on artificial intelligence publications for the period from 2000 to\n2019, in this work, we comprehensively investigated the effects of several\ndriving factors on acquiring key positions in scientific collaboration networks\nthrough a gender lens. It was found that, regardless of gender, scientific\nperformance in terms of quantity and impact plays a crucial in possessing the\n\"social researcher\" in the network. However, subtle differences were observed\nbetween female and male researchers in acquiring the \"local influencer\" role.",
    "descriptor": "\nComments: 20 pages, 6 figures\n",
    "authors": [
      "Anahita Hajibabaei",
      "Andrea Schiffauerova",
      "Ashkan Ebadi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12339"
  },
  {
    "id": "arXiv:2205.12342",
    "title": "Face2Text revisited: Improved data set and baseline results",
    "abstract": "Current image description generation models do not transfer well to the task\nof describing human faces. To encourage the development of more human-focused\ndescriptions, we developed a new data set of facial descriptions based on the\nCelebA image data set. We describe the properties of this data set, and present\nresults from a face description generator trained on it, which explores the\nfeasibility of using transfer learning from VGGFace/ResNet CNNs. Comparisons\nare drawn through both automated metrics and human evaluation by 76\nEnglish-speaking participants. The descriptions generated by the VGGFace-LSTM +\nAttention model are closest to the ground truth according to human evaluation\nwhilst the ResNet-LSTM + Attention model obtained the highest CIDEr and CIDEr-D\nresults (1.252 and 0.686 respectively). Together, the new data set and these\nexperimental results provide data and baselines for future work in this area.",
    "descriptor": "\nComments: 7 pages, 5 figures, 4 tables, to appear in LREC 2022 (P-VLAM workshop)\n",
    "authors": [
      "Marc Tanti",
      "Shaun Abdilla",
      "Adrian Muscat",
      "Claudia Borg",
      "Reuben A. Farrugia",
      "Albert Gatt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.12342"
  },
  {
    "id": "arXiv:2205.12344",
    "title": "El-WaveHoltz: A Time-Domain Iterative Solver for Time-Harmonic Elastic  Waves",
    "abstract": "We consider the application of the WaveHoltz iteration to time-harmonic\nelastic wave equations with energy conserving boundary conditions. The original\nWaveHoltz iteration for acoustic Helmholtz problems is a fixed-point iteration\nthat filters the solution of the wave equation with time-harmonic forcing and\nboundary data. As in the original WaveHoltz method, we reformulate the fixed\npoint iteration as a positive definite linear system of equations that is\niteratively solved by a Krylov method. We present two time-stepping schemes,\none explicit and one (novel) implicit, which completely remove time\ndiscretization error from the WaveHoltz solution by performing a simple\nmodification of the initial data and time-stepping scheme. Numerical\nexperiments indicate an iteration scaling similar to that of the original\nWaveHoltz method, and that the convergence rate is dictated by the shortest\n(shear) wave speed of the problem. We additionally show that the implicit\nscheme can be advantageous in practice for meshes with disparate element sizes.",
    "descriptor": "",
    "authors": [
      "Daniel Appel\u00f6",
      "Fortino Garcia",
      "Allen Alvarez Loya",
      "Olof Runborg"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12344"
  },
  {
    "id": "arXiv:2205.12349",
    "title": "Extensions and Analysis of an Iterative Solution of the Helmholtz  Equation via the Wave Equation",
    "abstract": "In this paper we extend analysis of the WaveHoltz iteration -- a time-domain\niterative method for the solution of the Helmholtz equation. We expand the\nprevious analysis of energy conserving problems and prove convergence of the\nWaveHoltz iteration for problems with impedance boundary conditions in a single\nspatial dimension. We then consider interior Dirichlet/Neumann problems with\ndamping in any spatial dimension, and show that for a sufficient level of\ndamping the WaveHoltz iteration converges in a number of iteration independent\nof the frequency. Finally, we present a discrete analysis of the WaveHoltz\niteration for a family of higher order time-stepping schemes. We show that the\nfixed-point of the discrete WaveHoltz iteration converges to the discrete\nHelmholtz solution with the order of the time-stepper chosen. We present\nnumerical examples and demonstrate that it is possible to completely remove\ntime discretization error from the WaveHoltz solution through careful analysis\nof the discrete iteration together with updated quadrature formulas.",
    "descriptor": "",
    "authors": [
      "Fortino Garcia",
      "Daniel Appel\u00f6",
      "Olof Runborg"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12349"
  },
  {
    "id": "arXiv:2205.12350",
    "title": "Telechain: Bridging Telecom Policy and Blockchain Practice",
    "abstract": "The use of blockchain in regulatory ecosystems is a promising approach to\naddress challenges of compliance among mutually untrusted entities. In this\nwork, we consider applications of blockchain technologies in telecom\nregulations. In particular, we address growing concerns around Unsolicited\nCommercial Communication (UCC aka. spam) sent through text messages (SMS) and\nphone calls in India. Despite several regulatory measures taken to curb the\nmenace of spam it continues to be a nuisance to subscribers while posing\nchallenges to telecom operators and regulators alike.\nIn this paper, we present a consortium blockchain based architecture to\naddress the problem of UCC in India. Our solution improves subscriber\nexperiences, improves the efficiency of regulatory processes while also\npositively impacting all stakeholders in the telecom ecosystem. Unlike previous\napproaches to the problem of UCC, which are all ex-post, our approach to\nadherence to the regulations is ex-ante. The proposal described in this paper\nis a primary contributor to the revision of regulations concerning UCC and spam\nby the Telecom Regulatory Authority of India (TRAI). The new regulations\npublished in July 2018 were first of a kind in the world and amended the 2010\nTelecom Commercial Communication Customer Preference Regulation (TCCCPR),\nthrough mandating the use of a blockchain/distributed ledgers in addressing the\nUCC problem. In this paper, we provide a holistic account of of the projects'\nevolution from (1) its design and strategy, to (2) regulatory and policy\naction, (3) country wide implementation and deployment, and (4) evaluation and\nimpact of the work.",
    "descriptor": "\nComments: 20 pages, 6 figures, 1 table\n",
    "authors": [
      "Sudheesh Singanamalla",
      "Apurv Mehra",
      "Nishanth Chandran",
      "Himanshi Lohchab",
      "Seshanuradha Chava",
      "Asit Kadayan",
      "Sunil Bajpai",
      "Kurtis Heimerl",
      "Richard Anderson",
      "Satya Lokam"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.12350"
  },
  {
    "id": "arXiv:2205.12352",
    "title": "Image Based Password Authentication System",
    "abstract": "Preservation of information and computer security is broadly dependent on the\nsecured authentication system which is underpinned by password. Text based\npassword is a commonly used and available system for authentication. But it\nbears many limitations like shoulder surfing, dictionary attack, Phishing,\nguessing the password etc. In order to overwhelm these vulnerabilities of\nancient textual password, many graphical or image based password authentication\nsystem has been introduced form last few years. But none of this graphical\nsystem is considered as enough adventurous to keep pace with these issues. Here\nwe have proposed an image based password authentication system which is more\nmethodical and can cope up with every vulnerability of recent password\nauthentication system. To make our system hassle free and more reliable, we\nwill only take username from an user for registration purpose as our system\nwill generate a unique key number for that particular user and this key will be\nused as password for later login procedure. The user name and key both will be\nencrypted using a cryptography algorithm to prevent database hacking. There\nwill be a randomized clickable image grid in our system. By clicking on this\nimage grid, user will input the password key for login purpose. Here we have\ndeveloped another method namely shoulder surfing resistant password. To prevent\nthe attack of shoulder surfing, if any user wishes to change our system\nprovided password key then he or she is allowed to do so by using this method.\nBesides this method allows user to change the password every single time of\nlogin. A user doesn't need to enter any textual password for authentication in\nour recent module and hence combination of all these features improve the\nsecurity, usability and user friendliness of our system.",
    "descriptor": "",
    "authors": [
      "Sanjida Akter Sharna",
      "Sheikh Ashraf Ali"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12352"
  },
  {
    "id": "arXiv:2205.12358",
    "title": "A Benchmark and Asymmetrical-Similarity Learning for Practical Image  Copy Detection",
    "abstract": "Image copy detection (ICD) aims to determine whether a query image is an\nedited copy of any image from a reference set. Currently, there are very\nlimited public benchmarks for ICD, while all overlook a critical challenge in\nreal-world applications, i.e., the distraction from hard negative queries.\nSpecifically, some queries are not edited copies but are inherently similar to\nsome reference images. These hard negative queries are easily false recognized\nas edited copies, significantly compromising the ICD accuracy. This observation\nmotivates us to build the first ICD benchmark featuring this characteristic.\nBased on existing ICD datasets, this paper constructs a new dataset by\nadditionally adding 100, 000 and 24, 252 hard negative pairs into the training\nand test set, respectively. Moreover, this paper further reveals a unique\ndifficulty for solving the hard negative problem in ICD, i.e., there is a\nfundamental conflict between current metric learning and ICD. This conflict is:\nthe metric learning adopts symmetric distance while the edited copy is an\nasymmetric (unidirectional) process, e.g., a partial crop is close to its\nholistic reference image and is an edited copy, while the latter cannot be the\nedited copy of the former (in spite the distance is equally small). This\ninsight results in an Asymmetrical-Similarity Learning (ASL) method, which\nallows the similarity in two directions (the query <-> the reference image) to\nbe different from each other. Experimental results show that ASL outperforms\nstate-of-the-art methods by a clear margin, confirming that solving the\nsymmetric-asymmetric conflict is critical for ICD.",
    "descriptor": "",
    "authors": [
      "Wenhao Wang",
      "Yifan Sun",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12358"
  },
  {
    "id": "arXiv:2205.12367",
    "title": "Contour Integration for Eigenvector Nonlinearities",
    "abstract": "Solving polynomial eigenvalue problems with eigenvector nonlinearities (PEPv)\nis an interesting computational challenge, outside the reach of the\nwell-developed methods for nonlinear eigenvalue problems. We present a natural\ngeneralization of these methods which leads to a contour integration approach\nfor computing all eigenvalues of a PEPv in a compact region of the complex\nplane. Our methods can be used to solve any suitably generic system of\npolynomial or rational function equations.",
    "descriptor": "\nComments: 24 pages, 6 figures\n",
    "authors": [
      "Rob Claes",
      "Karl Meerbergen",
      "Simon Telen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12367"
  },
  {
    "id": "arXiv:2205.12368",
    "title": "Medical Scientific Table-to-Text Generation with Human-in-the-Loop under  the Data Sparsity Constraint",
    "abstract": "Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.",
    "descriptor": "",
    "authors": [
      "Heng-Yi Wu",
      "Jingqing Zhang",
      "Julia Ive",
      "Tong Li",
      "Narges Tabari",
      "Bingyuan Chen",
      "Vibhor Gupta",
      "Yike Guo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12368"
  },
  {
    "id": "arXiv:2205.12371",
    "title": "recommenderlab: An R Framework for Developing and Testing Recommendation  Algorithms",
    "abstract": "Algorithms that create recommendations based on observed data have\nsignificant commercial value for online retailers and many other industries.\nRecommender systems have a significant research community, and studying such\nsystems is part of most modern data science curricula. While there is an\nabundance of software that implements recommendation algorithms, there is\nlittle in terms of supporting recommender system research and education. This\npaper describes the open-source software recommenderlab which was created with\nsupporting research and education in mind. The package can be directly\ninstalled in R or downloaded from https://github.com/mhahsler/recommenderlab.",
    "descriptor": "",
    "authors": [
      "Michael Hahsler"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2205.12371"
  },
  {
    "id": "arXiv:2205.12372",
    "title": "TorchNTK: A Library for Calculation of Neural Tangent Kernels of PyTorch  Models",
    "abstract": "We introduce torchNTK, a python library to calculate the empirical neural\ntangent kernel (NTK) of neural network models in the PyTorch framework. We\nprovide an efficient method to calculate the NTK of multilayer perceptrons. We\ncompare the explicit differentiation implementation against autodifferentiation\nimplementations, which have the benefit of extending the utility of the library\nto any architecture supported by PyTorch, such as convolutional networks. A\nfeature of the library is that we expose the user to layerwise NTK components,\nand show that in some regimes a layerwise calculation is more memory efficient.\nWe conduct preliminary experiments to demonstrate use cases for the software\nand probe the NTK.",
    "descriptor": "\nComments: 19 pages, 5 figures\n",
    "authors": [
      "Andrew Engel",
      "Zhichao Wang",
      "Anand D. Sarwate",
      "Sutanay Choudhury",
      "Tony Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12372"
  },
  {
    "id": "arXiv:2205.12374",
    "title": "Learning to Model Editing Processes",
    "abstract": "Most existing sequence generation models produce outputs in one pass, usually\nleft-to-right. However, this is in contrast with a more natural approach that\nhumans use in generating content; iterative refinement and editing. Recent work\nhas introduced edit-based models for various tasks (such as neural machine\ntranslation and text style transfer), but these generally model a single edit\nstep. In this work, we propose modeling editing processes, modeling the whole\nprocess of iteratively generating sequences. We form a conceptual framework to\ndescribe the likelihood of multi-step edits, and describe neural models that\ncan learn a generative model of sequences based on these multistep edits. We\nintroduce baseline results and metrics on this task, finding that modeling\nediting processes improves performance on a variety of axes on both our\nproposed task and related downstream tasks compared to previous single-step\nmodels of edits.",
    "descriptor": "",
    "authors": [
      "Machel Reid",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12374"
  },
  {
    "id": "arXiv:2205.12376",
    "title": "A Comparative Analysis of Ookla Speedtest and Measurement Labs Network  Diagnostic Test (NDT7)",
    "abstract": "Consumers, regulators, and ISPs all use client-based \"speed tests\" to measure\nnetwork performance, both in single-user settings and in aggregate. Two\nprevalent speed tests, Ookla's Speedtest and Measurement Lab's Network\nDiagnostic Test (NDT), are often used for similar purposes, despite having\nsignificant differences in both the test design and implementation and in the\ninfrastructure used to conduct measurements. In this paper, we present a\ncomparative evaluation of Ookla and NDT7 (the latest version of NDT), both in\ncontrolled and wide-area settings. Our goal is to characterize when, how much\nand under what circumstances these two speed tests differ, as well as what\nfactors contribute to the differences. To study the effects of the test design,\nwe conduct a series of controlled, in-lab experiments under a variety of\nnetwork conditions and usage modes (TCP congestion control, native vs. browser\nclient). Our results show that Ookla and NDT7 report similar speeds when the\nlatency between the client and server is low, but that the tools diverge when\npath latency is high. To characterize the behavior of these tools in wide-area\ndeployment, we collect more than 40,000 pairs of Ookla and NDT7 measurements\nacross six months and 67 households, with a range of ISPs and speed tiers. Our\nanalysis demonstrates various systemic issues, including high variability in\nNDT7 test results and systematically under-performing servers in the Ookla\nnetwork.",
    "descriptor": "",
    "authors": [
      "Kyle MacMillan",
      "Tarun Mangla",
      "James Saxon",
      "Nicole P. Marwell",
      "Nick Feamster"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12376"
  },
  {
    "id": "arXiv:2205.12377",
    "title": "Hardness of Maximum Likelihood Learning of DPPs",
    "abstract": "Determinantal Point Processes (DPPs) are a widely used probabilistic model\nfor negatively correlated sets. DPPs have been successfully employed in Machine\nLearning applications to select a diverse, yet representative subset of data.\nIn seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD\nThesis (2011) that the problem is NP-complete. The lack of a formal proof\nprompted Brunel, Moitra, Rigollet and Urschel (COLT 2017) to conjecture that,\nin opposition to Kulesza's conjecture, there exists a polynomial-time algorithm\nfor computing a maximum-likelihood DPP. They also presented some preliminary\nevidence supporting their conjecture.\nIn this work we prove Kulesza's conjecture. In fact, we prove the following\nstronger hardness of approximation result: even computing a\n$\\left(1-O(\\frac{1}{\\log^9{N}})\\right)$-approximation to the maximum\nlog-likelihood of a DPP on a ground set of $N$ elements is NP-complete. At the\nsame time, we also obtain the first polynomial-time algorithm that achieves a\nnontrivial worst-case approximation to the optimal log-likelihood: the\napproximation factor is $\\frac{1}{(1+o(1))\\log{m}}$ unconditionally (for data\nsets that consist of $m$ subsets), and can be improved to $1-\\frac{1+o(1)}{\\log\nN}$ if all $N$ elements appear in a $O(1/N)$-fraction of the subsets.\nIn terms of techniques, we reduce approximating the maximum log-likelihood of\nDPPs on a data set to solving a gap instance of a \"vector coloring\" problem on\na hypergraph. Such a hypergraph is built on a bounded-degree graph construction\nof Bogdanov, Obata and Trevisan (FOCS 2002), and is further enhanced by the\nstrong expanders of Alon and Capalbo (FOCS 2007) to serve our purposes.",
    "descriptor": "",
    "authors": [
      "Elena Grigorescu",
      "Brendan Juba",
      "Karl Wimmer",
      "Ning Xie"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12377"
  },
  {
    "id": "arXiv:2205.12378",
    "title": "Lyapunov based Stochastic Stability of a Quantum Decision System for  Human-Machine Interaction",
    "abstract": "In mathematical psychology, decision makers are modeled using the Lindbladian\nequations from quantum mechanics to capture important human-centric features\nsuch as order effects and violation of the sure thing principle. We consider\nhuman-machine interaction involving a quantum decision maker (human) and a\ncontroller (machine). Given a sequence of human decisions over time, how can\nthe controller dynamically provide input messages to adapt these decisions so\nas to converge to a specific decision? We show via novel stochastic Lyapunov\narguments how the Lindbladian dynamics of the quantum decision maker can be\ncontrolled to converge to a specific decision asymptotically. Our methodology\nyields a useful mathematical framework for human-sensor decision making. The\nstochastic Lyapunov results are also of independent interest as they generalize\nrecent results in the literature.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2204.00059\n",
    "authors": [
      "Luke Snow",
      "Shashwat Jain",
      "Vikram Krishnamurthy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12378"
  },
  {
    "id": "arXiv:2205.12379",
    "title": "Imposing Gaussian Pre-Activations in a Neural Network",
    "abstract": "The goal of the present work is to propose a way to modify both the\ninitialization distribution of the weights of a neural network and its\nactivation function, such that all pre-activations are Gaussian. We propose a\nfamily of pairs initialization/activation, where the activation functions span\na continuum from bounded functions (such as Heaviside or tanh) to the identity\nfunction.\nThis work is motivated by the contradiction between existing works dealing\nwith Gaussian pre-activations: on one side, the works in the line of the Neural\nTangent Kernels and the Edge of Chaos are assuming it, while on the other side,\ntheoretical and experimental results challenge this hypothesis.\nThe family of pairs initialization/activation we are proposing will help us\nto answer this hot question: is it desirable to have Gaussian pre-activations\nin a neural network?",
    "descriptor": "",
    "authors": [
      "Pierre Wolinski",
      "Julyan Arbel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12379"
  },
  {
    "id": "arXiv:2205.12381",
    "title": "First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual  Information Maximization",
    "abstract": "How can we train an assistive human-machine interface (e.g., an\nelectromyography-based limb prosthesis) to translate a user's raw command\nsignals into the actions of a robot or computer when there is no prior mapping,\nwe cannot ask the user for supervision in the form of action labels or reward\nfeedback, and we do not have prior knowledge of the tasks the user is trying to\naccomplish? The key idea in this paper is that, regardless of the task, when an\ninterface is more intuitive, the user's commands are less noisy. We formalize\nthis idea as a completely unsupervised objective for optimizing interfaces: the\nmutual information between the user's command signals and the induced state\ntransitions in the environment. To evaluate whether this mutual information\nscore can distinguish between effective and ineffective interfaces, we conduct\nan observational study on 540K examples of users operating various keyboard and\neye gaze interfaces for typing, controlling simulated robots, and playing video\ngames. The results show that our mutual information scores are predictive of\nthe ground-truth task completion metrics in a variety of domains, with an\naverage Spearman's rank correlation of 0.43. In addition to offline evaluation\nof existing interfaces, we use our unsupervised objective to learn an interface\nfrom scratch: we randomly initialize the interface, have the user attempt to\nperform their desired tasks using the interface, measure the mutual information\nscore, and update the interface to maximize mutual information through\nreinforcement learning. We evaluate our method through a user study with 12\nparticipants who perform a 2D cursor control task using a perturbed mouse, and\nan experiment with one user playing the Lunar Lander game using hand gestures.\nThe results show that we can learn an interface from scratch, without any user\nsupervision or prior knowledge of tasks, in under 30 minutes.",
    "descriptor": "",
    "authors": [
      "Siddharth Reddy",
      "Sergey Levine",
      "Anca D. Dragan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12381"
  },
  {
    "id": "arXiv:2205.12382",
    "title": "VoynaSlov: A Data Set of Russian Social Media Activity during the 2022  Ukraine-Russia War",
    "abstract": "In this report, we describe a new data set called VoynaSlov which contains\n21M+ Russian-language social media activities (i.e. tweets, posts, comments)\nmade by Russian media outlets and by the general public during the time of war\nbetween Ukraine and Russia. We scraped the data from two major platforms that\nare widely used in Russia: Twitter and VKontakte (VK), a Russian social media\nplatform based in Saint Petersburg commonly referred to as \"Russian Facebook\".\nWe provide descriptions of our data collection process and data statistics that\ncompare state-affiliated and independent Russian media, and also the two\nplatforms, VK and Twitter. The main differences that distinguish our data from\npreviously released data related to the ongoing war are its focus on Russian\nmedia and consideration of state-affiliation as well as the inclusion of data\nfrom VK, which is more suitable than Twitter for understanding Russian public\nsentiment considering its wide use within Russia. We hope our data set can\nfacilitate future research on information warfare and ultimately enable the\nreduction and prevention of disinformation and opinion manipulation campaigns.\nThe data set is available at https://github.com/chan0park/VoynaSlov and will be\nregularly updated as we continuously collect more data.",
    "descriptor": "",
    "authors": [
      "Chan Young Park",
      "Julia Mendelsohn",
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12382"
  },
  {
    "id": "arXiv:2205.12386",
    "title": "PLAtE: A Large-scale Dataset for List Page Web Extraction",
    "abstract": "Recently, neural models have been leveraged to significantly improve the\nperformance of information extraction from semi-structured websites. However, a\nbarrier for continued progress is the small number of datasets large enough to\ntrain these models. In this work, we introduce the PLAtE (Pages of Lists\nAttribute Extraction) dataset as a challenging new web extraction task. PLAtE\nfocuses on shopping data, specifically extractions from product review pages\nwith multiple items. PLAtE encompasses both the tasks of: (1) finding\nproduct-list segmentation boundaries and (2) extracting attributes for each\nproduct. PLAtE is composed of 53, 905 items from 6, 810 pages, making it the\nfirst large-scale list page web extraction dataset. We construct PLAtE by\ncollecting list pages from Common Crawl, then annotating them on Mechanical\nTurk. Quantitative and qualitative analyses are performed to demonstrate PLAtE\nhas high-quality annotations. We establish strong baseline performance on PLAtE\nwith a SOTA model achieving an F1-score of 0.750 for attribute classification\nand 0.915 for segmentation, indicating opportunities for future research\ninnovations in web extraction.",
    "descriptor": "",
    "authors": [
      "Aidan San",
      "Jan Bakus",
      "Colin Lockard",
      "David Ciemiewicz",
      "Yangfeng Ji",
      "Sandeep Atluri",
      "Kevin Small",
      "Heba Elfardy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12386"
  },
  {
    "id": "arXiv:2205.12388",
    "title": "DASP: A Framework for Driving the Adoption of Software Security  Practices",
    "abstract": "Implementing software security practices is a critical concern in modern\nsoftware development. Industry practitioners, security tool providers, and\nresearchers have provided standard security guidelines and sophisticated\nsecurity development tools to ensure a secure software development pipeline.\nBut despite these efforts, there continues to be an increase in the number of\nvulnerabilities that can be exploited by malicious hackers. There is thus an\nurgent need to understand why developers still introduce security\nvulnerabilities into their applications and to understand what can be done to\nmotivate them to write more secure code. To understand and address this problem\nfurther, we propose DASP, a framework for diagnosing and driving the adoption\nof software security practices among developers. DASP was conceived by\ncombining behavioral science theories to shape a cross-sectional interview\nstudy with 28 software practitioners. Our interviews lead to a framework that\nconsists of a comprehensive set of 33 drivers grouped into 7 higher-level\ncategories that represent what needs to happen or change so that the adoption\nof software security practices occurs. Using the DASP framework, organizations\ncan design interventions suitable for developers' specific development contexts\nthat will motivate them to write more secure code.",
    "descriptor": "\nComments: 27pages, 12 figures\n",
    "authors": [
      "Enrique Larios-Vargas",
      "Omar Elazhary",
      "Soroush Yousefi",
      "Derek Lowlind",
      "Michael L. W. Vliek",
      "Margaret-Anne Storey"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.12388"
  },
  {
    "id": "arXiv:2205.12390",
    "title": "Toxicity Detection with Generative Prompt-based Inference",
    "abstract": "Due to the subtleness, implicity, and different possible interpretations\nperceived by different people, detecting undesirable content from text is a\nnuanced difficulty. It is a long-known risk that language models (LMs), once\ntrained on corpus containing undesirable content, have the power to manifest\nbiases and toxicity. However, recent studies imply that, as a remedy, LMs are\nalso capable of identifying toxic content without additional fine-tuning.\nPrompt-methods have been shown to effectively harvest this surprising\nself-diagnosing capability. However, existing prompt-based methods usually\nspecify an instruction to a language model in a discriminative way. In this\nwork, we explore the generative variant of zero-shot prompt-based toxicity\ndetection with comprehensive trials on prompt engineering. We evaluate on three\ndatasets with toxicity labels annotated on social media posts. Our analysis\nhighlights the strengths of our generative classification approach both\nquantitatively and qualitatively. Interesting aspects of self-diagnosis and its\nethical implications are discussed.",
    "descriptor": "",
    "authors": [
      "Yau-Shian Wang",
      "Yingshan Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12390"
  },
  {
    "id": "arXiv:2205.12391",
    "title": "Toward Understanding Bias Correlations for Mitigation in NLP",
    "abstract": "Natural Language Processing (NLP) models have been found discriminative\nagainst groups of different social identities such as gender and race. With the\nnegative consequences of these undesired biases, researchers have responded\nwith unprecedented effort and proposed promising approaches for bias\nmitigation. In spite of considerable practical importance, current algorithmic\nfairness literature lacks an in-depth understanding of the relations between\ndifferent forms of biases. Social bias is complex by nature. Numerous studies\nin social psychology identify the \"generalized prejudice\", i.e., generalized\ndevaluing sentiments across different groups. For example, people who devalue\nethnic minorities are also likely to devalue women and gays. Therefore, this\nwork aims to provide a first systematic study toward understanding bias\ncorrelations in mitigation. In particular, we examine bias mitigation in two\ncommon NLP tasks -- toxicity detection and word embeddings -- on three social\nidentities, i.e., race, gender, and religion. Our findings suggest that biases\nare correlated and present scenarios in which independent debiasing approaches\ndominant in current literature may be insufficient. We further investigate\nwhether jointly mitigating correlated biases is more desired than independent\nand individual debiasing. Lastly, we shed light on the inherent issue of\ndebiasing-accuracy trade-off in bias mitigation. This study serves to motivate\nfuture research on joint bias mitigation that accounts for correlated biases.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Lu Cheng",
      "Suyu Ge",
      "Huan Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12391"
  },
  {
    "id": "arXiv:2205.12392",
    "title": "Emergent Communication through Metropolis-Hastings Naming Game with Deep  Generative Models",
    "abstract": "Emergent communication, also known as symbol emergence, seeks to investigate\ncomputational models that can better explain human language evolution and the\ncreation of symbol systems. This study aims to provide a new model for emergent\ncommunication, which is based on a probabilistic generative model. We define\nthe Metropolis-Hastings (MH) naming game by generalizing a model proposed by\nHagiwara et al. \\cite{hagiwara2019symbol}. The MH naming game is a sort of MH\nalgorithm for an integrative probabilistic generative model that combines two\nagents playing the naming game. From this viewpoint, symbol emergence is\nregarded as decentralized Bayesian inference, and semiotic communication is\nregarded as inter-personal cross-modal inference. We also offer Inter-GMM+VAE,\na deep generative model for simulating emergent communication, in which two\nagents create internal representations and categories and share signs (i.e.,\nnames of objects) from raw visual images observed from different viewpoints.\nThe model has been validated on MNIST and Fruits 360 datasets. Experiment\nfindings show that categories are formed from real images observed by agents,\nand signs are correctly shared across agents by successfully utilizing both of\nthe agents' views via the MH naming game. Furthermore, it has been verified\nthat the visual images were recalled from the signs uttered by the agents.\nNotably, emergent communication without supervision and reward feedback\nimproved the performance of unsupervised representation learning.",
    "descriptor": "\nComments: 17 pages, 12 figures\n",
    "authors": [
      "Tadahiro Taniguchi",
      "Yuto Yoshida",
      "Akira Taniguchi",
      "Yoshinobu Hagiwara"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12392"
  },
  {
    "id": "arXiv:2205.12393",
    "title": "Continual-T0: Progressively Instructing 50+ Tasks to Language Models  Without Forgetting",
    "abstract": "Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.",
    "descriptor": "",
    "authors": [
      "Thomas Scialom",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12393"
  },
  {
    "id": "arXiv:2205.12394",
    "title": "MaskEval: Weighted MLM-Based Evaluation for Text Summarization and  Simplification",
    "abstract": "In text summarization and simplification, system outputs must be evaluated\nalong multiple dimensions such as relevance, factual consistency, fluency, and\ngrammaticality, and a wide range of possible outputs could be of high quality.\nThese properties make the development of an adaptable, reference-less\nevaluation metric both necessary and challenging. We introduce MaskEval, a\nreference-less metric for text summarization and simplification that operates\nby performing masked language modeling (MLM) on the concatenation of the\ncandidate and the source texts. It features an attention-like weighting\nmechanism to modulate the relative importance of each MLM step, which crucially\nallows MaskEval to be adapted to evaluate different quality dimensions. We\ndemonstrate its effectiveness on English summarization and on multilingual text\nsimplification in terms of correlations with human judgments.",
    "descriptor": "",
    "authors": [
      "Yu Lu Liu",
      "Rachel Bawden",
      "Thomas Scaliom",
      "Beno\u00eet Sagot",
      "Jackie Chi Kit Cheung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12394"
  },
  {
    "id": "arXiv:2205.12396",
    "title": "Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural  Networks",
    "abstract": "Learning effective recipe representations is essential in food studies.\nUnlike what has been developed for image-based recipe retrieval or learning\nstructural text embeddings, the combined effect of multi-modal information\n(i.e., recipe images, text, and relation data) receives less attention. In this\npaper, we formalize the problem of multi-modal recipe representation learning\nto integrate the visual, textual, and relational information into recipe\nembeddings. In particular, we first present Large-RG, a new recipe graph data\nwith over half a million nodes, making it the largest recipe graph to date. We\nthen propose Recipe2Vec, a novel graph neural network based recipe embedding\nmodel to capture multi-modal information. Additionally, we introduce an\nadversarial attack strategy to ensure stable learning and improve performance.\nFinally, we design a joint objective function of node classification and\nadversarial learning to optimize the model. Extensive experiments demonstrate\nthat Recipe2Vec outperforms state-of-the-art baselines on two classic food\nstudy tasks, i.e., cuisine category classification and region prediction.\nDataset and codes are available at https://github.com/meettyj/Recipe2Vec.",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Yijun Tian",
      "Chuxu Zhang",
      "Zhichun Guo",
      "Yihong Ma",
      "Ronald Metoyer",
      "Nitesh V. Chawla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12396"
  },
  {
    "id": "arXiv:2205.12397",
    "title": "Predicting Post-Route Quality of Results Estimates for HLS Designs using  Machine Learning",
    "abstract": "Machine learning (ML) has been widely used to improve the predictability of\nEDA tools. The use of CAD tools that express designs at higher levels of\nabstraction makes machine learning even more important to highlight the\nperformance of various design steps. Behavioral descriptions used during the\nhigh-level synthesis (HLS) are completely technology independent making it hard\nfor designers to interpret how changes in the synthesis options affect the\nresultant circuit. FPGA design flows are completely embracing HLS based\nmethodologies so that software engineers with almost no hardware design skills\ncan easily use their tools. HLS tools allow design space exploration by\nmodifying synthesis options, however, they lack accuracy in the Quality of\nResults (QoR) reported right after HLS. This lack of correctness results in\nsub-optimal designs with problems in timing closure. This paper presents a\nrobust ML based design flow that can accurately predict post-route QoR for a\ngiven behavioral description without the need to synthesize the design. The\nmodel is an important design exploration tool where a designer can quickly view\nthe impact on overall design quality when local and global optimization\ndirectives are changed. The proposed methodology presents two strong\nadvantages: (i) Accurate prediction of the design quality (QoR), and (ii)\ncomplete elimination of the need to execute high-level synthesis for each\ndesign option. We predict three post route parameters, (i). Area, (ii). Latency\nand (iii). Clock Period of a design just by analyzing the high level behavioral\ncode and some intermediate representation codes. We have integrated the\nmethodology with Xilinx HLS tools and have demonstrated accurate estimation on\na variety of FPGA families. Our estimated results are within 10\\% of actual\ncomputed values",
    "descriptor": "\nComments: This paper was accepted for publication in ISQED 2022\n",
    "authors": [
      "Pingakshya Goswami",
      "Dinesh Bhatia"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.12397"
  },
  {
    "id": "arXiv:2205.12399",
    "title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT",
    "abstract": "We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. The Sparse Mixer slightly outperforms (<1%) BERT on GLUE\nand SuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms (<0.2%) BERT on SuperGLUE, but trains and runs\nnearly twice as fast: 89% faster training and 98% faster inference. We justify\nthe design of these two models by carefully ablating through various mixing\nmechanisms, MoE configurations and model hyperparameters. The Sparse Mixer\novercomes many of the latency and stability concerns of MoE models and offers\nthe prospect of serving sparse student models, without resorting to distilling\nthem to dense variants.",
    "descriptor": "",
    "authors": [
      "James Lee-Thorp",
      "Joshua Ainslie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12399"
  },
  {
    "id": "arXiv:2205.12401",
    "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement  Learning",
    "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often\nrequires meticulous reward engineering. Preference-based RL methods are able to\nlearn a more flexible reward model based on human preferences by actively\nincorporating human feedback, i.e. teacher's preferences between two clips of\nbehaviors. However, poor feedback-efficiency still remains a problem in current\npreference-based RL algorithms, as tailored human feedback is very expensive.\nTo handle this issue, previous methods have mainly focused on improving query\nselection and policy initialization. At the same time, recent exploration\nmethods have proven to be a recipe for improving sample-efficiency in RL. We\npresent an exploration method specifically for preference-based RL algorithms.\nOur main idea is to design an intrinsic reward by measuring the novelty based\non learned reward. Specifically, we utilize disagreement across ensemble of\nlearned reward models. Our intuition is that disagreement in learned reward\nmodel reflects uncertainty in tailored human feedback and could be useful for\nexploration. Our experiments show that exploration bonus from uncertainty in\nlearned reward improves both feedback- and sample-efficiency of\npreference-based RL algorithms on complex robot manipulation tasks from\nMetaWorld benchmarks, compared with other existing exploration methods that\nmeasure the novelty of state visitation.",
    "descriptor": "\nComments: ICLR 2022. Last two authors advised equally\n",
    "authors": [
      "Xinran Liang",
      "Katherine Shu",
      "Kimin Lee",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12401"
  },
  {
    "id": "arXiv:2205.12402",
    "title": "Loop Closure Prioritization for Efficient and Scalable Multi-Robot SLAM",
    "abstract": "Multi-robot SLAM systems in GPS-denied environments require loop closures to\nmaintain a drift-free centralized map. With an increasing number of robots and\nsize of the environment, checking and computing the transformation for all the\nloop closure candidates becomes computationally infeasible. In this work, we\ndescribe a loop closure module that is able to prioritize which loop closures\nto compute based on the underlying pose graph, the proximity to known beacons,\nand the characteristics of the point clouds. We validate this system in the\ncontext of the DARPA Subterranean Challenge and on numerous challenging\nunderground datasets and demonstrate the ability of this system to generate and\nmaintain a map with low error. We find that our proposed techniques are able to\nselect effective loop closures which results in 51% mean reduction in median\nerror when compared to an odometric solution and 75% mean reduction in median\nerror when compared to a baseline version of this system with no\nprioritization. We also find our proposed system is able to find a lower error\nin the mission time of one hour when compared to a system that processes every\npossible loop closure in four and a half hours.",
    "descriptor": "",
    "authors": [
      "Christopher E. Denniston",
      "Yun Chang",
      "Andrzej Reinke",
      "Kamak Ebadi",
      "Gaurav S. Sukhatme",
      "Luca Carlone",
      "Benjamin Morrell",
      "Ali-akbar Agha-mohammadi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12402"
  },
  {
    "id": "arXiv:2205.12403",
    "title": "Jointly Optimizing Color Rendition and In-Camera Backgrounds in an RGB  Virtual Production Stage",
    "abstract": "While the LED panels used in virtual production systems can display vibrant\nimagery with a wide color gamut, they produce problematic color shifts when\nused as lighting due to their peaky spectral output from narrow-band red,\ngreen, and blue LEDs. In this work, we present an improved color calibration\nprocess for virtual production stages which ameliorates this color rendition\nproblem while also passing through accurate in-camera background colors. We do\nthis by optimizing linear color correction transformations for 1) the LED panel\npixels visible in the field of view of the camera, 2) the pixels outside the\nfield of view of the camera illuminating the subjects, and, as a post-process,\n3) the pixel values recorded by the camera. The result is that footage shot in\nan RGB LED panel virtual production stage can exhibit more accurate skin tones\nand costume colors while still reproducing the desired colors of the in-camera\nbackground.",
    "descriptor": "",
    "authors": [
      "Chloe LeGendre",
      "Lukas Lepicovsky",
      "Paul Debevec"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.12403"
  },
  {
    "id": "arXiv:2205.12404",
    "title": "FLUTE: Figurative Language Understanding and Textual Explanations",
    "abstract": "In spite of the prevalence of figurative language, transformer-based models\nstruggle to demonstrate an understanding of it. Meanwhile, even classical\nnatural language inference (NLI) tasks have been plagued by spurious\ncorrelations and annotation artifacts. Datasets like eSNLI have been released,\nallowing to probe whether language models are right for the right reasons. Yet\nno such data exists for figurative language, making it harder to asses genuine\nunderstanding of such expressions. In light of the above, we release FLUTE, a\ndataset of 8,000 figurative NLI instances with explanations, spanning three\ncategories: Sarcasm, Simile, and Metaphor. We collect the data through the\nHuman-AI collaboration framework based on GPT-3, crowdworkers, and expert\nannotation. We show how utilizing GPT-3 in conjunction with human experts can\naid in scaling up the creation of datasets even for such complex linguistic\nphenomena as figurative language. Baseline performance of the T5 model shows\nour dataset is a challenging testbed for figurative language understanding.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Tuhin Chakrabarty",
      "Arkadiy Saakyan",
      "Debanjan Ghosh",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12404"
  },
  {
    "id": "arXiv:2205.12406",
    "title": "Multi-Head Online Learning for Delayed Feedback Modeling",
    "abstract": "In online advertising, it is highly important to predict the probability and\nthe value of a conversion (e.g., a purchase). It not only impacts user\nexperience by showing relevant ads, but also affects ROI of advertisers and\nrevenue of marketplaces. Unlike clicks, which often occur within minutes after\nimpressions, conversions are expected to happen over a long period of time\n(e.g., 30 days for online shopping). It creates a challenge, as the true labels\nare only available after the long delays. Either inaccurate labels (partial\nconversions) are used, or models are trained on stale data (e.g., from 30 days\nago). The problem is more eminent in online learning, which focuses on the live\nperformance on the latest data. In this paper, a novel solution is presented to\naddress this challenge using multi-head modeling. Unlike traditional methods,\nit directly quantizes conversions into multiple windows, such as day 1, day 2,\nday 3-7, and day 8-30. A sub-model is trained specifically on conversions\nwithin each window. Label freshness is maximally preserved in early models\n(e.g., day 1 and day 2), while late conversions are accurately utilized in\nmodels with longer delays (e.g., day 8-30). It is shown to greatly exceed the\nperformance of known methods in online learning experiments for both conversion\nrate (CVR) and value per click (VPC) predictions. Lastly, as a general method\nfor delayed feedback modeling, it can be combined with any advanced ML\ntechniques to further improve the performance.",
    "descriptor": "",
    "authors": [
      "Hui Gao",
      "Yihan Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12406"
  },
  {
    "id": "arXiv:2205.12407",
    "title": "Convolutional Neural Processes for Inpainting Satellite Images",
    "abstract": "The widespread availability of satellite images has allowed researchers to\nmodel complex systems such as disease dynamics. However, many satellite images\nhave missing values due to measurement defects, which render them unusable\nwithout data imputation. For example, the scanline corrector for the LANDSAT 7\nsatellite broke down in 2003, resulting in a loss of around 20\\% of its data.\nInpainting involves predicting what is missing based on the known pixels and is\nan old problem in image processing, classically based on PDEs or interpolation\nmethods, but recent deep learning approaches have shown promise. However, many\nof these methods do not explicitly take into account the inherent\nspatiotemporal structure of satellite images. In this work, we cast satellite\nimage inpainting as a natural meta-learning problem, and propose using\nconvolutional neural processes (ConvNPs) where we frame each satellite image as\nits own task or 2D regression problem. We show ConvNPs can outperform classical\nmethods and state-of-the-art deep learning inpainting models on a scanline\ninpainting problem for LANDSAT 7 satellite images, assessed on a variety of in\nand out-of-distribution images.",
    "descriptor": "",
    "authors": [
      "Alexander Pondaven",
      "M\u00e4rt Bakler",
      "Donghu Guo",
      "Hamzah Hashim",
      "Martin Ignatov",
      "Harrison Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12407"
  },
  {
    "id": "arXiv:2205.12408",
    "title": "Using user's local context to support local news",
    "abstract": "American local newspapers have been experiencing a large loss of reader\nretention and business within the past 15 years due to the proliferation of\nonline news sources. Local media companies are starting to shift from an\nadvertising-supported business model to one based on subscriptions to mitigate\nthis problem. With this subscription model, there is a need to increase user\nengagement and personalization, and recommender systems are one way for these\nnews companies to accomplish this goal. However, using standard modeling\napproaches that focus on users' global preferences is not appropriate in this\ncontext because the local preferences of users exhibit some specific\ncharacteristics which do not necessarily match their long-term or global\npreferences in the news. Our research explores a localized session-based\nrecommendation approach, using recommendations based on local news articles and\narticles pertaining to the different local news categories. Experiments\nperformed on a news dataset from a local newspaper show that these local\nmodels, particularly certain categories of items, do indeed provide more\naccuracy and effectiveness for personalization which, in turn, may lead to more\nuser engagement with local news content.",
    "descriptor": "",
    "authors": [
      "Payam Pourashraf",
      "Bashed Mobasher"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12408"
  },
  {
    "id": "arXiv:2205.12410",
    "title": "AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large  Language Models",
    "abstract": "Fine-tuning large-scale pre-trained language models to downstream tasks\nrequire updating hundreds of millions of parameters. This not only increases\nthe serving cost to store a large copy of the model weights for every task, but\nalso exhibits instability during few-shot task adaptation. Parameter-efficient\ntechniques have been developed that tune small trainable components (e.g.,\nadapters) injected in the large model while keeping most of the model weights\nfrozen. The prevalent mechanism to increase adapter capacity is to increase the\nbottleneck dimension which increases the adapter parameters. In this work, we\nintroduce a new mechanism to improve adapter capacity without increasing\nparameters or computational cost by two key techniques. (i) We introduce\nmultiple shared adapter components in each layer of the Transformer\narchitecture. We leverage sparse learning via random routing to update the\nadapter parameters (encoder is kept frozen) resulting in the same amount of\ncomputational cost (FLOPs) as that of training a single adapter. (ii) We\npropose a simple merging mechanism to average the weights of multiple adapter\ncomponents to collapse to a single adapter in each Transformer layer, thereby,\nkeeping the overall parameters also the same but with significant performance\nimprovement. We demonstrate these techniques to work well across multiple task\nsettings including fully supervised and few-shot Natural Language Understanding\ntasks. By only tuning 0.23% of a pre-trained language model's parameters, our\nmodel outperforms the full model fine-tuning performance and several competing\nmethods.",
    "descriptor": "",
    "authors": [
      "Yaqing Wang",
      "Subhabrata Mukherjee",
      "Xiaodong Liu",
      "Jing Gao",
      "Ahmed Hassan Awadallah",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12410"
  },
  {
    "id": "arXiv:2205.12411",
    "title": "Linear Connectivity Reveals Generalization Strategies",
    "abstract": "It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.",
    "descriptor": "",
    "authors": [
      "Jeevesh Juneja",
      "Rachit Bansal",
      "Kyunghyun Cho",
      "Jo\u00e3o Sedoc",
      "Naomi Saphra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12411"
  },
  {
    "id": "arXiv:2205.12412",
    "title": "Differentially Private AUC Computation in Vertical Federated Learning",
    "abstract": "Federated learning has gained great attention recently as a privacy-enhancing\ntool to jointly train a machine learning model by multiple parties. As a\nsub-category, vertical federated learning (vFL) focuses on the scenario where\nfeatures and labels are split into different parties. The prior work on vFL has\nmostly studied how to protect label privacy during model training. However,\nmodel evaluation in vFL might also lead to potential leakage of private label\ninformation. One mitigation strategy is to apply label differential privacy\n(DP) but it gives bad estimations of the true (non-private) metrics. In this\nwork, we propose two evaluation algorithms that can more accurately compute the\nwidely used AUC (area under curve) metric when using label DP in vFL. Through\nextensive experiments, we show our algorithms can achieve more accurate AUCs\ncompared to the baselines.",
    "descriptor": "",
    "authors": [
      "Jiankai Sun",
      "Xin Yang",
      "Yuanshun Yao",
      "Junyuan Xie",
      "Di Wu",
      "Chong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12412"
  },
  {
    "id": "arXiv:2205.12416",
    "title": "Counterfactual Data Augmentation improves Factuality of Abstractive  Summarization",
    "abstract": "Abstractive summarization systems based on pretrained language models often\ngenerate coherent but factually inconsistent sentences. In this paper, we\npresent a counterfactual data augmentation approach where we augment data with\nperturbed summaries that increase the training data diversity. Specifically, we\npresent three augmentation approaches based on replacing (i) entities from\nother and the same category and (ii) nouns with their corresponding WordNet\nhypernyms. We show that augmenting the training data with our approach improves\nthe factual correctness of summaries without significantly affecting the ROUGE\nscore. We show that in two commonly used summarization datasets (CNN/Dailymail\nand XSum), we improve the factual correctness by about 2.5 points on average",
    "descriptor": "",
    "authors": [
      "Dheeraj Rajagopal",
      "Siamak Shakeri",
      "Cicero Nogueira dos Santos",
      "Eduard Hovy",
      "Chung-Ching Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12416"
  },
  {
    "id": "arXiv:2205.12418",
    "title": "Tiered Reinforcement Learning: Pessimism in the Face of Uncertainty and  Constant Regret",
    "abstract": "We propose a new learning framework that captures the tiered structure of\nmany real-world user-interaction applications, where the users can be divided\ninto two groups based on their different tolerance on exploration risks and\nshould be treated separately. In this setting, we simultaneously maintain two\npolicies $\\pi^{\\text{O}}$ and $\\pi^{\\text{E}}$: $\\pi^{\\text{O}}$ (\"O\" for\n\"online\") interacts with more risk-tolerant users from the first tier and\nminimizes regret by balancing exploration and exploitation as usual, while\n$\\pi^{\\text{E}}$ (\"E\" for \"exploit\") exclusively focuses on exploitation for\nrisk-averse users from the second tier utilizing the data collected so far. An\nimportant question is whether such a separation yields advantages over the\nstandard online setting (i.e., $\\pi^{\\text{E}}=\\pi^{\\text{O}}$) for the\nrisk-averse users. We individually consider the gap-independent\nvs.~gap-dependent settings. For the former, we prove that the separation is\nindeed not beneficial from a minimax perspective. For the latter, we show that\nif choosing Pessimistic Value Iteration as the exploitation algorithm to\nproduce $\\pi^{\\text{E}}$, we can achieve a constant regret for risk-averse\nusers independent of the number of episodes $K$, which is in sharp contrast to\nthe $\\Omega(\\log K)$ regret for any online RL algorithms in the same setting,\nwhile the regret of $\\pi^{\\text{O}}$ (almost) maintains its online regret\noptimality and does not need to compromise for the success of $\\pi^{\\text{E}}$.",
    "descriptor": "\nComments: 38 pages\n",
    "authors": [
      "Jiawei Huang",
      "Li Zhao",
      "Tao Qin",
      "Wei Chen",
      "Nan Jiang",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12418"
  },
  {
    "id": "arXiv:2205.12420",
    "title": "Learning Action Conditions from Instructional Manuals for Instruction  Understanding",
    "abstract": "The ability to infer pre- and postconditions of an action is vital for\ncomprehending complex instructions, and is essential for applications such as\nautonomous instruction-guided agents and assistive AI that supports humans to\nperform physical tasks. In this work, we propose a task dubbed action condition\ninference, and collecting a high-quality, human annotated dataset of\npreconditions and postconditions of actions in instructional manuals. We\npropose a weakly supervised approach to automatically construct large-scale\ntraining instances from online instructional manuals, and curate a densely\nhuman-annotated and validated dataset to study how well the current NLP models\ncan infer action-condition dependencies in the instruction texts. We design two\ntypes of models differ by whether contextualized and global information is\nleveraged, as well as various combinations of heuristics to construct the weak\nsupervisions. Our experimental results show a >20% F1-score improvement with\nconsidering the entire instruction contexts and a >6% F1-score benefit with the\nproposed heuristics.",
    "descriptor": "",
    "authors": [
      "Te-Lin Wu",
      "Caiqi Zhang",
      "Qingyuan Hu",
      "Alex Spangher",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12420"
  },
  {
    "id": "arXiv:2205.12421",
    "title": "Substring Complexities on Run-length Compressed Strings",
    "abstract": "Let $S_{T}(k)$ denote the set of distinct substrings of length $k$ in a\nstring $T$, then the $k$-th substring complexity is defined by its cardinality\n$|S_{T}(k)|$. Recently, $\\delta = \\max \\{ |S_{T}(k)| / k : k \\ge 1 \\}$ is shown\nto be a good compressibility measure of highly-repetitive strings. In this\npaper, given $T$ of length $n$ in the run-length compressed form of size $r$,\nwe show that $\\delta$ can be computed in $\\mathit{C}_{\\mathsf{sort}}(r, n)$\ntime and $O(r)$ space, where $\\mathit{C}_{\\mathsf{sort}}(r, n) = O(\\min (r\n\\lg\\lg r, r \\lg_{r} n))$ is the time complexity for sorting $r$ $O(\\lg n)$-bit\nintegers in $O(r)$ space in the Word-RAM model with word size $\\Omega(\\lg n)$.",
    "descriptor": "",
    "authors": [
      "Akiyoshi Kawamoto",
      "Tomohiro I"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12421"
  },
  {
    "id": "arXiv:2205.12422",
    "title": "Active Programming by Example with a Natural Language Prior",
    "abstract": "We introduce APEL, a new framework that enables non-programmers to indirectly\nannotate natural language utterances with executable meaning representations,\nsuch as SQL programs. Based on a natural language utterance, we first run a\nseed semantic parser to generate a prior over a list of candidate programs. To\nobtain information about which candidate is correct, we synthesize an input on\nwhich the more likely programs tend to produce different outputs, and ask an\nannotator which output is appropriate for the utterance. Hence, the annotator\ndoes not have to directly inspect the programs. To further reduce effort\nrequired from annotators, we aim to synthesize simple input databases that\nnonetheless have high information gain. With human annotators and Bayesian\ninference to handle annotation errors, we outperform Codex's top-1 performance\n(59%) and achieve the same accuracy as the original expert annotators (75%), by\nsoliciting answers for each utterance on only 2 databases with an average of 9\nrecords each. In contrast, it would be impractical to solicit outputs on the\noriginal 30K-record databases provided by SPIDER",
    "descriptor": "",
    "authors": [
      "Ruiqi Zhong",
      "Charlie Snell",
      "Dan Klein",
      "Jason Eisner"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.12422"
  },
  {
    "id": "arXiv:2205.12423",
    "title": "Deletion and Insertion Tests in Regression Models",
    "abstract": "A basic task in explainable AI (XAI) is to identify the most important\nfeatures behind a prediction made by a black box function $f$. The insertion\nand deletion tests of \\cite{petsiuk2018rise} are used to judge the quality of\nalgorithms that rank pixels from most to least important for a classification.\nMotivated by regression problems we establish a formula for their area under\nthe curve (AUC) criteria in terms of certain main effects and interactions in\nan anchored decomposition of $f$. We find an expression for the expected value\nof the AUC under a random ordering of inputs to $f$ and propose an alternative\narea above a straight line for the regression setting. We use this criterion to\ncompare feature importances computed by integrated gradients (IG) to those\ncomputed by Kernel SHAP (KS). Exact computation of KS grows exponentially with\ndimension, while that of IG grows linearly with dimension. In two data sets\nincluding binary variables we find that KS is superior to IG in insertion and\ndeletion tests, but only by a very small amount. Our comparison problems\ninclude some binary inputs that pose a challenge to IG because it must use\nvalues between the possible variable levels.\nWe show that IG will match KS when $f$ is an additive function plus a\nmultilinear function of the variables. This includes a multilinear\ninterpolation over the binary variables that would cause IG to have exponential\ncost in a naive implementation.",
    "descriptor": "",
    "authors": [
      "Naofumi Hama",
      "Masayoshi Mase",
      "Art B. Owen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12423"
  },
  {
    "id": "arXiv:2205.12424",
    "title": "VulBERTa: Simplified Source Code Pre-Training for Vulnerability  Detection",
    "abstract": "This paper presents VulBERTa, a deep learning approach to detect security\nvulnerabilities in source code. Our approach pre-trains a RoBERTa model with a\ncustom tokenisation pipeline on real-world code from open-source C/C++\nprojects. The model learns a deep knowledge representation of the code syntax\nand semantics, which we leverage to train vulnerability detection classifiers.\nWe evaluate our approach on binary and multi-class vulnerability detection\ntasks across several datasets (Vuldeepecker, Draper, REVEAL and muVuldeepecker)\nand benchmarks (CodeXGLUE and D2A). The evaluation results show that VulBERTa\nachieves state-of-the-art performance and outperforms existing approaches\nacross different datasets, despite its conceptual simplicity, and limited cost\nin terms of size of training data and number of model parameters.",
    "descriptor": "\nComments: Accepted as a conference paper at IJCNN 2022\n",
    "authors": [
      "Hazim Hanif",
      "Sergio Maffeis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12424"
  },
  {
    "id": "arXiv:2205.12425",
    "title": "Synthesizing CRDTs from Sequential Data Types with Verified Lifting",
    "abstract": "Conflict-free replicated data types (CRDTs) are a powerful tool for designing\nscalable, coordination-free distributed systems. However, constructing correct\nCRDTs is difficult, posing a challenge for even seasoned developers. As a\nresult, CRDT development is still the domain of academics, with new designs\noften awaiting peer review and a manual proof of correctness. In this paper, we\npresent a program synthesis-based system that can automatically synthesize\nverified CRDT designs from sequential data type implementations. Key to this\nprocess is a new formal definition of CRDT correctness that combines a\nreference sequential type with a lightweight ordering constraint that resolves\nconflicts between non-commutative operations. Our process follows the tradition\nof work in verified lifting, including an encoding of correctness into SMT\nlogic using synthesized inductive invariants and hand-crafted grammars for the\nCRDT state and runtime. Our algorithm is able to automatically synthesize CRDTs\nfor a wide variety of scenarios, from reproducing classic CRDTs to synthesizing\nnovel designs based on specifications in existing literature. Crucially, our\nsynthesized CRDTs are fully, automatically verified, eliminating entire classes\nof common errors and reducing the process of producing a new CRDT from a\npainstaking paper proof of correctness to a lightweight specification.",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Shadaj Laddad",
      "Conor Power",
      "Mae Milano",
      "Alvin Cheung",
      "Joseph M. Hellerstein"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12425"
  },
  {
    "id": "arXiv:2205.12427",
    "title": "Non-stationary Bandits with Knapsacks",
    "abstract": "In this paper, we study the problem of bandits with knapsacks (BwK) in a\nnon-stationary environment. The BwK problem generalizes the multi-arm bandit\n(MAB) problem to model the resource consumption associated with playing each\narm. At each time, the decision maker/player chooses to play an arm, and s/he\nwill receive a reward and consume certain amount of resource from each of the\nmultiple resource types. The objective is to maximize the cumulative reward\nover a finite horizon subject to some knapsack constraints on the resources.\nExisting works study the BwK problem under either a stochastic or adversarial\nenvironment. Our paper considers a non-stationary environment which\ncontinuously interpolates between these two extremes. We first show that the\ntraditional notion of variation budget is insufficient to characterize the\nnon-stationarity of the BwK problem for a sublinear regret due to the presence\nof the constraints, and then we propose a new notion of global non-stationarity\nmeasure. We employ both non-stationarity measures to derive upper and lower\nbounds for the problem. Our results are based on a primal-dual analysis of the\nunderlying linear programs and highlight the interplay between the constraints\nand the non-stationarity. Finally, we also extend the non-stationarity measure\nto the problem of online convex optimization with constraints and obtain new\nregret bounds accordingly.",
    "descriptor": "",
    "authors": [
      "Shang Liu",
      "Jiashuo Jiang",
      "Xiaocheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12427"
  },
  {
    "id": "arXiv:2205.12428",
    "title": "Towards Understanding Label Regularization for Fine-tuning Pre-trained  Language Models",
    "abstract": "Knowledge Distillation (KD) is a prominent neural model compression technique\nwhich heavily relies on teacher network predictions to guide the training of a\nstudent model. Considering the ever-growing size of pre-trained language models\n(PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is\nevident that in KD, deploying the teacher network during training adds to the\nmemory and computational requirements of training. In the computer vision\nliterature, the necessity of the teacher network is put under scrutiny by\nshowing that KD is a label regularization technique that can be replaced with\nlighter teacher-free variants such as the label-smoothing technique. However,\nto the best of our knowledge, this issue is not investigated in NLP. Therefore,\nthis work concerns studying different label regularization techniques and\nwhether we actually need the teacher labels to fine-tune smaller PLM student\nnetworks on downstream tasks. In this regard, we did a comprehensive set of\nexperiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600\ndistinct trials and ran each configuration five times. This investigation led\nto a surprising observation that KD and other label regularization techniques\ndo not play any meaningful role over regular fine-tuning when the student model\nis pre-trained. We further explore this phenomenon in different settings of NLP\nand computer vision tasks and demonstrate that pre-training itself acts as a\nkind of regularization, and additional label regularization is unnecessary.",
    "descriptor": "",
    "authors": [
      "Ivan Kobyzev",
      "Aref Jafari",
      "Mehdi Rezagholizadeh",
      "Tianda Li",
      "Alan Do-Omri",
      "Peng Lu",
      "Ali Ghodsi",
      "Pascal Poupart"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12428"
  },
  {
    "id": "arXiv:2205.12430",
    "title": "Additive Logistic Mechanism for Privacy-Preserving Self-Supervised  Learning",
    "abstract": "We study the privacy risks that are associated with training a neural\nnetwork's weights with self-supervised learning algorithms. Through empirical\nevidence, we show that the fine-tuning stage, in which the network weights are\nupdated with an informative and often private dataset, is vulnerable to privacy\nattacks. To address the vulnerabilities, we design a post-training\nprivacy-protection algorithm that adds noise to the fine-tuned weights and\npropose a novel differential privacy mechanism that samples noise from the\nlogistic distribution. Compared to the two conventional additive noise\nmechanisms, namely the Laplace and the Gaussian mechanisms, the proposed\nmechanism uses a bell-shaped distribution that resembles the distribution of\nthe Gaussian mechanism, and it satisfies pure $\\epsilon$-differential privacy\nsimilar to the Laplace mechanism. We apply membership inference attacks on both\nunprotected and protected models to quantify the trade-off between the models'\nprivacy and performance. We show that the proposed protection algorithm can\neffectively reduce the attack accuracy to roughly 50\\%-equivalent to random\nguessing-while maintaining a performance loss below 5\\%.",
    "descriptor": "\nComments: 15 pages, 2 figures\n",
    "authors": [
      "Yunhao Yang",
      "Parham Gohari",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12430"
  },
  {
    "id": "arXiv:2205.12443",
    "title": "Generating Natural Language Proofs with Verifier-Guided Search",
    "abstract": "Deductive reasoning (drawing conclusions from assumptions) is a challenging\nproblem in NLP. In this work, we focus on proof generation: given a hypothesis\nand a set of supporting facts in natural language, the model generates a proof\ntree indicating how to deduce the hypothesis from supporting facts. Instead of\ngenerating the entire proof in one shot, prior work has demonstrated the\npromise of stepwise generation but achieved limited success on real-world data.\nExisting stepwise methods struggle to generate proof steps that are both valid\nand relevant. In this paper, we present a novel stepwise method NLProofS\n(Natural Language Proof Search), which learns to generate relevant steps\nconditioning on the hypothesis. At the core of our approach, we train an\nindependent verifier to check the validity of proof steps. Instead of\ngenerating steps greedily, we search for proofs maximizing a global proof score\njudged by the verifier. NLProofS achieves state-of-the-art performance on\nEntailmentBank and RuleTaker. For example, it improves the percentage of\ncorrectly predicted proofs from 20.9% to 33.3% in the distractor setting of\nEntailmentBank. This is the first time stepwise methods have led to better\ngeneration of challenging human-authored proofs.",
    "descriptor": "",
    "authors": [
      "Kaiyu Yang",
      "Jia Deng",
      "Danqi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.12443"
  },
  {
    "id": "arXiv:2205.12446",
    "title": "FLEURS: Few-shot Learning Evaluation of Universal Representations of  Speech",
    "abstract": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal\nRepresentations of Speech benchmark. FLEURS is an n-way parallel speech dataset\nin 102 languages built on top of the machine translation FLoRes-101 benchmark,\nwith approximately 12 hours of speech supervision per language. FLEURS can be\nused for a variety of speech tasks, including Automatic Speech Recognition\n(ASR), Speech Language Identification (Speech LangID), Translation and\nRetrieval. In this paper, we provide baselines for the tasks based on\nmultilingual pre-trained models like mSLAM. The goal of FLEURS is to enable\nspeech technology in more languages and catalyze research in low-resource\nspeech understanding.",
    "descriptor": "",
    "authors": [
      "Alexis Conneau",
      "Min Ma",
      "Simran Khanuja",
      "Yu Zhang",
      "Vera Axelrod",
      "Siddharth Dalmia",
      "Jason Riesa",
      "Clara Rivera",
      "Ankur Bapna"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12446"
  },
  {
    "id": "arXiv:2205.12449",
    "title": "MAVIPER: Learning Decision Tree Policies for Interpretable Multi-Agent  Reinforcement Learning",
    "abstract": "Many recent breakthroughs in multi-agent reinforcement learning (MARL)\nrequire the use of deep neural networks, which are challenging for human\nexperts to interpret and understand. On the other hand, existing work on\ninterpretable RL has shown promise in extracting more interpretable decision\ntree-based policies, but only in the single-agent setting. To fill this gap, we\npropose the first set of interpretable MARL algorithms that extract\ndecision-tree policies from neural networks trained with MARL. The first\nalgorithm, IVIPER, extends VIPER, a recent method for single-agent\ninterpretable RL, to the multi-agent setting. We demonstrate that IVIPER can\nlearn high-quality decision-tree policies for each agent. To better capture\ncoordination between agents, we propose a novel centralized decision-tree\ntraining algorithm, MAVIPER. MAVIPER jointly grows the trees of each agent by\npredicting the behavior of the other agents using their anticipated trees, and\nuses resampling to focus on states that are critical for its interactions with\nother agents. We show that both algorithms generally outperform the baselines\nand that MAVIPER-trained agents achieve better-coordinated performance than\nIVIPER-trained agents on three different multi-agent particle-world\nenvironments.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Stephanie Milani",
      "Zhicheng Zhang",
      "Nicholay Topin",
      "Zheyuan Ryan Shi",
      "Charles Kamhoua",
      "Evangelos E. Papalexakis",
      "Fei Fang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2205.12449"
  },
  {
    "id": "arXiv:2205.12450",
    "title": "Cross-Domain Style Mixing for Face Cartoonization",
    "abstract": "Cartoon domain has recently gained increasing popularity. Previous studies\nhave attempted quality portrait stylization into the cartoon domain; however,\nthis poses a great challenge since they have not properly addressed the\ncritical constraints, such as requiring a large number of training images or\nthe lack of support for abstract cartoon faces. Recently, a layer swapping\nmethod has been used for stylization requiring only a limited number of\ntraining images; however, its use cases are still narrow as it inherits the\nremaining issues. In this paper, we propose a novel method called Cross-domain\nStyle mixing, which combines two latent codes from two different domains. Our\nmethod effectively stylizes faces into multiple cartoon characters at various\nface abstraction levels using only a single generator without even using a\nlarge number of training images.",
    "descriptor": "",
    "authors": [
      "Seungkwon Kim",
      "Chaeheon Gwak",
      "Dohyun Kim",
      "Kwangho Lee",
      "Jihye Back",
      "Namhyuk Ahn",
      "Daesik Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12450"
  },
  {
    "id": "arXiv:2205.12451",
    "title": "Region-aware Knowledge Distillation for Efficient Image-to-Image  Translation",
    "abstract": "Recent progress in image-to-image translation has witnessed the success of\ngenerative adversarial networks (GANs). However, GANs usually contain a huge\nnumber of parameters, which lead to intolerant memory and computation\nconsumption and limit their deployment on edge devices. To address this issue,\nknowledge distillation is proposed to transfer the knowledge from a cumbersome\nteacher model to an efficient student model. However, most previous knowledge\ndistillation methods are designed for image classification and lead to limited\nperformance in image-to-image translation. In this paper, we propose\nRegion-aware Knowledge Distillation ReKo to compress image-to-image translation\nmodels. Firstly, ReKo adaptively finds the crucial regions in the images with\nan attention module. Then, patch-wise contrastive learning is adopted to\nmaximize the mutual information between students and teachers in these crucial\nregions. Experiments with eight comparison methods on nine datasets demonstrate\nthe substantial effectiveness of ReKo on both paired and unpaired\nimage-to-image translation. For instance, our 7.08X compressed and 6.80X\naccelerated CycleGAN student outperforms its teacher by 1.33 and 1.04 FID\nscores on Horse to Zebra and Zebra to Horse, respectively. Codes will be\nreleased on GitHub.",
    "descriptor": "",
    "authors": [
      "Linfeng Zhang",
      "Xin Chen",
      "Runpei Dong",
      "Kaisheng Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12451"
  },
  {
    "id": "arXiv:2205.12452",
    "title": "Sparse*BERT: Sparse Models are Robust",
    "abstract": "Large Language Models have become the core architecture upon which most\nmodern natural language processing (NLP) systems build. These models can\nconsistently deliver impressive accuracy and robustness across tasks and\ndomains, but their high computational overhead can make inference difficult and\nexpensive. To make the usage of these models less costly recent work has\nexplored leveraging structured and unstructured pruning, quantization, and\ndistillation as ways to improve inference speed and decrease size. This paper\nstudies how models pruned using Gradual Unstructured Magnitude Pruning can\ntransfer between domains and tasks. Our experimentation shows that models that\nare pruned during pretraining using general domain masked language models can\ntransfer to novel domains and tasks without extensive hyperparameter\nexploration or specialized approaches. We demonstrate that our general sparse\nmodel Sparse*BERT can become SparseBioBERT simply by pretraining the compressed\narchitecture on unstructured biomedical text. Moreover, we show that\nSparseBioBERT can match the quality of BioBERT with only 10\\% of the\nparameters.",
    "descriptor": "",
    "authors": [
      "Daniel Campos",
      "Alexandre Marques",
      "Tuan Nguyen",
      "Mark Kurtz",
      "ChengXiang Zhai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12452"
  },
  {
    "id": "arXiv:2205.12453",
    "title": "Know Where You're Going: Meta-Learning for Parameter-Efficient  Fine-tuning",
    "abstract": "A recent family of techniques, dubbed as lightweight fine-tuning methods,\nfacilitates parameter-efficient transfer learning by updating only a small set\nof additional parameters while keeping the parameters of the pretrained\nlanguage model frozen. While proven to be an effective method, there are no\nexisting studies on if and how such knowledge of the downstream fine-tuning\napproach should affect the pretraining stage. In this work, we show that taking\nthe ultimate choice of fine-tuning method into consideration boosts the\nperformance of parameter-efficient fine-tuning. By relying on\noptimization-based meta-learning using MAML with certain modifications for our\ndistinct purpose, we prime the pretrained model specifically for\nparameter-efficient fine-tuning, resulting in gains of up to 1.7 points on\ncross-lingual NER fine-tuning. Our ablation settings and analyses further\nreveal that the tweaks we introduce in MAML are crucial for the attained gains.",
    "descriptor": "",
    "authors": [
      "Mozhdeh Gheini",
      "Xuezhe Ma",
      "Jonathan May"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12453"
  },
  {
    "id": "arXiv:2205.12454",
    "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
    "abstract": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph\nTransformer with linear complexity and state-of-the-art results on a diverse\nset of benchmarks. Graph Transformers (GTs) have gained popularity in the field\nof graph representation learning with a variety of recent publications but they\nlack a common foundation about what constitutes a good positional or structural\nencoding, and what differentiates them. In this paper, we summarize the\ndifferent types of encodings with a clearer definition and categorize them as\nbeing $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. Further, GTs\nremain constrained to small graphs with few hundred nodes, and we propose the\nfirst architecture with a complexity linear to the number of nodes and edges\n$O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected\nTransformer. We argue that this decoupling does not negatively affect the\nexpressivity, with our architecture being a universal function approximator for\ngraphs. Our GPS recipe consists of choosing 3 main ingredients: (i)\npositional/structural encoding, (ii) local message-passing mechanism, and (iii)\nglobal attention mechanism. We build and open-source a modular framework\n$\\textit{GraphGPS}$ that supports multiple types of encodings and that provides\nefficiency and scalability both in small and large graphs. We test our\narchitecture on 11 benchmarks and show very competitive results on all of them,\nshow-casing the empirical benefits gained by the modularity and the combination\nof different strategies.",
    "descriptor": "",
    "authors": [
      "Ladislav Ramp\u00e1\u0161ek",
      "Mikhail Galkin",
      "Vijay Prakash Dwivedi",
      "Anh Tuan Luu",
      "Guy Wolf",
      "Dominique Beaini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12454"
  },
  {
    "id": "arXiv:2205.12456",
    "title": "Investigating Information Inconsistency in Multilingual Open-Domain  Question Answering",
    "abstract": "Retrieval based open-domain QA systems use retrieved documents and\nanswer-span selection over retrieved documents to find best-answer candidates.\nWe hypothesize that multilingual Question Answering (QA) systems are prone to\ninformation inconsistency when it comes to documents written in different\nlanguages, because these documents tend to provide a model with varying\ninformation about the same topic. To understand the effects of the biased\navailability of information and cultural influence, we analyze the behavior of\nmultilingual open-domain question answering models with a focus on retrieval\nbias. We analyze if different retriever models present different passages given\nthe same question in different languages on TyDi QA and XOR-TyDi QA, two\nmultilingualQA datasets. We speculate that the content differences in documents\nacross languages might reflect cultural divergences and/or social biases.",
    "descriptor": "",
    "authors": [
      "Shramay Palta",
      "Haozhe An",
      "Yifan Yang",
      "Shuaiyi Huang",
      "Maharshi Gor"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12456"
  },
  {
    "id": "arXiv:2205.12458",
    "title": "A Lightweight NMS-free Framework for Real-time Visual Fault Detection  System of Freight Trains",
    "abstract": "Real-time vision-based system of fault detection (RVBS-FD) for freight trains\nis an essential part of ensuring railway transportation safety. Most existing\nvision-based methods still have high computational costs based on convolutional\nneural networks. The computational cost is mainly reflected in the backbone,\nneck, and post-processing, i.e., non-maximum suppression (NMS). In this paper,\nwe propose a lightweight NMS-free framework to achieve real-time detection and\nhigh accuracy simultaneously. First, we use a lightweight backbone for feature\nextraction and design a fault detection pyramid to process features. This fault\ndetection pyramid includes three novel individual modules using attention\nmechanism, bottleneck, and dilated convolution for feature enhancement and\ncomputation reduction. Instead of using NMS, we calculate different loss\nfunctions, including classification and location costs in the detection head,\nto further reduce computation. Experimental results show that our framework\nachieves over 83 frames per second speed with a smaller model size and higher\naccuracy than the state-of-the-art detectors. Meanwhile, the hardware resource\nrequirements of our method are low during the training and testing process.",
    "descriptor": "\nComments: 11 pages, 5 figures, accepted by IEEE Transactions on Instrumentation and Measurement\n",
    "authors": [
      "Guodong Sun",
      "Yang Zhou",
      "Huilin Pan",
      "Bo Wu",
      "Ye Hu",
      "Yang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12458"
  },
  {
    "id": "arXiv:2205.12459",
    "title": "A CNN with Noise Inclined Module and Denoise Framework for Hyperspectral  Image Classification",
    "abstract": "Deep Neural Networks have been successfully applied in hyperspectral image\nclassification. However, most of prior works adopt general deep architectures\nwhile ignore the intrinsic structure of the hyperspectral image, such as the\nphysical noise generation. This would make these deep models unable to generate\ndiscriminative features and provide impressive classification performance. To\nleverage such intrinsic information, this work develops a novel deep learning\nframework with the noise inclined module and denoise framework for\nhyperspectral image classification. First, we model the spectral signature of\nhyperspectral image with the physical noise model to describe the high\nintraclass variance of each class and great overlapping between different\nclasses in the image. Then, a noise inclined module is developed to capture the\nphysical noise within each object and a denoise framework is then followed to\nremove such noise from the object. Finally, the CNN with noise inclined module\nand the denoise framework is developed to obtain discriminative features and\nprovides good classification performance of hyperspectral image. Experiments\nare conducted over two commonly used real-world datasets and the experimental\nresults show the effectiveness of the proposed method. The implementation of\nthe proposed method and other compared methods could be accessed at\nhttps://github.com/shendu-sw/noise-physical-framework.",
    "descriptor": "",
    "authors": [
      "Zhiqiang Gong",
      "Ping Zhong",
      "Jiahao Qi",
      "Panhe Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12459"
  },
  {
    "id": "arXiv:2205.12461",
    "title": "Augmentation-induced Consistency Regularization for Classification",
    "abstract": "Deep neural networks have become popular in many supervised learning tasks,\nbut they may suffer from overfitting when the training dataset is limited. To\nmitigate this, many researchers use data augmentation, which is a widely used\nand effective method for increasing the variety of datasets. However, the\nrandomness introduced by data augmentation causes inevitable inconsistency\nbetween training and inference, which leads to poor improvement. In this paper,\nwe propose a consistency regularization framework based on data augmentation,\ncalled CR-Aug, which forces the output distributions of different sub models\ngenerated by data augmentation to be consistent with each other. Specifically,\nCR-Aug evaluates the discrepancy between the output distributions of two\naugmented versions of each sample, and it utilizes a stop-gradient operation to\nminimize the consistency loss. We implement CR-Aug to image and audio\nclassification tasks and conduct extensive experiments to verify its\neffectiveness in improving the generalization ability of classifiers. Our\nCR-Aug framework is ready-to-use, it can be easily adapted to many\nstate-of-the-art network architectures. Our empirical results show that CR-Aug\noutperforms baseline methods by a significant margin.",
    "descriptor": "",
    "authors": [
      "Jianhan Wu",
      "Shijing Si",
      "Jianzong Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12461"
  },
  {
    "id": "arXiv:2205.12462",
    "title": "Improving CTC-based ASR Models with Gated Interlayer Collaboration",
    "abstract": "For Automatic Speech Recognition (ASR), the CTC-based methods have become a\ndominant paradigm due to its simple architecture and efficient\nnon-autoregressive inference manner. However, these methods without external\nlanguage models usually lack the capacity of modeling the conditional\ndependencies and the textual interaction. In this work, we present a Gated\nInterlayer Collaboration (GIC) mechanism which introduces the contextual\ninformation into the models and relaxes the conditional independence assumption\nof the CTC-based models. Specifically, we train the model with intermediate CTC\nlosses calculated by the interlayer outputs of the model, in which the\nprobability distributions of the intermediate layers naturally serve as soft\nlabel sequences. The GIC block consists of an embedding layer to obtain the\ntextual embedding of the soft label at each position, and a gate unit to fuse\nthe textual embedding and the acoustic features. Experiments on AISHELL-1 and\nAIDATATANG benchmarks show that the proposed method outperforms the recently\npublished CTC-based ASR models. Specifically, our method achieves CER of\n4.0%/4.4% on AISHELL-1 dev/test sets and CER of 3.8%/4.4% on AIDATATANG\ndev/test sets using CTC greedy search decoding without external language\nmodels.",
    "descriptor": "\nComments: Submitted to INTERSPEECH2022\n",
    "authors": [
      "Yuting Yang",
      "Yuke Li",
      "Binbin Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12462"
  },
  {
    "id": "arXiv:2205.12464",
    "title": "sat2pc: Estimating Point Cloud of Building Roofs from 2D Satellite  Images",
    "abstract": "Three-dimensional (3D) urban models have gained interest because of their\napplications in many use-cases such as urban planning and virtual reality.\nHowever, generating these 3D representations requires LiDAR data, which are not\nalways readily available. Thus, the applicability of automated 3D model\ngeneration algorithms is limited to a few locations. In this paper, we propose\nsat2pc, a deep learning architecture that predicts the point cloud of a\nbuilding roof from a single 2D satellite image. Our architecture combines\nChamfer distance and EMD loss, resulting in better 2D to 3D performance. We\nextensively evaluate our model and perform ablation studies on a building roof\ndataset. Our results show that sat2pc was able to outperform existing baselines\nby at least 18.6%. Further, we show that the predicted point cloud captures\nmore detail and geometric characteristics than other baselines.",
    "descriptor": "",
    "authors": [
      "Yoones Rezaei",
      "Stephen Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12464"
  },
  {
    "id": "arXiv:2205.12465",
    "title": "FBNETGEN: Task-aware GNN-based fMRI Analysis via Functional Brain  Network Generation",
    "abstract": "Functional magnetic resonance imaging (fMRI) is one of the most common\nimaging modalities to investigate brain functions. Recent studies in\nneuroscience stress the great potential of functional brain networks\nconstructed from fMRI data for clinical predictions. Traditional functional\nbrain networks, however, are noisy and unaware of downstream prediction tasks,\nwhile also incompatible with the deep graph neural network (GNN) models. In\norder to fully unleash the power of GNNs in network-based fMRI analysis, we\ndevelop FBNETGEN, a task-aware and interpretable fMRI analysis framework via\ndeep brain network generation. In particular, we formulate (1) prominent region\nof interest (ROI) features extraction, (2) brain networks generation, and (3)\nclinical predictions with GNNs, in an end-to-end trainable model under the\nguidance of particular prediction tasks. Along with the process, the key novel\ncomponent is the graph generator which learns to transform raw time-series\nfeatures into task-oriented brain networks. Our learnable graphs also provide\nunique interpretations by highlighting prediction-related brain regions.\nComprehensive experiments on two datasets, i.e., the recently released and\ncurrently largest publicly available fMRI dataset Adolescent Brain Cognitive\nDevelopment (ABCD), and the widely-used fMRI dataset PNC, prove the superior\neffectiveness and interpretability of FBNETGEN. The implementation is available\nat https://github.com/Wayfear/FBNETGEN.}",
    "descriptor": "\nComments: This paper has been accepted for presentation in MIDL 2022\n",
    "authors": [
      "Xuan Kan",
      "Hejie Cui",
      "Joshua Lukemire",
      "Ying Guo",
      "Carl Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2205.12465"
  },
  {
    "id": "arXiv:2205.12466",
    "title": "Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning",
    "abstract": "Learning harmful shortcuts such as spurious correlations and biases prevents\ndeep neural networks from learning the meaningful and useful representations,\nthus jeopardizing the generalizability and interpretability of the learned\nrepresentation. The situation becomes even more serious in medical imaging,\nwhere the clinical data (e.g., MR images with pathology) are limited and scarce\nwhile the reliability, generalizability and transparency of the learned model\nare highly required. To address this problem, we propose to infuse human\nexperts' intelligence and domain knowledge into the training of deep neural\nnetworks. The core idea is that we infuse the visual attention information from\nexpert radiologists to proactively guide the deep model to focus on regions\nwith potential pathology and avoid being trapped in learning harmful shortcuts.\nTo do so, we propose a novel eye-gaze-guided vision transformer (EG-ViT) for\ndiagnosis with limited medical image data. We mask the input image patches that\nare out of the radiologists' interest and add an additional residual connection\nin the last encoder layer of EG-ViT to maintain the correlations of all\npatches. The experiments on two public datasets of INbreast and SIIM-ACR\ndemonstrate our EG-ViT model can effectively learn/transfer experts' domain\nknowledge and achieve much better performance than baselines. Meanwhile, it\nsuccessfully rectifies the harmful shortcut learning and significantly improves\nthe EG-ViT model's interpretability. In general, EG-ViT takes the advantages of\nboth human expert's prior knowledge and the power of deep neural networks. This\nwork opens new avenues for advancing current artificial intelligence paradigms\nby infusing human intelligence.",
    "descriptor": "",
    "authors": [
      "Chong Ma",
      "Lin Zhao",
      "Yuzhong Chen",
      "Lu Zhang",
      "Zhenxiang Xiao",
      "Haixing Dai",
      "David Liu",
      "Zihao Wu",
      "Zhengliang Liu",
      "Sheng Wang",
      "Jiaxing Gao",
      "Changhe Li",
      "Xi Jiang",
      "Tuo Zhang",
      "Qian Wang",
      "Dinggang Shen",
      "Dajiang Zhu",
      "Tianming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12466"
  },
  {
    "id": "arXiv:2205.12467",
    "title": "R2D2: Robust Data-to-Text with Replacement Detection",
    "abstract": "Unfaithful text generation is a common problem for text generation systems.\nIn the case of Data-to-Text (D2T) systems, the factuality of the generated text\nis particularly crucial for any real-world applications. We introduce R2D2, a\ntraining framework that addresses unfaithful Data-to-Text generation by\ntraining a system both as a generator and a faithfulness discriminator with\nadditional replacement detection and unlikelihood learning tasks. To facilitate\nsuch training, we propose two methods for sampling unfaithful sentences. We\nargue that the poor entity retrieval capability of D2T systems is one of the\nprimary sources of unfaithfulness, so in addition to the existing metrics, we\nfurther propose NER-based metrics to evaluate the fidelity of D2T generations.\nOur experimental results show that R2D2 systems could effectively mitigate the\nunfaithful text generation, and they achieve new state-of-the-art results on\nFeTaQA, LogicNLG, and ToTTo, all with significant improvements.",
    "descriptor": "",
    "authors": [
      "Linyong Nan",
      "Lorenzo Jaime Yu Flores",
      "Yilun Zhao",
      "Yixin Liu",
      "Luke Benson",
      "Weijin Zou",
      "Dragomir Radev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12467"
  },
  {
    "id": "arXiv:2205.12468",
    "title": "Efficient Textured Mesh Recovery from Multiple Views with Differentiable  Rendering",
    "abstract": "Despite of the promising results on shape and color recovery using\nself-supervision, the multi-layer perceptrons-based methods usually costs hours\nto train the deep neural network due to the implicit surface representation.\nMoreover, it is quite computational intensive to render a single image, since a\nforward network inference is required for each pixel. To tackle these\nchallenges, in this paper, we propose an efficient coarse-to-fine approach to\nrecover the textured mesh from multi-view images. Specifically, we take\nadvantage of a differentiable Poisson Solver to represent the shape, which is\nable to produce topology-agnostic and watertight surfaces. To account for the\ndepth information, we optimize the shape geometry by minimizing the difference\nbetween the rendered mesh with the depth predicted by the learning-based\nmulti-view stereo algorithm. In contrast to the implicit neural representation\non shape and color, we introduce a physically based inverse rendering scheme to\njointly estimate the lighting and reflectance of the objects, which is able to\nrender the high resolution image at real-time. Additionally, we fine-tune the\nextracted mesh by inverse rendering to obtain the mesh with fine details and\nhigh fidelity image. We have conducted the extensive experiments on several\nmulti-view stereo datasets, whose promising results demonstrate the efficacy of\nour proposed approach. We will make our full implementation publicly available.",
    "descriptor": "",
    "authors": [
      "Lixiang Lin",
      "Yisu Zhang",
      "Jianke Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12468"
  },
  {
    "id": "arXiv:2205.12469",
    "title": "Logical Satisfiability of Counterfactuals for Faithful Explanations in  NLI",
    "abstract": "Evaluating an explanation's faithfulness is desired for many reasons such as\ntrust, interpretability and diagnosing the sources of model's errors. In this\nwork, which focuses on the NLI task, we introduce the methodology of\nFaithfulness-through-Counterfactuals, which first generates a counterfactual\nhypothesis based on the logical predicates expressed in the explanation, and\nthen evaluates if the model's prediction on the counterfactual is consistent\nwith that expressed logic (i.e. if the new formula is \\textit{logically\nsatisfiable}). In contrast to existing approaches, this does not require any\nexplanations for training a separate verification model. We first validate the\nefficacy of automatic counterfactual hypothesis generation, leveraging on the\nfew-shot priming paradigm. Next, we show that our proposed metric distinguishes\nbetween human-model agreement and disagreement on new counterfactual input. In\naddition, we conduct a sensitivity analysis to validate that our metric is\nsensitive to unfaithful explanations.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Suzanna Sia",
      "Anton Belyy",
      "Amjad Almahairi",
      "Madian Khabsa",
      "Luke Zettlemoyer",
      "Lambert Mathias"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12469"
  },
  {
    "id": "arXiv:2205.12470",
    "title": "Vehicle Guidance and Tracking Systems",
    "abstract": "Our application of command and control is the Aegis Combat System. Major\ncomponents of this system include missile guidance and missile tracking. To\nlook further into some of the aspects of these systems, an extremely simplified\nmodel of the Aegis Combat System will be designed. In this simplified model, a\nsmall-scale car will autonomously follow a small-scale remote-controlled car.\nThere will be three major components of this system: the controller and the two\nsmall-scale cars. Through this model, the team can demonstrate the real-world\napplication of certain aspects of C2 such as command, communication, and sensor\ndata fusion. Figure 1 shows a picture of the Aegis Combat System.",
    "descriptor": "",
    "authors": [
      "Ryan Baker",
      "John Garvey",
      "Mitchell Kraft",
      "Manoj Mathews",
      "Kieran O Connor",
      "Matthew Wolf"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12470"
  },
  {
    "id": "arXiv:2205.12471",
    "title": "Learning a Better Initialization for Soft Prompts via Meta-Learning",
    "abstract": "Prompt tuning (PT) is an effective approach to adapting pre-trained language\nmodels to downstream tasks. Without a good initialization, prompt tuning\ndoesn't perform well under few-shot settings. So pre-trained prompt tuning\n(PPT) is proposed to initialize prompts by leveraging pre-training data. We\npropose MetaPT (Meta-learned Prompt Tuning) to further improve PPT's\ninitialization by considering latent structure within the pre-training data.\nSpecifically, we introduce the structure by first clustering pre-training data\ninto different auxiliary tasks with unsupervised methods. Then we use these\ntasks to pre-train prompts with a meta-learning algorithm. Such a process can\nmake prompts learn a better initialization by discovering commonalities among\nthese auxiliary tasks. We evaluate our method on seven downstream tasks. Our\nMetaPT achieves better and more stable performance than the state-of-the-art\nmethod.",
    "descriptor": "",
    "authors": [
      "Yukun Huang",
      "Kun Qian",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12471"
  },
  {
    "id": "arXiv:2205.12472",
    "title": "Mathematical Modelling of TEAM and VTEAM Memristor Model Using VerilogA",
    "abstract": "Anyone who looks into the circuitry world will be familiar with the three\nfundamental circuit elements - capacitor, resistor, and inductor. These circuit\nelements are defined by the relation between two of the four fundamental\ncircuit variables current, voltage, charge, and flux. However, in 1971, Prof.\nLeon Chua proposed on the grounds of symmetry that there should be a fourth\nfundamental circuit element that gives the relation between flux and charge. He\nnamed this the memristor, which is short for memory resistor. This theory was\nthen practicallymodeled, in May 2008 when the researchers at HP Labs published\na paper announcing a model for a physical realization of a memristor. This\nreport mainly focuses on the model of memristor and its applications. The\nadvantages of variable resistance, flexibility, no leakage current, and\ncompatibility with CMOS. The element memristor exhibits different\ncharacteristics for different applications which results in the formation of\ndifferent models of the memristor. This paper gives a review of different\nmodels of the memristor. Memristors devices can be used in many applications\nsuch as memory, logic, and neuromorphic systems. A computer model of the\nmemristor would be a useful tool for analyzing circuit behavior to help in\ndeveloping the application of this memristor is a passive circuit element via\nsimulation. In this paper, various VerilogA model of memristor devices are\nsimulated for sinusoidal inputs and output are verified Various window\nfunctions has been used. The circuit analysis of the various memristor models\nis done",
    "descriptor": "",
    "authors": [
      "Manoj Mathews"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12472"
  },
  {
    "id": "arXiv:2205.12474",
    "title": "A Correlation Analysis and Visualization of Climate Change using  Post-Disaster Heterogeneous Datasets",
    "abstract": "There are numerous geo-climatic and human factors that contribute to the\noccurrence of natural disasters in the real-world scenario. Besides the study\nof causes and preconditions of such calamities, post-disaster analysis is\nessential for the efficient management of the disaster situation. This process\nneeds timely and accurate data in light of the increasing frequency and\nseverity of climate change-related extreme weather events. The analysis of\ndisaster data involves the challenging task of integrating multiple\nheterogeneous sources, data ingestion and visualization. This paper aims at\nproviding a three-dimensional analytical view of disaster data as time-series\ncharts and a statistical model to evaluate the correlation between the\noccurrence of disasters, climate change and the corresponding economic damages\n(as a percentage of GDP). Therefore, statistical methodologies are leveraged to\nplay important role in managing disasters, from preparation to recovery and\nreporting. The graphical representations provide insights on regional trends\nthat follow, related to factors such as the proportion of each type of disaster\nin the various losses incurred. Therefore, obtaining reliable information about\nthe population, the economy and climate are crucial both for risk management\nand preparedness and for responding to disasters. The research puts forth a\ndetailed statistical methodology with a spatial dimension to study the impacts\non the economy and infrastructure in the aftermath of a disaster thereby\nensuring specialized assistance. The inference of the analysis confirmed a\npositive correlation between climate change and occurrence of natural\ndisasters. Therefore, statistical evidence of an important phenomenon like\nclimate change affecting natural disasters brings awareness among the\npopulation in the society to be more environmentally responsible.",
    "descriptor": "",
    "authors": [
      "Sukeerthi Mandyam",
      "Shanmuga Priya",
      "Shalini Suresh",
      "Kavitha Srinivasan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12474"
  },
  {
    "id": "arXiv:2205.12475",
    "title": "Low Resource Style Transfer via Domain Adaptive Meta Learning",
    "abstract": "Text style transfer (TST) without parallel data has achieved some practical\nsuccess. However, most of the existing unsupervised text style transfer methods\nsuffer from (i) requiring massive amounts of non-parallel data to guide\ntransferring different text styles. (ii) colossal performance degradation when\nfine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain\nAdaptive Meta-Learning with Adversarial Transfer Model), which consists of two\nparts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn\ngeneral knowledge in multiple heterogeneous source domains, capable of adapting\nto new unseen domains with a small amount of data. Moreover, we propose a new\nunsupervised TST approach Adversarial Transfer Model (ATM), composed of a\nsequence-to-sequence pre-trained language model and uses adversarial style\ntraining for better content preservation and style transfer. Results on\nmulti-domain datasets demonstrate that our approach generalizes well on unseen\nlow-resource domains, achieving state-of-the-art results against ten strong\nbaselines.",
    "descriptor": "\nComments: Accept in NAACL 2022(oral)\n",
    "authors": [
      "Xiangyang Li",
      "Xiang Long",
      "Yu Xia",
      "Sujian Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12475"
  },
  {
    "id": "arXiv:2205.12476",
    "title": "Leveraging Locality in Abstractive Text Summarization",
    "abstract": "Despite the successes of neural attention models for natural language\ngeneration tasks, the quadratic memory complexity of the self-attention module\nwith respect to the input length hinders their applications in long text\nsummarization. Instead of designing more efficient attention modules, we\napproach this problem by investigating if models with a restricted context can\nhave competitive performance compared with the memory-efficient attention\nmodels that maintain a global context by treating the input as an entire\nsequence. Our model is applied to individual pages, which contain parts of\ninputs grouped by the principle of locality, during both encoding and decoding\nstages. We empirically investigated three kinds of localities in text\nsummarization at different levels, ranging from sentences to documents. Our\nexperimental results show that our model can have better performance compared\nwith strong baseline models with efficient attention modules, and our analysis\nprovides further insights of our locality-aware modeling strategy.",
    "descriptor": "",
    "authors": [
      "Yixin Liu",
      "Ansong Ni",
      "Linyong Nan",
      "Budhaditya Deb",
      "Chenguang Zhu",
      "Ahmed H. Awadallah",
      "Dragomir Radev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12476"
  },
  {
    "id": "arXiv:2205.12484",
    "title": "GisPy: A Tool for Measuring Gist Inference Score in Text",
    "abstract": "Decision making theories such as Fuzzy-Trace Theory (FTT) suggest that\nindividuals tend to rely on gist, or bottom-line meaning, in the text when\nmaking decisions. In this work, we delineate the process of developing GisPy,\nan open-source tool in Python for measuring the Gist Inference Score (GIS) in\ntext. Evaluation of GisPy on documents in three benchmarks from the news and\nscientific text domains demonstrates that scores generated by our tool\nsignificantly distinguish low vs. high gist documents. Our tool is publicly\navailable to use at: https://github.com/phosseini/GisPy.",
    "descriptor": "\nComments: Accepted to the 4th Workshop on Narrative Understanding @ NAACL 2022\n",
    "authors": [
      "Pedram Hosseini",
      "Christopher R. Wolfe",
      "Mona Diab",
      "David A. Broniatowski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12484"
  },
  {
    "id": "arXiv:2205.12485",
    "title": "Conditional set generation using Seq2seq models",
    "abstract": "Conditional set generation learns a mapping from an input sequence of tokens\nto a set. Several NLP tasks, such as entity typing and dialogue emotion\ntagging, are instances of set generation. Sequence-to-sequence~(Seq2seq) models\nare a popular choice to model set generation, but they treat a set as a\nsequence and do not fully leverage its key properties, namely order-invariance\nand cardinality. We propose a novel algorithm for effectively sampling\ninformative orders over the combinatorial space of label orders. Further, we\njointly model the set cardinality and output by adding the set size as the\nfirst element and taking advantage of the autoregressive factorization used by\nSeq2seq models. Our method is a model-independent data augmentation approach\nthat endows any Seq2seq model with the signals of order-invariance and\ncardinality. Training a Seq2seq model on this new augmented data~(without any\nadditional annotations) gets an average relative improvement of 20% for four\nbenchmarks datasets across models spanning from BART-base, T5-xxl, and GPT-3.",
    "descriptor": "",
    "authors": [
      "Aman Madaan",
      "Dheeraj Rajagopal",
      "Niket Tandon",
      "Yiming Yang",
      "Antoine Bosselut"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12485"
  },
  {
    "id": "arXiv:2205.12486",
    "title": "Factorizing Content and Budget Decisions in Abstractive Summarization of  Long Documents by Sampling Summary Views",
    "abstract": "We argue that disentangling content selection from the budget used to cover\nsalient content improves the performance and applicability of abstractive\nsummarizers. Our method, FactorSum, does this disentanglement by factorizing\nsummarization into two steps through an energy function: (1) generation of\nabstractive summary views; (2) combination of these views into a final summary,\nfollowing a budget and content guidance. This guidance may come from different\nsources, including from an advisor model such as BART or BigBird, or in oracle\nmode -- from the reference. This factorization achieves significantly higher\nROUGE scores on multiple benchmarks for long document summarization, namely\nPubMed, arXiv, and GovReport. Most notably, our model is effective for domain\nadaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1\nscore on arXiv, which indicates a strong performance due to more flexible\nbudget adaptation and content selection less dependent on domain-specific\ntextual structure.",
    "descriptor": "",
    "authors": [
      "Marcio Fonseca",
      "Yftah Ziser",
      "Shay B. Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12486"
  },
  {
    "id": "arXiv:2205.12487",
    "title": "End-to-End Multimodal Fact-Checking and Explanation Generation: A  Challenging Dataset and Models",
    "abstract": "We propose the end-to-end multimodal fact-checking and explanation\ngeneration, where the input is a claim and a large collection of web sources,\nincluding articles, images, videos, and tweets, and the goal is to assess the\ntruthfulness of the claim by retrieving relevant evidence and predicting a\ntruthfulness label (i.e., support, refute and not enough information), and\ngenerate a rationalization statement to explain the reasoning and ruling\nprocess. To support this research, we construct Mocheg, a large-scale dataset\nthat consists of 21,184 claims where each claim is assigned with a truthfulness\nlabel and ruling statement, with 58,523 evidence in the form of text and\nimages. To establish baseline performances on Mocheg, we experiment with\nseveral state-of-the-art neural architectures on the three pipelined subtasks:\nmultimodal evidence retrieval, claim verification, and explanation generation,\nand demonstrate the current state-of-the-art performance of end-to-end\nmultimodal fact-checking is still far from satisfying. To the best of our\nknowledge, we are the first to build the benchmark dataset and solutions for\nend-to-end multimodal fact-checking and justification.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Barry Menglong Yao",
      "Aditya Shah",
      "Lichao Sun",
      "Jin-Hee Cho",
      "Lifu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12487"
  },
  {
    "id": "arXiv:2205.12490",
    "title": "Improve Event Extraction via Self-Training with Gradient Guidance",
    "abstract": "Data scarcity and imbalance have been the main factors that hinder the\nprogress of event extraction (EE). In this work, we propose a self-training\nwith gradient guidance (STGG) framework which consists of (1) a base event\nextraction model which is firstly trained on existing event annotations and\nthen applied to large-scale unlabeled corpora to predict new event mentions,\nand (2) a scoring model that takes in each predicted event trigger and argument\nas well as their path in the Abstract Meaning Representation (AMR) graph to\nestimate a probability score indicating the correctness of the event\nprediction. The new event predictions along with their correctness scores are\nthen used as pseudo labeled examples to improve the base event extraction model\nwhile the magnitude and direction of its gradients are guided by the\ncorrectness scores. Experimental results on three benchmark datasets, including\nACE05-E, ACE05-E+ and ERE-EN, demonstrate the effectiveness of the STGG\nframework on event extraction task with up to 1.9 F-score improvement over the\nbase event extraction models. Our experimental analysis further shows that STGG\nis a general framework as it can be applied to any base event extraction models\nand improve their performance by leveraging broad unlabeled data, even when the\nhigh-quality AMR graph annotations are not available.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Zhiyang Xu",
      "Lifu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12490"
  },
  {
    "id": "arXiv:2205.12491",
    "title": "Fine-grained Contrastive Learning for Relation Extraction",
    "abstract": "Recent relation extraction (RE) works have shown encouraging improvements by\nconducting contrastive learning on silver labels generated by distant\nsupervision before fine-tuning on gold labels. Existing methods typically\nassume all these silver labels are accurate and therefore treat them equally in\ncontrastive learning; however, distant supervision is inevitably noisy -- some\nsilver labels are more reliable than others. In this paper, we first assess the\nquality of silver labels via a simple and automatic approach we call \"learning\norder denoising,\" where we train a language model to learn these relations and\nrecord the order of learned training instances. We show that learning order\nlargely corresponds to label accuracy -- early learned silver labels have, on\naverage, more accurate labels compared to later learned silver labels. We then\npropose a novel fine-grained contrastive learning (FineCL) for RE, which\nleverages this additional, fine-grained information about which silver labels\nare and are not noisy to improve the quality of learned relationship\nrepresentations for RE. Experiments on many RE benchmarks show consistent,\nsignificant performance gains of FineCL over state-of-the-art methods.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "William Hogan",
      "Jiacheng Li",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12491"
  },
  {
    "id": "arXiv:2205.12493",
    "title": "Federated Self-supervised Learning for Heterogeneous Clients",
    "abstract": "Federated Learning has become an important learning paradigm due to its\nprivacy and computational benefits. As the field advances, two key challenges\nthat still remain to be addressed are: (1) system heterogeneity - variability\nin the compute and/or data resources present on each client, and (2) lack of\nlabeled data in certain federated settings. Several recent developments have\ntried to overcome these challenges independently. In this work, we propose a\nunified and systematic framework, \\emph{Heterogeneous Self-supervised Federated\nLearning} (Hetero-SSFL) for enabling self-supervised learning with federation\non heterogeneous clients. The proposed framework allows collaborative\nrepresentation learning across all the clients without imposing architectural\nconstraints or requiring presence of labeled data. The key idea in Hetero-SSFL\nis to let each client train its unique self-supervised model and enable the\njoint learning across clients by aligning the lower dimensional representations\non a common dataset. The entire training procedure could be viewed as self and\npeer-supervised as both the local training and the alignment procedures do not\nrequire presence of any labeled data. As in conventional self-supervised\nlearning, the obtained client models are task independent and can be used for\nvaried end-tasks. We provide a convergence guarantee of the proposed framework\nfor non-convex objectives in heterogeneous settings and also empirically\ndemonstrate that our proposed approach outperforms the state of the art methods\nby a significant margin.",
    "descriptor": "",
    "authors": [
      "Disha Makhija",
      "Nhat Ho",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12493"
  },
  {
    "id": "arXiv:2205.12494",
    "title": "A Multi-domain Magneto Tunnel Junction for Racetrack Nanowire Strips",
    "abstract": "Domain-wall memory (DWM) has SRAM class access performance, low energy, high\nendurance, high density, and CMOS compatibility. Recently, shift reliability\nand processing-using-memory (PuM) proposals developed a need to count the\nnumber of parallel or anti-parallel domains in a portion of the DWM nanowire.\nIn this paper we propose a multi-domain magneto-tunnel junction (MTJ) that can\ndetect different resistance levels as a function of a the number of parallel or\nanti-parallel domains. Using detailed micromagnetic simulation with LLG, we\ndemonstrate the multi-domain MTJ, study the benefit of its macro-size on\nresilience to process variation and present a macro-model for scaling the size\nof the multi-domain MTJ. Our results indicate scalability to seven-domains\nwhile maintaining a 16.3mV sense margin.",
    "descriptor": "\nComments: This paper is under review for possible publication by the IEEE\n",
    "authors": [
      "Prayash Dutta",
      "Albert Lee",
      "Kang L. Wang",
      "Alex K. Jones",
      "Sanjukta Bhanja"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2205.12494"
  },
  {
    "id": "arXiv:2205.12495",
    "title": "ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate  Speech Detection",
    "abstract": "Hate speech detection is complex; it relies on commonsense reasoning,\nknowledge of stereotypes, and an understanding of social nuance that differs\nfrom one culture to the next. It is also difficult to collect a large-scale\nhate speech annotated dataset. In this work, we frame this problem as a\nfew-shot learning task, and show significant gains with decomposing the task\ninto its \"constituent\" parts. In addition, we see that infusing knowledge from\nreasoning datasets (e.g. Atomic2020) improves the performance even further.\nMoreover, we observe that the trained models generalize to out-of-distribution\ndatasets, showing the superiority of task decomposition and knowledge infusion\ncompared to previously used methods. Concretely, our method outperforms the\nbaseline by 17.83% absolute gain in the 16-shot case.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Badr AlKhamissi",
      "Faisal Ladhak",
      "Srini Iyer",
      "Ves Stoyanov",
      "Zornitsa Kozareva",
      "Xian Li",
      "Pascale Fung",
      "Lambert Mathias",
      "Asli Celikyilmaz",
      "Mona Diab"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12495"
  },
  {
    "id": "arXiv:2205.12496",
    "title": "Teaching Broad Reasoning Skills via Decomposition-Guided Contexts",
    "abstract": "Question-answering datasets require a broad set of reasoning skills. We show\nhow to use question decompositions to teach language models these broad\nreasoning skills in a robust fashion. Specifically, we use widely available\nQDMR representations to programmatically create synthetic contexts for real\nquestions in six multihop reasoning datasets. These contexts are carefully\ndesigned to avoid common reasoning shortcuts prevalent in real contexts that\nprevent models from learning the right skills. This results in a pretraining\ndataset, named TeaBReaC, containing 525K multihop questions (with associated\nformal programs) covering about 900 reasoning patterns. We show that\npretraining standard language models (LMs) on TeaBReaC before fine-tuning them\non target datasets improves their performance by up to 13 EM points across 3\nmultihop QA datasets, with a 30 point gain on more complex questions. The\nresulting models also demonstrate higher robustness, with a 6-11 point\nimprovement on two contrast sets. Furthermore, TeaBReaC pretraining\nsubstantially improves model performance and robustness even when starting with\nnumeracy-aware LMs pretrained using recent methods (e.g., PReasM). Our work\nthus shows how one can effectively use decomposition-guided contexts to\nrobustly teach multihop reasoning.",
    "descriptor": "",
    "authors": [
      "Harsh Trivedi",
      "Niranjan Balasubramanian",
      "Tushar Khot",
      "Ashish Sabharwal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12496"
  },
  {
    "id": "arXiv:2205.12498",
    "title": "A Survey of Graph-Theoretic Approaches for Analyzing the Resilience of  Networked Control Systems",
    "abstract": "As the scale of networked control systems increases and interactions between\ndifferent subsystems become more sophisticated, questions of the resilience of\nsuch networks increase in importance. The need to redefine classical system and\ncontrol-theoretic notions using the language of graphs has recently started to\ngain attention as a fertile and important area of research. This paper presents\nan overview of graph-theoretic methods for analyzing the resilience of\nnetworked control systems. We discuss various distributed algorithms operating\non networked systems and investigate their resilience against adversarial\nactions by looking at the structural properties of their underlying networks.\nWe present graph-theoretic methods to quantify the attack impact, and\nreinterpret some system-theoretic notions of robustness from a graph-theoretic\nstandpoint to mitigate the impact of the attacks. Moreover, we discuss\nmiscellaneous problems in the security of networked control systems which use\ngraph-theory as a tool in their analyses. We conclude by introducing some\navenues for further research in this field.",
    "descriptor": "",
    "authors": [
      "Mohammad Pirani",
      "Aritra Mitra",
      "Shreyas Sundaram"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.12498"
  },
  {
    "id": "arXiv:2205.12502",
    "title": "The Dialog Must Go On: Improving Visual Dialog via Generative  Self-Training",
    "abstract": "Visual dialog (VisDial) is a task of answering a sequence of questions\ngrounded in an image, using the dialog history as context. Prior work has\ntrained the dialog agents solely on VisDial data via supervised learning or\nleveraged pre-training on related vision-and-language datasets. This paper\npresents a semi-supervised learning approach for visually-grounded dialog,\ncalled Generative Self-Training (GST), to leverage unlabeled images on the Web.\nSpecifically, GST first retrieves in-domain images through out-of-distribution\ndetection and generates synthetic dialogs regarding the images via multimodal\nconditional text generation. GST then trains a dialog agent on the synthetic\nand the original VisDial data. As a result, GST scales the amount of training\ndata up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For\nrobust training of the generated dialogs, we also propose perplexity-based data\nselection and multimodal consistency regularization. Evaluation on VisDial v1.0\nand v0.9 datasets shows that GST achieves new state-of-the-art results on both\ndatasets. We further observe strong performance gains in the low-data regime\n(up to 9.35 absolute points on NDCG).",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Gi-Cheon Kang",
      "Sungdong Kim",
      "Jin-Hwa Kim",
      "Donghyun Kwak",
      "Byoung-Tak Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12502"
  },
  {
    "id": "arXiv:2205.12503",
    "title": "Maximising the Influence of Temporary Participants in Opinion Formation",
    "abstract": "DeGroot-style opinion formation presumes a continuous interaction among\nagents of a social network. Hence, it cannot handle agents external to the\nsocial network that interact only temporarily with the permanent ones. Many\nreal-world organisations and individuals fall into such a category. For\ninstance, a company tries to persuade as many as possible to buy its products\nand, due to various constraints, can only exert its influence for a limited\namount of time. We propose a variant of the DeGroot model that allows an\nexternal agent to interact with the permanent ones for a preset period of time.\nWe obtain several insights on maximising an external agent's influence in\nopinion formation by analysing and simulating the variant.",
    "descriptor": "",
    "authors": [
      "Zhiqiang Zhuang",
      "Kewen Wang",
      "Zhe Wang",
      "Junhu Wang",
      "Yinong Yang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.12503"
  },
  {
    "id": "arXiv:2205.12504",
    "title": "Deadlock-Free Method for Multi-Agent Pickup and Delivery Problem Using  Priority Inheritance with Temporary Priority",
    "abstract": "This paper proposes a control method for the multi-agent pickup and delivery\nproblem (MAPD problem) by extending the priority inheritance with backtracking\n(PIBT) method to make it applicable to more general environments. PIBT is an\neffective algorithm that introduces a priority to each agent, and at each\ntimestep, the agents, in descending order of priority, decide their next\nneighboring locations in the next timestep through communications only with the\nlocal agents. Unfortunately, PIBT is only applicable to environments that are\nmodeled as a bi-connected area, and if it contains dead-ends, such as\ntree-shaped paths, PIBT may cause deadlocks. However, in the real-world\nenvironment, there are many dead-end paths to locations such as the shelves\nwhere materials are stored as well as loading/unloading locations to\ntransportation trucks. Our proposed method enables MAPD tasks to be performed\nin environments with some tree-shaped paths without deadlock while preserving\nthe PIBT feature; it does this by allowing the agents to have temporary\npriorities and restricting agents' movements in the trees. First, we\ndemonstrate that agents can always reach their delivery without deadlock. Our\nexperiments indicate that the proposed method is very efficient, even in\nenvironments where PIBT is not applicable, by comparing them with those\nobtained using the well-known token passing method as a baseline.",
    "descriptor": "\nComments: The paper was accepted at 26th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2022)\n",
    "authors": [
      "Yukita Fujitani",
      "Tomoki Yamauchi",
      "Yuki Miyashita",
      "Toshiharu Sugawara"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12504"
  },
  {
    "id": "arXiv:2205.12505",
    "title": "GENEVA: Pushing the Limit of Generalizability for Event Argument  Extraction with 100+ Event Types",
    "abstract": "Numerous events occur worldwide and are documented in the news, social media,\nand various online platforms in raw text. Extracting useful and succinct\ninformation about these events is crucial to various downstream applications.\nEvent Argument Extraction (EAE) deals with the task of extracting\nevent-specific information from natural language text. In order to cater to new\nevents and domains in a realistic low-data setting, there is a growing urgency\nfor EAE models to be generalizable. Consequentially, there is a necessity for\nbenchmarking setups to evaluate the generalizability of EAE models. But most\nexisting benchmarking datasets like ACE and ERE have limited coverage in terms\nof events and cannot adequately evaluate the generalizability of EAE models. To\nalleviate this issue, we introduce a new dataset GENEVA covering a diverse\nrange of 115 events and 187 argument roles. Using this dataset, we create four\nbenchmarking test suites to assess the model's generalization capability from\ndifferent perspectives. We benchmark various representative models on these\ntest suites and compare their generalizability relatively. Finally, we propose\na new model SCAD that outperforms the previous models and serves as a strong\nbenchmark for these test suites.",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Tanmay Parekh",
      "I-Hung Hsu",
      "Kuan-Hao Huang",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12505"
  },
  {
    "id": "arXiv:2205.12506",
    "title": "Memorization in NLP Fine-tuning Methods",
    "abstract": "Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.",
    "descriptor": "",
    "authors": [
      "Fatemehsadat Mireshghallah",
      "Archit Uniyal",
      "Tianhao Wang",
      "David Evans",
      "Taylor Berg-Kirkpatrick"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12506"
  },
  {
    "id": "arXiv:2205.12507",
    "title": "Revisiting Calibration for Question Answering",
    "abstract": "Model calibration aims to adjust (calibrate) models' confidence so that they\nmatch expected accuracy. We argue that the traditional evaluation of\ncalibration (expected calibration error; ECE) does not reflect usefulness of\nthe model confidence. For example, after conventional temperature scaling,\nconfidence scores become similar for all predictions, which makes it hard for\nusers to distinguish correct predictions from wrong ones, even though it\nachieves low ECE. Building on those observations, we propose a new calibration\nmetric, MacroCE, that better captures whether the model assigns low confidence\nto wrong predictions and high confidence to correct predictions. We examine\nvarious conventional calibration methods including temperature scaling,\nfeature-based classifier, neural answer reranking, and label smoothing, all of\nwhich do not bring significant gains under our new MacroCE metric. Towards more\neffective calibration, we propose a new calibration method based on the model's\nprediction consistency along the training trajectory. This new method, which we\nname as consistency calibration, shows promise for better calibration.",
    "descriptor": "\nComments: Preprint; Feedback is welcome\n",
    "authors": [
      "Chenglei Si",
      "Chen Zhao",
      "Sewon Min",
      "Jordan Boyd-Graber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12507"
  },
  {
    "id": "arXiv:2205.12510",
    "title": "Exact Phase Transitions in Deep Learning",
    "abstract": "This work reports deep-learning-unique first-order and second-order phase\ntransitions, whose phenomenology closely follows that in statistical physics.\nIn particular, we prove that the competition between prediction error and model\ncomplexity in the training loss leads to the second-order phase transition for\nnets with one hidden layer and the first-order phase transition for nets with\nmore than one hidden layer. The proposed theory is directly relevant to the\noptimization of neural networks and points to an origin of the posterior\ncollapse problem in Bayesian deep learning.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Liu Ziyin",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.12510"
  },
  {
    "id": "arXiv:2205.12512",
    "title": "Text-to-Face Generation with StyleGAN2",
    "abstract": "Synthesizing images from text descriptions has become an active research area\nwith the advent of Generative Adversarial Networks. The main goal here is to\ngenerate photo-realistic images that are aligned with the input descriptions.\nText-to-Face generation (T2F) is a sub-domain of Text-to-Image generation (T2I)\nthat is more challenging due to the complexity and variation of facial\nattributes. It has a number of applications mainly in the domain of public\nsafety. Even though several models are available for T2F, there is still the\nneed to improve the image quality and the semantic alignment. In this research,\nwe propose a novel framework, to generate facial images that are well-aligned\nwith the input descriptions. Our framework utilizes the high-resolution face\ngenerator, StyleGAN2, and explores the possibility of using it in T2F. Here, we\nembed text in the input latent space of StyleGAN2 using BERT embeddings and\noversee the generation of facial images using text descriptions. We trained our\nframework on attribute-based descriptions to generate images of 1024x1024 in\nresolution. The images generated exhibit a 57% similarity to the ground truth\nimages, with a face semantic distance of 0.92, outperforming\nstate-of-the-artwork. The generated images have a FID score of 118.097 and the\nexperimental results show that our model generates promising images.",
    "descriptor": "\nComments: 16 pages, 5 figures, for conference, this https URL\n",
    "authors": [
      "D. M. A. Ayanthi",
      "Sarasi Munasinghe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12512"
  },
  {
    "id": "arXiv:2205.12514",
    "title": "Machine Translation Robustness to Natural Asemantic Variation",
    "abstract": "We introduce and formalize an under-studied linguistic phenomenon we call\nNatural Asemantic Variation (NAV) and investigate it in the context of Machine\nTranslation (MT) robustness. Standard MT models are shown to be less robust to\nrarer, nuanced language forms, and current robustness techniques do not account\nfor this kind of perturbation despite their prevalence in \"real world\" data.\nExperiment results provide more insight into the nature of NAV and we\ndemonstrate strategies to improve performance on NAV. We also show that NAV\nrobustness can be transferred across languages and fine that synthetic\nperturbations can achieve some but not all of the benefits of human-generated\nNAV data.",
    "descriptor": "",
    "authors": [
      "Jacob Bremerman",
      "Xiang Ren",
      "Jonathan May"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12514"
  },
  {
    "id": "arXiv:2205.12515",
    "title": "Toward Discovering Options that Achieve Faster Planning",
    "abstract": "We propose a new objective for option discovery that emphasizes the\ncomputational advantage of using options in planning. For a given set of\nepisodic tasks and a given number of options, the objective prefers options\nthat can be used to achieve a high return by composing few options. By\ncomposing few options, fast planning can be achieved. When faced with new tasks\nsimilar to the given ones, the discovered options are also expected to\naccelerate planning. Our objective extends the objective proposed by Harb et\nal. (2018) for the single-task setting to the multi-task setting. A closer look\nat Harb et al.'s objective shows that the best options discovered given one\ntask are not likely to be useful for future unseen tasks and that the\nmulti-task setting is indeed necessary for this purpose. In the same paper,\nHarb et al. also proposed an algorithm to optimize their objective, and the\nalgorithm can be naturally extended to the multi-task setting. We empirically\nshow that in the four-room domain the extension does not achieve a high\nobjective value and propose a new algorithm that better optimizes the proposed\nobjective. In the same four-room domain, we show that 1) a higher objective\nvalue is typically associated with options with which fewer planning iterations\nare needed to achieve near-optimal performance, 2) our new algorithm achieves a\nhigh objective value, which is close to the value achieved by a set of\nhuman-designed options, 3) the best number of planning iterations given the\ndiscovered options is much smaller and matches it obtained given human-designed\noptions, and 4) the options produced by our algorithm also make intuitive sense\nbecause they move to and terminate at cells near hallways connecting two\nneighbor rooms.",
    "descriptor": "",
    "authors": [
      "Yi Wan",
      "Richard S. Sutton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12515"
  },
  {
    "id": "arXiv:2205.12519",
    "title": "Structure Aware and Class Balanced 3D Object Detection on nuScenes  Dataset",
    "abstract": "3-D object detection is pivotal for autonomous driving. Point cloud based\nmethods have become increasingly popular for 3-D object detection, owing to\ntheir accurate depth information. NuTonomy's nuScenes dataset greatly extends\ncommonly used datasets such as KITTI in size, sensor modalities, categories,\nand annotation numbers. However, it suffers from severe class imbalance. The\nClass-balanced Grouping and Sampling paper addresses this issue and suggests\naugmentation and sampling strategy. However, the localization precision of this\nmodel is affected by the loss of spatial information in the downscaled feature\nmaps. We propose to enhance the performance of the CBGS model by designing an\nauxiliary network, that makes full use of the structure information of the 3D\npoint cloud, in order to improve the localization accuracy. The detachable\nauxiliary network is jointly optimized by two point-level supervisions, namely\nforeground segmentation and center estimation. The auxiliary network does not\nintroduce any extra computation during inference, since it can be detached at\ntest time.",
    "descriptor": "",
    "authors": [
      "Sushruth Nagesh",
      "Asfiya Baig",
      "Savitha Srinivasan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12519"
  },
  {
    "id": "arXiv:2205.12520",
    "title": "Molecular Absorption Effect: A Double-edged Sword of Terahertz  Communications",
    "abstract": "Communications in the terahertz band (THz) (0.1--10~THz) have been regarded\nas a promising technology for future 6G and beyond wireless systems, to\novercome the challenges of evergrowing wireless data traffic and crowded\nspectrum. As the frequency increases from the microwave band to the THz band,\nnew spectrum features pose unprecedented challenges to wireless communication\nsystem design. The molecular absorption effect is one of the new THz spectrum\nproperties, which enlarges the path loss and noise at specific frequencies.\nThis brings in a double-edged sword for THz wireless communication systems. On\none hand, from the data rate viewpoint, molecular absorption is detrimental,\nsince it mitigates the received signal power and degrades the channel capacity.\nOn the other hand, it is worth noticing that for wireless security and\ncovertness, the molecular absorption effect can be utilized to safeguard THz\ncommunications among users. In this paper, the features of the molecular\nabsorption effect and their impact on the THz system design are analyzed under\nvarious scenarios, with the ultimate goal of providing guidelines to how better\nexploit this unique THz phenomenon. Specifically, since the molecular\nabsorption greatly depends on the propagation medium, different communication\nscenarios consisting of various media are discussed, including terrestrial, air\nand space, sea surface and nano-scale communications. Furthermore, two novel\nmolecular absorption enlightened secure and covert communication schemes are\npresented, where the molecular absorption effect is utilized as the key and\nunique feature to boost security and covertness.",
    "descriptor": "",
    "authors": [
      "Chong Han",
      "Weijun Gao",
      "Nan Yang",
      "Josep M. Jornet"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12520"
  },
  {
    "id": "arXiv:2205.12522",
    "title": "Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset",
    "abstract": "Research in massively multilingual image captioning has been severely\nhampered by a lack of high-quality evaluation datasets. In this paper we\npresent the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse\nset of 3600 images annotated with human-generated reference captions in 36\nlanguages. The images were selected from across the world, covering regions\nwhere the 36 languages are spoken, and annotated with captions that achieve\nconsistency in terms of style across all languages, while avoiding annotation\nartifacts due to direct translation. We apply this benchmark to model selection\nfor massively multilingual image captioning models, and show superior\ncorrelation results with human evaluations when using XM3600 as golden\nreferences for automatic metrics.",
    "descriptor": "",
    "authors": [
      "Ashish V. Thapliyal",
      "Jordi Pont-Tuset",
      "Xi Chen",
      "Radu Soricut"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12522"
  },
  {
    "id": "arXiv:2205.12523",
    "title": "TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation",
    "abstract": "Direct speech-to-speech translation (S2ST) systems leverage recent progress\nin speech representation learning, where a sequence of discrete representations\n(units) derived in a self-supervised manner, are predicted from the model and\npassed to a vocoder for speech synthesis, still facing the following\nchallenges: 1) Acoustic multimodality: the discrete units derived from speech\nwith same content could be indeterministic due to the acoustic property (e.g.,\nrhythm, pitch, and energy), which causes deterioration of translation accuracy;\n2) high latency: current S2ST systems utilize autoregressive models which\npredict each unit conditioned on the sequence previously generated, failing to\ntake full advantage of parallelism. In this work, we propose TranSpeech, a\nspeech-to-speech translation model with bilateral perturbation. To alleviate\nthe acoustic multimodal problem, we propose bilateral perturbation, which\nconsists of the style normalization and information enhancement stages, to\nlearn only the linguistic information from speech samples and generate more\ndeterministic representations. With reduced multimodality, we step forward and\nbecome the first to establish a non-autoregressive S2ST technique, which\nrepeatedly masks and predicts unit choices and produces high-accuracy results\nin just a few cycles. Experimental results on three language pairs demonstrate\nthe state-of-the-art results by up to 2.5 BLEU points over the best\npublicly-available textless S2ST baseline. Moreover, TranSpeech shows a\nsignificant improvement in inference latency, enabling speedup up to 21.4x than\nautoregressive technique. Audio samples are available at\n\\url{https://TranSpeech.github.io/}",
    "descriptor": "",
    "authors": [
      "Rongjie Huang",
      "Zhou Zhao",
      "Jinglin Liu",
      "Huadai Liu",
      "Yi Ren",
      "Lichao Zhang",
      "Jinzheng He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12523"
  },
  {
    "id": "arXiv:2205.12524",
    "title": "Accelerating Diffusion Models via Early Stop of the Diffusion Process",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive\nperformance on various generation tasks. By modeling the reverse process of\ngradually diffusing the data distribution into a Gaussian distribution,\ngenerating a sample in DDPMs can be regarded as iteratively denoising a\nrandomly sampled Gaussian noise. However, in practice DDPMs often need hundreds\neven thousands of denoising steps to obtain a high-quality sample from the\nGaussian noise, leading to extremely low inference efficiency. In this work, we\npropose a principled acceleration strategy, referred to as Early-Stopped DDPM\n(ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where\nonly the few initial diffusing steps are considered and the reverse denoising\nprocess starts from a non-Gaussian distribution. By further adopting a powerful\npre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from\nthe target non-Gaussian distribution can be efficiently achieved by diffusing\nsamples obtained from the pre-trained generative model. In this way, the number\nof required denoising steps is significantly reduced. In the meantime, the\nsample quality of ES-DDPM also improves substantially, outperforming both the\nvanilla DDPM and the adopted pre-trained generative model. On extensive\nexperiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat,\nES-DDPM obtains promising acceleration effect and performance improvement over\nrepresentative baseline methods. Moreover, ES-DDPM also demonstrates several\nattractive properties, including being orthogonal to existing acceleration\nmethods, as well as simultaneously enabling both global semantic and local\npixel-level control in image generation.",
    "descriptor": "",
    "authors": [
      "Zhaoyang Lyu",
      "Xudong XU",
      "Ceyuan Yang",
      "Dahua Lin",
      "Bo Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12524"
  },
  {
    "id": "arXiv:2205.12527",
    "title": "Segmenting Numerical Substitution Ciphers",
    "abstract": "Deciphering historical substitution ciphers is a challenging problem. Example\nproblems that have been previously studied include detecting cipher type,\ndetecting plaintext language, and acquiring the substitution key for segmented\nciphers. However, attacking unsegmented, space-free ciphers is still a\nchallenging task. Segmentation (i.e. finding substitution units) is the first\nstep towards cracking those ciphers. In this work, we propose the first\nautomatic methods to segment those ciphers using Byte Pair Encoding (BPE) and\nunigram language models. Our methods achieve an average segmentation error of\n2\\% on 100 randomly-generated monoalphabetic ciphers and 27\\% on 3 real\nhomophonic ciphers. We also propose a method for solving non-deterministic\nciphers with existing keys using a lattice and a pretrained language model. Our\nmethod leads to the full solution of the IA cipher; a real historical cipher\nthat has not been fully solved until this work.",
    "descriptor": "",
    "authors": [
      "Nada Aldarrab",
      "Jonathan May"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12527"
  },
  {
    "id": "arXiv:2205.12528",
    "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly  Supervised Text Classification",
    "abstract": "Weakly supervised text classification methods typically train a deep neural\nclassifier based on pseudo-labels. The quality of pseudo-labels is crucial to\nfinal performance but they are inevitably noisy due to their heuristic nature,\nso selecting the correct ones has a huge potential for performance boost. One\nstraightforward solution is to select samples based on the softmax probability\nscores in the neural classifier corresponding to their pseudo-labels. However,\nwe show through our experiments that such solutions are ineffective and\nunstable due to the erroneously high-confidence predictions from poorly\ncalibrated models. Recent studies on the memorization effects of deep neural\nmodels suggest that these models first memorize training samples with clean\nlabels and then those with noisy labels. Inspired by this observation, we\npropose a novel pseudo-label selection method LOPS that takes learning order of\nsamples into consideration. We hypothesize that the learning order reflects the\nprobability of wrong annotation in terms of ranking, and therefore, propose to\nselect the samples that are learnt earlier. LOPS can be viewed as a strong\nperformance-boost plug-in to most of existing weakly-supervised text\nclassification methods, as confirmed in extensive experiments on four\nreal-world datasets.",
    "descriptor": "",
    "authors": [
      "Dheeraj Mekala",
      "Chengyu Dong",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12528"
  },
  {
    "id": "arXiv:2205.12532",
    "title": "Skill Machines: Temporal Logic Composition in Reinforcement Learning",
    "abstract": "A major challenge in reinforcement learning is specifying tasks in a manner\nthat is both interpretable and verifiable. One common approach is to specify\ntasks through reward machines -- finite state machines that encode the task to\nbe solved. We introduce skill machines, a representation that can be learned\ndirectly from these reward machines that encode the solution to such tasks. We\npropose a framework where an agent first learns a set of base skills in a\nreward-free setting, and then combines these skills with the learned skill\nmachine to produce composite behaviours specified by any regular language, such\nas linear temporal logics. This provides the agent with the ability to map from\ncomplex logical task specifications to near-optimal behaviours zero-shot. We\ndemonstrate our approach in both a tabular and high-dimensional video game\nenvironment, where an agent is faced with several of these complex,\nlong-horizon tasks. Our results indicate that the agent is capable of\nsatisfying extremely complex task specifications, producing near optimal\nperformance with no further learning. Finally, we demonstrate that the\nperformance of skill machines can be improved with regular offline\nreinforcement learning algorithms when optimal behaviours are desired.",
    "descriptor": "",
    "authors": [
      "Geraud Nangue Tasse",
      "Devon Jarvis",
      "Steven James",
      "Benjamin Rosman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.12532"
  },
  {
    "id": "arXiv:2205.12533",
    "title": "Structured Uncertainty in the Observation Space of Variational  Autoencoders",
    "abstract": "Variational autoencoders (VAEs) are a popular class of deep generative models\nwith many variants and a wide range of applications. Improvements upon the\nstandard VAE mostly focus on the modelling of the posterior distribution over\nthe latent space and the properties of the neural network decoder. In contrast,\nimproving the model for the observational distribution is rarely considered and\ntypically defaults to a pixel-wise independent categorical or normal\ndistribution. In image synthesis, sampling from such distributions produces\nspatially-incoherent results with uncorrelated pixel noise, resulting in only\nthe sample mean being somewhat useful as an output prediction. In this paper,\nwe aim to stay true to VAE theory by improving the samples from the\nobservational distribution. We propose an alternative model for the observation\nspace, encoding spatial dependencies via a low-rank parameterisation. We\ndemonstrate that this new observational distribution has the ability to capture\nrelevant covariance between pixels, resulting in spatially-coherent samples. In\ncontrast to pixel-wise independent distributions, our samples seem to contain\nsemantically meaningful variations from the mean allowing the prediction of\nmultiple plausible outputs with a single forward pass.",
    "descriptor": "",
    "authors": [
      "James Langley",
      "Miguel Monteiro",
      "Charles Jones",
      "Nick Pawlowski",
      "Ben Glocker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12533"
  },
  {
    "id": "arXiv:2205.12535",
    "title": "\"Help! Can You Hear Me?\": Understanding How Help-Seeking Posts are  Overwhelmed on Social Media during a Natural Disaster",
    "abstract": "Posting help-seeking requests on social media has been broadly adopted by\nvictims during natural disasters to look for urgent rescue and supplies. The\nhelp-seeking requests need to get sufficient public attention and be promptly\nrouted to the intended target(s) for timely responses. However, the huge volume\nand diverse types of crisis-related posts on social media might limit\nhelp-seeking requests to receive adequate engagement and lead to their\noverwhelm. To understand this problem, this work proposes a mixed-methods\napproach to figure out the overwhelm situation of help-seeking requests, and\nindividuals' and online communities' strategies to cope. We focused on the 2021\nHenan Floods in China and collected 141,674 help-seeking posts with the keyword\n\"Henan Rainstorm Mutual Aid\" on a popular Chinese social media platform Weibo.\nThe findings indicate that help-seeking posts confront critical challenges of\nboth external overwhelm (i.e., an enormous number of non-help-seeking posts\nwith the help-seeking-related keyword distracting public attention) and\ninternal overwhelm (i.e., attention inequality with 5% help-seeking posts\nreceiving more than 95% likes, comments, and shares). We discover linguistic\nand non-linguistic help-seeking strategies that could help to prevent the\noverwhelm, such as including contact information, disclosing situational\nvulnerabilities, using subjective narratives, and structuring help-seeking\nposts to a normalized syntax. We also illustrate how community members\nspontaneously work to prevent the overwhelm with their collective wisdom (e.g.,\nnorm development through discussion) and collaborative work (e.g.,\ncross-community support). We reflect on how the findings enrich the literature\nin crisis informatics and raise design implications that facilitate effective\nhelp-seeking on social media during natural disasters.",
    "descriptor": "\nComments: 25 pages, 4 figures. Accepted for publication at CSCW 2022. Forthcoming in the Proceedings of the ACM on Human-Computer Interaction\n",
    "authors": [
      "Changyang He",
      "Yue Deng",
      "Wenjie Yang",
      "Bo Li"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.12535"
  },
  {
    "id": "arXiv:2205.12538",
    "title": "Is a Question Decomposition Unit All We Need?",
    "abstract": "Large Language Models (LMs) have achieved state-of-the-art performance on\nmany Natural Language Processing (NLP) benchmarks. With the growing number of\nnew benchmarks, we build bigger and more complex LMs. However, building new LMs\nmay not be an ideal option owing to the cost, time and environmental impact\nassociated with it. We explore an alternative route: can we modify data by\nexpressing it in terms of the model's strengths, so that a question becomes\neasier for models to answer? We investigate if humans can decompose a hard\nquestion into a set of simpler questions that are relatively easier for models\nto solve. We analyze a range of datasets involving various forms of reasoning\nand find that it is indeed possible to significantly improve model performance\n(24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via\ndecomposition. Our approach provides a viable option to involve people in NLP\nresearch in a meaningful way. Our findings indicate that Human-in-the-loop\nQuestion Decomposition (HQD) can potentially provide an alternate path to\nbuilding large LMs.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Pruthvi Patel",
      "Swaroop Mishra",
      "Mihir Parmar",
      "Chitta Baral"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12538"
  },
  {
    "id": "arXiv:2205.12539",
    "title": "Apport des ontologies pour le calcul de la similarit\u00e9 s\u00e9mantique au  sein d'un syst\u00e8me de recommandation",
    "abstract": "Measurement of the semantic relatedness or likeness between terms, words, or\ntext data plays an important role in different applications dealing with\ntextual data such as knowledge acquisition, recommender system, and natural\nlanguage processing. Over the past few years, many ontologies have been\ndeveloped and used as a form of structured representation of knowledge bases\nfor information systems. The calculation of semantic similarity from ontology\nhas developed and depending on the context is complemented by other similarity\ncalculation methods. In this paper, we propose and carry on an approach for the\ncalculation of ontology-based semantic similarity using in the context of a\nrecommender system.",
    "descriptor": "\nComments: in French language\n",
    "authors": [
      "Le Ngoc Luyen",
      "Marie-H\u00e9l\u00e8ne Abel",
      "Philippe Gouspillou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12539"
  },
  {
    "id": "arXiv:2205.12542",
    "title": "ER-TEST: Evaluating Explanation Regularization Methods for NLP Models",
    "abstract": "Neural language models' (NLMs') reasoning processes are notoriously hard to\nexplain. Recently, there has been much progress in automatically generating\nmachine rationales of NLM behavior, but less in utilizing the rationales to\nimprove NLM behavior. For the latter, explanation regularization (ER) aims to\nimprove NLM generalization by pushing the machine rationales to align with\nhuman rationales. Whereas prior works primarily evaluate such ER models via\nin-distribution (ID) generalization, ER's impact on out-of-distribution (OOD)\nis largely underexplored. Plus, little is understood about how ER model\nperformance is affected by the choice of ER criteria or by the number/choice of\ntraining instances with human rationales. In light of this, we propose ER-TEST,\na protocol for evaluating ER models' OOD generalization along three dimensions:\n(1) unseen datasets, (2) contrast set tests, and (3) functional tests. Using\nER-TEST, we study three key questions: (A) Which ER criteria are most effective\nfor the given OOD setting? (B) How is ER affected by the number/choice of\ntraining instances with human rationales? (C) Is ER effective with distantly\nsupervised human rationales? ER-TEST enables comprehensive analysis of these\nquestions by considering a diverse range of tasks and datasets. Through\nER-TEST, we show that ER has little impact on ID performance, but can yield\nlarge gains on OOD performance w.r.t. (1)-(3). Also, we find that the best ER\ncriterion is task-dependent, while ER can improve OOD performance even with\nlimited and distantly-supervised human rationales.",
    "descriptor": "\nComments: 19 pages, 10 figures\n",
    "authors": [
      "Brihi Joshi",
      "Aaron Chan",
      "Ziyi Liu",
      "Shaoliang Nie",
      "Maziar Sanjabi",
      "Hamed Firooz",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12542"
  },
  {
    "id": "arXiv:2205.12543",
    "title": "Misleading Deep-Fake Detection with GAN Fingerprints",
    "abstract": "Generative adversarial networks (GANs) have made remarkable progress in\nsynthesizing realistic-looking images that effectively outsmart even humans.\nAlthough several detection methods can recognize these deep fakes by checking\nfor image artifacts from the generation process, multiple counterattacks have\ndemonstrated their limitations. These attacks, however, still require certain\nconditions to hold, such as interacting with the detection method or adjusting\nthe GAN directly. In this paper, we introduce a novel class of simple\ncounterattacks that overcomes these limitations. In particular, we show that an\nadversary can remove indicative artifacts, the GAN fingerprint, directly from\nthe frequency spectrum of a generated image. We explore different realizations\nof this removal, ranging from filtering high frequencies to more nuanced\nfrequency-peak cleansing. We evaluate the performance of our attack with\ndifferent detection methods, GAN architectures, and datasets. Our results show\nthat an adversary can often remove GAN fingerprints and thus evade the\ndetection of generated images.",
    "descriptor": "\nComments: In IEEE Deep Learning and Security Workshop (DLS) 2022\n",
    "authors": [
      "Vera Wesselkamp",
      "Konrad Rieck",
      "Daniel Arp",
      "Erwin Quiring"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12543"
  },
  {
    "id": "arXiv:2205.12544",
    "title": "Deep Dense Local Feature Matching and Vehicle Removal for Indoor Visual  Localization",
    "abstract": "Visual localization is an essential component of intelligent transportation\nsystems, enabling broad applications that require understanding one's self\nlocation when other sensors are not available. It is mostly tackled by image\nretrieval such that the location of a query image is determined by its closest\nmatch in the previously collected images. Existing approaches focus on large\nscale localization where landmarks are helpful in finding the location.\nHowever, visual localization becomes challenging in small scale environments\nwhere objects are hardly recognizable. In this paper, we propose a visual\nlocalization framework that robustly finds the match for a query among the\nimages collected from indoor parking lots. It is a challenging problem when the\nvehicles in the images share similar appearances and are frequently replaced\nsuch as parking lots. We propose to employ a deep dense local feature matching\nthat resembles human perception to find correspondences and eliminating matches\nfrom vehicles automatically with a vehicle detector. The proposed solution is\nrobust to the scenes with low textures and invariant to false matches caused by\nvehicles. We compare our framework with alternatives to validate our\nsuperiority on a benchmark dataset containing 267 pre-collected images and 99\nquery images taken from 34 sections of a parking lot. Our method achieves 86.9\npercent accuracy, outperforming the alternatives.",
    "descriptor": "\nComments: 8 pages, 12 figures, 2 tables\n",
    "authors": [
      "Kyung Ho Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12544"
  },
  {
    "id": "arXiv:2205.12546",
    "title": "Some equivalence relation between persistent homology and morphological  dynamics",
    "abstract": "In Mathematical Morphology (MM), connected filters based on dynamics are used\nto filter the extrema of an image. Similarly, persistence is a concept coming\nfrom Persistent Homology (PH) and Morse Theory (MT) that represents the\nstability of the extrema of a Morse function. Since these two concepts seem to\nbe closely related, in this paper we examine their relationship, and we prove\nthat they are equal on n-D Morse functions, n $\\ge$ 1. More exactly, pairing a\nminimum with a 1-saddle by dynamics or pairing the same 1-saddle with a minimum\nby persistence leads exactly to the same pairing, assuming that the critical\nvalues of the studied Morse function are unique. This result is a step further\nto show how much topological data analysis and mathematical morphology are\nrelated, paving the way for a more in-depth study of the relations between\nthese two research fields.",
    "descriptor": "\nComments: Journal of Mathematical Imaging and Vision, Springer Verlag, In press\n",
    "authors": [
      "Nicolas Boutry",
      "Laurent Najman",
      "Thierry G\u00e9raud"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)",
      "Algebraic Topology (math.AT)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2205.12546"
  },
  {
    "id": "arXiv:2205.12548",
    "title": "RLPrompt: Optimizing Discrete Text Prompts With Reinforcement Learning",
    "abstract": "Prompting has shown impressive success in enabling large pretrained language\nmodels (LMs) to perform diverse NLP tasks, especially when only few downstream\ndata are available. Automatically finding the optimal prompt for each task,\nhowever, is challenging. Most existing work resorts to tuning soft prompt\n(e.g., embeddings) which falls short of interpretability, reusability across\nLMs, and applicability when gradients are not accessible. Discrete prompt, on\nthe other hand, is difficult to optimize, and is often created by \"enumeration\n(e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt\nspace systematically. This paper proposes RLPrompt, an efficient discrete\nprompt optimization approach with reinforcement learning (RL). RLPrompt\nformulates a parameter-efficient policy network that generates the desired\ndiscrete prompt after training with reward. To overcome the complexity and\nstochasticity of reward signals by the large LM environment, we incorporate\neffective reward stabilization that substantially enhances the training\nefficiency. RLPrompt is flexibly applicable to different types of LMs, such as\nmasked (e.g., BERT) and left-to-right models (e.g., GPTs), for both\nclassification and generation tasks. Experiments on few-shot classification and\nunsupervised text style transfer show superior performance over a wide range of\nexisting finetuning or prompting methods. Interestingly, the resulting\noptimized prompts are often ungrammatical gibberish text; and surprisingly,\nthose gibberish prompts are transferrable between different LMs to retain\nsignificant performance, indicating LM prompting may not follow human language\npatterns.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Mingkai Deng",
      "Jianyu Wang",
      "Cheng-Ping Hsieh",
      "Yihan Wang",
      "Han Guo",
      "Tianmin Shu",
      "Meng Song",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12548"
  },
  {
    "id": "arXiv:2205.12549",
    "title": "Learning from time-dependent streaming data with online stochastic  algorithms",
    "abstract": "We study stochastic algorithms in a streaming framework, trained on samples\ncoming from a dependent data source. In this streaming framework, we analyze\nthe convergence of Stochastic Gradient (SG) methods in a non-asymptotic manner;\nthis includes various SG methods such as the well-known stochastic gradient\ndescent (i.e., Robbins-Monro algorithm), mini-batch SG methods, together with\ntheir averaged estimates (i.e., Polyak-Ruppert averaged). Our results form a\nheuristic by linking the level of dependency and convexity to the rest of the\nmodel parameters. This heuristic provides new insights into choosing the\noptimal learning rate, which can help increase the stability of SGbased\nmethods; these investigations suggest large streaming batches with slow\ndecaying learning rates for highly dependent data sources.",
    "descriptor": "",
    "authors": [
      "Antoine Godichon-Baggioni",
      "Nicklas Werge",
      "Olivier Wintenberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12549"
  },
  {
    "id": "arXiv:2205.12550",
    "title": "Learning dynamics from partial observations with structured neural ODEs",
    "abstract": "Identifying dynamical systems from experimental data is a notably difficult\ntask. Prior knowledge generally helps, but the extent of this knowledge varies\nwith the application, and customized models are often needed. We propose a\nflexible framework to incorporate a broad spectrum of physical insight into\nneural ODE-based system identification, giving physical interpretability to the\nresulting latent space. This insight is either enforced through hard\nconstraints in the optimization problem or added in its cost function. In order\nto link the partial and possibly noisy observations to the latent state, we\nrely on tools from nonlinear observer theory to build a recognition model. We\ndemonstrate the performance of the proposed approach on numerical simulations\nand on an experimental dataset from a robotic exoskeleton.",
    "descriptor": "",
    "authors": [
      "Mona Buisson-Fenet",
      "Valery Morgenthaler",
      "Sebastian Trimpe",
      "Florent Di Meglio"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12550"
  },
  {
    "id": "arXiv:2205.12551",
    "title": "Breaking the Chain of Gradient Leakage in Vision Transformers",
    "abstract": "User privacy is of great concern in Federated Learning, while Vision\nTransformers (ViTs) have been revealed to be vulnerable to gradient-based\ninversion attacks. We show that the learned low-dimensional spatial prior in\nposition embeddings (PEs) accelerates the training of ViTs. As a side effect,\nit makes the ViTs tend to be position sensitive and at high risk of privacy\nleakage. We observe that enhancing the position-insensitive property of a ViT\nmodel is a promising way to protect data privacy against these gradient\nattacks. However, simply removing the PEs may not only harm the convergence and\naccuracy of ViTs but also places the model at more severe privacy risk. To deal\nwith the aforementioned contradiction, we propose a simple yet efficient Masked\nJigsaw Puzzle (MJP) method to break the chain of gradient leakage in ViTs. MJP\ncan be easily plugged into existing ViTs and their derived variants. Extensive\nexperiments demonstrate that our proposed MJP method not only boosts the\nperformance on large-scale datasets (i.e., ImageNet-1K), but can also improve\nthe privacy preservation capacity in the typical gradient attacks by a large\nmargin. Our code is available at: https://github.com/yhlleo/MJP.",
    "descriptor": "\nComments: 18 pages, 9 figures\n",
    "authors": [
      "Yahui Liu",
      "Bin Ren",
      "Yue Song",
      "Wei Bi",
      "Nicu Sebe",
      "Wei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12551"
  },
  {
    "id": "arXiv:2205.12554",
    "title": "Helpfulness and Fairness of Task-Oriented Dialogue Systems",
    "abstract": "Task-oriented dialogue systems aim to answer questions from users and provide\nimmediate help. Therefore, how humans perceive their helpfulness is important.\nHowever, neither the human-perceived helpfulness of task-oriented dialogue\nsystems nor its fairness implication has been studied yet. In this paper, we\ndefine a dialogue response as helpful if it is relevant & coherent, useful, and\ninformative to a query and study computational measurements of helpfulness.\nThen, we propose utilizing the helpfulness level of different groups to gauge\nthe fairness of a dialogue system. To study this, we collect human annotations\nfor the helpfulness of dialogue responses and build a classifier that can\nautomatically determine the helpfulness of a response. We design experiments\nunder 3 information-seeking scenarios and collect instances for each from\nWikipedia. With collected instances, we use carefully-constructed questions to\nquery the state-of-the-art dialogue systems. Through analysis, we find that\ndialogue systems tend to be more helpful for highly-developed countries than\nless-developed countries, uncovering a fairness issue underlying these dialogue\nsystems.",
    "descriptor": "\nComments: 16 pages, 5 figures and 8 tables\n",
    "authors": [
      "Jiao Sun",
      "Yu Hou",
      "Jiin Kim",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12554"
  },
  {
    "id": "arXiv:2205.12557",
    "title": "A model of reactive settling of activated sludge: comparison with  experimental data",
    "abstract": "A non-negligible part of the biological reactions in the activated sludge\nprocess for treatment of wastewater takes place in secondary settling tanks\nthat follow biological reactors. It is therefore of interest to develop models\nof so-called reactive settling that describe the spatial variability of\nreaction rates caused by the variation of local concentration of biomass due to\nhindered settling and compression. A reactive-settling model described by a\nsystem of nonlinear partial differential equations and a numerical scheme are\nintroduced for the simulation of hindered settling of flocculated particles,\ncompression at high concentrations, dispersion of the flocculated particles in\nthe suspension, dispersion of the dissolved substrates in the fluid, and the\nmixing that occurs near the feed inlet. The model is fitted to experiments from\na pilot plant where the sedimentation tank has a varying cross-sectional area.\nFor the reactions, a modified version of the activated sludge model no. 1\n(ASM1) is used with standard coefficients. The constitutive functions for\nhindered settling and compression are adjusted to a series of conventional\nbatch settling experiments after the initial induction period of turbulence and\nreflocculation has been transformed away. Further (but not substantial)\nimprovements of prediction of experimental steady-state scenarios can be\nachieved by also fitting additional terms modelling hydrodynamic dispersion.",
    "descriptor": "",
    "authors": [
      "Raimund B\u00fcrger",
      "Julio Careaga",
      "Stefan Diehl",
      "Romel Pineda"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12557"
  },
  {
    "id": "arXiv:2205.12558",
    "title": "Constrained Sampling from Language Models via Langevin Dynamics in  Embedding Spaces",
    "abstract": "Large pre-trained language models are well-established for their ability to\ngenerate text seemingly indistinguishable from humans. In this work, we study\nthe problem of constrained sampling from such language models. That is,\ngenerating text that satisfies user-defined constraints. Typical decoding\nstrategies which generate samples left-to-right are not always conducive to\nimposing such constraints globally. Instead, we propose MuCoLa -- a sampling\nprocedure that combines the log-likelihood of the language model with arbitrary\ndifferentiable constraints into a single energy function; and generates samples\nby initializing the entire output sequence with noise and following a Markov\nchain defined by Langevin Dynamics using the gradients of this energy. We\nevaluate our approach on different text generation tasks with soft and hard\nconstraints as well as their combinations with competitive results for toxicity\navoidance, sentiment control, and keyword-guided generation.",
    "descriptor": "",
    "authors": [
      "Sachin Kumar",
      "Biswajit Paria",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12558"
  },
  {
    "id": "arXiv:2205.12559",
    "title": "Envy-Free Cake Cutting with Graph Constraints",
    "abstract": "We study the classic problem of fairly dividing a heterogeneous and divisible\nresource -- represented by a cake, $[0,1]$ -- among $n$ agents. This work\nconsiders an interesting variant of the problem where agents are embedded on a\ngraph. The graphical constraint entails that each agent evaluates her allocated\nshare only against her neighbor's share. Given a graph, the goal is to\nefficiently find a locally envy-free allocation where every agent values her\nshare to be at least as much as any of her neighbor's share.\nThe best known algorithm (by Aziz and Mackenzie) for finding envy-free cake\ndivisions has a hyper-exponential query complexity. One of the key technical\ncontributions of this work is to identify a non-trivial graph structure -- tree\ngraphs with depth at-most two (Depth2Tree) -- on $n$ agents that admits a query\nefficient cake-cutting protocol (under the Robertson-Webb query model). In\nparticular, we develop a discrete protocol that finds a locally envy-free\nallocation among $n$ agents on depth-two trees with at-most $O(n^3 \\log(n))$\ncuts on the cake. For the special case of Depth2Tree where every non-root agent\nis connected to at-most two agents (2-Star), we show that $O(n^2)$ queries\nsuffice. We complement our algorithmic results with establishing a lower bound\nof $\\Omega(n^2)$ (evaluation) queries for finding a locally envy-free\nallocation among $n$ agents on a 1-Star graph (under the assumption that the\nroot agent partitions the cake into $n$ connected pieces).",
    "descriptor": "",
    "authors": [
      "Ganesh Ghalme",
      "Xin Huang",
      "Nidhi Rathi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.12559"
  },
  {
    "id": "arXiv:2205.12562",
    "title": "Power-based Safety Layer for Aerial Vehicles in Physical Interaction  using Lyapunov Exponents",
    "abstract": "As the performance of autonomous systems increases, safety concerns arise,\nespecially when operating in non-structured environments. To deal with these\nconcerns, this work presents a safety layer for mechanical systems that detects\nand responds to unstable dynamics caused by external disturbances. The safety\nlayer is implemented independently and on top of already present nominal\ncontrollers, like pose or wrench tracking, and limits power flow when the\nsystem's response would lead to instability. This approach is based on the\ncomputation of the Largest Lyapunov Exponent (LLE) of the system's error\ndynamics, which represent a measure of the dynamics' divergence or convergence\nrate. By actively computing this metric, divergent and possibly dangerous\nsystem behaviors can be promptly detected. The LLE is then used in combination\nwith Control Barrier Functions (CBFs) to impose power limit constraints on a\njerk controlled system. The proposed architecture is experimentally validated\non an Omnidirectional Micro Aerial Vehicle (OMAV) both in free flight and\ninteraction tasks.",
    "descriptor": "",
    "authors": [
      "Eugenio Cuniato",
      "Nicholas Lawrance",
      "Marco Tognon",
      "Roland Siegwart"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12562"
  },
  {
    "id": "arXiv:2205.12564",
    "title": "Spotlights: Probing Shapes from Spherical Viewpoints",
    "abstract": "Recent years have witnessed the surge of learned representations that\ndirectly build upon point clouds. Though becoming increasingly expressive, most\nexisting representations still struggle to generate ordered point sets.\nInspired by spherical multi-view scanners, we propose a novel sampling model\ncalled Spotlights to represent a 3D shape as a compact 1D array of depth\nvalues. It simulates the configuration of cameras evenly distributed on a\nsphere, where each virtual camera casts light rays from its principal point\nthrough sample points on a small concentric spherical cap to probe for the\npossible intersections with the object surrounded by the sphere. The structured\npoint cloud is hence given implicitly as a function of depths. We provide a\ndetailed geometric analysis of this new sampling scheme and prove its\neffectiveness in the context of the point cloud completion task. Experimental\nresults on both synthetic and real data demonstrate that our method achieves\ncompetitive accuracy and consistency while having a significantly reduced\ncomputational cost. Furthermore, we show superior performance on the downstream\npoint cloud registration task over state-of-the-art completion methods.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Jiaxin Wei",
      "Lige Liu",
      "Ran Cheng",
      "Wenqing Jiang",
      "Minghao Xu",
      "Xinyu Jiang",
      "Tao Sun",
      "Soren Schwertfeger",
      "Laurent Kneip"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.12564"
  },
  {
    "id": "arXiv:2205.12568",
    "title": "The security of the Coordicide: the implementation and analysis of  possible attack vectors",
    "abstract": "The goal of the thesis is to study and perform an analysis of the possible\nattack vectors on the Iota network 2.0 version of the protocol. In this work,\nexisting attack vectors on Distributed Ledger Technologies are studied and\ntheir applicability to the Iota 2.0 protocol is discussed. A specific attack\nthat targets the capability of honest participants to write to the ledger is\npresented and analyzed in a network of nodes that run a full node software\nversion.",
    "descriptor": "",
    "authors": [
      "Daria Dziuba\u0142towska"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12568"
  },
  {
    "id": "arXiv:2205.12569",
    "title": "Towards a Fair Comparison and Realistic Design and Evaluation Framework  of Android Malware Detectors",
    "abstract": "As in other cybersecurity areas, machine learning (ML) techniques have\nemerged as a promising solution to detect Android malware. In this sense, many\nproposals employing a variety of algorithms and feature sets have been\npresented to date, often reporting impresive detection performances. However,\nthe lack of reproducibility and the absence of a standard evaluation framework\nmake these proposals difficult to compare. In this paper, we perform an\nanalysis of 10 influential research works on Android malware detection using a\ncommon evaluation framework. We have identified five factors that, if not taken\ninto account when creating datasets and designing detectors, significantly\naffect the trained ML models and their performances. In particular, we analyze\nthe effect of (1) the presence of duplicated samples, (2) label\n(goodware/greyware/malware) attribution, (3) class imbalance, (4) the presence\nof apps that use evasion techniques and, (5) the evolution of apps. Based on\nthis extensive experimentation, we conclude that the studied ML-based detectors\nhave been evaluated optimistically, which justifies the good published results.\nOur findings also highlight that it is imperative to generate realistic\ndatasets, taking into account the factors mentioned above, to enable the design\nand evaluation of better solutions for Android malware detection.",
    "descriptor": "",
    "authors": [
      "Borja Molina-Coronado",
      "Usue Mori",
      "Alexander Mendiburu",
      "Jose Miguel-Alonso"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.12569"
  },
  {
    "id": "arXiv:2205.12570",
    "title": "EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery  and Indexing",
    "abstract": "Existing work on Entity Linking mostly assumes that the reference knowledge\nbase is complete, and therefore all mentions can be linked. In practice this is\nhardly ever the case, as knowledge bases are incomplete and because novel\nconcepts arise constantly. This paper created the Unknown Entity Discovery and\nIndexing (EDIN) benchmark where unknown entities, that is entities without a\ndescription in the knowledge base and labeled mentions, have to be integrated\ninto an existing entity linking system. By contrasting EDIN with zero-shot\nentity linking, we provide insight on the additional challenges it poses.\nBuilding on dense-retrieval based entity linking, we introduce the end-to-end\nEDIN pipeline that detects, clusters, and indexes mentions of unknown entities\nin context. Experiments show that indexing a single embedding per entity\nunifying the information of multiple mentions works better than indexing\nmentions independently.",
    "descriptor": "",
    "authors": [
      "Nora Kassner",
      "Fabio Petroni",
      "Mikhail Plekhanov",
      "Sebastian Riedel",
      "Nicola Cancedda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12570"
  },
  {
    "id": "arXiv:2205.12572",
    "title": "The SO(3) and SE(3) Lie Algebras of Rigid Body Rotations and Motions and  their Application to Discrete Integration, Gradient Descent Optimization, and  State Estimation",
    "abstract": "Common mathematical techniques such as discrete integration, gradient descent\noptimization, and state estimation (exemplified by the Runge-Kutta method,\nGauss-Newton minimization, and extended Kalman filter or EKF, respectively),\nrely on linear algebra and hence are only applicable to state vectors belonging\nto Euclidean spaces when implemented as described in the literature. This\narticle describes how to modify these methods so they can be applied to non\nEuclidean state vectors, such as those containing rotations and full motions of\nrigid bodies. To do so, this article provides an in-depth review of the SO(3)\nand SE(3) Lie groups, known as the special orthogonal and special Euclidean\ngroups of R3, which represent the rigid body rotations and motions, placing\nspecial emphasis on the different possible representations, their tangent\nspaces, the analysis of perturbations, and in particular the definitions of the\njacobians required to employ the previously mentioned calculus methods.",
    "descriptor": "\nComments: 66 pages\n",
    "authors": [
      "Eduardo Gallo"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12572"
  },
  {
    "id": "arXiv:2205.12579",
    "title": "From Pedestrian Detection to Crosswalk Estimation: An EM Algorithm and  Analysis on Diverse Datasets",
    "abstract": "In this work, we contribute an EM algorithm for estimation of corner points\nand linear crossing segments for both marked and unmarked pedestrian crosswalks\nusing the detections of pedestrians from processed LiDAR point clouds or camera\nimages. We demonstrate the algorithmic performance by analyzing three\nreal-world datasets containing multiple periods of data collection for\nfour-corner and two-corner intersections with marked and unmarked crosswalks.\nAdditionally, we include a Python video tool to visualize the crossing\nparameter estimation, pedestrian trajectories, and phase intervals in our\npublic source code.",
    "descriptor": "",
    "authors": [
      "Ross Greer",
      "Mohan Trivedi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12579"
  },
  {
    "id": "arXiv:2205.12580",
    "title": "On-Demand Redundancy Grouping: Selectable Soft-Error Tolerance for a  Multicore Cluster",
    "abstract": "With the shrinking of technology nodes and the use of parallel processor\nclusters in hostile and critical environments, such as space, run-time faults\ncaused by radiation are a serious cross-cutting concern, also impacting\narchitectural design. This paper introduces an architectural approach to\nrun-time configurable soft-error tolerance at the core level, augmenting a\nsix-core open-source RISC-V cluster with a novel On-Demand Redundancy Grouping\n(ODRG) scheme. ODRG allows the cluster to operate either as two fault-tolerant\ncores, or six individual cores for high-performance, with limited overhead to\nswitch between these modes during run-time. The ODRG unit adds less than 11% of\na core's area for a three-core group, or a total of 1% of the cluster area, and\nshows negligible timing increase, which compares favorably to a commercial\nstate-of-the-art implementation, and is 2.5$\\times$ faster in fault recovery\nre-synchronization. Furthermore, unlike other implementations, when redundancy\nis not necessary, the ODRG approach allows the redundant cores to be used for\nindependent computation, allowing up to 2.96$\\times$ increase in performance\nfor selected applications.",
    "descriptor": "",
    "authors": [
      "Michael Rogenmoser",
      "Nils Wistoff",
      "Pirmin Vogel",
      "Frank G\u00fcrkaynak",
      "Luca Benini"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12580"
  },
  {
    "id": "arXiv:2205.12581",
    "title": "Diffusion of tangential tensor fields: numerical issues and influence of  geometric properties",
    "abstract": "We study the diffusion of tangential tensor-valued data on curved surfaces.\nFor this several finite element based numerical methods are collected and used\nto solve a tangential surface n-tensor heat flow problem. These methods differ\nwith respect to the surface representation used, the required geometric\ninformation and the treatment of the tangentiality condition. We highlight the\nimportance of geometric properties and their increasing influence if the\ntensorial degree changes from n=0 to n>=1. A specific example is presented that\nillustrates how curvature drastically affects the behavior of the solution.",
    "descriptor": "\nComments: 28 pages, 6 figures\n",
    "authors": [
      "Elena Bachini",
      "Philip Brandner",
      "Thomas Jankuhn",
      "Michael Nestler",
      "Simon Praetorius",
      "Arnold Reusken",
      "Axel Voigt"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12581"
  },
  {
    "id": "arXiv:2205.12583",
    "title": "MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose",
    "abstract": "Reconstructing multi-human body mesh from a single monocular image is an\nimportant but challenging computer vision problem. In addition to the\nindividual body mesh models, we need to estimate relative 3D positions among\nsubjects to generate a coherent representation. In this work, through a single\ngraph neural network, named MUG (Multi-hUman Graph network), we construct\ncoherent multi-human meshes using only multi-human 2D pose as input. Compared\nwith existing methods, which adopt a detection-style pipeline (i.e., extracting\nimage features and then locating human instances and recovering body meshes\nfrom that) and suffer from the significant domain gap between lab-collected\ntraining datasets and in-the-wild testing datasets, our method benefits from\nthe 2D pose which has a relatively consistent geometric property across\ndatasets. Our method works like the following: First, to model the multi-human\nenvironment, it processes multi-human 2D poses and builds a novel heterogeneous\ngraph, where nodes from different people and within one person are connected to\ncapture inter-human interactions and draw the body geometry (i.e., skeleton and\nmesh structure). Second, it employs a dual-branch graph neural network\nstructure -- one for predicting inter-human depth relation and the other one\nfor predicting root-joint-relative mesh coordinates. Finally, the entire\nmulti-human 3D meshes are constructed by combining the output from both\nbranches. Extensive experiments demonstrate that MUG outperforms previous\nmulti-human mesh estimation methods on standard 3D human benchmarks --\nPanoptic, MuPoTS-3D and 3DPW.",
    "descriptor": "",
    "authors": [
      "Chenyan Wu",
      "Yandong Li",
      "Xianfeng Tang",
      "James Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12583"
  },
  {
    "id": "arXiv:2205.12585",
    "title": "A Simple and Unified Tagging Model with Priming for Relational Structure  Predictions",
    "abstract": "Relational structure extraction covers a wide range of tasks and plays an\nimportant role in natural language processing. Recently, many approaches tend\nto design sophisticated graphical models to capture the complex relations\nbetween objects that are described in a sentence. In this work, we demonstrate\nthat simple tagging models can surprisingly achieve competitive performances\nwith a small trick -- priming. Tagging models with priming append information\nabout the operated objects to the input sequence of pretrained language model.\nMaking use of the contextualized nature of pretrained language model, the\npriming approach help the contextualized representation of the sentence better\nembed the information about the operated objects, hence, becomes more suitable\nfor addressing relational structure extraction. We conduct extensive\nexperiments on three different tasks that span ten datasets across five\ndifferent languages, and show that our model is a general and effective model,\ndespite its simplicity. We further carry out comprehensive analysis to\nunderstand our model and propose an efficient approximation to our method,\nwhich can perform almost the same performance but with faster inference speed.",
    "descriptor": "",
    "authors": [
      "I-Hung Hsu",
      "Kuan-Hao Huang",
      "Shuning Zhang",
      "Wenxin Cheng",
      "Premkumar Natarajan",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12585"
  },
  {
    "id": "arXiv:2205.12586",
    "title": "Perturbation Augmentation for Fairer NLP",
    "abstract": "Unwanted and often harmful social biases are becoming ever more salient in\nNLP research, affecting both models and datasets. In this work, we ask: does\ntraining on demographically perturbed data lead to more fair language models?\nWe collect a large dataset of human annotated text perturbations and train an\nautomatic perturber on it, which we show to outperform heuristic alternatives.\nWe find: (i) Language models (LMs) pre-trained on demographically perturbed\ncorpora are more fair, at least, according to our current best metrics for\nmeasuring model fairness, and (ii) LMs finetuned on perturbed GLUE datasets\nexhibit less demographic bias on downstream tasks. We find that improved\nfairness does not come at the expense of accuracy. Although our findings appear\npromising, there are still some limitations, as well as outstanding questions\nabout how best to evaluate the (un)fairness of large language models. We hope\nthat this initial exploration of neural demographic perturbation will help\ndrive more improvement towards fairer NLP.",
    "descriptor": "",
    "authors": [
      "Rebecca Qian",
      "Candace Ross",
      "Jude Fernandes",
      "Eric Smith",
      "Douwe Kiela",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12586"
  },
  {
    "id": "arXiv:2205.12587",
    "title": "Deniable Steganography",
    "abstract": "Steganography conceals the secret message into the cover media, generating a\nstego media which can be transmitted on public channels without drawing\nsuspicion. As its countermeasure, steganalysis mainly aims to detect whether\nthe secret message is hidden in a given media. Although the steganography\ntechniques are improving constantly, the sophisticated steganalysis can always\nbreak a known steganographic method to some extent. With a stego media\ndiscovered, the adversary could find out the sender or receiver and coerce them\nto disclose the secret message, which we name as coercive attack in this paper.\nInspired by the idea of deniable encryption, we build up the concepts of\ndeniable steganography for the first time and discuss the feasible\nconstructions for it. As an example, we propose a receiver-deniable\nsteganographic scheme to deal with the receiver-side coercive attack using deep\nneural networks (DNN). Specifically, besides the real secret message, a piece\nof fake message is also embedded into the cover. On the receiver side, the real\nmessage can be extracted with an extraction module; while once the receiver has\nto surrender a piece of secret message under coercive attack, he can extract\nthe fake message to deceive the adversary with another extraction module.\nExperiments demonstrate the scalability and sensitivity of the DNN-based\nreceiver-deniable steganographic scheme.",
    "descriptor": "",
    "authors": [
      "Yong Xu",
      "Zhihua Xia",
      "Zichi Wang",
      "Xinpeng Zhang",
      "Jian Weng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12587"
  },
  {
    "id": "arXiv:2205.12590",
    "title": "RSTGen: Imbuing Fine-Grained Interpretable Control into Long-FormText  Generators",
    "abstract": "In this paper, we study the task of improving the cohesion and coherence of\nlong-form text generated by language models. To this end, we propose RSTGen, a\nframework that utilises Rhetorical Structure Theory (RST), a classical language\ntheory, to control the discourse structure, semantics and topics of generated\ntext. Firstly, we demonstrate our model's ability to control structural\ndiscourse and semantic features of generated text in open generation\nevaluation. Then we experiment on the two challenging long-form text tasks of\nargument generation and story generation. Evaluation using automated metrics\nand a metric with high correlation to human evaluation, shows that our model\nperforms competitively against existing models, while offering significantly\nmore controls over generated text than alternative methods.",
    "descriptor": "\nComments: NAACL 2022\n",
    "authors": [
      "Rilwan A. Adewoyin",
      "Ritabrata Dutta",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12590"
  },
  {
    "id": "arXiv:2205.12591",
    "title": "On Secure NOMA-CDRT Systems with Physical Layer Network Coding",
    "abstract": "This paper proposes a new scheme to enhance the secrecy performance of a\nNOMA-based coordinated direct relay transmission system (NOMA-CDRT) with an\nuntrusted relay. The physical-layer network coding and the non-orthogonal\nmultiple access scheme are combined to improve the spectrum efficiency.\nFurthermore, inter-user interference and friendly jamming signals are utilized\nto suppress the eavesdropping ability of the untrusted relay without affecting\nthe acceptance quality of legitimate users. Specifically, the far user in the\nfirst slot and the near user in the second slot act as jammers to generate\njamming signals to ensure secure transmissions of the confidential signals. We\ninvestigate the secrecy performance of the proposed scheme in NOMA-CDRT systems\nand derive the closed-form expression for the ergodic secrecy sum rate. The\nasymptotic analysis at high signal-to-noise ratio is performed to obtain more\ninsights. Finally, simulation results are presented to demonstrate the\neffectiveness of the proposed scheme and the correctness of the theoretical\nanalysis.",
    "descriptor": "\nComments: submitted to IEEE Journal for review\n",
    "authors": [
      "Hongjiang Lei",
      "Xusheng She",
      "Ki-Hong Park",
      "Imran Shafique Ansari",
      "Zheng Shi",
      "Jing Jiang",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12591"
  },
  {
    "id": "arXiv:2205.12593",
    "title": "Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious  Feature-Label Correlation",
    "abstract": "Many recent works indicate that the deep neural networks tend to take dataset\nbiases as shortcuts to make decision, rather than understand the tasks, which\nresults in failures on the real-world applications. In this work, we focus on\nthe spurious correlation between feature and label, which derive from the\nbiased data distribution in the training data, and analyze it concretely. In\nparticular, we define the word highly co-occurring with a specific label as\nbiased word, and the example containing biased word as biased example. Our\nanalysis reveals that the biased examples with spurious correlations are easier\nfor models to learn, and when predicting, the biased words make significantly\nhigher contributions to models' predictions than other words, and the models\ntend to assign the labels over-relying on the spurious correlation between\nwords and labels. To mitigate the model's over-reliance on the shortcut, we\npropose a training strategy Less-Learn-Shortcut (LLS): we quantify the biased\ndegree of the biased examples, and down-weight them with the biased degree.\nExperimental results on QM and NLI tasks show that the models improve the\nperformances both on in-domain and adversarial data (1.57% on DuQM and 2.12% on\nHANS) with our LLS.",
    "descriptor": "",
    "authors": [
      "Yanrui Du",
      "Jing Yan",
      "Yan Chen",
      "Jing Liu",
      "Sendong Zhao",
      "Hua Wu",
      "Haifeng Wang",
      "Bing Qin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12593"
  },
  {
    "id": "arXiv:2205.12594",
    "title": "Heterogeneous Reservoir Computing Models for Persian Speech Recognition",
    "abstract": "Over the last decade, deep-learning methods have been gradually incorporated\ninto conventional automatic speech recognition (ASR) frameworks to create\nacoustic, pronunciation, and language models. Although it led to significant\nimprovements in ASRs' recognition accuracy, due to their hard constraints\nrelated to hardware requirements (e.g., computing power and memory usage), it\nis unclear if such approaches are the most computationally- and\nenergy-efficient options for embedded ASR applications. Reservoir computing\n(RC) models (e.g., echo state networks (ESNs) and liquid state machines\n(LSMs)), on the other hand, have been proven inexpensive to train, have vastly\nfewer parameters, and are compatible with emergent hardware technologies.\nHowever, their performance in speech processing tasks is relatively inferior to\nthat of the deep-learning-based models. To enhance the accuracy of the RC in\nASR applications, we propose heterogeneous single and multi-layer ESNs to\ncreate non-linear transformations of the inputs that capture temporal context\nat different scales. To test our models, we performed a speech recognition task\non the Farsdat Persian dataset. Since, to the best of our knowledge, standard\nRC has not yet been employed to conduct any Persian ASR tasks, we also trained\nconventional single-layer and deep ESNs to provide baselines for comparison.\nBesides, we compared the RC performance with a standard long-short-term memory\n(LSTM) model. Heterogeneous RC models (1) show improved performance to the\nstandard RC models; (2) perform on par in terms of recognition accuracy with\nthe LSTM, and (3) reduce the training time considerably.",
    "descriptor": "\nComments: This paper was accepted for oral presentation in IEEE WCCI 2022 + IJCNN 2022, special session on Reservoir Computing: algorithms, implementations and applications\n",
    "authors": [
      "Zohreh Ansari",
      "Farzin Pourhoseini",
      "Fatemeh Hadaeghi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12594"
  },
  {
    "id": "arXiv:2205.12595",
    "title": "Wildcat: Online Continuous-Time 3D Lidar-Inertial SLAM",
    "abstract": "We present Wildcat, a novel online 3D lidar-inertial SLAM system with\nexceptional versatility and robustness. At its core, Wildcat combines a robust\nreal-time lidar-inertial odometry module, utilising a continuous-time\ntrajectory representation, with an efficient pose-graph optimisation module\nthat seamlessly supports both the single- and multi-agent settings. The\nrobustness of Wildcat was recently demonstrated in the DARPA Subterranean\nChallenge where it outperformed other SLAM systems across various types of\nsensing-degraded and perceptually challenging environments. In this paper, we\nextensively evaluate Wildcat in a diverse set of new and publicly available\nreal-world datasets and showcase its superior robustness and versatility over\ntwo existing state-of-the-art lidar-inertial SLAM systems.",
    "descriptor": "\nComments: 13 pages, 18 figures\n",
    "authors": [
      "Milad Ramezani",
      "Kasra Khosoussi",
      "Gavin Catt",
      "Peyman Moghadam",
      "Jason Williams",
      "Paulo Borges",
      "Fred Pauling",
      "Navinda Kottege"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12595"
  },
  {
    "id": "arXiv:2205.12596",
    "title": "Certify the Uncertified: Towards Assessment of Virtualization for  Mixed-criticality in the Automotive Domain",
    "abstract": "Nowadays, a feature-rich automotive vehicle offers several technologies to\nassist the driver during his trip and guarantee an amusing infotainment system\nto the other passengers, too. Consolidating worlds at different criticalities\nis a welcomed challenge for car manufacturers that have recently tried to\nleverage virtualization technologies due to reduced maintenance, deployment,\nand shipping costs. For this reason, more and more mixed-criticality systems\nare emerging, trying to assure compliance with the ISO 26262 Road Vehicle\nSafety standard. In this short paper, we provide a preliminary investigation of\nthe certification capabilities for Jailhouse, a popular open-source\npartitioning hypervisor. To this aim, we propose a testing methodology and\nshowcase the results, pointing out when the software gets to a faulting state,\ndeviating from its expected behavior. The ultimate goal is to picture the right\ndirection for the hypervisor towards a potential certification process.",
    "descriptor": "\nComments: 4 pages, to be published in \"8th international workshop on safety and security of intelligentt vehicles\" [SSIV 2022]\n",
    "authors": [
      "Marcello Cinque",
      "Luigi De Simone",
      "Andrea Marchetta"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12596"
  },
  {
    "id": "arXiv:2205.12597",
    "title": "Near-Optimal Leader Election in Population Protocols on Graphs",
    "abstract": "In the stochastic population protocol model, we are given a connected graph\nwith $n$ nodes, and in every time step, a scheduler samples an edge of the\ngraph uniformly at random and the nodes connected by this edge interact. A\nfundamental task in this model is stable leader election, in which all nodes\nstart in an identical state and the aim is to reach a configuration in which\n(1) exactly one node is elected as leader and (2) this node remains as the\nunique leader no matter what sequence of interactions follows. On cliques, the\ncomplexity of this problem has recently been settled: time-optimal protocols\nstabilize in $\\Theta(n \\log n)$ expected steps using $\\Theta(\\log \\log n)$\nstates, whereas protocols that use $O(1)$ states require $\\Theta(n^2)$ expected\nsteps.\nIn this work, we investigate the complexity of stable leader election on\ngeneral graphs. We provide the first non-trivial time lower bounds for leader\nelection on general graphs, showing that, when moving beyond cliques, the\ncomplexity landscape of leader election becomes very diverse: the time required\nto elect a leader can range from $O(1)$ to $\\Theta(n^3)$ expected steps. On the\nupper bound side, we first observe that there exists a protocol that is\ntime-optimal on many graph families, but uses polynomially-many states. In\ncontrast, we give a near-time-optimal protocol that uses only $O(\\log^2n)$\nstates that is at most a factor $\\log n$ slower. Finally, we show that the\nconstant-state protocol of Beauquier et al. [OPODIS 2013] is at most a factor\n$n \\log n$ slower than the fast polynomial-state protocol. Moreover, among\nconstant-state protocols, this protocol has near-optimal average case\ncomplexity on dense random graphs.",
    "descriptor": "\nComments: 45 pages\n",
    "authors": [
      "Dan Alistarh",
      "Joel Rybicki",
      "Sasha Voitovych"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12597"
  },
  {
    "id": "arXiv:2205.12598",
    "title": "RobustLR: Evaluating Robustness to Logical Perturbation in Deductive  Reasoning",
    "abstract": "Transformers have been shown to be able to perform deductive reasoning on a\nlogical rulebase containing rules and statements written in English natural\nlanguage. While the progress is promising, it is currently unclear if these\nmodels indeed perform logical reasoning by understanding the underlying logical\nsemantics in the language. To this end, we propose RobustLR, a suite of\nevaluation datasets that evaluate the robustness of these models to minimal\nlogical edits in rulebases and some standard logical equivalence conditions. In\nour experiments with RoBERTa and T5, we find that the models trained in prior\nworks do not perform consistently on the different perturbations in RobustLR,\nthus showing that the models are not robust to the proposed logical\nperturbations. Further, we find that the models find it especially hard to\nlearn logical negation and disjunction operators. Overall, using our evaluation\nsets, we demonstrate some shortcomings of the deductive reasoning-based\nlanguage models, which can eventually help towards designing better models for\nlogical reasoning over natural language.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Soumya Sanyal",
      "Zeyi Liao",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.12598"
  },
  {
    "id": "arXiv:2205.12600",
    "title": "ORCA: Interpreting Prompted Language Models via Locating Supporting Data  Evidence in the Ocean of Pretraining Data",
    "abstract": "Large pretrained language models have been performing increasingly well in a\nvariety of downstream tasks via prompting. However, it remains unclear from\nwhere the model learns the task-specific knowledge, especially in a zero-shot\nsetup. In this work, we want to find evidence of the model's task-specific\ncompetence from pretraining and are specifically interested in locating a very\nsmall subset of pretraining data that directly supports the model in the task.\nWe call such a subset supporting data evidence and propose a novel method ORCA\nto effectively identify it, by iteratively using gradient information related\nto the downstream task. This supporting data evidence offers interesting\ninsights about the prompted language models: in the tasks of sentiment analysis\nand textual entailment, BERT shows a substantial reliance on BookCorpus, the\nsmaller corpus of BERT's two pretraining corpora, as well as on pretraining\nexamples that mask out synonyms to the task verbalizers.",
    "descriptor": "",
    "authors": [
      "Xiaochuang Han",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12600"
  },
  {
    "id": "arXiv:2205.12601",
    "title": "Learning Distributions by Generative Adversarial Networks: Approximation  and Generalization",
    "abstract": "We study how well generative adversarial networks (GAN) learn probability\ndistributions from finite samples by analyzing the convergence rates of these\nmodels. Our analysis is based on a new oracle inequality that decomposes the\nestimation error of GAN into the discriminator and generator approximation\nerrors, generalization error and optimization error. To estimate the\ndiscriminator approximation error, we establish error bounds on approximating\nH\\\"older functions by ReLU neural networks, with explicit upper bounds on the\nLipschitz constant of the network or norm constraint on the weights. For\ngenerator approximation error, we show that neural network can approximately\ntransform a low-dimensional source distribution to a high-dimensional target\ndistribution and bound such approximation error by the width and depth of\nneural network. Combining the approximation results with generalization bounds\nof neural networks from statistical learning theory, we establish the\nconvergence rates of GANs in various settings, when the error is measured by a\ncollection of integral probability metrics defined through H\\\"older classes,\nincluding the Wasserstein distance as a special case. In particular, for\ndistributions concentrated around a low-dimensional set, we show that the\nconvergence rates of GANs do not depend on the high ambient dimension, but on\nthe lower intrinsic dimension.",
    "descriptor": "\nComments: PhD Thesis. The contents are mainly from our previous works: arXiv:2105.13010, arXiv:2101.12353, arXiv:2201.09418 and arXiv:2005.11949\n",
    "authors": [
      "Yunfei Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12601"
  },
  {
    "id": "arXiv:2205.12602",
    "title": "VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose  Estimation",
    "abstract": "This paper presents Volumetric Transformer Pose estimator (VTP), the first 3D\nvolumetric transformer framework for multi-view multi-person 3D human pose\nestimation. VTP aggregates features from 2D keypoints in all camera views and\ndirectly learns the spatial relationships in the 3D voxel space in an\nend-to-end fashion. The aggregated 3D features are passed through 3D\nconvolutions before being flattened into sequential embeddings and fed into a\ntransformer. A residual structure is designed to further improve the\nperformance. In addition, the sparse Sinkhorn attention is empowered to reduce\nthe memory cost, which is a major bottleneck for volumetric representations,\nwhile also achieving excellent performance. The output of the transformer is\nagain concatenated with 3D convolutional features by a residual design. The\nproposed VTP framework integrates the high performance of the transformer with\nvolumetric representations, which can be used as a good alternative to the\nconvolutional backbones. Experiments on the Shelf, Campus and CMU Panoptic\nbenchmarks show promising results in terms of both Mean Per Joint Position\nError (MPJPE) and Percentage of Correctly estimated Parts (PCP). Our code will\nbe available.",
    "descriptor": "",
    "authors": [
      "Yuxing Chen",
      "Renshu Gu",
      "Ouhan Huang",
      "Gangyong Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12602"
  },
  {
    "id": "arXiv:2205.12604",
    "title": "Intermediate Training on Question Answering Datasets Improves Generative  Data Augmentation",
    "abstract": "Manually annotating datasets requires domain experts to read through many\ndocuments and carefully label them, which is often expensive. Recently,\npre-trained generative language models (GLMs) have demonstrated exceptional\nabilities in generating text which motivates to leverage them for generative\ndata augmentation. We improve generative data augmentation by formulating the\ndata generation as context generation task and use question answering (QA)\ndatasets for intermediate training. Specifically, we view QA to be more as a\nformat than of a task and train GLMs as context generators for a given question\nand its respective answer. Then, we cast downstream tasks into question\nanswering format and adapt the fine-tuned context generators to the target task\ndomain. Finally, we use the fine-tuned GLM to generate relevant contexts, which\nis further used as synthetic training data for their corresponding tasks. We\nperform extensive experiments, case studies, and ablation studies on multiple\nsentiment and topic classification datasets and demonstrate substantial\nimprovements in performance in few-shot, zero-shot settings. Remarkably, on the\nSST-2 dataset, intermediate training on SocialIQA dataset achieves an\nimprovement of 40% on Macro-F1 score. Through thorough analyses, we observe\nthat QA datasets that requires high-level reasoning abilities (e.g.,\nabstractive and common-sense QA datasets) tend to give the best boost in\nperformance in both few-shot and zero-shot settings.",
    "descriptor": "",
    "authors": [
      "Dheeraj Mekala",
      "Tu Vu",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12604"
  },
  {
    "id": "arXiv:2205.12606",
    "title": "ReSmooth: Detecting and Utilizing OOD Samples when Training with Data  Augmentation",
    "abstract": "Data augmentation (DA) is a widely used technique for enhancing the training\nof deep neural networks. Recent DA techniques which achieve state-of-the-art\nperformance always meet the need for diversity in augmented training samples.\nHowever, an augmentation strategy that has a high diversity usually introduces\nout-of-distribution (OOD) augmented samples and these samples consequently\nimpair the performance. To alleviate this issue, we propose ReSmooth, a\nframework that firstly detects OOD samples in augmented samples and then\nleverages them. To be specific, we first use a Gaussian mixture model to fit\nthe loss distribution of both the original and augmented samples and\naccordingly split these samples into in-distribution (ID) samples and OOD\nsamples. Then we start a new training where ID and OOD samples are incorporated\nwith different smooth labels. By treating ID samples and OOD samples unequally,\nwe can make better use of the diverse augmented data. Further, we incorporate\nour ReSmooth framework with negative data augmentation strategies. By properly\nhandling their intentionally created ODD samples, the classification\nperformance of negative data augmentations is largely ameliorated. Experiments\non several classification benchmarks show that ReSmooth can be easily extended\nto existing augmentation strategies (such as RandAugment, rotate, and jigsaw)\nand improve on them.",
    "descriptor": "",
    "authors": [
      "Chenyang Wang",
      "Junjun Jiang",
      "Xiong Zhou",
      "Xianming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12606"
  },
  {
    "id": "arXiv:2205.12609",
    "title": "Towards More Realistic Generation of Information-Seeking Conversations",
    "abstract": "In this paper, we introduce a novel framework SimSeek (simulating\ninformation-seeking conversation from unlabeled documents) and compare two\nvariants of it to provide a deeper perspective into the information-seeking\nbehavior. We first introduce a strong simulator for information-symmetric\nconversation, SimSeek-sym, where questioner and answerer share all knowledge\nwhen conversing with one another. Although it simulates reasonable\nconversations, we take a further step toward more realistic information-seeking\nconversation. Hence, we propose SimSeek-asym that assumes information asymmetry\nbetween two agents, which encourages the questioner to seek new information\nfrom an inaccessible document. In our experiments, we demonstrate that\nSimSeek-asym successfully generates information-seeking conversations for two\ndownstream tasks, CQA and conversational search. In particular, SimSeek-asym\nimproves baseline models by 1.1-1.9 F1 score in QuAC, and by 1.1 of MRR in\nOR-QuAC. Moreover, we thoroughly analyze our synthetic datasets to identify\ncrucial factors for realistic information-seeking conversation.",
    "descriptor": "\nComments: 10 pages preprint\n",
    "authors": [
      "Gangwoo Kim",
      "Sungdong Kim",
      "Kang Min Yoo",
      "Jaewoo Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12609"
  },
  {
    "id": "arXiv:2205.12611",
    "title": "Deep Aesthetic Assessment and Retrieval of Breast Cancer Treatment  Outcomes",
    "abstract": "Treatments for breast cancer have continued to evolve and improve in recent\nyears, resulting in a substantial increase in survival rates, with\napproximately 80\\% of patients having a 10-year survival period. Given the\nserious impact that breast cancer treatments can have on a patient's body\nimage, consequently affecting her self-confidence and sexual and intimate\nrelationships, it is paramount to ensure that women receive the treatment that\noptimizes both survival and aesthetic outcomes. Currently, there is no gold\nstandard for evaluating the aesthetic outcome of breast cancer treatment. In\naddition, there is no standard way to show patients the potential outcome of\nsurgery. The presentation of similar cases from the past would be extremely\nimportant to manage women's expectations of the possible outcome. In this work,\nwe propose a deep neural network to perform the aesthetic evaluation. As a\nproof-of-concept, we focus on a binary aesthetic evaluation. Besides its use\nfor classification, this deep neural network can also be used to find the most\nsimilar past cases by searching for nearest neighbours in the highly semantic\nspace before classification. We performed the experiments on a dataset\nconsisting of 143 photos of women after conservative treatment for breast\ncancer. The results for accuracy and balanced accuracy showed the superior\nperformance of our proposed model compared to the state of the art in aesthetic\nevaluation of breast cancer treatments. In addition, the model showed a good\nability to retrieve similar previous cases, with the retrieved cases having the\nsame or adjacent class (in the 4-class setting) and having similar types of\nasymmetry. Finally, a qualitative interpretability assessment was also\nperformed to analyse the robustness and trustworthiness of the model.",
    "descriptor": "",
    "authors": [
      "Wilson Silva",
      "Maria Carvalho",
      "Carlos Mavioso",
      "Maria J. Cardoso",
      "Jaime S. Cardoso"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12611"
  },
  {
    "id": "arXiv:2205.12615",
    "title": "Autoformalization with Large Language Models",
    "abstract": "Autoformalization is the process of automatically translating from natural\nlanguage mathematics to formal specifications and proofs. A successful\nautoformalization system could advance the fields of formal verification,\nprogram synthesis, and artificial intelligence. While the long-term goal of\nautoformalization seemed elusive for a long time, we show large language models\nprovide new prospects towards this goal. We make the surprising observation\nthat LLMs can correctly translate a significant portion ($25.3\\%$) of\nmathematical competition problems perfectly to formal specifications in\nIsabelle/HOL. We demonstrate the usefulness of this process by improving a\npreviously introduced neural theorem prover via training on these\nautoformalized theorems. Our methodology results in a new state-of-the-art\nresult on the MiniF2F theorem proving benchmark, improving the proof rate from\n$29.6\\%$ to $35.2\\%$.",
    "descriptor": "\nComments: 44 pages\n",
    "authors": [
      "Yuhuai Wu",
      "Albert Q. Jiang",
      "Wenda Li",
      "Markus N. Rabe",
      "Charles Staats",
      "Mateja Jamnik",
      "Christian Szegedy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.12615"
  },
  {
    "id": "arXiv:2205.12616",
    "title": "Guiding Visual Question Answering with Attention Priors",
    "abstract": "The current success of modern visual reasoning systems is arguably attributed\nto cross-modality attention mechanisms. However, in deliberative reasoning such\nas in VQA, attention is unconstrained at each step, and thus may serve as a\nstatistical pooling mechanism rather than a semantic operation intended to\nselect information relevant to inference. This is because at training time,\nattention is only guided by a very sparse signal (i.e. the answer label) at the\nend of the inference chain. This causes the cross-modality attention weights to\ndeviate from the desired visual-language bindings. To rectify this deviation,\nwe propose to guide the attention mechanism using explicit linguistic-visual\ngrounding. This grounding is derived by connecting structured linguistic\nconcepts in the query to their referents among the visual objects. Here we\nlearn the grounding from the pairing of questions and images alone, without the\nneed for answer annotation or external grounding supervision. This grounding\nguides the attention mechanism inside VQA models through a duality of\nmechanisms: pre-training attention weight calculation and directly guiding the\nweights at inference time on a case-by-case basis. The resultant algorithm is\ncapable of probing attention-based reasoning models, injecting relevant\nassociative knowledge, and regulating the core reasoning process. This scalable\nenhancement improves the performance of VQA models, fortifies their robustness\nto limited access to supervised data, and increases interpretability.",
    "descriptor": "\nComments: Preprint, 10 pages\n",
    "authors": [
      "Thao Minh Le",
      "Vuong Le",
      "Sunil Gupta",
      "Svetha Venkatesh",
      "Truyen Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12616"
  },
  {
    "id": "arXiv:2205.12617",
    "title": "DisinfoMeme: A Multimodal Dataset for Detecting Meme Intentionally  Spreading Out Disinformation",
    "abstract": "Disinformation has become a serious problem on social media. In particular,\ngiven their short format, visual attraction, and humorous nature, memes have a\nsignificant advantage in dissemination among online communities, making them an\neffective vehicle for the spread of disinformation. We present DisinfoMeme to\nhelp detect disinformation memes. The dataset contains memes mined from Reddit\ncovering three current topics: the COVID-19 pandemic, the Black Lives Matter\nmovement, and veganism/vegetarianism. The dataset poses multiple unique\nchallenges: limited data and label imbalance, reliance on external knowledge,\nmultimodal reasoning, layout dependency, and noise from OCR. We test multiple\nwidely-used unimodal and multimodal models on this dataset. The experiments\nshow that the room for improvement is still huge for current models.",
    "descriptor": "",
    "authors": [
      "Jingnong Qu",
      "Liunian Harold Li",
      "Jieyu Zhao",
      "Sunipa Dev",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12617"
  },
  {
    "id": "arXiv:2205.12618",
    "title": "Semantic Embeddings in Semilattices",
    "abstract": "To represent anything from mathematical concepts to real-world objects, we\nhave to resort to an encoding. Encodings, such as written language, usually\nassume a decoder that understands a rich shared code. A semantic embedding is a\nform of encoding that assumes a decoder with no knowledge, or little knowledge,\nbeyond the basic rules of a mathematical formalism such as an algebra. Here we\ngive a formal definition of a semantic embedding in a semilattice which can be\nused to resolve machine learning and classic computer science problems.\nSpecifically, a semantic embedding of a problem is here an encoding of the\nproblem as sentences in an algebraic theory that extends the theory of\nsemilattices. We use the recently introduced formalism of finite atomized\nsemilattices to study the properties of the embeddings and their finite models.\nFor a problem embedded in a semilattice, we show that every solution has a\nmodel atomized by an irreducible subset of the non-redundant atoms of the\nfreest model of the embedding. We give examples of semantic embeddings that can\nbe used to find solutions for the N-Queen's completion, the Sudoku, and the\nHamiltonian Path problems.",
    "descriptor": "\nComments: 58 pages\n",
    "authors": [
      "Fernando Martin-Maroto",
      "Gonzalo G. de Polavieja"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Logic in Computer Science (cs.LO)",
      "Commutative Algebra (math.AC)",
      "Logic (math.LO)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2205.12618"
  },
  {
    "id": "arXiv:2205.12619",
    "title": "Location-free Human Pose Estimation",
    "abstract": "Human pose estimation (HPE) usually requires large-scale training data to\nreach high performance. However, it is rather time-consuming to collect\nhigh-quality and fine-grained annotations for human body. To alleviate this\nissue, we revisit HPE and propose a location-free framework without supervision\nof keypoint locations. We reformulate the regression-based HPE from the\nperspective of classification. Inspired by the CAM-based weakly-supervised\nobject localization, we observe that the coarse keypoint locations can be\nacquired through the part-aware CAMs but unsatisfactory due to the gap between\nthe fine-grained HPE and the object-level localization. To this end, we propose\na customized transformer framework to mine the fine-grained representation of\nhuman context, equipped with the structural relation to capture subtle\ndifferences among keypoints. Concretely, we design a Multi-scale Spatial-guided\nContext Encoder to fully capture the global human context while focusing on the\npart-aware regions and a Relation-encoded Pose Prototype Generation module to\nencode the structural relations. All these works together for strengthening the\nweak supervision from image-level category labels on locations. Our model\nachieves competitive performance on three datasets when only supervised at a\ncategory-level and importantly, it can achieve comparable results with\nfully-supervised methods with only 25\\% location labels on MS-COCO and MPII.",
    "descriptor": "\nComments: Beijing Jiaotong University, Tencent Toutu Lab\n",
    "authors": [
      "Xixia Xu",
      "Yingguo Gao",
      "Ke Yan",
      "Xue Lin",
      "Qi Zou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12619"
  },
  {
    "id": "arXiv:2205.12621",
    "title": "Unbiased and Efficient Sampling of Dependency Trees",
    "abstract": "Distributions over spanning trees are the most common way of computational\nmodeling of dependency syntax. However, most treebanks require that every valid\ndependency tree has a single edge coming out of the ROOT node, a constraint\nthat is not part of the definition of spanning trees. For this reason all\nstandard inference algorithms for spanning trees are sub-optimal for modeling\ndependency trees.\nZmigrod et al. (2021b) have recently proposed algorithms for sampling with\nand without replacement from the single-root dependency tree distribution. In\nthis paper we show that their fastest algorithm for sampling with replacement,\nWilson-RC, is in fact producing biased samples and we provide two alternatives\nthat are unbiased. Additionally, we propose two algorithms (one incremental,\none parallel) that reduce the asymptotic runtime of their algorithm for\nsampling $k$ trees without replacement to $\\mathcal{O}(kn^3)$. These algorithms\nare both asymptotically and practically more efficient.",
    "descriptor": "\nComments: 15 pages, 4 algorithms, 6 figures\n",
    "authors": [
      "Milo\u0161 Stanojevi\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12621"
  },
  {
    "id": "arXiv:2205.12626",
    "title": "On non-detectability of non-computability and the degree of  non-computability of solutions of circuit and wave equations on digital  computers",
    "abstract": "It is known that there exist mathematical problems of practical relevance\nwhich cannot be computed on a Turing machine. An important example is the\ncalculation of the first derivative of continuously differentiable functions.\nThis paper precisely classifies the non-computability of the first derivative,\nand of the maximum-norm of the first derivative in the Zheng-Weihrauch\nhierarchy. Based on this classification, the paper investigates whether it is\npossible that a Turing machine detects this non-computability of the first\nderivative by observing the data of the problem, and whether it is possible to\ndetect upper bounds for the peak value of the first derivative of continuously\ndifferentiable functions. So from a practical point of view, the question is\nwhether it is possible to implement an exit-flag functionality for observing\nnon-computability of the first derivative. This paper even studies two\ndifferent types of exit-flag functionality. A strong one, where the Turing\nmachine always has to stop, and a weak one, where the Turing machine stops if\nand only if the input lies within the corresponding set of interest. It will be\nshown that non-computability of the first derivative is not detectable by a\nTuring machine for two concrete examples, namely for the problem of computing\nthe input--output behavior of simple analog circuits and for solutions of the\nthree-dimensional wave equation. In addition, it is shown that it is even\nimpossible to detect an upper bound for the maximum norm of the first\nderivative. In particular, it is shown that all three problems are not even\nsemidecidable. Finally, we briefly discuss implications of these results for\nanalog and quantum computing.",
    "descriptor": "\nComments: To appear in IEEE Trans. Inf. Theory. This paper influenced the article of F. Fitzek and H. Boche \"Metaverse at the campfire of the future\" in Germany's major newspaper, the Frankfurter Allgemeine Zeitung (FAZ), this https URL Technological challenges for the design of the Metaverse are discussed in this article\n",
    "authors": [
      "Holger Boche",
      "Volker Pohl"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12626"
  },
  {
    "id": "arXiv:2205.12627",
    "title": "Primitive3D: 3D Object Dataset Synthesis from Randomly Assembled  Primitives",
    "abstract": "Numerous advancements in deep learning can be attributed to the access to\nlarge-scale and well-annotated datasets. However, such a dataset is\nprohibitively expensive in 3D computer vision due to the substantial collection\ncost. To alleviate this issue, we propose a cost-effective method for\nautomatically generating a large amount of 3D objects with annotations. In\nparticular, we synthesize objects simply by assembling multiple random\nprimitives. These objects are thus auto-annotated with part labels originating\nfrom primitives. This allows us to perform multi-task learning by combining the\nsupervised segmentation with unsupervised reconstruction. Considering the large\noverhead of learning on the generated dataset, we further propose a dataset\ndistillation strategy to remove redundant samples regarding a target dataset.\nWe conduct extensive experiments for the downstream tasks of 3D object\nclassification. The results indicate that our dataset, together with multi-task\npretraining on its annotations, achieves the best performance compared to other\ncommonly used datasets. Further study suggests that our strategy can improve\nthe model performance by pretraining and fine-tuning scheme, especially for the\ndataset with a small scale. In addition, pretraining with the proposed dataset\ndistillation method can save 86\\% of the pretraining time with negligible\nperformance degradation. We expect that our attempt provides a new data-centric\nperspective for training 3D deep models.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Xinke Li",
      "Henghui Ding",
      "Zekun Tong",
      "Yuwei Wu",
      "Yeow Meng Chee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12627"
  },
  {
    "id": "arXiv:2205.12628",
    "title": "Are Large Pre-Trained Language Models Leaking Your Personal Information?",
    "abstract": "Large Pre-Trained Language Models (PLMs) have facilitated and dominated many\nNLP tasks in recent years. However, despite the great success of PLMs, there\nare also privacy concerns brought with PLMs. For example, recent studies show\nthat PLMs memorize a lot of training data, including sensitive information,\nwhile the information may be leaked unintentionally and be utilized by\nmalicious attackers.\nIn this paper, we propose to measure whether PLMs are prone to leaking\npersonal information. Specifically, we attempt to query PLMs for email\naddresses with contexts of the email address or prompts containing the owner's\nname. We find that PLMs do leak personal information due to memorization.\nHowever, the risk of specific personal information being extracted by attackers\nis low because the models are weak at associating the personal information with\nits owner. We hope this work could help the community to better understand the\nprivacy risk of PLMs and bring new insights to make PLMs safe.",
    "descriptor": "",
    "authors": [
      "Jie Huang",
      "Hanyin Shao",
      "Kevin Chen-Chuan Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12628"
  },
  {
    "id": "arXiv:2205.12630",
    "title": "Multimodal Knowledge Alignment with Reinforcement Learning",
    "abstract": "Large language models readily adapt to novel settings, even without\ntask-specific training data. Can their zero-shot capacity be extended to\nmultimodal inputs? In this work, we propose ESPER which extends language-only\nzero-shot models to unseen multimodal tasks, like image and audio captioning.\nOur key novelty is to use reinforcement learning to align multimodal inputs to\nlanguage model generations without direct supervision: for example, in the\nimage case our reward optimization relies only on cosine similarity derived\nfrom CLIP, and thus requires no additional explicitly paired (image, caption)\ndata. Because the parameters of the language model are left unchanged, the\nmodel maintains its capacity for zero-shot generalization. Experiments\ndemonstrate that ESPER outperforms baselines and prior work on a variety of\nzero-shot tasks; these include a new benchmark we collect+release, ESP dataset,\nwhich tasks models with generating several diversely-styled captions for each\nimage.",
    "descriptor": "",
    "authors": [
      "Youngjae Yu",
      "Jiwan Chung",
      "Heeseung Yun",
      "Jack Hessel",
      "JaeSung Park",
      "Ximing Lu",
      "Prithviraj Ammanabrolu",
      "Rowan Zellers",
      "Ronan Le Bras",
      "Gunhee Kim",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12630"
  },
  {
    "id": "arXiv:2205.12633",
    "title": "NTIRE 2022 Challenge on High Dynamic Range Imaging: Methods and Results",
    "abstract": "This paper reviews the challenge on constrained high dynamic range (HDR)\nimaging that was part of the New Trends in Image Restoration and Enhancement\n(NTIRE) workshop, held in conjunction with CVPR 2022. This manuscript focuses\non the competition set-up, datasets, the proposed methods and their results.\nThe challenge aims at estimating an HDR image from multiple respective low\ndynamic range (LDR) observations, which might suffer from under- or\nover-exposed regions and different sources of noise. The challenge is composed\nof two tracks with an emphasis on fidelity and complexity constraints: In Track\n1, participants are asked to optimize objective fidelity scores while imposing\na low-complexity constraint (i.e. solutions can not exceed a given number of\noperations). In Track 2, participants are asked to minimize the complexity of\ntheir solutions while imposing a constraint on fidelity scores (i.e. solutions\nare required to obtain a higher fidelity score than the prescribed baseline).\nBoth tracks use the same data and metrics: Fidelity is measured by means of\nPSNR with respect to a ground-truth HDR image (computed both directly and with\na canonical tonemapping operation), while complexity metrics include the number\nof Multiply-Accumulate (MAC) operations and runtime (in seconds).",
    "descriptor": "\nComments: CVPR Workshops 2022. 15 pages, 21 figures, 2 tables\n",
    "authors": [
      "Eduardo P\u00e9rez-Pellitero",
      "Sibi Catley-Chandar",
      "Richard Shaw",
      "Ale\u0161 Leonardis",
      "Radu Timofte",
      "Zexin Zhang",
      "Cen Liu",
      "Yunbo Peng",
      "Yue Lin",
      "Gaocheng Yu",
      "Jin Zhang",
      "Zhe Ma",
      "Hongbin Wang",
      "Xiangyu Chen",
      "Xintao Wang",
      "Haiwei Wu",
      "Lin Liu",
      "Chao Dong",
      "Jiantao Zhou",
      "Qingsen Yan",
      "Song Zhang",
      "Weiye Chen",
      "Yuhang Liu",
      "Zhen Zhang",
      "Yanning Zhang",
      "Javen Qinfeng Shi",
      "Dong Gong",
      "Dan Zhu",
      "Mengdi Sun",
      "Guannan Chen",
      "Yang Hu",
      "Haowei Li",
      "Baozhu Zou",
      "Zhen Liu",
      "Wenjie Lin",
      "Ting Jiang",
      "Chengzhi Jiang",
      "Xinpeng Li",
      "Mingyan Han",
      "Haoqiang Fan",
      "Jian Sun",
      "Shuaicheng Liu",
      "Juan Mar\u00edn-Vega",
      "Michael Sloth",
      "Peter Schneider-Kamp",
      "Richard R\u00f6ttger",
      "Chunyang Li",
      "Long Bao",
      "Gang He",
      "Ziyao Xu",
      "Li Xu",
      "Gen Zhan",
      "Ming Sun",
      "Xing Wen",
      "Junlin Li",
      "Jinjing Li",
      "Chenghua Li",
      "Ruipeng Gang",
      "Fangya Li",
      "Chenming Liu",
      "Shuang Feng",
      "Fei Lei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12633"
  },
  {
    "id": "arXiv:2205.12634",
    "title": "Real-Time Video Deblurring via Lightweight Motion Compensation",
    "abstract": "While motion compensation greatly improves video deblurring quality,\nseparately performing motion compensation and video deblurring demands huge\ncomputational overhead. This paper proposes a real-time video deblurring\nframework consisting of a lightweight multi-task unit that supports both video\ndeblurring and motion compensation in an efficient way. The multi-task unit is\nspecifically designed to handle large portions of the two tasks using a single\nshared network, and consists of a multi-task detail network and simple networks\nfor deblurring and motion compensation. The multi-task unit minimizes the cost\nof incorporating motion compensation into video deblurring and enables\nreal-time deblurring. Moreover, by stacking multiple multi-task units, our\nframework provides flexible control between the cost and deblurring quality. We\nexperimentally validate the state-of-the-art deblurring quality of our\napproach, which runs at a much faster speed compared to previous methods, and\nshow practical real-time performance (30.99dB@30fps measured in the DVD\ndataset).",
    "descriptor": "",
    "authors": [
      "Hyeongseok Son",
      "Junyong Lee",
      "Sunghyun Cho",
      "Seungyong Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12634"
  },
  {
    "id": "arXiv:2205.12635",
    "title": "MoCoViT: Mobile Convolutional Vision Transformer",
    "abstract": "Recently, Transformer networks have achieved impressive results on a variety\nof vision tasks. However, most of them are computationally expensive and not\nsuitable for real-world mobile applications. In this work, we present Mobile\nConvolutional Vision Transformer (MoCoViT), which improves in performance and\nefficiency by introducing transformer into mobile convolutional networks to\nleverage the benefits of both architectures. Different from recent works on\nvision transformer, the mobile transformer block in MoCoViT is carefully\ndesigned for mobile devices and is very lightweight, accomplished through two\nprimary modifications: the Mobile Self-Attention (MoSA) module and the Mobile\nFeed Forward Network (MoFFN). MoSA simplifies the calculation of the attention\nmap through Branch Sharing scheme while MoFFN serves as a mobile version of MLP\nin the transformer, further reducing the computation by a large margin.\nComprehensive experiments verify that our proposed MoCoViT family outperform\nstate-of-the-art portable CNNs and transformer neural architectures on various\nvision tasks. On ImageNet classification, it achieves 74.5% top-1 accuracy at\n147M FLOPs, gaining 1.2% over MobileNetV3 with less computations. And on the\nCOCO object detection task, MoCoViT outperforms GhostNet by 2.1 AP in RetinaNet\nframework.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2108.05895, arXiv:1911.11907 by other authors\n",
    "authors": [
      "Hailong Ma",
      "Xin Xia",
      "Xing Wang",
      "Xuefeng Xiao",
      "Jiashi Li",
      "Min Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12635"
  },
  {
    "id": "arXiv:2205.12636",
    "title": "Exploring industrial safety knowledge via Zipf law",
    "abstract": "The hazard and operability analysis (HAZOP) report contains precious\nindustrial safety knowledge (ISK) with expert experience and process nature,\nwhich is of great significance to the development of industrial intelligence.\nSubject to the attributes of ISK, existing researches mine them through\nsequence labeling in deep learning. Yet, there are two thorny issues: (1)\nUneven distribution of ISK and (2) Consistent importance of ISK: for safety\nreview. In this study, we propose a novel generative mining strategy called\nCRGM to explore ISK. Inspired Zipf law in linguistics, CRGM consists of\ncommon-rare discriminator, induction-extension generator and ISK extractor.\nFirstly, the common-rare discriminator divides HAZOP descriptions into common\nwords and rare words, and obtains the common description and the rare\ndescription, where the latter contains more industrial substances. Then, they\nare operated by the induction-extension generator in the way of deep text\ngeneration, the common description is induced and the rare description is\nextended, the material knowledge and the equipment knowledge can be enriched.\nFinally, the ISK extractor processes the material knowledge and equipment\nknowledge from the generated description through the rule template method, the\nadditional ISK is regarded as the supplement of the training set to train the\nproposed sequence labeling model. We conduct multiple evaluation experiments on\ntwo industrial safety datasets. The results show that CRGM has promising and\ngratifying aptitudes, greatly improves the performance of the model, and is\nefficient and generalized. Our sequence labeling model also shows the expected\nperformance, which is better than the existing research. Our research provides\na new perspective for exploring ISK, we hope it can contribute support for the\nintelligent progress of industrial safety.",
    "descriptor": "\nComments: Expert Systems with Applications\n",
    "authors": [
      "Zhenhua Wang",
      "Ming Ren",
      "Dong Gao",
      "Zhuang Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12636"
  },
  {
    "id": "arXiv:2205.12639",
    "title": "TreEnhance: An Automatic Tree-Search Based Method for Low-Light Image  Enhancement",
    "abstract": "In this paper we present TreEnhance, an automatic method for low-light image\nenhancement capable of improving the quality of digital images. The method\ncombines tree search theory, and in particular the Monte Carlo Tree Search\n(MCTS) algorithm, with deep reinforcement learning. Given as input a low-light\nimage, TreEnhance produces as output its enhanced version together with the\nsequence of image editing operations used to obtain it. The method repeatedly\nalternates two main phases. In the generation phase a modified version of MCTS\nexplores the space of image editing operations and selects the most promising\nsequence. In the optimization phase the parameters of a neural network,\nimplementing the enhancement policy, are updated. After training, two different\ninference solutions are proposed for the enhancement of new images: one is\nbased on MCTS and is more accurate but more time and memory consuming; the\nother directly applies the learned policy and is faster but slightly less\nprecise. Unlike other methods from the state of the art, TreEnhance does not\npose any constraint on the image resolution and can be used in a variety of\nscenarios with minimal tuning. We tested the method on two datasets: the\nLow-Light dataset and the Adobe Five-K dataset obtaining good results from both\na qualitative and a quantitative point of view.",
    "descriptor": "",
    "authors": [
      "Marco Cotogni",
      "Claudio Cusano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.12639"
  },
  {
    "id": "arXiv:2205.12640",
    "title": "Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating  Spurious Correlations in Entity Typing",
    "abstract": "The entity typing task aims at predicting one or more words or phrases that\ndescribe the type(s) of a specific mention in a sentence. Due to shortcuts from\nsurface patterns to annotated entity labels and biased training, existing\nentity typing models are subject to the problem of spurious correlations. To\ncomprehensively investigate the faithfulness and reliability of entity typing\nmethods, we first systematically define distinct kinds of model biases that are\nreflected mainly from spurious correlations. Particularly, we identify six\ntypes of existing model biases, including mention-context bias, lexical\noverlapping bias, named entity bias, pronoun bias, dependency bias, and\novergeneralization bias. To mitigate these model biases, we then introduce a\ncounterfactual data augmentation method. By augmenting the original training\nset with their bias-free counterparts, models are forced to fully comprehend\nthe sentences and discover the fundamental cues for entity typing, rather than\nrelying on spurious correlations for shortcuts. Experimental results on the\nUFET dataset show that our counterfactual data augmentation approach helps\nimprove generalization of different entity typing models with consistently\nbetter performance on both in- and out-of-distribution test sets.",
    "descriptor": "\nComments: work in progress\n",
    "authors": [
      "Nan Xu",
      "Fei Wang",
      "Bangzheng Li",
      "Mingtao Dong",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12640"
  },
  {
    "id": "arXiv:2205.12643",
    "title": "Asking the Right Questions in Low Resource Template Extraction",
    "abstract": "Information Extraction (IE) researchers are mapping tasks to Question\nAnswering (QA) in order to leverage existing large QA resources, and thereby\nimprove data efficiency. Especially in template extraction (TE), mapping an\nontology to a set of questions can be more time-efficient than collecting\nlabeled examples. We ask whether end users of TE systems can design these\nquestions, and whether it is beneficial to involve an NLP practitioner in the\nprocess. We compare questions to other ways of phrasing natural language\nprompts for TE. We propose a novel model to perform TE with prompts, and find\nit benefits from questions over other styles of prompts, and that they do not\nrequire an NLP background to author.",
    "descriptor": "",
    "authors": [
      "Nils Holzenberger",
      "Yunmo Chen",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12643"
  },
  {
    "id": "arXiv:2205.12644",
    "title": "LingMess: Linguistically Informed Multi Expert Scorers for Coreference  Resolution",
    "abstract": "While coreference resolution typically involves various linguistic\nchallenges, recent models are based on a single pairwise scorer for all types\nof pairs. We present LingMess, a new coreference model that defines different\ncategories of coreference cases and optimize multiple pairwise scorers, where\neach scorer learns a specific set of linguistic challenges. Our model\nsubstantially improves pairwise scores for most categories and outperforms\ncluster-level performance on Ontonotes. Our model is available in\nhttps://github.com/shon-otmazgin/lingmess-coref",
    "descriptor": "",
    "authors": [
      "Shon Otmazgin",
      "Arie Cattan",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12644"
  },
  {
    "id": "arXiv:2205.12646",
    "title": "UniInst: Unique Representation for End-to-End Instance Segmentation",
    "abstract": "Existing instance segmentation methods have achieved impressive performance\nbut still suffer from a common dilemma: redundant representations (e.g.,\nmultiple boxes, grids, and anchor points) are inferred for one instance, which\nleads to multiple duplicated predictions. Thus, mainstream methods usually rely\non a hand-designed non-maximum suppression (NMS) post-processing to select the\noptimal prediction result, which hinders end-to-end training. To address this\nissue, we propose a box-free and NMS-free end-to-end instance segmentation\nframework, termed UniInst, that yields only one unique representation for each\ninstance. Specifically, we design an instance-aware one-to-one assignment\nscheme, namely Only Yield One Representation (OYOR), which dynamically assigns\none unique representation to one instance according to the matching quality\nbetween predictions and ground truths. Then, a novel prediction re-ranking\nstrategy is elegantly integrated into the framework to address the misalignment\nbetween the classification score and the mask quality, enabling the learned\nrepresentation to be more discriminative. With these techniques, our UniInst,\nthe first FCN-based end-to-end instance segmentation framework, achieves\ncompetitive performance, e.g., 39.0 mask AP with ResNet-50-FPN and 40.2 mask AP\nwith ResNet-101-FPN, against mainstream methods on the COCO benchmark.\nMoreover, the proposed instance-aware method is robust to occlusion scenes,\noutperforming common baselines by remarkable mask AP on the heavily-occluded\nOCHuman benchmark. Our codes will be available upon publication.",
    "descriptor": "\nComments: This work is in the revision phase of the journal Neurocomputing. Codes will be available upon publication\n",
    "authors": [
      "Yimin Ou",
      "Rui Yang",
      "Lufan Ma",
      "Yong Liu",
      "Jiangpeng Yan",
      "Shang Xu",
      "Chengjie Wang",
      "Xiu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12646"
  },
  {
    "id": "arXiv:2205.12647",
    "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
    "abstract": "In this paper, we explore the challenging problem of performing a generative\ntask (i.e., summarization) in a target language when labeled data is only\navailable in English. We assume a strict setting with no access to parallel\ndata or machine translation. Prior work has shown, and we confirm, that\nstandard transfer learning techniques struggle in this setting, as a generative\nmultilingual model fine-tuned purely on English catastrophically forgets how to\ngenerate non-English. Given the recent rise of parameter-efficient adaptation\ntechniques (e.g., prompt tuning), we conduct the first investigation into how\nwell these methods can overcome catastrophic forgetting to enable zero-shot\ncross-lingual generation. We find that parameter-efficient adaptation provides\ngains over standard fine-tuning when transferring between less-related\nlanguages, e.g., from English to Thai. However, a significant gap still remains\nbetween these methods and fully-supervised baselines. To improve cross-lingual\ntransfer further, we explore three approaches: (1) mixing in unlabeled\nmultilingual data, (2) pre-training prompts on target language data, and (3)\nexplicitly factoring prompts into recombinable language and task components.\nOur methods can provide further quality gains, suggesting that robust zero-shot\ncross-lingual generation is within reach.",
    "descriptor": "",
    "authors": [
      "Tu Vu",
      "Aditya Barua",
      "Brian Lester",
      "Daniel Cer",
      "Mohit Iyyer",
      "Noah Constant"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12647"
  },
  {
    "id": "arXiv:2205.12648",
    "title": "Fast Inference and Transfer of Compositional Task Structures for  Few-shot Task Generalization",
    "abstract": "We tackle real-world problems with complex structures beyond the pixel-based\ngame or simulator. We formulate it as a few-shot reinforcement learning problem\nwhere a task is characterized by a subtask graph that defines a set of subtasks\nand their dependencies that are unknown to the agent. Different from the\nprevious meta-rl methods trying to directly infer the unstructured task\nembedding, our multi-task subtask graph inferencer (MTSGI) first infers the\ncommon high-level task structure in terms of the subtask graph from the\ntraining tasks, and use it as a prior to improve the task inference in testing.\nOur experiment results on 2D grid-world and complex web navigation domains show\nthat the proposed method can learn and leverage the common underlying structure\nof the tasks for faster adaptation to the unseen tasks than various existing\nalgorithms such as meta reinforcement learning, hierarchical reinforcement\nlearning, and other heuristic agents.",
    "descriptor": "\nComments: Accepted to UAI 2022 as an oral presentation\n",
    "authors": [
      "Sungryull Sohn",
      "Hyunjae Woo",
      "Jongwook Choi",
      "lyubing qiang",
      "Izzeddin Gur",
      "Aleksandra Faust",
      "Honglak Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12648"
  },
  {
    "id": "arXiv:2205.12649",
    "title": "Investigating Lexical Replacements for Arabic-English Code-Switched Data  Augmentation",
    "abstract": "Code-switching (CS) poses several challenges to NLP tasks, where data\nsparsity is a main problem hindering the development of CS NLP systems. In this\npaper, we investigate data augmentation techniques for synthesizing Dialectal\nArabic-English CS text. We perform lexical replacements using parallel corpora\nand alignments where CS points are either randomly chosen or learnt using a\nsequence-to-sequence model. We evaluate the effectiveness of data augmentation\non language modeling (LM), machine translation (MT), and automatic speech\nrecognition (ASR) tasks. Results show that in the case of using 1-1 alignments,\nusing trained predictive models produces more natural CS sentences, as\nreflected in perplexity. By relying on grow-diag-final alignments, we then\nidentify aligning segments and perform replacements accordingly. By replacing\nsegments instead of words, the quality of synthesized data is greatly improved.\nWith this improvement, random-based approach outperforms using trained\npredictive models on all extrinsic tasks. Our best models achieve 33.6%\nimprovement in perplexity, +3.2-5.6 BLEU points on MT task, and 7% relative\nimprovement on WER for ASR task. We also contribute in filling the gap in\nresources by collecting and publishing the first Arabic English CS-English\nparallel corpus.",
    "descriptor": "",
    "authors": [
      "Injy Hamed",
      "Nizar Habash",
      "Slim Abdennadher",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12649"
  },
  {
    "id": "arXiv:2205.12650",
    "title": "LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA",
    "abstract": "We study unsupervised multi-hop reranking for multi-hop QA (MQA) with\nopen-domain questions. Since MQA requires piecing information from multiple\ndocuments, the main challenge thus resides in retrieving and reranking chains\nof passages that support the reasoning process. Our approach relies on LargE\nmodels with Prompt-Utilizing reranking Strategy (LEPUS): we construct an\ninstruction-like prompt based on a candidate document path and compute a\nrelevance score of the path as the probability of generating a given question,\naccording to a pre-trained language model. Though unsupervised, LEPUS yields\ncompetitive reranking performance against state-of-the-art methods that are\ntrained on thousands of examples. Adding a small number of samples (e.g., $2$),\nwe demonstrate further performance gain using in-context learning. Finally, we\nshow that when integrated with a reader module, LEPUS can obtain competitive\nmulti-hop QA performance, e.g., outperforming fully-supervised QA systems.\nCode will be released at https://github.com/mukhal/LEPUS",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Muhammad Khalifa",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Lu Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12650"
  },
  {
    "id": "arXiv:2205.12654",
    "title": "Bitext Mining Using Distilled Sentence Representations for Low-Resource  Languages",
    "abstract": "Scaling multilingual representation learning beyond the hundred most frequent\nlanguages is challenging, in particular to cover the long tail of low-resource\nlanguages. A promising approach has been to train one-for-all multilingual\nmodels capable of cross-lingual transfer, but these models often suffer from\ninsufficient capacity and interference between unrelated languages. Instead, we\nmove away from this approach and focus on training multiple language (family)\nspecific representations, but most prominently enable all languages to still be\nencoded in the same representational space. To achieve this, we focus on\nteacher-student training, allowing all encoders to be mutually compatible for\nbitext mining, and enabling fast learning of new languages. We introduce a new\nteacher-student training scheme which combines supervised and self-supervised\ntraining, allowing encoders to take advantage of monolingual training data,\nwhich is valuable in the low-resource setting.\nOur approach significantly outperforms the original LASER encoder. We study\nvery low-resource languages and handle 50 African languages, many of which are\nnot covered by any other model. For these languages, we train sentence\nencoders, mine bitexts, and validate the bitexts by training NMT systems.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Kevin Heffernan",
      "Onur \u00c7elebi",
      "Holger Schwenk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12654"
  },
  {
    "id": "arXiv:2205.12656",
    "title": "Optimizing UAV Recharge Scheduling for Heterogeneous and Persistent  Aerial Service",
    "abstract": "The adoption of UAVs in communication networks is becoming reality thanks to\nthe deployment of advanced solutions for connecting UAVs and using them as\ncommunication relays. However, the use of UAVs introduces novel energy\nconstraints and scheduling challenges in the dynamic management of network\ndevices, due to the need to call back and recharge, or substitute, UAVs that\nrun out of energy. In this paper, we design UAV recharging schemes under\nrealistic assumptions on limited flight times and time consuming charging\noperations. Such schemes are designed to minimize the size of the fleet to be\ndevoted to a persistent service of a set of aerial locations, hence its cost.\nWe consider a fleet of homogeneous UAVs both under homogeneous and\nheterogeneous service topologies. For UAVs serving aerial locations with\nhomogeneous distances to a recharge station, we design a simple scheduling,\nthat we name HORR, which we prove to be feasible and optimal, in the sense that\nit uses the minimum possible number of UAVs to guarantee the coverage of the\naerial service locations. For the case of non-evenly distributed aerial\nlocations, we demonstrate that the problem becomes NP-hard, and design a\nlightweight recharging scheduling scheme, PHERR, that extends the operation of\nHORR to the heterogeneous case, leveraging the partitioning of the set of\nservice locations. We show that PHERR is near-optimal because it approaches the\nperformance limits identified through a lower bound that we formulate on the\ntotal fleet size.",
    "descriptor": "",
    "authors": [
      "Edgar Arribas",
      "Vicent Cholvi",
      "Vincenzo Mancuso"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12656"
  },
  {
    "id": "arXiv:2205.12660",
    "title": "MAPLE-X: Latency Prediction with Explicit Microprocessor Prior Knowledge",
    "abstract": "Deep neural network (DNN) latency characterization is a time-consuming\nprocess and adds significant cost to Neural Architecture Search (NAS) processes\nwhen searching for efficient convolutional neural networks for embedded vision\napplications. DNN Latency is a hardware dependent metric and requires direct\nmeasurement or inference on target hardware. A recently introduced latency\nestimation technique known as MAPLE predicts DNN execution time on previously\nunseen hardware devices by using hardware performance counters. Leveraging\nthese hardware counters in the form of an implicit prior, MAPLE achieves\nstate-of-the-art performance in latency prediction. Here, we propose MAPLE-X\nwhich extends MAPLE by incorporating explicit prior knowledge of hardware\ndevices and DNN architecture latency to better account for model stability and\nrobustness. First, by identifying DNN architectures that exhibit a similar\nlatency to each other, we can generate multiple virtual examples to\nsignificantly improve the accuracy over MAPLE. Secondly, the hardware\nspecifications are used to determine the similarity between training and test\nhardware to emphasize training samples captured from comparable devices\n(domains) and encourages improved domain alignment. Experimental results using\na convolution neural network NAS benchmark across different types of devices,\nincluding an Intel processor that is now used for embedded vision applications,\ndemonstrate a 5% improvement over MAPLE and 9% over HELP. Furthermore, we\ninclude ablation studies to independently assess the benefits of virtual\nexamples and hardware-based sample importance.",
    "descriptor": "\nComments: 6 pages, 4 figures\n",
    "authors": [
      "Saad Abbasi",
      "Alexander Wong",
      "Mohammad Javad Shafiee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12660"
  },
  {
    "id": "arXiv:2205.12661",
    "title": "Implicit Function Theorem: Estimates on the size of the domain",
    "abstract": "In this article, we provide explicit estimates on the domain on which the\nImplicit Function Theorem (ImFT) and the Inverse Function Theorem (IFT) are\nvalid. For maps that are continuously differentiable upto second order, the\nestimates depend upon the magnitude of the first-order derivatives evaluated at\nthe point of interest and a bound on the second-order derivatives over a region\nof interest. One of the key contributions of this article is to come up with\nestimates that require minimal numerical computation. In particular, we do not\nperform any optimization to come up with these estimates. The derived bounds\nare then applied to compute the robustness margin for Quadratic Problems and\nthen utilize these bounds to compute the allowable power variations to ensure\nstable operations of the Power System Networks. Another key application of\nthese bounds is in estimating the domain of feedback linearization for\ndiscrete-time control systems which are to be presented in a companion paper\nalong with results on the feedback linearizability of the numerical integration\ntechniques.",
    "descriptor": "\nComments: 25 pages, 5 figures\n",
    "authors": [
      "Ashutossh Jindal",
      "Debasish Chatterjee",
      "Ravi Banavar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12661"
  },
  {
    "id": "arXiv:2205.12662",
    "title": "DialogZoo: Large-Scale Dialog-Oriented Task Learning",
    "abstract": "Building unified conversational agents has been a long-standing goal of the\ndialogue research community. Most previous works only focus on a subset of\nvarious dialogue tasks. In this work, we aim to build a unified foundation\nmodel which can solve massive diverse dialogue tasks. To achieve this goal, we\nfirst collect a large-scale well-labeled dialogue dataset from 73 publicly\navailable datasets. In addition to this dataset, we further propose two\ndialogue-oriented self-supervised tasks, and finally use the mixture of\nsupervised and self-supervised datasets to train our foundation model. The\nsupervised examples make the model learn task-specific skills, while the\nself-supervised examples make the model learn more general skills. We evaluate\nour model on various downstream dialogue tasks. The experimental results show\nthat our method not only improves the ability of dialogue generation and\nknowledge distillation, but also the representation ability of models.",
    "descriptor": "\nComments: Work in Progress\n",
    "authors": [
      "Zhi Chen",
      "Jijia Bao",
      "Lu Chen",
      "Yuncong Liu",
      "Da Ma",
      "Bei Chen",
      "Mengyue Wu",
      "Su Zhu",
      "Jian-Guang Lou",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12662"
  },
  {
    "id": "arXiv:2205.12665",
    "title": "QAMPARI: : An Open-domain Question Answering Benchmark for Questions  with Many Answers from Multiple Paragraphs",
    "abstract": "Existing benchmarks for open-domain question answering (ODQA) typically focus\non questions whose answers can be extracted from a single paragraph. By\ncontrast, many natural questions, such as \"What players were drafted by the\nBrooklyn Nets?\" have a list of answers. Answering such questions requires\nretrieving and reading from many passages, in a large corpus. We introduce\nQAMPARI, an ODQA benchmark, where question answers are lists of entities,\nspread across many paragraphs. We created QAMPARI by (a) generating questions\nwith multiple answers from Wikipedia's knowledge graph and tables, (b)\nautomatically pairing answers with supporting evidence in Wikipedia paragraphs,\nand (c) manually paraphrasing questions and validating each answer. We train\nODQA models from the retrieve-and-read family and find that QAMPARI is\nchallenging in terms of both passage retrieval and answer generation, reaching\nan F1 score of 26.6 at best. Our results highlight the need for developing ODQA\nmodels that handle a broad range of question types, including single and\nmulti-answer questions.",
    "descriptor": "",
    "authors": [
      "Samuel Joseph Amouyal Ohad Rubin",
      "Ori Yoran",
      "Tomer Wolfson",
      "Jonathan Herzig",
      "Jonathan Berant"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12665"
  },
  {
    "id": "arXiv:2205.12672",
    "title": "Discovering Language-neutral Sub-networks in Multilingual Language  Models",
    "abstract": "Multilingual pre-trained language models perform remarkably well on\ncross-lingual transfer for downstream tasks. Despite their impressive\nperformance, our understanding of their language neutrality (i.e., the extent\nto which they use shared representations to encode similar phenomena across\nlanguages) and its role in achieving such performance remain open questions. In\nthis work, we conceptualize language neutrality of multilingual models as a\nfunction of the overlap between language-encoding sub-networks of these models.\nUsing mBERT as a foundation, we employ the lottery ticket hypothesis to\ndiscover sub-networks that are individually optimized for various languages and\ntasks. Using three distinct tasks and eleven typologically-diverse languages in\nour evaluation, we show that the sub-networks found for different languages are\nin fact quite similar, supporting the idea that mBERT jointly encodes multiple\nlanguages in shared parameters. We conclude that mBERT is comprised of a\nlanguage-neutral sub-network shared among many languages, along with multiple\nancillary language-specific sub-networks, with the former playing a more\nprominent role in mBERT's impressive cross-lingual performance.",
    "descriptor": "",
    "authors": [
      "Negar Foroutan",
      "Mohammadreza Banaei",
      "Remi Lebret",
      "Antoine Bosselut",
      "Karl Aberer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12672"
  },
  {
    "id": "arXiv:2205.12673",
    "title": "Improving Zero and Few-shot Generalization in Dialogue through  Instruction Tuning",
    "abstract": "Instruction tuning is an emergent paradigm in NLP wherein natural language\ninstructions are leveraged with language models to induce zero-shot performance\non unseen tasks. Instructions have been shown to enable good performance on\nunseen tasks and datasets in both large and small language models. Dialogue is\nan especially interesting area to explore instruction tuning because dialogue\nsystems perform multiple kinds of tasks related to language (e.g., natural\nlanguage understanding and generation, domain-specific interaction), yet\ninstruction tuning has not been systematically explored for dialogue-related\ntasks. We introduce InstructDial, an instruction tuning framework for dialogue,\nwhich consists of a repository of 48 diverse dialogue tasks in a unified\ntext-to-text format created from 59 openly available dialogue datasets. Next,\nwe explore cross-task generalization ability on models tuned on InstructDial\nacross diverse dialogue tasks. Our analysis reveals that InstructDial enables\ngood zero-shot performance on unseen datasets and tasks such as dialogue\nevaluation and intent detection, and even better performance in a few-shot\nsetting. To ensure that models adhere to instructions, we introduce novel\nmeta-tasks. We establish benchmark zero-shot and few-shot performance of models\ntrained using the proposed framework on multiple dialogue tasks.",
    "descriptor": "",
    "authors": [
      "Prakhar Gupta",
      "Cathy Jiao",
      "Yi-Ting Yeh",
      "Shikib Mehri",
      "Maxine Eskenazi",
      "Jeffrey P. Bigham"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12673"
  },
  {
    "id": "arXiv:2205.12674",
    "title": "Training Language Models with Memory Augmentation",
    "abstract": "Recent work has improved language models remarkably by equipping them with a\nnon-parametric memory component. However, most existing approaches only\nintroduce memories at testing time, or represent them using a separately\ntrained encoder -- resulting in sub-optimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining language models with memory augmentation. Our approach uses a training\nobjective that directly takes in-batch examples as accessible memory. We also\npresent new methods for memory construction and data batching, which are used\nfor adapting to different sets of memories -- local, long-term, and external\nmemory -- at testing time. We evaluate our approach on multiple language\nmodeling and machine translation benchmarks. We find that simply replacing the\nvanilla language modeling objective by ours greatly reduces the perplexity,\nwithout modifying the model architecture or incorporating extra context (e.g.,\n18.70 $\\to$ 17.76 on WikiText-103). We further augment language models with\nlong-range contexts and external knowledge and demonstrate significant gains\nover previous memory-augmented approaches.",
    "descriptor": "\nComments: Our code and models will be available at this https URL\n",
    "authors": [
      "Zexuan Zhong",
      "Tao Lei",
      "Danqi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12674"
  },
  {
    "id": "arXiv:2205.12676",
    "title": "Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A  Case Study for Indian Languages",
    "abstract": "In order for NLP technology to be widely applicable and useful, it needs to\nbe inclusive of users across the world's languages, equitable, i.e., not unduly\nbiased towards any particular language, and accessible to users, particularly\nin low-resource settings where compute constraints are common. In this paper,\nwe propose an evaluation paradigm that assesses NLP technologies across all\nthree dimensions, hence quantifying the diversity of users they can serve.\nWhile inclusion and accessibility have received attention in recent literature,\nequity is currently unexplored. We propose to address this gap using the Gini\ncoefficient, a well-established metric used for estimating societal wealth\ninequality. Using our paradigm, we highlight the distressed state of diversity\nof current technologies for Indian (IN) languages, motivated by their\nlinguistic diversity and large, varied speaker population. To improve upon\nthese metrics, we demonstrate the importance of region-specific choices in\nmodel building and dataset creation and also propose a novel approach to\noptimal resource allocation during fine-tuning. Finally, we discuss steps that\nmust be taken to mitigate these biases and call upon the community to\nincorporate our evaluation paradigm when building linguistically diverse\ntechnologies.",
    "descriptor": "",
    "authors": [
      "Simran Khanuja",
      "Sebastian Ruder",
      "Partha Talukdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12676"
  },
  {
    "id": "arXiv:2205.12677",
    "title": "Language Anisotropic Cross-Lingual Model Editing",
    "abstract": "Pre-trained language models learn large amounts of knowledge from their\ntraining corpus, while the memorized facts could become outdated over a few\nyears. Model editing aims to make post-hoc updates on specific facts in a model\nwhile leaving irrelevant knowledge unchanged. However, existing work studies\nonly the monolingual scenario. In this paper, we focus on cross-lingual model\nediting. Firstly, we propose the definition and metrics of the cross-lingual\nmodel editing, where updates in a single language should take effect in the\nothers as well. Next, we propose a simple framework to convert a monolingual\nmodel editing approach to its cross-lingual variant using the parallel corpus.\nExperiments show that such an approach outperforms monolingual baselines by a\nlarge margin. Furthermore, we propose language anisotropic editing to improve\ncross-lingual editing by estimating parameter importance for each language.\nExperiments reveal that language anisotropic editing decreases the editing\nfailing rate by another $26\\%$ relatively.",
    "descriptor": "",
    "authors": [
      "Yang Xu",
      "Yutai Hou",
      "Wanxiang Che"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12677"
  },
  {
    "id": "arXiv:2205.12679",
    "title": "ZeroGen$^+$: Self-Guided High-Quality Data Generation in Efficient  Zero-Shot Learning",
    "abstract": "Nowadays, owing to the superior capacity of the large pre-trained language\nmodels (PLM), the PLM-based zero-shot learning has shown promising performances\non various natural language processing tasks. There are emerging interests in\nfurther exploring the zero-shot learning potential of PLMs. Among them, ZeroGen\nattempts to purely use PLM to generate data and train a tiny model without\nrelying on any task-specific annotation. Despite its remarkable results, we\nobserve that the synthesized data from PLM contains a significant portion of\nsamples with low quality, overfitting on such data greatly hampers the\nperformance of the trained model and makes it unreliable for deployment.Since\nno gold data is accessible in zero-shot scenario, it is hard to perform\nmodel/data selection to prevent overfitting to the low-quality data. To address\nthis problem, we propose a noise-robust bi-level re-weighting framework which\nis able to learn the per-sample weights measuring the data quality without\nrequiring any gold data. With the learnt weights, clean subsets of different\nsizes can then be sampled to train the task model. We theoretically and\nempirically verify our method is able to construct synthetic dataset with good\nquality. Our method yeilds a 7.1% relative improvement than ZeroGen on average\naccuracy across five different established text classification tasks.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Jiahui Gao",
      "Renjie Pi",
      "Yong Lin",
      "Hang Xu",
      "Jiacheng Ye",
      "Zhiyong Wu",
      "Xiaodan Liang",
      "Zhenguo Li",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12679"
  },
  {
    "id": "arXiv:2205.12680",
    "title": "Refining Query Representations for Dense Retrieval at Test Time",
    "abstract": "Dense retrieval uses a contrastive learning framework to learn dense\nrepresentations of queries and contexts. Trained encoders are directly used for\neach test query, but they often fail to accurately represent out-of-domain\nqueries. In this paper, we introduce a framework that refines instance-level\nquery representations at test time, with only the signals coming from the\nintermediate retrieval results. We optimize the query representation based on\nthe retrieval result similar to pseudo relevance feedback (PRF) in information\nretrieval. Specifically, we adopt a cross-encoder labeler to provide pseudo\nlabels over the retrieval result and iteratively refine the query\nrepresentation with a gradient descent method, treating each test query as a\nsingle data point to train on. Our theoretical analysis reveals that our\nframework can be viewed as a generalization of the classical Rocchio's\nalgorithm for PRF, which leads us to propose interesting variants of our\nmethod. We show that our test-time query refinement strategy improves the\nperformance of phrase retrieval (+8.1% Acc@1) and passage retrieval (+3.7%\nAcc@20) for open-domain QA with large improvements on out-of-domain queries.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Mujeen Sung",
      "Jungsoo Park",
      "Jaewoo Kang",
      "Danqi Chen",
      "Jinhyuk Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12680"
  },
  {
    "id": "arXiv:2205.12682",
    "title": "TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning  Questions over Tabular Data",
    "abstract": "Existing auto-regressive pre-trained language models (PLMs) like T5 and BART,\nhave been well applied to table question answering by UNIFIEDSKG and TAPEX,\nrespectively, and demonstrated state-of-the-art results on multiple benchmarks.\nHowever, auto-regressive PLMs are challenged by recent emerging numerical\nreasoning datasets, such as TAT-QA, due to the error-prone implicit\ncalculation. In this paper, we present TaCube, to pre-compute\naggregation/arithmetic results for the table in advance, so that they are handy\nand readily available for PLMs to answer numerical reasoning questions. TaCube\nsystematically and comprehensively covers a collection of computational\noperations over table segments. By simply concatenating TaCube to the input\nsequence of PLMs, it shows significant experimental effectiveness. TaCube\npromotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new\nstate-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube's\nimprovements on numerical reasoning cases are even more notable: on TAT-QA,\nTaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5%\non average, 36.6% on substraction, and 22.2% on division. We believe that\nTaCube is a general and portable pre-computation solution that can be\npotentially integrated to various numerical reasoning frameworks",
    "descriptor": "",
    "authors": [
      "Fan Zhou",
      "Mengkang Hu",
      "Haoyu Dong",
      "Zhoujun Cheng",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12682"
  },
  {
    "id": "arXiv:2205.12683",
    "title": "Rethinking Fano's Inequality in Ensemble Learning",
    "abstract": "We propose a fundamental theory on ensemble learning that evaluates a given\nensemble system by a well-grounded set of metrics. Previous studies used a\nvariant of Fano's inequality of information theory and derived a lower bound of\nthe classification error rate on the basis of the accuracy and diversity of\nmodels. We revisit the original Fano's inequality and argue that the studies\ndid not take into account the information lost when multiple model predictions\nare combined into a final prediction. To address this issue, we generalize the\nprevious theory to incorporate the information loss. Further, we empirically\nvalidate and demonstrate the proposed theory through extensive experiments on\nactual systems. The theory reveals the strengths and weaknesses of systems on\neach metric, which will push the theoretical understanding of ensemble learning\nand give us insights into designing systems.",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "Terufumi Morishita",
      "Gaku Morio",
      "Shota Horiguchi",
      "Hiroaki Ozaki",
      "Nobuo Nukaga"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12683"
  },
  {
    "id": "arXiv:2205.12685",
    "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label  Demonstrations",
    "abstract": "Despite recent explosion in research interests, in-context learning and the\nprecise impact of the quality of demonstrations remain elusive. While, based on\ncurrent literature, it is expected that in-context learning shares a similar\nmechanism to supervised learning, Min et al. (2022) recently reported that,\nsurprisingly, input-label correspondence is less important than other aspects\nof prompt demonstrations. Inspired by this counter-intuitive observation, we\nre-examine the importance of ground truth labels on in-context learning from\ndiverse and statistical points of view. With the aid of the newly introduced\nmetrics, i.e., Ground-truth Label Effect Ratio (GLER), demo-gain, and label\nsensitivity, we find that the impact of the correct input-label matching can\nvary according to different configurations. Expanding upon the previous key\nfinding on the role of demonstrations, the complementary and contrastive\nresults suggest that one might need to take more care when estimating the\nimpact of each component in in-context learning demonstrations.",
    "descriptor": "",
    "authors": [
      "Junyeob Kim",
      "Hyuhng Joon Kim",
      "Hyunsoo Cho",
      "Hwiyeol Jo",
      "Sang-Woo Lee",
      "Sang-goo Lee",
      "Kang Min Yoo",
      "Taeuk Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12685"
  },
  {
    "id": "arXiv:2205.12686",
    "title": "Deterministic Massively Parallel Algorithms for Ruling Sets",
    "abstract": "In this paper we present a deterministic $O(\\log\\log n)$-round algorithm for\nthe 2-ruling set problem in the Massively Parallel Computation model with\n$\\tilde{O}(n)$ memory; this algorithm also runs in $O(\\log\\log n)$ rounds in\nthe Congested Clique model. This is exponentially faster than the fastest known\ndeterministic 2-ruling set algorithm for these models, which is simply the\n$O(\\log \\Delta)$-round deterministic Maximal Independent Set algorithm due to\nCzumaj, Davies, and Parter (SPAA 2020). Our result is obtained by derandomizing\nthe 2-ruling set algorithm of Kothapalli and Pemmaraju (FSTTCS 2012).",
    "descriptor": "",
    "authors": [
      "Shreyas Pai",
      "Sriram V. Pemmaraju"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12686"
  },
  {
    "id": "arXiv:2205.12688",
    "title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents",
    "abstract": "Most existing dialogue systems fail to respond properly to potentially unsafe\nuser utterances by either ignoring or passively agreeing with them. To address\nthis issue, we introduce ProsocialDialog, the first large-scale multi-turn\ndialogue dataset to teach conversational agents to respond to problematic\ncontent following social norms. Covering diverse unethical, problematic,\nbiased, and toxic situations, ProsocialDialog contains responses that encourage\nprosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,\nRoTs). Created via a human-AI collaborative framework, ProsocialDialog consists\nof 58K dialogues, with 331K utterances, 160K RoTs, and 497K dialogue safety\nlabels accompanied by free-form rationales.\nWith this dataset, we introduce a dialogue safety detection module, Canary,\ncapable of generating RoTs given conversational context, and a\nsocially-informed dialogue agent, Prost. Empirical results show that Prost\ngenerates more socially acceptable dialogues compared to other state-of-the-art\nlanguage and dialogue models in both in-domain and out-of-domain settings.\nAdditionally, Canary effectively guides conversational agents and off-the-shelf\nlanguage models to generate significantly more prosocial responses. Our work\nhighlights the promise and importance of creating and steering conversational\nAI to be socially responsible.",
    "descriptor": "\nComments: 25 pages, 10 figures\n",
    "authors": [
      "Hyunwoo Kim",
      "Youngjae Yu",
      "Liwei Jiang",
      "Ximing Lu",
      "Daniel Khashabi",
      "Gunhee Kim",
      "Yejin Choi",
      "Maarten Sap"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12688"
  },
  {
    "id": "arXiv:2205.12689",
    "title": "Large Language Models are Zero-Shot Clinical Information Extractors",
    "abstract": "We show that large language models, such as GPT-3, perform well at zero-shot\ninformation extraction from clinical text despite not being trained\nspecifically for the clinical domain. We present several examples showing how\nto use these models as tools for the diverse tasks of (i) concept\ndisambiguation, (ii) evidence extraction, (iii) coreference resolution, and\n(iv) concept extraction, all on clinical text. The key to good performance is\nthe use of simple task-specific programs that map from the language model\noutputs to the label space of the task. We refer to these programs as\nresolvers, a generalization of the verbalizer, which defines a mapping between\noutput tokens and a discrete label space. We show in our examples that good\nresolvers share common components (e.g., \"safety checks\" that ensure the\nlanguage model outputs faithfully match the input data), and that the common\npatterns across tasks make resolvers lightweight and easy to create. To better\nevaluate these systems, we also introduce two new datasets for benchmarking\nzero-shot clinical information extraction based on manual relabeling of the\nCASI dataset (Moon et al., 2014) with labels for new tasks. On the clinical\nextraction tasks we studied, the GPT-3 + resolver systems significantly\noutperform existing zero- and few-shot baselines.",
    "descriptor": "",
    "authors": [
      "Monica Agrawal",
      "Stefan Hegselmann",
      "Hunter Lang",
      "Yoon Kim",
      "David Sontag"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12689"
  },
  {
    "id": "arXiv:2205.12691",
    "title": "Understanding Natural Language in Context",
    "abstract": "Recent years have seen an increasing number of applications that have a\nnatural language interface, either in the form of chatbots or via personal\nassistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana\n(Microsoft). To use these applications, a basic dialog between the robot and\nthe human is required.\nWhile this kind of dialog exists today mainly within \"static\" robots that do\nnot make any movement in the household space, the challenge of reasoning about\nthe information conveyed by the environment increases significantly when\ndealing with robots that can move and manipulate objects in our home\nenvironment.\nIn this paper, we focus on cognitive robots, which have some knowledge-based\nmodels of the world and operate by reasoning and planning with this model.\nThus, when the robot and the human communicate, there is already some formalism\nthey can use - the robot's knowledge representation formalism.\nOur goal in this research is to translate natural language utterances into\nthis robot's formalism, allowing much more complicated household tasks to be\ncompleted. We do so by combining off-the-shelf SOTA language models, planning\ntools, and the robot's knowledge-base for better communication. In addition, we\nanalyze different directive types and illustrate the contribution of the\nworld's context to the translation process.",
    "descriptor": "",
    "authors": [
      "Avichai Levy",
      "Erez Karpas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12691"
  },
  {
    "id": "arXiv:2205.12693",
    "title": "Contrastive Learning with Boosted Memorization",
    "abstract": "Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.",
    "descriptor": "",
    "authors": [
      "Zhihan Zhou",
      "Jiangchao Yao",
      "Yanfeng Wang",
      "Bo Han",
      "Ya Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12693"
  },
  {
    "id": "arXiv:2205.12694",
    "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More  Compressible Models",
    "abstract": "Model compression by way of parameter pruning, quantization, or distillation\nhas recently gained popularity as an approach for reducing the computational\nrequirements of modern deep neural network models for NLP. Pruning unnecessary\nparameters has emerged as a simple and effective method for compressing large\nmodels that is compatible with a wide variety of contemporary off-the-shelf\nhardware (unlike quantization), and that requires little additional training\n(unlike distillation). Pruning approaches typically take a large, accurate\nmodel as input, then attempt to discover a smaller subnetwork of that model\ncapable of achieving end-task accuracy comparable to the full model. Inspired\nby previous work suggesting a connection between simpler, more generalizable\nmodels and those that lie within flat basins in the loss landscape, we propose\nto directly optimize for flat minima while performing task-specific pruning,\nwhich we hypothesize should lead to simpler parameterizations and thus more\ncompressible models. In experiments combining sharpness-aware minimization with\nboth iterative magnitude pruning and structured pruning approaches, we show\nthat optimizing for flat minima consistently leads to greater compressibility\nof parameters compared to standard Adam optimization when fine-tuning BERT\nmodels, leading to higher rates of compression with little to no loss in\naccuracy on the GLUE classification benchmark.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Clara Na",
      "Sanket Vaibhav Mehta",
      "Emma Strubell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12694"
  },
  {
    "id": "arXiv:2205.12696",
    "title": "Revisiting DocRED -- Addressing the Overlooked False Negative Problem in  Relation Extraction",
    "abstract": "The DocRED dataset is one of the most popular and widely used benchmarks for\ndocument-level relation extraction (RE). It adopts a recommend-revise\nannotation scheme so as to have a large-scale annotated dataset. However, we\nfind that the annotation of DocRED is incomplete, i.e., the false negative\nsamples are prevalent. We analyze the causes and effects of the overwhelming\nfalse negative problem in the DocRED dataset. To address the shortcoming, we\nre-annotate 4,053 documents in the DocRED dataset by adding the missed relation\ntriples back to the original DocRED. We name our revised DocRED dataset\nRe-DocRED. We conduct extensive experiments with state-of-the-art neural models\non both datasets, and the experimental results show that the models trained and\nevaluated on our Re-DocRED achieve performance improvements of around 13 F1\npoints. Moreover, we propose different metrics to comprehensively evaluate the\ndocument-level RE task. We make our data publicly available at\nhttps://github.com/tonytan48/Re-DocRED.",
    "descriptor": "\nComments: 15 pages, 1 figure, 17 tables\n",
    "authors": [
      "Qingyu Tan",
      "Lu Xu",
      "Lidong Bing",
      "Hwee Tou Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12696"
  },
  {
    "id": "arXiv:2205.12697",
    "title": "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation",
    "abstract": "Logical table-to-text generation is a task that involves generating logically\nfaithful sentences from tables, which requires models to derive logical level\nfacts from table records via logical inference. It raises a new challenge on\nthe logical-level content planning of table-to-text models. However, directly\nlearning the logical inference knowledge from table-text pairs is very\ndifficult for neural models because of the ambiguity of natural language and\nthe scarcity of parallel data. Hence even large-scale pre-trained language\nmodels present low logical fidelity on logical table-to-text. In this work, we\npropose a PLOG (Pretrained Logical Form Generator) framework to improve the\ngeneration fidelity. Specifically, PLOG is first pretrained on a\ntable-to-logic-form generation (table-to-logic) task, then finetuned on\ndownstream table-to-text tasks. The formal definition of logical forms enables\nus to collect large amount of accurate logical forms from tables without human\nannotation. In addition, PLOG can learn logical inference from table-logic\npairs much more definitely than from table-text pairs. To evaluate our model,\nwe further collect a controlled logical table-to-text dataset CONTLOG based on\nan existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms\nstrong baselines by a large margin on the logical fidelity, demonstrating the\neffectiveness of table-to-logic pretraining.",
    "descriptor": "",
    "authors": [
      "Ao Liu",
      "Haoyu Dong",
      "Naoaki Okazaki",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12697"
  },
  {
    "id": "arXiv:2205.12698",
    "title": "Empathic Conversations: A Multi-level Dataset of Contextualized  Conversations",
    "abstract": "Empathy is a cognitive and emotional reaction to an observed situation of\nothers. Empathy has recently attracted interest because it has numerous\napplications in psychology and AI, but it is unclear how different forms of\nempathy (e.g., self-report vs counterpart other-report, concern vs. distress)\ninteract with other affective phenomena or demographics like gender and age. To\nbetter understand this, we created the {\\it Empathic Conversations} dataset of\nannotated negative, empathy-eliciting dialogues in which pairs of participants\nconverse about news articles. People differ in their perception of the empathy\nof others. These differences are associated with certain characteristics such\nas personality and demographics. Hence, we collected detailed characterization\nof the participants' traits, their self-reported empathetic response to news\narticles, their conversational partner other-report, and turn-by-turn\nthird-party assessments of the level of self-disclosure, emotion, and empathy\nexpressed. This dataset is the first to present empathy in multiple forms along\nwith personal distress, emotion, personality characteristics, and person-level\ndemographic information. We present baseline models for predicting some of\nthese features from conversations.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Damilola Omitaomu",
      "Shabnam Tafreshi",
      "Tingting Liu",
      "Sven Buechel",
      "Chris Callison-Burch",
      "Johannes Eichstaedt",
      "Lyle Ungar",
      "Jo\u00e3o Sedoc"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12698"
  },
  {
    "id": "arXiv:2205.12700",
    "title": "Textual Backdoor Attacks with Iterative Trigger Injection",
    "abstract": "The backdoor attack has become an emerging threat for Natural Language\nProcessing (NLP) systems. A victim model trained on poisoned data can be\nembedded with a \"backdoor\", making it predict the adversary-specified output\n(e.g., the positive sentiment label) on inputs satisfying the trigger pattern\n(e.g., containing a certain keyword). In this paper, we demonstrate that it's\npossible to design an effective and stealthy backdoor attack by iteratively\ninjecting \"triggers\" into a small set of training data. While all triggers are\ncommon words that fit into the context, our poisoning process strongly\nassociates them with the target label, forming the model backdoor. Experiments\non sentiment analysis and hate speech detection show that our proposed attack\nis both stealthy and effective, raising alarm on the usage of untrusted\ntraining data. We further propose a defense method to combat this threat.",
    "descriptor": "",
    "authors": [
      "Jun Yan",
      "Vansh Gupta",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12700"
  },
  {
    "id": "arXiv:2205.12701",
    "title": "Eliciting Transferability in Multi-task Learning with Task-level  Mixture-of-Experts",
    "abstract": "Recent work suggests that transformer models are capable of multi-task\nlearning on diverse NLP tasks. However, the potential of these models may be\nlimited as they use the same set of parameters for all tasks. In contrast,\nhumans tackle tasks in a more flexible way, by making proper presumptions on\nwhat skills and knowledge are relevant and executing only the necessary\ncomputations. Inspired by this, we propose to use task-level mixture-of-expert\nmodels, which has a collection of transformer layers (i.e., experts) and a\nrouter component to choose among these experts dynamically and flexibly. We\nshow that the learned routing decisions and experts partially rediscover human\ncategorization of NLP tasks -- certain experts are strongly associated with\nextractive tasks, some with classification tasks, and some with tasks requiring\nworld knowledge.",
    "descriptor": "",
    "authors": [
      "Qinyuan Ye",
      "Juan Zha",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12701"
  },
  {
    "id": "arXiv:2205.12702",
    "title": "Detecting Label Errors using Pre-Trained Language Models",
    "abstract": "We show that large pre-trained language models are extremely capable of\nidentifying label errors in datasets: simply verifying data points in\ndescending order of out-of-distribution loss significantly outperforms more\ncomplex mechanisms for detecting label errors on natural language datasets. We\ncontribute a novel method to produce highly realistic, human-originated label\nnoise from crowdsourced data, and demonstrate the effectiveness of this method\non TweetNLP, providing an otherwise difficult to obtain measure of realistic\nrecall.",
    "descriptor": "",
    "authors": [
      "Derek Chong",
      "Jenny Hong",
      "Christopher D. Manning"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12702"
  },
  {
    "id": "arXiv:2205.12703",
    "title": "Unambiguous polynomial closure explained",
    "abstract": "We investigate a standard operator on classes of languages: unambiguous\npolynomial closure. We prove that for every class C of regular languages\nsatisfying mild properties, the membership problem for its unambiguous\npolynomial closure UPol(C) reduces to the same problem for C. We also show that\nunambiguous polynomial closure coincides with alternating left and right\ndeterministic closure. Moreover, we prove that if additionally C is finite, the\nseparation and covering problems are decidable for UPol(C). Finally, we present\nan overview of the generic logical characterizations of the classes built using\nunambiguous polynomial closure.",
    "descriptor": "",
    "authors": [
      "Thomas Place",
      "Marc Zeitoun"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.12703"
  },
  {
    "id": "arXiv:2205.12706",
    "title": "Scalable Online Change Detection for High-dimensional Data Streams",
    "abstract": "Detecting changes in data streams is a core objective in their analysis and\nhas applications in, say, predictive maintenance, fraud detection, and\nmedicine. A principled approach to detect changes is to compare distributions\nobserved within the stream to each other. However, data streams often are\nhigh-dimensional, and changes can be complex, e.g., only manifest themselves in\nhigher moments. The streaming setting also imposes heavy memory and computation\nrestrictions. We propose an algorithm, Maximum Mean Discrepancy Adaptive\nWindowing (MMDAW), which leverages the well-known Maximum Mean Discrepancy\n(MMD) two-sample test, and facilitates its efficient online computation on\nwindows whose size it flexibly adapts. As MMD is sensitive to any change in the\nunderlying distribution, our algorithm is a general-purpose non-parametric\nchange detector that fulfills the requirements imposed by the streaming\nsetting. Our experiments show that MMDAW achieves better detection quality than\nstate-of-the-art competitors.",
    "descriptor": "",
    "authors": [
      "Florian Kalinke",
      "Marco Heyden",
      "Edouard Fouch\u00e9",
      "Klemens B\u00f6hm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12706"
  },
  {
    "id": "arXiv:2205.12709",
    "title": "VeriFi: Towards Verifiable Federated Unlearning",
    "abstract": "Federated learning (FL) is a collaborative learning paradigm where\nparticipants jointly train a powerful model without sharing their private data.\nOne desirable property for FL is the implementation of the right to be\nforgotten (RTBF), i.e., a leaving participant has the right to request to\ndelete its private data from the global model. However, unlearning itself may\nnot be enough to implement RTBF unless the unlearning effect can be\nindependently verified, an important aspect that has been overlooked in the\ncurrent literature. In this paper, we prompt the concept of verifiable\nfederated unlearning, and propose VeriFi, a unified framework integrating\nfederated unlearning and verification that allows systematic analysis of the\nunlearning and quantification of its effect, with different combinations of\nmultiple unlearning and verification methods. In VeriFi, the leaving\nparticipant is granted the right to verify (RTV), that is, the participant\nnotifies the server before leaving, then actively verifies the unlearning\neffect in the next few communication rounds. The unlearning is done at the\nserver side immediately after receiving the leaving notification, while the\nverification is done locally by the leaving participant via two steps: marking\n(injecting carefully-designed markers to fingerprint the leaver) and checking\n(examining the change of the global model's performance on the markers). Based\non VeriFi, we conduct the first systematic and large-scale study for verifiable\nfederated unlearning, considering 7 unlearning methods and 5 verification\nmethods. Particularly, we propose a more efficient and FL-friendly unlearning\nmethod, and two more effective and robust non-invasive-verification methods. We\nextensively evaluate VeriFi on 7 datasets and 4 types of deep learning models.\nOur analysis establishes important empirical understandings for more\ntrustworthy federated unlearning.",
    "descriptor": "",
    "authors": [
      "Xiangshan Gao",
      "Xingjun Ma",
      "Jingyi Wang",
      "Youcheng Sun",
      "Bo Li",
      "Shouling Ji",
      "Peng Cheng",
      "Jiming Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12709"
  },
  {
    "id": "arXiv:2205.12711",
    "title": "Service Discovery in Social Internet of Things using Graph Neural  Networks",
    "abstract": "Internet-of-Things (IoT) networks intelligently connect thousands of physical\nentities to provide various services for the community. It is witnessing an\nexponential expansion, which is complicating the process of discovering IoT\ndevices existing in the network and requesting corresponding services from\nthem. As the highly dynamic nature of the IoT environment hinders the use of\ntraditional solutions of service discovery, we aim, in this paper, to address\nthis issue by proposing a scalable resource allocation neural model adequate\nfor heterogeneous large-scale IoT networks. We devise a Graph Neural Network\n(GNN) approach that utilizes the social relationships formed between the\ndevices in the IoT network to reduce the search space of any entity lookup and\nacquire a service from another device in the network. This proposed resource\nallocation approach surpasses standardization issues and embeds the structure\nand characteristics of the social IoT graph, by the means of GNNs, for eventual\nclustering analysis process. Simulation results applied on a real-world dataset\nillustrate the performance of this solution and its significant efficiency to\noperate on large-scale IoT networks.",
    "descriptor": "\nComments: Accepted for publications in the 65 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS'22) 7 figures, 1 table, 5 pages\n",
    "authors": [
      "Aymen Hamrouni",
      "Hakim Ghazzai",
      "Yehia Massoud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12711"
  },
  {
    "id": "arXiv:2205.12713",
    "title": "jTrans: Jump-Aware Transformer for Binary Code Similarity",
    "abstract": "Binary code similarity detection (BCSD) has important applications in various\nfields such as vulnerability detection, software component analysis, and\nreverse engineering. Recent studies have shown that deep neural networks (DNNs)\ncan comprehend instructions or control-flow graphs (CFG) of binary code and\nsupport BCSD. In this study, we propose a novel Transformer-based approach,\nnamely jTrans, to learn representations of binary code. It is the first\nsolution that embeds control flow information of binary code into\nTransformer-based language models, by using a novel jump-aware representation\nof the analyzed binaries and a newly-designed pre-training task. Additionally,\nwe release to the community a newly-created large dataset of binaries,\nBinaryCorp, which is the most diverse to date. Evaluation results show that\njTrans outperforms state-of-the-art (SOTA) approaches on this more challenging\ndataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known\nvulnerability searching, jTrans achieves a recall that is 2X higher than\nexisting SOTA baselines.",
    "descriptor": "\nComments: In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA) 2022\n",
    "authors": [
      "Hao Wang",
      "Wenjie Qu",
      "Gilad Katz",
      "Wenyu Zhu",
      "Zeyu Gao",
      "Han Qiu",
      "Jianwei Zhuge",
      "Chao Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.12713"
  },
  {
    "id": "arXiv:2205.12718",
    "title": "DPSNN: A Differentially Private Spiking Neural Network",
    "abstract": "Privacy-preserving is a key problem for the machine learning algorithm.\nSpiking neural network (SNN) plays an important role in many domains, such as\nimage classification, object detection, and speech recognition, but the study\non the privacy protection of SNN is urgently needed. This study combines the\ndifferential privacy (DP) algorithm and SNN and proposes differentially private\nspiking neural network (DPSNN). DP injects noise into the gradient, and SNN\ntransmits information in discrete spike trains so that our differentially\nprivate SNN can maintain strong privacy protection while still ensuring high\naccuracy. We conducted experiments on MNIST, Fashion-MNIST, and the face\nrecognition dataset Extended YaleB. When the privacy protection is improved,\nthe accuracy of the artificial neural network(ANN) drops significantly, but our\nalgorithm shows little change in performance. Meanwhile, we analyzed different\nfactors that affect the privacy protection of SNN. Firstly, the less precise\nthe surrogate gradient is, the better the privacy protection of the SNN.\nSecondly, the Integrate-And-Fire (IF) neurons perform better than leaky\nIntegrate-And-Fire (LIF) neurons. Thirdly, a large time window contributes more\nto privacy protection and performance.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Jihang Wang",
      "Dongcheng Zhao",
      "Guobin Shen",
      "Qian Zhang",
      "Yi Zeng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12718"
  },
  {
    "id": "arXiv:2205.12721",
    "title": "Accelerating High-Order Mesh Optimization Using Finite Element Partial  Assembly on GPUs",
    "abstract": "In this paper we present a new GPU-oriented mesh optimization method based on\nhigh-order finite elements. Our approach relies on node movement with fixed\ntopology, through the Target-Matrix Optimization Paradigm (TMOP) and uses a\nglobal nonlinear solve over the whole computational mesh, i.e., all mesh nodes\nare moved together. A key property of the method is that the mesh optimization\nprocess is recast in terms of finite element operations, which allows us to\nutilize recent advances in the field of GPU-accelerated high-order finite\nelement algorithms. For example, we reduce data motion by using tensor\nfactorization and matrix-free methods, which have superior performance\ncharacteristics compared to traditional full finite element matrix assembly and\noffer advantages for GPU-based HPC hardware. We describe the major mathematical\ncomponents of the method along with their efficient GPU-oriented\nimplementation. In addition, we propose an easily reproducible mesh\noptimization test that can serve as a performance benchmark for the mesh\noptimization community.",
    "descriptor": "\nComments: 28 pages, 10 figures, 2 tables\n",
    "authors": [
      "Jean-Sylvain Camier",
      "Veselin Dobrev",
      "Patrick Knupp",
      "Tzanio Kolev",
      "Ketan Mittal",
      "Robert Rieben",
      "Vladimir Tomov"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.12721"
  },
  {
    "id": "arXiv:2205.12722",
    "title": "Mathematical Models of Human Drivers Using Artificial Risk Fields",
    "abstract": "In this paper, we use the concept of artificial risk fields to predict how\nhuman operators control a vehicle in response to upcoming road situations. A\nrisk field assigns a non-negative risk measure to the state of the system in\norder to model how close that state is to violating a safety property, such as\nhitting an obstacle or exiting the road. Using risk fields, we construct a\nstochastic model of the operator that maps from states to likely actions. We\ndemonstrate our approach on a driving task wherein human subjects are asked to\ndrive a car inside a realistic driving simulator while avoiding obstacles\nplaced on the road. We show that the most likely risk field given the driving\ndata is obtained by solving a convex optimization problem. Next, we apply the\ninferred risk fields to generate distinct driving behaviors while comparing\npredicted trajectories against ground truth measurements. We observe that the\nrisk fields are excellent at predicting future trajectory distributions with\nhigh prediction accuracy for up to twenty seconds prediction horizons. At the\nsame time, we observe some challenges such as the inability to account for how\ndrivers choose to accelerate/decelerate based on the road conditions.",
    "descriptor": "\nComments: 8 pages, 4 figures, submitted to Intelligent Transportation Systems Conference\n",
    "authors": [
      "Emily Jensen",
      "Maya Luster",
      "Hansol Yoon",
      "Brandon Pitts",
      "Sriram Sankaranarayanan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12722"
  },
  {
    "id": "arXiv:2205.12723",
    "title": "Interpretable Feature Engineering for Time Series Predictors using  Attention Networks",
    "abstract": "Regression problems with time-series predictors are common in banking and\nmany other areas of application. In this paper, we use multi-head attention\nnetworks to develop interpretable features and use them to achieve good\npredictive performance. The customized attention layer explicitly uses\nmultiplicative interactions and builds feature-engineering heads that capture\ntemporal dynamics in a parsimonious manner. Convolutional layers are used to\ncombine multivariate time series. We also discuss methods for handling static\ncovariates in the modeling process. Visualization and explanation tools are\nused to interpret the results and explain the relationship between the inputs\nand the extracted features. Both simulation and real dataset are used to\nillustrate the usefulness of the methodology. Keyword: Attention heads, Deep\nneural networks, Interpretable feature engineering",
    "descriptor": "",
    "authors": [
      "Tianjie Wang",
      "Jie Chen",
      "Joel Vaughan",
      "Vijayan N. Nair"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12723"
  },
  {
    "id": "arXiv:2205.12725",
    "title": "Wigner-Smith Time Delay Matrix for Acoustic Scattering: Computational  Aspects",
    "abstract": "The Wigner-Smith (WS) time delay matrix relates an acoustic system's\nscattering matrix to its wavenumber derivative. The entries of the WS time\ndelay matrix can be expressed in terms of energy density-like volume integrals,\nwhich cannot be efficiently evaluated in a boundary element method framework.\nThis paper presents two schemes for efficiently populating the WS time delay\nmatrix. The direct formulation casts the energy density-like volume integrals\ninto integrals of the incident field and the field and/or its normal derivative\nover the scatterer surface. The indirect formulation computes the system's\nscattering matrix and its wavenumber derivative, again via surface integration,\nand then invokes the WS relationship to compute the WS time delay matrix. Both\nthe direct and the indirect formulations yield equivalent results and can be\neasily integrated into standard boundary element codes.",
    "descriptor": "\nComments: Submitted the Journal of the Acoustical Society of America\n",
    "authors": [
      "Utkarsh R. Patel",
      "Yiqian Mao",
      "Jack Hamel",
      "Eric Michielssen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.12725"
  },
  {
    "id": "arXiv:2205.12730",
    "title": "Uncertainty Quantification for Transport in Porous media using  Parameterized Physics Informed neural Networks",
    "abstract": "We present a Parametrization of the Physics Informed Neural Network (P-PINN)\napproach to tackle the problem of uncertainty quantification in reservoir\nengineering problems. We demonstrate the approach with the immiscible two phase\nflow displacement (Buckley-Leverett problem) in heterogeneous porous medium.\nThe reservoir properties (porosity, permeability) are treated as random\nvariables. The distribution of these properties can affect dynamic properties\nsuch as the fluids saturation, front propagation speed or breakthrough time. We\nexplore and use to our advantage the ability of networks to interpolate complex\nhigh dimensional functions. We observe that the additional dimensions resulting\nfrom a stochastic treatment of the partial differential equations tend to\nproduce smoother solutions on quantities of interest (distributions parameters)\nwhich is shown to improve the performance of PINNS. We show that provided a\nproper parameterization of the uncertainty space, PINN can produce solutions\nthat match closely both the ensemble realizations and the stochastic moments.\nWe demonstrate applications for both homogeneous and heterogeneous fields of\nproperties. We are able to solve problems that can be challenging for classical\nmethods. This approach gives rise to trained models that are both more robust\nto variations in the input space and can compete in performance with\ntraditional stochastic sampling methods.",
    "descriptor": "",
    "authors": [
      "Cedric Fraces Gasmi",
      "Hamdi Tchelepi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12730"
  },
  {
    "id": "arXiv:2205.12735",
    "title": "Inductive Learning of Complex Knowledge from Raw Data",
    "abstract": "One of the ultimate goals of Artificial Intelligence is to learn generalised\nand human-interpretable knowledge from raw data. Neuro-symbolic reasoning\napproaches partly tackle this problem by improving the training of a neural\nnetwork using a manually engineered symbolic knowledge base. In the case where\nsymbolic knowledge is learned from raw data, this knowledge lacks the\nexpressivity required to solve complex problems. In this paper, we introduce\nNeuro-Symbolic Inductive Learner (NSIL), an approach that trains a neural\nnetwork to extract latent concepts from raw data, whilst learning symbolic\nknowledge that solves complex problems, defined in terms of these latent\nconcepts. The novelty of our approach is a method for biasing a symbolic\nlearner to learn improved knowledge, based on the in-training performance of\nboth neural and symbolic components. We evaluate NSIL on two problem domains\nthat require learning knowledge with different levels of complexity, and\ndemonstrate that NSIL learns knowledge that is not possible to learn with other\nneuro-symbolic systems, whilst outperforming baseline models in terms of\naccuracy and data efficiency.",
    "descriptor": "",
    "authors": [
      "Daniel Cunnington",
      "Mark Law",
      "Jorge Lobo",
      "Alessandra Russo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12735"
  },
  {
    "id": "arXiv:2205.12737",
    "title": "On the Routing Convergence Delay in the Lightning Network",
    "abstract": "Nodes in the Lightning Network synchronise routing information through a\ngossip protocol that makes use of a staggered broadcast mechanism. In this\nwork, we show that the convergence delay in the network is larger than what\nwould be expected from the protocol's specification and that payment attempt\nfailures caused by the delay are more frequent, the larger the delay is. To\nthis end, we measure the convergence delay incurred in the network and analyse\nwhat its primary causes are. Moreover, we further investigate and confirm our\nfindings through a time-discrete simulation of the Lightning Network gossip\nprotocol. We explore the use of alternative gossip protocols as well as\nparameter variations of the current protocol and evaluate them by the resulting\nbandwidth usage and convergence delay. Our research shows that there are\nmultiple ways of lowering the convergence delay, ranging from simple parameter\nchanges to overhauling the entire protocol.",
    "descriptor": "",
    "authors": [
      "Niklas G\u00f6gge",
      "Elias Rohrer",
      "Florian Tschorsch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12737"
  },
  {
    "id": "arXiv:2205.12738",
    "title": "Generic Decoding in the Cover Metric",
    "abstract": "In this paper, we study the hardness of decoding a random code endowed with\nthe cover metric. As the cover metric lies in between the Hamming and rank\nmetric, it presents itself as a promising candidate for code-based\ncryptography. We give a polynomial-time reduction from the classical\nHamming-metric decoding problem, which proves the NP-hardness of the decoding\nproblem in the cover metric. We then provide a generic decoder, following the\ninformation set decoding idea from Prange's algorithm in the Hamming metric. A\nstudy of its cost then shows that the complexity is exponential in the number\nof rows and columns, which is in contrast to the behaviour in the Hamming\nmetric, where the complexity grows exponentially in the number of code symbols.",
    "descriptor": "",
    "authors": [
      "Sebastian Bitzer",
      "Julian Renner",
      "Antonia Wachter-Zeh",
      "Violetta Weger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12738"
  },
  {
    "id": "arXiv:2205.12740",
    "title": "SIoU Loss: More Powerful Learning for Bounding Box Regression",
    "abstract": "The effectiveness of Object Detection, one of the central problems in\ncomputer vision tasks, highly depends on the definition of the loss function -\na measure of how accurately your ML model can predict the expected outcome.\nConventional object detection loss functions depend on aggregation of metrics\nof bounding box regression such as the distance, overlap area and aspect ratio\nof the predicted and ground truth boxes (i.e. GIoU, CIoU, ICIoU etc). However,\nnone of the methods proposed and used to date considers the direction of the\nmismatch between the desired ground box and the predicted, \"experimental\" box.\nThis shortage results in slower and less effective convergence as the predicted\nbox can \"wander around\" during the training process and eventually end up\nproducing a worse model. In this paper a new loss function SIoU was suggested,\nwhere penalty metrics were redefined considering the angle of the vector\nbetween the desired regression. Applied to conventional Neural Networks and\ndatasets it is shown that SIoU improves both the speed of training and the\naccuracy of the inference. The effectiveness of the proposed loss function was\nrevealed in a number of simulations and tests.",
    "descriptor": "",
    "authors": [
      "Zhora Gevorgyan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12740"
  },
  {
    "id": "arXiv:2205.12742",
    "title": "SoK: Hardware-supported Trusted Execution Environments",
    "abstract": "The growing complexity of modern computing platforms and the need for strong\nisolation protections among their software components has led to the increased\nadoption of Trusted Execution Environments (TEEs). While several commercial and\nacademic TEE architectures have emerged in recent times, they remain hard to\ncompare and contrast. More generally, existing TEEs have not been subject to a\nholistic systematization to understand the available design alternatives for\nvarious aspects of TEE design and their corresponding pros-and-cons.\nTherefore, in this work, we analyze the design of existing TEEs and\nsystematize the mechanisms that TEEs implement to achieve their security goals,\nnamely, verifiable launch, run-time isolation, trusted IO, and secure storage.\nMore specifically, we analyze the typical architectural building blocks\nunderlying TEE solutions, design alternatives for each of these components and\nthe trade-offs that they entail. We focus on hardware-assisted TEEs and cover a\nwide range of TEE proposals from academia and the industry. Our analysis shows\nthat although TEEs are diverse in terms of their goals, usage models, and\ninstruction set architectures, they all share many common building blocks in\nterms of their design.",
    "descriptor": "",
    "authors": [
      "Moritz Schneider",
      "Ramya Jayaram Masti",
      "Shweta Shinde",
      "Srdjan Capkun",
      "Ronald Perez"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.12742"
  },
  {
    "id": "arXiv:2205.12748",
    "title": "Secure and Efficient Tunneling of MACsec for Modern Industrial Use Cases",
    "abstract": "Trends like Industry 4.0 will pose new challenges for future industrial\nnetworks. Greater interconnectedness, higher data volumes as well as new\nrequirements for speeds as well as security will make new approaches necessary.\nPerformanceoptimized networking techniques will be demanded to implement new\nuse cases, like network separation and isolation, in a secure fashion. A new\nand highly efficient protocol, that will be vital for that purpose, is MACsec.\nIt is a Layer 2 encryption protocol that was previously extended specifically\nfor industrial environments. Yet, it lacks the ability to bridge local\nnetworks. Therefore, in this work, we propose a secure and efficient Layer 3\ntunneling scheme for MACsec. We design and implement two approaches, that are\nequally secure and considerably outperform comparable state-of-the-art\ntechniques.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Tim Lackorzynski",
      "Sebastian Rehms",
      "Tao Li",
      "Stefan K\u00f6psell",
      "Hermann H\u00e4rtig"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12748"
  },
  {
    "id": "arXiv:2205.12749",
    "title": "A Human-Centric Assessment Framework for AI",
    "abstract": "With the rise of AI systems in real-world applications comes the need for\nreliable and trustworthy AI. An important aspect for this are explainable AI\nsystems. However, there is no agreed standard on how explainable AI systems\nshould be assessed. Inspired by the Turing test, we introduce a human-centric\nassessment framework where a leading domain expert accepts or rejects the\nsolutions of an AI system and another domain expert. By comparing the\nacceptance rates of provided solutions, we can assess how the AI system\nperforms in comparison to the domain expert, and in turn whether or not the AI\nsystem's explanations (if provided) are human understandable. This setup --\ncomparable to the Turing test -- can serve as framework for a wide range of\nhuman-centric AI system assessments. We demonstrate this by presenting two\ninstantiations: (1) an assessment that measures the classification accuracy of\na system with the option to incorporate label uncertainties; (2) an assessment\nwhere the usefulness of provided explanations is determined in a human-centric\nmanner.",
    "descriptor": "",
    "authors": [
      "Sascha Saralajew",
      "Ammar Shaker",
      "Zhao Xu",
      "Kiril Gashteovski",
      "Bhushan Kotnis",
      "Wiem Ben-Rim",
      "J\u00fcrgen Quittek",
      "Carolin Lawrence"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.12749"
  },
  {
    "id": "arXiv:2205.12752",
    "title": "NECA: Network-Embedded Deep Representation Learning for Categorical Data",
    "abstract": "We propose NECA, a deep representation learning method for categorical data.\nBuilt upon the foundations of network embedding and deep unsupervised\nrepresentation learning, NECA deeply embeds the intrinsic relationship among\nattribute values and explicitly expresses data objects with numeric vector\nrepresentations. Designed specifically for categorical data, NECA can support\nimportant downstream data mining tasks, such as clustering. Extensive\nexperimental analysis demonstrated the effectiveness of NECA.",
    "descriptor": "",
    "authors": [
      "Xiaonan Gao",
      "Sen Wu",
      "Wenjun Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12752"
  },
  {
    "id": "arXiv:2205.12753",
    "title": "An Empirical Study on Distribution Shift Robustness From the Perspective  of Pre-Training and Data Augmentation",
    "abstract": "The performance of machine learning models under distribution shift has been\nthe focus of the community in recent years. Most of current methods have been\nproposed to improve the robustness to distribution shift from the algorithmic\nperspective, i.e., designing better training algorithms to help the\ngeneralization in shifted test distributions. This paper studies the\ndistribution shift problem from the perspective of pre-training and data\naugmentation, two important factors in the practice of deep learning that have\nnot been systematically investigated by existing work. By evaluating seven\npre-trained models, including ResNets and ViT's with self-supervision and\nsupervision mode, on five important distribution-shift datasets, from WILDS and\nDomainBed benchmarks, with five different learning algorithms, we provide the\nfirst comprehensive empirical study focusing on pre-training and data\naugmentation. With our empirical result obtained from 1,330 models, we provide\nthe following main observations: 1) ERM combined with data augmentation can\nachieve state-of-the-art performance if we choose a proper pre-trained model\nrespecting the data property; 2) specialized algorithms further improve the\nrobustness on top of ERM when handling a specific type of distribution shift,\ne.g., GroupDRO for spurious correlation and CORAL for large-scale\nout-of-distribution data; 3) Comparing different pre-training modes,\narchitectures and data sizes, we provide novel observations about pre-training\non distribution shift, which sheds light on designing or selecting pre-training\nstrategy for different kinds of distribution shifts. In summary, our empirical\nstudy provides a comprehensive baseline for a wide range of pre-training models\nfine-tuned with data augmentation, which potentially inspires research\nexploiting the power of pre-training and data augmentation in the future of\ndistribution shift study.",
    "descriptor": "",
    "authors": [
      "Ziquan Liu",
      "Yi Xu",
      "Yuanhong Xu",
      "Qi Qian",
      "Hao Li",
      "Rong Jin",
      "Xiangyang Ji",
      "Antoni B. Chan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12753"
  },
  {
    "id": "arXiv:2205.12755",
    "title": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale  Multitask Learning Systems",
    "abstract": "Multitask learning assumes that models capable of learning from multiple\ntasks can achieve better quality and efficiency via knowledge transfer, a key\nfeature of human learning. Though, state of the art ML models rely on high\ncustomization for each task and leverage size and data scale rather than\nscaling the number of tasks. Also, continual learning, that adds the temporal\naspect to multitask, is often focused to the study of common pitfalls such as\ncatastrophic forgetting instead of being studied at a large scale as a critical\ncomponent to build the next generation artificial intelligence. We propose an\nevolutionary method that can generate a large scale multitask model, and can\nsupport the dynamic and continuous addition of new tasks. The generated\nmultitask model is sparsely activated and integrates a task-based routing that\nguarantees bounded compute cost and fewer added parameters per task as the\nmodel expands. The proposed method relies on a knowledge compartmentalization\ntechnique to achieve immunity against catastrophic forgetting and other common\npitfalls such as gradient interference and negative transfer. We empirically\nshow that the proposed method can jointly solve and achieve competitive results\non 69image classification tasks, for example achieving the best test accuracy\nreported fora model trained only on public data for competitive tasks such as\ncifar10: 99.43%.",
    "descriptor": "",
    "authors": [
      "Andrea Gesmundo",
      "Jeff Dean"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.12755"
  },
  {
    "id": "arXiv:2205.12756",
    "title": "Development of a Stereo-Vision Based High-Throughput Robotic System for  Mouse Tail Vein Injection",
    "abstract": "In this paper, we present a robotic device for mouse tail vein injection. We\npropose a mouse holding mechanism to realize vein injection without\nanesthetizing the mouse, which consists of a tourniquet, vacuum port, and\nadaptive tail-end fixture. The position of the target vein in 3D space is\nreconstructed from a high-resolution stereo vision. The vein is detected by a\nsimple but robust vein line detector. Thanks to the proposed two-staged\ncalibration process, the total time for the injection process is limited to 1.5\nminutes, despite that the position of needle and tail vein varies for each\ntrial. We performed an injection experiment targeting 40 mice and succeeded to\ninject saline to 37 of them, resulting 92.5% success ratio.",
    "descriptor": "\nComments: accepted to ICRA2022 (7 pages, 11 figures, 2 tables)\n",
    "authors": [
      "Tianyi Ko",
      "Koichi Nishiwaki",
      "Koji Terada",
      "Yusuke Tanaka",
      "Shun Mitsumata",
      "Ryuichi Katagiri",
      "Taketo Junko",
      "Naoshi Horiba",
      "Hideyoshi Igata",
      "Kazue Mizuno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.12756"
  },
  {
    "id": "arXiv:2205.12757",
    "title": "A Case for Practical Configuration Management Using Hardware-based  Security Tokens",
    "abstract": "Future industrial networks will consist of a complex mixture of new and\nlegacy components, while new use cases and applications envisioned by Industry\n4.0 will demand increased flexibility and dynamics from these networks.\nIndustrial security gateways will become an important building block to tackle\nnew security requirements demanded by these changes. Their introduction will\nfurther increase the already high complexity of these networks, demanding more\nefforts in properly and securely configuring them. Yet, past research showed,\nthat most operators of industrial networks are already today unable to\nconfigure industrial networks in a secure fashion.\nTherefore, we propose a scheme that allows factory operators to configure\nsecurity gateways in an easy and practical way that is also understandable for\nstaff not trained in the security domain. We employ hardware security tokens\nthat allow to reduce every day configuration to one physical interaction. Our\nresults show the practical feasibility of our proposed scheme and that it does\nnot reduce the security level of industrial security gateways in any way.",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Tim Lackorzynski",
      "Max Ostermann",
      "Stefan K\u00f6psell",
      "Hermann H\u00e4rtig"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.12757"
  },
  {
    "id": "arXiv:2205.12760",
    "title": "Guiding Vector Fields for Following Occluded Paths",
    "abstract": "Accurately following a geometric desired path in a two-dimensional space is a\nfundamental task for many engineering systems, in particular mobile robots.\nWhen the desired path is occluded by obstacles, it is necessary and crucial to\ntemporarily deviate from the path for obstacle/collision avoidance. In this\npaper, we develop a composite guiding vector field via the use of smooth bump\nfunctions, and provide theoretical guarantees that the integral curves of the\nvector field can follow an arbitrary sufficiently smooth desired path and avoid\ncollision with obstacles of arbitrary shapes. These two behaviors are reactive\nsince path (re)-planning and global map construction are not involved. To deal\nwith the common deadlock problem, we introduce a switching vector field, and\nthe Zeno behavior is excluded. Simulations are conducted to support the\ntheoretical results.",
    "descriptor": "",
    "authors": [
      "Weijia Yao",
      "Bohuan Lin",
      "Brian D. O. Anderson",
      "Ming Cao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12760"
  },
  {
    "id": "arXiv:2205.12764",
    "title": "Square roots of nearly planar graphs",
    "abstract": "We prove that it is NP-hard to decide whether a graph is the square of a\n6-apex graph. This shows that the square root problem is not tractable for\nsquares of sparse graphs (or even graphs from proper minor-closed classes).",
    "descriptor": "\nComments: 6 pages, no figures\n",
    "authors": [
      "Zden\u011bk Dvo\u0159\u00e1k",
      "Benjamin Moore",
      "Abhiruk Lahari"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.12764"
  },
  {
    "id": "arXiv:2205.12765",
    "title": "No Time for Downtime: Understanding Post-Attack Behaviors by Customers  of Managed DNS Providers",
    "abstract": "We leverage large-scale DNS measurement data on authoritative name servers to\nstudy the reactions of domain owners affected by the 2016 DDoS attack on Dyn.\nWe use industry sources of information about domain names to study the\ninfluence of factors such as industry sector and website popularity on the\nwillingness of domain managers to invest in high availability of online\nservices. Specifically, we correlate business characteristics of domain owners\nwith their resilience strategies in the wake of DoS attacks affecting their\ndomains. Our analysis revealed correlations between two properties of domains\n-- industry sector and popularity -- and post-attack strategies. Specifically,\nowners of more popular domains were more likely to re-act to increase the\ndiversity of their authoritative DNS service for their domains. Similarly,\ndomains in certain industry sectors were more likely to seek out such diversity\nin their DNS service. For example, domains categorized as General News were\nnearly 6 times more likely to re-act than domains categorized as Internet\nServices. Our results can inform managed DNS and other network service\nproviders regarding the potential impact of downtime on their customer\nportfolio.",
    "descriptor": "",
    "authors": [
      "Muhammad Yasir Muzayan Haq",
      "Mattijs Jonker",
      "Roland van Rijswijk-Deij",
      "KC Claffy",
      "Lambert J.M. Nieuwenhuis",
      "Abhishta Abhishta"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12765"
  },
  {
    "id": "arXiv:2205.12768",
    "title": "Would You Ask it that Way? Measuring and Improving Question Naturalness  for Knowledge Graph Question Answering",
    "abstract": "Knowledge graph question answering (KGQA) facilitates information access by\nleveraging structured data without requiring formal query language expertise\nfrom the user. Instead, users can express their information needs by simply\nasking their questions in natural language (NL). Datasets used to train KGQA\nmodels that would provide such a service are expensive to construct, both in\nterms of expert and crowdsourced labor. Typically, crowdsourced labor is used\nto improve template-based pseudo-natural questions generated from formal\nqueries. However, the resulting datasets often fall short of representing\ngenuinely natural and fluent language. In the present work, we investigate ways\nto characterize and remedy these shortcomings. We create the IQN-KGQA test\ncollection by sampling questions from existing KGQA datasets and evaluating\nthem with regards to five different aspects of naturalness. Then, the questions\nare rewritten to improve their fluency. Finally, the performance of existing\nKGQA models is compared on the original and rewritten versions of the NL\nquestions. We find that some KGQA systems fare worse when presented with more\nrealistic formulations of NL questions. The IQN-KGQA test collection is a\nresource to help evaluate KGQA systems in a more realistic setting. The\nconstruction of this test collection also sheds light on the challenges of\nconstructing large-scale KGQA datasets with genuinely NL questions.",
    "descriptor": "\nComments: 9 pages, 3 figures. Accepted for publication as a resource paper in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22), July 11-15, 2022, Madrid, Spain. For test collection, see this https URL\n",
    "authors": [
      "Trond Linjordet",
      "Krisztian Balog"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12768"
  },
  {
    "id": "arXiv:2205.12770",
    "title": "An Experimental Comparison Between Temporal Difference and Residual  Gradient with Neural Network Approximation",
    "abstract": "Gradient descent or its variants are popular in training neural networks.\nHowever, in deep Q-learning with neural network approximation, a type of\nreinforcement learning, gradient descent (also known as Residual Gradient (RG))\nis barely used to solve Bellman residual minimization problem. On the contrary,\nTemporal Difference (TD), an incomplete gradient descent method prevails. In\nthis work, we perform extensive experiments to show that TD outperforms RG,\nthat is, when the training leads to a small Bellman residual error, the\nsolution found by TD has a better policy and is more robust against the\nperturbation of neural network parameters. We further use experiments to reveal\na key difference between reinforcement learning and supervised learning, that\nis, a small Bellman residual error can correspond to a bad policy in\nreinforcement learning while the test loss function in supervised learning is a\nstandard index to indicate the performance. We also empirically examine that\nthe missing term in TD is a key reason why RG performs badly. Our work shows\nthat the performance of a deep Q-learning solution is closely related to the\ntraining dynamics and how an incomplete gradient descent method can find a good\npolicy is interesting for future study.",
    "descriptor": "",
    "authors": [
      "Shuyu Yin",
      "Tao Luo",
      "Peilin Liu",
      "Zhi-Qin John Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12770"
  },
  {
    "id": "arXiv:2205.12771",
    "title": "Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy",
    "abstract": "In an effort to guarantee that machine learning model outputs conform with\nhuman moral values, recent work has begun exploring the possibility of\nexplicitly training models to learn the difference between right and wrong.\nThis is typically done in a bottom-up fashion, by exposing the model to\ndifferent scenarios, annotated with human moral judgements. One question,\nhowever, is whether the trained models actually learn any consistent,\nhigher-level ethical principles from these datasets -- and if so, what? Here,\nwe probe the Allen AI Delphi model with a set of standardized morality\nquestionnaires, and find that, despite some inconsistencies, Delphi tends to\nmirror the moral principles associated with the demographic groups involved in\nthe annotation process. We question whether this is desirable and discuss how\nwe might move forward with this knowledge.",
    "descriptor": "\nComments: To appear at TrustNLP Workshop @ NAACL 2022\n",
    "authors": [
      "Kathleen C. Fraser",
      "Svetlana Kiritchenko",
      "Esma Balkir"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12771"
  },
  {
    "id": "arXiv:2205.12772",
    "title": "A systematic approach to Lyapunov analyses of continuous-time models in  convex optimization",
    "abstract": "First-order methods are often analyzed via their continuous-time models,\nwhere their worst-case convergence properties are usually approached via\nLyapunov functions. In this work, we provide a systematic and principled\napproach to find and verify Lyapunov functions for classes of ordinary and\nstochastic differential equations. More precisely, we extend the performance\nestimation framework, originally proposed by Drori and Teboulle [10], to\ncontinuous-time models. We retrieve convergence results comparable to those of\ndiscrete methods using fewer assumptions and convexity inequalities, and\nprovide new results for stochastic accelerated gradient flows.",
    "descriptor": "",
    "authors": [
      "C\u00e9line Moucer",
      "Adrien Taylor",
      "Francis Bach"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12772"
  },
  {
    "id": "arXiv:2205.12775",
    "title": "Residual-Concatenate Neural Network with Deep Regularization Layers for  Binary Classification",
    "abstract": "Many complex Deep Learning models are used with different variations for\nvarious prognostication tasks. The higher learning parameters not necessarily\nensure great accuracy. This can be solved by considering changes in very deep\nmodels with many regularization based techniques. In this paper we train a deep\nneural network that uses many regularization layers with residual and\nconcatenation process for best fit with Polycystic Ovary Syndrome Diagnosis\nprognostication. The network was built with improvements from every step of\nfailure to meet the needs of the data and achieves an accuracy of 99.3%\nseamlessly.",
    "descriptor": "\nComments: 7 pages, 5 figures. To appear in the proceedings of 6th International Conference on Intelligent Computing and Control Systems (ICICCS 2022)\n",
    "authors": [
      "Abhishek Gupta",
      "Sruthi Nair",
      "Raunak Joshi",
      "Vidya Chitre"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12775"
  },
  {
    "id": "arXiv:2205.12781",
    "title": "Ultra-compact Binary Neural Networks for Human Activity Recognition on  RISC-V Processors",
    "abstract": "Human Activity Recognition (HAR) is a relevant inference task in many mobile\napplications. State-of-the-art HAR at the edge is typically achieved with\nlightweight machine learning models such as decision trees and Random Forests\n(RFs), whereas deep learning is less common due to its high computational\ncomplexity. In this work, we propose a novel implementation of HAR based on\ndeep neural networks, and precisely on Binary Neural Networks (BNNs), targeting\nlow-power general purpose processors with a RISC-V instruction set. BNNs yield\nvery small memory footprints and low inference complexity, thanks to the\nreplacement of arithmetic operations with bit-wise ones. However, existing BNN\nimplementations on general purpose processors impose constraints tailored to\ncomplex computer vision tasks, which result in over-parametrized models for\nsimpler problems like HAR. Therefore, we also introduce a new BNN inference\nlibrary, which targets ultra-compact models explicitly. With experiments on a\nsingle-core RISC-V processor, we show that BNNs trained on two HAR datasets\nobtain higher classification accuracy compared to a state-of-the-art baseline\nbased on RFs. Furthermore, our BNN reaches the same accuracy of a RF with\neither less memory (up to 91%) or more energy-efficiency (up to 70%), depending\non the complexity of the features extracted by the RF.",
    "descriptor": "\nComments: Published in: 2021 18th ACM International Conference on Computing Frontiers (CF)\n",
    "authors": [
      "Francesco Daghero",
      "Chen Xie",
      "Daniele Jahier Pagliari",
      "Alessio Burrello",
      "Marco Castellano",
      "Luca Gandolfi",
      "Andrea Calimera",
      "Enrico Macii",
      "Massimo Poncino"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12781"
  },
  {
    "id": "arXiv:2205.12784",
    "title": "TrustGNN: Graph Neural Network based Trust Evaluation via Learnable  Propagative and Composable Nature",
    "abstract": "Trust evaluation is critical for many applications such as cyber security,\nsocial communication and recommender systems. Users and trust relationships\namong them can be seen as a graph. Graph neural networks (GNNs) show their\npowerful ability for analyzing graph-structural data. Very recently, existing\nwork attempted to introduce the attributes and asymmetry of edges into GNNs for\ntrust evaluation, while failed to capture some essential properties (e.g., the\npropagative and composable nature) of trust graphs. In this work, we propose a\nnew GNN based trust evaluation method named TrustGNN, which integrates smartly\nthe propagative and composable nature of trust graphs into a GNN framework for\nbetter trust evaluation. Specifically, TrustGNN designs specific propagative\npatterns for different propagative processes of trust, and distinguishes the\ncontribution of different propagative processes to create new trust. Thus,\nTrustGNN can learn comprehensive node embeddings and predict trust\nrelationships based on these embeddings. Experiments on some widely-used\nreal-world datasets indicate that TrustGNN significantly outperforms the\nstate-of-the-art methods. We further perform analytical experiments to\ndemonstrate the effectiveness of the key designs in TrustGNN.",
    "descriptor": "",
    "authors": [
      "Cuiying Huo",
      "Di Jin",
      "Chundong Liang",
      "Dongxiao He",
      "Tie Qiu",
      "Lingfei Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12784"
  },
  {
    "id": "arXiv:2205.12785",
    "title": "AO2-DETR: Arbitrary-Oriented Object Detection Transformer",
    "abstract": "Arbitrary-oriented object detection (AOOD) is a challenging task to detect\nobjects in the wild with arbitrary orientations and cluttered arrangements.\nExisting approaches are mainly based on anchor-based boxes or dense points,\nwhich rely on complicated hand-designed processing steps and inductive bias,\nsuch as anchor generation, transformation, and non-maximum suppression\nreasoning. Recently, the emerging transformer-based approaches view object\ndetection as a direct set prediction problem that effectively removes the need\nfor hand-designed components and inductive biases. In this paper, we propose an\nArbitrary-Oriented Object DEtection TRansformer framework, termed AO2-DETR,\nwhich comprises three dedicated components. More precisely, an oriented\nproposal generation mechanism is proposed to explicitly generate oriented\nproposals, which provides better positional priors for pooling features to\nmodulate the cross-attention in the transformer decoder. An adaptive oriented\nproposal refinement module is introduced to extract rotation-invariant region\nfeatures and eliminate the misalignment between region features and objects.\nAnd a rotation-aware set matching loss is used to ensure the one-to-one\nmatching process for direct set prediction without duplicate predictions. Our\nmethod considerably simplifies the overall pipeline and presents a new AOOD\nparadigm. Comprehensive experiments on several challenging datasets show that\nour method achieves superior performance on the AOOD task.",
    "descriptor": "",
    "authors": [
      "Linhui Dai",
      "Hong Liu",
      "Hao Tang",
      "Zhiwei Wu",
      "Pinhao Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12785"
  },
  {
    "id": "arXiv:2205.12787",
    "title": "Impartial Games: A Challenge for Reinforcement Learning",
    "abstract": "The AlphaZero algorithm and its successor MuZero have revolutionised several\ncompetitive strategy games, including chess, Go, and shogi and video games like\nAtari, by learning to play these games better than any human and any\nspecialised computer program. Aside from knowing the rules, AlphaZero had no\nprior knowledge of each game. This dramatically advanced progress on a\nlong-standing AI challenge to create programs that can learn for themselves\nfrom first principles.\nTheoretically, there are well-known limits to the power of deep learning for\nstrategy games like chess, Go, and shogi, as they are known to be NEXPTIME\nhard. Some papers have argued that the AlphaZero methodology has limitations\nand is unsuitable for general AI. However, none of these works has suggested\nany specific limits for any particular game.\nIn this paper, we provide more powerful bottlenecks than previously\nsuggested. We present the first concrete example of a game - namely the\n(children) game of nim - and other impartial games that seem to be a stumbling\nblock for AlphaZero and similar reinforcement learning algorithms. We show\nexperimentally that the bottlenecks apply to both the policy and value\nnetworks. Since solving nim can be done in linear time using logarithmic space\ni.e. has very low-complexity, our experimental results supersede known\ntheoretical limits based on many games' PSPACE (and NEXPTIME) completeness.\nWe show that nim can be learned on small boards, but when the board size\nincreases, AlphaZero style algorithms rapidly fail to improve.\nWe quantify the difficulties for various setups, parameter settings and\ncomputational resources. Our results might help expand the AlphaZero self-play\nparadigm by allowing it to use meta-actions during training and/or actual game\nplay like applying abstract transformations, or reading and writing to an\nexternal memory.",
    "descriptor": "",
    "authors": [
      "Bei Zhou",
      "S\u00f8ren Riis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12787"
  },
  {
    "id": "arXiv:2205.12796",
    "title": "Non-rigid Point Cloud Registration with Neural Deformation Pyramid",
    "abstract": "Non-rigid point cloud registration is a key component in many computer vision\nand computer graphics applications. The high complexity of the unknown\nnon-rigid motion make this task a challenging problem. In this paper, we break\ndown this problem via hierarchical motion decomposition. Our method called\nNeural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid\narchitecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),\ntakes as input a sinusoidally encoded 3D point and outputs its motion\nincrements from the previous level. The sinusoidal function starts with a low\ninput frequency and gradually increases when the pyramid level goes down. This\nallows a multi-level rigid to nonrigid motion decomposition and also speeds up\nthe solving by 50 times compared to the existing MLP-based approach. Our method\nachieves advanced partialto-partial non-rigid point cloud registration results\non the 4DMatch/4DLoMatch benchmark under both no-learned and supervised\nsettings.",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Yang Li",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12796"
  },
  {
    "id": "arXiv:2205.12797",
    "title": "Gradient-based explanations for Gaussian Process regression and  classification models",
    "abstract": "Gaussian Processes (GPs) have proven themselves as a reliable and effective\nmethod in probabilistic Machine Learning. Thanks to recent and current\nadvances, modeling complex data with GPs is becoming more and more feasible.\nThus, these types of models are, nowadays, an interesting alternative to Neural\nand Deep Learning methods, which are arguably the current state-of-the-art in\nMachine Learning. For the latter, we see an increasing interest in so-called\nexplainable approaches - in essence methods that aim to make a Machine Learning\nmodel's decision process transparent to humans. Such methods are particularly\nneeded when illogical or biased reasoning can lead to actual disadvantageous\nconsequences for humans. Ideally, explainable Machine Learning should help\ndetect such flaws in a model and aid a subsequent debugging process. One active\nline of research in Machine Learning explainability are gradient-based methods,\nwhich have been successfully applied to complex neural networks. Given that GPs\nare closed under differentiation, gradient-based explainability for GPs appears\nas a promising field of research. This paper is primarily focused on explaining\nGP classifiers via gradients where, contrary to GP regression, derivative GPs\nare not straightforward to obtain.",
    "descriptor": "",
    "authors": [
      "Sarem Seitz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12797"
  },
  {
    "id": "arXiv:2205.12799",
    "title": "SAT Preprocessors and Symmetry",
    "abstract": "Exploitation of symmetries is an indispensable approach to solve certain\nclasses of difficult SAT instances. Numerous techniques for the use of symmetry\nin SAT have evolved over the past few decades. But no matter how symmetries are\nused precisely, they have to be detected first. We investigate how to detect\nmore symmetry, faster. The initial idea is to reap the benefits of SAT\npreprocessing for symmetry detection. As it turns out, applying an\noff-the-shelf preprocessor before handling symmetry runs into problems: the\npreprocessor can haphazardly remove symmetry from formulas, severely impeding\nsymmetry exploitation.\nOur main contribution is a theoretical framework that captures the\nrelationship of SAT preprocessing techniques and symmetry. Based on this, we\ncreate a symmetry-aware preprocessor that can be applied safely before handling\nsymmetry. We then demonstrate that applying the preprocessor does not only\nsubstantially decrease symmetry detection and breaking times, but also uncovers\nhidden symmetry not detectable in the original instances. Overall, we depart\nthe conventional view of treating symmetry detection as a black-box, presenting\na new application-specific approach to symmetry detection in SAT.",
    "descriptor": "",
    "authors": [
      "Markus Anders"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12799"
  },
  {
    "id": "arXiv:2205.12808",
    "title": "Mirror Descent Maximizes Generalized Margin and Can Be Implemented  Efficiently",
    "abstract": "Driven by the empirical success and wide use of deep neural networks,\nunderstanding the generalization performance of overparameterized models has\nbecome an increasingly popular question. To this end, there has been\nsubstantial effort to characterize the implicit bias of the optimization\nalgorithms used, such as gradient descent (GD), and the structural properties\nof their preferred solutions. This paper answers an open question in this\nliterature: For the classification setting, what solution does mirror descent\n(MD) converge to? Specifically, motivated by its efficient implementation, we\nconsider the family of mirror descent algorithms with potential function chosen\nas the $p$-th power of the $\\ell_p$-norm, which is an important generalization\nof GD. We call this algorithm $p$-$\\textsf{GD}$. For this family, we\ncharacterize the solutions it obtains and show that it converges in direction\nto a generalized maximum-margin solution with respect to the $\\ell_p$-norm for\nlinearly separable classification. While the MD update rule is in general\nexpensive to compute and perhaps not suitable for deep learning,\n$p$-$\\textsf{GD}$ is fully parallelizable in the same manner as SGD and can be\nused to train deep neural networks with virtually no additional computational\noverhead. Using comprehensive experiments with both linear and deep neural\nnetwork models, we demonstrate that $p$-$\\textsf{GD}$ can noticeably affect the\nstructure and the generalization performance of the learned models.",
    "descriptor": "",
    "authors": [
      "Haoyuan Sun",
      "Kwangjun Ahn",
      "Christos Thrampoulidis",
      "Navid Azizan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12808"
  },
  {
    "id": "arXiv:2205.12811",
    "title": "Automatic question generation based on sentence structure analysis using  machine learning approach",
    "abstract": "Automatic question generation is one of the most challenging tasks of Natural\nLanguage Processing. It requires \"bidirectional\" language processing: firstly,\nthe system has to understand the input text (Natural Language Understanding)\nand it then has to generate questions also in the form of text (Natural\nLanguage Generation). In this article, we introduce our framework for\ngenerating the factual questions from unstructured text in the English\nlanguage. It uses a combination of traditional linguistic approaches based on\nsentence patterns with several machine learning methods. We firstly obtain\nlexical, syntactic and semantic information from an input text and we then\nconstruct a hierarchical set of patterns for each sentence. The set of features\nis extracted from the patterns and it is then used for automated learning of\nnew transformation rules. Our learning process is totally data-driven because\nthe transformation rules are obtained from a set of initial sentence-question\npairs. The advantages of this approach lie in a simple expansion of new\ntransformation rules which allows us to generate various types of questions and\nalso in the continuous improvement of the system by reinforcement learning. The\nframework also includes a question evaluation module which estimates the\nquality of generated questions. It serves as a filter for selecting the best\nquestions and eliminating incorrect ones or duplicates. We have performed\nseveral experiments to evaluate the correctness of generated questions and we\nhave also compared our system with several state-of-the-art systems. Our\nresults indicate that the quality of generated questions outperforms the\nstate-of-the-art systems and our questions are also comparable to questions\ncreated by humans. We have also created and published an interface with all\ncreated datasets and evaluated questions, so it is possible to follow up on our\nwork.",
    "descriptor": "",
    "authors": [
      "Miroslav Bl\u0161t\u00e1k",
      "Viera Rozinajov\u00e1"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12811"
  },
  {
    "id": "arXiv:2205.12816",
    "title": "P4Filter: A two level defensive mechanism against attacks in SDN using  P4",
    "abstract": "The advancements in networking technologies have led to a new paradigm of\ncontrolling networks, with data plane programmability as a basis. This facility\nopens up many advantages, such as flexibility in packet processing and better\nnetwork management, which leads to better security in the network. However, the\ncurrent literature lacks network security solutions concerning authentication\nand preventing unauthorized access. In this work, our goal is to avoid attacks\nin a two level defense mechanism (P4Filter). The first level is a dynamic\nfirewall logic, which blocks packets generated from an unauthorized source. The\nsecond level is an authentication mechanism based on dynamic port knocking. The\ntwo security levels were tested in a virtual environment with P4 based\nswitches. The packets arriving at the switch from unknown hosts are sent to the\ncontroller. The controller maintains an ACL using which it assigns rules for\nboth the levels to allow or drop the packets. For port knocking a new random\nsequence is generated for every new host. Hosts can only connect using the\ncorrect sequence assigned to them.The tests conducted show this approach\nperforms better than the previous P4 based firewall approaches due to two\nsecurity levels. Moreover, it is successful in mitigating specific security\nattacks by blocking unauthorized access to the network.",
    "descriptor": "",
    "authors": [
      "Ananya Saxena",
      "Ritvik Muttreja",
      "Shivam Upadhyay",
      "K. Shiv Kumar",
      "Dr Venkanna U"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12816"
  },
  {
    "id": "arXiv:2205.12818",
    "title": "On Building Spoken Language Understanding Systems for Low Resourced  Languages",
    "abstract": "Spoken dialog systems are slowly becoming and integral part of the human\nexperience due to their various advantages over textual interfaces. Spoken\nlanguage understanding (SLU) systems are fundamental building blocks of spoken\ndialog systems. But creating SLU systems for low resourced languages is still a\nchallenge. In a large number of low resourced language, we don't have access to\nenough data to build automatic speech recognition (ASR) technologies, which are\nfundamental to any SLU system. Also, ASR based SLU systems do not generalize to\nunwritten languages. In this paper, we present a series of experiments to\nexplore extremely low-resourced settings where we perform intent classification\nwith systems trained on as low as one data-point per intent and with only one\nspeaker in the dataset. We also work in a low-resourced setting where we do not\nuse language specific ASR systems to transcribe input speech, which compounds\nthe challenge of building SLU systems to simulate a true low-resourced setting.\nWe test our system on Belgian Dutch (Flemish) and English and find that using\nphonetic transcriptions to make intent classification systems in such\nlow-resourced setting performs significantly better than using speech features.\nSpecifically, when using a phonetic transcription based system over a feature\nbased system, we see average improvements of 12.37% and 13.08% for binary and\nfour-class classification problems respectively, when averaged over 49\ndifferent experimental settings.",
    "descriptor": "",
    "authors": [
      "Akshat Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.12818"
  },
  {
    "id": "arXiv:2205.12821",
    "title": "The complexity of blocking (semi)total dominating sets with edge  contractions",
    "abstract": "We consider the problem of reducing the (semi)total domination number of\ngraph by one by contracting edges. It is known that this can always be done\nwith at most three edge contractions and that deciding whether one edge\ncontraction suffices is an $\\mathsf{NP}$-hard problem. We show that for every\nfixed $k \\in \\{2,3\\}$, deciding whether exactly $k$ edge contractions are\nnecessary is $\\mathsf{NP}$-hard and further provide for $k=2$ complete\ncomplexity dichotomies on monogenic graph classes.",
    "descriptor": "\nComments: 38 pages, 13 figures\n",
    "authors": [
      "Esther Galby"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12821"
  },
  {
    "id": "arXiv:2205.12823",
    "title": "Real-time Visualization of Stream-based Monitoring Data",
    "abstract": "Stream-based runtime monitors are used in safety-critical applications such\nas Unmanned Aerial Systems (UAS) to compute comprehensive statistics and\nlogical assessments of system health that provide the human operator with\ncritical information in hand-over situations. In such applications, a visual\ndisplay of the monitoring data can be much more helpful than the textual alerts\nprovided by a more traditional user interface. This visualization requires\nextensive real-time data processing, which includes the synchronization of data\nfrom different streams, filtering and aggregation, and priorization and\nmanagement of user attention. We present a visualization approach for the\n\\rtlola monitoring framework. Our approach is based on the principle that the\nnecessary data processing is the responsibility of the monitor itself, rather\nthan the responsibility of some external visualization tool. We show how the\nvarious aspects of the data transformation can be described as RTLola stream\nequations and linked to the visualization component through a bidirectional\nsynchronous interface. In our experience, this approach leads to highly\ninformative visualizations as well as to understandable and easily maintainable\nmonitoring code.",
    "descriptor": "",
    "authors": [
      "Jan Baumeister",
      "Bernd Finkbeiner",
      "Stefan Gumhold",
      "Malte Schledjewski"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2205.12823"
  },
  {
    "id": "arXiv:2205.12825",
    "title": "A Paradigm Change for Formal Syntax: Computational Algorithms in the  Grammar of English",
    "abstract": "Language sciences rely less and less on formal syntax as their base. The\nreason is probably its lack of psychological reality, knowingly avoided.\nPhilosophers of science call for a paradigm shift in which explanations are by\nmechanisms, as in biology. We turned to programming languages as heuristic\nmodels for a process-based syntax of English. The combination of a functional\nword and a content word was chosen as the topic of modeling. Such combinations\nare very frequent, and their output is the important immediate constituents of\nsentences. We found their parallel in Object Oriented Programming where an\nall-methods element serves as an interface, and the content-full element serves\nas its implementation, defining computational objects. The fit of the model was\ntested by deriving three functional characteristics crucial for the algorithm\nand checking their presence in English grammar. We tested the reality of the\ninterface-implementation mechanism on psycholinguistic and neurolinguistic\nevidence concerning processing, development and loss of syntax. The close fit\nand psychological reality of the mechanism suggests that a paradigm shift to an\nalgorithmic theory of syntax is a possibility.",
    "descriptor": "\nComments: 54 pages\n",
    "authors": [
      "Anat Ninio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12825"
  },
  {
    "id": "arXiv:2205.12828",
    "title": "An Asymptotic $\\left(\\frac{4}{3}+\\varepsilon\\right)$-Approximation for  the 2-Dimensional Vector Bin Packing Problem",
    "abstract": "We study the $2$-Dimensional Vector Bin Packing Problem (2VBP), a\ngeneralization of classic Bin Packing that is widely applicable in resource\nallocation and scheduling. In 2VBP we are given a set of items, where each item\nis associated with a two-dimensional volume vector. The objective is to\npartition the items into a minimal number of subsets (bins), such that the\ntotal volume of items in each subset is at most $1$ in each dimension.\nWe give an asymptotic $\\left(\\frac{4}{3}+\\varepsilon\\right)$-approximation\nfor the problem, thus improving upon the best known asymptotic ratio of\n$\\left(1+\\ln \\frac{3}{2}+\\varepsilon\\right)\\approx 1.406$ due to Bansal, Elias\nand Khan (SODA 2016). Our algorithm applies a novel Round&Round approach which\niteratively solves a configuration LP relaxation for the residual instance and\nsamples a small number of configurations based on the solution for the\nconfiguration LP. For the analysis we derive an iteration-dependent upper bound\non the solution size for the configuration LP, which holds with high\nprobability. To facilitate the analysis, we introduce key structural properties\nof 2VBP instances, leveraging the recent fractional grouping technique of\nFairstein et al. (ESA 2021).",
    "descriptor": "",
    "authors": [
      "Ariel Kulik",
      "Matthias Mnich",
      "Hadas Shachnai"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12828"
  },
  {
    "id": "arXiv:2205.12830",
    "title": "How to Wake Up Your Neighbors: Safe and Nearly Optimal Generic Energy  Conservation in Radio Networks",
    "abstract": "Recent work has shown that it is sometimes feasible to significantly reduce\nthe energy usage of some radio-network algorithms by adaptively powering down\nthe radio receiver when it is not needed. Although past work has focused on\nmodifying specific network algorithms in this way, we now ask the question of\nwhether this problem can be solved in a generic way, treating the algorithm as\na kind of black box.\nWe are able to answer this question in the affirmative, presenting a new\ngeneral way to modify arbitrary radio-network algorithms in an attempt to save\nenergy. At the expense of a small increase in the time complexity, we can\nprovably reduce the energy usage to an extent that is provably nearly optimal\nwithin a certain class of general-purpose algorithms.\nAs an application, we show that our algorithm reduces the energy cost of\nbreadth-first search in radio networks from the previous best bound of\n$2^{O(\\sqrt{\\log n})}$ to $\\mathrm{polylog}(n)$, where $n$ is the number of\nnodes in the network\nA key ingredient in our algorithm is hierarchical clustering based on\nadditive Voronoi decomposition done at multiple scales. Similar clustering\nalgorithms have been used in other recent work on energy-aware computation in\nradio networks, but we believe the specific approach presented here may be of\nindependent interest.",
    "descriptor": "",
    "authors": [
      "Varsha Dani",
      "Thomas P. Hayes"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12830"
  },
  {
    "id": "arXiv:2205.12840",
    "title": "DistillAdapt: Source-Free Active Visual Domain Adaptation",
    "abstract": "We present a novel method, DistillAdapt, for the challenging problem of\nSource-Free Active Domain Adaptation (SF-ADA). The problem requires adapting a\npretrained source domain network to a target domain, within a provided budget\nfor acquiring labels in the target domain, while assuming that the source data\nis not available for adaptation due to privacy concerns or otherwise.\nDistillAdapt is one of the first approaches for SF-ADA, and holistically\naddresses the challenges of SF-ADA via a novel Guided Attention Transfer\nNetwork (GATN) and an active learning heuristic, H_AL. The GATN enables\nselective distillation of features from the pre-trained network to the target\nnetwork using a small subset of annotated target samples mined by H_AL. H_AL\nacquires samples at batch-level and balances transfer-ability from the\npre-trained network and uncertainty of the target network. DistillAdapt is\ntask-agnostic, and can be applied across visual tasks such as classification,\nsegmentation and detection. Moreover, DistillAdapt can handle shifts in output\nlabel space. We conduct experiments and extensive ablation studies across 3\nvisual tasks, viz. digits classification (MNIST, SVHN), synthetic (GTA5) to\nreal (CityScapes) image segmentation, and document layout detection (PubLayNet\nto DSSE). We show that our source-free approach, DistillAdapt, results in an\nimprovement of 0.5% - 31.3% (across datasets and tasks) over prior adaptation\nmethods that assume access to large amounts of annotated source data for\nadaptation.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Divya Kothandaraman",
      "Sumit Shekhar",
      "Abhilasha Sancheti",
      "Manoj Ghuhan",
      "Tripti Shukla",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12840"
  },
  {
    "id": "arXiv:2205.12850",
    "title": "A Universal Error Measure for Input Predictions Applied to Online Graph  Problems",
    "abstract": "We introduce a novel measure for quantifying the error in input predictions.\nThe error is based on a minimum-cost hyperedge cover in a suitably defined\nhypergraph and provides a general template which we apply to online graph\nproblems. The measure captures errors due to absent predicted requests as well\nas unpredicted actual requests; hence, predicted and actual inputs can be of\narbitrary size. We achieve refined performance guarantees for previously\nstudied network design problems in the online-list model, such as Steiner tree\nand facility location. Further, we initiate the study of learning-augmented\nalgorithms for online routing problems, such as the traveling salesperson\nproblem and dial-a-ride problem, where (transportation) requests arrive over\ntime (online-time model). We provide a general algorithmic framework and we\ngive error-dependent performance bounds that improve upon known worst-case\nbarriers, when given accurate predictions, at the cost of slightly increased\nworst-case bounds when given predictions of arbitrary quality.",
    "descriptor": "",
    "authors": [
      "Giulia Bernardini",
      "Alexander Lindermayr",
      "Alberto Marchetti-Spaccamela",
      "Nicole Megow",
      "Leen Stougie",
      "Michelle Sweering"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12850"
  },
  {
    "id": "arXiv:2205.12853",
    "title": "Deep Gradient Learning for Efficient Camouflaged Object Detection",
    "abstract": "This paper introduces DGNet, a novel deep framework that exploits object\ngradient supervision for camouflaged object detection (COD). It decouples the\ntask into two connected branches, i.e., a context and a texture encoder. The\nessential connection is the gradient-induced transition, representing a soft\ngrouping between context and texture features. Benefiting from the simple but\nefficient framework, DGNet outperforms existing state-of-the-art COD models by\na large margin. Notably, our efficient version, DGNet-S, runs in real-time (80\nfps) and achieves comparable results to the cutting-edge model\nJCSOD-CVPR$_{21}$ with only 6.82% parameters. Application results also show\nthat the proposed DGNet performs well in polyp segmentation, defect detection,\nand transparent object segmentation tasks. Codes will be made available at\nhttps://github.com/GewelsJI/DGNet.",
    "descriptor": "\nComments: Technical Report\n",
    "authors": [
      "Ge-Peng Ji",
      "Deng-Ping Fan",
      "Yu-Cheng Chou",
      "Dengxin Dai",
      "Alexander Liniger",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12853"
  },
  {
    "id": "arXiv:2205.12854",
    "title": "Understanding Factual Errors in Summarization: Errors, Summarizers,  Datasets, Error Detectors",
    "abstract": "The propensity of abstractive summarization systems to make factual errors\nhas been the subject of significant study, including work on models to detect\nfactual errors and annotation of errors in current systems' outputs. However,\nthe ever-evolving nature of summarization systems, error detectors, and\nannotated benchmarks make factuality evaluation a moving target; it is hard to\nget a clear picture of how techniques compare. In this work, we collect labeled\nfactuality errors from across nine datasets of annotated summary outputs and\nstratify them in a new way, focusing on what kind of base summarization model\nwas used. To support finer-grained analysis, we unify the labeled error types\ninto a single taxonomy and project each of the datasets' errors into this\nshared labeled space. We then contrast five state-of-the-art error detection\nmethods on this benchmark. Our findings show that benchmarks built on modern\nsummary outputs (those from pre-trained models) show significantly different\nresults than benchmarks using pre-Transformer models. Furthermore, no one\nfactuality technique is superior in all settings or for all error types,\nsuggesting that system developers should take care to choose the right system\nfor their task at hand.",
    "descriptor": "\nComments: 11 pages (15 with references and appendix), 4 figures, 8 Tables\n",
    "authors": [
      "Liyan Tang",
      "Tanya Goyal",
      "Alexander R. Fabbri",
      "Philippe Laban",
      "Jiacheng Xu",
      "Semih Yahvuz",
      "Wojciech Kry\u015bci\u0144ski",
      "Justin F. Rousseau",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12854"
  },
  {
    "id": "arXiv:2205.12856",
    "title": "Stochastic Second-Order Methods Provably Beat SGD For Gradient-Dominated  Functions",
    "abstract": "We study the performance of Stochastic Cubic Regularized Newton (SCRN) on a\nclass of functions satisfying gradient dominance property which holds in a wide\nrange of applications in machine learning and signal processing. This condition\nensures that any first-order stationary point is a global optimum. We prove\nthat SCRN improves the best-known sample complexity of stochastic gradient\ndescent in achieving $\\epsilon$-global optimum by a factor of\n$\\mathcal{O}(\\epsilon^{-1/2})$. Even under a weak version of gradient dominance\nproperty, which is applicable to policy-based reinforcement learning (RL), SCRN\nachieves the same improvement over stochastic policy gradient methods.\nAdditionally, we show that the sample complexity of SCRN can be improved by a\nfactor of ${\\mathcal{O}}(\\epsilon^{-1/2})$ using a variance reduction method\nwith time-varying batch sizes. Experimental results in various RL settings\nshowcase the remarkable performance of SCRN compared to first-order methods.",
    "descriptor": "",
    "authors": [
      "Saeed Masiha",
      "Saber Salehkaleybar",
      "Niao He",
      "Negar Kiyavash",
      "Patrick Thiran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.12856"
  },
  {
    "id": "arXiv:2205.12858",
    "title": "Worldwide Energy Harvesting Potential of Hybrid CPV/PV Technology",
    "abstract": "Hybridization of multi-junction concentrator photovoltaics with\nsingle-junction flat plate solar cells (CPV/PV) can deliver the highest power\noutput per module area of any PV technology. Conversion efficiencies up to\n34.2% have been published under the AM1.5g spectrum at standard test conditions\nfor the EyeCon module which combines Fresnel lenses and III-V four-junction\nsolar cells with bifacial c-Si. We investigate here its energy yield and\ncompare it to conventional CPV as well as flat plate PV. The advantage of the\nhybrid CPV/PV module is that it converts direct sunlight with the most advanced\nmulti-junction cell technology, while accessing diffuse, lens-scattered and\nback side irradiance with a Si cell that also serves as the heat distributor\nfor the concentrator cells. This article quantifies that hybrid bifacial CPV/PV\nmodules are expected to generate a 25 - 35% higher energy yield with respect to\ntheir closest competitor in regions with a diffuse irradiance fraction around\n50%. Additionally, the relative cost of electricity generated by hybrid CPV/PV\ntechnology was calculated worldwide under certain economic assumptions.\nTherefore, this article gives clear guidance towards establishing competitive\nbusiness cases for the technology.",
    "descriptor": "",
    "authors": [
      "Juan F. Mart\u00ednez",
      "Marc Steiner",
      "Maike Wiesenfarth",
      "Henning Helmers",
      "Gerald Siefer",
      "Stefan W. Glunz",
      "Frank Dimroth"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12858"
  },
  {
    "id": "arXiv:2205.12867",
    "title": "Image Colorization using U-Net with Skip Connections and Fusion Layer on  Landscape Images",
    "abstract": "We present a novel technique to automatically colorize grayscale images that\ncombine the U-Net model and Fusion Layer features. This approach allows the\nmodel to learn the colorization of images from pre-trained U-Net. Moreover, the\nFusion layer is applied to merge local information results dependent on small\nimage patches with global priors of an entire image on each class, forming\nvisually more compelling colorization results. Finally, we validate our\napproach with a user study evaluation and compare it against state-of-the-art,\nresulting in improvements.",
    "descriptor": "",
    "authors": [
      "Muhammad Hisyam Zayd",
      "Novanto Yudistira",
      "Randy Cahya Wihandika"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12867"
  },
  {
    "id": "arXiv:2205.12869",
    "title": "Over-the-Air Federated Learning with Energy Harvesting Devices",
    "abstract": "We consider federated edge learning (FEEL) among mobile devices that harvest\nthe required energy from their surroundings, and share their updates with the\nparameter server (PS) through a shared wireless channel. In particular, we\nconsider energy harvesting FL with over-the-air (OTA) aggregation, where the\nparticipating devices perform local computations and wireless transmission only\nwhen they have the required energy available, and transmit the local updates\nsimultaneously over the same channel bandwidth. In order to prevent bias among\nheterogeneous devices, we utilize a weighted averaging with respect to their\nlatest energy arrivals and data cardinalities. We provide a convergence\nanalysis and carry out numerical experiments with different energy arrival\nprofiles, which show that even though the proposed scheme is robust against\ndevices with heterogeneous energy arrivals in error-free scenarios, we observe\na 5-10% performance loss in energy harvesting OTA FL.",
    "descriptor": "\nComments: 7 pages, 3 figures\n",
    "authors": [
      "Ozan Ayg\u00fcn",
      "Mohammad Kazemi",
      "Deniz G\u00fcnd\u00fcz",
      "Tolga M. Duman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.12869"
  },
  {
    "id": "arXiv:2205.12870",
    "title": "Open-Domain Sign Language Translation Learned from Online Video",
    "abstract": "Existing work on sign language translation--that is, translation from sign\nlanguage videos into sentences in a written language--has focused mainly on (1)\ndata collected in a controlled environment or (2) data in a specific domain,\nwhich limits the applicability to real-world settings. In this paper, we\nintroduce OpenASL, a large-scale ASL-English dataset collected from online\nvideo sites (e.g., YouTube). OpenASL contains 288 hours of ASL videos in\nvarious domains (news, VLOGs, etc.) from over 200 signers and is the largest\npublicly available ASL translation dataset to date. To tackle the challenges of\nsign language translation in realistic settings and without glosses, we propose\na set of techniques including sign search as a pretext task for pre-training\nand fusion of mouthing and handshape features. The proposed techniques produce\nconsistent and large improvements in translation quality, over baseline models\nbased on prior work. Our data, code and model will be publicly available at\nhttps://github.com/chevalierNoir/OpenASL",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Bowen Shi",
      "Diane Brentari",
      "Greg Shakhnarovich",
      "Karen Livescu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12870"
  },
  {
    "id": "arXiv:2205.12879",
    "title": "Understanding Programmatic Weak Supervision via Source-aware Influence  Function",
    "abstract": "Programmatic Weak Supervision (PWS) aggregates the source votes of multiple\nweak supervision sources into probabilistic training labels, which are in turn\nused to train an end model. With its increasing popularity, it is critical to\nhave some tool for users to understand the influence of each component (e.g.,\nthe source vote or training data) in the pipeline and interpret the end model\nbehavior. To achieve this, we build on Influence Function (IF) and propose\nsource-aware IF, which leverages the generation process of the probabilistic\nlabels to decompose the end model's training objective and then calculate the\ninfluence associated with each (data, source, class) tuple. These primitive\ninfluence score can then be used to estimate the influence of individual\ncomponent of PWS, such as source vote, supervision source, and training data.\nOn datasets of diverse domains, we demonstrate multiple use cases: (1)\ninterpreting incorrect predictions from multiple angles that reveals insights\nfor debugging the PWS pipeline, (2) identifying mislabeling of sources with a\ngain of 9%-37% over baselines, and (3) improving the end model's generalization\nperformance by removing harmful components in the training objective (13%-24%\nbetter than ordinary IF).",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Jieyu Zhang",
      "Haonan Wang",
      "Cheng-Yu Hsieh",
      "Alexander Ratner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12879"
  },
  {
    "id": "arXiv:2205.12880",
    "title": "Trust-based Consensus in Multi-Agent Reinforcement Learning Systems",
    "abstract": "An often neglected issue in multi-agent reinforcement learning (MARL) is the\npotential presence of unreliable agents in the environment whose deviations\nfrom expected behavior can prevent a system from accomplishing its intended\ntasks. In particular, consensus is a fundamental underpinning problem of\ncooperative distributed multi-agent systems. Consensus requires different\nagents, situated in a decentralized communication network, to reach an\nagreement out of a set of initial proposals that they put forward.\nLearning-based agents should adopt a protocol that allows them to reach\nconsensus despite having one or more unreliable agents in the system. This\npaper investigates the problem of unreliable agents in MARL, considering\nconsensus as case study. Echoing established results in the distributed systems\nliterature, our experiments show that even a moderate fraction of such agents\ncan greatly impact the ability of reaching consensus in a networked\nenvironment. We propose Reinforcement Learning-based Trusted Consensus (RLTC),\na decentralized trust mechanism, in which agents can independently decide which\nneighbors to communicate with. We empirically demonstrate that our trust\nmechanism is able to deal with unreliable agents effectively, as evidenced by\nhigher consensus success rates.",
    "descriptor": "\nComments: 18 pages, 17 figures\n",
    "authors": [
      "Ho Long Fung",
      "Victor-Alexandru Darvariu",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12880"
  },
  {
    "id": "arXiv:2205.12883",
    "title": "SOPHIE: SOft and flexible aerial vehicle for PHysical Interaction with  the Environment",
    "abstract": "This paper presents the first design of a soft, 3D-printed in flexible\nfilament, lightweight UAV, capable of adapting to the industrial environment\nusing soft tendons, specifically landing and stabilizing on pipelines without\nthe need for an auxiliary system. The flexibility of the UAV can be controlled\nduring the additive manufacturing process by adjusting the infill rate\ndistribution. However, the increase in flexibility implies difficulties in\ncontrolling the UAV, as well as structural, aerodynamic and aeroelastic\neffects. This article provides insight into the dynamics of the system and\nvalidates the flyability of the vehicle for densities as low as 6%, enough to\nconsider the vehicle as \"soft\". At lower densities, strong non-linear dynamics\nappear, which translates to complex modeling, and it is suggested to switch to\ndata-based approaches.",
    "descriptor": "\nComments: Submitted to Robotics and Automation Letters\n",
    "authors": [
      "Fernando Ruiz",
      "Bego\u00f1a Arrue",
      "An\u00edbal Ollero"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12883"
  },
  {
    "id": "arXiv:2205.12886",
    "title": "You Need to Read Again: Multi-granularity Perception Network for Moment  Retrieval in Videos",
    "abstract": "Moment retrieval in videos is a challenging task that aims to retrieve the\nmost relevant video moment in an untrimmed video given a sentence description.\nPrevious methods tend to perform self-modal learning and cross-modal\ninteraction in a coarse manner, which neglect fine-grained clues contained in\nvideo content, query context, and their alignment. To this end, we propose a\nnovel Multi-Granularity Perception Network (MGPN) that perceives intra-modality\nand inter-modality information at a multi-granularity level. Specifically, we\nformulate moment retrieval as a multi-choice reading comprehension task and\nintegrate human reading strategies into our framework. A coarse-grained feature\nencoder and a co-attention mechanism are utilized to obtain a preliminary\nperception of intra-modality and inter-modality information. Then a\nfine-grained feature encoder and a conditioned interaction module are\nintroduced to enhance the initial perception inspired by how humans address\nreading comprehension problems. Moreover, to alleviate the huge computation\nburden of some existing methods, we further design an efficient choice\ncomparison module and reduce the hidden size with imperceptible quality loss.\nExtensive experiments on Charades-STA, TACoS, and ActivityNet Captions datasets\ndemonstrate that our solution outperforms existing state-of-the-art methods.",
    "descriptor": "\nComments: in SIGIR 2022\n",
    "authors": [
      "Xin Sun",
      "Xuan Wang",
      "Jialin Gao",
      "Qiong Liu",
      "Xi Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12886"
  },
  {
    "id": "arXiv:2205.12887",
    "title": "SPANSE: combining sparsity with density for efficient one-time  code-based digital signatures",
    "abstract": "The use of codes defined by sparse characteristic matrices, like QC-LDPC and\nQC-MDPC codes, has become an established solution to design secure and\nefficient code-based public-key encryption schemes, as also witnessed by the\nongoing NIST post-quantum cryptography standardization process. However,\nsimilar approaches have been less fortunate in the context of code-based\ndigital signatures, since no secure and efficient signature scheme based on\nthese codes is available to date. The main limitation of previous attempts in\nthis line of research has been the use of sparse signatures, which produces\nsome leakage of information about the private key. In this paper, we propose a\nnew code-based digital signature scheme that overcomes such a problem by\npublishing signatures that are abnormally dense, rather than sparse. This\neliminates the possibility of deducing information from the sparsity of\nsignatures, and follows a recent trend in code-based cryptography exploiting\nthe hardness of the decoding problem for large-weight vectors, instead of its\nclassical version based on small-weight vectors. In this study we focus on\none-time use and provide some preliminary instances of the new scheme, showing\nthat it achieves very fast signature generation and verification with\nreasonably small public keys.",
    "descriptor": "\nComments: 19 pages, 1 figure\n",
    "authors": [
      "Marco Baldi",
      "Franco Chiaraluce",
      "Paolo Santini"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12887"
  },
  {
    "id": "arXiv:2205.12888",
    "title": "Robust Reinforcement Learning on Graphs for Logistics optimization",
    "abstract": "Logistics optimization nowadays is becoming one of the hottest areas in the\nAI community. In the past year, significant advancements in the domain were\nachieved by representing the problem in a form of graph. Another promising area\nof research was to apply reinforcement learning algorithms to the above task.\nIn our work, we made advantage of using both approaches and apply reinforcement\nlearning on a graph. To do that, we have analyzed the most recent results in\nboth fields and selected SOTA algorithms both from graph neural networks and\nreinforcement learning. Then, we combined selected models on the problem of\nAMOD systems optimization for the transportation network of New York city. Our\nteam compared three algorithms - GAT, Pro-CNN and PTDNet - to bring to the fore\nthe important nodes on a graph representation. Finally, we achieved SOTA\nresults on AMOD systems optimization problem employing PTDNet with GNN and\ntraining them in reinforcement fashion.\nKeywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement\nLearning",
    "descriptor": "\nComments: Keywords: Graph Neural Network (GNN), Logistics optimization, Reinforcement Learning\n",
    "authors": [
      "Zangir Iklassov",
      "Dmitrii Medvedev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12888"
  },
  {
    "id": "arXiv:2205.12891",
    "title": "Scheduling to Optimize Sojourn Time of Successful Jobs",
    "abstract": "Deep neural networks training jobs and other iterative computations\nfrequently include checkpoints where jobs can be canceled based on the current\nvalue of monitored metrics. While most of existing results focus on the\nperformance of all jobs (both successfully completed and canceled), in this\nwork we explore scheduling policies that improve the sojourn time of successful\njobs, which are typically more valuable to the user. Our model assumes that\neach job has a known discrete size distribution (e.g., estimated from previous\nexecution logs) where the largest size value indicates a successful completion,\nwhile other size values correspond to termination checkpoints. In the\nsingle-server case where all jobs are available for scheduling simultaneously,\nwe prove that optimal schedules do not preempt jobs, even when preemption\noverhead is negligible. Based on this, we develop a scheduling policy that\nminimizes the sojourn time of successful jobs asymptotically, i.e., when the\nnumber of jobs grows to infinity. Through an extensive numerical study, we show\nthat this policy performs better than existing alternatives even when the\nnumber of jobs is finite. For more realistic scenarios with multiple servers\nand dynamic jobs arrivals, we propose an online approach based on our\nsingle-server scheduling policy. Through an extensive simulation study, using\nreal-world traces, we demonstrate that this online approach results in better\naverage sojourn time for successful jobs as compared to existing techniques.",
    "descriptor": "",
    "authors": [
      "Yuan Yao",
      "Marco Paolieri",
      "Leana Golubchik"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.12891"
  },
  {
    "id": "arXiv:2205.12895",
    "title": "On a large-stepsize integrator for charged-particle dynamics",
    "abstract": "Xiao and Qin [Computer Physics Comm., 265:107981, 2021] recently proposed a\nremarkably simple modification of the Boris algorithm to compute the guiding\ncentre of the highly oscillatory motion of a charged particle with step sizes\nthat are much larger than the period of gyrorotations. They gave strong\nnumerical evidence but no error analysis. This paper provides an analysis of\nthe large-stepsize modified Boris method in a setting that has a strong\nnon-uniform magnetic field and moderately bounded velocities, considered over a\nfixed finite time interval. The error analysis is based on comparing the\nmodulated Fourier expansions of the exact and numerical solutions, for which\nthe differential equations of the dominant terms are derived explicitly.\nNumerical experiments illustrate and complement the theoretical results.",
    "descriptor": "",
    "authors": [
      "Christian Lubich",
      "Yanyan Shi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Plasma Physics (physics.plasm-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.12895"
  },
  {
    "id": "arXiv:2205.12897",
    "title": "Cryptocurreny Giveaway Scam with YouTube Live Stream",
    "abstract": "This paper investigates the cryptocurrency giveaway scam with the YouTube\nlive stream carried out on 5/15/2022 and 5/16/2022. In this scam scheme, the\nscammer plays a recorded video of a famous person in a YouTube live stream\nannotated with a cryptocurrency giveaway announcement. In the annotated\nannouncement, the victims are directed to the scammer's webpage. The scammer's\nwebpage is designed intelligently to deceive victims such that they believe the\nlegitimacy of the giveaway. The scammer claims that whatever donation the\nvictim sends to a cryptocurrency wallet address, the giveaway scheme will\ndouble the donated amount and immediately send it back to the victim. By\nanalyzing the scammers' wallet addresses, it can be seen that scammers could\nsteal a significant amount of money in a short time. After analyzing the\nattackers' techniques, tactics, and procedures, this paper discusses the\ncountermeasures that can be applied to mitigate such a fraudulent activity in\nthe future.",
    "descriptor": "",
    "authors": [
      "Iman Vakilinia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12897"
  },
  {
    "id": "arXiv:2205.12898",
    "title": "Reasoning over Logically Interacted Conditions for Question Answering",
    "abstract": "Some questions have multiple answers that are not equally correct, i.e.\nanswers are different under different conditions. Conditions are used to\ndistinguish answers as well as to provide additional information to support\nthem. In this paper, we study a more challenging task where answers are\nconstrained by a list of conditions that logically interact, which requires\nperforming logical reasoning over the conditions to determine the correctness\nof the answers. Even more challenging, we only provide evidences for a subset\nof the conditions, so some questions may not have deterministic answers. In\nsuch cases, models are asked to find probable answers and identify conditions\nthat need to be satisfied to make the answers correct. We propose a new model,\nTReasoner, for this challenging reasoning task. TReasoner consists of an\nentailment module, a reasoning module, and a generation module (if the answers\nare free-form text spans). TReasoner achieves state-of-the-art performance on\ntwo benchmark conditional QA datasets, outperforming the previous\nstate-of-the-art by 3-10 points.",
    "descriptor": "",
    "authors": [
      "Haitian Sun",
      "William W. Cohen",
      "Ruslan Salakhutdinov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12898"
  },
  {
    "id": "arXiv:2205.12901",
    "title": "Fairness of Exposure in Light of Incomplete Exposure Estimation",
    "abstract": "Fairness of exposure is a commonly used notion of fairness for ranking\nsystems. It is based on the idea that all items or item groups should get\nexposure proportional to the merit of the item or the collective merit of the\nitems in the group. Often, stochastic ranking policies are used to ensure\nfairness of exposure. Previous work unrealistically assumes that we can\nreliably estimate the expected exposure for all items in each ranking produced\nby the stochastic policy. In this work, we discuss how to approach fairness of\nexposure in cases where the policy contains rankings of which, due to\ninter-item dependencies, we cannot reliably estimate the exposure distribution.\nIn such cases, we cannot determine whether the policy can be considered fair.\nOur contributions in this paper are twofold. First, we define a method called\nFELIX for finding stochastic policies that avoid showing rankings with unknown\nexposure distribution to the user without having to compromise user utility or\nitem fairness. Second, we extend the study of fairness of exposure to the top-k\nsetting and also assess FELIX in this setting. We find that FELIX can\nsignificantly reduce the number of rankings with unknown exposure distribution\nwithout a drop in user utility or fairness compared to existing fair ranking\nmethods, both for full-length and top-k rankings. This is an important first\nstep in developing fair ranking methods for cases where we have incomplete\nknowledge about the user's behaviour.",
    "descriptor": "\nComments: 12 pages, 2 figures, will be published at SIGIR 2022\n",
    "authors": [
      "Maria Heuss",
      "Fatemeh Sarvi",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12901"
  },
  {
    "id": "arXiv:2205.12903",
    "title": "Information Set Decoding for Lee-Metric Codes using Restricted Balls",
    "abstract": "The Lee metric syndrome decoding problem is an NP-hard problem and several\ngeneric decoders have been proposed. The observation that such decoders come\nwith a larger cost than their Hamming metric counterparts make the Lee metric a\npromising alternative for classical code-based cryptography. Unlike in the\nHamming metric, an error vector that is chosen uniform at random of a given Lee\nweight is expected to have only few entries with large Lee weight. Using this\nexpected distribution of entries, we are able to drastically decrease the cost\nof generic decoders in the Lee metric, by reducing the original problem to a\nsmaller instance, whose solution lives in restricted balls.",
    "descriptor": "",
    "authors": [
      "Jessica Bariffi",
      "Karan Khathuria",
      "Violetta Weger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12903"
  },
  {
    "id": "arXiv:2205.12904",
    "title": "A Neural Tangent Kernel Formula for Ensembles of Soft Trees with  Arbitrary Architectures",
    "abstract": "A soft tree is an actively studied variant of a decision tree that updates\nsplitting rules using the gradient method. Although it can have various tree\narchitectures, the theoretical properties of their impact are not well known.\nIn this paper, we formulate and analyze the Neural Tangent Kernel (NTK) induced\nby soft tree ensembles for arbitrary tree architectures. This kernel leads to\nthe remarkable finding that only the number of leaves at each depth is relevant\nfor the tree architecture in ensemble learning with infinitely many trees. In\nother words, if the number of leaves at each depth is fixed, the training\nbehavior in function space and the generalization performance are exactly the\nsame across different tree architectures, even if they are not isomorphic. We\nalso show that the NTK of asymmetric trees like decision lists does not\ndegenerate when they get infinitely deep. This is in contrast to the perfect\nbinary trees, whose NTK is known to degenerate and leads to worse\ngeneralization performance for deeper trees.",
    "descriptor": "",
    "authors": [
      "Ryuichi Kanoh",
      "Mahito Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12904"
  },
  {
    "id": "arXiv:2205.12905",
    "title": "Analytics of Business Time Series Using Machine Learning and Bayesian  Inference",
    "abstract": "In the survey we consider the case studies on sales time series forecasting,\nthe deep learning approach for forecasting non-stationary time series using\ntime trend correction, dynamic price and supply optimization using Q-learning,\nBitcoin price modeling, COVID-19 spread impact on stock market, using social\nnetworks signals in analytics. The use of machine learning and Bayesian\ninference in predictive analytics has been analyzed.",
    "descriptor": "\nComments: Survey article. arXiv admin note: text overlap with arXiv:2201.02034, arXiv:2201.02058, arXiv:2201.02729, arXiv:2201.02049, arXiv:2004.01489\n",
    "authors": [
      "Bohdan M. Pavlyshenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12905"
  },
  {
    "id": "arXiv:2205.12907",
    "title": "Highly efficient energy-conserving moment method for the  multi-dimensional Vlasov-Maxwell system",
    "abstract": "We present an energy-conserving numerical scheme to solve the Vlasov-Maxwell\n(VM) system based on the regularized moment method proposed in [Z. Cai, Y. Fan,\nand R. Li. CPAM, 2014]. The globally hyperbolic moment system is deduced for\nthe multi-dimensional VM system under the framework of the Hermite expansions,\nwhere the expansion center and the scaling factor are set as the macroscopic\nvelocity and local temperature, respectively. Thus, the effect of the Lorentz\nforce term could be reduced into several ODEs about the macroscopic velocity\nand the moment coefficients of higher order, which could significantly reduce\nthe computational cost of the whole system. An energy-conserving numerical\nscheme is proposed to solve the moment equations and the Maxwell equations,\nwhere only a linear equation system needs to be solved. Several numerical\nexamples such as the two-stream instability, Weibel instability, and the\ntwo-dimensional Orszag Tang vortex problem are studied to validate the\nefficiency and excellent energy-preserving property of the numerical scheme.",
    "descriptor": "",
    "authors": [
      "Tianai Yin",
      "Xinghui Zhong",
      "Yanli Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12907"
  },
  {
    "id": "arXiv:2205.12910",
    "title": "NaturalProver: Grounded Mathematical Proof Generation with Language  Models",
    "abstract": "Theorem proving in natural mathematical language - the mixture of symbolic\nand natural language used by humans - plays a central role in mathematical\nadvances and education, and tests aspects of reasoning that are core to\nintelligence. Yet it has remained underexplored with modern generative models.\nWe study large-scale language models on two new generation tasks: suggesting\nthe next step in a mathematical proof, and full proof generation. Naively\napplying language models to these problems yields proofs riddled with\nhallucinations and logical incoherence. We develop NaturalProver, a language\nmodel that generates proofs by conditioning on background references (e.g.\ntheorems and definitions that are either retrieved or human-provided), and\noptionally enforces their presence with constrained decoding. On theorems from\nthe NaturalProofs benchmark, NaturalProver improves the quality of next-step\nsuggestions and generated proofs over fine-tuned GPT-3, according to human\nevaluations from university-level mathematics students. NaturalProver is\ncapable of proving some theorems that require short (2-6 step) proofs, and\nproviding next-step suggestions that are rated as correct and useful over 40%\nof the time, which is to our knowledge the first demonstration of these\ncapabilities using neural language models.",
    "descriptor": "",
    "authors": [
      "Sean Welleck",
      "Jiacheng Liu",
      "Ximing Lu",
      "Hannaneh Hajishirzi",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12910"
  },
  {
    "id": "arXiv:2205.12911",
    "title": "SoK: Cross-border Criminal Investigations and Digital Evidence",
    "abstract": "Digital evidence underpin the majority of crimes as their analysis is an\nintegral part of almost every criminal investigation. Even if we temporarily\ndisregard the numerous challenges in the collection and analysis of digital\nevidence, the exchange of the evidence among the different stakeholders has\nmany thorny issues. Of specific interest are cross-border criminal\ninvestigations as the complexity is significantly high due to the heterogeneity\nof legal frameworks which beyond time bottlenecks can also become prohibiting.\nThe aim of this article is to analyse the current state of practice of\ncross-border investigations considering the efficacy of current collaboration\nprotocols along with the challenges and drawbacks to be overcome. Further to\nperforming a legally-oriented research treatise, we recall all the challenges\nraised in the literature and discuss them from a more practical yet global\nperspective. Thus, this article paves the way to enabling practitioners and\nstakeholders to leverage horizontal strategies to fill in the identified gaps\ntimely and accurately.",
    "descriptor": "",
    "authors": [
      "Fran Casino",
      "Claudia Pina",
      "Pablo L\u00f3pez-Aguilar",
      "Edgar Batista",
      "Agusti Solanas",
      "Constantinos Patsakis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12911"
  },
  {
    "id": "arXiv:2205.12912",
    "title": "Context-Aware Video Reconstruction for Rolling Shutter Cameras",
    "abstract": "With the ubiquity of rolling shutter (RS) cameras, it is becoming\nincreasingly attractive to recover the latent global shutter (GS) video from\ntwo consecutive RS frames, which also places a higher demand on realism.\nExisting solutions, using deep neural networks or optimization, achieve\npromising performance. However, these methods generate intermediate GS frames\nthrough image warping based on the RS model, which inevitably result in black\nholes and noticeable motion artifacts. In this paper, we alleviate these issues\nby proposing a context-aware GS video reconstruction architecture. It\nfacilitates the advantages such as occlusion reasoning, motion compensation,\nand temporal abstraction. Specifically, we first estimate the bilateral motion\nfield so that the pixels of the two RS frames are warped to a common GS frame\naccordingly. Then, a refinement scheme is proposed to guide the GS frame\nsynthesis along with bilateral occlusion masks to produce high-fidelity GS\nvideo frames at arbitrary times. Furthermore, we derive an approximated\nbilateral motion field model, which can serve as an alternative to provide a\nsimple but effective GS frame initialization for related tasks. Experiments on\nsynthetic and real data show that our approach achieves superior performance\nover state-of-the-art methods in terms of objective metrics and subjective\nvisual quality. Code is available at \\url{https://github.com/GitCVfb/CVR}.",
    "descriptor": "\nComments: Accepted to IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2022)\n",
    "authors": [
      "Bin Fan",
      "Yuchao Dai",
      "Zhiyuan Zhang",
      "Qi Liu",
      "Mingyi He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12912"
  },
  {
    "id": "arXiv:2205.12914",
    "title": "New Intent Discovery with Pre-training and Contrastive Learning",
    "abstract": "New intent discovery aims to uncover novel intent categories from user\nutterances to expand the set of supported intent classes. It is a critical task\nfor the development and service expansion of a practical dialogue system.\nDespite its importance, this problem remains under-explored in the literature.\nExisting approaches typically rely on a large amount of labeled utterances and\nemploy pseudo-labeling methods for representation learning and clustering,\nwhich are label-intensive, inefficient, and inaccurate. In this paper, we\nprovide new solutions to two important research questions for new intent\ndiscovery: (1) how to learn semantic utterance representations and (2) how to\nbetter cluster utterances. Particularly, we first propose a multi-task\npre-training strategy to leverage rich unlabeled data along with external\nlabeled data for representation learning. Then, we design a new contrastive\nloss to exploit self-supervisory signals in unlabeled data for clustering.\nExtensive experiments on three intent recognition benchmarks demonstrate the\nhigh effectiveness of our proposed method, which outperforms state-of-the-art\nmethods by a large margin in both unsupervised and semi-supervised scenarios.\nThe source code will be available at\n\\url{https://github.com/zhang-yu-wei/MTP-CLNN}.",
    "descriptor": "\nComments: Accepted to ACL 2022\n",
    "authors": [
      "Yuwei Zhang",
      "Haode Zhang",
      "Li-Ming Zhan",
      "Xiao-Ming Wu",
      "Albert Y.S. Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12914"
  },
  {
    "id": "arXiv:2205.12918",
    "title": "A Low Memory Footprint Quantized Neural Network for Depth Completion of  Very Sparse Time-of-Flight Depth Maps",
    "abstract": "Sparse active illumination enables precise time-of-flight depth sensing as it\nmaximizes signal-to-noise ratio for low power budgets. However, depth\ncompletion is required to produce dense depth maps for 3D perception. We\naddress this task with realistic illumination and sensor resolution constraints\nby simulating ToF datasets for indoor 3D perception with challenging sparsity\nlevels. We propose a quantized convolutional encoder-decoder network for this\ntask. Our model achieves optimal depth map quality by means of input\npre-processing and carefully tuned training with a geometry-preserving loss\nfunction. We also achieve low memory footprint for weights and activations by\nmeans of mixed precision quantization-at-training techniques. The resulting\nquantized models are comparable to the state of the art in terms of quality,\nbut they require very low GPU times and achieve up to 14-fold memory size\nreduction for the weights w.r.t. their floating point counterpart with minimal\nimpact on quality metrics.",
    "descriptor": "\nComments: In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops 2022. Presented at the 5th Efficient Deep Learning for Computer Vision Workshop\n",
    "authors": [
      "Xiaowen Jiang",
      "Valerio Cambareri",
      "Gianluca Agresti",
      "Cynthia Ifeyinwa Ugwu",
      "Adriano Simonetto",
      "Fabien Cardinaux",
      "Pietro Zanuttigh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12918"
  },
  {
    "id": "arXiv:2205.12920",
    "title": "DH-GAN: A Physics-driven Untrained Generative Adversarial Network for 3D  Microscopic Imaging using Digital Holography",
    "abstract": "Digital holography is a 3D imaging technique by emitting a laser beam with a\nplane wavefront to an object and measuring the intensity of the diffracted\nwaveform, called holograms. The object's 3D shape can be obtained by numerical\nanalysis of the captured holograms and recovering the incurred phase. Recently,\ndeep learning (DL) methods have been used for more accurate holographic\nprocessing. However, most supervised methods require large datasets to train\nthe model, which is rarely available in most DH applications due to the\nscarcity of samples or privacy concerns. A few one-shot DL-based recovery\nmethods exist with no reliance on large datasets of paired images. Still, most\nof these methods often neglect the underlying physics law that governs wave\npropagation. These methods offer a black-box operation, which is not\nexplainable, generalizable, and transferrable to other samples and\napplications. In this work, we propose a new DL architecture based on\ngenerative adversarial networks that uses a discriminative network for\nrealizing a semantic measure for reconstruction quality while using a\ngenerative network as a function approximator to model the inverse of hologram\nformation. We impose smoothness on the background part of the recovered image\nusing a progressive masking module powered by simulated annealing to enhance\nthe reconstruction quality. The proposed method is one of its kind that\nexhibits high transferability to similar samples, which facilitates its fast\ndeployment in time-sensitive applications without the need for retraining the\nnetwork. The results show a considerable improvement to competitor methods in\nreconstruction quality (about 5 dB PSNR gain) and robustness to noise (about\n50% reduction in PSNR vs noise increase rate).",
    "descriptor": "",
    "authors": [
      "Xiwen Chen",
      "Hao Wang",
      "Abofazl Razi",
      "Michael Kozicki",
      "Christopher Mann"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12920"
  },
  {
    "id": "arXiv:2205.12923",
    "title": "Domain Adaptation for Object Detection using SE Adaptors and Center Loss",
    "abstract": "Despite growing interest in object detection, very few works address the\nextremely practical problem of cross-domain robustness especially for\nautomative applications. In order to prevent drops in performance due to domain\nshift, we introduce an unsupervised domain adaptation method built on the\nfoundation of faster-RCNN with two domain adaptation components addressing the\nshift at the instance and image levels respectively and apply a consistency\nregularization between them. We also introduce a family of adaptation layers\nthat leverage the squeeze excitation mechanism called SE Adaptors to improve\ndomain attention and thus improves performance without any prior requirement of\nknowledge of the new target domain. Finally, we incorporate a center loss in\nthe instance and image level representations to improve the intra-class\nvariance. We report all results with Cityscapes as our source domain and Foggy\nCityscapes as the target domain outperforming previous baselines.",
    "descriptor": "",
    "authors": [
      "Sushruth Nagesh",
      "Shreyas Rajesh",
      "Asfiya Baig",
      "Savitha Srinivasan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12923"
  },
  {
    "id": "arXiv:2205.12925",
    "title": "These Maps Are Made For Walking: Real-Time Terrain Property Estimation  for Mobile Robots",
    "abstract": "The equations of motion governing mobile robots are dependent on terrain\nproperties such as the coefficient of friction, and contact model parameters.\nEstimating these properties is thus essential for robotic navigation. Ideally\nany map estimating terrain properties should run in real time, mitigate sensor\nnoise, and provide probability distributions of the aforementioned properties,\nthus enabling risk-mitigating navigation and planning. This paper addresses\nthese needs and proposes a Bayesian inference framework for semantic mapping\nwhich recursively estimates both the terrain surface profile and a probability\ndistribution for terrain properties using data from a single RGB-D camera. The\nproposed framework is evaluated in simulation against other semantic mapping\nmethods and is shown to outperform these state-of-the-art methods in terms of\ncorrectly estimating simulated ground-truth terrain properties when evaluated\nusing a precision-recall curve and the Kullback-Leibler divergence test.\nAdditionally, the proposed method is deployed on a physical legged robotic\nplatform in both indoor and outdoor environments, and we show our method\ncorrectly predicts terrain properties in both cases. The proposed framework\nruns in real-time and includes a ROS interface for easy integration.",
    "descriptor": "",
    "authors": [
      "Parker Ewen",
      "Adam Li",
      "Yuxin Chen",
      "Steven Hong",
      "Ram Vasudevan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12925"
  },
  {
    "id": "arXiv:2205.12934",
    "title": "Amortized Inference for Causal Structure Learning",
    "abstract": "Learning causal structure poses a combinatorial search problem that typically\ninvolves evaluating structures using a score or independence test. The\nresulting search is costly, and designing suitable scores or tests that capture\nprior knowledge is difficult. In this work, we propose to amortize the process\nof causal structure learning. Rather than searching over causal structures\ndirectly, we train a variational inference model to predict the causal\nstructure from observational/interventional data. Our inference model acquires\ndomain-specific inductive bias for causal discovery solely from data generated\nby a simulator. This allows us to bypass both the search over graphs and the\nhand-engineering of suitable score functions. Moreover, the architecture of our\ninference model is permutation invariant w.r.t. the data points and permutation\nequivariant w.r.t. the variables, facilitating generalization to significantly\nlarger problem instances than seen during training. On synthetic data and\nsemi-synthetic gene expression data, our models exhibit robust generalization\ncapabilities under substantial distribution shift and significantly outperform\nexisting algorithms, especially in the challenging genomics domain.",
    "descriptor": "",
    "authors": [
      "Lars Lorch",
      "Scott Sussex",
      "Jonas Rothfuss",
      "Andreas Krause",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12934"
  },
  {
    "id": "arXiv:2205.12938",
    "title": "Joint Beam Management and Power Allocation in THz-NOMA Networks",
    "abstract": "This paper investigates how to apply non-orthogonal multiple access (NOMA) as\nan add-on in terahertz (THz) networks. In particular, prior to the\nimplementation of NOMA, it is assumed that there exists a legacy THz system,\nwhere spatial beams have already been configured to serve legacy primary users.\nThe aim of this paper is to study how these pre-configured spatial beams can be\nused as a type of bandwidth resources, on which additional secondary users are\nserved without degrading the performance of the legacy primary users. A joint\nbeam management and power allocation problem is first formulated as a mixed\ncombinatorial non-convex optimization problem, and then solved by two methods\nwith different performance-complexity tradeoffs, one based on the branch and\nbound method and the other based on successive convex approximation. Both\nanalytical and simulation results are presented to illustrate the new features\nof beam-based resource allocation in THz-NOMA networks and also demonstrate\nthat those pre-configured spatial beams can be employed to improve the system\nthroughput and connectivity in a spectrally efficient manner.",
    "descriptor": "",
    "authors": [
      "Zhiguo Ding",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12938"
  },
  {
    "id": "arXiv:2205.12943",
    "title": "Transitions from P to NP-hardness: the case of the Linear Ordering  Problem",
    "abstract": "In this paper we evaluate how constructive heuristics degrade when a problem\ntransits from P to NP-hard. This is done by means of the linear ordering\nproblem. More specifically, for this problem we prove that the objective\nfunction can be expressed as the sum of two objective functions, one of which\nis associated with a P problem (an exact polynomial time algorithm is proposed\nto solve it), while the other is associated with an NP-hard problem. We study\nhow different constructive algorithms whose behaviour only depends on\nunivariate information perform depending on the contribution of the P or\nNP-hard components of the problem. A number of experiments are conducted with\nreduced dimensions, where the global optimum of the problems is known, giving\ndifferent weights to the NP-hard component, while the weight of the P component\nis fixed. It is observed how the performance of the constructive algorithms\ngets worse as the weight given to the NP-hard component increases.",
    "descriptor": "",
    "authors": [
      "Anne Elorza",
      "Leticia Hernando",
      "Jose A. Lozano"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.12943"
  },
  {
    "id": "arXiv:2205.12944",
    "title": "Learning Mean Field Games: A Survey",
    "abstract": "Non-cooperative and cooperative games with a very large number of players\nhave many applications but remain generally intractable when the number of\nplayers increases. Introduced by Lasry and Lions, and Huang, Caines and\nMalham\\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow\nthe number of players to grow to infinity. Traditional methods for solving\nthese games generally rely on solving partial or stochastic differential\nequations with a full knowledge of the model. Recently, Reinforcement Learning\n(RL) has appeared promising to solve complex problems. By combining MFGs and\nRL, we hope to solve games at a very large scale both in terms of population\nsize and environment complexity. In this survey, we review the quickly growing\nrecent literature on RL methods to learn Nash equilibria in MFGs. We first\nidentify the most common settings (static, stationary, and evolutive). We then\npresent a general framework for classical iterative methods (based on\nbest-response computation or policy evaluation) to solve MFGs in an exact way.\nBuilding on these algorithms and the connection with Markov Decision Processes,\nwe explain how RL can be used to learn MFG solutions in a model-free way. Last,\nwe present numerical illustrations on a benchmark problem, and conclude with\nsome perspectives.",
    "descriptor": "",
    "authors": [
      "Mathieu Lauri\u00e8re",
      "Sarah Perrin",
      "Matthieu Geist",
      "Olivier Pietquin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.12944"
  },
  {
    "id": "arXiv:2205.12952",
    "title": "Pretraining is All You Need for Image-to-Image Translation",
    "abstract": "We propose to use pretraining to boost general image-to-image translation.\nPrior image-to-image translation methods usually need dedicated architectural\ndesign and train individual translation models from scratch, struggling for\nhigh-quality generation of complex scenes, especially when paired training data\nare not abundant. In this paper, we regard each image-to-image translation\nproblem as a downstream task and introduce a simple and generic framework that\nadapts a pretrained diffusion model to accommodate various kinds of\nimage-to-image translation. We also propose adversarial training to enhance the\ntexture synthesis in the diffusion model training, in conjunction with\nnormalized guidance sampling to improve the generation quality. We present\nextensive empirical comparison across various tasks on challenging benchmarks\nsuch as ADE20K, COCO-Stuff, and DIODE, showing the proposed pretraining-based\nimage-to-image translation (PITI) is capable of synthesizing images of\nunprecedented realism and faithfulness.",
    "descriptor": "\nComments: Project Page: this https URL\n",
    "authors": [
      "Tengfei Wang",
      "Ting Zhang",
      "Bo Zhang",
      "Hao Ouyang",
      "Dong Chen",
      "Qifeng Chen",
      "Fang Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12952"
  },
  {
    "id": "arXiv:2205.12955",
    "title": "Neural 3D Reconstruction in the Wild",
    "abstract": "We are witnessing an explosion of neural implicit representations in computer\nvision and graphics. Their applicability has recently expanded beyond tasks\nsuch as shape generation and image-based rendering to the fundamental problem\nof image-based 3D reconstruction. However, existing methods typically assume\nconstrained 3D environments with constant illumination captured by a small set\nof roughly uniformly distributed cameras. We introduce a new method that\nenables efficient and accurate surface reconstruction from Internet photo\ncollections in the presence of varying illumination. To achieve this, we\npropose a hybrid voxel- and surface-guided sampling technique that allows for\nmore efficient ray sampling around surfaces and leads to significant\nimprovements in reconstruction quality. Further, we present a new benchmark and\nprotocol for evaluating reconstruction performance on such in-the-wild scenes.\nWe perform extensive experiments, demonstrating that our approach surpasses\nboth classical and neural reconstruction methods on a wide variety of metrics.",
    "descriptor": "\nComments: Accepted to SIGGRAPH 2022 (Conference Proceedings). Project page: this https URL\n",
    "authors": [
      "Jiaming Sun",
      "Xi Chen",
      "Qianqian Wang",
      "Zhengqi Li",
      "Hadar Averbuch-Elor",
      "Xiaowei Zhou",
      "Noah Snavely"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.12955"
  },
  {
    "id": "arXiv:2205.12956",
    "title": "Inception Transformer",
    "abstract": "Recent studies show that Transformer has strong capability of building\nlong-range dependencies, yet is incompetent in capturing high frequencies that\npredominantly convey local information. To tackle this issue, we present a\nnovel and general-purpose Inception Transformer, or iFormer for short, that\neffectively learns comprehensive features with both high- and low-frequency\ninformation in visual data. Specifically, we design an Inception mixer to\nexplicitly graft the advantages of convolution and max-pooling for capturing\nthe high-frequency information to Transformers. Different from recent hybrid\nframeworks, the Inception mixer brings greater efficiency through a channel\nsplitting mechanism to adopt parallel convolution/max-pooling path and\nself-attention path as high- and low-frequency mixers, while having the\nflexibility to model discriminative information scattered within a wide\nfrequency range. Considering that bottom layers play more roles in capturing\nhigh-frequency details while top layers more in modeling low-frequency global\ninformation, we further introduce a frequency ramp structure, i.e. gradually\ndecreasing the dimensions fed to the high-frequency mixer and increasing those\nto the low-frequency mixer, which can effectively trade-off high- and\nlow-frequency components across different layers. We benchmark the iFormer on a\nseries of vision tasks, and showcase that it achieves impressive performance on\nimage classification, COCO detection and ADE20K segmentation. For example, our\niFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than\nDeiT-S by 3.6%, and even slightly better than much bigger model Swin-B (83.3%)\nwith only 1/4 parameters and 1/3 FLOPs. Code and models will be released at\nhttps://github.com/sail-sg/iFormer.",
    "descriptor": "\nComments: Code and models will be released at this https URL\n",
    "authors": [
      "Chenyang Si",
      "Weihao Yu",
      "Pan Zhou",
      "Yichen Zhou",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12956"
  },
  {
    "id": "arXiv:2205.12127",
    "title": "Privacy and correctness trade-offs for information-theoretically secure  quantum homomorphic encryption",
    "abstract": "Quantum homomorphic encryption, which allows computation by a server directly\non encrypted data, is a fundamental primitive out of which more complex quantum\ncryptography protocols can be built. For such constructions to be possible,\nquantum homomorphic encryption must satisfy two privacy properties: data\nprivacy which ensures that the input data is private from the server, and\ncircuit privacy which ensures that the ciphertext after the computation does\nnot reveal any additional information about the circuit used to perform it,\nbeyond the output of the computation itself. While circuit privacy is\nwell-studied in classical cryptography and many homomorphic encryption schemes\ncan be equipped with it, its quantum analogue has received little attention.\nHere we establish a definition of circuit privacy for quantum homomorphic\nencryption with information-theoretic security. Furthermore, we reduce quantum\noblivious transfer to quantum homomorphic encryption. Using this reduction, our\nwork unravels fundamental trade-offs between circuit privacy, data privacy and\ncorrectness for a broad family of quantum homomorphic encryption protocols,\nincluding schemes that allow only computation of Clifford circuits.",
    "descriptor": "",
    "authors": [
      "Yanglin Hu",
      "Yingkai Ouyang",
      "Marco Tomamichel"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12127"
  },
  {
    "id": "arXiv:2205.12300",
    "title": "On Observer-based Asymptotic Stabilization of Non-uniformly Observable  Systems via Hybrid and Smooth Control: a Case Study",
    "abstract": "For systems that are not observable at the very equilibrium of interest to be\nstabilized, output-feedback stabilization is considerably challenging. In this\npaper we solve this control problem for the case-study of a second-order system\nthat is bilinear and affine, both in the input and the output, but it is\nunobservable at the target equilibrium. The case-study is representative of a\nwell-studied class of non-uniformly observable systems and stems from\nautomotive control. Our main contribution is a novel certainty-equivalence\nhybrid controller that achieves asymptotic stabilization semiglobally. The\ncontroller relies on a switched observer that estimates the state, provided\nthat the latter is 'kept away' from the singular equilibrium. To achieve both\ncompeting tasks, stabilization and estimation, the controller also relies on\nthe keen construction of a piecewise-constant, converging, reference. Our main\nresults are illustrated via numerical simulations on a meaningful example.",
    "descriptor": "",
    "authors": [
      "Mohamed Maghenem",
      "William Pasillas-L\u00e9pine",
      "Antonio Lor\u00eda",
      "Missie Aguado-Rojas"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12300"
  },
  {
    "id": "arXiv:2205.12313",
    "title": "Numerical stability of solitary waves in flows with constant vorticity  for the Euler equations",
    "abstract": "The study of the Euler equations in flows with constant vorticity has piqued\nthe curiosity of a considerable number of researchers over the years. Much\nresearch has been conducted on this subject under the assumption of steady\nflow. In this work, we provide a numerical approach that allows us to compute\nsolitary waves in flows with constant vorticity and analyse their stability.\nThrough a conformal mapping technique, we compute solutions of the steady Euler\nequations, then feed them as initial data for the time-dependent Euler\nequations. We focus on analysing to what extent the steady solitary waves are\nstable within the time-dependent framework. Our numerical simulations indicate\nthat although it is possible to compute solitary waves for the steady Euler\nequations in flows with large values of vorticity, such waves are not\nnumerically stable for vorticities with absolute value much greater than one.\nBesides, we notice that large waves are unstable even for small values of\nvorticity.",
    "descriptor": "",
    "authors": [
      "Eduardo M. Castro",
      "Marcelo V. Flamarion",
      "Roberto Ribeiro-Jr"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12313"
  },
  {
    "id": "arXiv:2205.12338",
    "title": "Hippocluster: an efficient, hippocampus-inspired algorithm for graph  clustering",
    "abstract": "Random walks can reveal communities or clusters in networks, because they are\nmore likely to stay within a cluster than leave it. Thus, one family of\ncommunity detection algorithms uses random walks to measure distance between\npairs of nodes in various ways, and then applies K-Means or other generic\nclustering methods to these distances. Interestingly, information processing in\nthe brain may suggest a simpler method of learning clusters directly from\nrandom walks. Drawing inspiration from the hippocampus, we describe a simple\ntwo-layer neural learning framework. Neurons in one layer are associated with\ngraph nodes and simulate random walks. These simulations cause neurons in the\nsecond layer to become tuned to graph clusters through simple associative\nlearning. We show that if these neuronal interactions are modelled a particular\nway, the system is essentially a variant of K-Means clustering applied directly\nin the walk-space, bypassing the usual step of computing node\ndistances/similarities. The result is an efficient graph clustering method.\nBiological information processing systems are known for high efficiency and\nadaptability. In tests on benchmark graphs, our framework demonstrates this\nhigh data-efficiency, low memory use, low complexity, and real-time adaptation\nto graph changes, while still achieving clustering quality comparable to other\nalgorithms.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Eric Chalmers",
      "Artur Luczak"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.12338"
  },
  {
    "id": "arXiv:2205.12354",
    "title": "Optimal Entanglement Distribution using Satellite Based Quantum Networks",
    "abstract": "Recent technological advancements in satellite based quantum communication\nhas made it a promising technology for realizing global scale quantum networks.\nDue to better loss distance scaling compared to ground based fiber\ncommunication, satellite quantum communication can distribute high quality\nquantum entanglements among ground stations that are geographically separated\nat very long distances. This work focuses on optimal distribution of bipartite\nentanglements to a set of pair of ground stations using a constellation of\norbiting satellites. In particular, we characterize the optimal\nsatellite-to-ground station transmission scheduling policy with respect to the\naggregate entanglement distribution rate subject to various resource\nconstraints at the satellites and ground stations. We cast the optimal\ntransmission scheduling problem as an integer linear programming problem and\nsolve it efficiently for some specific scenarios. Our framework can also be\nused as a benchmark tool to measure the performance of other potential\ntransmission scheduling policies.",
    "descriptor": "",
    "authors": [
      "Nitish K. Panigrahy",
      "Prajit Dhara",
      "Don Towsley",
      "Saikat Guha",
      "Leandros Tassiulas"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.12354"
  },
  {
    "id": "arXiv:2205.12363",
    "title": "On the number of error correcting codes",
    "abstract": "We show that for a fixed $q$, the number of $q$-ary $t$-error correcting\ncodes of length $n$ is at most $2^{(1 + o(1)) H_q(n,t)}$ for all $t \\leq (1 -\nq^{-1})n - C_q\\sqrt{n \\log n}$ (for sufficiently large constant $C_q$), where\n$H_q(n, t) = q^n / V_q(n,t)$ is the Hamming bound and $V_q(n,t)$ is the\ncardinality of the radius $t$ Hamming ball. This proves a conjecture of Balogh,\nTreglown, and Wagner, who showed the result for $t = o(n^{1/3} (\\log\nn)^{-2/3})$.",
    "descriptor": "\nComments: 13 pages. Comments welcome!\n",
    "authors": [
      "Dingding Dong",
      "Nitya Mani",
      "Yufei Zhao"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12363"
  },
  {
    "id": "arXiv:2205.12365",
    "title": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing",
    "abstract": "The matching principles behind optimal transport (OT) play an increasingly\nimportant role in machine learning, a trend which can be observed when OT is\nused to disambiguate datasets in applications (e.g. single-cell genomics) or\nused to improve more complex methods (e.g. balanced attention in transformers\nor self-supervised learning). To scale to more challenging problems, there is a\ngrowing consensus that OT requires solvers that can operate on millions, not\nthousands, of points. The low-rank optimal transport (LOT) approach advocated\nin \\cite{scetbon2021lowrank} holds several promises in that regard, and was\nshown to complement more established entropic regularization approaches, being\nable to insert itself in more complex pipelines, such as quadratic OT. LOT\nrestricts the search for low-cost couplings to those that have a\nlow-nonnegative rank, yielding linear time algorithms in cases of interest.\nHowever, these promises can only be fulfilled if the LOT approach is seen as a\nlegitimate contender to entropic regularization when compared on properties of\ninterest, where the scorecard typically includes theoretical properties\n(statistical bounds, relation to other methods) or practical aspects\n(debiasing, hyperparameter tuning, initialization). We target each of these\nareas in this paper in order to cement the impact of low-rank approaches in\ncomputational OT.",
    "descriptor": "",
    "authors": [
      "Meyer Scetbon",
      "Marco Cuturi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12365"
  },
  {
    "id": "arXiv:2205.12419",
    "title": "Physics Guided Machine Learning for Variational Multiscale Reduced Order  Modeling",
    "abstract": "We propose a new physics guided machine learning (PGML) paradigm that\nleverages the variational multiscale (VMS) framework and available data to\ndramatically increase the accuracy of reduced order models (ROMs) at a modest\ncomputational cost. The hierarchical structure of the ROM basis and the VMS\nframework enable a natural separation of the resolved and unresolved ROM\nspatial scales. Modern PGML algorithms are used to construct novel models for\nthe interaction among the resolved and unresolved ROM scales. Specifically, the\nnew framework builds ROM operators that are closest to the true interaction\nterms in the VMS framework. Finally, machine learning is used to reduce the\nprojection error and further increase the ROM accuracy. Our numerical\nexperiments for a two-dimensional vorticity transport problem show that the\nnovel PGML-VMS-ROM paradigm maintains the low computational cost of current\nROMs, while significantly increasing the ROM accuracy.",
    "descriptor": "",
    "authors": [
      "Shady E. Ahmed",
      "Omer San",
      "Adil Rasheed",
      "Traian Iliescu",
      "Alessandro Veneziani"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Pattern Formation and Solitons (nlin.PS)"
    ],
    "url": "https://arxiv.org/abs/2205.12419"
  },
  {
    "id": "arXiv:2205.12429",
    "title": "Interaction of a priori Anatomic Knowledge with Self-Supervised  Contrastive Learning in Cardiac Magnetic Resonance Imaging",
    "abstract": "Training deep learning models on cardiac magnetic resonance imaging (CMR) can\nbe a challenge due to the small amount of expert generated labels and inherent\ncomplexity of data source. Self-supervised contrastive learning (SSCL) has\nrecently been shown to boost performance in several medical imaging tasks.\nHowever, it is unclear how much the pre-trained representation reflects the\nprimary organ of interest compared to spurious surrounding tissue. In this\nwork, we evaluate the optimal method of incorporating prior knowledge of\nanatomy into a SSCL training paradigm. Specifically, we evaluate using a\nsegmentation network to explicitly local the heart in CMR images, followed by\nSSCL pretraining in multiple diagnostic tasks. We find that using a priori\nknowledge of anatomy can greatly improve the downstream diagnostic performance.\nFurthermore, SSCL pre-training with in-domain data generally improved\ndownstream performance and more human-like saliency compared to end-to-end\ntraining and ImageNet pre-trained networks. However, introducing anatomic\nknowledge to pre-training generally does not have significant impact.",
    "descriptor": "\nComments: Under review at Machine Learning in Healthcare\n",
    "authors": [
      "Makiya Nakashima",
      "Inyeop Jang",
      "Ramesh Basnet",
      "Mitchel Benovoy",
      "W.H. Wilson Tang",
      "Christopher Nguyen",
      "Deborah Kwon",
      "Tae Hyun Hwang",
      "David Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12429"
  },
  {
    "id": "arXiv:2205.12438",
    "title": "Skin Cancer Diagnostics with an All-Inclusive Smartphone Application",
    "abstract": "Among the different types of skin cancer, melanoma is considered to be the\ndeadliest and is difficult to treat at advanced stages. Detection of melanoma\nat earlier stages can lead to reduced mortality rates. Desktop-based\ncomputer-aided systems have been developed to assist dermatologists with early\ndiagnosis. However, there is significant interest in developing portable,\nat-home melanoma diagnostic systems which can assess the risk of cancerous skin\nlesions. Here, we present a smartphone application that combines image capture\ncapabilities with preprocessing and segmentation to extract the Asymmetry,\nBorder irregularity, Color variegation, and Diameter (ABCD) features of a skin\nlesion. Using the feature sets, classification of malignancy is achieved\nthrough support vector machine classifiers. By using adaptive algorithms in the\nindividual data-processing stages, our approach is made computationally light,\nuser friendly, and reliable in discriminating melanoma cases from benign ones.\nImages of skin lesions are either captured with the smartphone camera or\nimported from public datasets. The entire process from image capture to\nclassification runs on an Android smartphone equipped with a detachable 10x\nlens, and processes an image in less than a second. The overall performance\nmetrics are evaluated on a public database of 200 images with Synthetic\nMinority Over-sampling Technique (SMOTE) (80% sensitivity, 90% specificity, 88%\naccuracy, and 0.85 area under curve (AUC)) and without SMOTE (55% sensitivity,\n95% specificity, 90% accuracy, and 0.75 AUC). The evaluated performance metrics\nand computation times are comparable or better than previous methods. This\nall-inclusive smartphone application is designed to be easy-to-download and\neasy-to-navigate for the end user, which is imperative for the eventual\ndemocratization of such medical diagnostic systems.",
    "descriptor": "",
    "authors": [
      "Upender Kalwa",
      "Christopher Legner",
      "Taejoon Kong",
      "Santosh Pandey"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.12438"
  },
  {
    "id": "arXiv:2205.12442",
    "title": "Lyapunov function approach for approximation algorithm design and  analysis: with applications in submodular maximization",
    "abstract": "We propose a two-phase systematical framework for approximation algorithm\ndesign and analysis via Lyapunov function. The first phase consists of using\nLyapunov function as a guideline to design a continuous-time algorithm with\nprovable approximation ratio. The second phase then converts the\ncontinuous-time algorithm to a discrete-time algorithm with the same\napproximation ratio and a provable time complexity. Some immediate benefits of\nthe Lyapunov function approach include: (i) unifying many existing algorithms;\n(ii) providing a guideline to design and analyze new algorithms; and (iii)\noffer new perspectives to potentially improve existing algorithms. We use\nvarious submodular maximization problems as running examples to illustrate our\nframework.",
    "descriptor": "",
    "authors": [
      "Donglei Du"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12442"
  },
  {
    "id": "arXiv:2205.12445",
    "title": "Over-the-Air Design of GAN Training for mmWave MIMO Channel Estimation",
    "abstract": "Future wireless systems are trending towards higher carrier frequencies that\noffer larger communication bandwidth but necessitate the use of large antenna\narrays. Existing signal processing techniques for channel estimation do not\nscale well to this \"high-dimensional\" regime in terms of performance and pilot\noverhead. Meanwhile, training deep learning based approaches for channel\nestimation requires large labeled datasets mapping pilot measurements to clean\nchannel realizations, which can only be generated offline using simulated\nchannels. In this paper, we develop a novel unsupervised over-the-air (OTA)\nalgorithm that utilizes noisy received pilot measurements to train a deep\ngenerative model to output beamspace MIMO channel realizations. Our approach\nleverages Generative Adversarial Networks (GAN), while using a conditional\ninput to distinguish between Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS)\nchannel realizations. We also present a federated implementation of the OTA\nalgorithm that distributes the GAN training over multiple users and greatly\nreduces the user side computation. We then formulate channel estimation from a\nlimited number of pilot measurements as an inverse problem and reconstruct the\nchannel by optimizing the input vector of the trained generative model. Our\nproposed approach significantly outperforms Orthogonal Matching Pursuit on both\nLOS and NLOS channel models, and EM-GM-AMP -- an Approximate Message Passing\nalgorithm -- on LOS channel models, while achieving comparable performance on\nNLOS channel models in terms of the normalized channel reconstruction error.\nMore importantly, our proposed framework has the potential to be trained online\nusing real noisy pilot measurements, is not restricted to a specific channel\nmodel and can even be utilized for a federated OTA design of a dataset\ngenerator from noisy data.",
    "descriptor": "\nComments: 34 pages, 12 figures, 5 tables. Under review for publication in IEEE Journal of Sel. Areas in Information Theory\n",
    "authors": [
      "Akash Doshi",
      "Manan Gupta",
      "Jeffrey G. Andrews"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12445"
  },
  {
    "id": "arXiv:2205.12447",
    "title": "Uniformly Bounded Regret in Dynamic Fair Allocation",
    "abstract": "We study a dynamic allocation problem in which $T$ sequentially arriving\ndivisible resources need to be allocated to $n$ fixed agents with additive\nutilities. Agents' utilities are drawn stochastically from a known\ndistribution, and decisions are made immediately and irrevocably. Most works on\ndynamic resource allocation aim to maximize the utilitarian welfare of the\nagents, which may result in unfair concentration of resources at select agents\nwhile leaving others' demands under-fulfilled. In this paper, we consider the\negalitarian welfare objective instead, which aims at balancing the efficiency\nand fairness of the allocation. To this end, we first study a fluid-based\npolicy derived from a deterministic approximation to the underlying problem and\nshow that it attains a regret of order $\\Theta(\\sqrt{T})$ against the hindsight\noptimum, i.e., the optimal egalitarian allocation when all utilities are known\nin advance. We then propose a new policy, called Backward Infrequent Re-solving\nwith Thresholding ($\\mathsf{BIRT}$), which consists of re-solving the fluid\nproblem at most $O(\\log\\log T)$ times. We prove the $\\mathsf{BIRT}$ policy\nattains $O(1)$ regret against the hindsight optimum, independently of the time\nhorizon length $T$ and initial welfare. We also present numerical experiments\nto illustrate the significant performance improvement against several benchmark\npolicies.",
    "descriptor": "",
    "authors": [
      "Santiago R. Balseiro",
      "Shangzhou Xia"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12447"
  },
  {
    "id": "arXiv:2205.12448",
    "title": "Transportation-Inequalities, Lyapunov Stability and Sampling for  Dynamical Systems on Continuous State Space",
    "abstract": "We study the concentration phenomenon for discrete-time random dynamical\nsystems with an unbounded state space. We develop a heuristic approach towards\nobtaining exponential concentration inequalities for dynamical systems using an\nentirely functional analytic framework. We also show that existence of\nexponential-type Lyapunov function, compared to the purely deterministic\nsetting, not only implies stability but also exponential concentration\ninequalities for sampling from the stationary distribution, via\n\\emph{transport-entropy inequality} (T-E). These results have significant\nimpact in \\emph{reinforcement learning} (RL) and \\emph{controls}, leading to\nexponential concentration inequalities even for unbounded observables, while\nneither assuming reversibility nor exact knowledge of random dynamical system\n(assumptions at heart of concentration inequalities in statistical mechanics\nand Markov diffusion processes).",
    "descriptor": "",
    "authors": [
      "Muhammad Abdullah Naeem",
      "Miroslav Pajic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12448"
  },
  {
    "id": "arXiv:2205.12457",
    "title": "Eigenvalues of the laplacian matrices of the cycles with one weighted  edge",
    "abstract": "In this paper we study the eigenvalues of the laplacian matrices of the\ncyclic graphs with one edge of weight $\\alpha$ and the others of weight $1$. We\ndenote by $n$ the order of the graph and suppose that $n$ tends to infinity. We\nnotice that the characteristic polynomial and the eigenvalues depend only on\n$\\operatorname{Re}(\\alpha)$. After that, through the rest of the paper we\nsuppose that $0<\\alpha<1$. It is easy to see that the eigenvalues belong to\n$[0,4]$ and are asymptotically distributed as the function $g(x)=4\\sin^2(x/2)$\non $[0,\\pi]$. We obtain a series of results about the individual behavior of\nthe eigenvalues. First, we describe more precisely their localization in\nsubintervals of $[0,4]$. Second, we transform the characteristic equation to a\nform convenient to solve by numerical methods. In particular, we prove that\nNewton's method converges for every $n\\ge3$. Third, we derive asymptotic\nformulas for all eigenvalues, where the errors are uniformly bounded with\nrespect to the number of the eigenvalue.",
    "descriptor": "\nComments: 29 pages, 5 figures\n",
    "authors": [
      "Sergei M. Grudsky",
      "Egor A. Maximenko",
      "Alejandro Soto-Gonz\u00e1lez"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)",
      "Spectral Theory (math.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.12457"
  },
  {
    "id": "arXiv:2205.12460",
    "title": "Linear Algorithms for Nonparametric Multiclass Probability Estimation",
    "abstract": "Multiclass probability estimation is the problem of estimating conditional\nprobabilities of a data point belonging to a class given its covariate\ninformation. It has broad applications in statistical analysis and data\nscience. Recently a class of weighted Support Vector Machines (wSVMs) have been\ndeveloped to estimate class probabilities through ensemble learning for\n$K$-class problems (Wang, Shen and Liu, 2008; Wang, Zhang and Wu, 2019), where\n$K$ is the number of classes. The estimators are robust and achieve high\naccuracy for probability estimation, but their learning is implemented through\npairwise coupling, which demand polynomial time in $K$. In this paper, we\npropose two new learning schemes, the baseline learning and the One-vs-All\n(OVA) learning, to further improve wSVMs in terms of computational efficiency\nand estimation accuracy. In particular, the baseline learning has optimal\ncomputational complexity in the sense that it is linear in $K$. The resulting\nestimators are distribution-free and shown to be consistent. We further conduct\nextensive numerical experiments to demonstrate finite sample performance.",
    "descriptor": "",
    "authors": [
      "Liyun Zeng",
      "Hao Helen Zhang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12460"
  },
  {
    "id": "arXiv:2205.12477",
    "title": "An Investigation on Applying Acoustic Feature Conversion to ASR of Adult  and Child Speech",
    "abstract": "The performance of child speech recognition is generally less satisfactory\ncompared to adult speech due to limited amount of training data. Significant\nperformance degradation is expected when applying an automatic speech\nrecognition (ASR) system trained on adult speech to child speech directly, as a\nresult of domain mismatch. The present study is focused on adult-to-child\nacoustic feature conversion to alleviate this mismatch. Different acoustic\nfeature conversion approaches, including deep neural network based and signal\nprocessing based, are investigated and compared under a fair experimental\nsetting, in which converted acoustic features from the same amount of labeled\nadult speech are used to train the ASR models from scratch. Experimental\nresults reveal that not all of the conversion methods lead to ASR performance\ngain. Specifically, as a classic unsupervised domain adaptation method, the\nstatistic matching does not show an effectiveness. A disentanglement-based\nauto-encoder (DAE) conversion framework is found to be useful and the approach\nof F0 normalization achieves the best performance. It is noted that the F0\ndistribution of converted features is an important attribute to reflect the\nconversion quality, while utilizing an adult-child deep classification model to\nmake judgment is shown to be inappropriate.",
    "descriptor": "\nComments: 5 pages, 4 figures, submitted to InterSpeech2022\n",
    "authors": [
      "Wei Liu",
      "Jingyu Li",
      "Tan Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.12477"
  },
  {
    "id": "arXiv:2205.12481",
    "title": "A Convergence Theory for Over-parameterized Variational Quantum  Eigensolvers",
    "abstract": "The Variational Quantum Eigensolver (VQE) is a promising candidate for\nquantum applications on near-term Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Despite a lot of empirical studies and recent progress in\ntheoretical understanding of VQE's optimization landscape, the convergence for\noptimizing VQE is far less understood. We provide the first rigorous analysis\nof the convergence of VQEs in the over-parameterization regime. By connecting\nthe training dynamics with the Riemannian Gradient Flow on the unit-sphere, we\nestablish a threshold on the sufficient number of parameters for efficient\nconvergence, which depends polynomially on the system dimension and the\nspectral ratio, a property of the problem Hamiltonian, and could be resilient\nto gradient noise to some extent. We further illustrate that this\noverparameterization threshold could be vastly reduced for specific VQE\ninstances by establishing an ansatz-dependent threshold paralleling our main\nresult. We showcase that our ansatz-dependent threshold could serve as a proxy\nof the trainability of different VQE ansatzes without performing empirical\nexperiments, which hence leads to a principled way of evaluating ansatz design.\nFinally, we conclude with a comprehensive empirical study that supports our\ntheoretical findings.",
    "descriptor": "",
    "authors": [
      "Xuchen You",
      "Shouvanik Chakrabarti",
      "Xiaodi Wu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12481"
  },
  {
    "id": "arXiv:2205.12501",
    "title": "Using Loaded N-port Structures to Achieve the Continuous-Space  Electromagnetic Channel Capacity Bound",
    "abstract": "A method for achieving the continuous-space electromagnetic channel capacity\nbound using loaded N-port structures is described. It is relevant for the\ndesign of compact multiple-input multiple-output (MIMO) antennas that can\nachieve channel capacity bounds when constrained by size. The method is not\nrestricted to a specific antenna configuration and a closed-form expression for\nthe channel capacity limits are provided with various constraints. Furthermore,\nusing loaded N-port structures to represent arbitrary antenna geometries, an\nefficient optimization approach is proposed for finding the optimum MIMO\nantenna design that achieves the channel capacity bounds. Simulation results of\nthe channel capacity bounds achieved using our MIMO antenna design with one\nsquare wavelength size are provided. These show that at least 18 ports can be\nsupported in one square wavelength and achieve the continuous-space\nelectromagnetic channel capacity bound. The results demonstrate that our method\ncan link continuous-space electromagnetic channel capacity bounds to MIMO\nantenna design.",
    "descriptor": "",
    "authors": [
      "Zixiang Han",
      "Shanpu Shen",
      "Yujie Zhang",
      "Shiwen Tang",
      "Chi-Yuk Chiu",
      "Ross Murch"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12501"
  },
  {
    "id": "arXiv:2205.12632",
    "title": "Robust Differential Dynamic Programming",
    "abstract": "Differential Dynamic Programming is an optimal control technique often used\nfor trajectory generation. Many variations of this algorithm have been\ndeveloped in the literature, including algorithms for stochastic dynamics or\nstate and input constraints. In this contribution, we develop a robust version\nof Differential Dynamic Programming that uses generalized plants and multiplier\nrelaxations for uncertainties. To this end, we study a version of the Bellman\nprinciple and use convex relaxations to account for uncertainties in the\ndynamic program. The resulting algorithm can be seen as a robust trajectory\ngeneration tool for nonlinear systems.",
    "descriptor": "\nComments: submitted to IEEE Conference on Decision and Control, 2022\n",
    "authors": [
      "Dennis Gramlich",
      "Carsten W. Scherer",
      "Christian Ebenbauer"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.12632"
  },
  {
    "id": "arXiv:2205.12642",
    "title": "On the Interpretability of Regularisation for Neural Networks Through  Model Gradient Similarity",
    "abstract": "Most complex machine learning and modelling techniques are prone to\nover-fitting and may subsequently generalise poorly to future data. Artificial\nneural networks are no different in this regard and, despite having a level of\nimplicit regularisation when trained with gradient descent, often require the\naid of explicit regularisers. We introduce a new framework, Model Gradient\nSimilarity (MGS), that (1) serves as a metric of regularisation, which can be\nused to monitor neural network training, (2) adds insight into how explicit\nregularisers, while derived from widely different principles, operate via the\nsame mechanism underneath by increasing MGS, and (3) provides the basis for a\nnew regularisation scheme which exhibits excellent performance, especially in\nchallenging settings such as high levels of label noise or limited sample\nsizes.",
    "descriptor": "",
    "authors": [
      "Vincent Szolnoky",
      "Viktor Andersson",
      "Balazs Kulcsar",
      "Rebecka J\u00f6rnsten"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.12642"
  },
  {
    "id": "arXiv:2205.12667",
    "title": "Trilateration-Based Device-Free Sensing: Two Base Stations and One  Passive IRS Are Sufficient",
    "abstract": "The classic trilateration technique can localize each target based on its\ndistances to three anchors with known coordinates. Usually, this technique\nrequires all the anchors and targets, e.g., the satellites and the mobile\nphones in Global Navigation Satellite System (GNSS), to actively\ntransmit/receive radio signals such that the delay of the one-way radio signal\npropagated between each anchor and each target can be measured. Excitingly,\nthis paper will show that the trilateration technique can be generalized to the\nscenario where one of the three anchors and all the targets merely reflect the\nradio signals passively as in radar networks, even if the propagation delay\nbetween the passive IRS and the passive targets is difficult to be measured\ndirectly, and the data association issue for multi-sensor multi-target tracking\narises. Specifically, we consider device-free sensing in a cellular network\nconsisting of two base stations (BSs), one passive intelligent reflecting\nsurface (IRS), and multiple passive targets, to realize integrated sensing and\ncommunication (ISAC). The two BSs transmit the orthogonal frequency division\nmultiplexing (OFDM) signals in the downlink and estimate the locations of the\ntargets based on their reflected signals via/not via the IRS. We propose an\nefficient trilateration-based strategy that can first estimate the distances of\neach target to the two BSs and the IRS and then localize the targets. Numerical\nresults show that the considered networked sensing architecture with\nheterogenous anchors can outperform its counterpart with three BSs.",
    "descriptor": "\nComments: submitted for possible publication\n",
    "authors": [
      "Qipeng Wang",
      "Liang Liu",
      "Shuowen Zhang",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.12667"
  },
  {
    "id": "arXiv:2205.12695",
    "title": "Surprises in adversarially-trained linear regression",
    "abstract": "State-of-the-art machine learning models can be vulnerable to very small\ninput perturbations that are adversarially constructed. Adversarial training is\none of the most effective approaches to defend against such examples. We show\nthat for linear regression problems, adversarial training can be formulated as\na convex problem. This fact is then used to show that $\\ell_\\infty$-adversarial\ntraining produces sparse solutions and has many similarities to the lasso\nmethod. Similarly, $\\ell_2$-adversarial training has similarities with ridge\nregression. We use a robust regression framework to analyze and understand\nthese similarities and also point to some differences. Finally, we show how\nadversarial training behaves differently from other regularization methods when\nestimating overparameterized models (i.e., models with more parameters than\ndatapoints). It minimizes a sum of three terms which regularizes the solution,\nbut unlike lasso and ridge regression, it can sharply transition into an\ninterpolation mode. We show that for sufficiently many features or sufficiently\nsmall regularization parameters, the learned model perfectly interpolates the\ntraining data while still exhibiting good out-of-sample performance.",
    "descriptor": "",
    "authors": [
      "Ant\u00f4nio H. Ribeiro",
      "Dave Zachariah",
      "Thomas B. Sch\u00f6n"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.12695"
  },
  {
    "id": "arXiv:2205.12705",
    "title": "COVID-19 Severity Classification on Chest X-ray Images",
    "abstract": "Biomedical imaging analysis combined with artificial intelligence (AI)\nmethods has proven to be quite valuable in order to diagnose COVID-19. So far,\nvarious classification models have been used for diagnosing COVID-19. However,\nclassification of patients based on their severity level is not yet analyzed.\nIn this work, we classify covid images based on the severity of the infection.\nFirst, we pre-process the X-ray images using a median filter and histogram\nequalization. Enhanced X-ray images are then augmented using SMOTE technique\nfor achieving a balanced dataset. Pre-trained Resnet50, VGG16 model and SVM\nclassifier are then used for feature extraction and classification. The result\nof the classification model confirms that compared with the alternatives, with\nchest X-Ray images, the ResNet-50 model produced remarkable classification\nresults in terms of accuracy (95%), recall (0.94), and F1-Score (0.92), and\nprecision (0.91).",
    "descriptor": "",
    "authors": [
      "Aditi Sagar",
      "Aman Swaraj",
      "Karan Verma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12705"
  },
  {
    "id": "arXiv:2205.12727",
    "title": "Semantic-preserved Communication System for Highly Efficient Speech  Transmission",
    "abstract": "Deep learning (DL) based semantic communication methods have been explored\nfor the efficient transmission of images, text, and speech in recent years. In\ncontrast to traditional wireless communication methods that focus on the\ntransmission of abstract symbols, semantic communication approaches attempt to\nachieve better transmission efficiency by only sending the semantic-related\ninformation of the source data. In this paper, we consider semantic-oriented\nspeech transmission which transmits only the semantic-relevant information over\nthe channel for the speech recognition task, and a compact additional set of\nsemantic-irrelevant information for the speech reconstruction task. We propose\na novel end-to-end DL-based transceiver which extracts and encodes the semantic\ninformation from the input speech spectrums at the transmitter and outputs the\ncorresponding transcriptions from the decoded semantic information at the\nreceiver. For the speech to speech transmission, we further include a CTC\nalignment module that extracts a small number of additional semantic-irrelevant\nbut speech-related information for the better reconstruction of the original\nspeech signals at the receiver. The simulation results confirm that our\nproposed method outperforms current methods in terms of the accuracy of the\npredicted text for the speech to text transmission and the quality of the\nrecovered speech signals for the speech to speech transmission, and\nsignificantly improves transmission efficiency. More specifically, the proposed\nmethod only sends 16% of the amount of the transmitted symbols required by the\nexisting methods while achieving about 10% reduction in WER for the speech to\ntext transmission. For the speech to speech transmission, it results in an even\nmore remarkable improvement in terms of transmission efficiency with only 0.2%\nof the amount of the transmitted symbols required by the existing method.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2202.03211\n",
    "authors": [
      "Tianxiao Han",
      "Qianqian Yang",
      "Zhiguo Shi",
      "Shibo He",
      "Zhaoyang Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.12727"
  },
  {
    "id": "arXiv:2205.12728",
    "title": "Step Size is a Consequential Parameter in Continuous Cellular Automata",
    "abstract": "Step size in continuous cellular automata (CA) plays an important role in the\nstability and behavior of self-organizing patterns. Continous CA dynamics are\ndefined by formula very similar to numerical estimation of physics-based\nordinary differential equations, specifically Euler's method, for which a large\nstep size is often inaccurate and unstable. Rather than asymptotically\napproaching more accurate estimates of CA dynamics with decreasing step size,\ncontinuous CA may support different self-organizing patterns at different\nranges of step size. We discuss several examples of mobile patterns that become\nunstable at step sizes that are too small as well as too large. Additionally,\nan individual mobile pattern may exhibit qualitatively different behavior\nacross a range of step sizes. We demonstrate examples of the effects of step\nsize in pattern stability and qualitative behavior in continuous CA implemented\nin the Lenia framework and its variant, Glaberish.",
    "descriptor": "\nComments: Accepted to ALife 2022 as an extended abstract\n",
    "authors": [
      "Q. Tyrell Davis",
      "Josh Bongard"
    ],
    "subjectives": [
      "Cellular Automata and Lattice Gases (nlin.CG)",
      "Other Computer Science (cs.OH)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2205.12728"
  },
  {
    "id": "arXiv:2205.12729",
    "title": "Deep interpretable ensembles",
    "abstract": "Ensembles improve prediction performance and allow uncertainty quantification\nby aggregating predictions from multiple models. In deep ensembling, the\nindividual models are usually black box neural networks, or recently, partially\ninterpretable semi-structured deep transformation models. However,\ninterpretability of the ensemble members is generally lost upon aggregation.\nThis is a crucial drawback of deep ensembles in high-stake decision fields, in\nwhich interpretable models are desired. We propose a novel transformation\nensemble which aggregates probabilistic predictions with the guarantee to\npreserve interpretability and yield uniformly better predictions than the\nensemble members on average. Transformation ensembles are tailored towards\ninterpretable deep transformation models but are applicable to a wider range of\nprobabilistic neural networks. In experiments on several publicly available\ndata sets, we demonstrate that transformation ensembles perform on par with\nclassical deep ensembles in terms of prediction performance, discrimination,\nand calibration. In addition, we demonstrate how transformation ensembles\nquantify both aleatoric and epistemic uncertainty, and produce minimax optimal\npredictions under certain conditions.",
    "descriptor": "\nComments: 22 pages main text, 8 figures\n",
    "authors": [
      "Lucas Kook",
      "Andrea G\u00f6tschi",
      "Philipp FM Baumann",
      "Torsten Hothorn",
      "Beate Sick"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12729"
  },
  {
    "id": "arXiv:2205.12731",
    "title": "Machine learning methods for Schlieren imaging of a plasma channel in  tenuous atomic vapor",
    "abstract": "We investigate the usage of a Schlieren imaging setup to measure the\ngeometrical dimensions of a plasma channel in atomic vapor. Near resonant probe\nlight is used to image the plasma channel in a tenuous vapor and machine\nlearning techniques are tested for extracting quantitative information from the\nimages. By building a database of simulated signals with a range of plasma\nparameters for training Deep Neural Networks, we demonstrate that they can\nextract from the Schlieren images reliably and with high accuracy the location,\nthe radius and the maximum ionization fraction of the plasma channel as well as\nthe width of the transition region between the core of the plasma channel and\nthe unionized vapor. We test several different neural network architectures\nwith supervised learning and show that the parameter estimations supplied by\nthe networks are resilient with respect to slight changes of the experimental\nparameters that may occur in the course of a measurement.",
    "descriptor": "\nComments: 26 pages, 13 figures, 1 table\n",
    "authors": [
      "G\u00e1bor B\u00edr\u00f3",
      "Mih\u00e1ly Pocsai",
      "Imre Ferenc Barna",
      "Joshua T. Moody",
      "G\u00e1bor Demeter"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12731"
  },
  {
    "id": "arXiv:2205.12734",
    "title": "Global geomagnetic perturbation forecasting using Deep Learning",
    "abstract": "Geomagnetically Induced Currents (GICs) arise from spatio-temporal changes to\nEarth's magnetic field which arise from the interaction of the solar wind with\nEarth's magnetosphere, and drive catastrophic destruction to our\ntechnologically dependent society. Hence, computational models to forecast GICs\nglobally with large forecast horizon, high spatial resolution and temporal\ncadence are of increasing importance to perform prompt necessary mitigation.\nSince GIC data is proprietary, the time variability of horizontal component of\nthe magnetic field perturbation (dB/dt) is used as a proxy for GICs. In this\nwork, we develop a fast, global dB/dt forecasting model, which forecasts 30\nminutes into the future using only solar wind measurements as input. The model\nsummarizes 2 hours of solar wind measurement using a Gated Recurrent Unit, and\ngenerates forecasts of coefficients which are folded with a spherical harmonic\nbasis to enable global forecasts. When deployed, our model produces results in\nunder a second, and generates global forecasts for horizontal magnetic\nperturbation components at 1-minute cadence. We evaluate our model across\nmodels in literature for two specific storms of 5 August 2011 and 17 March\n2015, while having a self-consistent benchmark model set. Our model\noutperforms, or has consistent performance with state-of-the-practice high time\ncadence local and low time cadence global models, while also\noutperforming/having comparable performance with the benchmark models. Such\nquick inferences at high temporal cadence and arbitrary spatial resolutions may\nultimately enable accurate forewarning of dB/dt for any place on Earth,\nresulting in precautionary measures to be taken in an informed manner.",
    "descriptor": "\nComments: 23 pages, 8 figures, 5 tables; accepted for publication in AGU: Spaceweather\n",
    "authors": [
      "Vishal Upendran",
      "Panagiotis Tigas",
      "Banafsheh Ferdousi",
      "Teo Bloch",
      "Mark C. M. Cheung",
      "Siddha Ganju",
      "Asti Bhatt",
      "Ryan M. McGranaghan",
      "Yarin Gal"
    ],
    "subjectives": [
      "Space Physics (physics.space-ph)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12734"
  },
  {
    "id": "arXiv:2205.12746",
    "title": "Machine learning method for return direction forecasting of Exchange  Traded Funds using classification and regression models",
    "abstract": "This article aims to propose and apply a machine learning method to analyze\nthe direction of returns from Exchange Traded Funds (ETFs) using the historical\nreturn data of its components, helping to make investment strategy decisions\nthrough a trading algorithm. In methodological terms, regression and\nclassification models were applied, using standard datasets from Brazilian and\nAmerican markets, in addition to algorithmic error metrics. In terms of\nresearch results, they were analyzed and compared to those of the Na\\\"ive\nforecast and the returns obtained by the buy & hold technique in the same\nperiod of time. In terms of risk and return, the models mostly performed better\nthan the control metrics, with emphasis on the linear regression model and the\nclassification models by logistic regression, support vector machine (using the\nLinearSVC model), Gaussian Naive Bayes and K-Nearest Neighbors, where in\ncertain datasets the returns exceeded by two times and the Sharpe ratio by up\nto four times those of the buy & hold control model.",
    "descriptor": "\nComments: 21 pages, 8 figures and 12 tables\n",
    "authors": [
      "Raphael P. B. Piovezan",
      "Pedro Paulo de Andrade Junior"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Pricing of Securities (q-fin.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12746"
  },
  {
    "id": "arXiv:2205.12751",
    "title": "Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe  Algorithm under Parallelization",
    "abstract": "We consider the problem of minimizing the sum of two convex functions. One of\nthose functions has Lipschitz-continuous gradients, and can be accessed via\nstochastic oracles, whereas the other is \"simple\". We provide a Bregman-type\nalgorithm with accelerated convergence in function values to a ball containing\nthe minimum. The radius of this ball depends on problem-dependent constants,\nincluding the variance of the stochastic oracle. We further show that this\nalgorithmic setup naturally leads to a variant of Frank-Wolfe achieving\nacceleration under parallelization. More precisely, when minimizing a smooth\nconvex function on a bounded domain, we show that one can achieve an $\\epsilon$\nprimal-dual gap (in expectation) in $\\tilde{O}(1/ \\sqrt{\\epsilon})$ iterations,\nby only accessing gradients of the original function and a linear maximization\noracle with $O(1/\\sqrt{\\epsilon})$ computing units in parallel. We illustrate\nthis fast convergence on synthetic numerical experiments.",
    "descriptor": "",
    "authors": [
      "Benjamin Dubois-Taine",
      "Francis Bach",
      "Quentin Berthet",
      "Adrien Taylor"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12751"
  },
  {
    "id": "arXiv:2205.12803",
    "title": "Graph Agnostic Randomized Experimental Design",
    "abstract": "Randomized experiments are widely used to estimate causal effects of proposed\n\"treatments\" in domains spanning across physical sciences, social sciences,\nmedicine, and technology industries. However, classical approaches to\nexperimental design rely on critical independence assumptions that are violated\nwhen the outcome of an individual a may be affected by the treatment of another\nindividual b, referred to as network interference. Under such network\ninterference, naively using popular estimators and randomized experimental\ndesigns can result in significant bias and loss of efficiency. We consider a\nheterogeneous linear outcomes model that can capture network interference that\narises from spillover, peer effects, and contagion. Under this model, we\ncharacterize the limitations and possibilities for estimating the total\ntreatment effect, average direct treatment effect, and average interference\neffect. Given access to average historical baseline measurements prior to the\nexperiment, we propose simple estimators and randomized designs that output\nunbiased estimates with low variance for these three estimands. Furthermore,\nour solution and statistical guarantees do not require knowledge of the\nunderlying network structure, and thus can be used for scenarios where the\nnetwork is unknown and complex. We believe our results are poised to impact\ncurrent randomized experimentation strategies due to its ease of interpretation\nand implementation, alongside its provable statistical guarantees under\nheterogeneous network effects.",
    "descriptor": "",
    "authors": [
      "Christina Lee Yu",
      "Edoardo M Airoldi",
      "Christian Borgs",
      "Jennifer T Chayes"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.12803"
  },
  {
    "id": "arXiv:2205.12841",
    "title": "Removing the fat from your posterior samples with margarine",
    "abstract": "Bayesian workflows often require the introduction of nuisance parameters, yet\nfor core science modelling one needs access to a marginal posterior density. In\nthis work we use masked autoregressive flows and kernel density estimators to\nencapsulate the marginal posterior, allowing us to compute marginal\nKullback-Leibler divergences and marginal Bayesian model dimensionalities in\naddition to generating samples and computing marginal log probabilities. We\ndemonstrate this in application to topical cosmological examples of the Dark\nEnergy Survey, and global 21cm signal experiments. In addition to the\ncomputation of marginal Bayesian statistics, this work is important for further\napplications in Bayesian experimental design, complex prior modelling and\nlikelihood emulation. This technique is made publicly available in the\npip-installable code margarine.",
    "descriptor": "\nComments: Submitted to NeurIPS\n",
    "authors": [
      "Harry T. J. Bevins",
      "William J. Handley",
      "Pablo Lemos",
      "Peter H. Sims",
      "Eloy de Lera Acedo",
      "Anastasia Fialkov",
      "Justin Alsing"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12841"
  },
  {
    "id": "arXiv:2205.12843",
    "title": "A Comparative Study of Gastric Histopathology Sub-size Image  Classification: from Linear Regression to Visual Transformer",
    "abstract": "Gastric cancer is the fifth most common cancer in the world. At the same\ntime, it is also the fourth most deadly cancer. Early detection of cancer\nexists as a guide for the treatment of gastric cancer. Nowadays, computer\ntechnology has advanced rapidly to assist physicians in the diagnosis of\npathological pictures of gastric cancer. Ensemble learning is a way to improve\nthe accuracy of algorithms, and finding multiple learning models with\ncomplementarity types is the basis of ensemble learning. The complementarity of\nsub-size pathology image classifiers when machine performance is insufficient\nis explored in this experimental platform. We choose seven classical machine\nlearning classifiers and four deep learning classifiers for classification\nexperiments on the GasHisSDB database. Among them, classical machine learning\nalgorithms extract five different image virtual features to match multiple\nclassifier algorithms. For deep learning, we choose three convolutional neural\nnetwork classifiers. In addition, we also choose a novel Transformer-based\nclassifier. The experimental platform, in which a large number of classical\nmachine learning and deep learning methods are performed, demonstrates that\nthere are differences in the performance of different classifiers on GasHisSDB.\nClassical machine learning models exist for classifiers that classify Abnormal\ncategories very well, while classifiers that excel in classifying Normal\ncategories also exist. Deep learning models also exist with multiple models\nthat can be complementarity. Suitable classifiers are selected for ensemble\nlearning, when machine performance is insufficient. This experimental platform\ndemonstrates that multiple classifiers are indeed complementarity and can\nimprove the efficiency of ensemble learning. This can better assist doctors in\ndiagnosis, improve the detection of gastric cancer, and increase the cure rate.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2106.02473\n",
    "authors": [
      "Weiming Hu",
      "Haoyuan Chen",
      "Wanli Liu",
      "Xiaoyan Li",
      "Hongzan Sun",
      "Xinyu Huang",
      "Marcin Grzegorzek",
      "Chen Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12843"
  },
  {
    "id": "arXiv:2205.12857",
    "title": "Structure Unbiased Adversarial Model for Medical Image Segmentation",
    "abstract": "Generative models have been widely proposed in image recognition to generate\nmore images where the distribution is similar to that of the real images. It\noften introduces a discriminator network to discriminate original real data and\ngenerated data.\nHowever, such discriminator often considers the distribution of the data and\ndid not pay enough attention to the intrinsic gap due to structure.\nIn this paper, we reformulate a new image to image translation problem to\nreduce structural gap, in addition to the typical intensity distribution gap.\nWe further propose a simple yet important Structure Unbiased Adversarial Model\nfor Medical Image Segmentation (SUAM) with learnable inverse structural\ndeformation for medical image segmentation. It consists of a structure\nextractor, an attention diffeomorphic registration and a structure \\& intensity\ndistribution rendering module. The structure extractor aims to extract the\ndominant structure of the input image. The attention diffeomorphic registration\nis proposed to reduce the structure gap with an inverse deformation field to\nwarp the prediction masks back to their original form. The structure rendering\nmodule is to render the deformed structure to an image with targeted intensity\ndistribution. We apply the proposed SUAM on both optical coherence tomography\n(OCT), magnetic resonance imaging (MRI) and computerized tomography (CT) data.\nExperimental results show that the proposed method has the capability to\ntransfer both intensity and structure distributions.",
    "descriptor": "",
    "authors": [
      "Tianyang Zhang",
      "Shaoming Zheng",
      "Jun Cheng",
      "Xi Jia",
      "Joseph Bartlett",
      "Huazhu Fu",
      "Zhaowen Qiu",
      "Jiang Liu",
      "Jinming Duan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12857"
  },
  {
    "id": "arXiv:2205.12881",
    "title": "A Continuum Model of Stable Matching With Finite Capacities",
    "abstract": "This paper introduces a unified framework for stable matching, which nests\nthe traditional definition of stable matching in finite markets and the\ncontinuum definition of stable matching from Azevedo and Leshno (2016) as\nspecial cases. Within this framework, I identify a novel continuum model, which\nmakes individual-level probabilistic predictions.\nThis new model always has a unique stable outcome, which can be found using\nan analog of the Deferred Acceptance algorithm. The crucial difference between\nthis model and that of Azevedo and Leshno (2016) is that they assume that the\namount of student interest at each school is deterministic, whereas my proposed\nalternative assumes that it follows a Poisson distribution. As a result, this\nnew model accurately predicts the simulated distribution of cutoffs, even for\nmarkets with only ten schools and twenty students.\nThis model generates new insights about the number and quality of matches.\nWhen schools are homogeneous, it provides upper and lower bounds on students'\naverage rank, which match results from Ashlagi, Kanoria and Leshno (2017) but\napply to more general settings. This model also provides clean analytical\nexpressions for the number of matches in a platform pricing setting considered\nby Marx and Schummer (2021).",
    "descriptor": "",
    "authors": [
      "Nick Arnosti"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2205.12881"
  },
  {
    "id": "arXiv:2205.12900",
    "title": "Differentially Private Data Generation Needs Better Features",
    "abstract": "Training even moderately-sized generative models with differentially-private\nstochastic gradient descent (DP-SGD) is difficult: the required level of noise\nfor reasonable levels of privacy is simply too large. We advocate instead\nbuilding off a good, relevant representation on public data, then using private\ndata only for \"transfer learning.\" In particular, we minimize the maximum mean\ndiscrepancy (MMD) between private target data and the generated distribution,\nusing a kernel based on perceptual features from a public dataset. With the\nMMD, we can simply privatize the data-dependent term once and for all, rather\nthan introducing noise at each step of optimization as in DP-SGD. Our algorithm\nallows us to generate CIFAR10-level images faithfully with $\\varepsilon \\approx\n2$, far surpassing the current state of the art, which only models MNIST and\nFashionMNIST at $\\varepsilon \\approx 10$. Our work introduces simple yet\npowerful foundations for reducing the gap between private and non-private deep\ngenerative models.",
    "descriptor": "",
    "authors": [
      "Fredrik Harder",
      "Milad Jalali Asadabadi",
      "Danica J. Sutherland",
      "Mijung Park"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12900"
  },
  {
    "id": "arXiv:2205.12902",
    "title": "RADNet: Ensemble Model for Robust Glaucoma Classification in Color  Fundus Images",
    "abstract": "Glaucoma is one of the most severe eye diseases, characterized by rapid\nprogression and leading to irreversible blindness. It is often the case that\npathology diagnostics is carried out when the one's sight has already\nsignificantly degraded due to the lack of noticeable symptoms at early stage of\nthe disease. Regular glaucoma screenings of the population shall improve\nearly-stage detection, however the desirable frequency of etymological checkups\nis often not feasible due to excessive load imposed by manual diagnostics on\nlimited number of specialists. Considering the basic methodology to detect\nglaucoma is to analyze fundus images for the \\textit{optic-disc-to-optic-cup\nratio}, Machine Learning domain can offer sophisticated tooling for image\nprocessing and classification. In our work, we propose an advanced image\npre-processing technique combined with an ensemble of deep classification\nnetworks. Our \\textit{Retinal Auto Detection (RADNet)} model has been\nsuccessfully tested on Rotterdam EyePACS AIROGS train dataset with AUC of 0.92,\nand then additionally finetuned and tested on a fraction of RIM-ONE DL dataset\nwith AUC of 0.91.",
    "descriptor": "\nComments: Keywords: Glaucoma Classification, Color Fundus Images. Computer Aided Diagnosis\n",
    "authors": [
      "Dmitrii Medvedev",
      "Rand Muhtaseb",
      "Ahmed Al Mahrooqi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12902"
  },
  {
    "id": "arXiv:2205.12909",
    "title": "Upper bound for the number of privileged words",
    "abstract": "A non-empty word $w$ is a \\emph{border} of a word $u$ if $\\vert w\\vert<\\vert\nu\\vert$ and $w$ is both a prefix and a suffix of $u$. A word $u$ is\n\\emph{privileged} if $\\vert u\\vert\\leq 1$ or if $u$ has a privileged border $w$\nthat appears exactly twice in $u$. Peltom\\\"aki (2016) presented the following\nopen problem: ``Give a nontrivial upper bound for $B(n)$'', where $B(n)$\ndenotes the number of privileged words of length $n$.\nLet $\\ln^{[0]}{(n)}=n$ and let $\\ln^{[j]}{(n)}=\\ln{(\\ln^{[j-1]}{(n)})}$,\nwhere $j,n$ are positive integers. We show that if $q>1$ is a size of the\nalphabet and $j\\geq 3$ is an integer then there are constants $\\alpha_j$ and\n$n_j$ such that \\[B(n)\\leq\n\\alpha_j\\frac{q^{n}\\sqrt{\\ln{n}}}{\\sqrt{n}}\\ln^{[j]}{(n)}\\prod_{i=2}^{j-1}\\sqrt{\\ln^{[i]}(n)}\\mbox{,\nwhere }n\\geq n_j\\mbox{.}\\] This result improves the upper bound of Rukavicka\n(2020).",
    "descriptor": "\nComments: 13 pages, 0 figures\n",
    "authors": [
      "Josef Rukavicka"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.12909"
  },
  {
    "id": "arXiv:2205.12927",
    "title": "Quantum security and theory of decoherence",
    "abstract": "We sketch a relation between two crucial, yet independent, fields in quantum\ninformation research, viz. quantum decoherence and quantum cryptography. We\ninvestigate here how the standard cryptographic assumption of shielded\nlaboratory, stating that data generated by a secure quantum device remain\nprivate unless explicitly published, is disturbed by the einselection mechanism\nof quantum Darwinism explaining the measurement process by interaction with the\nexternal environment. We illustrate the idea with a paradigmatic example of a\nquantum random number generator compromised by an analog of the Van Eck\nphreaking. In particular, we derive a trade-off relation between eavesdropper's\nguessing probability $P_{guess}$ and the collective decoherence factor $\\Gamma$\nof the simple form $P_{guess} + \\Gamma \\geq 1$.",
    "descriptor": "\nComments: A short idea with an illustration. 5 pages\n",
    "authors": [
      "Piotr Mironowicz"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12927"
  },
  {
    "id": "arXiv:2205.12932",
    "title": "Prime Holdout Problems",
    "abstract": "This paper introduces prime holdout problems, a problem class related to the\nCollatz conjecture. After applying a linear function, instead of removing a\nfinite set of prime factors, a holdout problem specifies a set of primes to be\nretained. A proof that all positive integers converge to 1 is given for both a\nfinite and an infinite holdout problem. It is conjectured that finite holdout\nproblems cannot diverge for any starting value, which has implications for\ndivergent sequences in the Collatz conjecture.",
    "descriptor": "\nComments: 7 pages, no figures\n",
    "authors": [
      "Max Milkert",
      "Alex Ruchti",
      "Josiah Yoder"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.12932"
  },
  {
    "id": "arXiv:2205.12933",
    "title": "Boosting Tail Neural Network for Realtime Custom Keyword Spotting",
    "abstract": "In this paper, we propose a Boosting Tail Neural Network (BTNN) for improving\nthe performance of Realtime Custom Keyword Spotting (RCKS) that is still an\nindustrial challenge for demanding powerful classification ability with limited\ncomputation resources. Inspired by Brain Science that a brain is only partly\nactivated for a nerve simulation and numerous machine learning algorithms are\ndeveloped to use a batch of weak classifiers to resolve arduous problems, which\nare often proved to be effective. We show that this method is helpful to the\nRCKS problem. The proposed approach achieve better performances in terms of\nwakeup rate and false alarm.\nIn our experiments compared with those traditional algorithms that use only\none strong classifier, it gets 18\\% relative improvement. We also point out\nthat this approach may be promising in future ASR exploration.",
    "descriptor": "\nComments: 4 pages, 8 figures, 2 tables\n",
    "authors": [
      "Sihao Xue",
      "Qianyao Shen",
      "Guoqing Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.12933"
  },
  {
    "id": "arXiv:2205.12937",
    "title": "Mitigating multiple descents: A model-agnostic framework for risk  monotonization",
    "abstract": "Recent empirical and theoretical analyses of several commonly used prediction\nprocedures reveal a peculiar risk behavior in high dimensions, referred to as\ndouble/multiple descent, in which the asymptotic risk is a non-monotonic\nfunction of the limiting aspect ratio of the number of features or parameters\nto the sample size. To mitigate this undesirable behavior, we develop a general\nframework for risk monotonization based on cross-validation that takes as input\na generic prediction procedure and returns a modified procedure whose\nout-of-sample prediction risk is, asymptotically, monotonic in the limiting\naspect ratio. As part of our framework, we propose two data-driven\nmethodologies, namely zero- and one-step, that are akin to bagging and\nboosting, respectively, and show that, under very mild assumptions, they\nprovably achieve monotonic asymptotic risk behavior. Our results are applicable\nto a broad variety of prediction procedures and loss functions, and do not\nrequire a well-specified (parametric) model. We exemplify our framework with\nconcrete analyses of the minimum $\\ell_2$, $\\ell_1$-norm least squares\nprediction procedures. As one of the ingredients in our analysis, we also\nderive novel additive and multiplicative forms of oracle risk inequalities for\nsplit cross-validation that are of independent interest.",
    "descriptor": "\nComments: 110 pages, 15 figures\n",
    "authors": [
      "Pratik Patil",
      "Arun Kumar Kuchibhotla",
      "Yuting Wei",
      "Alessandro Rinaldo"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12937"
  },
  {
    "id": "arXiv:2205.12940",
    "title": "Conformal Prediction Intervals with Temporal Dependence",
    "abstract": "Cross-sectional prediction is common in many domains such as healthcare,\nincluding forecasting tasks using electronic health records, where different\npatients form a cross-section. We focus on the task of constructing valid\nprediction intervals (PIs) in time-series regression with a cross-section. A\nprediction interval is considered valid if it covers the true response with (a\npre-specified) high probability. We first distinguish between two notions of\nvalidity in such a setting: cross-sectional and longitudinal. Cross-sectional\nvalidity is concerned with validity across the cross-section of the time series\ndata, while longitudinal validity accounts for the temporal dimension. Coverage\nguarantees along both these dimensions are ideally desirable; however, we show\nthat distribution-free longitudinal validity is theoretically impossible.\nDespite this limitation, we propose Conformal Prediction with Temporal\nDependence (CPTD), a procedure which is able to maintain strict cross-sectional\nvalidity while improving longitudinal coverage. CPTD is post-hoc and\nlight-weight, and can easily be used in conjunction with any prediction model\nas long as a calibration set is available. We focus on neural networks due to\ntheir ability to model complicated data such as diagnosis codes for time-series\nregression, and perform extensive experimental validation to verify the\nefficacy of our approach. We find that CPTD outperforms baselines on a variety\nof datasets by improving longitudinal coverage and often providing more\nefficient (narrower) PIs.",
    "descriptor": "\nComments: 15 pages (main paper, including references) + 4 pages (supplementary material)\n",
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.12940"
  },
  {
    "id": "arXiv:1808.03260",
    "title": "Few Cuts Meet Many Point Sets",
    "abstract": "Few Cuts Meet Many Point Sets",
    "descriptor": "",
    "authors": [
      "Sariel Har-Peled",
      "Mitchell Jones"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/1808.03260"
  },
  {
    "id": "arXiv:1901.05894",
    "title": "LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation  Function for Neural Networks",
    "abstract": "LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation  Function for Neural Networks",
    "descriptor": "",
    "authors": [
      "Swalpa Kumar Roy",
      "Suvojit Manna",
      "Shiv Ram Dubey",
      "Bidyut Baran Chaudhuri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1901.05894"
  },
  {
    "id": "arXiv:1904.07957",
    "title": "Almost-Smooth Histograms and Sliding-Window Graph Algorithms",
    "abstract": "Almost-Smooth Histograms and Sliding-Window Graph Algorithms",
    "descriptor": "",
    "authors": [
      "Robert Krauthgamer",
      "David Reitblat"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1904.07957"
  },
  {
    "id": "arXiv:1907.00586",
    "title": "A Kernel Stein Test for Comparing Latent Variable Models",
    "abstract": "A Kernel Stein Test for Comparing Latent Variable Models",
    "descriptor": "",
    "authors": [
      "Heishiro Kanagawa",
      "Wittawat Jitkrittum",
      "Lester Mackey",
      "Kenji Fukumizu",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1907.00586"
  },
  {
    "id": "arXiv:2002.03977",
    "title": "Multimodal active speaker detection and virtual cinematography for video  conferencing",
    "abstract": "Multimodal active speaker detection and virtual cinematography for video  conferencing",
    "descriptor": "",
    "authors": [
      "Ross Cutler",
      "Ramin Mehran",
      "Sam Johnson",
      "Cha Zhang",
      "Adam Kirk",
      "Oliver Whyte",
      "Adarsh Kowdle"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.03977"
  },
  {
    "id": "arXiv:2002.10371",
    "title": "A Hardware Architecture for Reconfigurable Intelligent Surfaces with  Minimal Active Elements for Explicit Channel Estimation",
    "abstract": "Comments: 5 pages, 2 figures, invited/accepted to IEEE ICASSP 2020",
    "descriptor": "\nComments: 5 pages, 2 figures, invited/accepted to IEEE ICASSP 2020\n",
    "authors": [
      "George C. Alexandropoulos",
      "Evangelos Vlachos"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2002.10371"
  },
  {
    "id": "arXiv:2003.03886",
    "title": "Divided Differences, Falling Factorials, and Discrete Splines: Another  Look at Trend Filtering and Related Problems",
    "abstract": "Comments: 75 pages, 9 figures; 1 table",
    "descriptor": "\nComments: 75 pages, 9 figures; 1 table\n",
    "authors": [
      "Ryan J. Tibshirani"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Numerical Analysis (math.NA)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2003.03886"
  },
  {
    "id": "arXiv:2004.09679",
    "title": "MGX: Near-Zero Overhead Memory Protection for Data-Intensive  Accelerators",
    "abstract": "Comments: Accepted to the 49th International Symposium on Computer Architecture (ISCA'22)",
    "descriptor": "\nComments: Accepted to the 49th International Symposium on Computer Architecture (ISCA'22)\n",
    "authors": [
      "Weizhe Hua",
      "Muhammad Umar",
      "Zhiru Zhang",
      "G. Edward Suh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.09679"
  },
  {
    "id": "arXiv:2007.01442",
    "title": "Multi-Agent Low-Dimensional Linear Bandits",
    "abstract": "Comments: To appear in IEEE Transactions on Automatic Control",
    "descriptor": "\nComments: To appear in IEEE Transactions on Automatic Control\n",
    "authors": [
      "Ronshee Chawla",
      "Abishek Sankararaman",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.01442"
  },
  {
    "id": "arXiv:2008.11632",
    "title": "GuardNN: Secure Accelerator Architecture for Privacy-Preserving Deep  Learning",
    "abstract": "Comments: Accepted to the 59th Design Automation Conference (DAC'22)",
    "descriptor": "\nComments: Accepted to the 59th Design Automation Conference (DAC'22)\n",
    "authors": [
      "Weizhe Hua",
      "Muhammad Umar",
      "Zhiru Zhang",
      "G. Edward Suh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.11632"
  },
  {
    "id": "arXiv:2009.05823",
    "title": "On Achieving Leximin Fairness and Stability in Many-to-One Matchings",
    "abstract": "On Achieving Leximin Fairness and Stability in Many-to-One Matchings",
    "descriptor": "",
    "authors": [
      "Shivika Narang",
      "Arpita Biswas",
      "Y Narahari"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2009.05823"
  },
  {
    "id": "arXiv:2011.01174",
    "title": "Learning to Maximize Speech Quality Directly Using MOS Prediction for  Neural Text-to-Speech",
    "abstract": "Comments: 9 pages, 5 figures, 4 tables",
    "descriptor": "\nComments: 9 pages, 5 figures, 4 tables\n",
    "authors": [
      "Yeunju Choi",
      "Youngmoon Jung",
      "Youngjoo Suh",
      "Hoirin Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2011.01174"
  },
  {
    "id": "arXiv:2011.12193",
    "title": "xFraud: Explainable Fraud Transaction Detection",
    "abstract": "Comments: This is the extended version of a full paper to appear in PVLDB 15 (3) (VLDB 2022)",
    "descriptor": "\nComments: This is the extended version of a full paper to appear in PVLDB 15 (3) (VLDB 2022)\n",
    "authors": [
      "Susie Xi Rao",
      "Shuai Zhang",
      "Zhichao Han",
      "Zitao Zhang",
      "Wei Min",
      "Zhiyao Chen",
      "Yinan Shan",
      "Yang Zhao",
      "Ce Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2011.12193"
  },
  {
    "id": "arXiv:2011.12954",
    "title": "RELLIS-3D Dataset: Data, Benchmarks and Analysis",
    "abstract": "Comments: 7 pages, 7 figures",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "Peng Jiang",
      "Philip Osteen",
      "Maggie Wigness",
      "Srikanth Saripalli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2011.12954"
  },
  {
    "id": "arXiv:2101.03956",
    "title": "A wavelet-in-time, finite element-in-space adaptive method for parabolic  evolution equations",
    "abstract": "A wavelet-in-time, finite element-in-space adaptive method for parabolic  evolution equations",
    "descriptor": "",
    "authors": [
      "Rob Stevenson",
      "Raymond van Veneti\u00eb",
      "Jan Westerdiep"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.03956"
  },
  {
    "id": "arXiv:2102.08504",
    "title": "Label Leakage and Protection in Two-party Split Learning",
    "abstract": "Comments: Accepted to ICLR 2022 (this https URL)",
    "descriptor": "\nComments: Accepted to ICLR 2022 (this https URL)\n",
    "authors": [
      "Oscar Li",
      "Jiankai Sun",
      "Xin Yang",
      "Weihao Gao",
      "Hongyi Zhang",
      "Junyuan Xie",
      "Virginia Smith",
      "Chong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2102.08504"
  },
  {
    "id": "arXiv:2102.10359",
    "title": "Regression Filtration with Resetting to Provide Exponential Convergence  of MRAC for Plants with Jump Change of Unknown Parameters",
    "abstract": "Comments: 8 pages, 4 figures",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Anton Glushchenko",
      "Vladislav Petrov",
      "Konstantin Lastochkin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.10359"
  },
  {
    "id": "arXiv:2103.01932",
    "title": "Meta-Learning-Based Robust Adaptive Flight Control Under Uncertain Wind  Conditions",
    "abstract": "Comments: 7 pages, 7 figures; this article is an early draft and presents preliminary results; the full method and improved results were published in Science Robotics on May 4th, 2022: doi.org/10.1126/scirobotics.abm6597; arXiv: doi.org/10.48550/arXiv.2205.06908",
    "descriptor": "\nComments: 7 pages, 7 figures; this article is an early draft and presents preliminary results; the full method and improved results were published in Science Robotics on May 4th, 2022: doi.org/10.1126/scirobotics.abm6597; arXiv: doi.org/10.48550/arXiv.2205.06908\n",
    "authors": [
      "Michael O'Connell",
      "Guanya Shi",
      "Xichen Shi",
      "Soon-Jo Chung"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.01932"
  },
  {
    "id": "arXiv:2104.05070",
    "title": "Proof of Travel for Trust-Based Data Validation in V2I Communication",
    "abstract": "Proof of Travel for Trust-Based Data Validation in V2I Communication",
    "descriptor": "",
    "authors": [
      "Dajiang Suo",
      "Baichuan Mo",
      "Jinhua Zhao",
      "Sanjay E. Sarma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.05070"
  },
  {
    "id": "arXiv:2104.06873",
    "title": "Linear-Time and Deterministic Algorithms for Cardinality-Constrained  Non-Monotone Submodular Maximization",
    "abstract": "Linear-Time and Deterministic Algorithms for Cardinality-Constrained  Non-Monotone Submodular Maximization",
    "descriptor": "",
    "authors": [
      "Alan Kuhnle"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2104.06873"
  },
  {
    "id": "arXiv:2104.13138",
    "title": "Finding Good Proofs for Description Logic Entailments Using Recursive  Quality Measures (Extended Technical Report)",
    "abstract": "Comments: Extended version of a paper accepted at CADE-28",
    "descriptor": "\nComments: Extended version of a paper accepted at CADE-28\n",
    "authors": [
      "Christian Alrabbaa",
      "Franz Baader",
      "Stefan Borgwardt",
      "Patrick Koopmann",
      "Alisa Kovtunova"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.13138"
  },
  {
    "id": "arXiv:2104.13790",
    "title": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive  Optimizers by Exploiting Strong Convexity",
    "abstract": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive  Optimizers by Exploiting Strong Convexity",
    "descriptor": "",
    "authors": [
      "Yangfan Zhou",
      "Kaizhu Huang",
      "Cheng Cheng",
      "Xuguang Wang",
      "Amir Hussain",
      "Xin Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.13790"
  },
  {
    "id": "arXiv:2105.08840",
    "title": "Training Heterogeneous Features in Sequence to Sequence Tasks: Latent  Enhanced Multi-filter Seq2Seq Model",
    "abstract": "Comments: Accepted to Intelligent Systems Conference 2022",
    "descriptor": "\nComments: Accepted to Intelligent Systems Conference 2022\n",
    "authors": [
      "Yunhao Yang",
      "Zhaokun Xue"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.08840"
  },
  {
    "id": "arXiv:2106.01524",
    "title": "Combinatorial Conditions for Directed Collapsing",
    "abstract": "Comments: 23 pages, 11 figures",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Robin Belton",
      "Robyn Brooks",
      "Stefania Ebli",
      "Lisbeth Fajstrup",
      "Brittany Terese Fasy",
      "Nicole Sanderson",
      "Elizabeth Vidaurre"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2106.01524"
  },
  {
    "id": "arXiv:2106.07548",
    "title": "A scalable multi-step least squares method for network identification  with unknown disturbance topology",
    "abstract": "Comments: 17 pages, 4 figures, accepted and published in Automatica Volume 141, July 2022",
    "descriptor": "\nComments: 17 pages, 4 figures, accepted and published in Automatica Volume 141, July 2022\n",
    "authors": [
      "Stefanie J.M. Fonken",
      "Karthik R. Ramaswamy",
      "Paul M.J. Van den Hof"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07548"
  },
  {
    "id": "arXiv:2106.10663",
    "title": "A system of of Hamilton-Jacobi equations characterizing geodesic  centroidal tessellations",
    "abstract": "A system of of Hamilton-Jacobi equations characterizing geodesic  centroidal tessellations",
    "descriptor": "",
    "authors": [
      "Fabio Camilli",
      "Adriano Festa"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.10663"
  },
  {
    "id": "arXiv:2106.10823",
    "title": "3D Object Detection for Autonomous Driving: A Survey",
    "abstract": "Comments: The manuscript is accepted by Pattern Recognition on 14 May 2022",
    "descriptor": "\nComments: The manuscript is accepted by Pattern Recognition on 14 May 2022\n",
    "authors": [
      "Rui Qian",
      "Xin Lai",
      "Xirong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.10823"
  },
  {
    "id": "arXiv:2106.13940",
    "title": "Adaptive Smooth Disturbance Observer-Based Fast Finite-Time Attitude  Tracking Control of a Small Unmanned Helicopter",
    "abstract": "Comments: 26 pages,9 figures",
    "descriptor": "\nComments: 26 pages,9 figures\n",
    "authors": [
      "Xidong Wang",
      "Zhan Li",
      "Xinghu Yu",
      "Zhen He"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.13940"
  },
  {
    "id": "arXiv:2106.13948",
    "title": "Core Challenges in Embodied Vision-Language Planning",
    "abstract": "Comments: Journal of Artificial Intelligence Research 74 (2022) 459-515",
    "descriptor": "\nComments: Journal of Artificial Intelligence Research 74 (2022) 459-515\n",
    "authors": [
      "Jonathan Francis",
      "Nariaki Kitamura",
      "Felix Labelle",
      "Xiaopeng Lu",
      "Ingrid Navarro",
      "Jean Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.13948"
  },
  {
    "id": "arXiv:2107.00946",
    "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information  Aggregation",
    "abstract": "Comments: This paper has been accepted to TPAMI",
    "descriptor": "\nComments: This paper has been accepted to TPAMI\n",
    "authors": [
      "Lingbo Liu",
      "Yuying Zhu",
      "Guanbin Li",
      "Ziyi Wu",
      "Lei Bai",
      "Liang Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.00946"
  },
  {
    "id": "arXiv:2107.11839",
    "title": "Differential Privacy in the Shuffle Model: A Survey of Separations",
    "abstract": "Comments: 24 May '22 version includes more recent work on sums, histograms, and an appendix comparing the shuffle model with other distributed models",
    "descriptor": "\nComments: 24 May '22 version includes more recent work on sums, histograms, and an appendix comparing the shuffle model with other distributed models\n",
    "authors": [
      "Albert Cheu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2107.11839"
  },
  {
    "id": "arXiv:2108.03375",
    "title": "Temporal Action Localization Using Gated Recurrent Units",
    "abstract": "Comments: 10 pages, 6 figures",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Hassan Keshvarikhojasteh",
      "Hoda Mohammadzade",
      "Hamid Behroozi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2108.03375"
  },
  {
    "id": "arXiv:2108.03499",
    "title": "Learning Foveated Reconstruction to Preserve Perceived Image Statistics",
    "abstract": "Learning Foveated Reconstruction to Preserve Perceived Image Statistics",
    "descriptor": "",
    "authors": [
      "Luca Surace",
      "Marek Wernikowski",
      "Cara Tursun",
      "Karol Myszkowski",
      "Rados\u0142aw Mantiuk",
      "Piotr Didyk"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.03499"
  },
  {
    "id": "arXiv:2108.06076",
    "title": "PVT: Point-Voxel Transformer for Point Cloud Learning",
    "abstract": "Comments: 29 pages",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Cheng Zhang",
      "Haocheng Wan",
      "Xinyi Shen",
      "Zizhao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2108.06076"
  },
  {
    "id": "arXiv:2108.12947",
    "title": "Learning JPEG Compression Artifacts for Image Manipulation Detection and  Localization",
    "abstract": "Comments: The version of record of this article, published in the International Journal of Computer Vision (IJCV), is available online at Publisher's website: this https URL ; Code is available at: this https URL",
    "descriptor": "\nComments: The version of record of this article, published in the International Journal of Computer Vision (IJCV), is available online at Publisher's website: this https URL ; Code is available at: this https URL\n",
    "authors": [
      "Myung-Joon Kwon",
      "Seung-Hun Nam",
      "In-Jae Yu",
      "Heung-Kyu Lee",
      "Changick Kim"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2108.12947"
  },
  {
    "id": "arXiv:2109.00974",
    "title": "Root-max Problems, Hybrid Expansion-Contraction, and Quadratically  Convergent Optimization of Passive Systems",
    "abstract": "Comments: Revision #1",
    "descriptor": "\nComments: Revision #1\n",
    "authors": [
      "Tim Mitchell",
      "Paul Van Dooren"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.00974"
  },
  {
    "id": "arXiv:2109.01226",
    "title": "So Cloze yet so Far: N400 Amplitude is Better Predicted by  Distributional Information than Human Predictability Judgements",
    "abstract": "Comments: Accepted",
    "descriptor": "\nComments: Accepted\n",
    "authors": [
      "James A. Michaelov",
      "Seana Coulson",
      "Benjamin K. Bergen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.01226"
  },
  {
    "id": "arXiv:2109.03596",
    "title": "Learn2Agree: Fitting with Multiple Annotators without Objective Ground  Truth",
    "abstract": "Comments: Codes are kept for our ongoing development, but should be easily re-produced :)",
    "descriptor": "\nComments: Codes are kept for our ongoing development, but should be easily re-produced :)\n",
    "authors": [
      "Chongyang Wang",
      "Yuan Gao",
      "Chenyou Fan",
      "Junjie Hu",
      "Tin Lun Lam",
      "Nicholas D. Lane",
      "Nadia Bianchi-Berthouze"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2109.03596"
  },
  {
    "id": "arXiv:2109.06404",
    "title": "Detecting Multi-Sensor Fusion Errors in Advanced Driver-Assistance  Systems",
    "abstract": "Detecting Multi-Sensor Fusion Errors in Advanced Driver-Assistance  Systems",
    "descriptor": "",
    "authors": [
      "Ziyuan Zhong",
      "Zhisheng Hu",
      "Shengjian Guo",
      "Xinyang Zhang",
      "Zhenyu Zhong",
      "Baishakhi Ray"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2109.06404"
  },
  {
    "id": "arXiv:2109.06604",
    "title": "Non-Parametric Unsupervised Domain Adaptation for Neural Machine  Translation",
    "abstract": "Comments: Findings of EMNLP 2021",
    "descriptor": "\nComments: Findings of EMNLP 2021\n",
    "authors": [
      "Xin Zheng",
      "Zhirui Zhang",
      "Shujian Huang",
      "Boxing Chen",
      "Jun Xie",
      "Weihua Luo",
      "Jiajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.06604"
  },
  {
    "id": "arXiv:2109.10610",
    "title": "When can forward stable algorithms be composed stably?",
    "abstract": "When can forward stable algorithms be composed stably?",
    "descriptor": "",
    "authors": [
      "Carlos Beltr\u00e1n",
      "Vanni Noferini",
      "Nick Vannieuwenhoven"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.10610"
  },
  {
    "id": "arXiv:2109.13081",
    "title": "Semi-Autonomous Teleoperation via Learning Non-Prehensile Manipulation  Skills",
    "abstract": "Semi-Autonomous Teleoperation via Learning Non-Prehensile Manipulation  Skills",
    "descriptor": "",
    "authors": [
      "Sangbeom Park",
      "Yoonbyung Chai",
      "Sunghyun Park",
      "Jeongeun Park",
      "Kyungjae Lee",
      "Sungjoon Choi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.13081"
  },
  {
    "id": "arXiv:2110.01174",
    "title": "Deep Kernel Representation for Image Reconstruction in PET",
    "abstract": "Deep Kernel Representation for Image Reconstruction in PET",
    "descriptor": "",
    "authors": [
      "Siqi Li",
      "Guobao Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.01174"
  },
  {
    "id": "arXiv:2110.05063",
    "title": "Efficient Extensional Binary Tries",
    "abstract": "Efficient Extensional Binary Tries",
    "descriptor": "",
    "authors": [
      "Andrew Appel",
      "Xavier Leroy"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.05063"
  },
  {
    "id": "arXiv:2110.07736",
    "title": "Identifying and Mitigating Spurious Correlations for Improving  Robustness in NLP Models",
    "abstract": "Comments: 8 pages, accepted to NAACL 2022 Findings",
    "descriptor": "\nComments: 8 pages, accepted to NAACL 2022 Findings\n",
    "authors": [
      "Tianlu Wang",
      "Rohit Sridhar",
      "Diyi Yang",
      "Xuezhi Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07736"
  },
  {
    "id": "arXiv:2110.08607",
    "title": "Physics-guided Deep Markov Models for Learning Nonlinear Dynamical  Systems with Uncertainty",
    "abstract": "Comments: Accepted for publication in Mechanical Systems and Signal Processing",
    "descriptor": "\nComments: Accepted for publication in Mechanical Systems and Signal Processing\n",
    "authors": [
      "Wei Liu",
      "Zhilu Lai",
      "Kiran Bacsa",
      "Eleni Chatzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Chaotic Dynamics (nlin.CD)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.08607"
  },
  {
    "id": "arXiv:2110.12184",
    "title": "Domain Adaptation via Maximizing Surrogate Mutual Information",
    "abstract": "Domain Adaptation via Maximizing Surrogate Mutual Information",
    "descriptor": "",
    "authors": [
      "Haiteng Zhao",
      "Chang Ma",
      "Qinyu Chen",
      "Zhi-Hong Deng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.12184"
  },
  {
    "id": "arXiv:2110.12805",
    "title": "Algorithms for the Communication of Samples",
    "abstract": "Comments: Proceedings of the 39th International Conference on Machine Learning, 2022",
    "descriptor": "\nComments: Proceedings of the 39th International Conference on Machine Learning, 2022\n",
    "authors": [
      "Lucas Theis",
      "Noureldin Yosri"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.12805"
  },
  {
    "id": "arXiv:2111.03690",
    "title": "Do we still need ImageNet pre-training in remote sensing scene  classification?",
    "abstract": "Do we still need ImageNet pre-training in remote sensing scene  classification?",
    "descriptor": "",
    "authors": [
      "Vladimir Risojevi\u0107",
      "Vladan Stojni\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.03690"
  },
  {
    "id": "arXiv:2111.04964",
    "title": "On Representation Knowledge Distillation for Graph Neural Networks",
    "abstract": "On Representation Knowledge Distillation for Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Chaitanya K. Joshi",
      "Fayao Liu",
      "Xu Xun",
      "Jie Lin",
      "Chuan-Sheng Foo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.04964"
  },
  {
    "id": "arXiv:2111.07006",
    "title": "Optimization Framework for Splitting DNN Inference Jobs over Computing  Networks",
    "abstract": "Comments: Submitted for publication",
    "descriptor": "\nComments: Submitted for publication\n",
    "authors": [
      "Sehun Jung",
      "Hyang-Won Lee"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2111.07006"
  },
  {
    "id": "arXiv:2111.07671",
    "title": "NeuralPDE: Modelling Dynamical Systems from Data",
    "abstract": "NeuralPDE: Modelling Dynamical Systems from Data",
    "descriptor": "",
    "authors": [
      "Andrzej Dulny",
      "Andreas Hotho",
      "Anna Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.07671"
  },
  {
    "id": "arXiv:2111.08617",
    "title": "CGX: Adaptive System Support for Communication-Efficient Deep Learning",
    "abstract": "CGX: Adaptive System Support for Communication-Efficient Deep Learning",
    "descriptor": "",
    "authors": [
      "Ilia Markov",
      "Hamidreza Ramezanikebrya",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.08617"
  },
  {
    "id": "arXiv:2111.11165",
    "title": "Graph-Based Similarity of Neural Network Representations",
    "abstract": "Graph-Based Similarity of Neural Network Representations",
    "descriptor": "",
    "authors": [
      "Zuohui Chen",
      "Yao Lu",
      "Jinxuan Hu",
      "Wen Yang",
      "Qi Xuan",
      "Zhen Wang",
      "Xiaoniu Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.11165"
  },
  {
    "id": "arXiv:2111.15106",
    "title": "MAPLE: Microprocessor A Priori for Latency Estimation",
    "abstract": "Comments: 13 pages, 4 figures",
    "descriptor": "\nComments: 13 pages, 4 figures\n",
    "authors": [
      "Saad Abbasi",
      "Alexander Wong",
      "Mohammad Javad Shafiee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.15106"
  },
  {
    "id": "arXiv:2111.15119",
    "title": "Aerial Images Meet Crowdsourced Trajectories: A New Approach to Robust  Road Extraction",
    "abstract": "Comments: This work has been accepted by IEEE Transactions on Neural Networks and Learning Systems",
    "descriptor": "\nComments: This work has been accepted by IEEE Transactions on Neural Networks and Learning Systems\n",
    "authors": [
      "Lingbo Liu",
      "Zewei Yang",
      "Guanbin Li",
      "Kuo Wang",
      "Tianshui Chen",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.15119"
  },
  {
    "id": "arXiv:2111.15546",
    "title": "Black box tests for algorithmic stability",
    "abstract": "Comments: 26 pages. Updates to Section 2.1.1 and Sections B.1 & B.2",
    "descriptor": "\nComments: 26 pages. Updates to Section 2.1.1 and Sections B.1 & B.2\n",
    "authors": [
      "Byol Kim",
      "Rina Foygel Barber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2111.15546"
  },
  {
    "id": "arXiv:2112.00229",
    "title": "Frequency Fitness Assignment: Optimization without Bias for Good  Solutions can be Efficient",
    "abstract": "Frequency Fitness Assignment: Optimization without Bias for Good  Solutions can be Efficient",
    "descriptor": "",
    "authors": [
      "Thomas Weise",
      "Zhize Wu",
      "Xinlu Li",
      "Yan Chen",
      "J\u00f6rg L\u00e4ssig"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.00229"
  },
  {
    "id": "arXiv:2112.00954",
    "title": "Temporally Resolution Decrement: Utilizing the Shape Consistency for  Higher Computational Efficiency",
    "abstract": "Temporally Resolution Decrement: Utilizing the Shape Consistency for  Higher Computational Efficiency",
    "descriptor": "",
    "authors": [
      "Tianshu Xie",
      "Xuan Cheng",
      "Minghui Liu",
      "Jiali Deng",
      "Xiaomin Wang",
      "Ming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00954"
  },
  {
    "id": "arXiv:2112.01646",
    "title": "Investigating the usefulness of Quantum Blur",
    "abstract": "Investigating the usefulness of Quantum Blur",
    "descriptor": "",
    "authors": [
      "James R. Wootton",
      "Marcel Pfaffhauser"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2112.01646"
  },
  {
    "id": "arXiv:2112.03753",
    "title": "Tell me why! Explanations support learning relational and causal  structure",
    "abstract": "Comments: ICML 2022; 23 pages",
    "descriptor": "\nComments: ICML 2022; 23 pages\n",
    "authors": [
      "Andrew K. Lampinen",
      "Nicholas A. Roy",
      "Ishita Dasgupta",
      "Stephanie C. Y. Chan",
      "Allison C. Tam",
      "James L. McClelland",
      "Chen Yan",
      "Adam Santoro",
      "Neil C. Rabinowitz",
      "Jane X. Wang",
      "Felix Hill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.03753"
  },
  {
    "id": "arXiv:2112.05593",
    "title": "A Review of Indoor Millimeter Wave Device-based Localization and  Device-free Sensing Technologies and Applications",
    "abstract": "Comments: 43 pages, 13 figures. Accepted in IEEE Communications Surveys & Tutorials (IEEE COMST)",
    "descriptor": "\nComments: 43 pages, 13 figures. Accepted in IEEE Communications Surveys & Tutorials (IEEE COMST)\n",
    "authors": [
      "Anish Shastri",
      "Neharika Valecha",
      "Enver Bashirov",
      "Harsh Tataria",
      "Michael Lentmaier",
      "Fredrik Tufvesson",
      "Michele Rossi",
      "Paolo Casari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.05593"
  },
  {
    "id": "arXiv:2112.08609",
    "title": "DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions  for Evaluating the Robustness of Question Matching Models",
    "abstract": "DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions  for Evaluating the Robustness of Question Matching Models",
    "descriptor": "",
    "authors": [
      "Hongyu Zhu",
      "Yan Chen",
      "Jing Yan",
      "Jing Liu",
      "Yu Hong",
      "Ying Chen",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08609"
  },
  {
    "id": "arXiv:2112.08772",
    "title": "Sharpness-Aware Minimization with Dynamic Reweighting",
    "abstract": "Sharpness-Aware Minimization with Dynamic Reweighting",
    "descriptor": "",
    "authors": [
      "Wenxuan Zhou",
      "Fangyu Liu",
      "Huan Zhang",
      "Muhao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08772"
  },
  {
    "id": "arXiv:2112.09237",
    "title": "Examining Single Sentence Label Leakage in Natural Language Inference  Datasets",
    "abstract": "Comments: 12 pages, 7 figures, 4 tables",
    "descriptor": "\nComments: 12 pages, 7 figures, 4 tables\n",
    "authors": [
      "Michael Saxon",
      "Xinyi Wang",
      "Wenda Xu",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.09237"
  },
  {
    "id": "arXiv:2112.09924",
    "title": "The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large  Web Corpus",
    "abstract": "The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large  Web Corpus",
    "descriptor": "",
    "authors": [
      "Aleksandra Piktus",
      "Fabio Petroni",
      "Vladimir Karpukhin",
      "Dmytro Okhonko",
      "Samuel Broscheit",
      "Gautier Izacard",
      "Patrick Lewis",
      "Barlas O\u011fuz",
      "Edouard Grave",
      "Wen-tau Yih",
      "Sebastian Riedel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09924"
  },
  {
    "id": "arXiv:2112.10807",
    "title": "Surprise-Guided Search for Learning Task Specifications from  Demonstrations",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Marcell Vazquez-Chanlatte",
      "Ameesh Shah",
      "Gil Lederman",
      "Sanjit A. Seshia"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.10807"
  },
  {
    "id": "arXiv:2112.10991",
    "title": "Regularizing End-to-End Speech Translation with Triangular Decomposition  Agreement",
    "abstract": "Comments: AAAI 2022",
    "descriptor": "\nComments: AAAI 2022\n",
    "authors": [
      "Yichao Du",
      "Zhirui Zhang",
      "Weizhi Wang",
      "Boxing Chen",
      "Jun Xie",
      "Tong Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2112.10991"
  },
  {
    "id": "arXiv:2112.13161",
    "title": "BEAT: Blockchain-Enabled Accountable and Transparent Infrastructure  Sharing in 6G and Beyond",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2107.04328",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2107.04328\n",
    "authors": [
      "Tooba Faisal",
      "Mischa Dohler",
      "Simone Mangiante",
      "Diego R. Lopez"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2112.13161"
  },
  {
    "id": "arXiv:2112.13798",
    "title": "PORTFILER: Port-Level Network Profiling for Self-Propagating Malware  Detection",
    "abstract": "Comments: An earlier version is accepted to be published in IEEE Conference on Communications and Network Security (CNS) 2021",
    "descriptor": "\nComments: An earlier version is accepted to be published in IEEE Conference on Communications and Network Security (CNS) 2021\n",
    "authors": [
      "Talha Ongun",
      "Oliver Spohngellert",
      "Benjamin Miller",
      "Simona Boboila",
      "Alina Oprea",
      "Tina Eliassi-Rad",
      "Jason Hiser",
      "Alastair Nottingham",
      "Jack Davidson",
      "Malathi Veeraraghavan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.13798"
  },
  {
    "id": "arXiv:2201.01221",
    "title": "A Deeper Understanding of State-Based Critics in Multi-Agent  Reinforcement Learning",
    "abstract": "A Deeper Understanding of State-Based Critics in Multi-Agent  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Xueguang Lyu",
      "Andrea Baisero",
      "Yuchen Xiao",
      "Christopher Amato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.01221"
  },
  {
    "id": "arXiv:2201.01549",
    "title": "SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code  Representations",
    "abstract": "Comments: ICSE 2022: Technical Track",
    "descriptor": "\nComments: ICSE 2022: Technical Track\n",
    "authors": [
      "Changan Niu",
      "Chuanyi Li",
      "Vincent Ng",
      "Jidong Ge",
      "Liguo Huang",
      "Bin Luo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.01549"
  },
  {
    "id": "arXiv:2201.04472",
    "title": "Numerical and Experimental Characterization of LoRa-based Helmet-to-UAV  links on Flat Lands",
    "abstract": "Numerical and Experimental Characterization of LoRa-based Helmet-to-UAV  links on Flat Lands",
    "descriptor": "",
    "authors": [
      "Giulio Maria Bianco",
      "Abraham Mejia-Aguilar",
      "Gaetano Marrocco"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.04472"
  },
  {
    "id": "arXiv:2201.07070",
    "title": "Attention-based Proposals Refinement for 3D Object Detection",
    "abstract": "Comments: Accepted for IV 2022",
    "descriptor": "\nComments: Accepted for IV 2022\n",
    "authors": [
      "Minh-Quan Dao",
      "Elwan H\u00e9ry",
      "Vincent Fr\u00e9mont"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.07070"
  },
  {
    "id": "arXiv:2201.07546",
    "title": "Welfare vs. Representation in Participatory Budgeting",
    "abstract": "Welfare vs. Representation in Participatory Budgeting",
    "descriptor": "",
    "authors": [
      "Roy Fairstein",
      "Reshef Meir",
      "Dan Vilenchik",
      "Kobi Gal"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.07546"
  },
  {
    "id": "arXiv:2201.11211",
    "title": "Learning Mixtures of Linear Dynamical Systems",
    "abstract": "Comments: Accepted to ICML 2022. arXiv v2 update: add references and experiments",
    "descriptor": "\nComments: Accepted to ICML 2022. arXiv v2 update: add references and experiments\n",
    "authors": [
      "Yanxi Chen",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2201.11211"
  },
  {
    "id": "arXiv:2202.01085",
    "title": "Giga-scale Kernel Matrix Vector Multiplication on GPU",
    "abstract": "Giga-scale Kernel Matrix Vector Multiplication on GPU",
    "descriptor": "",
    "authors": [
      "Robert Hu",
      "Siu Lun Chau",
      "Dino Sejdinovic",
      "Joan Alexis Glaun\u00e8s"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Mathematical Software (cs.MS)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2202.01085"
  },
  {
    "id": "arXiv:2202.02096",
    "title": "To Impute or not to Impute? Missing Data in Treatment Effect Estimation",
    "abstract": "To Impute or not to Impute? Missing Data in Treatment Effect Estimation",
    "descriptor": "",
    "authors": [
      "Jeroen Berrevoets",
      "Fergus Imrie",
      "Trent Kyono",
      "James Jordon",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02096"
  },
  {
    "id": "arXiv:2202.03012",
    "title": "EDCHO: High Order Exact Dynamic Consensus",
    "abstract": "Comments: This is the preprint version of the accepted Manuscript: Rodrigo Aldana-Lopez, Rosario Aragues, Carlos Sagues, EDCHO: High order exact dynamic consensus, Automatica, Volume 131, 2021, ISSN 0005-1098. Please cite the publisher's version",
    "descriptor": "\nComments: This is the preprint version of the accepted Manuscript: Rodrigo Aldana-Lopez, Rosario Aragues, Carlos Sagues, EDCHO: High order exact dynamic consensus, Automatica, Volume 131, 2021, ISSN 0005-1098. Please cite the publisher's version\n",
    "authors": [
      "Rodrigo Aldana-L\u00f3pez",
      "Rosario Arag\u00fc\u00e9s",
      "Carlos Sag\u00fc\u00e9s"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.03012"
  },
  {
    "id": "arXiv:2202.03486",
    "title": "Optimizing Warfarin Dosing using Deep Reinforcement Learning",
    "abstract": "Comments: submitted to Journal of Biomedical Informatics",
    "descriptor": "\nComments: submitted to Journal of Biomedical Informatics\n",
    "authors": [
      "Sadjad Anzabi Zadeh",
      "W. Nick Street",
      "Barrett W. Thomas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03486"
  },
  {
    "id": "arXiv:2202.04178",
    "title": "VAEL: Bridging Variational Autoencoders and Probabilistic Logic  Programming",
    "abstract": "VAEL: Bridging Variational Autoencoders and Probabilistic Logic  Programming",
    "descriptor": "",
    "authors": [
      "Eleonora Misino",
      "Giuseppe Marra",
      "Emanuele Sansone"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04178"
  },
  {
    "id": "arXiv:2202.04579",
    "title": "Neural Sheaf Diffusion: A Topological Perspective on Heterophily and  Oversmoothing in GNNs",
    "abstract": "Comments: 27 pages, 9 figures",
    "descriptor": "\nComments: 27 pages, 9 figures\n",
    "authors": [
      "Cristian Bodnar",
      "Francesco Di Giovanni",
      "Benjamin Paul Chamberlain",
      "Pietro Li\u00f2",
      "Michael M. Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2202.04579"
  },
  {
    "id": "arXiv:2202.05100",
    "title": "Adaptively Exploiting d-Separators with Causal Bandits",
    "abstract": "Comments: 33 pages, 3 figures",
    "descriptor": "\nComments: 33 pages, 3 figures\n",
    "authors": [
      "Blair Bilodeau",
      "Linbo Wang",
      "Daniel M. Roy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05100"
  },
  {
    "id": "arXiv:2202.05713",
    "title": "Cross Domain Few-Shot Learning via Meta Adversarial Training",
    "abstract": "Comments: 6 pages including references",
    "descriptor": "\nComments: 6 pages including references\n",
    "authors": [
      "Jirui Qi",
      "Richong Zhang",
      "Chune Li",
      "Yongyi Mao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.05713"
  },
  {
    "id": "arXiv:2202.08005",
    "title": "Should You Mask 15% in Masked Language Modeling?",
    "abstract": "Comments: The code and pre-trained models are available at this https URL",
    "descriptor": "\nComments: The code and pre-trained models are available at this https URL\n",
    "authors": [
      "Alexander Wettig",
      "Tianyu Gao",
      "Zexuan Zhong",
      "Danqi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08005"
  },
  {
    "id": "arXiv:2202.08125",
    "title": "Processing the structure of documents: Logical Layout Analysis of  historical newspapers in French",
    "abstract": "Comments: 25 pages, 5 figures, 16 tables. This paper is to be submited to the NLP4DH edition of the Journal of Data Mining and Digital Humanities",
    "descriptor": "\nComments: 25 pages, 5 figures, 16 tables. This paper is to be submited to the NLP4DH edition of the Journal of Data Mining and Digital Humanities\n",
    "authors": [
      "Nicolas Gutehrl\u00e9",
      "Iana Atanassova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.08125"
  },
  {
    "id": "arXiv:2203.01137",
    "title": "Self-Supervised Scene Flow Estimation with 4D Automotive Radar",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Fangqiang Ding",
      "Zhijun Pan",
      "Yimin Deng",
      "Jianning Deng",
      "Chris Xiaoxuan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.01137"
  },
  {
    "id": "arXiv:2203.01451",
    "title": "Label Leakage and Protection from Forward Embedding in Vertical  Federated Learning",
    "abstract": "Label Leakage and Protection from Forward Embedding in Vertical  Federated Learning",
    "descriptor": "",
    "authors": [
      "Jiankai Sun",
      "Xin Yang",
      "Yuanshun Yao",
      "Chong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2203.01451"
  },
  {
    "id": "arXiv:2203.05092",
    "title": "A Tree-Structured Multi-Task Model Recommender",
    "abstract": "Comments: 12 pages, 2 figures; Accepted by AutoML-Conf 2022",
    "descriptor": "\nComments: 12 pages, 2 figures; Accepted by AutoML-Conf 2022\n",
    "authors": [
      "Lijun Zhang",
      "Xiao Liu",
      "Hui Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05092"
  },
  {
    "id": "arXiv:2203.05221",
    "title": "Realizing Implicit Computational Complexity",
    "abstract": "Realizing Implicit Computational Complexity",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Aubert",
      "Thomas Rubiano",
      "Neea Rusch",
      "Thomas Seiller"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2203.05221"
  },
  {
    "id": "arXiv:2203.05757",
    "title": "A comparative study of non-deep learning, deep learning, and ensemble  learning methods for sunspot number prediction",
    "abstract": "A comparative study of non-deep learning, deep learning, and ensemble  learning methods for sunspot number prediction",
    "descriptor": "",
    "authors": [
      "Yuchen Dang",
      "Ziqi Chen",
      "Heng Li",
      "Hai Shu"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.05757"
  },
  {
    "id": "arXiv:2203.06634",
    "title": "Adaptive Bit Rate Control in Semantic Communication with Incremental  Knowledge-based HARQ",
    "abstract": "Adaptive Bit Rate Control in Semantic Communication with Incremental  Knowledge-based HARQ",
    "descriptor": "",
    "authors": [
      "Qingyang Zhou",
      "Rongpeng Li",
      "Zhifeng Zhao",
      "Yong Xiao",
      "Honggang Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.06634"
  },
  {
    "id": "arXiv:2203.07648",
    "title": "Contrastive Learning of Sociopragmatic Meaning in Social Media",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Chiyu Zhang",
      "Muhammad Abdul-Mageed",
      "Ganesh Jawahar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.07648"
  },
  {
    "id": "arXiv:2203.08299",
    "title": "FastKASSIM: A Fast Tree Kernel-Based Syntactic Similarity Metric",
    "abstract": "Comments: 13 pages, 8 figures. code available at this https URL",
    "descriptor": "\nComments: 13 pages, 8 figures. code available at this https URL\n",
    "authors": [
      "Maximillian Chen",
      "Caitlyn Chen",
      "Xiao Yu",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08299"
  },
  {
    "id": "arXiv:2203.08304",
    "title": "Hyperdecoders: Instance-specific decoders for multi-task NLP",
    "abstract": "Comments: Updated experiments and appendix",
    "descriptor": "\nComments: Updated experiments and appendix\n",
    "authors": [
      "Hamish Ivison",
      "Matthew E. Peters"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08304"
  },
  {
    "id": "arXiv:2203.09040",
    "title": "A Survey of Multi-Tenant Deep Learning Inference on GPU",
    "abstract": "Comments: Accepted in MLSys'22 Workshop on Cloud Intelligence / AIOps",
    "descriptor": "\nComments: Accepted in MLSys'22 Workshop on Cloud Intelligence / AIOps\n",
    "authors": [
      "Fuxun Yu",
      "Di Wang",
      "Longfei Shangguan",
      "Minjia Zhang",
      "Chenchen Liu",
      "Xiang Chen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2203.09040"
  },
  {
    "id": "arXiv:2203.10093",
    "title": "Deep Reinforcement Learning Guided Graph Neural Networks for Brain  Network Analysis",
    "abstract": "Deep Reinforcement Learning Guided Graph Neural Networks for Brain  Network Analysis",
    "descriptor": "",
    "authors": [
      "Xusheng Zhao",
      "Jia Wu",
      "Hao Peng",
      "Amin Beheshti",
      "Jessica Monaghan",
      "David McAlpine",
      "Heivet Hernandez-Perez",
      "Mark Dras",
      "Qiong Dai",
      "Yangyang Li",
      "Philip S. Yu",
      "Lifang He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2203.10093"
  },
  {
    "id": "arXiv:2203.10232",
    "title": "DuReader_retrieval: A Large-scale Chinese Benchmark for Passage  Retrieval from Web Search Engine",
    "abstract": "DuReader_retrieval: A Large-scale Chinese Benchmark for Passage  Retrieval from Web Search Engine",
    "descriptor": "",
    "authors": [
      "Yifu Qiu",
      "Hongyu Li",
      "Yingqi Qu",
      "Ying Chen",
      "Qiaoqiao She",
      "Jing Liu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2203.10232"
  },
  {
    "id": "arXiv:2204.00122",
    "title": "Synthesis of Stabilizing Recurrent Equilibrium Network Controllers",
    "abstract": "Comments: Submitted to IEEE CDC 2022. arXiv admin note: text overlap with arXiv:2109.03861",
    "descriptor": "\nComments: Submitted to IEEE CDC 2022. arXiv admin note: text overlap with arXiv:2109.03861\n",
    "authors": [
      "Neelay Junnarkar",
      "He Yin",
      "Fangda Gu",
      "Murat Arcak",
      "Peter Seiler"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.00122"
  },
  {
    "id": "arXiv:2204.02491",
    "title": "Text2LIVE: Text-Driven Layered Image and Video Editing",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Omer Bar-Tal",
      "Dolev Ofri-Amar",
      "Rafail Fridman",
      "Yoni Kasten",
      "Tali Dekel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.02491"
  },
  {
    "id": "arXiv:2204.02821",
    "title": "drsphelps at SemEval-2022 Task 2: Learning idiom representations using  BERTRAM",
    "abstract": "drsphelps at SemEval-2022 Task 2: Learning idiom representations using  BERTRAM",
    "descriptor": "",
    "authors": [
      "Dylan Phelps"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.02821"
  },
  {
    "id": "arXiv:2204.02836",
    "title": "A note on the van der Waerden conjecture on random polynomials with  symmetric Galois group for function fields",
    "abstract": "A note on the van der Waerden conjecture on random polynomials with  symmetric Galois group for function fields",
    "descriptor": "",
    "authors": [
      "Erich L. Kaltofen"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2204.02836"
  },
  {
    "id": "arXiv:2204.02892",
    "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
    "abstract": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
    "descriptor": "",
    "authors": [
      "Noam Wies",
      "Yoav Levine",
      "Amnon Shashua"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.02892"
  },
  {
    "id": "arXiv:2204.03080",
    "title": "Graph Neural Networks Designed for Different Graph Types: A Survey",
    "abstract": "Graph Neural Networks Designed for Different Graph Types: A Survey",
    "descriptor": "",
    "authors": [
      "Josephine M. Thomas",
      "Alice Moallemy-Oureh",
      "Silvia Beddar-Wiesing",
      "Clara Holzh\u00fcter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.03080"
  },
  {
    "id": "arXiv:2204.06929",
    "title": "Sketch guided and progressive growing GAN for realistic and editable  ultrasound image synthesis",
    "abstract": "Comments: Accepted by Medical Image Analysis (13 figures, 4 tabels)",
    "descriptor": "\nComments: Accepted by Medical Image Analysis (13 figures, 4 tabels)\n",
    "authors": [
      "Jiamin Liang",
      "Xin Yang",
      "Yuhao Huang",
      "Haoming Li",
      "Shuangchi He",
      "Xindi Hu",
      "Zejian Chen",
      "Wufeng Xue",
      "Jun Cheng",
      "Dong Ni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.06929"
  },
  {
    "id": "arXiv:2204.08967",
    "title": "When Is Partially Observable Reinforcement Learning Not Scary?",
    "abstract": "When Is Partially Observable Reinforcement Learning Not Scary?",
    "descriptor": "",
    "authors": [
      "Qinghua Liu",
      "Alan Chung",
      "Csaba Szepesv\u00e1ri",
      "Chi Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.08967"
  },
  {
    "id": "arXiv:2204.09148",
    "title": "What Makes Instruction Learning Hard? An Investigation and a New  Challenge in a Synthetic Environment",
    "abstract": "Comments: Typos corrected, rewordings",
    "descriptor": "\nComments: Typos corrected, rewordings\n",
    "authors": [
      "Matthew Finlayson",
      "Kyle Richardson",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.09148"
  },
  {
    "id": "arXiv:2204.10629",
    "title": "MEKER: Memory Efficient Knowledge Embedding Representation for Link  Prediction and Question Answering",
    "abstract": "MEKER: Memory Efficient Knowledge Embedding Representation for Link  Prediction and Question Answering",
    "descriptor": "",
    "authors": [
      "Viktoriia Chekalina",
      "Anton Razzhigaev",
      "Albert Sayapin",
      "Evgeny Frolov",
      "Alexander Panchenko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.10629"
  },
  {
    "id": "arXiv:2204.12344",
    "title": "REDCHO: Robust Exact Dynamic Consensus of High Order",
    "abstract": "Comments: This is the preprint version of the accepted Manuscript: Rodrigo Aldana-Lopez, Rosario Aragues, Carlos Sagues, REDCHO: Robust Exact Dynamic Consensus of High Order, Automatica, Volume 141, 2022, ISSN 0005-1098",
    "descriptor": "\nComments: This is the preprint version of the accepted Manuscript: Rodrigo Aldana-Lopez, Rosario Aragues, Carlos Sagues, REDCHO: Robust Exact Dynamic Consensus of High Order, Automatica, Volume 141, 2022, ISSN 0005-1098\n",
    "authors": [
      "Rodrigo Aldana-L\u00f3pez",
      "Rosario Arag\u00fc\u00e9s",
      "Carlos Sag\u00fc\u00e9s"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2204.12344"
  },
  {
    "id": "arXiv:2204.12581",
    "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning",
    "abstract": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Marc Rigter",
      "Bruno Lacerda",
      "Nick Hawes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.12581"
  },
  {
    "id": "arXiv:2204.13324",
    "title": "Controllable Image Captioning",
    "abstract": "Comments: This submission has been withdrawn by arXiv administrators because the identity of the submitter and author could not be verified",
    "descriptor": "\nComments: This submission has been withdrawn by arXiv administrators because the identity of the submitter and author could not be verified\n",
    "authors": [
      "Luka Maxwell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.13324"
  },
  {
    "id": "arXiv:2204.13596",
    "title": "Generative Multi-hop Retrieval",
    "abstract": "Generative Multi-hop Retrieval",
    "descriptor": "",
    "authors": [
      "Hyunji Lee",
      "Sohee Yang",
      "Hanseok Oh",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.13596"
  },
  {
    "id": "arXiv:2204.13705",
    "title": "Coupling Deep Imputation with Multitask Learning for Downstream Tasks on  Genomics Data",
    "abstract": "Comments: Accepted as Oral presentation at The International Joint Conference on Neural Networks (IJCNN) 2022",
    "descriptor": "\nComments: Accepted as Oral presentation at The International Joint Conference on Neural Networks (IJCNN) 2022\n",
    "authors": [
      "Sophie Peacock",
      "Etai Jacob",
      "Nikolay Burlutskiy"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13705"
  },
  {
    "id": "arXiv:2204.13791",
    "title": "Depth Estimation with Simplified Transformer",
    "abstract": "Comments: Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop",
    "descriptor": "\nComments: Accepted for the CVPR 2022 Transformers For Vision (T4V) workshop\n",
    "authors": [
      "John Yang",
      "Le An",
      "Anurag Dixit",
      "Jinkyu Koo",
      "Su Inn Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13791"
  },
  {
    "id": "arXiv:2204.14028",
    "title": "Quantum Computing for Power Flow Algorithms: Testing on real Quantum  Computers",
    "abstract": "Comments: 8 pages, 6 figures, 4 tables, Accepted for Presentation in 11th Bulk Power Systems Dynamics and Control Symposium, July 25-30, 2022, Banff, Canada",
    "descriptor": "\nComments: 8 pages, 6 figures, 4 tables, Accepted for Presentation in 11th Bulk Power Systems Dynamics and Control Symposium, July 25-30, 2022, Banff, Canada\n",
    "authors": [
      "Brynjar S\u00e6varsson",
      "Spyros Chatzivasileiadis",
      "Hj\u00f6rtur J\u00f3hannsson",
      "Jacob \u00d8stergaard"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.14028"
  },
  {
    "id": "arXiv:2205.00119",
    "title": "MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud",
    "abstract": "MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud",
    "descriptor": "",
    "authors": [
      "Zhen Zhang",
      "Shuai Zheng",
      "Yida Wang",
      "Justin Chiu",
      "George Karypis",
      "Trishul Chilimbi",
      "Mu Li",
      "Xin Jin"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.00119"
  },
  {
    "id": "arXiv:2205.00690",
    "title": "From Noisy Prediction to True Label: Noisy Prediction Calibration via  Generative Model",
    "abstract": "Comments: 21 pages, 9 figures. International Conference on Machine Learning (ICML 2022), Baltimore, Jul 17, 2022",
    "descriptor": "\nComments: 21 pages, 9 figures. International Conference on Machine Learning (ICML 2022), Baltimore, Jul 17, 2022\n",
    "authors": [
      "HeeSun Bae",
      "Seungjae Shin",
      "Byeonghu Na",
      "JoonHo Jang",
      "Kyungwoo Song",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00690"
  },
  {
    "id": "arXiv:2205.01681",
    "title": "Growing Isotropic Neural Cellular Automata",
    "abstract": "Growing Isotropic Neural Cellular Automata",
    "descriptor": "",
    "authors": [
      "Alexander Mordvintsev",
      "Ettore Randazzo",
      "Craig Fouts"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Cell Behavior (q-bio.CB)"
    ],
    "url": "https://arxiv.org/abs/2205.01681"
  },
  {
    "id": "arXiv:2205.02694",
    "title": "Quantifying Language Variation Acoustically with Few Resources",
    "abstract": "Comments: Accepted at NAACL 2022",
    "descriptor": "\nComments: Accepted at NAACL 2022\n",
    "authors": [
      "Martijn Bartelds",
      "Martijn Wieling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.02694"
  },
  {
    "id": "arXiv:2205.03915",
    "title": "FOLPETTI: A Novel Multi-Armed Bandit Smart Attack for Wireless Networks",
    "abstract": "FOLPETTI: A Novel Multi-Armed Bandit Smart Attack for Wireless Networks",
    "descriptor": "",
    "authors": [
      "Emilie Bout",
      "Alessandro Brighente",
      "Mauro Conti",
      "Valeria Loscri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.03915"
  },
  {
    "id": "arXiv:2205.04080",
    "title": "Linear quantum systems: a tutorial",
    "abstract": "Comments: 55 pages, 4 figures, to appear in Annual Reviews in Control",
    "descriptor": "\nComments: 55 pages, 4 figures, to appear in Annual Reviews in Control\n",
    "authors": [
      "Guofeng Zhang",
      "Zhiyuan Dong"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.04080"
  },
  {
    "id": "arXiv:2205.05265",
    "title": "What is Proxy Discrimination?",
    "abstract": "Comments: To appear as in the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea. ACM, New York, NY, USA. this https URL",
    "descriptor": "\nComments: To appear as in the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea. ACM, New York, NY, USA. this https URL\n",
    "authors": [
      "Michael Carl Tschantz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.05265"
  },
  {
    "id": "arXiv:2205.06589",
    "title": "Discrete density comonads and graph parameters",
    "abstract": "Discrete density comonads and graph parameters",
    "descriptor": "",
    "authors": [
      "Samson Abramsky",
      "Tom\u00e1\u0161 Jakl",
      "Thomas Paine"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Logic in Computer Science (cs.LO)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.06589"
  },
  {
    "id": "arXiv:2205.07246",
    "title": "FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning",
    "abstract": "Comments: Preprint. 18 pages. Codebase: this https URL",
    "descriptor": "\nComments: Preprint. 18 pages. Codebase: this https URL\n",
    "authors": [
      "Yidong Wang",
      "Hao Chen",
      "Qiang Heng",
      "Wenxin Hou",
      "Yue Fan",
      "Zhen Wu",
      "Jindong Wang",
      "Marios Savvides",
      "Takahiro Shinozaki",
      "Bhiksha Raj",
      "Bernt Schiele",
      "Xing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.07246"
  },
  {
    "id": "arXiv:2205.07759",
    "title": "Software Updates Strategies: a Quantitative Evaluation against Advanced  Persistent Threats",
    "abstract": "Software Updates Strategies: a Quantitative Evaluation against Advanced  Persistent Threats",
    "descriptor": "",
    "authors": [
      "Giorgio Di Tizio",
      "Michele Armellini",
      "Fabio Massacci"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.07759"
  },
  {
    "id": "arXiv:2205.08304",
    "title": "Bayesian Physics-Informed Neural Networks for real-world nonlinear  dynamical systems",
    "abstract": "Bayesian Physics-Informed Neural Networks for real-world nonlinear  dynamical systems",
    "descriptor": "",
    "authors": [
      "Kevin Linka",
      "Amelie Schafer",
      "Xuhui Meng",
      "Zongren Zou",
      "George Em Karniadakis",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2205.08304"
  },
  {
    "id": "arXiv:2205.08451",
    "title": "MAS2HP: A Multi Agent System to predict protein structure in 2D HP model",
    "abstract": "MAS2HP: A Multi Agent System to predict protein structure in 2D HP model",
    "descriptor": "",
    "authors": [
      "Hossein Parineh",
      "Nasser Mozayani"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.08451"
  },
  {
    "id": "arXiv:2205.08717",
    "title": "A Regression Approach to Learning-Augmented Online Algorithms",
    "abstract": "A Regression Approach to Learning-Augmented Online Algorithms",
    "descriptor": "",
    "authors": [
      "Keerti Anand",
      "Rong Ge",
      "Amit Kumar",
      "Debmalya Panigrahi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.08717"
  },
  {
    "id": "arXiv:2205.08938",
    "title": "SplitBFT: Improving Byzantine Fault Tolerance Safety Using Trusted  Compartments",
    "abstract": "SplitBFT: Improving Byzantine Fault Tolerance Safety Using Trusted  Compartments",
    "descriptor": "",
    "authors": [
      "Ines Messadi",
      "Markus Horst Becker",
      "Kai Bleeke",
      "Leander Jehl",
      "Sonia Ben Mokhtar",
      "R\u00fcdiger Kapitza"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.08938"
  },
  {
    "id": "arXiv:2205.09588",
    "title": "How catastrophic can catastrophic forgetting be in linear regression?",
    "abstract": "How catastrophic can catastrophic forgetting be in linear regression?",
    "descriptor": "",
    "authors": [
      "Itay Evron",
      "Edward Moroshko",
      "Rachel Ward",
      "Nati Srebro",
      "Daniel Soudry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.09588"
  },
  {
    "id": "arXiv:2205.09612",
    "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "abstract": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "descriptor": "",
    "authors": [
      "Yao-Ching Yu",
      "Shi-Jinn Horng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09612"
  },
  {
    "id": "arXiv:2205.09628",
    "title": "What killed the Convex Booster ?",
    "abstract": "What killed the Convex Booster ?",
    "descriptor": "",
    "authors": [
      "Yishay Mansour",
      "Richard Nock",
      "Robert C. Williamson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09628"
  },
  {
    "id": "arXiv:2205.09690",
    "title": "VNT-Net: Rotational Invariant Vector Neuron Transformers",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2104.12229 by other authors",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2104.12229 by other authors\n",
    "authors": [
      "Hedi Zisling",
      "Andrei Sharf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09690"
  },
  {
    "id": "arXiv:2205.10086",
    "title": "People Tracking and Re-Identifying in Distributed Contexts: Extension  Study of PoseTReID",
    "abstract": "Comments: 6 pages, 5 figures, 3 tables, To be submitted to EECSI2022",
    "descriptor": "\nComments: 6 pages, 5 figures, 3 tables, To be submitted to EECSI2022\n",
    "authors": [
      "Ratha Siv",
      "Matei Mancas",
      "Bernard Gosselin",
      "Dona Valy",
      "Sokchenda Sreng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10086"
  },
  {
    "id": "arXiv:2205.10264",
    "title": "DEMAND: Deep Matrix Approximately Nonlinear Decomposition to Identify  Meta, Canonical, and Sub-Spatial Pattern of functional Magnetic Resonance  Imaging in the Human Brain",
    "abstract": "Comments: 10 pages, 6 figures, an advanced deep nonlinear matrix factorization technique",
    "descriptor": "\nComments: 10 pages, 6 figures, an advanced deep nonlinear matrix factorization technique\n",
    "authors": [
      "Wei Zhang",
      "Yu Bao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10264"
  },
  {
    "id": "arXiv:2205.10390",
    "title": "EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex  Structures",
    "abstract": "Comments: 18 pages, 3 figures, and 8 tables. Under review",
    "descriptor": "\nComments: 18 pages, 3 figures, and 8 tables. Under review\n",
    "authors": [
      "Alex Morehead",
      "Xiao Chen",
      "Tianqi Wu",
      "Jian Liu",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10390"
  },
  {
    "id": "arXiv:2205.10607",
    "title": "Coordinating Policies Among Multiple Agents via an Intelligent  Communication Channel",
    "abstract": "Coordinating Policies Among Multiple Agents via an Intelligent  Communication Channel",
    "descriptor": "",
    "authors": [
      "Dianbo Liu",
      "Vedant Shah",
      "Oussama Boussif",
      "Cristian Meo",
      "Anirudh Goyal",
      "Tianmin Shu",
      "Michael Mozer",
      "Nicolas Heess",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10607"
  },
  {
    "id": "arXiv:2205.10937",
    "title": "muNet: Evolving Pretrained Deep Neural Networks into Scalable  Auto-tuning Multitask Systems",
    "abstract": "muNet: Evolving Pretrained Deep Neural Networks into Scalable  Auto-tuning Multitask Systems",
    "descriptor": "",
    "authors": [
      "Andrea Gesmundo",
      "Jeff Dean"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.10937"
  },
  {
    "id": "arXiv:2205.10962",
    "title": "Digital Twin for Secure Semiconductor Lifecycle Management: Prospects  and Applications",
    "abstract": "Comments: 37 pages including citations, 14 figures, first edit contained minor repositioning of some of the images",
    "descriptor": "\nComments: 37 pages including citations, 14 figures, first edit contained minor repositioning of some of the images\n",
    "authors": [
      "Hasan Al Shaikh",
      "Mohammad Bin Monjil",
      "Shigang Chen",
      "Navid Asadizanjani",
      "Farimah Farahmandi",
      "Mark Tehranipoor",
      "Fahim Rahman"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10962"
  },
  {
    "id": "arXiv:2205.10963",
    "title": "Protecting File Activities via Deception for ARM TrustZone",
    "abstract": "Comments: Under submission",
    "descriptor": "\nComments: Under submission\n",
    "authors": [
      "Liwei Guo",
      "Kaiyang Zhao",
      "Yiying Zhang",
      "Felix Xiaozhu Lin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2205.10963"
  },
  {
    "id": "arXiv:2205.11127",
    "title": "A Survey of Research on Fair Recommender Systems",
    "abstract": "A Survey of Research on Fair Recommender Systems",
    "descriptor": "",
    "authors": [
      "Yashar Deldjoo",
      "Dietmar Jannach",
      "Alejandro Bellogin",
      "Alessandro Difonzo",
      "Dario Zanzonelli"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11127"
  },
  {
    "id": "arXiv:2205.11191",
    "title": "NPU-BOLT: A Dataset for Bolt Object Detection in Natural Scene Images",
    "abstract": "NPU-BOLT: A Dataset for Bolt Object Detection in Natural Scene Images",
    "descriptor": "",
    "authors": [
      "Yadian Zhao",
      "Zhenglin Yang",
      "Chao Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11191"
  },
  {
    "id": "arXiv:2205.11192",
    "title": "Active Domain Adaptation with Multi-level Contrastive Units for Semantic  Segmentation",
    "abstract": "Active Domain Adaptation with Multi-level Contrastive Units for Semantic  Segmentation",
    "descriptor": "",
    "authors": [
      "Hao Zhang",
      "Ruimao Zhang",
      "Zhanglin Peng",
      "Junle Wang",
      "Yanqing Jing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11192"
  },
  {
    "id": "arXiv:2205.11211",
    "title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation",
    "abstract": "Comments: work in progress",
    "descriptor": "\nComments: work in progress\n",
    "authors": [
      "Yichao Du",
      "Weizhi Wang",
      "Zhirui Zhang",
      "Boxing Chen",
      "Tong Xu",
      "Jun Xie",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11211"
  },
  {
    "id": "arXiv:2205.11461",
    "title": "Undecidability of Network Coding, Conditional Information Inequalities,  and Conditional Independence Implication",
    "abstract": "Comments: 18 pages, 8 figures",
    "descriptor": "\nComments: 18 pages, 8 figures\n",
    "authors": [
      "Cheuk Ting Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.11461"
  },
  {
    "id": "arXiv:2205.11502",
    "title": "On the Paradox of Learning to Reason from Data",
    "abstract": "Comments: Table 1 & 2 numbers were out-dated in v1; we have updated them; the observations and conclusions remain unchanged",
    "descriptor": "\nComments: Table 1 & 2 numbers were out-dated in v1; we have updated them; the observations and conclusions remain unchanged\n",
    "authors": [
      "Honghua Zhang",
      "Liunian Harold Li",
      "Tao Meng",
      "Kai-Wei Chang",
      "Guy Van den Broeck"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11502"
  },
  {
    "id": "arXiv:2205.11606",
    "title": "Discriminative Feature Learning through Feature Distance Loss",
    "abstract": "Discriminative Feature Learning through Feature Distance Loss",
    "descriptor": "",
    "authors": [
      "Tobias Schlagenhauf",
      "Yiwen Lin",
      "Benjamin Noack"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11606"
  },
  {
    "id": "arXiv:2205.11614",
    "title": "Trust, Professional Vision and Diagnostic Work",
    "abstract": "Comments: 6 pages",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Mark Rouncefield",
      "Rob Procter",
      "Peter Tolmie"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.11614"
  },
  {
    "id": "arXiv:2205.11624",
    "title": "Effectively Incorporating Weighted Cost-to-go Heuristic in Suboptimal  CBS",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Rishi Veerapaneni",
      "Tushar Kusnur",
      "Maxim Likhachev"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11624"
  },
  {
    "id": "arXiv:2205.11801",
    "title": "SepIt: Approaching a Single Channel Speech Separation Bound",
    "abstract": "SepIt: Approaching a Single Channel Speech Separation Bound",
    "descriptor": "",
    "authors": [
      "Shahar Lutati",
      "Eliya Nachmani",
      "Lior Wolf"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11801"
  },
  {
    "id": "arXiv:2205.11811",
    "title": "Sensing Performance of Multi-Channel RFID-based Finger Augmentation  Devices for Tactile Internet",
    "abstract": "Comments: Accepted for publication in \"IEEE Journal on Radio Frequency Identification\"",
    "descriptor": "\nComments: Accepted for publication in \"IEEE Journal on Radio Frequency Identification\"\n",
    "authors": [
      "Federica Naccarata",
      "Giulio Maria Bianco",
      "Gaetano Marrocco"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.11811"
  },
  {
    "id": "arXiv:2205.11913",
    "title": "Deep Learning Workload Scheduling in GPU Datacenters: Taxonomy,  Challenges and Vision",
    "abstract": "Comments: Submitted to ACM Computing Surveys",
    "descriptor": "\nComments: Submitted to ACM Computing Surveys\n",
    "authors": [
      "Wei Gao",
      "Qinghao Hu",
      "Zhisheng Ye",
      "Peng Sun",
      "Xiaolin Wang",
      "Yingwei Luo",
      "Tianwei Zhang",
      "Yonggang Wen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11913"
  },
  {
    "id": "arXiv:2205.11951",
    "title": "Diffuse Map Guiding Unsupervised Generative Adversarial Network for  SVBRDF Estimation",
    "abstract": "Diffuse Map Guiding Unsupervised Generative Adversarial Network for  SVBRDF Estimation",
    "descriptor": "",
    "authors": [
      "Zhiyao Luo",
      "Hongnan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.11951"
  },
  {
    "id": "arXiv:2205.11998",
    "title": "Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition",
    "abstract": "Comments: Submitted to INTERSPEECH2022",
    "descriptor": "\nComments: Submitted to INTERSPEECH2022\n",
    "authors": [
      "Yuting Yang",
      "Binbin Du",
      "Yuke Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.11998"
  },
  {
    "id": "arXiv:2205.12005",
    "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal  Skip-connections",
    "abstract": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal  Skip-connections",
    "descriptor": "",
    "authors": [
      "Chenliang Li",
      "Haiyang Xu",
      "Junfeng Tian",
      "Wei Wang",
      "Ming Yan",
      "Bin Bi",
      "Jiabo Ye",
      "Hehong Chen",
      "Guohai Xu",
      "Zheng Cao",
      "Ji Zhang",
      "Songfang Huang",
      "Fei Huang",
      "Jingren Zhou",
      "Luo Si"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12005"
  },
  {
    "id": "arXiv:2205.12023",
    "title": "A divergence preserving cut finite element method for Darcy flow",
    "abstract": "A divergence preserving cut finite element method for Darcy flow",
    "descriptor": "",
    "authors": [
      "Thomas Frachon",
      "Peter Hansbo",
      "Erik Nilsson",
      "Sara Zahedi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.12023"
  },
  {
    "id": "arXiv:2205.12183",
    "title": "StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D  Mutual Learning",
    "abstract": "Comments: Accepted by CVPR 2022",
    "descriptor": "\nComments: Accepted by CVPR 2022\n",
    "authors": [
      "Yi-Hua Huang",
      "Yue He",
      "Yu-Jie Yuan",
      "Yu-Kun Lai",
      "Lin Gao"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12183"
  }
]