[
  {
    "id": "arXiv:2210.16314",
    "title": "Hierarchical Automatic Power Plane Generation with Genetic Optimization  and Multilayer Perceptron",
    "abstract": "We present an automatic multilayer power plane generation method to\naccelerate the design of printed circuit boards (PCB). In PCB design, while\nautomatic solvers have been developed to predict important indicators such as\nthe IR-drop, power integrity, and signal integrity, the generation of the power\nplane itself still largely relies on laborious manual methods. Our automatic\npower plane generation approach is based on genetic optimization combined with\na multilayer perceptron and is able to automatically generate power planes\nacross a diverse set of problems with varying levels of difficulty. Our method\nGOMLP consists of an outer loop genetic optimizer (GO) and an inner loop\nmulti-layer perceptron (MLP) that generate power planes automatically. The\ncritical elements of our approach include contour detection, feature expansion,\nand a distance measure to enable island-minimizing complex power plane\ngeneration. We compare our approach to a baseline solution based on A*. The A*\nmethod consisting of a sequential island generation and merging process which\ncan produce less than ideal solutions. Our experimental results show that on\nsingle layer power plane problems, our method outperforms A* in 71% of the\nproblems with varying levels of board layout difficulty. We further describe\nH-GOMLP, which extends GOMLP to multilayer power plane problems using\nhierarchical clustering and net similarities based on the Hausdorff distance.",
    "descriptor": "",
    "authors": [
      "Haiguang Liao",
      "Vinay Patil",
      "Xuliang Dong",
      "Devika Shanbhag",
      "Elias Fallon",
      "Taylor Hogan",
      "Mirko Spasojevic",
      "Levent Burak Kara"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16314"
  },
  {
    "id": "arXiv:2210.16315",
    "title": "Beyond calibration: estimating the grouping loss of modern neural  networks",
    "abstract": "Good decision making requires machine-learning models to provide trustworthy\nconfidence scores. To this end, recent work has focused on miscalibration, i.e,\nthe over or under confidence of model scores. Yet, contrary to widespread\nbelief, calibration is not enough: even a classifier with the best possible\naccuracy and perfect calibration can have confidence scores far from the true\nposterior probabilities. This is due to the grouping loss, created by samples\nwith the same confidence scores but different true posterior probabilities.\nProper scoring rule theory shows that given the calibration loss, the missing\npiece to characterize individual errors is the grouping loss. While there are\nmany estimators of the calibration loss, none exists for the grouping loss in\nstandard settings. Here, we propose an estimator to approximate the grouping\nloss. We use it to study modern neural network architectures in vision and NLP.\nWe find that the grouping loss varies markedly across architectures, and that\nit is a key model-comparison factor across the most accurate, calibrated,\nmodels. We also show that distribution shifts lead to high grouping loss.",
    "descriptor": "",
    "authors": [
      "Alexandre Perez-Lebel",
      "Marine Le Morvan",
      "Ga\u00ebl Varoquaux"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16315"
  },
  {
    "id": "arXiv:2210.16316",
    "title": "The secret role of undesired physical effects in accurate shape sensing  with eccentric FBGs",
    "abstract": "Fiber optic shape sensors have enabled unique advances in various navigation\ntasks, from medical tool tracking to industrial applications. Eccentric fiber\nBragg gratings (FBG) are cheap and easy-to-fabricate shape sensors that are\noften interrogated with simple setups. However, using low-cost interrogation\nsystems for such intensity-based quasi-distributed sensors introduces further\ncomplications to the sensor's signal. Therefore, eccentric FBGs have not been\nable to accurately estimate complex multi-bend shapes. Here, we present a novel\ntechnique to overcome these limitations and provide accurate and precise shape\nestimation in eccentric FBG sensors. We investigate the most important\nbending-induced effects in curved optical fibers that are usually eliminated in\nintensity-based fiber sensors. These effects contain shape deformation\ninformation with a higher spatial resolution that we are now able to extract\nusing deep learning techniques. We design a deep learning model based on a\nconvolutional neural network that is trained to predict shapes given the\nsensor's spectra. We also provide a visual explanation, highlighting wavelength\nelements whose intensities are more relevant in making shape predictions. These\nfindings imply that deep learning techniques benefit from the bending-induced\neffects that impact the desired signal in a complex manner. This is the first\nstep toward cheap yet accurate fiber shape sensing solutions.",
    "descriptor": "\nComments: 18 pages, 5 figures, preprint\n",
    "authors": [
      "Samaneh Manavi Roodsari",
      "Sara Freund",
      "Martin Angelmahr",
      "Georg Rauter",
      "Azhar Zam",
      "Wolfgang Schade",
      "Philippe C. Cattin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.16316"
  },
  {
    "id": "arXiv:2210.16318",
    "title": "Filter and evolve: progressive pseudo label refining for semi-supervised  automatic speech recognition",
    "abstract": "Fine tuning self supervised pretrained models using pseudo labels can\neffectively improve speech recognition performance. But, low quality pseudo\nlabels can misguide decision boundaries and degrade performance. We propose a\nsimple yet effective strategy to filter low quality pseudo labels to alleviate\nthis problem. Specifically, pseudo-labels are produced over the entire training\nset and filtered via average probability scores calculated from the model\noutput. Subsequently, an optimal percentage of utterances with high probability\nscores are considered reliable training data with trustworthy labels. The model\nis iteratively updated to correct the unreliable pseudo labels to minimize the\neffect of noisy labels. The process above is repeated until unreliable pseudo\nabels have been adequately corrected. Extensive experiments on LibriSpeech show\nthat these filtered samples enable the refined model to yield more correct\npredictions, leading to better ASR performances under various experimental\nsettings.",
    "descriptor": "",
    "authors": [
      "Zezhong Jin",
      "Dading Zhong",
      "Xiao Song",
      "Zhaoyi Liu",
      "Naipeng Ye",
      "Qingcheng Zeng"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16318"
  },
  {
    "id": "arXiv:2210.16334",
    "title": "Learning to Detect Interesting Anomalies",
    "abstract": "Anomaly detection algorithms are typically applied to static, unchanging,\ndata features hand-crafted by the user. But how does a user systematically\ncraft good features for anomalies that have never been seen? Here we couple\ndeep learning with active learning -- in which an Oracle iteratively labels\nsmall amounts of data selected algorithmically over a series of rounds -- to\nautomatically and dynamically improve the data features for efficient outlier\ndetection. This approach, AHUNT, shows excellent performance on MNIST, CIFAR10,\nand Galaxy-DESI data, significantly outperforming both standard anomaly\ndetection and active learning algorithms with static feature spaces. Beyond\nimproved performance, AHUNT also allows the number of anomaly classes to grow\norganically in response to Oracle's evaluations. Extensive ablation studies\nexplore the impact of Oracle question selection strategy and loss function on\nperformance. We illustrate how the dynamic anomaly class taxonomy represents\nanother step towards fully personalized rankings of different anomaly classes\nthat reflect a user's interests, allowing the algorithm to learn to ignore\nstatistically significant but uninteresting outliers (e.g., noise). This should\nprove useful in the era of massive astronomical datasets serving diverse sets\nof users who can only review a tiny subset of the incoming data.",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Alireza Vafaei Sadr",
      "Bruce A. Bassett",
      "Emmanuel Sekyi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.16334"
  },
  {
    "id": "arXiv:2210.16345",
    "title": "Estimating oil recovery factor using machine learning: Applications of  XGBoost classification",
    "abstract": "In petroleum engineering, it is essential to determine the ultimate recovery\nfactor, RF, particularly before exploitation and exploration. However,\naccurately estimating requires data that is not necessarily available or\nmeasured at early stages of reservoir development. We, therefore, applied\nmachine learning (ML), using readily available features, to estimate oil RF for\nten classes defined in this study. To construct the ML models, we applied the\nXGBoost classification algorithm. Classification was chosen because recovery\nfactor is bounded from 0 to 1, much like probability. Three databases were\nmerged, leaving us with four different combinations to first train and test the\nML models and then further evaluate them using an independent database\nincluding unseen data. The cross-validation method with ten folds was applied\non the training datasets to assess the effectiveness of the models. To evaluate\nthe accuracy and reliability of the models, the accuracy, neighborhood\naccuracy, and macro averaged f1 score were determined. Overall, results showed\nthat the XGBoost classification algorithm could estimate the RF class with\nreasonable accuracies as high as 0.49 in the training datasets, 0.34 in the\ntesting datasets and 0.2 in the independent databases used. We found that the\nreliability of the XGBoost model depended on the data in the training dataset\nmeaning that the ML models were database dependent. The feature importance\nanalysis and the SHAP approach showed that the most important features were\nreserves and reservoir area and thickness.",
    "descriptor": "",
    "authors": [
      "Alireza Roustazadeh",
      "Behzad Ghanbarian",
      "Frank Male",
      "Mohammad B. Shadmand",
      "Vahid Taslimitehrani",
      "Larry W. Lake"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16345"
  },
  {
    "id": "arXiv:2210.16346",
    "title": "Improving Hyperspectral Adversarial Robustness using Ensemble Networks  in the Presences of Multiple Attacks",
    "abstract": "Semantic segmentation of hyperspectral images (HSI) has seen great strides in\nrecent years by incorporating knowledge from deep learning RGB classification\nmodels. Similar to their classification counterparts, semantic segmentation\nmodels are vulnerable to adversarial examples and need adversarial training to\ncounteract them. Traditional approaches to adversarial robustness focus on\ntraining or retraining a single network on attacked data, however, in the\npresence of multiple attacks these approaches decrease the performance compared\nto networks trained individually on each attack. To combat this issue we\npropose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses\non attack type detection and adversarial robustness under a unified model to\npreserve per data-type weight optimally while robustifiying the overall\nnetwork. In the proposed method, a discriminator network is used to separate\ndata by attack type into their specific attack-expert ensemble network. Our\napproach allows for the presence of multiple attacks mixed together while also\nlabeling attack types during testing. We experimentally show that ADE-Net\noutperforms the baseline, which is a single network adversarially trained under\na mix of multiple attacks, for HSI Indian Pines, Kennedy Space, and Houston\ndatasets.",
    "descriptor": "\nComments: 6 pages, 2 figures, 1 table, 1 algorithm\n",
    "authors": [
      "Nicholas Soucy",
      "Salimeh Yasaei Sekeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16346"
  },
  {
    "id": "arXiv:2210.16349",
    "title": "Numerical analysis of a time-stepping method for the Westervelt equation  with time-fractional damping",
    "abstract": "We develop a numerical method for the Westervelt equation, an important\nequation in nonlinear acoustics, in the form where the attenuation is\nrepresented by a class of non-local in time operators. A semi-discretisation in\ntime based on the trapezoidal rule and A-stable convolution quadrature is\nstated and analysed. Existence and regularity analysis of the continuous\nequations informs the stability and error analysis of the semi-discrete system.\nThe error analysis includes the consideration of the singularity at $t = 0$\nwhich is addressed by the use of a correction in the numerical scheme.\nExtensive numerical experiments confirm the theory.",
    "descriptor": "",
    "authors": [
      "Katherine Baker",
      "Lehel Banjai",
      "Mariya Ptashnyk"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.16349"
  },
  {
    "id": "arXiv:2210.16351",
    "title": "Parallel Breadth-First Search and Exact Shortest Paths and Stronger  Notions for Approximate Distances",
    "abstract": "We introduce stronger notions for approximate single-source shortest-path\ndistances, show how to efficiently compute them from weaker standard notions,\nand demonstrate the algorithmic power of these new notions and transformations.\nOne application is the first work-efficient parallel algorithm for computing\nexact single-source shortest paths graphs -- resolving a major open problem in\nparallel computing.\nGiven a source vertex in a directed graph with polynomially-bounded\nnonnegative integer lengths, the algorithm computes an exact shortest path tree\nin $m \\log^{O(1)} n$ work and $n^{1/2+o(1)}$ depth. Previously, no parallel\nalgorithm improving the trivial linear depths of Dijkstra's algorithm without\nsignificantly increasing the work was known, even for the case of undirected\nand unweighted graphs (i.e., for computing a BFS-tree).\nOur main result is a black-box transformation that uses $\\log^{O(1)} n$\nstandard approximate distance computations to produce approximate distances\nwhich also satisfy the subtractive triangle inequality (up to a\n$(1+\\varepsilon)$ factor) and even induce an exact shortest path tree in a\ngraph with only slightly perturbed edge lengths. These strengthened\napproximations are algorithmically significantly more powerful and overcome\nwell-known and often encountered barriers for using approximate distances. In\ndirected graphs they can even be boosted to exact distances. This results in a\nblack-box transformation of any (parallel or distributed) algorithm for\napproximate shortest paths in directed graphs into an algorithm computing exact\ndistances at essentially no cost. Applying this to the recent breakthroughs of\nFineman et al. for compute approximate SSSP-distances via approximate hopsets\ngives new parallel and distributed algorithm for exact shortest paths.",
    "descriptor": "",
    "authors": [
      "V\u00e1clav Rozho\u0148",
      "Bernhard Haeupler",
      "Anders Martinsson",
      "Christoph Grunau",
      "Goran Zuzic"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16351"
  },
  {
    "id": "arXiv:2210.16352",
    "title": "Towards Privacy Engineering for Real-Time Analytics in the  Human-Centered Internet of Things",
    "abstract": "Big data applications offer smart solutions to many urgent societal\nchallenges, such as health care, traffic coordination, energy management, etc.\nThe basic premise for these applications is \"the more data the better\". The\nfocus often lies on sensing infrastructures in the public realm that produce an\never-increasing amount of data. Yet, any smartphone and smartwatch owner could\nbe a continuous source of valuable data and contribute to many useful big data\napplications. However, such data can reveal a lot of sensitive information,\nlike the current location or the heart rate of the owner of such devices.\nProtection of personal data is important in our society and for example\nmanifested in the EU General Data Protection Regulation (GDPR). However,\nprivacy protection and useful big data applications are hard to bring together,\nparticularly in the human-centered IoT. Implementing proper privacy protection\nrequires skills that are typically not in the focus of data analysts and big\ndata developers. Thus, many individuals tend to share none of their data if in\ndoubt whether it will be properly protected. There exist excellent privacy\nsolutions between the \"all or nothing\" approach. For example, instead of\ncontinuously publishing the current location of individuals one might aggregate\nthis data and only publish information of how many individuals are in a certain\narea of the city. Thus, personal data is not revealed, while useful information\nfor certain applications like traffic coordination is retained. The goal of the\nParrot project is to provide tools for real-time data analysis applications\nthat leverage this \"middle ground\". Data analysts should only be required to\nspecify their data needs, and end-users can select the privacy requirements for\ntheir data as well as the applications and end-users they want to share their\ndata with.",
    "descriptor": "",
    "authors": [
      "Thomas Plagemann",
      "Vera Goebel",
      "Matthias Hollick",
      "Boris Koldehofe"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16352"
  },
  {
    "id": "arXiv:2210.16353",
    "title": "Proposal of FPGA logic change after service launch for environment  adaptation",
    "abstract": "In order to make full use of heterogeneous hardware, it is necessary to have\na technical skill of hardware such as OpenCL, and the current situation is that\nthe barrier is high. Based on this background, I have proposed\nenvironment-adaptive software that enables high-performance operation by\nautomatically converting application code written for normal CPUs by engineers\naccording to the deployed environment and setting appropriate amount of\nresources. Until now, I only considered conversions and settings before the\nstart of operation. In this paper, I verify that the logic is reconfigured\naccording to the usage characteristics during operation. I confirm that the\napplication running on the FPGA is reconfigured into another application\naccording to the usage characteristics.",
    "descriptor": "\nComments: 6 pages, 4 figures, in Japanese, IEICE Technical Report, SC2022-35, Nov. 2022\n",
    "authors": [
      "Yoji Yamato"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16353"
  },
  {
    "id": "arXiv:2210.16361",
    "title": "Regularized numerical methods for the nonlinear Schr\u00f6dinger equation  with singular nonlinearity",
    "abstract": "We present different regularizations and numerical methods for the nonlinear\nSchr\\\"odinger equation with singular nonlinearity (sNLSE) including the\nregularized Lie-Trotter time-splitting (LTTS) methods and regularized\nLawson-type exponential integrator (LTEI) methods. Due to the blowup of the\nsingular nonlinearity, i.e., $f(\\rho)=\\rho^{\\alpha}$ with a fixed exponent\n$\\alpha<0$ goes to infinity when $\\rho \\to 0^+$ ($\\rho = |\\psi|^2$ represents\nthe density with $\\psi$ being the complex-valued wave function or order\nparameter), there are significant difficulties in designing accurate and\nefficient numerical schemes to solve the sNLSE. In order to suppress the\nround-off error and avoid blowup near $\\rho = 0^+$, two types of\nregularizations for the sNLSE are proposed with a small regularization\nparameter $0 < \\eps \\ll 1$. One is based on the local energy regularization\n(LER) for the sNLSE via regularizing the energy density $F(\\rho) =\n\\frac{1}{\\alpha+1}\\rho^{\\alpha+1}$ locally near $\\rho = 0^+$ with a polynomial\napproximation and then obtaining a local energy regularized nonlinear\nSchr\\\"odinger equation via energy variation. The other one is the global\nnonlinearity regularization which directly regularizes the singular\nnonlinearity $f(\\rho)=\\rho^{\\alpha}$ to avoid blowup near $\\rho = 0^+$. For the\nregularized models, we apply the first-order Lie-Trotter time-splitting method\nand Lawson-type exponential integrator method for temporal discretization and\ncombine with the Fourier pseudospectral method in space to numerically solve\nthem. Numerical examples are provided to show the convergence of the\nregularized models to the sNLSE and they suggest that the local energy\nregularization performs better than directly regularizing the singular\nnonlinearity globally.",
    "descriptor": "",
    "authors": [
      "Weizhu Bao",
      "Yue Feng",
      "Ying Ma"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16361"
  },
  {
    "id": "arXiv:2210.16363",
    "title": "Predicting Brain Age using Transferable coVariance Neural Networks",
    "abstract": "The deviation between chronological age and biological age is a\nwell-recognized biomarker associated with cognitive decline and\nneurodegeneration. Age-related and pathology-driven changes to brain structure\nare captured by various neuroimaging modalities. These datasets are\ncharacterized by high dimensionality as well as collinearity, hence\napplications of graph neural networks in neuroimaging research routinely use\nsample covariance matrices as graphs. We have recently studied covariance\nneural networks (VNNs) that operate on sample covariance matrices using the\narchitecture derived from graph convolutional networks, and we showed VNNs\nenjoy significant advantages over traditional data analysis approaches. In this\npaper, we demonstrate the utility of VNNs in inferring brain age using cortical\nthickness data. Furthermore, our results show that VNNs exhibit multi-scale and\nmulti-site transferability for inferring {brain age}. In the context of brain\nage in Alzheimer's disease (AD), our experiments show that i) VNN outputs are\ninterpretable as brain age predicted using VNNs is significantly elevated for\nAD with respect to healthy subjects for different datasets; and ii) VNNs can be\ntransferable, i.e., VNNs trained on one dataset can be transferred to another\ndataset with different dimensions without retraining for brain age prediction.",
    "descriptor": "",
    "authors": [
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Corey McMillan",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.16363"
  },
  {
    "id": "arXiv:2210.16365",
    "title": "Elastic Weight Consolidation Improves the Robustness of Self-Supervised  Learning Methods under Transfer",
    "abstract": "Self-supervised representation learning (SSL) methods provide an effective\nlabel-free initial condition for fine-tuning downstream tasks. However, in\nnumerous realistic scenarios, the downstream task might be biased with respect\nto the target label distribution. This in turn moves the learned fine-tuned\nmodel posterior away from the initial (label) bias-free self-supervised model\nposterior. In this work, we re-interpret SSL fine-tuning under the lens of\nBayesian continual learning and consider regularization through the Elastic\nWeight Consolidation (EWC) framework. We demonstrate that self-regularization\nagainst an initial SSL backbone improves worst sub-group performance in\nWaterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture.\nFurthermore, to help simplify the use of EWC with SSL, we pre-compute and\npublicly release the Fisher Information Matrix (FIM), evaluated with 10,000\nImageNet-1K variates evaluated on large modern SSL architectures including\nViT-B/16 and ResNet50 trained with DINO.",
    "descriptor": "\nComments: NeurIPS 2022 Workshop: Self-Supervised Learning - Theory and Practice\n",
    "authors": [
      "Andrius Ovsianas",
      "Jason Ramapuram",
      "Dan Busbridge",
      "Eeshan Gunesh Dhekane",
      "Russ Webb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16365"
  },
  {
    "id": "arXiv:2210.16367",
    "title": "LAKEE: A Lightweight Authenticated Key Exchange Protocol for Power  Constrained Devices",
    "abstract": "The rapid development of IoT networks has led to a research trend in\ndesigning effective security features for them. Due to the power-constrained\nnature of IoT devices, the security features should remain as lightweight as\npossible. Currently, most of the IoT network traffic is unencrypted. The\nleakage of smart devices' unencrypted data can come with the significant cost\nof a privacy breach. To have a secure channel with encrypted traffic, two\nendpoints in a network have to authenticate each other and calculate a\nshort-term key. They can then communicate through an authenticated and secure\nchannel. This process is referred to as authenticated key exchange (AKE).\nAlthough Datagram Transport Layer Security (DTLS) offers an AKE protocol for\nIoT networks, research has proposed more efficient and case-specific\nalternatives. This paper presents LAKEE, a straightforward, lightweight AKE\nprotocol for IoT networks. Our protocol employs elliptic curve cryptography for\ngenerating a short-term session key. It reduces the communication and\ncomputational overhead of its alternatives while maintaining or improving their\nsecurity strength. The simplicity and low overhead of our protocol make it a\nfit for a network of constrained devices.",
    "descriptor": "",
    "authors": [
      "Seyedsina Nabavirazavi",
      "S. Sitharama Iyengar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.16367"
  },
  {
    "id": "arXiv:2210.16371",
    "title": "Distributed Black-box Attack against Image Classification Cloud Services",
    "abstract": "Black-box adversarial attacks can fool image classifiers into misclassifying\nimages without requiring access to model structure and weights. Recently\nproposed black-box attacks can achieve a success rate of more than 95\\% after\nless than 1,000 queries. The question then arises of whether black-box attacks\nhave become a real threat against IoT devices that rely on cloud APIs to\nachieve image classification. To shed some light on this, note that prior\nresearch has primarily focused on increasing the success rate and reducing the\nnumber of required queries. However, another crucial factor for black-box\nattacks against cloud APIs is the time required to perform the attack. This\npaper applies black-box attacks directly to cloud APIs rather than to local\nmodels, thereby avoiding multiple mistakes made in prior research. Further, we\nexploit load balancing to enable distributed black-box attacks that can reduce\nthe attack time by a factor of about five for both local search and gradient\nestimation methods.",
    "descriptor": "\nComments: 10 pages, 11 figures\n",
    "authors": [
      "Han Wu",
      "Sareh Rowlands",
      "Johan Wahlstrom"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16371"
  },
  {
    "id": "arXiv:2210.16377",
    "title": "Least-squares finite elements for distributed optimal control problems",
    "abstract": "We provide a framework for the numerical approximation of distributed optimal\ncontrol problems, based on least-squares finite element methods. Our proposed\nmethod simultaneously solves the state and adjoint equations and is\n$\\inf$--$\\sup$ stable for any choice of conforming discretization spaces. A\nreliable and efficient a posteriori error estimator is derived for problems\nwhere box constraints are imposed on the control. It can be localized and\ntherefore used to steer an adaptive algorithm. For unconstrained optimal\ncontrol problems we obtain a pure least-squares finite element method whereas\nfor constrained problems we derive and analyze a variational inequality where\nthe PDE part is tackled by least-squares finite element methods. We show that\nthe abstract framework can be applied to a wide range of problems, including\nscalar second-order PDEs, the Stokes problem, and parabolic problems on\nspace-time domains. Numerical examples for some selected problems are\npresented.",
    "descriptor": "",
    "authors": [
      "Thomas F\u00fchrer",
      "Michael Karkulik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16377"
  },
  {
    "id": "arXiv:2210.16378",
    "title": "Modeling and Rapid Prototyping of Integrated Transmission-Distribution  OPF Formulations with PowerModelsITD.jl",
    "abstract": "Conventional electric power systems are composed of different unidirectional\npower flow stages of generation, transmission, and distribution, managed\nindependently by transmission system and distribution system operators.\nHowever, as distribution systems increase in complexity due to the integration\nof distributed energy resources, coordination between transmission and\ndistribution networks will be imperative for the optimal operation of the power\ngrid. However, coupling models and formulations between transmission and\ndistribution is non-trivial, in particular due to the common practice of\nmodeling transmission systems as single-phase, and distribution systems as\nmulti-conductor phase-unbalanced. To enable the rapid prototyping of power flow\nformulations, in particular in the modeling of the boundary conditions between\nthese two seemingly incompatible data models, we introduce PowerModelsITD.jl, a\nfree, open-source toolkit written in Julia for integrated\ntransmission-distribution (ITD) optimization that leverages mature optimization\nlibraries from the InfrastructureModels.jl-ecosystem. The primary objective of\nthe proposed framework is to provide baseline implementations of steady-state\nITD optimization problems, while providing a common platform for the evaluation\nof emerging formulations and optimization problems. In this work, we introduce\nthe nonlinear formulations currently supported in PowerModelsITD.jl, which\ninclude AC-polar, AC-rectangular, current-voltage, and a linear network\ntransportation model. Results are validated using combinations of IEEE\ntransmission and distribution networks.",
    "descriptor": "",
    "authors": [
      "Juan Ospina",
      "David M. Fobes",
      "Russell Bent",
      "Andreas W\u00e4chter"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16378"
  },
  {
    "id": "arXiv:2210.16380",
    "title": "An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble  Modeling, 'Human Softmax' Distributions, and Entropic Measures of Uncertainty",
    "abstract": "Noisy, crowdsourced image datasets prove challenging, even for the best\nneural networks. Two issues which complicate classification on such datasets\nare class imbalance and ground-truth uncertainty in labeling. The AL-ALL and\nAL-PUB datasets-consisting of tightly cropped, individual characters from\nimages of ancient Greek papyri are strongly affected by both issues. The\napplication of ensemble modeling to such a dataset can help identify images\nwhere the ground-truth is questionable and quantify the trustworthiness of\nthose samples. We apply stacked generalization consisting of nearly identical\nResNets: one utilizing cross-entropy (CXE) and the other Kullback-Liebler\nDivergence (KLD). The CXE network uses standard labeling drawn from the\ncrowdsourced consensus. In contrast, the KLD network uses probabilistic\nlabeling for each image derived from the distribution of crowdsourced\nannotations. We refer to this labeling as the Human Softmax (HSM) distribution.\nFor our ensemble model, we apply a k-nearest neighbors model to the outputs of\nthe CXE and KLD networks. Individually, the ResNet models have approximately\n93% accuracy, while the ensemble model achieves an accuracy of >95%. We also\nperform an analysis of the Shannon entropy of the various models' output\ndistributions to measure classification uncertainty.",
    "descriptor": "",
    "authors": [
      "Graham West",
      "Matthew I. Swindall",
      "Ben Keener",
      "Timothy Player",
      "Alex C. Williams",
      "James H. Brusuelas",
      "John F. Wallin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16380"
  },
  {
    "id": "arXiv:2210.16381",
    "title": "Not Another Day Zero: Design Hackathons for Community-Based Water  Quality Monitoring",
    "abstract": "This study looks at water quality monitoring and management as a new form of\ncommunity engagement. Through a series of a unique research method called\n`design hackathons', we engaged with a hyperlocal community of citizens who are\nactively involved in monitoring and management of their local watershed. These\ndesign hackathons sought to understand the motivation, practices, collaboration\nand experiences of these citizens. Qualitative analysis of data revealed the\nnature of the complex stakeholder network, workflow practices, initiatives to\nengage with a larger community, current state of technological infrastructure\nbeing used, and innovative design scenarios proposed by the hackathon\nparticipants. Based on this comprehensive analysis, we conceptualize water\nquality monitoring and management as community-based monitoring and management,\nand water data as community data. Such a conceptualization sheds light on how\nthese practices can help in preempting water crisis by empowering citizens\nthrough increased awareness, active participation and informal learning of\nwater data and resources.",
    "descriptor": "\nComments: 21 pages, 3 figures, 3 tables\n",
    "authors": [
      "Srishti Gupta",
      "Chun-Hua Tsai",
      "John M. Carroll"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16381"
  },
  {
    "id": "arXiv:2210.16386",
    "title": "Dynamic Bandits with an Auto-Regressive Temporal Structure",
    "abstract": "Multi-armed bandit (MAB) problems are mainly studied under two extreme\nsettings known as stochastic and adversarial. These two settings, however, do\nnot capture realistic environments such as search engines and marketing and\nadvertising, in which rewards stochastically change in time. Motivated by that,\nwe introduce and study a dynamic MAB problem with stochastic temporal\nstructure, where the expected reward of each arm is governed by an\nauto-regressive (AR) model. Due to the dynamic nature of the rewards, simple\n\"explore and commit\" policies fail, as all arms have to be explored\ncontinuously over time. We formalize this by characterizing a per-round regret\nlower bound, where the regret is measured against a strong (dynamic) benchmark.\nWe then present an algorithm whose per-round regret almost matches our regret\nlower bound. Our algorithm relies on two mechanisms: (i) alternating between\nrecently pulled arms and unpulled arms with potential, and (ii) restarting.\nThese mechanisms enable the algorithm to dynamically adapt to changes and\ndiscard irrelevant past information at a suitable rate. In numerical studies,\nwe further demonstrate the strength of our algorithm under different types of\nnon-stationary settings.",
    "descriptor": "\nComments: 42 pages, 4 figures\n",
    "authors": [
      "Qinyi Chen",
      "Negin Golrezaei",
      "Djallel Bouneffouf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.16386"
  },
  {
    "id": "arXiv:2210.16391",
    "title": "Radically Lower Data-Labeling Costs for Visually Rich Document  Extraction Models",
    "abstract": "A key bottleneck in building automatic extraction models for visually rich\ndocuments like invoices is the cost of acquiring the several thousand\nhigh-quality labeled documents that are needed to train a model with acceptable\naccuracy. We propose Selective Labeling to simplify the labeling task to\nprovide \"yes/no\" labels for candidate extractions predicted by a model trained\non partially labeled documents. We combine this with a custom active learning\nstrategy to find the predictions that the model is most uncertain about. We\nshow through experiments on document types drawn from 3 different domains that\nselective labeling can reduce the cost of acquiring labeled data by $10\\times$\nwith a negligible loss in accuracy.",
    "descriptor": "\nComments: 9 pages, 8 figures, 3 tables\n",
    "authors": [
      "Yichao Zhou",
      "James B. Wendt",
      "Navneet Potti",
      "Jing Xie",
      "Sandeep Tata"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16391"
  },
  {
    "id": "arXiv:2210.16392",
    "title": "Physics-aware Graph Neural Network for Accurate RNA 3D Structure  Prediction",
    "abstract": "Biological functions of RNAs are determined by their three-dimensional (3D)\nstructures. Thus, given the limited number of experimentally determined RNA\nstructures, the prediction of RNA structures will facilitate elucidating RNA\nfunctions and RNA-targeted drug discovery, but remains a challenging task. In\nthis work, we propose a Graph Neural Network (GNN)-based scoring function\ntrained only with the atomic types and coordinates on limited solved RNA 3D\nstructures for distinguishing accurate structural models. The proposed\nPhysics-aware Multiplex Graph Neural Network (PaxNet) separately models the\nlocal and non-local interactions inspired by molecular mechanics. Furthermore,\nPaxNet contains an attention-based fusion module that learns the individual\ncontribution of each interaction type for the final prediction. We rigorously\nevaluate the performance of PaxNet on two benchmarks and compare it with\nseveral state-of-the-art baselines. The results show that PaxNet significantly\noutperforms all the baselines overall, and demonstrate the potential of PaxNet\nfor improving the 3D structure modeling of RNA and other macromolecules.",
    "descriptor": "\nComments: Accepted by the Machine Learning for Structural Biology Workshop (MLSB) at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Shuo Zhang",
      "Yang Liu",
      "Lei Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.16392"
  },
  {
    "id": "arXiv:2210.16394",
    "title": "HeartSiam: A Domain Invariant Model for Heart Sound Classification",
    "abstract": "Cardiovascular disease is one of the leading causes of death according to\nWHO. Phonocardiography (PCG) is a costeffective, non-invasive method suitable\nfor heart monitoring. The main aim of this work is to classify heart sounds\ninto normal/abnormal categories. Heart sounds are recorded using different\nstethoscopes, thus varying in the domain. Based on recent studies, this\nvariability can affect heart sound classification. This work presents a Siamese\nnetwork architecture for learning the similarity between normal vs. normal or\nabnormal vs. abnormal signals and the difference between normal vs. abnormal\nsignals. By applying this similarity and difference learning across all\ndomains, the task of domain invariant heart sound classification can be well\nachieved. We have used the multi-domain 2016 Physionet/CinC challenge dataset\nfor the evaluation method. Results: On the evaluation set provided by the\nchallenge, we have achieved a sensitivity of 82.8%, specificity of 75.3%, and\nmean accuracy of 79.1%. While overcoming the multi-domain problem, the proposed\nmethod has surpassed the first-place method of the Physionet challenge in terms\nof specificity up to 10.9% and mean accuracy up to 5.6%. Also, compared with\nsimilar state-of-the-art domain invariant methods, our model converges faster\nand performs better in specificity (4.1%) and mean accuracy (1.5%) with an\nequal number of epochs learned.",
    "descriptor": "",
    "authors": [
      "Reza Yousefi Mashhoor",
      "Ahmad Ayatollahi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16394"
  },
  {
    "id": "arXiv:2210.16395",
    "title": "Ensure Differential Privacy and Convergence Accuracy in Consensus  Tracking and Aggregative Games with Coupling Constraints",
    "abstract": "We address differential privacy for fully distributed aggregative games with\nshared coupling constraints. By co-designing the generalized Nash equilibrium\n(GNE) seeking mechanism and the differential-privacy noise injection mechanism,\nwe propose the first GNE seeking algorithm that can ensure both provable\nconvergence to the GNE and rigorous epsilon-differential privacy, even with the\nnumber of iterations tending to infinity. As a basis of the co-design, we also\npropose a differentially private consensus-tracking algorithm that can achieve\nrigorous epsilon-differential privacy while maintaining accurate tracking\nperformance, which, to our knowledge, has not been achieved before. To\nfacilitate the convergence analysis, we also establish a general convergence\nresult for stochastically-perturbed nonstationary fixed-point iteration\nprocesses, which lie at the core of numerous optimization and variational\nproblems. Numerical simulation results confirm the effectiveness of the\nproposed approach.",
    "descriptor": "",
    "authors": [
      "Yongqiang Wang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.16395"
  },
  {
    "id": "arXiv:2210.16396",
    "title": "Enhanced Energy-Saving Mechanisms in TSCH Networks for the IIoT: the  PRIL Approach",
    "abstract": "Lifetime of motes in wireless sensor networks can be enlarged by decreasing\nthe energy spent for communication. Approaches like time slotted channel\nhopping pursue this goal by performing frame exchanges according to a\npredefined schedule, which helps reducing the duty cycle. Unfortunately,\nwhenever the receiving radio interface is active but nobody in the network is\ntransmitting, idle listening occurs. If the traffic pattern is known in\nadvance, as in the relevant case of periodic sensing, proactive reduction of\nidle listening (PRIL) noticeably lowers energy waste by disabling receivers\nwhen no frames are expected for them. Optimal PRIL operation demands that, at\nany time, the transmitter and receiver sides of a link have a coherent view of\nits state (either enabled or disabled). However, this is not ensured in the\npresence of acknowledgment frame losses. This paper presents and analyzes some\nstrategies to cope with such events. An extensive experimental campaign has\nbeen carried out through discrete event simulation to determine what\nconsequences above errors may have from both a functional and performance\nviewpoint. Results show that, although no strategy is optimal in all\ncircumstances, different solutions can be profitably adopted depending on the\nspecific operating conditions.",
    "descriptor": "\nComments: preprint, 11 pages\n",
    "authors": [
      "Gianluca Cena",
      "Stefano Scanzio",
      "Adriano Valenzano"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.16396"
  },
  {
    "id": "arXiv:2210.16398",
    "title": "System Demo: Tool and Infrastructure for Offensive Language Error  Analysis (OLEA) in English",
    "abstract": "The automatic detection of offensive language is a pressing societal need.\nMany systems perform well on explicit offensive language but struggle to detect\nmore complex, nuanced, or implicit cases of offensive and hateful language.\nOLEA is an open-source Python library that provides easy-to-use tools for error\nanalysis in the context of detecting offensive language in English. OLEA also\nprovides an infrastructure for re-distribution of new datasets and analysis\nmethods requiring very little coding.",
    "descriptor": "\nComments: Source code and library download available on PyPI : this https URL\n",
    "authors": [
      "Marie Grace",
      "Xajavion \"Jay\" Seabrum",
      "Dananjay Srinivas",
      "Alexis Palmer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16398"
  },
  {
    "id": "arXiv:2210.16400",
    "title": "Flatter, faster: scaling momentum for optimal speedup of SGD",
    "abstract": "Commonly used optimization algorithms often show a trade-off between good\ngeneralization and fast training times. For instance, stochastic gradient\ndescent (SGD) tends to have good generalization; however, adaptive gradient\nmethods have superior training times. Momentum can help accelerate training\nwith SGD, but so far there has been no principled way to select the momentum\nhyperparameter. Here we study implicit bias arising from the interplay between\nSGD with label noise and momentum in the training of overparametrized neural\nnetworks. We find that scaling the momentum hyperparameter $1-\\beta$ with the\nlearning rate to the power of $2/3$ maximally accelerates training, without\nsacrificing generalization. To analytically derive this result we develop an\narchitecture-independent framework, where the main assumption is the existence\nof a degenerate manifold of global minimizers, as is natural in\noverparametrized models. Training dynamics display the emergence of two\ncharacteristic timescales that are well-separated for generic values of the\nhyperparameters. The maximum acceleration of training is reached when these two\ntimescales meet, which in turn determines the scaling limit we propose. We\nperform experiments, including matrix sensing and ResNet on CIFAR10, which\nprovide evidence for the robustness of these results.",
    "descriptor": "\nComments: 12+13 pages, 3 figures\n",
    "authors": [
      "Aditya Cowsik",
      "Tankut Can",
      "Paolo Glorioso"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ],
    "url": "https://arxiv.org/abs/2210.16400"
  },
  {
    "id": "arXiv:2210.16401",
    "title": "The Fisher-Rao Loss for Learning under Label Noise",
    "abstract": "Choosing a suitable loss function is essential when learning by empirical\nrisk minimisation. In many practical cases, the datasets used for training a\nclassifier may contain incorrect labels, which prompts the interest for using\nloss functions that are inherently robust to label noise. In this paper, we\nstudy the Fisher-Rao loss function, which emerges from the Fisher-Rao distance\nin the statistical manifold of discrete distributions. We derive an upper bound\nfor the performance degradation in the presence of label noise, and analyse the\nlearning speed of this loss. Comparing with other commonly used losses, we\nargue that the Fisher-Rao loss provides a natural trade-off between robustness\nand training dynamics. Numerical experiments with synthetic and MNIST datasets\nillustrate this performance.",
    "descriptor": "\nComments: 20 pages, 5 figures\n",
    "authors": [
      "Henrique K. Miyamoto",
      "F\u00e1bio C. C. Meneghetti",
      "Sueli I. R. Costa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16401"
  },
  {
    "id": "arXiv:2210.16402",
    "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better  Computational Complexity",
    "abstract": "In this work, we study distributed optimization algorithms that reduce the\nhigh communication costs of synchronization by allowing clients to perform\nmultiple local gradient steps in each communication round. Recently, Mishchenko\net al. (2022) proposed a new type of local method, called ProxSkip, that enjoys\nan accelerated communication complexity without any data similarity condition.\nHowever, their method requires all clients to call local gradient oracles with\nthe same frequency. Because of statistical heterogeneity, we argue that clients\nwith well-conditioned local problems should compute their local gradients less\nfrequently than clients with ill-conditioned local problems. Our first\ncontribution is the extension of the original ProxSkip method to the setup\nwhere clients are allowed to perform a different number of local gradient steps\nin each communication round. We prove that our modified method, GradSkip, still\nconverges linearly, has the same accelerated communication complexity, and the\nrequired frequency for local gradient computations is proportional to the local\ncondition number. Next, we generalize our method by extending the randomness of\nprobabilistic alternations to arbitrary unbiased compression operators and\nconsidering a generic proximable regularizer. This generalization, GradSkip+,\nrecovers several related methods in the literature. Finally, we present an\nempirical study to confirm our theoretical claims.",
    "descriptor": "\nComments: 21 pages, 2 algorithms, 3 figures\n",
    "authors": [
      "Artavazd Maranjyan",
      "Mher Safaryan",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.16402"
  },
  {
    "id": "arXiv:2210.16404",
    "title": "Experimental Evaluation of Seamless Redundancy Applied to Industrial  Wi-Fi Networks",
    "abstract": "Seamless redundancy can be profitably exploited to improve predictability of\nwireless networks in general and, in particular, IEEE 802.11. According to this\napproach, packets are transmitted by senders on two (or more) channels at the\nsame time and duplicate copies are discarded by receivers. As long as the\nbehavior of physical channels is uncorrelated, communication quality improves\nnoticeably, in terms of both transmission latencies and percentage of dropped\nframes. In this paper, communication over redundant links has been analyzed by\nmeans of a thorough experimental campaign, based on measurements carried out on\nreal devices. Results confirm that, under typical operating conditions, the\nassumption of independence among channels in properly designed systems is\nverified reasonably well. Indeed, in our experiments, measured link quality\nindices did not differ more than 10% from what we expected from theory. This\ngrants for redundant solutions tangible advantages over conventional Wi-Fi\nnetworks.",
    "descriptor": "\nComments: preprint, 10 pages\n",
    "authors": [
      "Gianluca Cena",
      "Stefano Scanzio",
      "Adriano Valenzano"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16404"
  },
  {
    "id": "arXiv:2210.16405",
    "title": "Evaluation of Categorical Generative Models -- Bridging the Gap Between  Real and Synthetic Data",
    "abstract": "The machine learning community has mainly relied on real data to benchmark\nalgorithms as it provides compelling evidence of model applicability.\nEvaluation on synthetic datasets can be a powerful tool to provide a better\nunderstanding of a model's strengths, weaknesses, and overall capabilities.\nGaining these insights can be particularly important for generative modeling as\nthe target quantity is completely unknown. Multiple issues related to the\nevaluation of generative models have been reported in the literature. We argue\nthose problems can be avoided by an evaluation based on ground truth. General\ncriticisms of synthetic experiments are that they are too simplified and not\nrepresentative of practical scenarios. As such, our experimental setting is\ntailored to a realistic generative task. We focus on categorical data and\nintroduce an appropriately scalable evaluation method. Our method involves\ntasking a generative model to learn a distribution in a high-dimensional\nsetting. We then successively bin the large space to obtain smaller probability\nspaces where meaningful statistical tests can be applied. We consider\nincreasingly large probability spaces, which correspond to increasingly\ndifficult modeling tasks and compare the generative models based on the highest\ntask difficulty they can reach before being detected as being too far from the\nground truth. We validate our evaluation procedure with synthetic experiments\non both synthetic generative models and current state-of-the-art categorical\ngenerative models.",
    "descriptor": "\nComments: Submitted to the 2023 International Conference on Acoustics, Speech, and Signal Processing (ICASSP). June 2023. 5 pages, 4 figures\n",
    "authors": [
      "Florence Regol",
      "Anja Kroon",
      "Mark Coates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16405"
  },
  {
    "id": "arXiv:2210.16407",
    "title": "Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE",
    "abstract": "Figurative language (e.g., \"he flew like the wind\") is challenging to\nunderstand, as it is hard to tell what implicit information is being conveyed\nfrom the surface form alone. We hypothesize that to perform this task well, the\nreader needs to mentally elaborate the scene being described to identify a\nsensible meaning of the language. We present DREAM-FLUTE, a figurative language\nunderstanding system that does this, first forming a \"mental model\" of\nsituations described in a premise and hypothesis before making an\nentailment/contradiction decision and generating an explanation. DREAM-FLUTE\nuses an existing scene elaboration model, DREAM, for constructing its \"mental\nmodel.\" In the FigLang2022 Shared Task evaluation, DREAM-FLUTE achieved (joint)\nfirst place (Acc@60=63.3%), and can perform even better with ensemble\ntechniques, demonstrating the effectiveness of this approach. More generally,\nthis work suggests that adding a reflective component to pretrained language\nmodels can improve their performance beyond standard fine-tuning (3.3%\nimprovement in Acc@60).",
    "descriptor": "\nComments: Accepted at The Third Workshop on Figurative Language Processing @ EMNLP 2022\n",
    "authors": [
      "Yuling Gu",
      "Yao Fu",
      "Valentina Pyatkin",
      "Ian Magnusson",
      "Bhavana Dalvi Mishra",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16407"
  },
  {
    "id": "arXiv:2210.16411",
    "title": "Recurrent Convolutional Deep Neural Networks for Modeling Time-Resolved  Wildfire Spread Behavior",
    "abstract": "The increasing incidence and severity of wildfires underscores the necessity\nof accurately predicting their behavior. While high-fidelity models derived\nfrom first principles offer physical accuracy, they are too computationally\nexpensive for use in real-time fire response. Low-fidelity models sacrifice\nsome physical accuracy and generalizability via the integration of empirical\nmeasurements, but enable real-time simulations for operational use in fire\nresponse. Machine learning techniques offer the ability to bridge these\nobjectives by learning first-principles physics while achieving computational\nspeedup. While deep learning approaches have demonstrated the ability to\npredict wildfire propagation over large time periods, time-resolved fire-spread\npredictions are needed for active fire management. In this work, we evaluate\nthe ability of deep learning approaches in accurately modeling the\ntime-resolved dynamics of wildfires. We use an autoregressive process in which\na convolutional recurrent deep learning model makes predictions that propagate\na wildfire over 15 minute increments. We demonstrate the model in application\nto three simulated datasets of increasing complexity, containing both field\nfires with homogeneous fuel distribution as well as real-world topologies\nsampled from the California region of the United States. We show that even\nafter 100 autoregressive predictions representing more than 24 hours of\nsimulated fire spread, the resulting models generate stable and realistic\npropagation dynamics, achieving a Jaccard score between 0.89 and 0.94 when\npredicting the resulting fire scar.",
    "descriptor": "\nComments: 32 pages (including refs and appendix). 10 figures. 2 Tables. 1 Appendix\n",
    "authors": [
      "John Burge",
      "Matthew R. Bonanni",
      "R. Lily Hu",
      "Matthias Ihme"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16411"
  },
  {
    "id": "arXiv:2210.16412",
    "title": "A State-Augmented Approach for Learning Optimal Resource Management  Decisions in Wireless Networks",
    "abstract": "We consider a radio resource management (RRM) problem in a multi-user\nwireless network, where the goal is to optimize a network-wide utility function\nsubject to constraints on the ergodic average performance of users. We propose\na state-augmented parameterization for the RRM policy, where alongside the\ninstantaneous network states, the RRM policy takes as input the set of dual\nvariables corresponding to the constraints. We provide theoretical\njustification for the feasibility and near-optimality of the RRM decisions\ngenerated by the proposed state-augmented algorithm. Focusing on the power\nallocation problem with RRM policies parameterized by a graph neural network\n(GNN) and dual variables sampled from the dual descent dynamics, we numerically\ndemonstrate that the proposed approach achieves a superior trade-off between\nmean, minimum, and 5th percentile rates than baseline methods.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2207.02242\n",
    "authors": [
      "Yi\u011fit Berkay Uslu",
      "Navid NaderiAlizadeh",
      "Mark Eisen",
      "Alejandro Riberio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16412"
  },
  {
    "id": "arXiv:2210.16413",
    "title": "When does mixup promote local linearity in learned representations?",
    "abstract": "Mixup is a regularization technique that artificially produces new samples\nusing convex combinations of original training points. This simple technique\nhas shown strong empirical performance, and has been heavily used as part of\nsemi-supervised learning techniques such as\nmixmatch~\\citep{berthelot2019mixmatch} and interpolation consistent training\n(ICT)~\\citep{verma2019interpolation}. In this paper, we look at Mixup through a\n\\emph{representation learning} lens in a semi-supervised learning setup. In\nparticular, we study the role of Mixup in promoting linearity in the learned\nnetwork representations. Towards this, we study two questions: (1) how does the\nMixup loss that enforces linearity in the \\emph{last} network layer propagate\nthe linearity to the \\emph{earlier} layers?; and (2) how does the enforcement\nof stronger Mixup loss on more than two data points affect the convergence of\ntraining? We empirically investigate these properties of Mixup on vision\ndatasets such as CIFAR-10, CIFAR-100 and SVHN. Our results show that supervised\nMixup training does not make \\emph{all} the network layers linear; in fact the\n\\emph{intermediate layers} become more non-linear during Mixup training\ncompared to a network that is trained \\emph{without} Mixup. However, when Mixup\nis used as an unsupervised loss, we observe that all the network layers become\nmore linear resulting in faster training convergence.",
    "descriptor": "",
    "authors": [
      "Arslan Chaudhry",
      "Aditya Krishna Menon",
      "Andreas Veit",
      "Sadeep Jayasumana",
      "Srikumar Ramalingam",
      "Sanjiv Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16413"
  },
  {
    "id": "arXiv:2210.16422",
    "title": "Toward Unifying Text Segmentation and Long Document Summarization",
    "abstract": "Text segmentation is important for signaling a document's structure. Without\nsegmenting a long document into topically coherent sections, it is difficult\nfor readers to comprehend the text, let alone find important information. The\nproblem is only exacerbated by a lack of segmentation in transcripts of\naudio/video recordings. In this paper, we explore the role that section\nsegmentation plays in extractive summarization of written and spoken documents.\nOur approach learns robust sentence representations by performing summarization\nand segmentation simultaneously, which is further enhanced by an\noptimization-based regularizer to promote selection of diverse summary\nsentences. We conduct experiments on multiple datasets ranging from scientific\narticles to spoken transcripts to evaluate the model's performance. Our\nfindings suggest that the model can not only achieve state-of-the-art\nperformance on publicly available benchmarks, but demonstrate better\ncross-genre transferability when equipped with text segmentation. We perform a\nseries of analyses to quantify the impact of section segmentation on\nsummarizing written and spoken documents of substantial length and complexity.",
    "descriptor": "\nComments: EMNLP 2022 (Long Paper)\n",
    "authors": [
      "Sangwoo Cho",
      "Kaiqiang Song",
      "Xiaoyang Wang",
      "Fei Liu",
      "Dong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16422"
  },
  {
    "id": "arXiv:2210.16423",
    "title": "Transferability-based Chain Motion Mapping from Humans to Humanoids for  Teleoperation",
    "abstract": "Although data-driven motion mapping methods are promising to allow intuitive\nrobot control and teleoperation that generate human-like robot movement, they\nnormally require tedious pair-wise training for each specific human and robot\npair. This paper proposes a transferability-based mapping scheme to allow new\nrobot and human input systems to leverage the mapping of existing trained pairs\nto form a mapping transfer chain, which will reduce the number of new\npair-specific mappings that need to be generated. The first part of the mapping\nschematic is the development of a Synergy Mapping via Dual-Autoencoder (SyDa)\nmethod. This method uses the latent features from two autoencoders to extract\nthe common synergy of the two agents. Secondly, a transferability metric is\ncreated that approximates how well the mapping between a pair of agents will\nperform compared to another pair before creating the motion mapping models.\nThus, it can guide the formation of an optimal mapping chain for the new\nhuman-robot pair. Experiments with human subjects and a Pepper robot\ndemonstrated 1) The SyDa method improves the accuracy and generalizability of\nthe pair mappings, 2) the SyDa method allows for bidirectional mapping that\ndoes not prioritize the direction of mapping motion, and 3) the transferability\nmetric measures how compatible two agents are for accurate teleoperation. The\ncombination of the SyDa method and transferability metric creates generalizable\nand accurate mapping need to create the transfer mapping chain.",
    "descriptor": "",
    "authors": [
      "Matthew Stanley",
      "Yunsik Jung",
      "Michael Bowman",
      "Lingfeng Tao",
      "Xiaoli Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16423"
  },
  {
    "id": "arXiv:2210.16424",
    "title": "Machine Unlearning of Federated Clusters",
    "abstract": "Federated clustering is an unsupervised learning problem that arises in a\nnumber of practical applications, including personalized recommender and\nhealthcare systems. With the adoption of recent laws ensuring the \"right to be\nforgotten\", the problem of machine unlearning for federated clustering methods\nhas become of significant importance. This work proposes the first known\nunlearning mechanism for federated clustering with privacy criteria that\nsupport simple, provable, and efficient data removal at the client and server\nlevel. The gist of our approach is to combine special initialization procedures\nwith quantization methods that allow for secure aggregation of estimated local\ncluster counts at the server unit. As part of our platform, we introduce secure\ncompressed multiset aggregation (SCMA), which is of independent interest for\nsecure sparse model aggregation. In order to simultaneously facilitate low\ncommunication complexity and secret sharing protocols, we integrate\nReed-Solomon encoding with special evaluation points into the new SCMA pipeline\nand derive bounds on the time and communication complexity of different\ncomponents of the scheme. Compared to completely retraining K-means++ locally\nand globally for each removal request, we obtain an average speed-up of roughly\n84x across seven datasets, two of which contain biological and medical\ninformation that is subject to frequent unlearning requests.",
    "descriptor": "",
    "authors": [
      "Chao Pan",
      "Jin Sima",
      "Saurav Prakash",
      "Vishal Rana",
      "Olgica Milenkovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.16424"
  },
  {
    "id": "arXiv:2210.16430",
    "title": "Second Order Finite Volume Scheme for Shallow Water Equations on  Manifolds",
    "abstract": "In this work, we propose a second-order accurate scheme for shallow water\nequations in general covariant coordinates over manifolds. In particular, the\ncovariant parametrization in general covariant coordinates is induced by the\nmetric tensor associated to the manifold. The model is then re-written in a\nhyperbolic form with a tuple of conserved variables composed both of the\nevolving physical quantities and the metric coefficients. This formulation\nallows the numerical scheme to i) automatically compute the curvature of the\nmanifold as long as the physical variables are evolved and ii) numerically\nstudy complex physical domains over simple computational domains.",
    "descriptor": "",
    "authors": [
      "Michele Giuliano Carlino",
      "Elena Gaburro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16430"
  },
  {
    "id": "arXiv:2210.16431",
    "title": "DiMBERT: Learning Vision-Language Grounded Representations with  Disentangled Multimodal-Attention",
    "abstract": "Vision-and-language (V-L) tasks require the system to understand both vision\ncontent and natural language, thus learning fine-grained joint representations\nof vision and language (a.k.a. V-L representations) is of paramount importance.\nRecently, various pre-trained V-L models are proposed to learn V-L\nrepresentations and achieve improved results in many tasks. However, the\nmainstream models process both vision and language inputs with the same set of\nattention matrices. As a result, the generated V-L representations are\nentangled in one common latent space. To tackle this problem, we propose\nDiMBERT (short for Disentangled Multimodal-Attention BERT), which is a novel\nframework that applies separated attention spaces for vision and language, and\nthe representations of multi-modalities can thus be disentangled explicitly. To\nenhance the correlation between vision and language in disentangled spaces, we\nintroduce the visual concepts to DiMBERT which represent visual information in\ntextual format. In this manner, visual concepts help to bridge the gap between\nthe two modalities. We pre-train DiMBERT on a large amount of image-sentence\npairs on two tasks: bidirectional language modeling and sequence-to-sequence\nlanguage modeling. After pre-train, DiMBERT is further fine-tuned for the\ndownstream tasks. Experiments show that DiMBERT sets new state-of-the-art\nperformance on three tasks (over four datasets), including both generation\ntasks (image captioning and visual storytelling) and classification tasks\n(referring expressions). The proposed DiM (short for Disentangled\nMultimodal-Attention) module can be easily incorporated into existing\npre-trained V-L models to boost their performance, up to a 5% increase on the\nrepresentative task. Finally, we conduct a systematic analysis and demonstrate\nthe effectiveness of our DiM and the introduced visual concepts.",
    "descriptor": "\nComments: Published in ACM TKDD2022 (ACM Transactions on Knowledge Discovery from Data)\n",
    "authors": [
      "Fenglin Liu",
      "Xian Wu",
      "Shen Ge",
      "Xuancheng Ren",
      "Wei Fan",
      "Xu Sun",
      "Yuexian Zou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16431"
  },
  {
    "id": "arXiv:2210.16433",
    "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language  Models",
    "abstract": "Fully-parametric language models generally require a huge number of model\nparameters to store the necessary knowledge for solving multiple natural\nlanguage tasks in zero/few-shot settings. In addition, it is hard to adapt to\nthe evolving world knowledge without the costly model re-training. In this\npaper, we develop a novel semi-parametric language model architecture,\nKnowledge-in-Context (KiC), which empowers a parametric text-to-text language\nmodel with a knowledge-rich external memory. Specifically, the external memory\ncontains six different types of knowledge: entity, dictionary, commonsense,\nevent, script, and causality knowledge. For each input instance, the KiC model\nadaptively selects a knowledge type and retrieves the most helpful pieces of\nknowledge. The input instance along with its knowledge augmentation is fed into\na text-to-text model (e.g., T5) to generate the output answer, where both the\ninput and the output are in natural language forms after prompting.\nInterestingly, we find that KiC can be identified as a special\nmixture-of-experts (MoE) model, where the knowledge selector plays the role of\na router that is used to determine the sequence-to-expert assignment in MoE.\nThis key observation inspires us to develop a novel algorithm for training KiC\nwith an instance-adaptive knowledge selector. As a knowledge-rich\nsemi-parametric language model, KiC only needs a much smaller parametric part\nto achieve superior zero-shot performance on unseen tasks. By evaluating on 40+\ndifferent tasks, we show that KiC_Large with 770M parameters easily outperforms\nlarge language models (LMs) that are 4-39x larger by a large margin. We also\ndemonstrate that KiC exhibits emergent abilities at a much smaller model scale\ncompared to the fully-parametric models.",
    "descriptor": "",
    "authors": [
      "Xiaoman Pan",
      "Wenlin Yao",
      "Hongming Zhang",
      "Dian Yu",
      "Dong Yu",
      "Jianshu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16433"
  },
  {
    "id": "arXiv:2210.16435",
    "title": "Scalable Spectral Clustering with Group Fairness Constraints",
    "abstract": "There are synergies of research interests and industrial efforts in modeling\nfairness and correcting algorithmic bias in machine learning. In this paper, we\npresent a scalable algorithm for spectral clustering (SC) with group fairness\nconstraints. Group fairness is also known as statistical parity where in each\ncluster, each protected group is represented with the same proportion as in the\nentirety. While FairSC algorithm (Kleindessner et al., 2019) is able to find\nthe fairer clustering, it is compromised by high costs due to the kernels of\ncomputing nullspaces and the square roots of dense matrices explicitly. We\npresent a new formulation of underlying spectral computation by incorporating\nnullspace projection and Hotelling's deflation such that the resulting\nalgorithm, called s-FairSC, only involves the sparse matrix-vector products and\nis able to fully exploit the sparsity of the fair SC model. The experimental\nresults on the modified stochastic block model demonstrate that s-FairSC is\ncomparable with FairSC in recovering fair clustering. Meanwhile, it is sped up\nby a factor of 12 for moderate model sizes. s-FairSC is further demonstrated to\nbe scalable in the sense that the computational costs of s-FairSC only increase\nmarginally compared to the SC without fairness constraints.",
    "descriptor": "",
    "authors": [
      "Ji Wang",
      "Ding Lu",
      "Zhaojun Bai",
      "Ian Davidson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16435"
  },
  {
    "id": "arXiv:2210.16441",
    "title": "GowFed -- A novel Federated Network Intrusion Detection System",
    "abstract": "Network intrusion detection systems are evolving into intelligent systems\nthat perform data analysis while searching for anomalies in their environment.\nIndeed, the development of deep learning techniques paved the way to build more\ncomplex and effective threat detection models. However, training those models\nmay be computationally infeasible in most Edge or IoT devices. Current\napproaches rely on powerful centralized servers that receive data from all\ntheir parties -- violating basic privacy constraints and substantially\naffecting response times and operational costs due to the huge communication\noverheads. To mitigate these issues, Federated Learning emerged as a promising\napproach, where different agents collaboratively train a shared model, without\nexposing training data to others or requiring a compute-intensive centralized\ninfrastructure. This work presents GowFed, a novel network threat detection\nsystem that combines the usage of Gower Dissimilarity matrices and Federated\naveraging. Different approaches of GowFed have been developed based on state-of\nthe-art knowledge: (1) a vanilla version; and (2) a version instrumented with\nan attention mechanism. Furthermore, each variant has been tested using\nsimulation oriented tools provided by TensorFlow Federated framework. In the\nsame way, a centralized analogous development of the Federated systems is\ncarried out to explore their differences in terms of scalability and\nperformance -- across a set of designed experiments/scenarios. Overall, GowFed\nintends to be the first stepping stone towards the combined usage of Federated\nLearning and Gower Dissimilarity matrices to detect network threats in\nindustrial-level networks.",
    "descriptor": "\nComments: 15 pages, 12 figures, currently under review at Journal of Network and Computer Applications (JNCA). arXiv admin note: text overlap with arXiv:2204.12443\n",
    "authors": [
      "Aitor Belenguer",
      "Jose A. Pascual",
      "Javier Navaridas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.16441"
  },
  {
    "id": "arXiv:2210.16442",
    "title": "Mesh Refinement for Anisotropic Diffusion in Magnetized Plasmas",
    "abstract": "Highly accurate simulation of plasma transport is needed for the successful\ndesign and operation of magnetically confined fusion reactors. Unfortunately,\nthe extreme anisotropy present in magnetized plasmas results in thin boundary\nlayers that are expensive to resolve. This work investigates how mesh\nrefinement strategies might reduce that expense to allow for more efficient\nsimulation. It is first verified that higher order discretization only realizes\nthe proper rate of convergence once the mesh resolves the thin boundary layer,\nmotivating the focusing of refinement on the boundary layer. Three mesh\nrefinement strategies are investigated: one that focuses the refinement across\nthe layer by using rectangular elements with a ratio equal to the boundary\nlayer width, one that allows for exponential growth in mesh spacing away from\nthe layer, and one adaptive strategy utilizing the established Zienkiewicz and\nZhu error estimator. Across 4 two-dimensional test cases with high anisotropy,\nthe adaptive mesh refinement strategy consistently achieves the same accuracy\nas uniform refinement using orders of magnitude less degrees of freedom. In the\ntest case where the magnetic field is aligned with the mesh, the other\nrefinement strategies also show substantial improvement in efficiency. This\nwork also includes a discussion generalizing the results to larger magnetic\nanisotropy ratios and to three-dimensional problems. It is shown that isotropic\nmesh refinement requires degrees of freedom on the order of either the layer\nwidth (2D) or the square of the layer width (3D), whereas anisotropic\nrefinement requires a number on the order of the log of layer width for all\ndimensions. It is also shown that the number of conjugate gradient iterations\nscales as a power of layer width when preconditioned with algebraic multigrid,\nwhereas the number is independent of layer width when preconditioned with ILU.",
    "descriptor": "",
    "authors": [
      "Christopher J. Vogl",
      "Ilon Joseph",
      "Milan Holec"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16442"
  },
  {
    "id": "arXiv:2210.16449",
    "title": "Gender Bias in Computing",
    "abstract": "This paper examines the historical dimension of gender bias in the US\ncomputing workforce. It offers new quantitative data on the computing workforce\nprior to the availability of US Census data in the 1970s. Computer user groups\n(including SHARE, Inc., and the Mark IV software user group) are taken as a\ncross-section of the computing workforce. A novel method of gender analysis is\ndeveloped to estimate women's and men's participation in computing beginning in\nthe 1950s. The data presented here are consistent with well-known NSF\nstatistics that show computer science undergraduate programs enrolling\nincreasing numbers of women students during 1965-1985. These findings challenge\nthe 'making programming masculine' thesis, and serve to correct the\nunrealistically high figures often cited for women's participation in early\ncomputer programming. Gender bias in computing today is traced not to 1960s\nprofessionalization but to cultural changes in the 1980s and beyond.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Thomas J. Misa"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.16449"
  },
  {
    "id": "arXiv:2210.16451",
    "title": "Robust Boosting Forests with Richer Deep Feature Hierarchy",
    "abstract": "We propose a robust variant of boosting forest to the various adversarial\ndefense methods, and apply it to enhance the robustness of the deep neural\nnetwork. We retain the deep network architecture, weights, and middle layer\nfeatures, then install gradient boosting forest to select the features from\neach layer of the deep network, and predict the target. For training each\ndecision tree, we propose a novel conservative and greedy trade-off, with\nconsideration for less misprediction instead of pure gain functions, therefore\nbeing suboptimal and conservative. We actively increase tree depth to remedy\nthe accuracy with splits in more features, being more greedy in growing tree\ndepth. We propose a new task on 3D face model, whose robustness has not been\ncarefully studied, despite the great security and privacy concerns related to\nface analytics. We tried a simple attack method on a pure convolutional neural\nnetwork (CNN) face shape estimator, making it degenerate to only output average\nface shape with invisible perturbation. Our conservative-greedy boosting forest\n(CGBF) on face landmark datasets showed a great improvement over original pure\ndeep learning methods under the adversarial attacks.",
    "descriptor": "",
    "authors": [
      "Jianqiao Wangni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.16451"
  },
  {
    "id": "arXiv:2210.16453",
    "title": "Joint Sub-component Level Segmentation and Classification for Anomaly  Detection within Dual-Energy X-Ray Security Imagery",
    "abstract": "X-ray baggage security screening is in widespread use and crucial to\nmaintaining transport security for threat/anomaly detection tasks. The\nautomatic detection of anomaly, which is concealed within cluttered and complex\nelectronics/electrical items, using 2D X-ray imagery is of primary interest in\nrecent years. We address this task by introducing joint object sub-component\nlevel segmentation and classification strategy using deep Convolution Neural\nNetwork architecture. The performance is evaluated over a dataset of cluttered\nX-ray baggage security imagery, consisting of consumer electrical and\nelectronics items using variants of dual-energy X-ray imagery (pseudo-colour,\nhigh, low, and effective-Z). The proposed joint sub-component level\nsegmentation and classification approach achieve ~99% true positive and ~5%\nfalse positive for anomaly detection task.",
    "descriptor": "",
    "authors": [
      "Neelanjan Bhowmik",
      "Toby P. Breckon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16453"
  },
  {
    "id": "arXiv:2210.16457",
    "title": "Region of Interest Detection in Melanocytic Skin Tumor Whole Slide  Images",
    "abstract": "Automated region of interest detection in histopathological image analysis is\na challenging and important topic with tremendous potential impact on clinical\npractice. The deep-learning methods used in computational pathology help us to\nreduce costs and increase the speed and accuracy of regions of interest\ndetection and cancer diagnosis. In this work, we propose a patch-based region\nof interest detection method for melanocytic skin tumor whole-slide images. We\nwork with a dataset that contains 165 primary melanomas and nevi Hematoxylin\nand Eosin whole-slide images and build a deep-learning method. The proposed\nmethod performs well on a hold-out test data set including five TCGA-SKCM\nslides (accuracy of 93.94\\% in slide classification task and intersection over\nunion rate of 41.27\\% in the region of interest detection task), showing the\noutstanding performance of our model on melanocytic skin tumor. Even though we\ntest the experiments on the skin tumor dataset, our work could also be extended\nto other medical image detection problems, such as various tumors'\nclassification and prediction, to help and benefit the clinical evaluation and\ndiagnosis of different tumors.",
    "descriptor": "\nComments: Accepted to MedNeurIPS 2022\n",
    "authors": [
      "Yi Cui",
      "Yao Li",
      "Jayson R. Miedema",
      "Sherif Farag",
      "J.S. Marron",
      "Nancy E. Thomas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16457"
  },
  {
    "id": "arXiv:2210.16461",
    "title": "Sentiment Classification of Code-Switched Text using Pre-trained  Multilingual Embeddings and Segmentation",
    "abstract": "With increasing globalization and immigration, various studies have estimated\nthat about half of the world population is bilingual. Consequently, individuals\nconcurrently use two or more languages or dialects in casual conversational\nsettings. However, most research is natural language processing is focused on\nmonolingual text. To further the work in code-switched sentiment analysis, we\npropose a multi-step natural language processing algorithm utilizing points of\ncode-switching in mixed text and conduct sentiment analysis around those\nidentified points. The proposed sentiment analysis algorithm uses semantic\nsimilarity derived from large pre-trained multilingual models with a\nhandcrafted set of positive and negative words to determine the polarity of\ncode-switched text. The proposed approach outperforms a comparable baseline\nmodel by 11.2% for accuracy and 11.64% for F1-score on a Spanish-English\ndataset. Theoretically, the proposed algorithm can be expanded for sentiment\nanalysis of multiple languages with limited human expertise.",
    "descriptor": "",
    "authors": [
      "Saurav K. Aryal",
      "Howard Prioleau",
      "Gloria Washington"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16461"
  },
  {
    "id": "arXiv:2210.16463",
    "title": "Augmented Reality and Mixed Reality Measurement Under Different  Environments: A Survey on Head-Mounted Devices",
    "abstract": "Augmented Reality (AR) and Mixed Reality (MR) have been two of the most\nexplosive research topics in the last few years. Head-Mounted Devices (HMDs)\nare essential intermediums for using AR and MR technology, playing an important\nrole in the research progress in these two areas. Behavioral research with\nusers is one way of evaluating the technical progress and effectiveness of\nHMDs. In addition, AR and MR technology is dependent upon virtual interactions\nwith the real environment. Thus, conditions in real environments can be a\nsignificant factor for AR and MR measurements with users. In this paper, we\nsurvey 87 environmental-related HMD papers with measurements from users,\nspanning over 32 years. We provide a thorough review of AR- and MR-related user\nexperiments with HMDs under different environmental factors. Then, we summarize\ntrends in this literature over time using a new classification method with four\nenvironmental factors, the presence or absence of user feedback in behavioral\nexperiments, and ten main categories to subdivide these papers (e.g., domain\nand method of user assessment). We also categorize characteristics of the\nbehavioral experiments, showing similarities and differences among papers.",
    "descriptor": "",
    "authors": [
      "Hung-Jui Guo",
      "Jonathan Z. Bakdash",
      "Laura R. Marusich",
      "Balakrishnan Prabhakaran"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16463"
  },
  {
    "id": "arXiv:2210.16467",
    "title": "ImplantFormer: Vision Transformer based Implant Position Regression  Using Dental CBCT Data",
    "abstract": "Implant prosthesis is the most optimum treatment of dentition defect or\ndentition loss, which usually involves a surgical guide design process to\ndecide the position of implant. However, such design heavily relies on the\nsubjective experiences of dentist. To relieve this problem, in this paper, a\ntransformer based Implant Position Regression Network, ImplantFormer, is\nproposed to automatically predict the implant position based on the oral CBCT\ndata. The 3D CBCT data is firstly transformed into a series of 2D transverse\nplane slice views. ImplantFormer is then proposed to predict the position of\nimplant based on the 2D slices of crown images. Convolutional stem and decoder\nare designed to coarsely extract image feature before the operation of patch\nembedding and integrate multi-levels feature map for robust prediction. The\npredictions of our network at tooth crown area are finally projected back to\nthe positions at tooth root. As both long-range relationship and local features\nare involved, our approach can better represent global information and achieves\nbetter location performance than the state-of-the-art detectors. Experimental\nresults on a dataset of 128 patients, collected from Shenzhen University\nGeneral Hospital, show that our ImplantFormer achieves superior performance\nthan benchmarks.",
    "descriptor": "\nComments: 12 pages, 11 figures\n",
    "authors": [
      "Xinquan Yang",
      "Xuguang Li",
      "Xuechen Li",
      "Peixi Wu",
      "Linlin Shen",
      "Xin Li",
      "Yongqiang Deng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16467"
  },
  {
    "id": "arXiv:2210.16468",
    "title": "Curiosity-Driven Multi-Agent Exploration with Mixed Objectives",
    "abstract": "Intrinsic rewards have been increasingly used to mitigate the sparse reward\nproblem in single-agent reinforcement learning. These intrinsic rewards\nencourage the agent to look for novel experiences, guiding the agent to explore\nthe environment sufficiently despite the lack of extrinsic rewards.\nCuriosity-driven exploration is a simple yet efficient approach that quantifies\nthis novelty as the prediction error of the agent's curiosity module, an\ninternal neural network that is trained to predict the agent's next state given\nits current state and action. We show here, however, that naively using this\ncuriosity-driven approach to guide exploration in sparse reward cooperative\nmulti-agent environments does not consistently lead to improved results.\nStraightforward multi-agent extensions of curiosity-driven exploration take\ninto consideration either individual or collective novelty only and thus, they\ndo not provide a distinct but collaborative intrinsic reward signal that is\nessential for learning in cooperative multi-agent tasks. In this work, we\npropose a curiosity-driven multi-agent exploration method that has the mixed\nobjective of motivating the agents to explore the environment in ways that are\nindividually and collectively novel. First, we develop a two-headed curiosity\nmodule that is trained to predict the corresponding agent's next observation in\nthe first head and the next joint observation in the second head. Second, we\ndesign the intrinsic reward formula to be the sum of the individual and joint\nprediction errors of this curiosity module. We empirically show that the\ncombination of our curiosity module architecture and intrinsic reward\nformulation guides multi-agent exploration more efficiently than baseline\napproaches, thereby providing the best performance boost to MARL algorithms in\ncooperative navigation environments with sparse rewards.",
    "descriptor": "",
    "authors": [
      "Roben Delos Reyes",
      "Kyunghwan Son",
      "Jinhwan Jung",
      "Wan Ju Kang",
      "Yung Yi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.16468"
  },
  {
    "id": "arXiv:2210.16470",
    "title": "Improving Audio Captioning Using Semantic Similarity Metrics",
    "abstract": "Audio captioning quality metrics which are typically borrowed from the\nmachine translation and image captioning areas measure the degree of overlap\nbetween predicted tokens and gold reference tokens. In this work, we consider a\nmetric measuring semantic similarities between predicted and reference captions\ninstead of measuring exact word overlap. We first evaluate its ability to\ncapture similarities among captions corresponding to the same audio file and\ncompare it to other established metrics. We then propose a fine-tuning method\nto directly optimize the metric by backpropagating through a sentence embedding\nextractor and audio captioning network. Such fine-tuning results in an\nimprovement in predicted captions as measured by both traditional metrics and\nthe proposed semantic similarity captioning metric.",
    "descriptor": "",
    "authors": [
      "Rehana Mahfuz",
      "Yinyi Guo",
      "Erik Visser"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.16470"
  },
  {
    "id": "arXiv:2210.16471",
    "title": "Fast Efficient Fixed-Size Memory Pool: No Loops and No Overhead",
    "abstract": "In this paper, we examine a ready-to-use, robust, and computationally fast\nfixed-size memory pool manager with no-loops and no-memory overhead that is\nhighly suited towards time-critical systems such as games. The algorithm\nachieves this by exploiting the unused memory slots for bookkeeping in\ncombination with a trouble-free indexing scheme. We explain how it works in\namalgamation with straightforward step-by-step examples. Furthermore, we\ncompare just how much faster the memory pool manager is when compared with a\nsystem allocator (e.g., malloc) over a range of allocations and sizes.",
    "descriptor": "",
    "authors": [
      "Ben Kenwright"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.16471"
  },
  {
    "id": "arXiv:2210.16472",
    "title": "Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source  Separation",
    "abstract": "There exists an unequivocal distinction between the sound produced by a\nstatic source and that produced by a moving one, especially when the source\nmoves towards or away from the microphone. In this paper, we propose to use\nthis connection between audio and visual dynamics for solving two challenging\ntasks simultaneously, namely: (i) separating audio sources from a mixture using\nvisual cues, and (ii) predicting the 3D visual motion of a sounding source\nusing its separated audio. Towards this end, we present Audio Separator and\nMotion Predictor (ASMP) -- a deep learning framework that leverages the 3D\nstructure of the scene and the motion of sound sources for better audio source\nseparation. At the heart of ASMP is a 2.5D scene graph capturing various\nobjects in the video and their pseudo-3D spatial proximities. This graph is\nconstructed by registering together 2.5D monocular depth predictions from the\n2D video frames and associating the 2.5D scene regions with the outputs of an\nobject detector applied on those frames. The ASMP task is then mathematically\nmodeled as the joint problem of: (i) recursively segmenting the 2.5D scene\ngraph into several sub-graphs, each associated with a constituent sound in the\ninput audio mixture (which is then separated) and (ii) predicting the 3D\nmotions of the corresponding sound sources from the separated audio. To\nempirically evaluate ASMP, we present experiments on two challenging\naudio-visual datasets, viz. Audio Separation in the Wild (ASIW) and Audio\nVisual Event (AVE). Our results demonstrate that ASMP achieves a clear\nimprovement in source separation quality, outperforming prior works on both\ndatasets, while also estimating the direction of motion of the sound sources\nbetter than other methods.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Moitreya Chatterjee",
      "Narendra Ahuja",
      "Anoop Cherian"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16472"
  },
  {
    "id": "arXiv:2210.16476",
    "title": "Pair DETR: Contrastive Learning Speeds Up DETR Training",
    "abstract": "The DETR object detection approach applies the transformer encoder and\ndecoder architecture to detect objects and achieves promising performance. In\nthis paper, we present a simple approach to address the main problem of DETR,\nthe slow convergence, by using representation learning technique. In this\napproach, we detect an object bounding box as a pair of keypoints, the top-left\ncorner and the center, using two decoders. By detecting objects as paired\nkeypoints, the model builds up a joint classification and pair association on\nthe output queries from two decoders. For the pair association we propose\nutilizing contrastive self-supervised learning algorithm without requiring\nspecialized architecture. Experimental results on MS COCO dataset show that\nPair DETR can converge at least 10x faster than original DETR and 1.5x faster\nthan Conditional DETR during training, while having consistently higher Average\nPrecision scores.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2108.06152\n",
    "authors": [
      "Mehdi Iranmanesh",
      "Xiaotong Chen",
      "Kuo-Chin Lien"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16476"
  },
  {
    "id": "arXiv:2210.16477",
    "title": "Adaptive Fuzzy Tracking Control with Global Prescribed-Time Prescribed  Performance for Uncertain Strict-Feedback Nonlinear Systems",
    "abstract": "Adaptive fuzzy control strategies are established to achieve global\nprescribed performance for uncertain strict-feedback nonlinear systems with\nprescribed time. Firstly, a class of prescribed-time prescribed performance\nfunction is designed to quantify the transient and steady performance\nconstraints of the tracking error. Secondly, based on dynamic surface control\nmethods, controllers with or without approximating structures are established\nto guarantee that the tracking error satisfies prescribed transient performance\nand converges into a prescribed bounded set within prescribed time. In\nparticular, the settling time and initial value of the prescribed performance\nfunction are completely independent of initial conditions of the tracking error\nand system parameters, which improves existing results. Moreover, with a novel\nLyapunov-like energy function, not only the differential explosion problem\nfrequently occurring in backstepping techniques is solved, but the drawback of\nthe semi-global boundedness of tracking error induced by dynamic surface\ncontrol can be overcome. The validity and effectiveness of the main results are\nverified by numerical simulations on practical examples.",
    "descriptor": "",
    "authors": [
      "Bing Mao",
      "Xiaoqun Wu",
      "Hui Liu",
      "Jinhu L\u00fc"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16477"
  },
  {
    "id": "arXiv:2210.16478",
    "title": "GPA-Net:No-Reference Point Cloud Quality Assessment with Multi-task  Graph Convolutional Network",
    "abstract": "With the rapid development of 3D vision, point cloud has become an\nincreasingly popular 3D visual media content. Due to the irregular structure,\npoint cloud has posed novel challenges to the related research, such as\ncompression, transmission, rendering and quality assessment. In these latest\nresearches, point cloud quality assessment (PCQA) has attracted wide attention\ndue to its significant role in guiding practical applications, especially in\nmany cases where the reference point cloud is unavailable. However, current\nno-reference metrics which based on prevalent deep neural network have apparent\ndisadvantages. For example, to adapt to the irregular structure of point cloud,\nthey require preprocessing such as voxelization and projection that introduce\nextra distortions, and the applied grid-kernel networks, such as Convolutional\nNeural Networks, fail to extract effective distortion-related features.\nBesides, they rarely consider the various distortion patterns and the\nphilosophy that PCQA should exhibit shifting, scaling, and rotational\ninvariance. In this paper, we propose a novel no-reference PCQA metric named\nthe Graph convolutional PCQA network (GPA-Net). To extract effective features\nfor PCQA, we propose a new graph convolution kernel, i.e., GPAConv, which\nattentively captures the perturbation of structure and texture. Then, we\npropose the multi-task framework consisting of one main task (quality\nregression) and two auxiliary tasks (distortion type and degree predictions).\nFinally, we propose a coordinate normalization module to stabilize the results\nof GPAConv under shift, scale and rotation transformations. Experimental\nresults on two independent databases show that GPA-Net achieves the best\nperformance compared to the state-of-the-art no-reference PCQA metrics, even\nbetter than some full-reference metrics in some cases.",
    "descriptor": "",
    "authors": [
      "Ziyu Shan",
      "Qi Yang",
      "Rui Ye",
      "Yujie Zhang",
      "Yiling Xu",
      "Xiaozhong Xu",
      "Shan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.16478"
  },
  {
    "id": "arXiv:2210.16482",
    "title": "Recursive Reasoning in Minimax Games: A Level $k$ Gradient Play Method",
    "abstract": "Despite the success of generative adversarial networks (GANs) in generating\nvisually appealing images, they are notoriously challenging to train. In order\nto stabilize the learning dynamics in minimax games, we propose a novel\nrecursive reasoning algorithm: Level $k$ Gradient Play (Lv.$k$ GP) algorithm.\nIn contrast to many existing algorithms, our algorithm does not require\nsophisticated heuristics or curvature information. We show that as $k$\nincreases, Lv.$k$ GP converges asymptotically towards an accurate estimation of\nplayers' future strategy. Moreover, we justify that Lv.$\\infty$ GP naturally\ngeneralizes a line of provably convergent game dynamics which rely on\npredictive updates. Furthermore, we provide its local convergence property in\nnonconvex-nonconcave zero-sum games and global convergence in bilinear and\nquadratic games. By combining Lv.$k$ GP with Adam optimizer, our algorithm\nshows a clear advantage in terms of performance and computational overhead\ncompared to other methods. Using a single Nvidia RTX3090 GPU and 30 times fewer\nparameters than BigGAN on CIFAR-10, we achieve an FID of 10.17 for\nunconditional image generation within 30 hours, allowing GAN training on common\ncomputational resources to reach state-of-the-art performance.",
    "descriptor": "\nComments: For the code associated with this paper, see this https URL\n",
    "authors": [
      "Zichu Liu",
      "Lacra Pavel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.16482"
  },
  {
    "id": "arXiv:2210.16484",
    "title": "A Systematic Survey of Molecular Pre-trained Models",
    "abstract": "Obtaining effective molecular representations is at the core of a series of\nimportant chemical tasks ranging from property prediction to drug design. So\nfar, deep learning has achieved remarkable success in learning representations\nfor molecules through automated feature learning in a data-driven fashion.\nHowever, training deep neural networks from scratch often requires sufficient\nlabeled molecules which are expensive to acquire in real-world scenarios. To\nalleviate this issue, inspired by the success of the pretrain-then-finetune\nparadigm in natural language processing, tremendous efforts have been devoted\nto Molecular Pre-trained Models (MPMs), where neural networks are pre-trained\nusing large-scale unlabeled molecular databases and then fine-tuned for diverse\ndownstream tasks. Despite the prosperity, this field is fast-growing and a\nsystematic roadmap is urgently needed for both methodology advancements and\npractical applications in both machine learning and scientific communities. To\nthis end, this paper provides a systematic survey of pre-trained models for\nmolecular representations. Firstly, to motivate MPMs studies, we highlight the\nlimitations of training deep neural networks for molecular representations.\nNext, we systematically review recent advances on this topic from several key\nperspectives including molecular descriptors, encoder architectures,\npre-training strategies, and applications. Finally, we identify several\nchallenges and discuss promising future research directions.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Jun Xia",
      "Yanqiao Zhu",
      "Yuanqi Du",
      "Yue Liu",
      "Stan Z.Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16484"
  },
  {
    "id": "arXiv:2210.16485",
    "title": "IM: An R-Package for Computation of Image Moments and Moment Invariants",
    "abstract": "Moment invariants are well-established and effective shape descriptors for\nimage classification. In this report, we introduce a package for R-language,\nnamed IM, that implements the calculation of moments for images and allows the\nreconstruction of images from moments within an object-oriented framework.\nSeveral types of moments may be computed using the IM library, including\ndiscrete and continuous Chebyshev, Gegenbauer, Legendre, Krawtchouk, dual Hahn,\ngeneralized pseudo-Zernike, Fourier-Mellin, and radial harmonic Fourier\nmoments. In addition, custom bivariate types of moments can be calculated using\ncombinations of two different types of polynomials. A method of polar\ntransformation of pixel coordinates is used to provide an approximate\ninvariance to rotation for moments that are orthogonal over a rectangle. The\ndifferent types of polynomials used to calculate moments are discussed in this\nreport, as well as comparisons of reconstruction and running time. Examples of\nimage classification using image moments are provided.",
    "descriptor": "\nComments: Apr 2014, technical report\n",
    "authors": [
      "Allison Irvine",
      "Tan Dang",
      "M. Murat Dundar",
      "Bartek Rajwa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2210.16485"
  },
  {
    "id": "arXiv:2210.16486",
    "title": "Learning Probabilistic Models from Generator Latent Spaces with Hat EBM",
    "abstract": "This work proposes a method for using any generator network as the foundation\nof an Energy-Based Model (EBM). Our formulation posits that observed images are\nthe sum of unobserved latent variables passed through the generator network and\na residual random variable that spans the gap between the generator output and\nthe image manifold. One can then define an EBM that includes the generator as\npart of its forward pass, which we call the Hat EBM. The model can be trained\nwithout inferring the latent variables of the observed data or calculating the\ngenerator Jacobian determinant. This enables explicit probabilistic modeling of\nthe output distribution of any type of generator network. Experiments show\nstrong performance of the proposed method on (1) unconditional ImageNet\nsynthesis at 128x128 resolution, (2) refining the output of existing\ngenerators, and (3) learning EBMs that incorporate non-probabilistic\ngenerators. Code and pretrained models to reproduce our results are available\nat https://github.com/point0bar1/hat-ebm.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Mitch Hill",
      "Erik Nijkamp",
      "Jonathan Mitchell",
      "Bo Pang",
      "Song-Chun Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16486"
  },
  {
    "id": "arXiv:2210.16489",
    "title": "STPrompt: Semantic-guided and Task-driven prompts for Effective Few-shot  Classification",
    "abstract": "The effectiveness of prompt learning has been demonstrated in different\npre-trained language models. By formulating suitable template and choosing\nrepresentative label mapping, prompt learning can be used as an efficient\nknowledge probe. However, finding suitable prompt in existing methods requires\nmultiple experimental attempts or appropriate vector initialization on\nformulating suitable template and choosing representative label mapping, which\nit is more common in few-shot learning tasks. Motivating by PLM working\nprocess, we try to construct the prompt from task semantic perspective and thus\npropose the STPrompt -Semantic-guided and Task-driven Prompt model.\nSpecifically, two novel prompts generated from the semantic dependency tree\n(Dep-prompt) and task-specific metadata description (Meta-prompt), are firstly\nconstructed in a prompt augmented pool, and the proposed model would\nautomatically select a suitable semantic prompt to motivating the prompt\nlearning process. Our results show that the proposed model achieves the\nstate-of-the-art performance in five different datasets of few-shot text\nclassification tasks, which prove that more semantic and significant prompts\ncould assume as a better knowledge proving tool.",
    "descriptor": "",
    "authors": [
      "Jinta Weng",
      "Yue Hu",
      "Jing Qiu",
      "Heyan Huan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16489"
  },
  {
    "id": "arXiv:2210.16494",
    "title": "Aligning Offline Metrics and Human Judgments of Value of AI-Pair  Programmers",
    "abstract": "Large language models trained on massive amounts of natural language data and\ncode have shown impressive capabilities in automatic code generation scenarios.\nDevelopment and evaluation of these models has largely been driven by offline\nfunctional correctness metrics, which consider a task to be solved if the\ngenerated code passes corresponding unit tests. While functional correctness is\nclearly an important property of a code generation model, we argue that it may\nnot fully capture what programmers value when collaborating with their AI pair\nprogrammers. For example, while a nearly correct suggestion that does not\nconsider edge cases may fail a unit test, it may still provide a substantial\nstarting point or hint to the programmer, thereby reducing total needed effort\nto complete a coding task. To investigate this, we conduct a user study with\n(N=49) experienced programmers, and find that while both correctness and effort\ncorrelate with value, the association is strongest for effort. We argue that\neffort should be considered as an important dimension of evaluation in code\ngeneration scenarios.\nWe also find that functional correctness remains better at identifying the\nhighest-value generations; but participants still saw considerable value in\ncode that failed unit tests. Conversely, similarity-based metrics are very good\nat identifying the lowest-value generations among those that fail unit tests.\nBased on these findings, we propose a simple hybrid metric, which combines\nfunctional correctness and similarity-based metrics to capture different\ndimensions of what programmers might value and show that this hybrid metric\nmore strongly correlates with both value and effort. Our findings emphasize the\nimportance of designing human-centered metrics that capture what programmers\nneed from and value in their AI pair programmers.",
    "descriptor": "",
    "authors": [
      "Victor Dibia",
      "Adam Fourney",
      "Gagan Bansal",
      "Forough Poursabzi-Sangdeh",
      "Han Liu",
      "Saleema Amershi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.16494"
  },
  {
    "id": "arXiv:2210.16495",
    "title": "Two is Better than Many? Binary Classification as an Effective Approach  to Multi-Choice Question Answering",
    "abstract": "We propose a simple refactoring of multi-choice question answering (MCQA)\ntasks as a series of binary classifications. The MCQA task is generally\nperformed by scoring each (question, answer) pair normalized over all the\npairs, and then selecting the answer from the pair that yield the highest\nscore. For n answer choices, this is equivalent to an n-class classification\nsetup where only one class (true answer) is correct. We instead show that\nclassifying (question, true answer) as positive instances and (question, false\nanswer) as negative instances is significantly more effective across various\nmodels and datasets. We show the efficacy of our proposed approach in different\ntasks -- abductive reasoning, commonsense question answering, science question\nanswering, and sentence completion. Our DeBERTa binary classification model\nreaches the top or close to the top performance on public leaderboards for\nthese tasks. The source code of the proposed approach is available at\nhttps://github.com/declare-lab/TEAM.",
    "descriptor": "",
    "authors": [
      "Deepanway Ghosal",
      "Navonil Majumder",
      "Rada Mihalcea",
      "Soujanya Poria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16495"
  },
  {
    "id": "arXiv:2210.16496",
    "title": "Hybridization of filter and wrapper approaches for the dimensionality  reduction and classification of hyperspectral images",
    "abstract": "The high dimensionality of hyperspectral images often imposes a heavy\ncomputational burden for image processing. Therefore, dimensionality reduction\nis often an essential step in order to remove the irrelevant, noisy and\nredundant bands. And consequently, increase the classification accuracy.\nHowever, identification of useful bands from hundreds or even thousands of\nrelated bands is a nontrivial task. This paper aims at identifying a small set\nof bands, for improving computational speed and prediction accuracy. Hence, we\nhave proposed a hybrid algorithm through band selection for dimensionality\nreduction of hyperspectral images. The proposed approach combines mutual\ninformation gain (MIG), Minimum Redundancy Maximum Relevance (mRMR) and Error\nprobability of Fano with Support Vector Machine Bands Elimination (SVM-PF). The\nproposed approach is compared to an effective reproduced filters approach based\non mutual information. Experimental results on HSI AVIRIS 92AV3C have shown\nthat the proposed approach outperforms the reproduced filters.\nKeywords - Hyperspectral images, Classification, band Selection, filter,\nwrapper, mutual information, information gain.",
    "descriptor": "",
    "authors": [
      "Asma Elmaizi",
      "Maria Merzouqi",
      "Elkebir Sarhrouni",
      "Ahmed hammouch",
      "Chafik Nacir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16496"
  },
  {
    "id": "arXiv:2210.16502",
    "title": "The solution set of fuzzy relation equations with addition-min  composition",
    "abstract": "This paper deals with the resolutions of fuzzy relation equations with\naddition-min composition. When the fuzzy relation equations have a solution, we\nfirst propose an algorithm to find all minimal solutions of the fuzzy relation\nequations and also supply an algorithm to find all maximal solutions of the\nfuzzy relation equations, which will be illustrated, respectively, by numeral\nexamples. Then we prove that every solution of the fuzzy relation equations is\nbetween a minimal solution and a maximal one, so that we describe the solution\nset of the fuzzy relation equations completely.",
    "descriptor": "\nComments: 19\n",
    "authors": [
      "Meng Li",
      "Xue-Ping Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16502"
  },
  {
    "id": "arXiv:2210.16504",
    "title": "A pruning method based on the dissimilarity of angle among channels and  filters",
    "abstract": "Convolutional Neural Network (CNN) is more and more widely used in various\nfileds, and its computation and memory-demand are also increasing\nsignificantly. In order to make it applicable to limited conditions such as\nembedded application, network compression comes out. Among them, researchers\npay more attention to network pruning. In this paper, we encode the convolution\nnetwork to obtain the similarity of different encoding nodes, and evaluate the\nconnectivity-power among convolutional kernels on the basis of the similarity.\nThen impose different level of penalty according to different\nconnectivity-power. Meanwhile, we propose Channel Pruning base on the\nDissimilarity of Angle (DACP). Firstly, we train a sparse model by GL penalty,\nand impose an angle dissimilarity constraint on the channels and filters of\nconvolutional network to obtain a more sparse structure. Eventually, the\neffectiveness of our method is demonstrated in the section of experiment. On\nCIFAR-10, we reduce 66.86% FLOPs on VGG-16 with 93.31% accuracy after pruning,\nwhere FLOPs represents the number of floating-point operations per second of\nthe model. Moreover, on ResNet-32, we reduce FLOPs by 58.46%, which makes the\naccuracy after pruning reach 91.76%.",
    "descriptor": "\nComments: Accepted by ICTAI 2022\n",
    "authors": [
      "Jiayi Yao",
      "Ping Li",
      "Xiatao Kang",
      "Yuzhe Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16504"
  },
  {
    "id": "arXiv:2210.16506",
    "title": "Observable Perfect Equilibrium",
    "abstract": "While Nash equilibrium has emerged as the central game-theoretic solution\nconcept, many important games contain several Nash equilibria and we must\ndetermine how to select between them in order to create real strategic agents.\nSeveral Nash equilibrium refinement concepts have been proposed and studied for\nsequential imperfect-information games, the most prominent being trembling-hand\nperfect equilibrium, quasi-perfect equilibrium, and recently one-sided\nquasi-perfect equilibrium. These concepts are robust to certain arbitrarily\nsmall mistakes, and are guaranteed to always exist; however, we argue that\nneither of these is the correct concept for developing strong agents in\nsequential games of imperfect information. We define a new equilibrium\nrefinement concept for extensive-form games called observable perfect\nequilibrium in which the solution is robust over trembles in\npublicly-observable action probabilities (not necessarily over all action\nprobabilities that may not be observable by opposing players). Observable\nperfect equilibrium correctly captures the assumption that the opponent is\nplaying as rationally as possible given mistakes that have been observed (while\nprevious solution concepts do not). We prove that observable perfect\nequilibrium is always guaranteed to exist, and demonstrate that it leads to a\ndifferent solution than the prior extensive-form refinements in no-limit poker.\nWe expect observable perfect equilibrium to be a useful equilibrium refinement\nconcept for modeling many important imperfect-information games of interest in\nartificial intelligence.",
    "descriptor": "",
    "authors": [
      "Sam Ganzfried"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2210.16506"
  },
  {
    "id": "arXiv:2210.16508",
    "title": "Clenshaw Graph Neural Networks",
    "abstract": "Graph Convolutional Networks (GCNs), which use a message-passing paradigm\nwith stacked convolution layers, are foundational methods for learning graph\nrepresentations. Recent GCN models use various residual connection techniques\nto alleviate the model degradation problem such as over-smoothing and gradient\nvanishing. Existing residual connection techniques, however, fail to make\nextensive use of underlying graph structure as in the graph spectral domain,\nwhich is critical for obtaining satisfactory results on heterophilic graphs. In\nthis paper, we introduce ClenshawGCN, a GNN model that employs the Clenshaw\nSummation Algorithm to enhance the expressiveness of the GCN model. ClenshawGCN\nequips the standard GCN model with two straightforward residual modules: the\nadaptive initial residual connection and the negative second-order residual\nconnection. We show that by adding these two residual modules, ClenshawGCN\nimplicitly simulates a polynomial filter under the Chebyshev basis, giving it\nat least as much expressive power as polynomial spectral GNNs. In addition, we\nconduct comprehensive experiments to demonstrate the superiority of our model\nover spatial and spectral GNN models.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Yuhe Guo",
      "Zhewei Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16508"
  },
  {
    "id": "arXiv:2210.16509",
    "title": "Fast Iterative Reconstruction for Multi-spectral CT by a Schmidt  Orthogonal Modification Algorithm (SOMA)",
    "abstract": "Multi-spectral CT (MSCT) is increasingly used in industrial non-destructive\ntesting and medical diagnosis because of its outstanding performance like\nmaterial distinguishability. The process of obtaining MSCT data can be modeled\nas nonlinear equations and the basis material decomposition comes down to the\ninverse problem of the nonlinear equations. For different spectra data,\ngeometric inconsistent parameters cause geometrical inconsistent rays, which\nwill lead to mismatched nonlinear equations. How to solve the mismatched\nnonlinear equations accurately and quickly is a hot issue. This paper proposes\na general iterative method to invert the mismatched nonlinear equations and\ndevelops Schmidt orthogonalization to accelerate convergence. The validity of\nthe proposed method is verified by MSCT basis material decomposition\nexperiments. The results show that the proposed method can decompose the basis\nmaterial images accurately and improve the convergence speed greatly.",
    "descriptor": "",
    "authors": [
      "Huiying Pan",
      "Shusen Zhao",
      "Weibin Zhang",
      "Huitao Zhang",
      "Xing Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.16509"
  },
  {
    "id": "arXiv:2210.16510",
    "title": "Generalized LOAM: LiDAR Odometry Estimation with Trainable Local  Geometric Features",
    "abstract": "This paper presents a LiDAR odometry estimation framework called Generalized\nLOAM. Our proposed method is generalized in that it can seamlessly fuse various\nlocal geometric shapes around points to improve the position estimation\naccuracy compared to the conventional LiDAR odometry and mapping (LOAM) method.\nTo utilize continuous geometric features for LiDAR odometry estimation, we\nincorporate tiny neural networks into a generalized iterative closest point\n(GICP) algorithm. These neural networks improve the data association metric and\nthe matching cost function using local geometric features. Experiments with the\nKITTI benchmark demonstrate that our proposed method reduces relative\ntrajectory errors compared to the other LiDAR odometry estimation methods.",
    "descriptor": "\nComments: 8 pages, 7 figures\n",
    "authors": [
      "Kohei Honda",
      "Kenji Koide",
      "Masashi Yokozuka",
      "Shuji Oishi",
      "Atsuhiko Banno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.16510"
  },
  {
    "id": "arXiv:2210.16511",
    "title": "A Universal Treatment of Architectures and Fusion Rules in Decentralized  Discrete-Event Systems",
    "abstract": "This paper provides a universal treatment of architectures in decentralized\ndiscrete-event systems. Current approaches in discrete-event systems do not\nprovide a direct way to compare existing architectures or proposed potentially\nnovel architectures. Determining whether a new architecture is more general\nthan an existing known architecture relies on producing examples ad hoc and on\nindividual inspiration that puts the conditions for solvability in each\narchitecture into some form that admits comparison. From these research\nefforts, a method has been extracted to yield a universal approach to\ndecentralized discrete-event system architectures and their attendant fusion\nrules. This treatment provides an easy and direct way to compare the fusion\nrules -- and hence to compare the strength or generality of the corresponding\narchitectures.",
    "descriptor": "",
    "authors": [
      "Richard Ean",
      "Karen Rudie"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16511"
  },
  {
    "id": "arXiv:2210.16512",
    "title": "MPC Builder for Autonomous Drive: Automatic Generation of MPCs for  Motion Planning and Control",
    "abstract": "This study presents a new framework for vehicle motion planning and control\nbased on the automatic generation of model predictive controllers (MPC) named\nMPC Builder. In this framework, several components necessary for MPC, such as\nmodels, constraints, and cost functions, are prepared in advance. The MPC\nBuilder then online generates various MPCs according to traffic situations in a\nunified manner. This scheme enabled us to represent various driving tasks with\nminimal design effort. The proposed framework was implemented considering the\ncontinuation/generalized minimum residual (C/GMRES) method optimization solver,\nwhich can reduce computational costs. Finally, numerical experiments on\nmultiple driving scenarios were presented.",
    "descriptor": "\nComments: 6 pages, 9 figures\n",
    "authors": [
      "Kohei Honda",
      "Hiroyuki Okuda",
      "Akira Ito",
      "Tatsuya Suzuki"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16512"
  },
  {
    "id": "arXiv:2210.16519",
    "title": "Security-Preserving Federated Learning via Byzantine-Sensitive Triplet  Distance",
    "abstract": "While being an effective framework of learning a shared model across multiple\nedge devices, federated learning (FL) is generally vulnerable to Byzantine\nattacks from adversarial edge devices. While existing works on FL mitigate such\ncompromised devices by only aggregating a subset of the local models at the\nserver side, they still cannot successfully ignore the outliers due to\nimprecise scoring rule. In this paper, we propose an effective Byzantine-robust\nFL framework, namely dummy contrastive aggregation, by defining a novel scoring\nfunction that sensitively discriminates whether the model has been poisoned or\nnot. Key idea is to extract essential information from every local models along\nwith the previous global model to define a distance measure in a manner similar\nto triplet loss. Numerical results validate the advantage of the proposed\napproach by showing improved performance as compared to the state-of-the-art\nByzantine-resilient aggregation methods, e.g., Krum, Trimmed-mean, and Fang.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Youngjoon Lee",
      "Sangwoo Park",
      "Joonhyuk Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16519"
  },
  {
    "id": "arXiv:2210.16520",
    "title": "Fast-Convergent Federated Learning via Cyclic Aggregation",
    "abstract": "Federated learning (FL) aims at optimizing a shared global model over\nmultiple edge devices without transmitting (private) data to the central\nserver. While it is theoretically well-known that FL yields an optimal model --\ncentrally trained model assuming availability of all the edge device data at\nthe central server -- under mild condition, in practice, it often requires\nmassive amount of iterations until convergence, especially under presence of\nstatistical/computational heterogeneity. This paper utilizes cyclic learning\nrate at the server side to reduce the number of training iterations with\nincreased performance without any additional computational costs for both the\nserver and the edge devices. Numerical results validate that, simply\nplugging-in the proposed cyclic aggregation to the existing FL algorithms\neffectively reduces the number of training iterations with improved\nperformance.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Youngjoon Lee",
      "Sangwoo Park",
      "Joonhyuk Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16520"
  },
  {
    "id": "arXiv:2210.16521",
    "title": "IRS-Assisted RF-powered IoT Networks:System Modeling and Performance  Analysis",
    "abstract": "Emerged as a promising solution for future wireless communication systems,\nintelligent reflecting surface (IRS) is capable of reconfiguring the wireless\npropagation environment by adjusting the phase-shift of a large number of\nreflecting elements. To quantify the gain achieved by IRSs in the radio\nfrequency (RF) powered Internet of Things (IoT) networks, in this work, we\nconsider an IRS-assisted cellular-based RFpowered IoT network, where the\ncellular base stations (BSs) broadcast energy signal to IoT devices for energy\nharvesting (EH) in the charging stage, which is utilized to support the uplink\n(UL) transmissions in the subsequent UL stage. With tools from stochastic\ngeometry, we first derive the distributions of the average signal power and\ninterference power which are then used to obtain the energy coverage\nprobability, UL coverage probability, overall coverage probability, spatial\nthroughput and power efficiency, respectively. With the proposed analytical\nframework, we finally evaluate the effect on network performance of key system\nparameters, such as IRS density, IRS reflecting element number, charging stage\nratio, etc. Compared with the conventional RF-powered IoT network, IRS passive\nbeamforming brings the same level of enhancement in both energy coverage and UL\ncoverage, leading to the unchanged optimal charging stage ratio when maximizing\nspatial throughput.",
    "descriptor": "\nComments: 32 pages, 11 figures, submitted to IEEE Transactions on Communications\n",
    "authors": [
      "Hongguang Sun",
      "Zelun Zhao",
      "Hu Cheng",
      "Jiangbin Lyu",
      "Xijun Wang",
      "Yan Zhang",
      "Tony Q. S. Quek"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.16521"
  },
  {
    "id": "arXiv:2210.16524",
    "title": "Federated clustering with GAN-based data synthesis",
    "abstract": "Federated clustering is an adaptation of centralized clustering in the\nfederated settings, which aims to cluster data based on a global similarity\nmeasure while keeping all data local. The key here is how to construct a global\nsimilarity measure without sharing private data. To handle this, k-FED and\nfederated fuzzy c-means (FFCM) respectively adapted K-means and fuzzy c-means\nto the federated learning settings, which aim to construct $K$ global cluster\ncentroids by running K-means on a set of all local cluster centroids. However,\nthe constructed global cluster centroids may be fragile and be sensitive to\ndifferent non-independent and identically distributed (Non-IID) levels among\nclients. To handle this, we propose a simple but effective federated clustering\nframework with GAN-based data synthesis, which is called synthetic data aided\nfederated clustering (SDA-FC). It outperforms k-FED and FFCM in terms of\neffectiveness and robustness, requires only one communication round, can run\nasynchronously, and can handle device failures. Moreover, although NMI is a far\nmore commonly used metric than Kappa, empirical results indicate that Kappa is\na more reliable one.",
    "descriptor": "",
    "authors": [
      "Jie Yan",
      "Jing Liu",
      "Ji Qi",
      "Zhong-Yuan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16524"
  },
  {
    "id": "arXiv:2210.16530",
    "title": "BIMRL: Brain Inspired Meta Reinforcement Learning",
    "abstract": "Sample efficiency has been a key issue in reinforcement learning (RL). An\nefficient agent must be able to leverage its prior experiences to quickly adapt\nto similar, but new tasks and situations. Meta-RL is one attempt at formalizing\nand addressing this issue. Inspired by recent progress in meta-RL, we introduce\nBIMRL, a novel multi-layer architecture along with a novel brain-inspired\nmemory module that will help agents quickly adapt to new tasks within a few\nepisodes. We also utilize this memory module to design a novel intrinsic reward\nthat will guide the agent's exploration. Our architecture is inspired by\nfindings in cognitive neuroscience and is compatible with the knowledge on\nconnectivity and functionality of different regions in the brain. We\nempirically validate the effectiveness of our proposed method by competing with\nor surpassing the performance of some strong baselines on multiple MiniGrid\nenvironments.",
    "descriptor": "\nComments: Accepted to 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Seyed Roozbeh Razavi Rohani",
      "Saeed Hedayatian",
      "Mahdieh Soleymani Baghshah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.16530"
  },
  {
    "id": "arXiv:2210.16534",
    "title": "Improved Approximation Algorithms for Capacitated Vehicle Routing with  Fixed Capacity",
    "abstract": "The Capacitated Vehicle Routing Problem (CVRP) is one of the most extensively\nstudied problems in combinatorial optimization. According to the property of\nthe demand of customers, we distinguish three variants of CVRP: unit-demand,\nsplittable and unsplittable. We consider $k$-CVRP in general metrics and\ngeneral graphs, where $k$ is the capacity of the vehicle and all the three\nversions are APX-hard for each fixed $k\\geq 3$.\nIn this paper, we give a $(5/2-\\Theta(\\sqrt{1/k}))$-approximation algorithm\nfor splittable and unit-demand $k$-CVRP and a\n$(5/2+\\ln2-\\Theta(\\sqrt{1/k}))$-approximation algorithm for unsplittable\n$k$-CVRP (assume the approximation ratio for metric TSP is $\\alpha=3/2$). Thus,\nour approximation ratio is better than previous results for sufficient large\n$k$, say $k\\leq 1.7\\times 10^7$.\nFor small $k$, we design independent algorithms by using more techniques to\nget further improvements. For splittable and unit-demand cases, we improve the\nratio from $1.934$ to $1.500$ for $k=3$, and from $1.750$ to $1.667$ for $k=4$.\nFor the unsplittable case, we improve the ratio from $2.693$ to $1.500$ for\n$k=3$, from $2.443$ to $1.750$ for $k=4$, and from $2.893$ to $2.157$ for\n$k=5$.",
    "descriptor": "",
    "authors": [
      "Jingyang Zhao",
      "Mingyu Xiao"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.16534"
  },
  {
    "id": "arXiv:2210.16535",
    "title": "Causal Discovery of Dynamic Models for Predicting Human Spatial  Interactions",
    "abstract": "Exploiting robots for activities in human-shared environments, whether\nwarehouses, shopping centres or hospitals, calls for such robots to understand\nthe underlying physical interactions between nearby agents and objects. In\nparticular, modelling cause-and-effect relations between the latter can help to\npredict unobserved human behaviours and anticipate the outcome of specific\nrobot interventions. In this paper, we propose an application of causal\ndiscovery methods to model human-robot spatial interactions, trying to\nunderstand human behaviours from real-world sensor data in two possible\nscenarios: humans interacting with the environment, and humans interacting with\nobstacles. New methods and practical solutions are discussed to exploit, for\nthe first time, a state-of-the-art causal discovery algorithm in some\nchallenging human environments, with potential application in many service\nrobotics scenarios. To demonstrate the utility of the causal models obtained\nfrom real-world datasets, we present a comparison between causal and non-causal\nprediction approaches. Our results show that the causal model correctly\ncaptures the underlying interactions of the considered scenarios and improves\nits prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Luca Castri",
      "Sariah Mghames",
      "Marc Hanheide",
      "Nicola Bellotto"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16535"
  },
  {
    "id": "arXiv:2210.16536",
    "title": "Differentiable Data Augmentation for Contrastive Sentence Representation  Learning",
    "abstract": "Fine-tuning a pre-trained language model via the contrastive learning\nframework with a large amount of unlabeled sentences or labeled sentence pairs\nis a common way to obtain high-quality sentence representations. Although the\ncontrastive learning framework has shown its superiority on sentence\nrepresentation learning over previous methods, the potential of such a\nframework is under-explored so far due to the simple method it used to\nconstruct positive pairs. Motivated by this, we propose a method that makes\nhard positives from the original training examples. A pivotal ingredient of our\napproach is the use of prefix that is attached to a pre-trained language model,\nwhich allows for differentiable data augmentation during contrastive learning.\nOur method can be summarized in two steps: supervised prefix-tuning followed by\njoint contrastive fine-tuning with unlabeled or labeled examples. Our\nexperiments confirm the effectiveness of our data augmentation approach. The\nproposed method yields significant improvements over existing methods under\nboth semi-supervised and supervised settings. Our experiments under a low\nlabeled data setting also show that our method is more label-efficient than the\nstate-of-the-art contrastive learning methods.",
    "descriptor": "\nComments: Accepted to EMNLP 2022. The code and pre-trained models are available at this https URL\n",
    "authors": [
      "Tianduo Wang",
      "Wei Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16536"
  },
  {
    "id": "arXiv:2210.16537",
    "title": "Phonemic Representation and Transcription for Speech to Text  Applications for Under-resourced Indigenous African Languages: The Case of  Kiswahili",
    "abstract": "Building automatic speech recognition (ASR) systems is a challenging task,\nespecially for under-resourced languages that need to construct corpora nearly\nfrom scratch and lack sufficient training data. It has emerged that several\nAfrican indigenous languages, including Kiswahili, are technologically\nunder-resourced. ASR systems are crucial, particularly for the hearing-impaired\npersons who can benefit from having transcripts in their native languages.\nHowever, the absence of transcribed speech datasets has complicated efforts to\ndevelop ASR models for these indigenous languages. This paper explores the\ntranscription process and the development of a Kiswahili speech corpus, which\nincludes both read-out texts and spontaneous speech data from native Kiswahili\nspeakers. The study also discusses the vowels and consonants in Kiswahili and\nprovides an updated Kiswahili phoneme dictionary for the ASR model that was\ncreated using the CMU Sphinx speech recognition toolbox, an open-source speech\nrecognition toolkit. The ASR model was trained using an extended phonetic set\nthat yielded a WER and SER of 18.87% and 49.5%, respectively, an improved\nperformance than previous similar research for under-resourced languages.",
    "descriptor": "",
    "authors": [
      "Ebbie Awino",
      "Lilian Wanzare",
      "Lawrence Muchemi",
      "Barack Wanjawa",
      "Edward Ombui",
      "Florence Indede",
      "Owen McOnyango",
      "Benard Okal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16537"
  },
  {
    "id": "arXiv:2210.16539",
    "title": "Exploiting prompt learning with pre-trained language models for  Alzheimer's Disease detection",
    "abstract": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care and to delay further progression. Speech based automatic AD\nscreening systems provide a non-intrusive and more scalable alternative to\nother clinical screening techniques. Textual embedding features produced by\npre-trained language models (PLMs) such as BERT are widely used in such\nsystems. However, PLM domain fine-tuning is commonly based on the masked word\nor sentence prediction costs that are inconsistent with the back-end AD\ndetection task. To this end, this paper investigates the use of prompt-based\nfine-tuning of PLMs that consistently uses AD classification errors as the\ntraining objective function. Disfluency features based on hesitation or pause\nfiller token frequencies are further incorporated into prompt phrases during\nPLM fine-tuning. The exploit of the complementarity between BERT or RoBERTa\nbased PLMs that are either prompt learning fine-tuned, or optimized using\nconventional masked word or sentence prediction costs, decision voting based\nsystem combination between them is further applied. Mean, standard deviation\nand the maximum among accuracy scores over 15 experiment runs are adopted as\nperformance measurements for the AD detection system. Mean detection accuracy\nof 84.20% (with std 2.09%, best 87.5%) and 82.64% (with std 4.0%, best 89.58%)\nwere obtained using manual and ASR speech transcripts respectively on the\nADReSS20 test set consisting of 48 elderly speakers.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Yi Wang",
      "Jiajun Deng",
      "Tianzi Wang",
      "Bo Zheng",
      "Shoukang Hu",
      "Xunying Liu",
      "Helen Meng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16539"
  },
  {
    "id": "arXiv:2210.16541",
    "title": "Entity-centered Cross-document Relation Extraction",
    "abstract": "Relation Extraction (RE) is a fundamental task of information extraction,\nwhich has attracted a large amount of research attention. Previous studies\nfocus on extracting the relations within a sentence or document, while\ncurrently researchers begin to explore cross-document RE. However, current\ncross-document RE methods directly utilize text snippets surrounding target\nentities in multiple given documents, which brings considerable noisy and\nnon-relevant sentences. Moreover, they utilize all the text paths in a document\nbag in a coarse-grained way, without considering the connections between these\ntext paths.In this paper, we aim to address both of these shortages and push\nthe state-of-the-art for cross-document RE. First, we focus on input\nconstruction for our RE model and propose an entity-based document-context\nfilter to retain useful information in the given documents by using the bridge\nentities in the text paths. Second, we propose a cross-document RE model based\non cross-path entity relation attention, which allow the entity relations\nacross text paths to interact with each other. We compare our cross-document RE\nmethod with the state-of-the-art methods in the dataset CodRED. Our method\noutperforms them by at least 10% in F1, thus demonstrating its effectiveness.",
    "descriptor": "\nComments: This paper was accepted by EMNLP 2022 conference\n",
    "authors": [
      "Fengqi Wang",
      "Fei Li",
      "Hao Fei",
      "Jingye Li",
      "Shengqiong Wu",
      "Fangfang Su",
      "Wenxuan Shi",
      "Donghong Ji",
      "Bo Cai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16541"
  },
  {
    "id": "arXiv:2210.16544",
    "title": "Better Lightweight Network for Free: Codeword Mimic Learning for Massive  MIMO CSI feedback",
    "abstract": "The channel state information (CSI) needs to be fed back from the user\nequipment (UE) to the base station (BS) in frequency division duplexing (FDD)\nmultiple-input multiple-output (MIMO) system. Recently, neural networks are\nwidely applied to CSI compressed feedback since the original overhead is too\nlarge for the massive MIMO system. Notably, lightweight feedback networks\nattract special attention due to their practicality of deployment. However, the\nfeedback accuracy is likely to be harmed by the network compression. In this\npaper, a cost free distillation technique named codeword mimic (CM) is proposed\nto train better feedback networks with the practical lightweight encoder. A\nmimic-explore training strategy with a special distillation scheduler is\ndesigned to enhance the CM learning. Experiments show that the proposed CM\nlearning outperforms the previous state-of-the-art feedback distillation\nmethod, boosting the performance of the lightweight feedback network without\nany extra inference cost.",
    "descriptor": "\nComments: 4 pages, 4 figures, 3 tables. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice\n",
    "authors": [
      "Zhilin Lu",
      "Xudong Zhang",
      "Rui Zeng",
      "Jintao Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16544"
  },
  {
    "id": "arXiv:2210.16549",
    "title": "Liminal Design: A Conceptual Framework and Three-Step Approach for  Developing Technology that Delivers Transcendence and Deeper Experiences",
    "abstract": "As ubiquitous technology is increasingly mediating our relationships with the\nworld and others, we argue that the sublime is struggling to find room in\nproduct design primarily aimed at commercial and transactional goals such as\nspeed and efficiency. We suggest a new category of products to promote deeper\nand more meaningful experiences, specifically those offering liminality,\ntranscendence, and personal transformation. This paper introduces a conceptual\nframework and related three-step design approach that looks at narrative\nparticipation in design through abstractions to promote, hold and deepen more\ncomplex emotions. We explore implications from a theoretical point of view and\nsuggest some liminal product design ideas as examples of how the model might be\napplied in practice.",
    "descriptor": "\nComments: 35 pages, 3 figures\n",
    "authors": [
      "Johan Liedgren",
      "Pieter Desmet",
      "Andrea Gaggioli"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16549"
  },
  {
    "id": "arXiv:2210.16554",
    "title": "End-to-end Spoken Language Understanding with Tree-constrained Pointer  Generator",
    "abstract": "End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.",
    "descriptor": "\nComments: 5 pages, submitted to ICASSP 2023\n",
    "authors": [
      "Guangzhi Sun",
      "Chao Zhang",
      "Philip C. Woodland"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16554"
  },
  {
    "id": "arXiv:2210.16556",
    "title": "MinUn: Accurate ML Inference on Microcontrollers",
    "abstract": "Running machine learning inference on tiny devices, known as TinyML, is an\nemerging research area. This task requires generating inference code that uses\nmemory frugally, a task that standard ML frameworks are ill-suited for. A\ndeployment framework for TinyML must be a) parametric in the number\nrepresentation to take advantage of the emerging representations like posits,\nb) carefully assign high-precision to a few tensors so that most tensors can be\nkept in low-precision while still maintaining model accuracy, and c) avoid\nmemory fragmentation. We describe MinUn, the first TinyML framework that\nholistically addresses these issues to generate efficient code for ARM\nmicrocontrollers (e.g., Arduino Uno, Due and STM32H747) that outperforms the\nprior TinyML frameworks.",
    "descriptor": "",
    "authors": [
      "Shikhar Jaiswal",
      "Rahul Kiran Kranti Goli",
      "Aayan Kumar",
      "Vivek Seshadri",
      "Rahul Sharma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.16556"
  },
  {
    "id": "arXiv:2210.16557",
    "title": "Towards Attribute-Entangled Controllable Text Generation: A Pilot Study  of Blessing Generation",
    "abstract": "Controllable Text Generation (CTG) has obtained great success due to its\nfine-grained generation ability obtained by focusing on multiple attributes.\nHowever, most existing CTG researches overlook how to utilize the attribute\nentanglement to enhance the diversity of the controlled generated texts. Facing\nthis dilemma, we focus on a novel CTG scenario, i.e., blessing generation which\nis challenging because high-quality blessing texts require CTG models to\ncomprehensively consider the entanglement between multiple attributes (e.g.,\nobjects and occasions). To promote the research on blessing generation, we\npresent EBleT, a large-scale Entangled Blessing Text dataset containing 293K\nEnglish sentences annotated with multiple attributes. Furthermore, we propose\nnovel evaluation metrics to measure the quality of the blessing texts generated\nby the baseline models we designed. Our study opens a new research direction\nfor controllable text generation and enables the development of\nattribute-entangled CTG models. Our dataset and source codes are available at\n\\url{https://github.com/huangshulin123/Blessing-Generation}.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 GEM Workshop\n",
    "authors": [
      "Shulin Huang",
      "Shirong Ma",
      "Yinghui Li",
      "Yangning Li",
      "Shiyang Lin",
      "Hai-Tao Zheng",
      "Ying Shen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16557"
  },
  {
    "id": "arXiv:2210.16559",
    "title": "Few-shot Image Generation via Adaptation-Aware Kernel Modulation",
    "abstract": "Few-shot image generation (FSIG) aims to learn to generate new and diverse\nsamples given an extremely limited number of samples from a domain, e.g., 10\ntraining samples. Recent work has addressed the problem using transfer learning\napproach, leveraging a GAN pretrained on a large-scale source domain dataset\nand adapting that model to the target domain based on very limited target\ndomain samples. Central to recent FSIG methods are knowledge preserving\ncriteria, which aim to select a subset of source model's knowledge to be\npreserved into the adapted model. However, a major limitation of existing\nmethods is that their knowledge preserving criteria consider only source\ndomain/source task, and they fail to consider target domain/adaptation task in\nselecting source model's knowledge, casting doubt on their suitability for\nsetups of different proximity between source and target domain. Our work makes\ntwo contributions. As our first contribution, we re-visit recent FSIG works and\ntheir experiments. Our important finding is that, under setups which assumption\nof close proximity between source and target domains is relaxed, existing\nstate-of-the-art (SOTA) methods which consider only source domain/source task\nin knowledge preserving perform no better than a baseline fine-tuning method.\nTo address the limitation of existing methods, as our second contribution, we\npropose Adaptation-Aware kernel Modulation (AdAM) to address general FSIG of\ndifferent source-target domain proximity. Extensive experimental results show\nthat the proposed method consistently achieves SOTA performance across\nsource/target domains of different proximity, including challenging setups when\nsource and target domains are more apart. Project Page:\nhttps://yunqing-me.github.io/AdAM/",
    "descriptor": "",
    "authors": [
      "Yunqing Zhao",
      "Keshigeyan Chandrasegaran",
      "Milad Abdollahzadeh",
      "Ngai-Man Cheung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.16559"
  },
  {
    "id": "arXiv:2210.16561",
    "title": "iSmallNet: Densely Nested Network with Label Decoupling for Infrared  Small Target Detection",
    "abstract": "Small targets are often submerged in cluttered backgrounds of infrared\nimages. Conventional detectors tend to generate false alarms, while CNN-based\ndetectors lose small targets in deep layers. To this end, we propose iSmallNet,\na multi-stream densely nested network with label decoupling for infrared small\nobject detection. On the one hand, to fully exploit the shape information of\nsmall targets, we decouple the original labeled ground-truth (GT) map into an\ninterior map and a boundary one. The GT map, in collaboration with the two\nadditional maps, tackles the unbalanced distribution of small object\nboundaries. On the other hand, two key modules are delicately designed and\nincorporated into the proposed network to boost the overall performance. First,\nto maintain small targets in deep layers, we develop a multi-scale nested\ninteraction module to explore a wide range of context information. Second, we\ndevelop an interior-boundary fusion module to integrate multi-granularity\ninformation. Experiments on NUAA-SIRST and NUDT-SIRST clearly show the\nsuperiority of iSmallNet over 11 state-of-the-art detectors.",
    "descriptor": "",
    "authors": [
      "Zhiheng Hu",
      "Yongzhen Wang",
      "Peng Li",
      "Jie Qin",
      "Haoran Xie",
      "Mingqiang Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16561"
  },
  {
    "id": "arXiv:2210.16565",
    "title": "The isotropy group of the matrix multiplication tensor",
    "abstract": "By an {\\em isotropy group} of a tensor $t\\in V_1 \\otimes V_2\\otimes\nV_3=\\widetilde V$ we mean the group of all invertible linear transformations of\n$\\widetilde V$ that leave $t$ invariant and are compatible (in an obvious\nsense) with the structure of tensor product on~$\\widetilde V$. We consider the\ncase where $t$ is the structure tensor of multiplication map of rectangular\nmatrices. The isotropy group of this tensor was studied in 1970s by de Groote,\nStrassen, and Brockett-Dobkin. In the present work we enlarge, make more\nprecise, expose in the language of group actions on tensor spaces, and endow\nwith proofs the results previously known. This is necessary for studying the\nalgorithms of fast matrix multiplication admitting symmetries. The latter seems\nto be a promising new way for constructing fast algorithms.",
    "descriptor": "\nComments: 14 pages. arXiv admin note: text overlap with arXiv:1508.01110\n",
    "authors": [
      "V.P.Burichenko"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Representation Theory (math.RT)"
    ],
    "url": "https://arxiv.org/abs/2210.16565"
  },
  {
    "id": "arXiv:2210.16567",
    "title": "DeFIX: Detecting and Fixing Failure Scenarios with Reinforcement  Learning in Imitation Learning Based Autonomous Driving",
    "abstract": "Safely navigating through an urban environment without violating any traffic\nrules is a crucial performance target for reliable autonomous driving. In this\npaper, we present a Reinforcement Learning (RL) based methodology to DEtect and\nFIX (DeFIX) failures of an Imitation Learning (IL) agent by extracting\ninfraction spots and re-constructing mini-scenarios on these infraction areas\nto train an RL agent for fixing the shortcomings of the IL approach. DeFIX is a\ncontinuous learning framework, where extraction of failure scenarios and\ntraining of RL agents are executed in an infinite loop. After each new policy\nis trained and added to the library of policies, a policy classifier method\neffectively decides on which policy to activate at each step during the\nevaluation. It is demonstrated that even with only one RL agent trained on\nfailure scenario of an IL agent, DeFIX method is either competitive or does\noutperform state-of-the-art IL and RL based autonomous urban driving\nbenchmarks. We trained and validated our approach on the most challenging map\n(Town05) of CARLA simulator which involves complex, realistic, and adversarial\ndriving scenarios. The source code is publicly available at\nhttps://github.com/data-and-decision-lab/DeFIX",
    "descriptor": "\nComments: 6 pages, 4 figures, 2 tables, published in IEEE International Conference on Intelligent Transportation Systems (ITSC), October 12, 2022, Macau, China\n",
    "authors": [
      "Resul Dagdanov",
      "Feyza Eksen",
      "Halil Durmus",
      "Ferhat Yurdakul",
      "Nazim Kemal Ure"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16567"
  },
  {
    "id": "arXiv:2210.16569",
    "title": "Linear Coding for Gaussian Two-Way Channels",
    "abstract": "We consider linear coding for Gaussian two-way channels (GTWCs), in which\neach user generates the transmit symbols by linearly encoding both its message\nand the past received symbols (i.e., the feedback information) from the other\nuser. In Gaussian one-way channels (GOWCs), Butman has proposed a\nwell-developed model for linear encoding that encapsulates feedback information\ninto transmit signals. However, such a model for GTWCs has not been well\nstudied since the coupling of the encoding processes at the users in GTWCs\nrenders the encoding design non-trivial and challenging. In this paper, we aim\nto fill this gap in the literature by extending the existing signal models in\nGOWCs to GTWCs. With our developed signal model for GTWCs, we formulate an\noptimization problem to jointly design the encoding/decoding schemes for both\nthe users, aiming to minimize the weighted sum of their transmit powers under\nsignal-to-noise ratio constraints. First, we derive an optimal form of the\nlinear decoding schemes under any arbitrary encoding schemes employed at the\nusers. Further, we provide new insights on the encoding design for GTWCs. In\nparticular, we show that it is optimal that one of the users (i) does not\ntransmit the feedback information to the other user at the last channel use,\nand (ii) transmits its message only over the last channel use. With these\nsolution behaviors, we further simplify the problem and solve it via an\niterative two-way optimization scheme. We numerically demonstrate that our\nproposed scheme for GTWCs achieves a better performance in terms of the\ntransmit power compared to the existing counterparts, such as the non-feedback\nscheme and one-way optimization scheme.",
    "descriptor": "\nComments: Accepted for publication in 58th Annual Allerton Conference on Communication, Control, and Computing\n",
    "authors": [
      "Junghoon Kim",
      "Seyyedali Hosseinalipour",
      "Taejoon Kim",
      "David J. Love",
      "Christopher G. Brinton"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16569"
  },
  {
    "id": "arXiv:2210.16572",
    "title": "SearchTrack: Multiple Object Tracking with Object-Customized Search and  Motion-Aware Features",
    "abstract": "The paper presents a new method, SearchTrack, for multiple object tracking\nand segmentation (MOTS). To address the association problem between detected\nobjects, SearchTrack proposes object-customized search and motion-aware\nfeatures. By maintaining a Kalman filter for each object, we encode the\npredicted motion into the motion-aware feature, which includes both motion and\nappearance cues. For each object, a customized fully convolutional search\nengine is created by SearchTrack by learning a set of weights for dynamic\nconvolutions specific to the object. Experiments demonstrate that our\nSearchTrack method outperforms competitive methods on both MOTS and MOT tasks,\nparticularly in terms of association accuracy. Our method achieves 71.5 HOTA\n(car) and 57.6 HOTA (pedestrian) on the KITTI MOTS and 53.4 HOTA on MOT17. In\nterms of association accuracy, our method achieves state-of-the-art performance\namong 2D online methods on the KITTI MOTS. Our code is available at\nhttps://github.com/qa276390/SearchTrack.",
    "descriptor": "\nComments: BMVC 2022. Code: this https URL\n",
    "authors": [
      "Zhong-Min Tsai",
      "Yu-Ju Tsai",
      "Chien-Yao Wang",
      "Hong-Yuan Liao",
      "Youn-Long Lin",
      "Yung-Yu Chuang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16572"
  },
  {
    "id": "arXiv:2210.16574",
    "title": "Boosting Monocular 3D Object Detection with Object-Centric Auxiliary  Depth Supervision",
    "abstract": "Recent advances in monocular 3D detection leverage a depth estimation network\nexplicitly as an intermediate stage of the 3D detection network. Depth map\napproaches yield more accurate depth to objects than other methods thanks to\nthe depth estimation network trained on a large-scale dataset. However, depth\nmap approaches can be limited by the accuracy of the depth map, and\nsequentially using two separated networks for depth estimation and 3D detection\nsignificantly increases computation cost and inference time. In this work, we\npropose a method to boost the RGB image-based 3D detector by jointly training\nthe detection network with a depth prediction loss analogous to the depth\nestimation task. In this way, our 3D detection network can be supervised by\nmore depth supervision from raw LiDAR points, which does not require any human\nannotation cost, to estimate accurate depth without explicitly predicting the\ndepth map. Our novel object-centric depth prediction loss focuses on depth\naround foreground objects, which is important for 3D object detection, to\nleverage pixel-wise depth supervision in an object-centric manner. Our depth\nregression model is further trained to predict the uncertainty of depth to\nrepresent the 3D confidence of objects. To effectively train the 3D detector\nwith raw LiDAR points and to enable end-to-end training, we revisit the\nregression target of 3D objects and design a network architecture. Extensive\nexperiments on KITTI and nuScenes benchmarks show that our method can\nsignificantly boost the monocular image-based 3D detector to outperform depth\nmap approaches while maintaining the real-time inference speed.",
    "descriptor": "\nComments: Accepted by IEEE Transaction on Intelligent Transportation System (T-ITS)\n",
    "authors": [
      "Youngseok Kim",
      "Sanmin Kim",
      "Sangmin Sim",
      "Jun Won Choi",
      "Dongsuk Kum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16574"
  },
  {
    "id": "arXiv:2210.16575",
    "title": "Self-Improving Safety Performance of Reinforcement Learning Based  Driving with Black-Box Verification Algorithms",
    "abstract": "In this work, we propose a self-improving artificial intelligence system for\nenhancing the safety performance of reinforcement learning (RL) based\nautonomous driving (AD) agents based on black-box verification methods. RL\nmethods have enjoyed popularity among AD applications in recent years. That\nbeing said, existing RL algorithms' performance strongly depends on the\ndiversity of training scenarios. Lack of safety-critical scenarios in the\ntraining phase might lead to poor generalization performance in real-world\ndriving applications. We propose a novel framework, where the weaknesses of the\ntraining set are explored via black-box verification methods. After the\ndiscovery of AD failure scenarios, the training of the RL agent is re-initiated\nto improve the performance of the previously unsafe scenarios. Simulation\nresults show that the proposed approach efficiently discovers such safety\nfailures in RL-based adaptive cruise control (ACC) applications and\nsignificantly reduces the number of vehicle collisions through iterative\napplications of our method.",
    "descriptor": "\nComments: 7 pages, 7 figures, 2 tables, under review in IEEE International Conference on Robotics and Automation (ICRA), June 2, 2023, London, UK\n",
    "authors": [
      "Resul Dagdanov",
      "Halil Durmus",
      "Nazim Kemal Ure"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.16575"
  },
  {
    "id": "arXiv:2210.16579",
    "title": "INR-V: A Continuous Representation Space for Video-based Generative  Tasks",
    "abstract": "Generating videos is a complex task that is accomplished by generating a set\nof temporally coherent images frame-by-frame. This limits the expressivity of\nvideos to only image-based operations on the individual video frames needing\nnetwork designs to obtain temporally coherent trajectories in the underlying\nimage space. We propose INR-V, a video representation network that learns a\ncontinuous space for video-based generative tasks. INR-V parameterizes videos\nusing implicit neural representations (INRs), a multi-layered perceptron that\npredicts an RGB value for each input pixel location of the video. The INR is\npredicted using a meta-network which is a hypernetwork trained on neural\nrepresentations of multiple video instances. Later, the meta-network can be\nsampled to generate diverse novel videos enabling many downstream video-based\ngenerative tasks. Interestingly, we find that conditional regularization and\nprogressive weight initialization play a crucial role in obtaining INR-V. The\nrepresentation space learned by INR-V is more expressive than an image space\nshowcasing many interesting properties not possible with the existing works.\nFor instance, INR-V can smoothly interpolate intermediate videos between known\nvideo instances (such as intermediate identities, expressions, and poses in\nface videos). It can also in-paint missing portions in videos to recover\ntemporally coherent full videos. In this work, we evaluate the space learned by\nINR-V on diverse generative tasks such as video interpolation, novel video\ngeneration, video inversion, and video inpainting against the existing\nbaselines. INR-V significantly outperforms the baselines on several of these\ndemonstrated tasks, clearly showcasing the potential of the proposed\nrepresentation space.",
    "descriptor": "\nComments: Published in Transactions on Machine Learning Research (10/2022); this https URL\n",
    "authors": [
      "Bipasha Sen",
      "Aditya Agarwal",
      "Vinay P Namboodiri",
      "C. V. Jawahar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16579"
  },
  {
    "id": "arXiv:2210.16580",
    "title": "GPC: A Pattern Calculus for Property Graphs",
    "abstract": "The development of practical query languages for graph databases runs well\nahead of the underlying theory. The ISO committee in charge of database query\nlanguages is currently developing a new standard called Graph Query Language\n(GQL) as well as an extension of the SQL Standard for querying property graphs\nrepresented by a relational schema, called SQL/PGQ. The main component of both\nis the pattern matching facility, which is shared by the two standards. In many\naspects, it goes well beyond RPQs, CRPQs, and similar queries on which the\nresearch community has focused for years. Our main contribution is to distill\nthe lengthy standard specification into a simple Graph Pattern Calculus (GPC)\nthat reflects all the key pattern matching features of GQL and SQL/PGQ, and at\nthe same time lends itself to rigorous theoretical investigation. We describe\nthe syntax and semantics of GPC, along with the typing rules that ensure its\nexpressions are well-defined, and state some basic properties of the language.\nWith this paper we provide the community a tool to embark on a study of query\nlanguages that will soon be widely adopted by industry.",
    "descriptor": "",
    "authors": [
      "Nadime Francis",
      "Am\u00e9lie Gheerbrant",
      "Paolo Guagliardo",
      "Leonid Libkin",
      "Victor Marsault",
      "Wim Martens",
      "Filip Murlak",
      "Liat Peterfreund",
      "Alexandra Rogova",
      "Domagoj Vrgo\u010d"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.16580"
  },
  {
    "id": "arXiv:2210.16586",
    "title": "NTULM: Enriching Social Media Text Representations with Non-Textual  Units",
    "abstract": "On social media, additional context is often present in the form of\nannotations and meta-data such as the post's author, mentions, Hashtags, and\nhyperlinks. We refer to these annotations as Non-Textual Units (NTUs). We posit\nthat NTUs provide social context beyond their textual semantics and leveraging\nthese units can enrich social media text representations. In this work we\nconstruct an NTU-centric social heterogeneous network to co-embed NTUs. We then\nprincipally integrate these NTU embeddings into a large pretrained language\nmodel by fine-tuning with these additional units. This adds context to noisy\nshort-text social media. Experiments show that utilizing NTU-augmented text\nrepresentations significantly outperforms existing text-only baselines by 2-5\\%\nrelative points on many downstream tasks highlighting the importance of context\nto social media NLP. We also highlight that including NTU context into the\ninitial layers of language model alongside text is better than using it after\nthe text embedding is generated. Our work leads to the generation of holistic\ngeneral purpose social media content embedding.",
    "descriptor": "\nComments: 14 pages, 5 figures, Accepted to the Proceedings of the Eighth Workshop on Noisy User-generated Text (W-NUT 2022). URL: this https URL\n",
    "authors": [
      "Jinning Li",
      "Shubhanshu Mishra",
      "Ahmed El-Kishky",
      "Sneha Mehta",
      "Vivek Kulkarni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.16586"
  },
  {
    "id": "arXiv:2210.16587",
    "title": "Relating Human Perception of Musicality to Prediction in a Predictive  Coding Model",
    "abstract": "We explore the use of a neural network inspired by predictive coding for\nmodeling human music perception. This network was developed based on the\ncomputational neuroscience theory of recurrent interactions in the hierarchical\nvisual cortex. When trained with video data using self-supervised learning, the\nmodel manifests behaviors consistent with human visual illusions. Here, we\nadapt this network to model the hierarchical auditory system and investigate\nwhether it will make similar choices to humans regarding the musicality of a\nset of random pitch sequences. When the model is trained with a large corpus of\ninstrumental classical music and popular melodies rendered as mel spectrograms,\nit exhibits greater prediction errors for random pitch sequences that are rated\nless musical by human subjects. We found that the prediction error depends on\nthe amount of information regarding the subsequent note, the pitch interval,\nand the temporal context. Our findings suggest that predictability is\ncorrelated with human perception of musicality and that a predictive coding\nneural network trained on music can be used to characterize the features and\nmotifs contributing to human perception of music.",
    "descriptor": "\nComments: 5 pages, 5 figures, currently in peer review\n",
    "authors": [
      "Nikolas McNeal",
      "Jennifer Huang",
      "Aniekan Umoren",
      "Shuqi Dai",
      "Roger Dannenberg",
      "Richard Randall",
      "Tai Sing Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16587"
  },
  {
    "id": "arXiv:2210.16589",
    "title": "Strong Lottery Ticket Hypothesis with $\\varepsilon$--perturbation",
    "abstract": "The strong Lottery Ticket Hypothesis (LTH) claims the existence of a\nsubnetwork in a sufficiently large, randomly initialized neural network that\napproximates some target neural network without the need of training. We extend\nthe theoretical guarantee of the strong LTH literature to a scenario more\nsimilar to the original LTH, by generalizing the weight change in the\npre-training step to some perturbation around initialization. In particular, we\nfocus on the following open questions: By allowing an $\\varepsilon$-scale\nperturbation on the random initial weights, can we reduce the\nover-parameterization requirement for the candidate network in the strong LTH?\nFurthermore, does the weight change by SGD coincide with a good set of such\nperturbation?\nWe answer the first question by first extending the theoretical result on\nsubset sum to allow perturbation on the candidates. Applying this result to the\nneural network setting, we show that such $\\varepsilon$-perturbation reduces\nthe over-parameterization requirement of the strong LTH. To answer the second\nquestion, we show via experiments that the perturbed weight achieved by the\nprojected SGD shows better performance under the strong LTH pruning.",
    "descriptor": "",
    "authors": [
      "Zheyang Xiong",
      "Fangshuo Liao",
      "Anastasios Kyrillidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.16589"
  },
  {
    "id": "arXiv:2210.16590",
    "title": "Track2Vec: fairness music recommendation with a GPU-free  customizable-driven framework",
    "abstract": "Recommendation systems have illustrated the significant progress made in\ncharacterizing users' preferences based on their past behaviors. Despite the\neffectiveness of recommending accurately, there exist several factors that are\nessential but unexplored for evaluating various facets of recommendation\nsystems, e.g., fairness, diversity, and limited resources. To address these\nissues, we propose Track2Vec, a GPU-free customizable-driven framework for\nfairness music recommendation. In order to take both accuracy and fairness into\naccount, our solution consists of three modules, a customized fairness-aware\ngroups for modeling different features based on configurable settings, a track\nrepresentation learning module for learning better user embedding, and an\nensemble module for ranking the recommendation results from different track\nrepresentation learning modules. Moreover, inspired by TF-IDF which has been\nwidely used in natural language processing, we introduce a metric called Miss\nRate - Inverse Ground Truth Frequency (MR-ITF) to measure the fairness.\nExtensive experiments demonstrate that our model achieves a 4th price ranking\nin a GPU-free environment on the leaderboard in the EvalRS @ CIKM 2022\nchallenge, which is superior to the official baseline by about 200% in terms of\nthe official scores. In addition, the ablation study illustrates the necessity\nof ensembling each group to acquire both accurate and fair recommendations.",
    "descriptor": "",
    "authors": [
      "Wei-Wei Du",
      "Wei-Yao Wang",
      "Wen-Chih Peng"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16590"
  },
  {
    "id": "arXiv:2210.16591",
    "title": "DisenPOI: Disentangling Sequential and Geographical Influence for  Point-of-Interest Recommendation",
    "abstract": "Point-of-Interest (POI) recommendation plays a vital role in various\nlocation-aware services. It has been observed that POI recommendation is driven\nby both sequential and geographical influences. However, since there is no\nannotated label of the dominant influence during recommendation, existing\nmethods tend to entangle these two influences, which may lead to sub-optimal\nrecommendation performance and poor interpretability. In this paper, we address\nthe above challenge by proposing DisenPOI, a novel Disentangled dual-graph\nframework for POI recommendation, which jointly utilizes sequential and\ngeographical relationships on two separate graphs and disentangles the two\ninfluences with self-supervision. The key novelty of our model compared with\nexisting approaches is to extract disentangled representations of both\nsequential and geographical influences with contrastive learning. To be\nspecific, we construct a geographical graph and a sequential graph based on the\ncheck-in sequence of a user. We tailor their propagation schemes to become\nsequence-/geo-aware to better capture the corresponding influences. Preference\nproxies are extracted from check-in sequence as pseudo labels for the two\ninfluences, which supervise the disentanglement via a contrastive loss.\nExtensive experiments on three datasets demonstrate the superiority of the\nproposed model.",
    "descriptor": "\nComments: Accepted by ACM International Conference on Web Search and Data Mining (WSDM'23)\n",
    "authors": [
      "Yifang Qin",
      "Yifan Wang",
      "Fang Sun",
      "Wei Ju",
      "Xuyang Hou",
      "Zhe Wang",
      "Jia Cheng",
      "Jun Lei",
      "Ming Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16591"
  },
  {
    "id": "arXiv:2210.16594",
    "title": "Design of non-diagonal stiffness matrix for assembly task",
    "abstract": "Compliance control is an increasingly employed technique used in the robotic\nfield. It is known that various mechanical properties can be reproduced\ndepending on the design of the stiffness matrix, but the design theory that\ntakes advantage of this high degree of design freedom has not been elucidated.\nThis paper, therefore, discusses the non-diagonal elements of the stiffness\nmatrix. We proposed a design method according to the conditions required for\nachieving stable motion. Additionally, we analyzed the displacement induced by\nthe non-diagonal elements in response to an external force and found that to\nobtain stable contact with a symmetric matrix, the matrix should be positive\ndefinite, i.e., all eigenvalues must be positive, however its parameter design\nis complicated. In this study, we focused on the use of asymmetric matrices in\ncompliance control and showed that the design of eigenvalues can be simplified\nby using a triangular matrix. This approach expands the range of the stiffness\ndesign and enhances the ability of the compliance control to induce motion. We\nconducted experiments using the stiffness matrix and confirmed that assembly\ncould be achieved without complicated trajectory planning.",
    "descriptor": "\nComments: 8 pages, 15 figures,2 tables\n",
    "authors": [
      "Tsukasa Kusakabe",
      "Sho Sakaino",
      "Toshiaki Tsuji"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.16594"
  },
  {
    "id": "arXiv:2210.16595",
    "title": "BEPHAP: A Blockchain-Based Efficient Privacy-Preserving Handover  Authentication Protocol with Key Agreement for Internet of Vehicles",
    "abstract": "The Internet of Vehicles (IoV) can significantly improve transportation\nefficiency and ensure traffic safety. Authentication is regarded as the\nfundamental defense line against attacks in IoV. However, the state-of-the-art\napproaches suffer from several drawbacks, including bottlenecks of the single\ncloud server model, high computational overhead of operations, excessive trust\nin cloud servers and roadside units (RSUs), and leakage of vehicle trajectory\nprivacy. In this paper, BEPHAP, a Blockchain-based Efficient Privacy-preserving\nHandover Authentication Protocol with key agreement for internet of vehicles,\nis introduced to address these problems. BEPHAP achieves anonymous cross-domain\nmutual handover authentication with key agreement based on the tamper-proof\nblockchain, symmetric cryptography, and the chameleon hash function under a\nsecurity model that cloud servers and RSUs may launch attacks. BEPHAP is\nparticularly well suited for IoV since it allows vehicles only need to perform\nlightweight cryptographic operations during the authentication phase. BEPHAP\nalso achieves data confidentiality, unlinkability, traceability,\nnon-repudiation, non-frameability, and key escrow freeness. Formal verification\nbased on ProVerif and formal security proofs based on the BAN logic indicates\nthat BEPHAP is resistant to various typical attacks, such as man-in-the-middle\nattacks, impersonation attacks, and replay attacks. Performance analysis\ndemonstrates that BEPHAP surpasses existing works in both computation and\ncommunication efficiencies. And the message loss rate remains 0 at 5000\nrequests per second, which meets the requirement of IoV.",
    "descriptor": "\nComments: 14 pages, 7 figures\n",
    "authors": [
      "Xianwang Xie",
      "Bin Wu",
      "Botao Hou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.16595"
  },
  {
    "id": "arXiv:2210.16598",
    "title": "Self-supervised predictive coding and multimodal fusion advance patient  deterioration prediction in fine-grained time resolution",
    "abstract": "In the Emergency Department (ED), accurate prediction of critical events\nusing Electronic Health Records (EHR) allows timely intervention and effective\nresource allocation. Though many studies have suggested automatic prediction\nmethods, their coarse-grained time resolutions limit their practical usage.\nTherefore, in this study, we propose an hourly prediction method of critical\nevents in ED, i.e., mortality and vasopressor need. Through extensive\nexperiments, we show that both 1) bi-modal fusion between EHR text and\ntime-series data and 2) self-supervised predictive regularization using L2 loss\nbetween normalized context vector and EHR future time-series data improve\npredictive performance, especially the far-future prediction. Our\nuni-modal/bi-modal/bi-modal self-supervision scored 0.846/0.877/0.897\n(0.824/0.855/0.886) and 0.817/0.820/0.858 (0.807/0.81/0.855) with mortality\n(far-future mortality) and with vasopressor need (far-future vasopressor need)\nprediction data in AUROC, respectively.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Kwanhyung Lee",
      "John Won",
      "Heejung Hyun",
      "Sangchul Hahn",
      "Edward Choi",
      "Joohyung Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16598"
  },
  {
    "id": "arXiv:2210.16602",
    "title": "an intelligent security centered resource-efficient resource management  model for cloud computing environments",
    "abstract": "This paper proposes a conceptual model for a secure and performance-efficient\nworkload management model in cloud environments. In this model, a resource\nmanagement unit is employed for energy and performance proficient allocation of\nvirtual machines while ensuring the secure processing of users' applications by\ndefending against data breaches due to unauthorized access to virtual machines\nin real-time. The resource management unit is guided by a secure virtual\nmachine management unit which is designed to generate information regarding\nunauthorized access or inter-communication links among active virtual machines.\nAlso, a workload analyzer unit operates concurrently to estimate resource\nutilization information to assist the resource management unit in the\nperformance-efficient allocation of virtual machines. Contrary to prior works\nwhich engage access control mechanisms, encryption, and decryption of data\nbefore the transfer and the use of tunneling for prevention of unauthorized\naccess to virtual machines which raises excess computational cost overhead, the\nproposed model operates diversely for efficiently serving the same purpose.",
    "descriptor": "\nComments: This article contains 12 pages, and 3 figures. It presents a conceptual resource management model for cloud data center environments which is the outcome of the authors' six to seven years of research work in the area of Cloud Computing\n",
    "authors": [
      "Deepika Saxena",
      "Ashutosh Kumar Singh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16602"
  },
  {
    "id": "arXiv:2210.16604",
    "title": "A Critical Reflection and Forward Perspective on Empathy and Natural  Language Processing",
    "abstract": "We review the state of research on empathy in natural language processing and\nidentify the following issues: (1) empathy definitions are absent or abstract,\nwhich (2) leads to low construct validity and reproducibility. Moreover, (3)\nemotional empathy is overemphasized, skewing our focus to a narrow subset of\nsimplified tasks. We believe these issues hinder research progress and argue\nthat current directions will benefit from a clear conceptualization that\nincludes operationalizing cognitive empathy components. Our main objectives are\nto provide insight and guidance on empathy conceptualization for NLP research\nobjectives and to encourage researchers to pursue the overlooked opportunities\nin this area, highly relevant, e.g., for clinical and educational sectors.",
    "descriptor": "\nComments: To appear at Findings of EMNLP 2022\n",
    "authors": [
      "Allison Lahnala",
      "Charles Welch",
      "David Jurgens",
      "Lucie Flek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16604"
  },
  {
    "id": "arXiv:2210.16606",
    "title": "Neural Combinatorial Logic Circuit Synthesis from Input-Output Examples",
    "abstract": "We propose a novel, fully explainable neural approach to synthesis of\ncombinatorial logic circuits from input-output examples. The carrying advantage\nof our method is that it readily extends to inductive scenarios, where the set\nof examples is incomplete but still indicative of the desired behaviour. Our\nmethod can be employed for a virtually arbitrary choice of atoms - from logic\ngates to FPGA blocks - as long as they can be formulated in a differentiable\nfashion, and consistently yields good results for synthesis of practical\ncircuits of increasing size. In particular, we succeed in learning a number of\narithmetic, bitwise, and signal-routing operations, and even generalise towards\nthe correct behaviour in inductive scenarios. Our method, attacking a discrete\nlogical synthesis problem with an explainable neural approach, hints at a wider\npromise for synthesis and reasoning-related tasks.",
    "descriptor": "\nComments: Accepted to the 2nd Workshop on Math-AI (MATH-AI@NeurIPS'22). 10 pages, 1 figure\n",
    "authors": [
      "Peter Belcak",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.16606"
  },
  {
    "id": "arXiv:2210.16609",
    "title": "$\\mathcal{H}^2$-matrices for translation-invariant kernel functions",
    "abstract": "Boundary element methods for elliptic partial differential equations\ntypically lead to boundary integral operators with translation-invariant kernel\nfunctions. Taking advantage of this property is fairly simple for particle\nmethods, e.g., Nystrom-type discretizations, but more challenging if the\nsupports of basis functions have to be taken into account.\nIn this article, we present a modified construction for\n$\\mathcal{H}^2$-matrices that uses translation-invariance to significantly\nreduce the storage requirements. Due to the uniformity of the boxes used for\nthe construction, we need only a few uncomplicated assumptions to prove\nestimates for the resulting storage complexity.",
    "descriptor": "\nComments: The work was funded by the DFG in project BO 3289/7-1\n",
    "authors": [
      "Steffen B\u00f6rm",
      "Janne Henningsen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16609"
  },
  {
    "id": "arXiv:2210.16610",
    "title": "Optimistic and Validity Rollups: Analysis and Comparison between  Optimism and StarkNet",
    "abstract": "The thesis addresses the problem of scalability in decentralized blockchains\nin the context of the trade-off between transaction throughput and hardware\nrequirements to participate in the network. Rollups are presented, that is\ntechnologies to verify on-chain blocks executed off-chain by minimizing the\nassumptions of trust. The variant of the Optimistic Rollups, in particular of\nOptimism and the use of invalidity proofs through interactive binary search and\nof the Validity Rollups, in particular of StarkNet, and the use of validity\nproofs through STARKs are discussed. Finally, the two solutions are compared on\nwithdrawal time, on the cost of transactions and techniques to minimize it, on\nthe possibility of applying the technology recursively, on compatibility with\nEthereum and on the licenses used.",
    "descriptor": "\nComments: 61 pages\n",
    "authors": [
      "Luca Donno"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.16610"
  },
  {
    "id": "arXiv:2210.16612",
    "title": "Weak Galerkin Finite Element Methods for Quad-Curl Problems",
    "abstract": "This article introduces a weak Galerkin (WG) finite element method for\nquad-curl problems in three dimensions. It is proved that the proposed WG\nmethod is stable and accurate in an optimal order of error estimates for the\nexact solution in discrete norms. In addition, an $L^2$ error estimate in an\noptimal order except the lowest orders $k=1, 2$ is derived for the WG solution.\nSome numerical experiments are conducted to verify the efficiency and accuracy\nof our WG method and furthermore a superconvergence has been observed from the\nnumerical results.",
    "descriptor": "\nComments: 16 pages, 2 tables\n",
    "authors": [
      "Chunmei Wang",
      "Junping Wang",
      "Shangyou Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16612"
  },
  {
    "id": "arXiv:2210.16613",
    "title": "Diverse Parallel Data Synthesis for Cross-Database Adaptation of  Text-to-SQL Parsers",
    "abstract": "Text-to-SQL parsers typically struggle with databases unseen during the train\ntime. Adapting parsers to new databases is a challenging problem due to the\nlack of natural language queries in the new schemas. We present ReFill, a\nframework for synthesizing high-quality and textually diverse parallel datasets\nfor adapting a Text-to-SQL parser to a target schema. ReFill learns to\nretrieve-and-edit text queries from the existing schemas and transfers them to\nthe target schema. We show that retrieving diverse existing text, masking their\nschema-specific tokens, and refilling with tokens relevant to the target\nschema, leads to significantly more diverse text queries than achievable by\nstandard SQL-to-Text generation methods. Through experiments spanning multiple\ndatabases, we demonstrate that fine-tuning parsers on datasets synthesized\nusing ReFill consistently outperforms the prior data-augmentation methods.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Abhijeet Awasthi",
      "Ashutosh Sathe",
      "Sunita Sarawagi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16613"
  },
  {
    "id": "arXiv:2210.16621",
    "title": "Empirical Evaluation of Post-Training Quantization Methods for Language  Tasks",
    "abstract": "Transformer-based architectures like BERT have achieved great success in a\nwide range of Natural Language tasks. Despite their decent performance, the\nmodels still have numerous parameters and high computational complexity,\nimpeding their deployment in resource-constrained environments. Post-Training\nQuantization (PTQ), which enables low-bit computations without extra training,\ncould be a promising tool. In this work, we conduct an empirical evaluation of\nthree PTQ methods on BERT-Base and BERT-Large: Linear Quantization (LQ),\nAnalytical Clipping for Integer Quantization (ACIQ), and Outlier Channel\nSplitting (OCS). OCS theoretically surpasses the others in minimizing the Mean\nSquare quantization Error and avoiding distorting the weights' outliers. That\nis consistent with the evaluation results of most language tasks of GLUE\nbenchmark and a reading comprehension task, SQuAD. Moreover, low-bit quantized\nBERT models could outperform the corresponding 32-bit baselines on several\nsmall language tasks, which we attribute to the alleviation of\nover-parameterization. We further explore the limit of quantization bit and\nshow that OCS could quantize BERT-Base and BERT-Large to 3-bits and retain 98%\nand 96% of the performance on the GLUE benchmark accordingly. Moreover, we\nconduct quantization on the whole BERT family, i.e., BERT models in different\nconfigurations, and comprehensively evaluate their performance on the GLUE\nbenchmark and SQuAD, hoping to provide valuable guidelines for their deployment\nin various computation environments.",
    "descriptor": "",
    "authors": [
      "Ting Hu",
      "Christoph Meinel",
      "Haojin Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16621"
  },
  {
    "id": "arXiv:2210.16624",
    "title": "LearningGroup: A Real-Time Sparse Training on FPGA via Learnable Weight  Grouping for Multi-Agent Reinforcement Learning",
    "abstract": "Multi-agent reinforcement learning (MARL) is a powerful technology to\nconstruct interactive artificial intelligent systems in various applications\nsuch as multi-robot control and self-driving cars. Unlike supervised model or\nsingle-agent reinforcement learning, which actively exploits network pruning,\nit is obscure that how pruning will work in multi-agent reinforcement learning\nwith its cooperative and interactive characteristics. \\par In this paper, we\npresent a real-time sparse training acceleration system named LearningGroup,\nwhich adopts network pruning on the training of MARL for the first time with an\nalgorithm/architecture co-design approach. We create sparsity using a weight\ngrouping algorithm and propose on-chip sparse data encoding loop (OSEL) that\nenables fast encoding with efficient implementation. Based on the OSEL's\nencoding format, LearningGroup performs efficient weight compression and\ncomputation workload allocation to multiple cores, where each core handles\nmultiple sparse rows of the weight matrix simultaneously with vector processing\nunits. As a result, LearningGroup system minimizes the cycle time and memory\nfootprint for sparse data generation up to 5.72x and 6.81x. Its FPGA\naccelerator shows 257.40-3629.48 GFLOPS throughput and 7.10-100.12 GFLOPS/W\nenergy efficiency for various conditions in MARL, which are 7.13x higher and\n12.43x more energy efficient than Nvidia Titan RTX GPU, thanks to the fully\non-chip training and highly optimized dataflow/data format provided by FPGA.\nMost importantly, the accelerator shows speedup up to 12.52x for processing\nsparse data over the dense case, which is the highest among state-of-the-art\nsparse training accelerators.",
    "descriptor": "\nComments: This paper will be published in IEEE International Conference on Field-Programmable Technology 2022\n",
    "authors": [
      "Je Yang",
      "JaeUk Kim",
      "Joo-Young Kim"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16624"
  },
  {
    "id": "arXiv:2210.16627",
    "title": "TFormer: 3D Tooth Segmentation in Mesh Scans with Geometry Guided  Transformer",
    "abstract": "Optical Intra-oral Scanners (IOS) are widely used in digital dentistry,\nproviding 3-Dimensional (3D) and high-resolution geometrical information of\ndental crowns and the gingiva. Accurate 3D tooth segmentation, which aims to\nprecisely delineate the tooth and gingiva instances in IOS, plays a critical\nrole in a variety of dental applications. However, segmentation performance of\nprevious methods are error-prone in complicated tooth-tooth or tooth-gingiva\nboundaries, and usually exhibit unsatisfactory results across various patients,\nyet the clinically applicability is not verified with large-scale dataset. In\nthis paper, we propose a novel method based on 3D transformer architectures\nthat is evaluated with large-scale and high-resolution 3D IOS datasets. Our\nmethod, termed TFormer, captures both local and global dependencies among\ndifferent teeth to distinguish various types of teeth with divergent anatomical\nstructures and confusing boundaries. Moreover, we design a geometry guided loss\nbased on a novel point curvature to exploit boundary geometric features, which\nhelps refine the boundary predictions for more accurate and smooth\nsegmentation. We further employ a multi-task learning scheme, where an\nadditional teeth-gingiva segmentation head is introduced to improve the\nperformance. Extensive experimental results in a large-scale dataset with\n16,000 IOS, the largest IOS dataset to our best knowledge, demonstrate that our\nTFormer can surpass existing state-of-the-art baselines with a large margin,\nwith its utility in real-world scenarios verified by a clinical applicability\ntest.",
    "descriptor": "",
    "authors": [
      "Huimin Xiong",
      "Kunle Li",
      "Kaiyuan Tan",
      "Yang Feng",
      "Joey Tianyi Zhou",
      "Jin Hao",
      "Zuozhu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16627"
  },
  {
    "id": "arXiv:2210.16628",
    "title": "Structure preserving schemes for Fokker-Planck equations of irreversible  processes",
    "abstract": "In this paper, we introduce second order and fourth order space\ndiscretization via finite difference implementation of the finite element\nmethod for solving Fokker-Planck equations associated with irreversible\nprocesses. The proposed schemes are first order in time and second order and\nfourth order in space. Under mild mesh conditions and time step constraints for\nsmooth solutions, the schemes are proved to be monotone, thus are\npositivity-preserving and energy dissipative. In particular, our scheme is\nsuitable for capturing steady state solutions in large final time simulations.",
    "descriptor": "\nComments: 2 figures\n",
    "authors": [
      "Chen Liu",
      "Yuan Gao",
      "Xiangxiong Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16628"
  },
  {
    "id": "arXiv:2210.16637",
    "title": "Beyond prompting: Making Pre-trained Language Models Better Zero-shot  Learners by Clustering Representations",
    "abstract": "Recent work has demonstrated that pre-trained language models (PLMs) are\nzero-shot learners. However, most existing zero-shot methods involve heavy\nhuman engineering or complicated self-training pipelines, hindering their\napplication to new situations. In this work, we show that zero-shot text\nclassification can be improved simply by clustering texts in the embedding\nspaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian\nGaussian Mixture Model after initializing cluster positions and shapes using\nclass names. Despite its simplicity, this approach achieves superior or\ncomparable performance on both topic and sentiment classification datasets and\noutperforms prior works significantly on unbalanced datasets. We further\nexplore the applicability of our clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths, and numbers of classes. Our\napproach achieves an average of 20% absolute improvement over prompt-based\nzero-shot learning. Finally, we compare different PLM embedding spaces and find\nthat texts are well-clustered by topics even if the PLM is not explicitly\npre-trained to generate meaningful sentence embeddings. This work indicates\nthat PLM embeddings can categorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize their knowledge and zero-shot\nlearning ability.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Yu Fei",
      "Ping Nie",
      "Zhao Meng",
      "Roger Wattenhofer",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16637"
  },
  {
    "id": "arXiv:2210.16639",
    "title": "GRACE: Loss-Resilient Real-Time Video Communication Using Data-Scalable  Autoencoder",
    "abstract": "Across many real-time video applications, we see a growing need (especially\nin long delays and dynamic bandwidth) to allow clients to decode each frame\nonce any (non-empty) subset of its packets is received and improve quality with\neach new packet. We call it data-scalable delivery. Unfortunately, existing\ntechniques (e.g., FEC, RS and Fountain Codes) fall short: they require either\ndelivery of a minimum number of packets to decode frames, and/or pad video data\nwith redundancy in anticipation of packet losses, which hurts video quality if\nno packets get lost. This work explores a new approach, inspired by recent\nadvances of neural-network autoencoders, which make data-scalable delivery\npossible. We present Grace, a concrete data-scalable real-time video system.\nWith the same video encoding, Grace's quality is slightly lower than\ntraditional codec without redundancy when no packet is lost, but with each\nmissed packet, its quality degrades much more gracefully than existing\nsolutions, allowing clients to flexibly trade between frame delay and video\nquality. Grace makes two contributions: (1) it trains new custom autoencoders\nto balance compression efficiency and resilience against a wide range of packet\nlosses; and (2) it uses a new transmission scheme to deliver autoencoder-coded\nframes as individually decodable packets. We test Grace (and traditional\nloss-resilient schemes and codecs) on real network traces and videos, and show\nthat while Grace's compression efficiency is slightly worse than heavily\nengineered video codecs, it significantly reduces tail video frame delay (by\n2$\\times$ at the 95th percentile) with the marginally lowered video quality",
    "descriptor": "",
    "authors": [
      "Yihua Cheng",
      "Anton Arapin",
      "Ziyi Zhang",
      "Qizheng Zhang",
      "Hanchen Li",
      "Nick Feamster",
      "Junchen Jiang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.16639"
  },
  {
    "id": "arXiv:2210.16642",
    "title": "Unifying the Discrete and Continuous Emotion labels for Speech Emotion  Recognition",
    "abstract": "Traditionally, in paralinguistic analysis for emotion detection from speech,\nemotions have been identified with discrete or dimensional (continuous-valued)\nlabels. Accordingly, models that have been proposed for emotion detection use\none or the other of these label types. However, psychologists like Russell and\nPlutchik have proposed theories and models that unite these views, maintaining\nthat these representations have shared and complementary information. This\npaper is an attempt to validate these viewpoints computationally. To this end,\nwe propose a model to jointly predict continuous and discrete emotional\nattributes and show how the relationship between these can be utilized to\nimprove the robustness and performance of emotion recognition tasks. Our\napproach comprises multi-task and hierarchical multi-task learning frameworks\nthat jointly model the relationships between continuous-valued and discrete\nemotion labels. Experimental results on two widely used datasets (IEMOCAP and\nMSPPodcast) for speech-based emotion recognition show that our model results in\nstatistically significant improvements in performance over strong baselines\nwith non-unified approaches. We also demonstrate that using one type of label\n(discrete or continuous-valued) for training improves recognition performance\nin tasks that use the other type of label. Experimental results and reasoning\nfor this approach (called the mismatched training approach) are also presented.",
    "descriptor": "\nComments: Under Review at ICASSP 2023\n",
    "authors": [
      "Roshan Sharma",
      "Hira Dhamyal",
      "Bhiksha Raj",
      "Rita Singh"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16642"
  },
  {
    "id": "arXiv:2210.16643",
    "title": "XNOR-FORMER: Learning Accurate Approximations in Long Speech  Transformers",
    "abstract": "Transformers are among the state of the art for many tasks in speech, vision,\nand natural language processing, among others. Self-attentions, which are\ncrucial contributors to this performance have quadratic computational\ncomplexity, which makes training on longer input sequences challenging. Prior\nwork has produced state-of-the-art transformer variants with linear attention,\nhowever, current models sacrifice performance to achieve efficient\nimplementations. In this work, we develop a novel linear transformer by\nexamining the properties of the key-query product within self-attentions. Our\nmodel outperforms state of the art approaches on speech recognition and speech\nsummarization, resulting in 1 % absolute WER improvement on the Librispeech-100\nspeech recognition benchmark and a new INTERVIEW speech recognition benchmark,\nand 5 points on ROUGE for summarization with How2.",
    "descriptor": "\nComments: Under review at ICASSP 2023\n",
    "authors": [
      "Roshan Sharma",
      "Bhiksha Raj"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16643"
  },
  {
    "id": "arXiv:2210.16644",
    "title": "Unsupervised Audio-Visual Lecture Segmentation",
    "abstract": "Over the last decade, online lecture videos have become increasingly popular\nand have experienced a meteoric rise during the pandemic. However,\nvideo-language research has primarily focused on instructional videos or\nmovies, and tools to help students navigate the growing online lectures are\nlacking. Our first contribution is to facilitate research in the educational\ndomain, by introducing AVLectures, a large-scale dataset consisting of 86\ncourses with over 2,350 lectures covering various STEM subjects. Each course\ncontains video lectures, transcripts, OCR outputs for lecture frames, and\noptionally lecture notes, slides, assignments, and related educational content\nthat can inspire a variety of tasks. Our second contribution is introducing\nvideo lecture segmentation that splits lectures into bite-sized topics that\nshow promise in improving learner engagement. We formulate lecture segmentation\nas an unsupervised task that leverages visual, textual, and OCR cues from the\nlecture, while clip representations are fine-tuned on a pretext self-supervised\ntask of matching the narration with the temporally aligned visual content. We\nuse these representations to generate segments using a temporally consistent\n1-nearest neighbor algorithm, TW-FINCH. We evaluate our method on 15 courses\nand compare it against various visual and textual baselines, outperforming all\nof them. Our comprehensive ablation studies also identify the key factors\ndriving the success of our approach.",
    "descriptor": "\nComments: 17 pages, 14 figures, 14 tables, Accepted to WACV 2023. Project page: this https URL\n",
    "authors": [
      "Darshan Singh S",
      "Anchit Gupta",
      "C. V. Jawahar",
      "Makarand Tapaswi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16644"
  },
  {
    "id": "arXiv:2210.16646",
    "title": "Breaking the Symmetry: Resolving Symmetry Ambiguities in Equivariant  Neural Networks",
    "abstract": "Equivariant networks have been adopted in many 3-D learning areas. Here we\nidentify a fundamental limitation of these networks: their ambiguity to\nsymmetries. Equivariant networks cannot complete symmetry-dependent tasks like\nsegmenting a left-right symmetric object into its left and right sides. We\ntackle this problem by adding components that resolve symmetry ambiguities\nwhile preserving rotational equivariance. We present OAVNN: Orientation Aware\nVector Neuron Network, an extension of the Vector Neuron Network. OAVNN is a\nrotation equivariant network that is robust to planar symmetric inputs. Our\nnetwork consists of three key components. 1) We introduce an algorithm to\ncalculate symmetry detecting features. 2) We create a symmetry-sensitive\norientation aware linear layer. 3) We construct an attention mechanism that\nrelates directional information across points. We evaluate the network using\nleft-right segmentation and find that the network quickly obtains accurate\nsegmentations. We hope this work motivates investigations on the expressivity\nof equivariant networks on symmetric objects.",
    "descriptor": "\nComments: 13 pages, 8 figures, NeurIPS Symmetry and Geometry in Neural Representations Conference\n",
    "authors": [
      "Sidhika Balachandar",
      "Adrien Poulenard",
      "Congyue Deng",
      "Leonidas Guibas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16646"
  },
  {
    "id": "arXiv:2210.16651",
    "title": "Libraries, Integrations and Hubs for Decentralized AI using IPFS",
    "abstract": "AI requires heavy amounts of storage and compute. As a result, AI developers\nare regular users of centralised cloud services such as AWS, GCP and Azure,\ncompute environments such as Jupyter and Colab notebooks, and AI Hubs such as\nHuggingFace and ActiveLoop. There services are associated with certain benefits\nand limitations that stem from the underlying infrastructure and governance\nsystems with which they are built. These limitations include high costs, lack\nof monetization and reward, lack of control and difficulty of reproducibility.\nAt the same time, there are few libraries that allow data scientists to\ninteract with decentralised storage in the language that data scientists are\nused to, and few hubs where they can discover and interact with AI assets. In\nthis report, we explore the potential of decentralized technologies - such as\nWeb3 wallets, peer-to-peer marketplaces, decentralized storage (IPFS and\nFilecoin) and compute, and DAOs - to address some of the above limitations. We\nshowcase some of the libraries and integrations that we have built to tackle\nthese issues, as well as a proof of concept of a decentralized AI Hub app, that\nall use IPFS as a core infrastructural component.",
    "descriptor": "\nComments: 19 pages, 3 figures\n",
    "authors": [
      "Richard Blythman",
      "Mohamed Arshath",
      "Jakub Sm\u00e9kal",
      "Hithesh Shaji",
      "Salvatore Vivona",
      "Tyrone Dunmore"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.16651"
  },
  {
    "id": "arXiv:2210.16656",
    "title": "Auxo: Heterogeneity-Mitigating Federated Learning via Scalable Client  Clustering",
    "abstract": "Federated learning (FL) is an emerging machine learning (ML) paradigm that\nenables heterogeneous edge devices to collaboratively train ML models without\nrevealing their raw data to a logically centralized server. Heterogeneity\nacross participants is a fundamental challenge in FL, both in terms of\nnon-independent and identically distributed (Non-IID) data distributions and\nvariations in device capabilities. Many existing works present point solutions\nto address issues like slow convergence, low final accuracy, and bias in FL,\nall stemming from the client heterogeneity. We observe that, in a large\npopulation, there exist groups of clients with statistically similar data\ndistributions (cohorts). In this paper, we propose Auxo to gradually identify\ncohorts among large-scale, low-participation, and resource-constrained FL\npopulations. Auxo then adaptively determines how to train cohort-specific\nmodels in order to achieve better model performance and ensure resource\nefficiency. By identifying cohorts with smaller heterogeneity and performing\nefficient cohort-based training, our extensive evaluations show that Auxo\nsubstantially boosts the state-of-the-art solutions in terms of final accuracy,\nconvergence time, and model bias.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Jiachen Liu",
      "Fan Lai",
      "Yinwei Dai",
      "Aditya Akella",
      "Harsha Madhyastha",
      "Mosharaf Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16656"
  },
  {
    "id": "arXiv:2210.16657",
    "title": "Improved Support Recovery in Universal One-bit Compressed Sensing",
    "abstract": "One-bit compressed sensing (1bCS) is an extremely quantized signal\nacquisition method that has been proposed and studied rigorously in the past\ndecade. In 1bCS, linear samples of a high dimensional signal are quantized to\nonly one bit per sample (sign of the measurement). Assuming the original signal\nvector to be sparse, existing results in 1bCS either aim to find the support of\nthe vector, or approximate the signal allowing a small error. The focus of this\npaper is support recovery, which often also computationally facilitate\napproximate signal recovery. A {\\em universal} measurement matrix for 1bCS\nrefers to one set of measurements that work for all sparse signals. With\nuniversality, it is known that $\\tilde{\\Theta}(k^2)$ 1bCS measurements are\nnecessary and sufficient for support recovery (where $k$ denotes the sparsity).\nTo improve the dependence on sparsity from quadratic to linear, in this work we\npropose approximate support recovery (allowing $\\epsilon>0$ proportion of\nerrors), and superset recovery (allowing $\\epsilon$ proportion of false\npositives). We show that the first type of recovery is possible with\n$\\tilde{O}(k/\\epsilon)$ measurements, while the later type of recovery, more\nchallenging, is possible with $\\tilde{O}(\\max\\{k/\\epsilon,k^{3/2}\\})$\nmeasurements. We also show that in both cases $\\Omega(k/\\epsilon)$ measurements\nwould be necessary for universal recovery.\nImproved results are possible if we consider universal recovery within a\nrestricted class of signals, such as rational signals, or signals with bounded\ndynamic range. In both cases superset recovery is possible with only\n$\\tilde{O}(k/\\epsilon)$ measurements. Other results on universal but\napproximate support recovery are also provided in this paper. All of our main\nrecovery algorithms are simple and polynomial-time.",
    "descriptor": "\nComments: 26 pages, no figures. This paper is an extended and improved version of arXiv:2107.09091 (accepted to ITCS 2022)\n",
    "authors": [
      "Namiko Matsumoto",
      "Arya Mazumdar",
      "Soumyabrata Pal"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16657"
  },
  {
    "id": "arXiv:2210.16658",
    "title": "Perturbation Analysis of Neural Collapse",
    "abstract": "Training deep neural networks for classification often includes minimizing\nthe training loss beyond the zero training error point. In this phase of\ntraining, a \"neural collapse\" behavior has been observed: the variability of\nfeatures (outputs of the penultimate layer) of within-class samples decreases\nand the mean features of different classes approach a certain tight frame\nstructure. Recent works analyze this behavior via idealized unconstrained\nfeatures models where all the minimizers exhibit exact collapse. However, with\npractical networks and datasets, the features typically do not reach exact\ncollapse, e.g., because deep layers cannot arbitrarily modify intermediate\nfeatures that are far from being collapsed. In this paper, we propose a richer\nmodel that can capture this phenomenon by forcing the features to stay in the\nvicinity of a predefined features matrix (e.g., intermediate features). We\nexplore the model in the small vicinity case via perturbation analysis and\nestablish results that cannot be obtained by the previously studied models. For\nexample, we prove reduction in the within-class variability of the optimized\nfeatures compared to the predefined input features (via analyzing gradient flow\non the \"central-path\" with minimal assumptions), analyze the minimizers in the\nnear-collapse regime, and provide insights on the effect of regularization\nhyperparameters on the closeness to collapse. We support our theory with\nexperiments in practical deep learning settings.",
    "descriptor": "",
    "authors": [
      "Tom Tirer",
      "Haoxiang Huang",
      "Jonathan Niles-Weed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16658"
  },
  {
    "id": "arXiv:2210.16659",
    "title": "Learning Dependencies of Discrete Speech Representations with Neural  Hidden Markov Models",
    "abstract": "While discrete latent variable models have had great success in\nself-supervised learning, most models assume that frames are independent. Due\nto the segmental nature of phonemes in speech perception, modeling dependencies\namong latent variables at the frame level can potentially improve the learned\nrepresentations on phonetic-related tasks. In this work, we assume Markovian\ndependencies among latent variables, and propose to learn speech\nrepresentations with neural hidden Markov models. Our general framework allows\nus to compare to self-supervised models that assume independence, while keeping\nthe number of parameters fixed. The added dependencies improve the\naccessibility of phonetic information, phonetic segmentation, and the cluster\npurity of phones, showcasing the benefit of the assumed dependencies.",
    "descriptor": "",
    "authors": [
      "Sung-Lin Yeh",
      "Hao Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16659"
  },
  {
    "id": "arXiv:2210.16660",
    "title": "Alya towards Exascale: Algorithmic Scalability using PSCToolkit",
    "abstract": "In this paper, we describe some work aimed at upgrading the Alya code with\nup-to-date parallel linear solvers capable of achieving reliability,\nefficiency, and scalability in the computation of the pressure field at each\ntime step of the numerical procedure for solving an LES formulation of the\nincompressible Navier-Stokes equations. We developed a software module in\nAlya's kernel to interface the libraries included in the current version of\nPSCToolkit, a framework for the iterative solution of sparse linear systems on\nparallel distributed-memory computers by Krylov methods coupled to Algebraic\nMultiGrid preconditioners. The Toolkit has undergone some extensions within the\nEoCoE-II project with the primary goal to face the exascale challenge. Results\non a realistic benchmark for airflow simulations in wind farm applications show\nthat the PSCToolkit solvers significantly outperform the original versions of\nthe Conjugate Gradient method available in the Alya kernel in terms of\nscalability and parallel efficiency and represent a very promising software\nlayer to move the Alya code towards exascale.",
    "descriptor": "",
    "authors": [
      "Herbert Owen",
      "Oriol Lehmkuhl",
      "Pasqua D'Ambra",
      "Fabio Durastante",
      "Salvatore Filippone"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16660"
  },
  {
    "id": "arXiv:2210.16670",
    "title": "A Comparative Study of Graph Neural Networks for Shape Classification in  Neuroimaging",
    "abstract": "Graph neural networks have emerged as a promising approach for the analysis\nof non-Euclidean data such as meshes. In medical imaging, mesh-like data plays\nan important role for modelling anatomical structures, and shape classification\ncan be used in computer aided diagnosis and disease detection. However, with a\nplethora of options, the best architectural choices for medical shape analysis\nusing GNNs remain unclear. We conduct a comparative analysis to provide\npractitioners with an overview of the current state-of-the-art in geometric\ndeep learning for shape classification in neuroimaging. Using biological sex\nclassification as a proof-of-concept task, we find that using FPFH as node\nfeatures substantially improves GNN performance and generalisation to\nout-of-distribution data; we compare the performance of three alternative\nconvolutional layers; and we reinforce the importance of data augmentation for\ngraph based learning. We then confirm these results hold for a clinically\nrelevant task, using the classification of Alzheimer's disease.",
    "descriptor": "\nComments: Accepted at GeoMedIA Workshop 2022 (Proceedings of Machine Learning Research). Code available on this https URL\n",
    "authors": [
      "Nairouz Shehata",
      "Wulfie Bain",
      "Ben Glocker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16670"
  },
  {
    "id": "arXiv:2210.16680",
    "title": "An Analytical Model for Stepwise Adiabatic Driver Energy Consumption",
    "abstract": "This paper presents a complete closed-form analytical model for determining\nthe per-cycle energy consumption of stepwise adiabatic drivers used for driving\na capacitive load such as a power FET gate. The model takes into account the\nnumber of steps used, the stepwise driver tank capacitance, the load\ncapacitance, and the stepwise driver switch resistance and on-time. Model\naccuracy is compared to that of simulation and models from previous work.",
    "descriptor": "\nComments: 17 pages, 7 Figures\n",
    "authors": [
      "Eric J. Carlson",
      "Joshua R. Smith"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16680"
  },
  {
    "id": "arXiv:2210.16682",
    "title": "Robust Distributed Learning Against Both Distributional Shifts and  Byzantine Attacks",
    "abstract": "In distributed learning systems, robustness issues may arise from two\nsources. On one hand, due to distributional shifts between training data and\ntest data, the trained model could exhibit poor out-of-sample performance. On\nthe other hand, a portion of working nodes might be subject to byzantine\nattacks which could invalidate the learning result. Existing works mostly deal\nwith these two issues separately. In this paper, we propose a new algorithm\nthat equips distributed learning with robustness measures against both\ndistributional shifts and byzantine attacks. Our algorithm is built on recent\nadvances in distributionally robust optimization as well as norm-based\nscreening (NBS), a robust aggregation scheme against byzantine attacks. We\nprovide convergence proofs in three cases of the learning model being\nnonconvex, convex, and strongly convex for the proposed algorithm, shedding\nlight on its convergence behaviors and endurability against byzantine attacks.\nIn particular, we deduce that any algorithm employing NBS (including ours)\ncannot converge when the percentage of byzantine nodes is 1/3 or higher,\ninstead of 1/2, which is the common belief in current literature. The\nexperimental results demonstrate the effectiveness of our algorithm against\nboth robustness issues. To the best of our knowledge, this is the first work to\naddress distributional shifts and byzantine attacks simultaneously.",
    "descriptor": "",
    "authors": [
      "Guanqiang Zhou",
      "Ping Xu",
      "Yue Wang",
      "Zhi Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16682"
  },
  {
    "id": "arXiv:2210.16690",
    "title": "On the Need of Neuromorphic Twins to Detect Denial-of-Service Attacks on  Communication Networks",
    "abstract": "As we are more and more dependent on the communication technologies,\nresilience against any attacks on communication networks is important to\nguarantee the digital sovereignty of our society. New developments of\ncommunication networks tackle the problem of resilience by in-network computing\napproaches for higher protocol layers, while the physical layer remains an open\nproblem. This is particularly true for wireless communication systems which are\ninherently vulnerable to adversarial attacks due to the open nature of the\nwireless medium. In denial-of-service (DoS) attacks, an active adversary is\nable to completely disrupt the communication and it has been shown that Turing\nmachines are incapable of detecting such attacks. As Turing machines provide\nthe fundamental limits of digital information processing and therewith of\ndigital twins, this implies that even the most powerful digital twins that\npreserve all information of the physical network error-free are not capable of\ndetecting such attacks. This stimulates the question of how powerful the\ninformation processing hardware must be to enable the detection of DoS attacks.\nTherefore, in the paper the need of neuromorphic twins is advocated and by the\nuse of Blum-Shub-Smale machines a first implementation that enables the\ndetection of DoS attacks is shown. This result holds for both cases of with and\nwithout constraints on the input and jamming sequences of the adversary.",
    "descriptor": "\nComments: submitted for publication\n",
    "authors": [
      "Holger Boche",
      "Rafael F. Schaefer",
      "H. Vincent Poor",
      "Frank H. P. Fitzek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16690"
  },
  {
    "id": "arXiv:2210.16691",
    "title": "Enabling Data Movement and Computation Pipelining in Deep Learning  Compiler",
    "abstract": "Pipelining between data loading and computation is a critical tensor program\noptimization for GPUs. Multi-stage pipelining across the multi-level buffer\nhierarchy of GPU is particularly indispensable on the latest NVIDIA Ampere GPUs\nto reduce resource idleness and guarantee kernel performance. Currently, people\nrely on libraries written by experts such as cuBLAS to access the pipelining\noptimization instead of through a tensor program transformation, which is\ninextensible to new operators and un-composable with prior tensor compiler\noptimizations. We present ALCOP, an automatic pipelining framework based on TVM\ninfrastructure that overcomes three critical obstacles in generating code for\npipelining: detection of pipelining-applicable buffers, program transformation\nfor multi-level multi-stage pipelining, and efficient schedule parameter search\nby incorporating static analysis. Experiments show that ALCOP can generate\nprograms with 1.23x speedup on average (up to 1.73x) over vanilla TVM. On\nend-to-end models, ALCOP can improve upon TVM by up to 1.18x, and XLA by up to\n1.64x. Besides, our performance model significantly improves the efficiency of\nthe schedule tuning process and can find schedules with 99% the performance\ngiven by exhaustive search while costing 40x fewer trials.",
    "descriptor": "",
    "authors": [
      "Guyue Huang",
      "Yang Bai",
      "Liu Liu",
      "Yuke Wang",
      "Bei Yu",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16691"
  },
  {
    "id": "arXiv:2210.16692",
    "title": "Single-Shot Domain Adaptation via Target-Aware Generative Augmentation",
    "abstract": "The problem of adapting models from a source domain using data from any\ntarget domain of interest has gained prominence, thanks to the brittle\ngeneralization in deep neural networks. While several test-time adaptation\ntechniques have emerged, they typically rely on synthetic data augmentations in\ncases of limited target data availability. In this paper, we consider the\nchallenging setting of single-shot adaptation and explore the design of\naugmentation strategies. We argue that augmentations utilized by existing\nmethods are insufficient to handle large distribution shifts, and hence propose\na new approach SiSTA (Single-Shot Target Augmentations), which first fine-tunes\na generative model from the source domain using a single-shot target, and then\nemploys novel sampling strategies for curating synthetic target data. Using\nexperiments with a state-of-the-art domain adaptation method, we find that\nSiSTA produces improvements as high as 20\\% over existing baselines under\nchallenging shifts in face attribute detection, and that it performs\ncompetitively to oracle models obtained by training on a larger target dataset.",
    "descriptor": "",
    "authors": [
      "Rakshith Subramanyam",
      "Kowshik Thopalli",
      "Spring Berman",
      "Pavan Turaga",
      "Jayaraman J. Thiagarajan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16692"
  },
  {
    "id": "arXiv:2210.16694",
    "title": "Linear Programs with Conjunctive Database Queries",
    "abstract": "In this paper, we study the problem of optimizing a linear program whose\nvariables are the answers to a conjunctive query. For this we propose the\nlanguage LP(CQ) for specifying linear programs whose constraints and objective\nfunctions depend on the answer sets of conjunctive queries. We contribute an\nefficient algorithm for solving programs in a fragment of LP(CQ). The natural\napproach constructs a linear program having as many variables as there are\nelements in the answer set of the queries. Our approach constructs a linear\nprogram having the same optimal value but fewer variables. This is done by\nexploiting the structure of the conjunctive queries using generalized hypertree\ndecompositions of small width to factorize elements of the answer set together.\nWe illustrate the various applications of LP(CQ) programs on three examples:\noptimizing deliveries of resources, minimizing noise for differential privacy,\nand computing the s-measure of patterns in graphs as needed for data mining.",
    "descriptor": "",
    "authors": [
      "Florent Capelli",
      "Nicolas Crosetti",
      "Joachim Niehren",
      "Jan Ramon"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.16694"
  },
  {
    "id": "arXiv:2210.16695",
    "title": "Impact of Reconfigurable Intelligent Surface Geometry on Communication  Performance",
    "abstract": "When beamforming is applied at the transmitter, only part of reconfigurable\nintelligent surfaces (RISs) will be active, and it becomes indispensable to\nstudy the impact of RIS geometry. This is the first study evaluating RIS\ngeometry, ranging from linear (1D), and planar (2D), to cylindrical (3D)\nstructures. We first derive the effective illuminated surface of different RIS\ntopologies and determine the resulting signal-to-noise ratio (SNR) and outage\nprobability. We then investigate the optimal RIS location to trade off active\narea versus received power considering near-field propagation. Results quantify\nthe benefit of different RIS geometric compactness and provide their applicable\nranges.",
    "descriptor": "",
    "authors": [
      "Zhuangzhuang Cui",
      "Sofie Pollin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16695"
  },
  {
    "id": "arXiv:2210.16703",
    "title": "Analog Twin Framework for Human and AI Supervisory Control and  Teleoperation of Robots",
    "abstract": "Resource-constrained mobile robots that lack the capability to be completely\nautonomous can rely on a human or AI supervisor acting at a remote site (e.g.,\ncontrol station or cloud) for their control. Such a supervised autonomy or\ncloud-based control of a robot poses high networking and computing capabilities\nrequirements at both sites, which are not easy to achieve. This paper\nintroduces and analyzes a new analog twin framework by synchronizing mobility\nbetween two mobile robots, where one robot acts as an analog twin to the other\nrobot. We devise a novel priority-based supervised bilateral teleoperation\nstrategy for goal navigation tasks to validate the proposed framework. The\npractical implementation of a supervised control strategy on this framework\nentails a mobile robot system divided into a Master-Client scheme over a\ncommunication channel where the Client robot resides on the site of operation\nguided by the Master robot through an agent (human or AI) from a remote\nlocation. The Master robot controls the Client robot with its autonomous\nnavigation algorithm, which reacts to the predictive force received from the\nClient robot. We analyze the proposed strategy in terms of network performance\n(throughput and delay), task performance (tracking error and goal reach\naccuracy), and computing efficiency (memory and CPU utilization). Extensive\nsimulations and real-world experiments demonstrate the method's novelty,\nflexibility, and versatility in realizing reactive planning applications with\nremote computational offloading capabilities compared to conventional\noffloading schemes.",
    "descriptor": "",
    "authors": [
      "Nazish Tahir",
      "Ramviyas Parasuraman"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16703"
  },
  {
    "id": "arXiv:2210.16707",
    "title": "Index Reduction for Degenerated Differential-Algebraic Equations by  Embedding",
    "abstract": "To find consistent initial data points for a system of differential-algebraic\nequations, requires the identification of its missing constraints. An efficient\nclass of structural methods exploiting a dependency graph for this task was\ninitiated by Pantiledes. More complete methods rely on differential-algebraic\ngeometry but suffer from other issues (e.g. high complexity). In this paper we\ngive a new class of efficient structural methods combined with new tools from\nnumerical real algebraic geometry that has much improved completeness\nproperties. Existing structural methods may fail for a system of\ndifferential-algebraic equations if its Jacobian matrix after differentiation\nis still singular due to symbolic cancellation or numerical degeneration.\nExisting structural methods can only handle degenerated cases caused by\nsymbolic cancellation. However, if a system has parameters, then its parametric\nJacobian matrix may be still singular after application of the structural\nmethod for certain values of the parameters. This case is called numerical\ndegeneration.\nFor polynomially nonlinear systems of differential-algebraic equations,\nnumerical methods are given to solve both degenerated cases using numerical\nreal algebraic geometry. First, we introduce a witness point method, which\nproduces at least one witness point on every constraint component. This can\nhelp to ensure constant rank and detection of degeneration on all components of\nsuch systems. Secondly, we present a Constant Rank Embedding Lemma, and based\non it propose an Index Reduction by Embedding (IRE) method which can construct\nan equivalent system with a full rank Jacobian matrix. Thirdly, IRE leads to a\nglobal structural differentiation method, to solve degenerated\ndifferential-algebraic equations on all components numerically. Application\nexamples from circuits, mechanics, are used to demonstrate our method.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2111.08160\n",
    "authors": [
      "Wenqiang Yang",
      "Wenyuan Wu",
      "Greg Reid"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.16707"
  },
  {
    "id": "arXiv:2210.16708",
    "title": "Data-driven low-dimensional dynamic model of Kolmogorov flow",
    "abstract": "Reduced order models (ROMs) that capture flow dynamics are of interest for\ndecreasing computational costs for simulation as well as for model-based\ncontrol approaches. This work presents a data-driven framework for\nminimal-dimensional models that effectively capture the dynamics and properties\nof the flow. We apply this to Kolmogorov flow in a regime consisting of chaotic\nand intermittent behavior, which is common in many flows processes and is\nchallenging to model. The trajectory of the flow travels near relative periodic\norbits (RPOs), interspersed with sporadic bursting events corresponding to\nexcursions between the regions containing the RPOs. The first step in\ndevelopment of the models is use of an undercomplete autoencoder to map from\nthe full state data down to a latent space of dramatically lower dimension.\nThen models of the discrete-time evolution of the dynamics in the latent space\nare developed. By analyzing the model performance as a function of latent space\ndimension we can estimate the minimum number of dimensions required to capture\nthe system dynamics. To further reduce the dimension of the dynamical model, we\nfactor out a phase variable in the direction of translational invariance for\nthe flow, leading to separate evolution equations for the pattern and phase\ndynamics. At a model dimension of five for the pattern dynamics, as opposed to\nthe full state dimension of 1024 (i.e. a 32x32 grid), accurate predictions are\nfound for individual trajectories out to about two Lyapunov times, as well as\nfor long-time statistics. The nearly heteroclinic connections between the\ndifferent RPOs, including the quiescent and bursting time scales, are well\ncaptured. We also capture key features of the phase dynamics. Finally, we use\nthe low-dimensional representation to predict future bursting events, finding\ngood success.",
    "descriptor": "",
    "authors": [
      "Carlos E. P\u00e9rez De Jes\u00fas",
      "Michael D. Graham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2210.16708"
  },
  {
    "id": "arXiv:2210.16712",
    "title": "Model-Free Learning of Optimal Beamformers for Passive IRS-Assisted  Sumrate Maximization",
    "abstract": "Although Intelligent Reflective Surfaces (IRSs) are a cost-effective\ntechnology promising high spectral efficiency in future wireless networks,\nobtaining optimal IRS beamformers is a challenging problem with several\npractical limitations. Assuming fully-passive, sensing-free IRS operation, we\nintroduce a new data-driven Zeroth-order Stochastic Gradient Ascent (ZoSGA)\nalgorithm for sumrate optimization in an IRS-aided downlink setting. ZoSGA does\nnot require access to channel model or network structure information, and\nenables learning of optimal long-term IRS beamformers jointly with standard\nshort-term precoding, based only on conventional effective channel state\ninformation. Supported by state-of-the-art (SOTA) convergence analysis,\ndetailed simulations confirm that ZoSGA exhibits SOTA empirical behavior as\nwell, consistently outperforming standard fully model-based baselines, in a\nvariety of scenarios.",
    "descriptor": "",
    "authors": [
      "Hassaan Hashmi",
      "Spyridon Pougkakiotis",
      "Dionysios S. Kalogerias"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16712"
  },
  {
    "id": "arXiv:2210.16716",
    "title": "Transmit Optimization for Multi-functional MIMO Systems Integrating  Sensing, Communication, and Powering",
    "abstract": "This paper unifies integrated sensing and communication (ISAC) and\nsimultaneous wireless information and power transfer (SWIPT), by investigating\na new multi-functional multiple-input multiple-output (MIMO) system integrating\nwireless sensing, communication, and powering. In this system, one\nmulti-antenna hybrid access point (H-AP) transmits wireless signals to\ncommunicate with one multi-antenna information decoding (ID) receiver,\nwirelessly charge one multi-antenna energy harvesting (EH) receiver, and\nperform radar sensing for a point target based on the echo signal at the same\ntime. Under this setup, we aim to reveal the fundamental performance tradeoff\nlimits of sensing, communication, and powering, in terms of the estimation\nCram{\\'e}r-Rao bound (CRB), achievable communication rate, and harvested energy\nlevel, respectively. Towards this end, we define the achievable CRB-rate-energy\n(C-R-E) region and characterize its Pareto boundary by maximizing the\nachievable rate at the ID receiver, subject to the estimation CRB requirement\nfor target sensing, the harvested energy requirement at the EH receiver, and\nthe maximum transmit power constraint at the H-AP. We obtain the\nsemi-closed-form optimal transmit covariance solution to the formulated problem\nby applying advanced convex optimization techniques. Numerical results show the\noptimal C-R-E region boundary achieved by our proposed design, as compared to\nthe benchmark scheme based on time switching.",
    "descriptor": "\nComments: 7 pages,4 figures, ICC-WC 2023\n",
    "authors": [
      "Yilong Chen",
      "Haocheng Hua",
      "Jie Xu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16716"
  },
  {
    "id": "arXiv:2210.16719",
    "title": "Multi-view Multi-label Anomaly Network Traffic Classification based on  MLP-Mixer Neural Network",
    "abstract": "Network traffic classification is the basis of many network security\napplications and has attracted enough attention in the field of cyberspace\nsecurity. Existing network traffic classification based on convolutional neural\nnetworks (CNNs) often emphasizes local patterns of traffic data while ignoring\nglobal information associations. In this paper, we propose a MLP-Mixer based\nmulti-view multi-label neural network for network traffic classification.\nCompared with the existing CNN-based methods, our method adopts the MLP-Mixer\nstructure, which is more in line with the structure of the packet than the\nconventional convolution operation. In our method, the packet is divided into\nthe packet header and the packet body, together with the flow features of the\npacket as input from different views. We utilize a multi-label setting to learn\ndifferent scenarios simultaneously to improve the classification performance by\nexploiting the correlations between different scenarios. Taking advantage of\nthe above characteristics, we propose an end-to-end network traffic\nclassification method. We conduct experiments on three public datasets, and the\nexperimental results show that our method can achieve superior performance.",
    "descriptor": "\nComments: 15 pages,6 figures\n",
    "authors": [
      "Yu Zheng",
      "Zhangxuan Dang",
      "Chunlei Peng",
      "Chao Yang",
      "Xinbo Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16719"
  },
  {
    "id": "arXiv:2210.16721",
    "title": "Exemplar Guided Deep Neural Network for Spatial Transcriptomics Analysis  of Gene Expression Prediction",
    "abstract": "Spatial transcriptomics (ST) is essential for understanding diseases and\ndeveloping novel treatments. It measures gene expression of each fine-grained\narea (i.e., different windows) in the tissue slide with low throughput. This\npaper proposes an Exemplar Guided Network (EGN) to accurately and efficiently\npredict gene expression directly from each window of a tissue slide image. We\napply exemplar learning to dynamically boost gene expression prediction from\nnearest/similar exemplars of a given tissue slide image window. Our EGN\nframework composes of three main components: 1) an extractor to structure a\nrepresentation space for unsupervised exemplar retrievals; 2) a vision\ntransformer (ViT) backbone to progressively extract representations of the\ninput window; and 3) an Exemplar Bridging (EB) block to adaptively revise the\nintermediate ViT representations by using the nearest exemplars. Finally, we\ncomplete the gene expression prediction task with a simple attention-based\nprediction block. Experiments on standard benchmark datasets indicate the\nsuperiority of our approach when comparing with the past state-of-the-art\n(SOTA) methods.",
    "descriptor": "",
    "authors": [
      "Yan Yang",
      "Md Zakir Hossain",
      "Eric A Stone",
      "Shafin Rahman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16721"
  },
  {
    "id": "arXiv:2210.16728",
    "title": "ISG: I can See Your Gene Expression",
    "abstract": "This paper aims to predict gene expression from a histology slide image\nprecisely. Such a slide image has a large resolution and sparsely distributed\ntextures. These obstruct extracting and interpreting discriminative features\nfrom the slide image for diverse gene types prediction. Existing gene\nexpression methods mainly use general components to filter textureless regions,\nextract features, and aggregate features uniformly across regions. However,\nthey ignore gaps and interactions between different image regions and are\ntherefore inferior in the gene expression task. Instead, we present ISG\nframework that harnesses interactions among discriminative features from\ntexture-abundant regions by three new modules: 1) a Shannon Selection module,\nbased on the Shannon information content and Solomonoff's theory, to filter out\ntextureless image regions; 2) a Feature Extraction network to extract\nexpressive low-dimensional feature representations for efficient region\ninteractions among a high-resolution image; 3) a Dual Attention network attends\nto regions with desired gene expression features and aggregates them for the\nprediction task. Extensive experiments on standard benchmark datasets show that\nthe proposed ISG framework outperforms state-of-the-art methods significantly.",
    "descriptor": "",
    "authors": [
      "Yan Yang",
      "LiYuan Pan",
      "Liu Liu",
      "Eric A Stone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16728"
  },
  {
    "id": "arXiv:2210.16730",
    "title": "Graph Fuzzy System: Concepts, Models and Algorithms",
    "abstract": "Fuzzy systems (FSs) have enjoyed wide applications in various fields,\nincluding pattern recognition, intelligent control, data mining and\nbioinformatics, which is attributed to the strong interpretation and learning\nability. In traditional application scenarios, FSs are mainly applied to model\nEuclidean space data and cannot be used to handle graph data of non-Euclidean\nstructure in nature, such as social networks and traffic route maps. Therefore,\ndevelopment of FS modeling method that is suitable for graph data and can\nretain the advantages of traditional FSs is an important research. To meet this\nchallenge, a new type of FS for graph data modeling called Graph Fuzzy System\n(GFS) is proposed in this paper, where the concepts, modeling framework and\nconstruction algorithms are systematically developed. First, GFS related\nconcepts, including graph fuzzy rule base, graph fuzzy sets and graph\nconsequent processing unit (GCPU), are defined. A GFS modeling framework is\nthen constructed and the antecedents and consequents of the GFS are presented\nand analyzed. Finally, a learning framework of GFS is proposed, in which a\nkernel K-prototype graph clustering (K2PGC) is proposed to develop the\nconstruction algorithm for the GFS antecedent generation, and then based on\ngraph neural network (GNNs), consequent parameters learning algorithm is\nproposed for GFS. Specifically, three different versions of the GFS\nimplementation algorithm are developed for comprehensive evaluations with\nexperiments on various benchmark graph classification datasets. The results\ndemonstrate that the proposed GFS inherits the advantages of both existing\nmainstream GNNs methods and conventional FSs methods while achieving better\nperformance than the counterparts.",
    "descriptor": "\nComments: This paper has been submitted to a journal\n",
    "authors": [
      "Fuping Hu",
      "Zhaohong Deng",
      "Zhenping Xie",
      "Kup-Sze Choi",
      "Shitong Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16730"
  },
  {
    "id": "arXiv:2210.16732",
    "title": "How Far are We from Robust Long Abstractive Summarization?",
    "abstract": "Abstractive summarization has made tremendous progress in recent years. In\nthis work, we perform fine-grained human annotations to evaluate long document\nabstractive summarization systems (i.e., models and metrics) with the aim of\nimplementing them to generate reliable summaries. For long document abstractive\nmodels, we show that the constant strive for state-of-the-art ROUGE results can\nlead us to generate more relevant summaries but not factual ones. For long\ndocument evaluation metrics, human evaluation results show that ROUGE remains\nthe best at evaluating the relevancy of a summary. It also reveals important\nlimitations of factuality metrics in detecting different types of factual\nerrors and the reasons behind the effectiveness of BARTScore. We then suggest\npromising directions in the endeavor of developing factual consistency metrics.\nFinally, we release our annotated long document dataset with the hope that it\ncan contribute to the development of metrics across a broader range of\nsummarization settings.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Huan Yee Koh",
      "Jiaxin Ju",
      "He Zhang",
      "Ming Liu",
      "Shirui Pan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16732"
  },
  {
    "id": "arXiv:2210.16735",
    "title": "Online Convex Optimization with Long Term Constraints for Predictable  Sequences",
    "abstract": "In this paper, we investigate the framework of Online Convex Optimization\n(OCO) for online learning. OCO offers a very powerful online learning framework\nfor many applications. In this context, we study a specific framework of OCO\ncalled {\\it OCO with long term constraints}. Long term constraints are\nintroduced typically as an alternative to reduce the complexity of the\nprojection at every update step in online optimization. While many algorithmic\nadvances have been made towards online optimization with long term constraints,\nthese algorithms typically assume that the sequence of cost functions over a\ncertain $T$ finite steps that determine the cost to the online learner are\nadversarially generated. In many circumstances, the sequence of cost functions\nmay not be unrelated, and thus predictable from those observed till a point of\ntime. In this paper, we study the setting where the sequences are predictable.\nWe present a novel online optimization algorithm for online optimization with\nlong term constraints that can leverage such predictability. We show that, with\na predictor that can supply the gradient information of the next function in\nthe sequence, our algorithm can achieve an overall regret and constraint\nviolation rate that is strictly less than the rate that is achievable without\nprediction.",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Jianjun Yuan",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16735"
  },
  {
    "id": "arXiv:2210.16740",
    "title": "Search to Pass Messages for Temporal Knowledge Graph Completion",
    "abstract": "Completing missing facts is a fundamental task for temporal knowledge graphs\n(TKGs). Recently, graph neural network (GNN) based methods, which can\nsimultaneously explore topological and temporal information, have become the\nstate-of-the-art (SOTA) to complete TKGs. However, these studies are based on\nhand-designed architectures and fail to explore the diverse topological and\ntemporal properties of TKG. To address this issue, we propose to use neural\narchitecture search (NAS) to design data-specific message passing architecture\nfor TKG completion. In particular, we develop a generalized framework to\nexplore topological and temporal information in TKGs. Based on this framework,\nwe design an expressive search space to fully capture various properties of\ndifferent TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet\nstructure by sampling single path for efficient search with less cost. We\nfurther conduct extensive experiments on three benchmark datasets. The results\nshow that the searched architectures by our method achieve the SOTA\nperformances. Besides, the searched models can also implicitly reveal diverse\nproperties in different TKGs. Our code is released in\nhttps://github.com/striderdu/SPA.",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Zhen Wang",
      "Haotong Du",
      "Quanming Yao",
      "Xuelong Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16740"
  },
  {
    "id": "arXiv:2210.16741",
    "title": "Versatile Semantic Coded Transmission over MIMO Fading Channels",
    "abstract": "Semantic communications have shown great potential to boost the end-to-end\ntransmission performance. To further improve the system efficiency, in this\npaper, we propose a class of novel semantic coded transmission (SCT) schemes\nover multiple-input multiple-output (MIMO) fading channels. In particular, we\npropose a high-efficiency SCT system supporting concurrent transmission of\nmultiple streams, which can maximize the multiplexing gain of end-to-end\nsemantic communication system. By jointly considering the entropy distribution\non the source semantic features and the wireless MIMO channel states, we design\na spatial multiplexing mechanism to realize adaptive coding rate allocation and\nstream mapping. As a result, source content and channel environment will be\nseamlessly coupled, which maximizes the coding gain of SCT system. Moreover,\nour SCT system is versatile: a single model can support various transmission\nrates. The whole model is optimized under the constraint of transmission\nrate-distortion (RD) tradeoff. Experimental results verify that our scheme\nsubstantially increases the throughput of semantic communication system. It\nalso outperforms traditional MIMO communication systems under realistic fading\nchannels.",
    "descriptor": "",
    "authors": [
      "Shengshi Yao",
      "Sixian Wang",
      "Jincheng Dai",
      "Kai Niu",
      "Ping Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16741"
  },
  {
    "id": "arXiv:2210.16742",
    "title": "On-the-fly Object Detection using StyleGAN with CLIP Guidance",
    "abstract": "We present a fully automated framework for building object detectors on\nsatellite imagery without requiring any human annotation or intervention. We\nachieve this by leveraging the combined power of modern generative models\n(e.g., StyleGAN) and recent advances in multi-modal learning (e.g., CLIP).\nWhile deep generative models effectively encode the key semantics pertinent to\na data distribution, this information is not immediately accessible for\ndownstream tasks, such as object detection. In this work, we exploit CLIP's\nability to associate image features with text descriptions to identify neurons\nin the generator network, which are subsequently used to build detectors\non-the-fly.",
    "descriptor": "",
    "authors": [
      "Yuzhe Lu",
      "Shusen Liu",
      "Jayaraman J. Thiagarajan",
      "Wesam Sakla",
      "Rushil Anirudh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16742"
  },
  {
    "id": "arXiv:2210.16744",
    "title": "gMeta: Template-based Regular Expression Generation over Noisy Examples",
    "abstract": "Regular expression is a technology widely used in software development for\nextracting textual data, validating the structure of textual documents, or\nformatting data. Existing regular expression generation works always assumes\nthat the examples are faultless, however, in real industrial scenarios, this\nassumption does not fully hold. In this paper, we present a simple but\neffective templated-based approach to generate regular expressions over noisy\nexamples. Specifically, we design an abstract data form (namely, MetaParam) to\napproximately describe and cluster the input examples. Then, we propose a\npractical dynamic thresholding scheme to filter out anomalous examples.\nFinally, we design a template-based regular expression generation algorithm,\nwhich is efficient, interpretable and extensible. We performed an experimental\nevaluation on two different extraction tasks applied to realworld datasets and\nobtained promising results in terms of precision. Moreover, gMeta achieves\nexcellent results in real industrial scenarios.",
    "descriptor": "",
    "authors": [
      "Shujun Wang",
      "Yongqiang Tian andDengcheng He"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.16744"
  },
  {
    "id": "arXiv:2210.16751",
    "title": "Formalizing Statistical Causality via Modal Logic",
    "abstract": "We propose a formal language for describing and explaining statistical\ncausality. Concretely, we define Statistical Causality Language (StaCL) for\nspecifying causal effects on random variables. StaCL incorporates modal\noperators for interventions to express causal properties between probability\ndistributions in different possible worlds in a Kripke model. We formalize\naxioms for probability distributions, interventions, and causal predicates\nusing StaCL formulas. These axioms are expressive enough to derive the rules of\nPearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used\nto prove and explain the correctness of statistical causal inference.",
    "descriptor": "",
    "authors": [
      "Yusuke Kawamoto",
      "Sato Tetsuya",
      "Kohei Suenaga"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.16751"
  },
  {
    "id": "arXiv:2210.16754",
    "title": "Mitigating Unfairness via Evolutionary Multi-objective Ensemble Learning",
    "abstract": "In the literature of mitigating unfairness in machine learning, many fairness\nmeasures are designed to evaluate predictions of learning models and also\nutilised to guide the training of fair models. It has been theoretically and\nempirically shown that there exist conflicts and inconsistencies among accuracy\nand multiple fairness measures. Optimising one or several fairness measures may\nsacrifice or deteriorate other measures. Two key questions should be\nconsidered, how to simultaneously optimise accuracy and multiple fairness\nmeasures, and how to optimise all the considered fairness measures more\neffectively. In this paper, we view the mitigating unfairness problem as a\nmulti-objective learning problem considering the conflicts among fairness\nmeasures. A multi-objective evolutionary learning framework is used to\nsimultaneously optimise several metrics (including accuracy and multiple\nfairness measures) of machine learning models. Then, ensembles are constructed\nbased on the learning models in order to automatically balance different\nmetrics. Empirical results on eight well-known datasets demonstrate that\ncompared with the state-of-the-art approaches for mitigating unfairness, our\nproposed algorithm can provide decision-makers with better tradeoffs among\naccuracy and multiple fairness metrics. Furthermore, the high-quality models\ngenerated by the framework can be used to construct an ensemble to\nautomatically achieve a better tradeoff among all the considered fairness\nmetrics than other ensemble methods. Our code is publicly available at\nhttps://github.com/qingquan63/FairEMOL",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Zhang Qingquan",
      "Liu Jialin",
      "Zhang Zeqi",
      "Wen Junyi",
      "Mao Bifei",
      "Yao Xin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.16754"
  },
  {
    "id": "arXiv:2210.16755",
    "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired  Speech and Text",
    "abstract": "Self-supervised pre-training has been successful in both text and speech\nprocessing. Speech and text offer different but complementary information. The\nquestion is whether we are able to perform a speech-text joint pre-training on\nunpaired speech and text. In this paper, we take the idea of self-supervised\npre-training one step further and propose token2vec, a novel joint pre-training\nframework for unpaired speech and text based on discrete representations of\nspeech. Firstly, due to the distinct characteristics between speech and text\nmodalities, where speech is continuous while text is discrete, we first\ndiscretize speech into a sequence of discrete speech tokens to solve the\nmodality mismatch problem. Secondly, to solve the length mismatch problem,\nwhere the speech sequence is usually much longer than text sequence, we convert\nthe words of text into phoneme sequences and randomly repeat each phoneme in\nthe sequences. Finally, we feed the discrete speech and text tokens into a\nmodality-agnostic Transformer encoder and pre-train with token-level masking\nlanguage modeling (tMLM). Experiments show that token2vec is significantly\nsuperior to various speech-only pre-training baselines, with up to 17.7%\nrelative WER reduction. Token2vec model is also validated on a non-ASR task,\ni.e., spoken intent classification, and shows good transferability.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Xianghu Yue",
      "Junyi Ao",
      "Xiaoxue Gao",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16755"
  },
  {
    "id": "arXiv:2210.16765",
    "title": "Benchmarking Adversarial Patch Against Aerial Detection",
    "abstract": "DNNs are vulnerable to adversarial examples, which poses great security\nconcerns for security-critical systems. In this paper, a novel\nadaptive-patch-based physical attack (AP-PA) framework is proposed, which aims\nto generate adversarial patches that are adaptive in both physical dynamics and\nvarying scales, and by which the particular targets can be hidden from being\ndetected. Furthermore, the adversarial patch is also gifted with attack\neffectiveness against all targets of the same class with a patch outside the\ntarget (No need to smear targeted objects) and robust enough in the physical\nworld. In addition, a new loss is devised to consider more available\ninformation of detected objects to optimize the adversarial patch, which can\nsignificantly improve the patch's attack efficacy (Average precision drop up to\n87.86% and 85.48% in white-box and black-box settings, respectively) and\noptimizing efficiency. We also establish one of the first comprehensive,\ncoherent, and rigorous benchmarks to evaluate the attack efficacy of\nadversarial patches on aerial detection tasks. Finally, several proportionally\nscaled experiments are performed physically to demonstrate that the elaborated\nadversarial patches can successfully deceive aerial detection algorithms in\ndynamic physical circumstances. The code is available at\nhttps://github.com/JiaweiLian/AP-PA.",
    "descriptor": "\nComments: 14 pages, 14 figures\n",
    "authors": [
      "Jiawei Lian",
      "Shaohui Mei",
      "Shun Zhang",
      "Mingyang Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16765"
  },
  {
    "id": "arXiv:2210.16771",
    "title": "Parameter-Efficient Tuning Makes a Good Classification Head",
    "abstract": "In recent years, pretrained models revolutionized the paradigm of natural\nlanguage understanding (NLU), where we append a randomly initialized\nclassification head after the pretrained backbone, e.g. BERT, and finetune the\nwhole model. As the pretrained backbone makes a major contribution to the\nimprovement, we naturally expect a good pretrained classification head can also\nbenefit the training. However, the final-layer output of the backbone, i.e. the\ninput of the classification head, will change greatly during finetuning, making\nthe usual head-only pretraining (LP-FT) ineffective. In this paper, we find\nthat parameter-efficient tuning makes a good classification head, with which we\ncan simply replace the randomly initialized heads for a stable performance\ngain. Our experiments demonstrate that the classification head jointly\npretrained with parameter-efficient tuning consistently improves the\nperformance on 9 tasks in GLUE and SuperGLUE.",
    "descriptor": "",
    "authors": [
      "Zhuoyi Yang",
      "Ming Ding",
      "Yanhui Guo",
      "Qingsong Lv",
      "Jie Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16771"
  },
  {
    "id": "arXiv:2210.16773",
    "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP  Tasks",
    "abstract": "Access to external knowledge is essential for many natural language\nprocessing tasks, such as question answering and dialogue. Existing methods\noften rely on a parametric model that stores knowledge in its parameters, or\nuse a retrieval-augmented model that has access to an external knowledge\nsource. Parametric and retrieval-augmented models have complementary strengths\nin terms of computational efficiency and predictive accuracy. To combine the\nstrength of both approaches, we propose the Efficient Memory-Augmented\nTransformer (EMAT) -- it encodes external knowledge into a key-value memory and\nexploits the fast maximum inner product search for memory querying. We also\nintroduce pre-training tasks that allow EMAT to encode informative key-value\nrepresentations, and to learn an implicit strategy to integrate multiple memory\nslots into the transformer. Experiments on various knowledge-intensive tasks\nsuch as question answering and dialogue datasets show that, simply augmenting\nparametric models (T5-base) using our method produces more accurate results\n(e.g., 25.8 -> 44.3 EM on NQ) while retaining a high throughput (e.g., 1000\nqueries/s on NQ). Compared to retrieval-augmented models, EMAT runs\nsubstantially faster across the board and produces more accurate results on WoW\nand ELI5. Our code and datasets are available at https://github.\ncom/uclnlp/EMAT.",
    "descriptor": "\nComments: EMNLP 2022 main conference long paper. 8 pages, 6 figures\n",
    "authors": [
      "Yuxiang Wu",
      "Yu Zhao",
      "Baotian Hu",
      "Pasquale Minervini",
      "Pontus Stenetorp",
      "Sebastian Riedel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16773"
  },
  {
    "id": "arXiv:2210.16774",
    "title": "Dataset Distillation via Factorization",
    "abstract": "In this paper, we study \\xw{dataset distillation (DD)}, from a novel\nperspective and introduce a \\emph{dataset factorization} approach, termed\n\\emph{HaBa}, which is a plug-and-play strategy portable to any existing DD\nbaseline. Unlike conventional DD approaches that aim to produce distilled and\nrepresentative samples, \\emph{HaBa} explores decomposing a dataset into two\ncomponents: data \\emph{Ha}llucination networks and \\emph{Ba}ses, where the\nlatter is fed into the former to reconstruct image samples. The flexible\ncombinations between bases and hallucination networks, therefore, equip the\ndistilled data with exponential informativeness gain, which largely increase\nthe representation capability of distilled datasets. To furthermore increase\nthe data efficiency of compression results, we further introduce a pair of\nadversarial contrastive constraints on the resultant hallucination networks and\nbases, which increase the diversity of generated images and inject more\ndiscriminant information into the factorization. Extensive comparisons and\nexperiments demonstrate that our method can yield significant improvement on\ndownstream classification tasks compared with previous state of the arts, while\nreducing the total number of compressed parameters by up to 65\\%. Moreover,\ndistilled datasets by our approach also achieve \\textasciitilde10\\% higher\naccuracy than baseline methods in cross-architecture generalization. Our code\nis available \\href{https://github.com/Huage001/DatasetFactorization}{here}.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Songhua Liu",
      "Kai Wang",
      "Xingyi Yang",
      "Jingwen Ye",
      "Xinchao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16774"
  },
  {
    "id": "arXiv:2210.16776",
    "title": "Saliency Can Be All You Need In Contrastive Self-Supervised Learning",
    "abstract": "We propose an augmentation policy for Contrastive Self-Supervised Learning\n(SSL) in the form of an already established Salient Image Segmentation\ntechnique entitled Global Contrast based Salient Region Detection. This\ndetection technique, which had been devised for unrelated Computer Vision\ntasks, was empirically observed to play the role of an augmentation facilitator\nwithin the SSL protocol. This observation is rooted in our practical attempts\nto learn, by SSL-fashion, aerial imagery of solar panels, which exhibit\nchallenging boundary patterns. Upon the successful integration of this\ntechnique on our problem domain, we formulated a generalized procedure and\nconducted a comprehensive, systematic performance assessment with various\nContrastive SSL algorithms subject to standard augmentation techniques. This\nevaluation, which was conducted across multiple datasets, indicated that the\nproposed technique indeed contributes to SSL. We hypothesize whether salient\nimage segmentation may suffice as the only augmentation policy in Contrastive\nSSL when treating downstream segmentation tasks.",
    "descriptor": "\nComments: Accepted for the 17th International Symposium on Visual Computing (ISVC 2022)\n",
    "authors": [
      "Veysel Kocaman",
      "Ofer M. Shir",
      "Thomas B\u00e4ck",
      "Ahmed Nabil Belbachir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16776"
  },
  {
    "id": "arXiv:2210.16777",
    "title": "Symmetric Saliency-based Adversarial Attack To Speaker Identification",
    "abstract": "Adversarial attack approaches to speaker identification either need high\ncomputational cost or are not very effective, to our knowledge. To address this\nissue, in this paper, we propose a novel generation-network-based approach,\ncalled symmetric saliency-based encoder-decoder (SSED), to generate adversarial\nvoice examples to speaker identification. It contains two novel components.\nFirst, it uses a novel saliency map decoder to learn the importance of speech\nsamples to the decision of a targeted speaker identification system, so as to\nmake the attacker focus on generating artificial noise to the important\nsamples. It also proposes an angular loss function to push the speaker\nembedding far away from the source speaker. Our experimental results\ndemonstrate that the proposed SSED yields the state-of-the-art performance,\ni.e. over 97% targeted attack success rate and a signal-to-noise level of over\n39 dB on both the open-set and close-set speaker identification tasks, with a\nlow computational cost.",
    "descriptor": "",
    "authors": [
      "Jiadi Yao",
      "Xing Chen",
      "Xiao-Lei Zhang",
      "Wei-Qiang Zhang",
      "Kunde Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16777"
  },
  {
    "id": "arXiv:2210.16780",
    "title": "Recognizing Handwriting Styles in a Historical Scanned Document Using  Scikit-Fuzzy c-means Clustering",
    "abstract": "The forensic attribution of the handwriting in a digitized document to\nmultiple scribes is a challenging problem of high dimensionality. Unique\nhandwriting styles may be dissimilar in a blend of several factors including\ncharacter size, stroke width, loops, ductus, slant angles, and cursive\nligatures. Previous work on labeled data with Hidden Markov models, support\nvector machines, and semi-supervised recurrent neural networks have provided\nmoderate to high success. In this study, we successfully detect hand shifts in\na historical manuscript through fuzzy soft clustering in combination with\nlinear principal component analysis. This advance demonstrates the successful\ndeployment of unsupervised methods for writer attribution of historical\ndocuments and forensic document analysis.",
    "descriptor": "\nComments: 21 pages in total, 4 figures and 1 table\n",
    "authors": [
      "Sriparna Majumdar",
      "Aaron Brick"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16780"
  },
  {
    "id": "arXiv:2210.16782",
    "title": "Unsupervised Learning of Structured Representations via Closed-Loop  Transcription",
    "abstract": "This paper proposes an unsupervised method for learning a unified\nrepresentation that serves both discriminative and generative purposes. While\nmost existing unsupervised learning approaches focus on a representation for\nonly one of these two goals, we show that a unified representation can enjoy\nthe mutual benefits of having both. Such a representation is attainable by\ngeneralizing the recently proposed \\textit{closed-loop transcription}\nframework, known as CTRL, to the unsupervised setting. This entails solving a\nconstrained maximin game over a rate reduction objective that expands features\nof all samples while compressing features of augmentations of each sample.\nThrough this process, we see discriminative low-dimensional structures emerge\nin the resulting representations. Under comparable experimental conditions and\nnetwork complexities, we demonstrate that these structured representations\nenable classification performance close to state-of-the-art unsupervised\ndiscriminative representations, and conditionally generated image quality\nsignificantly higher than that of state-of-the-art unsupervised generative\nmodels. Source code can be found at https://github.com/Delay-Xili/uCTRL.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Shengbang Tong",
      "Xili Dai",
      "Yubei Chen",
      "Mingyang Li",
      "Zengyi Li",
      "Brent Yi",
      "Yann LeCun",
      "Yi Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16782"
  },
  {
    "id": "arXiv:2210.16785",
    "title": "CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a  Deck of Playing Cards",
    "abstract": "Presence in virtual reality (VR) is meaningful for remotely connecting with\nothers and facilitating social interactions despite great distance while\nproviding a sense of \"being there.\" This work presents CardsVR, a two-person VR\nexperience that allows remote participants to play a game of cards together. An\nentire deck of tracked cards are used to recreate the sense of playing cards\nin-person. Prior work in VR commonly provides passive haptic feedback either\nthrough a single object or through static objects in the environment. CardsVR\nis novel in providing passive haptic feedback through multiple cards that are\nindividually tracked and represented in the virtual environment. Participants\ninteract with the physical cards by picking them up, holding them, playing\nthem, or moving them on the physical table. Our participant study (N=23) shows\nthat passive haptic feedback provides significant improvement in three standard\nmeasures of presence: Possibility to Act, Realism, and Haptics.",
    "descriptor": "",
    "authors": [
      "Andrew Huard",
      "Mengyu Chen",
      "Misha Sra"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16785"
  },
  {
    "id": "arXiv:2210.16786",
    "title": "Explainable Predictive Decision Mining for Operational Support",
    "abstract": "Several decision points exist in business processes (e.g., whether a purchase\norder needs a manager's approval or not), and different decisions are made for\ndifferent process instances based on their characteristics (e.g., a purchase\norder higher than $500 needs a manager approval). Decision mining in process\nmining aims to describe/predict the routing of a process instance at a decision\npoint of the process. By predicting the decision, one can take proactive\nactions to improve the process. For instance, when a bottleneck is developing\nin one of the possible decisions, one can predict the decision and bypass the\nbottleneck. However, despite its huge potential for such operational support,\nexisting techniques for decision mining have focused largely on describing\ndecisions but not on predicting them, deploying decision trees to produce\nlogical expressions to explain the decision. In this work, we aim to enhance\nthe predictive capability of decision mining to enable proactive operational\nsupport by deploying more advanced machine learning algorithms. Our proposed\napproach provides explanations of the predicted decisions using SHAP values to\nsupport the elicitation of proactive actions. We have implemented a Web\napplication to support the proposed approach and evaluated the approach using\nthe implementation.",
    "descriptor": "",
    "authors": [
      "Gyunam Park",
      "Aaron K\u00fcsters",
      "Mara Tews",
      "Cameron Pitsch",
      "Jonathan Schneider",
      "Wil M. P. van der Aalst"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16786"
  },
  {
    "id": "arXiv:2210.16788",
    "title": "Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation",
    "abstract": "RGB-based 3D hand pose estimation has been successful for decades thanks to\nlarge-scale databases and deep learning. However, the hand pose estimation\nnetwork does not operate well for hand pose images whose characteristics are\nfar different from the training data. This is caused by various factors such as\nilluminations, camera angles, diverse backgrounds in the input images, etc.\nMany existing methods tried to solve it by supplying additional large-scale\nunconstrained/target domain images to augment data space; however collecting\nsuch large-scale images takes a lot of labors. In this paper, we present a\nsimple image-free domain generalization approach for the hand pose estimation\nframework that uses only source domain data. We try to manipulate the image\nfeatures of the hand pose estimation network by adding the features from text\ndescriptions using the CLIP (Contrastive Language-Image Pre-training) model.\nThe manipulated image features are then exploited to train the hand pose\nestimation network via the contrastive learning framework. In experiments with\nSTB and RHD datasets, our algorithm shows improved performance over the\nstate-of-the-art domain generalization approaches.",
    "descriptor": "",
    "authors": [
      "Seongyeong Lee",
      "Hansoo Park",
      "Dong Uk Kim",
      "Jihyeon Kim",
      "Muhammadjon Boboev",
      "Seungryul Baek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16788"
  },
  {
    "id": "arXiv:2210.16789",
    "title": "STGC-GNNs: A GNN-based traffic prediction framework with a  spatial-temporal Granger causality graph",
    "abstract": "The key to traffic prediction is to accurately depict the temporal dynamics\nof traffic flow traveling in a road network, so it is important to model the\nspatial dependence of the road network. The essence of spatial dependence is to\naccurately describe how traffic information transmission is affected by other\nnodes in the road network, and the GNN-based traffic prediction model, as a\nbenchmark for traffic prediction, has become the most common method for the\nability to model spatial dependence by transmitting traffic information with\nthe message passing mechanism. However, existing methods model a local and\nstatic spatial dependence, which cannot transmit the global-dynamic traffic\ninformation (GDTi) required for long-term prediction. The challenge is the\ndifficulty of detecting the precise transmission of GDTi due to the uncertainty\nof individual transport, especially for long-term transmission. In this paper,\nwe propose a new hypothesis\\: GDTi behaves macroscopically as a transmitting\ncausal relationship (TCR) underlying traffic flow, which remains stable under\ndynamic changing traffic flow. We further propose spatial-temporal Granger\ncausality (STGC) to express TCR, which models global and dynamic spatial\ndependence. To model global transmission, we model the causal order and causal\nlag of TCRs global transmission by a spatial-temporal alignment algorithm. To\ncapture dynamic spatial dependence, we approximate the stable TCR underlying\ndynamic traffic flow by a Granger causality test. The experimental results on\nthree backbone models show that using STGC to model the spatial dependence has\nbetter results than the original model for 45 min and 1 h long-term prediction.",
    "descriptor": "\nComments: 14 pages, 16 figures, 4 tables\n",
    "authors": [
      "Silu He",
      "Qinyao Luo",
      "Ronghua Du",
      "Ling Zhao",
      "Haifeng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16789"
  },
  {
    "id": "arXiv:2210.16790",
    "title": "One Gradient Frank-Wolfe for Decentralized Online Convex and Submodular  Optimization",
    "abstract": "Decentralized learning has been studied intensively in recent years motivated\nby its wide applications in the context of federated learning. The majority of\nprevious research focuses on the offline setting in which the objective\nfunction is static. However, the offline setting becomes unrealistic in\nnumerous machine learning applications that witness the change of massive data.\nIn this paper, we propose \\emph{decentralized online} algorithm for convex and\ncontinuous DR-submodular optimization, two classes of functions that are\npresent in a variety of machine learning problems. Our algorithms achieve\nperformance guarantees comparable to those in the centralized offline setting.\nMoreover, on average, each participant performs only a \\emph{single} gradient\ncomputation per time step. Subsequently, we extend our algorithms to the bandit\nsetting. Finally, we illustrate the competitive performance of our algorithms\nin real-world experiments.",
    "descriptor": "",
    "authors": [
      "Tuan-Anh Nguyen",
      "Nguyen Kim Thang",
      "Denis Trystram"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.16790"
  },
  {
    "id": "arXiv:2210.16791",
    "title": "Adaptive Speech Quality Aware Complex Neural Network for Acoustic Echo  Cancellation with Supervised Contrastive Learning",
    "abstract": "Acoustic echo cancellation (AEC) is designed to remove echoes, reverberation,\nand unwanted added sounds from the microphone signal while maintaining the\nquality of the near-end speaker's speech. This paper proposes adaptive speech\nquality complex neural networks to focus on specific tasks for real-time\nacoustic echo cancellation. In specific, we propose a complex modularize neural\nnetwork with different stages to focus on, feature extraction acoustic\nseparation, and mask optimization receptively. Furthermore, we adopt the\ncontrastive learning framework and novel speech quality aware loss functions to\nfurther improve the performance. The model is trained with 72 hours for\npre-training and then 72 hours for fine-tuning. The proposed model outperforms\nthe state-of-the-art performance.",
    "descriptor": "",
    "authors": [
      "Bozhong Liu",
      "Xiaoxi Yu",
      "Hantao Huang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16791"
  },
  {
    "id": "arXiv:2210.16795",
    "title": "Two-Level Temporal Relation Model for Online Video Instance Segmentation",
    "abstract": "In Video Instance Segmentation (VIS), current approaches either focus on the\nquality of the results, by taking the whole video as input and processing it\noffline; or on speed, by handling it frame by frame at the cost of competitive\nperformance. In this work, we propose an online method that is on par with the\nperformance of the offline counterparts. We introduce a message-passing graph\nneural network that encodes objects and relates them through time. We\nadditionally propose a novel module to fuse features from the feature pyramid\nnetwork with residual connections. Our model, trained end-to-end, achieves\nstate-of-the-art performance on the YouTube-VIS dataset within the online\nmethods. Further experiments on DAVIS demonstrate the generalization capability\nof our model to the video object segmentation task. Code is available at:\n\\url{https://github.com/caganselim/TLTM}",
    "descriptor": "",
    "authors": [
      "\u00c7a\u011fan Selim \u00c7oban",
      "O\u011fuzhan Keskin",
      "Jordi Pont-Tuset",
      "Fatma G\u00fcney"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16795"
  },
  {
    "id": "arXiv:2210.16797",
    "title": "Adaptive and Fair Deployment Approach to Balance Offload Traffic in  Multi-UAV Cellular Networks",
    "abstract": "Unmanned aerial vehicle-aided communication (UAB-BS) is a promising solution\nto establish rapid wireless connectivity in sudden/temporary crowded events\nbecause of its more flexibility and mobility features than conventional ground\nbase station (GBS). Because of these benefits, UAV-BSs can easily be deployed\nat high altitudes to provide more line of sight (LoS) links than GBS.\nTherefore, users on the ground can obtain more reliable wireless channels. In\npractice, the mobile nature of the ground user can create uneven user density\nat different times and spaces. This phenomenon leads to unbalanced user\nassociations among UAV-BSs and may cause frequent UAV-BS overload. We propose a\nthree-dimensional adaptive and fair deployment approach to solve this problem.\nThe proposed approach can jointly optimize the altitude and transmission power\nof UAV-BS to offload the traffic from overloaded UAV-BSs. The simulation\nresults show that the network performance improves by 37.71% in total capacity,\n37.48% in total energy efficiency and 16.12% in the Jain fairness index\ncompared to the straightforward greedy approach.",
    "descriptor": "\nComments: 15 pages, 9 figures, to appear in IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Chuan-Chi Lai",
      "Bhola",
      "Ang-Hsun Tsai",
      "Li-Chun Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16797"
  },
  {
    "id": "arXiv:2210.16798",
    "title": "Generate, Discriminate and Contrast: A Semi-Supervised Sentence  Representation Learning Framework",
    "abstract": "Most sentence embedding techniques heavily rely on expensive human-annotated\nsentence pairs as the supervised signals. Despite the use of large-scale\nunlabeled data, the performance of unsupervised methods typically lags far\nbehind that of the supervised counterparts in most downstream tasks. In this\nwork, we propose a semi-supervised sentence embedding framework, GenSE, that\neffectively leverages large-scale unlabeled data. Our method include three\nparts: 1) Generate: A generator/discriminator model is jointly trained to\nsynthesize sentence pairs from open-domain unlabeled corpus; 2) Discriminate:\nNoisy sentence pairs are filtered out by the discriminator to acquire\nhigh-quality positive and negative sentence pairs; 3) Contrast: A prompt-based\ncontrastive approach is presented for sentence representation learning with\nboth annotated and synthesized data. Comprehensive experiments show that GenSE\nachieves an average correlation score of 85.19 on the STS datasets and\nconsistent performance improvement on four domain adaptation tasks,\nsignificantly surpassing the state-of-the-art methods and convincingly\ncorroborating its effectiveness and generalization ability.Code, Synthetic data\nand Models available at https://github.com/MatthewCYM/GenSE.",
    "descriptor": "\nComments: Accepted in EMNLP 2022 main conference\n",
    "authors": [
      "Yiming Chen",
      "Yan Zhang",
      "Bin Wang",
      "Zuozhu Liu",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16798"
  },
  {
    "id": "arXiv:2210.16800",
    "title": "Searching for Deviations in Trading Systems: Combining Control-Flow and  Data Perspectives",
    "abstract": "Trading systems are software platforms that support the exchange of\nsecurities (e.g., company shares) between participants. In this paper, we\npresent a method to search for deviations in trading systems by checking\nconformance between colored Petri nets and event logs. Colored Petri nets\n(CPNs) are an extension of Petri nets, a formalism for modeling of distributed\nsystems. CPNs allow us to describe an expected causal ordering between system\nactivities and how data attributes of domain-related objects (e.g., orders to\ntrade) must be transformed. Event logs consist of traces corresponding to runs\nof a real system. By comparing CPNs and event logs, different types of\ndeviations can be detected. Using this method, we report the validation of a\nreal-life trading system.",
    "descriptor": "",
    "authors": [
      "Julio C. Carrasquel",
      "Irina A. Lomazova"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Discrete Mathematics (cs.DM)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.16800"
  },
  {
    "id": "arXiv:2210.16805",
    "title": "SRTNet: Time Domain Speech Enhancement Via Stochastic Refinement",
    "abstract": "Diffusion model, as a new generative model which is very popular in image\ngeneration and audio synthesis, is rarely used in speech enhancement. In this\npaper, we use the diffusion model as a module for stochastic refinement. We\npropose SRTNet, a novel method for speech enhancement via Stochastic Refinement\nin complete Time domain. Specifically, we design a joint network consisting of\na deterministic module and a stochastic module, which makes up the\n``enhance-and-refine'' paradigm. We theoretically demonstrate the feasibility\nof our method and experimentally prove that our method achieves faster\ntraining, faster sampling and higher quality. Our code and enhanced samples are\navailable at https://github.com/zhibinQiu/SRTNet.git.",
    "descriptor": "",
    "authors": [
      "Zhibin Qiu",
      "Mengfan Fu",
      "Yinfeng Yu",
      "LiLi Yin",
      "Fuchun Sun",
      "Hao Huang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16805"
  },
  {
    "id": "arXiv:2210.16807",
    "title": "The Florence 4D Facial Expression Dataset",
    "abstract": "Human facial expressions change dynamically, so their recognition / analysis\nshould be conducted by accounting for the temporal evolution of face\ndeformations either in 2D or 3D. While abundant 2D video data do exist, this is\nnot the case in 3D, where few 3D dynamic (4D) datasets were released for public\nuse. The negative consequence of this scarcity of data is amplified by current\ndeep learning based-methods for facial expression analysis that require large\nquantities of variegate samples to be effectively trained. With the aim of\nsmoothing such limitations, in this paper we propose a large dataset, named\nFlorence 4D, composed of dynamic sequences of 3D face models, where a\ncombination of synthetic and real identities exhibit an unprecedented variety\nof 4D facial expressions, with variations that include the classical\nneutral-apex transition, but generalize to expression-to-expression. All these\ncharacteristics are not exposed by any of the existing 4D datasets and they\ncannot even be obtained by combining more than one dataset. We strongly believe\nthat making such a data corpora publicly available to the community will allow\ndesigning and experimenting new applications that were not possible to\ninvestigate till now. To show at some extent the difficulty of our data in\nterms of different identities and varying expressions, we also report a\nbaseline experimentation on the proposed dataset that can be used as baseline.",
    "descriptor": "",
    "authors": [
      "F. Principi",
      "S. Berretti",
      "C. Ferrari",
      "N. Otberdout",
      "M. Daoudi",
      "A. Del Bimbo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16807"
  },
  {
    "id": "arXiv:2210.16810",
    "title": "SL3D: Self-supervised-Self-labeled 3D Recognition",
    "abstract": "There are a lot of promising results in 3D recognition, including\nclassification, object detection, and semantic segmentation. However, many of\nthese results rely on manually collecting densely annotated real-world 3D data,\nwhich is highly time-consuming and expensive to obtain, limiting the\nscalability of 3D recognition tasks. Thus in this paper, we study unsupervised\n3D recognition and propose a Self-supervised-Self-Labeled 3D Recognition (SL3D)\nframework. SL3D simultaneously solves two coupled objectives, i.e., clustering\nand learning feature representation to generate pseudo labeled data for\nunsupervised 3D recognition. SL3D is a generic framework and can be applied to\nsolve different 3D recognition tasks, including classification, object\ndetection, and semantic segmentation. Extensive experiments demonstrate its\neffectiveness. Code is available at https://github.com/fcendra/sl3d.",
    "descriptor": "\nComments: Accepted at Neural Information Processing Systems (NeurIPS 2022) Workshop on Self-Supervised Learning: Theory and Practice\n",
    "authors": [
      "Fernando Julio Cendra",
      "Lan Ma",
      "Jiajun Shen",
      "Xiaojuan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16810"
  },
  {
    "id": "arXiv:2210.16815",
    "title": "CAD 3D Model classification by Graph Neural Networks: A new approach  based on STEP format",
    "abstract": "In this paper, we introduce a new approach for retrieval and classification\nof 3D models that directly performs in the Computer-Aided Design (CAD) format\nwithout any conversion to other representations like point clouds or meshes,\nthus avoiding any loss of information. Among the various CAD formats, we\nconsider the widely used STEP extension, which represents a standard for\nproduct manufacturing information. This particular format represents a 3D model\nas a set of primitive elements such as surfaces and vertices linked together.\nIn our approach, we exploit the linked structure of STEP files to create a\ngraph in which the nodes are the primitive elements and the arcs are the\nconnections between them. We then use Graph Neural Networks (GNNs) to solve the\nproblem of model classification. Finally, we created two datasets of 3D models\nin native CAD format, respectively, by collecting data from the Traceparts\nmodel library and from the Configurators software modeling company. We used\nthese datasets to test and compare our approach with respect to\nstate-of-the-art methods that consider other 3D formats. Our code is available\nat https://github.com/divanoLetto/3D_STEP_Classification",
    "descriptor": "",
    "authors": [
      "L. Mandelli",
      "S. Berretti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16815"
  },
  {
    "id": "arXiv:2210.16819",
    "title": "Relative Attention-based One-Class Adversarial Autoencoder for  Continuous Authentication of Smartphone Users",
    "abstract": "Behavioral biometrics-based continuous authentication is a promising\nauthentication scheme, which uses behavioral biometrics recorded by built-in\nsensors to authenticate smartphone users throughout the session. However,\ncurrent continuous authentication methods suffer some limitations: 1)\nbehavioral biometrics from impostors are needed to train continuous\nauthentication models. Since the distribution of negative samples from diverse\nattackers are unknown, it is a difficult problem to solve in real-world\nscenarios; 2) most deep learning-based continuous authentication methods need\nto train two models to improve authentication performance. A deep learning\nmodel for deep feature extraction, and a machine learning-based classifier for\nclassification; 3) weak capability of capturing users' behavioral patterns\nleads to poor authentication performance. To solve these issues, we propose a\nrelative attention-based one-class adversarial autoencoder for continuous\nauthentication of smartphone users. First, we propose a one-class adversarial\nautoencoder to learn latent representations of legitimate users' behavioral\npatterns, which is trained only with legitimate smartphone users' behavioral\nbiometrics. Second, we present the relative attention layer to capture richer\ncontextual semantic representation of users' behavioral patterns, which\nmodifies the standard self-attention mechanism using convolution projection\ninstead of linear projection to perform the attention maps. Experimental\nresults demonstrate that we can achieve superior performance of 1.05% EER,\n1.09% EER, and 1.08% EER with a high authentication frequency (0.7s) on three\npublic datasets.",
    "descriptor": "",
    "authors": [
      "Mingming Hu",
      "Kun Zhang",
      "Ruibang You",
      "Bibo Tu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.16819"
  },
  {
    "id": "arXiv:2210.16820",
    "title": "Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action  Recognition",
    "abstract": "We propose a Few-shot Learning pipeline for 3D skeleton-based action\nrecognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). To\nfactor out misalignment between query and support sequences of 3D body joints,\nwe propose an advanced variant of Dynamic Time Warping which jointly models\neach smooth path between the query and support frames to achieve simultaneously\nthe best alignment in the temporal and simulated camera viewpoint spaces for\nend-to-end learning under the limited few-shot training data. Sequences are\nencoded with a temporal block encoder based on Simple Spectral Graph\nConvolution, a lightweight linear Graph Neural Network backbone. We also\ninclude a setting with a transformer. Finally, we propose a similarity-based\nloss which encourages the alignment of sequences of the same class while\npreventing the alignment of unrelated sequences. We show state-of-the-art\nresults on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II.",
    "descriptor": "\nComments: Accepted as an oral paper at the 16th Asian Conference on Computer Vision (ACCV 2022). It extends our arXiv preprint arXiv:2112.12668 (2021)\n",
    "authors": [
      "Lei Wang",
      "Piotr Koniusz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16820"
  },
  {
    "id": "arXiv:2210.16822",
    "title": "Towards Versatile Embodied Navigation",
    "abstract": "With the emergence of varied visual navigation tasks (e.g,\nimage-/object-/audio-goal and vision-language navigation) that specify the\ntarget in different ways, the community has made appealing advances in training\nspecialized agents capable of handling individual navigation tasks well. Given\nplenty of embodied navigation tasks and task-specific solutions, we address a\nmore fundamental question: can we learn a single powerful agent that masters\nnot one but multiple navigation tasks concurrently? First, we propose VXN, a\nlarge-scale 3D dataset that instantiates four classic navigation tasks in\nstandardized, continuous, and audiovisual-rich environments. Second, we propose\nVienna, a versatile embodied navigation agent that simultaneously learns to\nperform the four navigation tasks with one model. Building upon a\nfull-attentive architecture, Vienna formulates various navigation tasks as a\nunified, parse-and-query procedure: the target description, augmented with four\ntask embeddings, is comprehensively interpreted into a set of diversified goal\nvectors, which are refined as the navigation progresses, and used as queries to\nretrieve supportive context from episodic history for decision making. This\nenables the reuse of knowledge across navigation tasks with varying input\ndomains/modalities. We empirically demonstrate that, compared with learning\neach visual navigation task individually, our multitask agent achieves\ncomparable or even better performance with reduced complexity.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022; Code: this https URL\n",
    "authors": [
      "Hanqing Wang",
      "Wei Liang",
      "Luc Van Gool",
      "Wenguan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16822"
  },
  {
    "id": "arXiv:2210.16829",
    "title": "Self-Regularized Prototypical Network for Few-Shot Semantic Segmentation",
    "abstract": "The deep CNNs in image semantic segmentation typically require a large number\nof densely-annotated images for training and have difficulties in generalizing\nto unseen object categories. Therefore, few-shot segmentation has been\ndeveloped to perform segmentation with just a few annotated examples. In this\nwork, we tackle the few-shot segmentation using a self-regularized prototypical\nnetwork (SRPNet) based on prototype extraction for better utilization of the\nsupport information. The proposed SRPNet extracts class-specific prototype\nrepresentations from support images and generates segmentation masks for query\nimages by a distance metric - the fidelity. A direct yet effective prototype\nregularization on support set is proposed in SRPNet, in which the generated\nprototypes are evaluated and regularized on the support set itself. The extent\nto which the generated prototypes restore the support mask imposes an upper\nlimit on performance. The performance on the query set should never exceed the\nupper limit no matter how complete the knowledge is generalized from support\nset to query set. With the specific prototype regularization, SRPNet fully\nexploits knowledge from the support and offers high-quality prototypes that are\nrepresentative for each semantic class and meanwhile discriminative for\ndifferent classes. The query performance is further improved by an iterative\nquery inference (IQI) module that combines a set of regularized prototypes. Our\nproposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot\nsegmentation benchmarks.",
    "descriptor": "\nComments: Pattern Recognition (PR)\n",
    "authors": [
      "Henghui Ding",
      "Hui Zhang",
      "Xudong Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16829"
  },
  {
    "id": "arXiv:2210.16834",
    "title": "Alleviating the Sample Selection Bias in Few-shot Learning by Removing  Projection to the Centroid",
    "abstract": "Few-shot learning (FSL) targets at generalization of vision models towards\nunseen tasks without sufficient annotations. Despite the emergence of a number\nof few-shot learning methods, the sample selection bias problem, i.e., the\nsensitivity to the limited amount of support data, has not been well\nunderstood. In this paper, we find that this problem usually occurs when the\npositions of support samples are in the vicinity of task centroid -- the mean\nof all class centroids in the task. This motivates us to propose an extremely\nsimple feature transformation to alleviate this problem, dubbed Task Centroid\nProjection Removing (TCPR). TCPR is applied directly to all image features in a\ngiven task, aiming at removing the dimension of features along the direction of\nthe task centroid. While the exact task centroid cannot be accurately obtained\nfrom limited data, we estimate it using base features that are each similar to\none of the support features. Our method effectively prevents features from\nbeing too close to the task centroid. Extensive experiments over ten datasets\nfrom different domains show that TCPR can reliably improve classification\naccuracy across various feature extractors, training algorithms and datasets.\nThe code has been made available at https://github.com/KikimorMay/FSL-TCBR.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Jing Xu",
      "Xu Luo",
      "Xinglin Pan",
      "Wenjie Pei",
      "Yanan Li",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16834"
  },
  {
    "id": "arXiv:2210.16836",
    "title": "Combining Attention Module and Pixel Shuffle for License Plate  Super-Resolution",
    "abstract": "The License Plate Recognition (LPR) field has made impressive advances in the\nlast decade due to novel deep learning approaches combined with the increased\navailability of training data. However, it still has some open issues,\nespecially when the data come from low-resolution (LR) and low-quality\nimages/videos, as in surveillance systems. This work focuses on license plate\n(LP) reconstruction in LR and low-quality images. We present a Single-Image\nSuper-Resolution (SISR) approach that extends the attention/transformer module\nconcept by exploiting the capabilities of PixelShuffle layers and that has an\nimproved loss function based on LPR predictions. For training the proposed\narchitecture, we use synthetic images generated by applying heavy Gaussian\nnoise in terms of Structural Similarity Index Measure (SSIM) to the original\nhigh-resolution (HR) images. In our experiments, the proposed method\noutperformed the baselines both quantitatively and qualitatively. The datasets\nwe created for this work are publicly available to the research community at\nhttps://github.com/valfride/lpr-rsr/",
    "descriptor": "\nComments: Accepted for presentation at the Conference on Graphics, Patterns and Images (SIBGRAPI) 2022\n",
    "authors": [
      "Valfride Nascimento",
      "Rayson Laroca",
      "Jorge de A. Lambert",
      "William Robson Schwartz",
      "David Menotti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16836"
  },
  {
    "id": "arXiv:2210.16838",
    "title": "Counterfactual Data Augmentation via Perspective Transition for  Open-Domain Dialogues",
    "abstract": "The construction of open-domain dialogue systems requires high-quality\ndialogue datasets. The dialogue data admits a wide variety of responses for a\ngiven dialogue history, especially responses with different semantics. However,\ncollecting high-quality such a dataset in most scenarios is labor-intensive and\ntime-consuming. In this paper, we propose a data augmentation method to\nautomatically augment high-quality responses with different semantics by\ncounterfactual inference. Specifically, given an observed dialogue, our\ncounterfactual generation model first infers semantically different responses\nby replacing the observed reply perspective with substituted ones. Furthermore,\nour data selection method filters out detrimental augmented responses.\nExperimental results show that our data augmentation method can augment\nhigh-quality responses with different semantics for a given dialogue history,\nand can outperform competitive baselines on multiple downstream tasks.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (main conference)\n",
    "authors": [
      "Jiao Ou",
      "Jinchao Zhang",
      "Yang Feng",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16838"
  },
  {
    "id": "arXiv:2210.16841",
    "title": "Actionable Phrase Detection using NLP",
    "abstract": "Actionable sentences are terms that, in the most basic sense, imply the\nnecessity of taking a specific action. In Linguistic terms, they are steps to\nachieve an operation, often through the usage of action verbs. For example, the\nsentence, `Get your homework finished by tomorrow` qualifies as actionable\nsince it demands a specific action (In this case, finishing homework) to be\ntaken. In contrast, a simple sentence such as, `I like to play the guitar` does\nnot qualify as an actionable phrase since it simply states a personal choice of\nthe person instead of demanding a task to be finished.\nIn this paper, the aim is to explore if Actionables can be extracted from raw\ntext using Linguistic filters designed from scratch. These filters are\nspecially catered to identifying actionable text using Transfer Learning as the\nlead role. Actionable Detection can be used in detecting emergency tasks during\na crisis, Instruction accuracy for First aid and can also be used to make\nproductivity tools like automatic ToDo list generators from conferences. To\naccomplish this, we use the Enron Email Dataset and apply our Linguistic\nfilters on the cleaned textual data. We then use Transfer Learning with the\nUniversal Sentence Encoder to train a model to classify whether a given string\nof raw text is actionable or not.",
    "descriptor": "",
    "authors": [
      "Adit Magotra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16841"
  },
  {
    "id": "arXiv:2210.16843",
    "title": "A Pipeline for Analysing Grant Applications",
    "abstract": "Data mining techniques can transform massive amounts of unstructured data\ninto quantitative data that quickly reveal insights, trends, and patterns\nbehind the original data. In this paper, a data mining model is applied to\nanalyse the 2019 grant applications submitted to an Australian Government\nresearch funding agency to investigate whether grant schemes successfully\nidentifies innovative project proposals, as intended. The grant applications\nare peer-reviewed research proposals that include specific ``innovation and\ncreativity'' (IC) scores assigned by reviewers. In addition to predicting the\nIC score for each research proposal, we are particularly interested in\nunderstanding the vocabulary of innovative proposals. In order to solve this\nproblem, various data mining models and feature encoding algorithms are studied\nand explored. As a result, we propose a model with the best performance, a\nRandom Forest (RF) classifier over documents encoded with features denoting the\npresence or absence of unigrams. In specific, the unigram terms are encoded by\na modified Term Frequency - Inverse Document Frequency (TF-IDF) algorithm,\nwhich only implements the IDF part of TF-IDF. Besides the proposed model, this\npaper also presents a rigorous experimental pipeline for analysing grant\napplications, and the experimental results prove its feasibility.",
    "descriptor": "",
    "authors": [
      "Shuaiqun Pan",
      "Sergio J. Rodr\u00edguez M\u00e9ndez",
      "Kerry Taylor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.16843"
  },
  {
    "id": "arXiv:2210.16844",
    "title": "Micro and Macro Level Graph Modeling for Graph Variational Auto-Encoders",
    "abstract": "Generative models for graph data are an important research topic in machine\nlearning. Graph data comprise two levels that are typically analyzed\nseparately: node-level properties such as the existence of a link between a\npair of nodes, and global aggregate graph-level statistics, such as motif\ncounts. This paper proposes a new multi-level framework that jointly models\nnode-level properties and graph-level statistics, as mutually reinforcing\nsources of information. We introduce a new micro-macro training objective for\ngraph generation that combines node-level and graph-level losses. We utilize\nthe micro-macro objective to improve graph generation with a GraphVAE, a\nwell-established model based on graph-level latent variables, that provides\nfast training and generation time for medium-sized graphs. Our experiments show\nthat adding micro-macro modeling to the GraphVAE model improves graph quality\nscores up to 2 orders of magnitude on five benchmark datasets, while\nmaintaining the GraphVAE generation speed advantage.",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems, 2022\n",
    "authors": [
      "Kiarash Zahirnia",
      "Oliver Schulte",
      "Parmis Naddaf",
      "Ke Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16844"
  },
  {
    "id": "arXiv:2210.16847",
    "title": "1st Place Solutions for UG2+ Challenge 2022 ATMOSPHERIC TURBULENCE  MITIGATION",
    "abstract": "In this technical report, we briefly introduce the solution of our team\n''summer'' for Atomospheric Turbulence Mitigation in UG$^2$+ Challenge in CVPR\n2022. In this task, we propose a unified end-to-end framework to reconstruct a\nhigh quality image from distorted frames, which is mainly consists of a\nRestormer-based image reconstruction module and a NIMA-based image quality\nassessment module. Our framework is efficient and generic, which is adapted to\nboth hot-air image and text pattern. Moreover, we elaborately synthesize more\nthan 10 thousands of images to simulate atmospheric turbulence. And these\nimages improve the robustness of the model. Finally, we achieve the average\naccuracy of 98.53\\% on the reconstruction result of the text patterns, ranking\n1st on the final leaderboard.",
    "descriptor": "",
    "authors": [
      "Zhuang Liu",
      "Zhichao Zhao",
      "Ye Yuan",
      "Zhi Qiao",
      "Jinfeng Bai",
      "Zhilong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16847"
  },
  {
    "id": "arXiv:2210.16848",
    "title": "Using Context-to-Vector with Graph Retrofitting to Improve Word  Embeddings",
    "abstract": "Although contextualized embeddings generated from large-scale pre-trained\nmodels perform well in many tasks, traditional static embeddings (e.g.,\nSkip-gram, Word2Vec) still play an important role in low-resource and\nlightweight settings due to their low computational cost, ease of deployment,\nand stability. In this paper, we aim to improve word embeddings by 1)\nincorporating more contextual information from existing pre-trained models into\nthe Skip-gram framework, which we call Context-to-Vec; 2) proposing a\npost-processing retrofitting method for static embeddings independent of\ntraining by employing priori synonym knowledge and weighted vector\ndistribution. Through extrinsic and intrinsic tasks, our methods are well\nproven to outperform the baselines by a large margin.",
    "descriptor": "",
    "authors": [
      "Jiangbin Zheng",
      "Yile Wang",
      "Ge Wang",
      "Jun Xia",
      "Yufei Huang",
      "Guojiang Zhao",
      "Yue Zhang",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16848"
  },
  {
    "id": "arXiv:2210.16849",
    "title": "TT-Net: Dual-path transformer based sound field translation in the  spherical harmonic domain",
    "abstract": "In the current method for the sound field translation tasks based on\nspherical harmonic (SH) analysis, the solution based on the additive theorem\nusually faces the problem of singular values caused by large matrix condition\nnumbers. The influence of different distances and frequencies of the spherical\nradial function on the stability of the translation matrix will affect the\naccuracy of the SH coefficients at the selected point. Due to the problems\nmentioned above, we propose a neural network scheme based on the dual-path\ntransformer. More specifically, the dual-path network is constructed by the\nself-attention module along the two dimensions of the frequency and order axes.\nThe transform-average-concatenate layer and upscaling layer are introduced in\nthe network, which provides solutions for multiple sampling points and\nupscaling. Numerical simulation results indicate that both the working\nfrequency range and the distance range of the translation are extended. More\naccurate higher-order SH coefficients are obtained with the proposed dual-path\nnetwork.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Yiwen Wang",
      "Zijian Lan",
      "Xihong Wu",
      "Tianshu Qu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16849"
  },
  {
    "id": "arXiv:2210.16850",
    "title": "Medical Codes Prediction from Clinical Notes: From Human Coders to  Machines",
    "abstract": "Prediction of medical codes from clinical notes is a practical and essential\nneed for every healthcare delivery organization within current medical systems.\nAutomating annotation will save significant time and excessive effort that\nhuman coders spend today. However, the biggest challenge is directly\nidentifying appropriate medical codes from several thousands of\nhigh-dimensional codes from unstructured free-text clinical notes. This complex\nmedical codes prediction problem from clinical notes has received substantial\ninterest in the NLP community, and several recent studies have shown the\nstate-of-the-art code prediction results of full-fledged deep learning-based\nmethods. This progress raises the fundamental question of how far automated\nmachine learning systems are from human coders' working performance, as well as\nthe important question of how well current explainability methods apply to\nadvanced neural network models such as transformers. This is to predict correct\ncodes and present references in clinical notes that support code prediction, as\nthis level of explainability and accuracy of the prediction outcomes is\ncritical to gaining trust from professional medical coders.",
    "descriptor": "\nComments: The 11th Bay Area Machine Learning Symposium (BayLearn 2022), San Francisco, CA, October 20, 2022. arXiv admin note: substantial text overlap with arXiv:2210.15882. substantial text overlap with arXiv:2107.10650\n",
    "authors": [
      "Byung-Hak Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16850"
  },
  {
    "id": "arXiv:2210.16859",
    "title": "A Solvable Model of Neural Scaling Laws",
    "abstract": "Large language models with a huge number of parameters, when trained on near\ninternet-sized number of tokens, have been empirically shown to obey neural\nscaling laws: specifically, their performance behaves predictably as a power\nlaw in either parameters or dataset size until bottlenecked by the other\nresource. To understand this better, we first identify the necessary properties\nallowing such scaling laws to arise and then propose a statistical model -- a\njoint generative data model and random feature model -- that captures this\nneural scaling phenomenology. By solving this model in the dual limit of large\ntraining set size and large number of parameters, we gain insight into (i) the\nstatistical structure of datasets and tasks that lead to scaling laws, (ii) the\nway nonlinear feature maps, such as those provided by neural networks, enable\nscaling laws when trained on these datasets, (iii) the optimality of the\nequiparameterization scaling of training sets and parameters, and (iv) whether\nsuch scaling laws can break down and how they behave when they do. Key findings\nare the manner in which the power laws that occur in the statistics of natural\ndatasets are extended by nonlinear random feature maps and then translated into\npower-law scalings of the test loss and how the finite extent of the data's\nspectral power law causes the model's performance to plateau.",
    "descriptor": "\nComments: 73 + 23 pages, 14 + 5 figures\n",
    "authors": [
      "Alexander Maloney",
      "Daniel A. Roberts",
      "James Sully"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16859"
  },
  {
    "id": "arXiv:2210.16863",
    "title": "Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum",
    "abstract": "With the development of Web 3.0 which emphasizes decentralization, blockchain\ntechnology ushers in its revolution and also brings numerous challenges,\nparticularly in the field of cryptocurrency. Recently, a large number of\ncriminal behaviors continuously emerge on blockchain, such as Ponzi schemes and\nphishing scams, which severely endanger decentralized finance. Existing\ngraph-based abnormal behavior detection methods on blockchain usually focus on\nconstructing homogeneous transaction graphs without distinguishing the\nheterogeneity of nodes and edges, resulting in partial loss of transaction\npattern information. Although existing heterogeneous modeling methods can\ndepict richer information through metapaths, the extracted metapaths generally\nneglect temporal dependencies between entities and do not reflect real\nbehavior. In this paper, we introduce Time-aware Metapath Feature Augmentation\n(TMFAug) as a plug-and-play module to capture the real metapath-based\ntransaction patterns during Ponzi scheme detection on Ethereum. The proposed\nmodule can be adaptively combined with existing graph-based Ponzi detection\nmethods. Extensive experimental results show that our TMFAug can help existing\nPonzi detection methods achieve significant performance improvements on the\nEthereum dataset, indicating the effectiveness of heterogeneous temporal\ninformation for Ponzi scheme detection.",
    "descriptor": "",
    "authors": [
      "Chengxiang Jin",
      "Jiajun Zhou",
      "Jie Jin",
      "Jiajing Wu",
      "Qi Xuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Finance (q-fin.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.16863"
  },
  {
    "id": "arXiv:2210.16865",
    "title": "Learning to Decompose: Hypothetical Question Decomposition Based on  Comparable Texts",
    "abstract": "Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Ben Zhou",
      "Kyle Richardson",
      "Xiaodong Yu",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16865"
  },
  {
    "id": "arXiv:2210.16870",
    "title": "A simple, efficient and scalable contrastive masked autoencoder for  learning visual representations",
    "abstract": "We introduce CAN, a simple, efficient and scalable method for self-supervised\nlearning of visual representations. Our framework is a minimal and conceptually\nclean synthesis of (C) contrastive learning, (A) masked autoencoders, and (N)\nthe noise prediction approach used in diffusion models. The learning mechanisms\nare complementary to one another: contrastive learning shapes the embedding\nspace across a batch of image samples; masked autoencoders focus on\nreconstruction of the low-frequency spatial correlations in a single image\nsample; and noise prediction encourages the reconstruction of the\nhigh-frequency components of an image. The combined approach results in a\nrobust, scalable and simple-to-implement algorithm. The training process is\nsymmetric, with 50% of patches in both views being masked at random, yielding a\nconsiderable efficiency improvement over prior contrastive learning methods.\nExtensive empirical studies demonstrate that CAN achieves strong downstream\nperformance under both linear and finetuning evaluations on transfer learning\nand robustness tasks. CAN outperforms MAE and SimCLR when pre-training on\nImageNet, but is especially useful for pre-training on larger uncurated\ndatasets such as JFT-300M: for linear probe on ImageNet, CAN achieves 75.4%\ncompared to 73.4% for SimCLR and 64.1% for MAE. The finetuned performance on\nImageNet of our ViT-L model is 86.1%, compared to 85.5% for SimCLR, and 85.4%\nfor MAE. The overall FLOPs load of SimCLR is 70% higher than CAN for ViT-L\nmodels.",
    "descriptor": "\nComments: Mishra and Robinson contributed equally\n",
    "authors": [
      "Shlok Mishra",
      "Joshua Robinson",
      "Huiwen Chang",
      "David Jacobs",
      "Aaron Sarna",
      "Aaron Maschinot",
      "Dilip Krishnan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16870"
  },
  {
    "id": "arXiv:2210.16872",
    "title": "Planning to the Information Horizon of BAMDPs via Epistemic State  Abstraction",
    "abstract": "The Bayes-Adaptive Markov Decision Process (BAMDP) formalism pursues the\nBayes-optimal solution to the exploration-exploitation trade-off in\nreinforcement learning. As the computation of exact solutions to Bayesian\nreinforcement-learning problems is intractable, much of the literature has\nfocused on developing suitable approximation algorithms. In this work, before\ndiving into algorithm design, we first define, under mild structural\nassumptions, a complexity measure for BAMDP planning. As efficient exploration\nin BAMDPs hinges upon the judicious acquisition of information, our complexity\nmeasure highlights the worst-case difficulty of gathering information and\nexhausting epistemic uncertainty. To illustrate its significance, we establish\na computationally-intractable, exact planning algorithm that takes advantage of\nthis measure to show more efficient planning. We then conclude by introducing a\nspecific form of state abstraction with the potential to reduce BAMDP\ncomplexity and gives rise to a computationally-tractable, approximate planning\nalgorithm.",
    "descriptor": "\nComments: Accepted to Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Dilip Arumugam",
      "Satinder Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16872"
  },
  {
    "id": "arXiv:2210.16875",
    "title": "A Multi-modal Deformable Land-air Robot for Complex Environments",
    "abstract": "Single locomotion robots often struggle to adapt in highly variable or\nuncertain environments, especially in emergencies. In this paper, a multi-modal\ndeformable robot is introduced that can both fly and drive. Compatibility\nissues with multi-modal locomotive fusion for this hybrid land-air robot are\nsolved using proposed design conceptions, including power settings, energy\nselection, and designs of deformable structure. The robot can also\nautomatically transform between land and air modes during 3D planning and\ntracking. Meanwhile, we proposed a algorithms for evaluation the performance of\nland-air robots. A series of comparisons and experiments were conducted to\ndemonstrate the robustness and reliability of the proposed structure in complex\nfield environments.",
    "descriptor": "",
    "authors": [
      "Xinyu Zhang",
      "Yuanhao Huang",
      "Kangyao Huang",
      "Xiaoyu Wang",
      "Dafeng Jin",
      "Huaping Liu",
      "Jun Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.16875"
  },
  {
    "id": "arXiv:2210.16877",
    "title": "On Rate-Distortion Theory in Capacity-Limited Cognition & Reinforcement  Learning",
    "abstract": "Throughout the cognitive-science literature, there is widespread agreement\nthat decision-making agents operating in the real world do so under limited\ninformation-processing capabilities and without access to unbounded cognitive\nor computational resources. Prior work has drawn inspiration from this fact and\nleveraged an information-theoretic model of such behaviors or policies as\ncommunication channels operating under a bounded rate constraint. Meanwhile, a\nparallel line of work also capitalizes on the same principles from\nrate-distortion theory to formalize capacity-limited decision making through\nthe notion of a learning target, which facilitates Bayesian regret bounds for\nprovably-efficient learning algorithms. In this paper, we aim to elucidate this\nlatter perspective by presenting a brief survey of these information-theoretic\nmodels of capacity-limited decision making in biological and artificial agents.",
    "descriptor": "\nComments: Accepted to the NeurIPS Workshop on Information-Theoretic Principles in Cognitive Systems (InfoCog) 2022. arXiv admin note: text overlap with arXiv:2206.02072\n",
    "authors": [
      "Dilip Arumugam",
      "Mark K. Ho",
      "Noah D. Goodman",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16877"
  },
  {
    "id": "arXiv:2210.16884",
    "title": "A Simple Hypergraph Kernel Convolution based on Discounted Markov  Diffusion Process",
    "abstract": "Kernels on discrete structures evaluate pairwise similarities between objects\nwhich capture semantics and inherent topology information. Existing kernels on\ndiscrete structures are only developed by topology information(such as\nadjacency matrix of graphs), without considering original attributes of\nobjects. This paper proposes a two-phase paradigm to aggregate comprehensive\ninformation on discrete structures leading to a Discount Markov Diffusion\nLearnable Kernel (DMDLK). Specifically, based on the underlying projection of\nDMDLK, we design a Simple Hypergraph Kernel Convolution (SHKC) for hidden\nrepresentation of vertices. SHKC can adjust diffusion steps rather than\nstacking convolution layers to aggregate information from long-range\nneighborhoods which prevents over-smoothing issues of existing hypergraph\nconvolutions. Moreover, we utilize the uniform stability bound theorem in\ntransductive learning to analyze critical factors for the effectiveness and\ngeneralization ability of SHKC from a theoretical perspective. The experimental\nresults on several benchmark datasets for node classification tasks verified\nthe superior performance of SHKC over state-of-the-art methods.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022 New Frontiers in Graph Learning Workshop\n",
    "authors": [
      "Fuyang Li",
      "Jiying Zhang",
      "Xi Xiao",
      "Bin Zhang",
      "Dijun Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16884"
  },
  {
    "id": "arXiv:2210.16886",
    "title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction",
    "abstract": "In text generation, models that generate text from scratch one token at a\ntime are currently the dominant paradigm. Despite being performant, these\nmodels lack the ability to revise existing text, which limits their usability\nin many practical scenarios. We look to address this, with DiffusER (Diffusion\nvia Edit-based Reconstruction), a new edit-based generative model for text\nbased on denoising diffusion models -- a class of models that use a Markov\nchain of denoising steps to incrementally generate data. DiffusER is not only a\nstrong generative model in general, rivalling autoregressive models on several\ntasks spanning machine translation, summarization, and style transfer; it can\nalso perform other varieties of generation that standard autoregressive models\nare not well-suited for. For instance, we demonstrate that DiffusER makes it\npossible for a user to condition generation on a prototype, or an incomplete\nsequence, and continue revising based on previous edit steps.",
    "descriptor": "\nComments: Preprint. Work in progress\n",
    "authors": [
      "Machel Reid",
      "Vincent J. Hellendoorn",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16886"
  },
  {
    "id": "arXiv:2210.16892",
    "title": "Partitioned Gradient Matching-based Data Subset Selection for  Compute-Efficient Robust ASR Training",
    "abstract": "Training state-of-the-art ASR systems such as RNN-T often has a high\nassociated financial and environmental cost. Training with a subset of training\ndata could mitigate this problem if the subset selected could achieve on-par\nperformance with training with the entire dataset. Although there are many data\nsubset selection(DSS) algorithms, direct application to the RNN-T is difficult,\nespecially the DSS algorithms that are adaptive and use learning dynamics such\nas gradients, as RNN-T tend to have gradients with a significantly larger\nmemory footprint. In this paper, we propose Partitioned Gradient Matching (PGM)\na novel distributable DSS algorithm, suitable for massive datasets like those\nused to train RNN-T. Through extensive experiments on Librispeech 100H and\nLibrispeech 960H, we show that PGM achieves between 3x to 6x speedup with only\na very small accuracy degradation (under 1% absolute WER difference). In\naddition, we demonstrate similar results for PGM even in settings where the\ntraining data is corrupted with noise.",
    "descriptor": "",
    "authors": [
      "Ashish Mittal",
      "Durga Sivasubramanian",
      "Rishabh Iyer",
      "Preethi Jyothi",
      "Ganesh Ramakrishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16892"
  },
  {
    "id": "arXiv:2210.16897",
    "title": "Time-rEversed diffusioN tEnsor Transformer: A new TENET of Few-Shot  Object Detection",
    "abstract": "In this paper, we tackle the challenging problem of Few-shot Object\nDetection. Existing FSOD pipelines (i) use average-pooled representations that\nresult in information loss; and/or (ii) discard position information that can\nhelp detect object instances. Consequently, such pipelines are sensitive to\nlarge intra-class appearance and geometric variations between support and query\nimages. To address these drawbacks, we propose a Time-rEversed diffusioN tEnsor\nTransformer (TENET), which i) forms high-order tensor representations that\ncapture multi-way feature occurrences that are highly discriminative, and ii)\nuses a transformer that dynamically extracts correlations between the query\nimage and the entire support set, instead of a single average-pooled support\nembedding. We also propose a Transformer Relation Head (TRH), equipped with\nhigher-order representations, which encodes correlations between query regions\nand the entire support set, while being sensitive to the positional variability\nof object instances. Our model achieves state-of-the-art results on PASCAL VOC,\nFSOD, and COCO.",
    "descriptor": "\nComments: Accepted at the 17th European Conference on Computer Vision (ECCV 2022)\n",
    "authors": [
      "Shan Zhang",
      "Naila Murray",
      "Lei Wang",
      "Piotr Koniusz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16897"
  },
  {
    "id": "arXiv:2210.16900",
    "title": "High Resolution Multi-Scale RAFT (Robust Vision Challenge 2022)",
    "abstract": "In this report, we present our optical flow approach, MS-RAFT+, that won the\nRobust Vision Challenge 2022. It is based on the MS-RAFT method, which\nsuccessfully integrates several multi-scale concepts into single-scale RAFT.\nOur approach extends this method by exploiting an additional finer scale for\nestimating the flow, which is made feasible by on-demand cost computation. This\nway, it can not only operate at half the original resolution, but also use\nMS-RAFT's shared convex upsampler to obtain full resolution flow. Moreover, our\napproach relies on an adjusted fine-tuning scheme during training. This in turn\naims at improving the generalization across benchmarks. Among all participating\nmethods in the Robust Vision Challenge, our approach ranks first on VIPER and\nsecond on KITTI, Sintel, and Middlebury, resulting in the first place of the\noverall ranking.",
    "descriptor": "\nComments: Technical report for the Robust Vision Challenge 2022\n",
    "authors": [
      "Azin Jahedi",
      "Maximilian Luz",
      "Lukas Mehl",
      "Marc Rivinius",
      "Andr\u00e9s Bruhn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16900"
  },
  {
    "id": "arXiv:2210.16901",
    "title": "Foreign Object Debris Detection for Airport Pavement Images based on  Self-supervised Localization and Vision Transformer",
    "abstract": "Supervised object detection methods provide subpar performance when applied\nto Foreign Object Debris (FOD) detection because FOD could be arbitrary objects\naccording to the Federal Aviation Administration (FAA) specification. Current\nsupervised object detection algorithms require datasets that contain annotated\nexamples of every to-be-detected object. While a large and expensive dataset\ncould be developed to include common FOD examples, it is infeasible to collect\nall possible FOD examples in the dataset representation because of the\nopen-ended nature of FOD. Limitations of the dataset could cause FOD detection\nsystems driven by those supervised algorithms to miss certain FOD, which can\nbecome dangerous to airport operations. To this end, this paper presents a\nself-supervised FOD localization by learning to predict the runway images,\nwhich avoids the enumeration of FOD annotation examples. The localization\nmethod utilizes the Vision Transformer (ViT) to improve localization\nperformance. The experiments show that the method successfully detects\narbitrary FOD in real-world runway situations. The paper also provides an\nextension to the localization result to perform classification; a feature that\ncan be useful to downstream tasks. To train the localization, this paper also\npresents a simple and realistic dataset creation framework that only collects\nclean runway images. The training and testing data for this method are\ncollected at a local airport using unmanned aircraft systems (UAS).\nAdditionally, the developed dataset is provided for public use and further\nstudies.",
    "descriptor": "\nComments: This paper has been accepted for publication by the 2022 International Conference on Computational Science & Computational Intelligence (CSCI'22), Research Track on Signal & Image Processing, Computer Vision & Pattern Recognition\n",
    "authors": [
      "Travis Munyer",
      "Daniel Brinkman",
      "Xin Zhong",
      "Chenyu Huang",
      "Iason Konstantzos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16901"
  },
  {
    "id": "arXiv:2210.16902",
    "title": "Atlas: Automate Online Service Configuration in Network Slicing",
    "abstract": "Network slicing achieves cost-efficient slice customization to support\nheterogeneous applications and services. Configuring cross-domain resources to\nend-to-end slices based on service-level agreements, however, is challenging,\ndue to the complicated underlying correlations and the simulation-to-reality\ndiscrepancy between simulators and real networks. In this paper, we propose\nAtlas, an online network slicing system, which automates the service\nconfiguration of slices via safe and sample-efficient learn-to-configure\napproaches in three interrelated stages. First, we design a learning-based\nsimulator to reduce the sim-to-real discrepancy, which is accomplished by a new\nparameter searching method based on Bayesian optimization. Second, we offline\ntrain the policy in the augmented simulator via a novel offline algorithm with\na Bayesian neural network and parallel Thompson sampling. Third, we online\nlearn the policy in real networks with a novel online algorithm with safe\nexploration and Gaussian process regression. We implement Atlas on an\nend-to-end network prototype based on OpenAirInterface RAN, OpenDayLight SDN\ntransport, OpenAir-CN core network, and Docker-based edge server. Experimental\nresults show that, compared to state-of-the-art solutions, Atlas achieves 63.9%\nand 85.7% regret reduction on resource usage and slice quality of experience\nduring the online learning stage, respectively.",
    "descriptor": "\nComments: Accepted by ACM CoNEXT 2022\n",
    "authors": [
      "Qiang Liu",
      "Nakjung Choi",
      "Tao Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16902"
  },
  {
    "id": "arXiv:2210.16906",
    "title": "DyG2Vec: Representation Learning for Dynamic Graphs with  Self-Supervision",
    "abstract": "The challenge in learning from dynamic graphs for predictive tasks lies in\nextracting fine-grained temporal motifs from an ever-evolving graph. Moreover,\ntask labels are often scarce, costly to obtain, and highly imbalanced for large\ndynamic graphs. Recent advances in self-supervised learning on graphs\ndemonstrate great potential, but focus on static graphs. State-of-the-art\n(SoTA) models for dynamic graphs are not only incompatible with the\nself-supervised learning (SSL) paradigm but also fail to forecast interactions\nbeyond the very near future. To address these limitations, we present DyG2Vec,\nan SSL-compatible, efficient model for representation learning on dynamic\ngraphs. DyG2Vec uses a window-based mechanism to generate task-agnostic node\nembeddings that can be used to forecast future interactions. DyG2Vec\nsignificantly outperforms SoTA baselines on benchmark datasets for downstream\ntasks while only requiring a fraction of the training/inference time. We adapt\ntwo SSL evaluation mechanisms to make them applicable to dynamic graphs and\nthus show that SSL pre-training helps learn more robust temporal node\nrepresentations, especially for scenarios with few labels.",
    "descriptor": "",
    "authors": [
      "Mohammad Ali Alomrani",
      "Mahdi Biparva",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.16906"
  },
  {
    "id": "arXiv:2210.16907",
    "title": "Curved Elements in Weak Galerkin Finite Element Methods",
    "abstract": "A mathematical analysis is established for the weak Galerkin finite element\nmethods for the Poisson equation with Dirichlet boundary value when the curved\nelements are involved on the interior edges of the finite element partition\nor/and on the boundary of the whole domain in two dimensions. The optimal\norders of error estimates for the weak Galerkin approximations in both the\n$H^1$-norm and the $L^2$-norm are established. Numerical results are reported\nto demonstrate the performance of the weak Galerkin methods on general curved\npolygonal partitions.",
    "descriptor": "\nComments: 25 pages, 7 figures, 3 tables\n",
    "authors": [
      "Dan Li",
      "Chunmei Wang",
      "Junping Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16907"
  },
  {
    "id": "arXiv:2210.16913",
    "title": "Revisiting Simple Regret Minimization in Multi-Armed Bandits",
    "abstract": "Simple regret is a natural and parameter-free performance criterion for\nidentifying a good arm in multi-armed bandits yet is less popular than the\nprobability of missing the best arm or an $\\epsilon$-good arm, perhaps due to\nlack of easy ways to characterize it. In this paper, we achieve improved simple\nregret upper bounds for both data-rich ($T\\ge n$) and data-poor regime ($T \\le\nn$) where $n$ is the number of arms and $T$ is the number of samples. At its\nheart is an improved analysis of the well-known Sequential Halving (SH)\nalgorithm that bounds the probability of returning an arm whose mean reward is\nnot within $\\epsilon$ from the best (i.e., not $\\epsilon$-good) for any choice\nof $\\epsilon>0$, although $\\epsilon$ is not an input to SH. We show that this\ndirectly implies an optimal simple regret bound of $\\mathcal{O}(\\sqrt{n/T})$.\nFurthermore, our upper bound gets smaller as a function of the number of\n$\\epsilon$-good arms. This results in an accelerated rate for the\n$(\\epsilon,\\delta)$-PAC criterion, which closes the gap between the upper and\nlower bounds in prior art. For the more challenging data-poor regime, we\npropose Bracketing SH (BSH) that enjoys the same improvement even without\nsampling each arm at least once. Our empirical study shows that BSH outperforms\nexisting methods on real-world tasks.",
    "descriptor": "",
    "authors": [
      "Yao Zhao",
      "Connor Stephens",
      "Csaba Szepesv\u00e1ri",
      "Kwang-Sung Jun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16913"
  },
  {
    "id": "arXiv:2210.16914",
    "title": "FatNet: High Resolution Kernels for Classification Using Fully  Convolutional Optical Neural Networks",
    "abstract": "This paper describes the transformation of a traditional in-silico\nclassification network into an optical fully convolutional neural network with\nhigh-resolution feature maps and kernels. When using the free-space 4f system\nto accelerate the inference speed of neural networks, higher resolutions of\nfeature maps and kernels can be used without the loss in frame rate. We present\nFatNet for the classification of images, which is more compatible with\nfree-space acceleration than standard convolutional classifiers. It neglects\nthe standard combination of convolutional feature extraction and classifier\ndense layers by performing both in one fully convolutional network. This\napproach takes full advantage of the parallelism in the 4f free-space system\nand performs fewer conversions between electronics and optics by reducing the\nnumber of channels and increasing the resolution, making the network faster in\noptics than off-the-shelf networks. To demonstrate the capabilities of FatNet,\nit trained with the CIFAR100 dataset on GPU and the simulator of the 4f system,\nthen compared the results against ResNet-18. The results show 8.2 times fewer\nconvolution operations at the cost of only 6% lower accuracy compared to the\noriginal network. These are promising results for the approach of training deep\nlearning with high-resolution kernels in the direction towards the upcoming\noptics era.",
    "descriptor": "",
    "authors": [
      "Riad Ibadulla",
      "Thomas M. Chen",
      "Constantino Carlos Reyes-Aldasoro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.16914"
  },
  {
    "id": "arXiv:2210.16915",
    "title": "Imitating Opponent to Win: Adversarial Policy Imitation Learning in  Two-player Competitive Games",
    "abstract": "Recent research on vulnerabilities of deep reinforcement learning (RL) has\nshown that adversarial policies adopted by an adversary agent can influence a\ntarget RL agent (victim agent) to perform poorly in a multi-agent environment.\nIn existing studies, adversarial policies are directly trained based on\nexperiences of interacting with the victim agent. There is a key shortcoming of\nthis approach; knowledge derived from historical interactions may not be\nproperly generalized to unexplored policy regions of the victim agent, making\nthe trained adversarial policy significantly less effective. In this work, we\ndesign a new effective adversarial policy learning algorithm that overcomes\nthis shortcoming. The core idea of our new algorithm is to create a new\nimitator to imitate the victim agent's policy while the adversarial policy will\nbe trained not only based on interactions with the victim agent but also based\non feedback from the imitator to forecast victim's intention. By doing so, we\ncan leverage the capability of imitation learning in well capturing underlying\ncharacteristics of the victim policy only based on sample trajectories of the\nvictim. Our victim imitation learning model differs from prior models as the\nenvironment's dynamics are driven by adversary's policy and will keep changing\nduring the adversarial policy training. We provide a provable bound to\nguarantee a desired imitating policy when the adversary's policy becomes\nstable. We further strengthen our adversarial policy learning by making our\nimitator a stronger version of the victim. Finally, our extensive experiments\nusing four competitive MuJoCo game environments show that our proposed\nadversarial policy learning algorithm outperforms state-of-the-art algorithms.",
    "descriptor": "",
    "authors": [
      "Viet Bui",
      "Tien Mai",
      "Thanh H. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16915"
  },
  {
    "id": "arXiv:2210.16917",
    "title": "PHY-Fed: An Information-Theoretic Secure Aggregation in Federated  Learning in Wireless Communications",
    "abstract": "Federated learning (FL) is a type of distributed machine learning at the\nwireless edge that preserves the privacy of clients' data from adversaries and\neven the central server. Existing federated learning approaches either use (i)\nsecure multiparty computation (SMC) which is vulnerable to inference or (ii)\ndifferential privacy which may decrease the test accuracy given a large number\nof parties with relatively small amounts of data each. To tackle the problem\nwith the existing methods in the literature, In this paper, we introduce\nPHY-Fed, a new framework that secures federated algorithms from an\ninformation-theoretic point of view.",
    "descriptor": "",
    "authors": [
      "Mitra Hassani",
      "Reza Gholizadeh"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16917"
  },
  {
    "id": "arXiv:2210.16918",
    "title": "Evaluation and comparison of federated learning algorithms for Human  Activity Recognition on smartphones",
    "abstract": "Pervasive computing promotes the integration of smart devices in our living\nspaces to develop services providing assistance to people. Such smart devices\nare increasingly relying on cloud-based Machine Learning, which raises\nquestions in terms of security (data privacy), reliance (latency), and\ncommunication costs. In this context, Federated Learning (FL) has been\nintroduced as a new machine learning paradigm enhancing the use of local\ndevices. At the server level, FL aggregates models learned locally on\ndistributed clients to obtain a more general model. In this way, no private\ndata is sent over the network, and the communication cost is reduced.\nUnfortunately, however, the most popular federated learning algorithms have\nbeen shown not to be adapted to some highly heterogeneous pervasive computing\nenvironments. In this paper, we propose a new FL algorithm, termed FedDist,\nwhich can modify models (here, deep neural network) during training by\nidentifying dissimilarities between neurons among the clients. This permits to\naccount for clients' specificity without impairing generalization. FedDist\nevaluated with three state-of-the-art federated learning algorithms on three\nlarge heterogeneous mobile Human Activity Recognition datasets. Results have\nshown the ability of FedDist to adapt to heterogeneous data and the capability\nof FL to deal with asynchronous situations.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2110.10223\n",
    "authors": [
      "Sannara Ek",
      "Fran\u00e7ois Portet",
      "Philippe Lalanda",
      "German Vega"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16918"
  },
  {
    "id": "arXiv:2210.16923",
    "title": "See as a Bee: UV Sensor for Aerial Strawberry Crop Monitoring",
    "abstract": "Precision agriculture aims to use technological tools for the agro-food\nsector to increase productivity, cut labor costs, and reduce the use of\nresources. This work takes inspiration from bees vision to design a remote\nsensing system tailored to incorporate UV-reflectance into a flower detector.\nWe demonstrate how this approach can provide feature-rich images for deep\nlearning strawberry flower detection and we apply it to a scalable, yet cost\neffective aerial monitoring robotic system in the field. We also compare the\nperformance of our UV-G-B image detector with a similar work that utilizes RGB\nimages.",
    "descriptor": "\nComments: The video reference is: this https URL This paper has been submitted to ICRA 2023\n",
    "authors": [
      "Megan Heath",
      "Ali Imran",
      "David St-Onge"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16923"
  },
  {
    "id": "arXiv:2210.16924",
    "title": "OGInfra: Geolocating Oil & Gas Infrastructure using Remote Sensing based  Active Fire Data",
    "abstract": "Remote sensing has become a crucial part of our daily lives, whether it be\nfrom triangulating our location using GPS or providing us with a weather\nforecast. It has multiple applications in domains such as military,\nsocio-economical, commercial, and even in supporting humanitarian efforts. This\nwork proposes a novel technique for the automated geo-location of Oil & Gas\ninfrastructure with the use of Active Fire Data from the NASA FIRMS data\nrepository & Deep Learning techniques; achieving a top accuracy of 90.68% with\nthe use of ResNet101.",
    "descriptor": "",
    "authors": [
      "Samyak Prajapati",
      "Amrit Raj",
      "Yash Chaudhari",
      "Akhilesh Nandwal",
      "Japman Singh Monga"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.16924"
  },
  {
    "id": "arXiv:2210.16928",
    "title": "Forget Embedding Layers: Representation Learning for Cold-start in  Recommender Systems",
    "abstract": "Recommender systems suffer from the cold-start problem whenever a new user\njoins the platform or a new item is added to the catalog. To address item\ncold-start, we propose to replace the embedding layer in sequential\nrecommenders with a dynamic storage that has no learnable weights and can keep\nan arbitrary number of representations. In this paper, we present FELRec, a\nlarge embedding network that refines the existing representations of users and\nitems in a recursive manner, as new information becomes available. In contrast\nto similar approaches, our model represents new users and items without side\ninformation or time-consuming fine-tuning. During item cold-start, our method\noutperforms similar method by 29.50%-47.45%. Further, our proposed model\ngeneralizes well to previously unseen datasets. The source code is publicly\navailable at github.com/kweimann/FELRec.",
    "descriptor": "",
    "authors": [
      "Kuba Weimann",
      "Tim O. F. Conrad"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16928"
  },
  {
    "id": "arXiv:2210.16933",
    "title": "Context-empowered Visual Attention Prediction in Pedestrian Scenarios",
    "abstract": "Effective and flexible allocation of visual attention is key for pedestrians\nwho have to navigate to a desired goal under different conditions of urgency\nand safety preferences. While automatic modelling of pedestrian attention holds\ngreat promise to improve simulations of pedestrian behavior, current saliency\nprediction approaches mostly focus on generic free-viewing scenarios and do not\nreflect the specific challenges present in pedestrian attention prediction. In\nthis paper, we present Context-SalNET, a novel encoder-decoder architecture\nthat explicitly addresses three key challenges of visual attention prediction\nin pedestrians: First, Context-SalNET explicitly models the context factors\nurgency and safety preference in the latent space of the encoder-decoder model.\nSecond, we propose the exponentially weighted mean squared error loss (ew-MSE)\nthat is able to better cope with the fact that only a small part of the ground\ntruth saliency maps consist of non-zero entries. Third, we explicitly model\nepistemic uncertainty to account for the fact that training data for pedestrian\nattention prediction is limited. To evaluate Context-SalNET, we recorded the\nfirst dataset of pedestrian visual attention in VR that includes explicit\nvariation of the context factors urgency and safety preference. Context-SalNET\nachieves clear improvements over state-of-the-art saliency prediction\napproaches as well as over ablations. Our novel dataset will be made fully\navailable and can serve as a valuable resource for further research on\npedestrian attention prediction.",
    "descriptor": "",
    "authors": [
      "Igor Vozniak",
      "Philipp Mueller",
      "Lorena Hell",
      "Nils Lipp",
      "Ahmed Abouelazm",
      "Christian Mueller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16933"
  },
  {
    "id": "arXiv:2210.16934",
    "title": "Learning to Compare Nodes in Branch and Bound with Graph Neural Networks",
    "abstract": "Branch-and-bound approaches in integer programming require ordering portions\nof the space to explore next, a problem known as node comparison. We propose a\nnew siamese graph neural network model to tackle this problem, where the nodes\nare represented as bipartite graphs with attributes. Similar to prior work, we\ntrain our model to imitate a diving oracle that plunges towards the optimal\nsolution. We evaluate our method by solving the instances in a plain framework\nwhere the nodes are explored according to their rank. On three NP-hard\nbenchmarks chosen to be particularly primal-difficult, our approach leads to\nfaster solving and smaller branch- and-bound trees than the default ranking\nfunction of the open-source solver SCIP, as well as competing machine learning\nmethods. Moreover, these results generalize to instances larger than used for\ntraining. Code for reproducing the experiments can be found at\nhttps://github.com/ds4dm/learn2comparenodes.",
    "descriptor": "\nComments: 7 pages, 3 figures, 2 tables\n",
    "authors": [
      "Abdel Ghani Labassi",
      "Didier Ch\u00e9telat",
      "Andrea Lodi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16934"
  },
  {
    "id": "arXiv:2210.16938",
    "title": "A view on model misspecification in uncertainty quantification",
    "abstract": "Estimating uncertainty of machine learning models is essential to assess the\nquality of the predictions that these models provide. However, there are\nseveral factors that influence the quality of uncertainty estimates, one of\nwhich is the amount of model misspecification. Model misspecification always\nexists as models are mere simplifications or approximations to reality. The\nquestion arises whether the estimated uncertainty under model misspecification\nis reliable or not. In this paper, we argue that model misspecification should\nreceive more attention, by providing thought experiments and contextualizing\nthese with relevant literature.",
    "descriptor": "",
    "authors": [
      "Yuko Kato",
      "David M.J. Tax",
      "Marco Loog"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16938"
  },
  {
    "id": "arXiv:2210.16940",
    "title": "FI-ODE: Certified and Robust Forward Invariance in Neural ODEs",
    "abstract": "We study how to certifiably enforce forward invariance properties in neural\nODEs. Forward invariance implies that the hidden states of the ODE will stay in\na ``good'' region, and a robust version would hold even under adversarial\nperturbations to the input. Such properties can be used to certify desirable\nbehaviors such as adversarial robustness (the hidden states stay in the region\nthat generates accurate classification even under input perturbations) and\nsafety in continuous control (the system never leaves some safe set). We\ndevelop a general approach using tools from non-linear control theory and\nsampling-based verification. Our approach empirically produces the strongest\nadversarial robustness guarantees compared to prior work on certifiably robust\nODE-based models (including implicit-depth models).",
    "descriptor": "",
    "authors": [
      "Yujia Huang",
      "Ivan Dario Jimenez Rodriguez",
      "Huan Zhang",
      "Yuanyuan Shi",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16940"
  },
  {
    "id": "arXiv:2210.16941",
    "title": "Hybrid Reusable Computational Analytics Workflow Management with  Cloudmesh",
    "abstract": "In this paper, we summarize our effort to create and utilize a simple\nframework to coordinate computational analytics tasks with the help of a\nworkflow system. Our design is based on a minimalistic approach while at the\nsame time allowing to access computational resources offered through the\nowner's computer, HPC computing centers, cloud resources, and distributed\nsystems in general. The access to this framework includes a simple GUI for\nmonitoring and managing the workflow, a REST service, a command line interface,\nas well as a Python interface. The resulting framework was developed for\nseveral examples targeting benchmarks of AI applications on hybrid compute\nresources and as an educational tool for teaching scientists and students\nsophisticated concepts to execute computations on resources ranging from a\nsingle computer to many thousands of computers as part of on-premise and cloud\ninfrastructure. We demonstrate the usefulness of the tool on a number of\nexamples. The code is available as an open-source project in GitHub and is\nbased on an easy-to-enhance tool called cloudmesh.",
    "descriptor": "\nComments: 12 pages, 3 apendies, 23 Figures, 4 Tables\n",
    "authors": [
      "Gregor von Laszewski",
      "J.P. Fleischer",
      "Geoffrey C. Fox"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.16941"
  },
  {
    "id": "arXiv:2210.16943",
    "title": "ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder  Facial Diagnosis",
    "abstract": "Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with\nvery high prevalence around the world. Research progress in the field of ASD\nfacial analysis in pediatric patients has been hindered due to a lack of\nwell-established baselines. In this paper, we propose the use of the Vision\nTransformer (ViT) for the computational analysis of pediatric ASD. The\npresented model, known as ViTASD, distills knowledge from large facial\nexpression datasets and offers model structure transferability. Specifically,\nViTASD employs a vanilla ViT to extract features from patients' face images and\nadopts a lightweight decoder with a Gaussian Process layer to enhance the\nrobustness for ASD analysis. Extensive experiments conducted on standard ASD\nfacial analysis benchmarks show that our method outperforms all of the\nrepresentative approaches in ASD facial analysis, while the ViTASD-L achieves a\nnew state-of-the-art. Our code and pretrained models are available at\nhttps://github.com/IrohXu/ViTASD.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Xu Cao",
      "Wenqian Ye",
      "Elena Sizikova",
      "Xue Bai",
      "Megan Coffee",
      "Hongwu Zeng",
      "Jianguo Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16943"
  },
  {
    "id": "arXiv:2210.16945",
    "title": "A new variable shape parameter strategy for RBF approximation using  neural networks",
    "abstract": "The choice of the shape parameter highly effects the behaviour of radial\nbasis function (RBF) approximations, as it needs to be selected to balance\nbetween ill-condition of the interpolation matrix and high accuracy. In this\npaper, we demonstrate how to use neural networks to determine the shape\nparameters in RBFs. In particular, we construct a multilayer perceptron trained\nusing an unsupervised learning strategy, and use it to predict shape parameters\nfor inverse multiquadric and Gaussian kernels. We test the neural network\napproach in RBF interpolation tasks and in a RBF-finite difference method in\none and two-space dimensions, demonstrating promising results.",
    "descriptor": "",
    "authors": [
      "Fatemeh Nassajian Mojarrad",
      "Maria Han Veiga",
      "Jan S. Hesthaven",
      "Philipp \u00d6ffner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16945"
  },
  {
    "id": "arXiv:2210.16947",
    "title": "Two Models are Better than One: Federated Learning Is Not Private For  Google GBoard Next Word Prediction",
    "abstract": "In this paper we present new attacks against federated learning when used to\ntrain natural language text models. We illustrate the effectiveness of the\nattacks against the next word prediction model used in Google's GBoard app, a\nwidely used mobile keyboard app that has been an early adopter of federated\nlearning for production use. We demonstrate that the words a user types on\ntheir mobile handset, e.g. when sending text messages, can be recovered with\nhigh accuracy under a wide range of conditions and that counter-measures such a\nuse of mini-batches and adding local noise are ineffective. We also show that\nthe word order (and so the actual sentences typed) can be reconstructed with\nhigh fidelity. This raises obvious privacy concerns, particularly since GBoard\nis in production use.",
    "descriptor": "",
    "authors": [
      "Mohamed Suliman",
      "Douglas Leith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16947"
  },
  {
    "id": "arXiv:2210.16949",
    "title": "Decentralized Channel Management in WLANs with Graph Neural Networks",
    "abstract": "Wireless local area networks (WLANs) manage multiple access points (APs) and\nassign scarce radio frequency resources to APs for satisfying traffic demands\nof associated user devices. This paper considers the channel allocation problem\nin WLANs that minimizes the mutual interference among APs, and puts forth a\nlearning-based solution that can be implemented in a decentralized manner. We\nformulate the channel allocation problem as an unsupervised learning problem,\nparameterize the control policy of radio channels with graph neural networks\n(GNNs), and train GNNs with the policy gradient method in a model-free manner.\nThe proposed approach allows for a decentralized implementation due to the\ndistributed nature of GNNs and is equivariant to network permutations. The\nformer provides an efficient and scalable solution for large network scenarios,\nand the latter renders our algorithm independent of the AP reordering.\nEmpirical results are presented to evaluate the proposed approach and\ncorroborate theoretical findings.",
    "descriptor": "",
    "authors": [
      "Zhan Gao",
      "Yulin Shao",
      "Deniz Gunduz",
      "Amanda Prorok"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16949"
  },
  {
    "id": "arXiv:2210.16952",
    "title": "Transfer Learning with Synthetic Corpora for Spatial Role Labeling and  Reasoning",
    "abstract": "Recent research shows synthetic data as a source of supervision helps\npretrained language models (PLM) transfer learning to new target tasks/domains.\nHowever, this idea is less explored for spatial language. We provide two new\ndata resources on multiple spatial language processing tasks. The first dataset\nis synthesized for transfer learning on spatial question answering (SQA) and\nspatial role labeling (SpRL). Compared to previous SQA datasets, we include a\nlarger variety of spatial relation types and spatial expressions. Our data\ngeneration process is easily extendable with new spatial expression lexicons.\nThe second one is a real-world SQA dataset with human-generated questions built\non an existing corpus with SPRL annotations. This dataset can be used to\nevaluate spatial language processing models in realistic situations. We show\npretraining with automatically generated data significantly improves the SOTA\nresults on several SQA and SPRL benchmarks, particularly when the training data\nin the target domain is small.",
    "descriptor": "\nComments: The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)\n",
    "authors": [
      "Roshanak Mirzaee",
      "Parisa Kordjamshidi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16952"
  },
  {
    "id": "arXiv:2210.16953",
    "title": "Improving Bilingual Lexicon Induction with Cross-Encoder Reranking",
    "abstract": "Bilingual lexicon induction (BLI) with limited bilingual supervision is a\ncrucial yet challenging task in multilingual NLP. Current state-of-the-art BLI\nmethods rely on the induction of cross-lingual word embeddings (CLWEs) to\ncapture cross-lingual word similarities; such CLWEs are obtained 1) via\ntraditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs\nfrom multilingual pretrained language models (mPLMs), or 3) through combining\nthe former two options. In this work, we propose a novel semi-supervised\npost-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking),\napplicable to any precalculated CLWE space, which improves their BLI\ncapability. The key idea is to 'extract' cross-lingual lexical knowledge from\nmPLMs, and then combine it with the original CLWEs. This crucial step is done\nvia 1) creating a word similarity dataset, comprising positive word pairs\n(i.e., true translations) and hard negative pairs induced from the original\nCLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a\ncross-encoder manner to predict the similarity scores. At inference, we 3)\ncombine the similarity score from the original CLWE space with the score from\nthe BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on\ntwo standard BLI benchmarks spanning a wide spectrum of diverse languages: it\nsubstantially outperforms a series of strong baselines across the board. We\nalso validate the robustness of BLICEr with different CLWEs.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Yaoyiran Li",
      "Fangyu Liu",
      "Ivan Vuli\u0107",
      "Anna Korhonen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16953"
  },
  {
    "id": "arXiv:2210.16954",
    "title": "Few-Shot Classification of Skin Lesions from Dermoscopic Images by  Meta-Learning Representative Embeddings",
    "abstract": "Annotated images and ground truth for the diagnosis of rare and novel\ndiseases are scarce. This is expected to prevail, considering the small number\nof affected patient population and limited clinical expertise to annotate\nimages. Further, the frequently occurring long-tailed class distributions in\nskin lesion and other disease classification datasets cause conventional\ntraining approaches to lead to poor generalization due to biased class priors.\nFew-shot learning, and meta-learning in general, aim to overcome these issues\nby aiming to perform well in low data regimes. This paper focuses on improving\nmeta-learning for the classification of dermoscopic images. Specifically, we\npropose a baseline supervised method on the meta-training set that allows a\nnetwork to learn highly representative and generalizable feature embeddings for\nimages, that are readily transferable to new few-shot learning tasks. We follow\nsome of the previous work in literature that posit that a representative\nfeature embedding can be more effective than complex meta-learning algorithms.\nWe empirically prove the efficacy of the proposed meta-training method on\ndermoscopic images for learning embeddings, and show that even simple linear\nclassifiers trained atop these representations suffice to outperform some of\nthe usual meta-learning methods.",
    "descriptor": "\nComments: 10 pages, 3 figures, 2 tables\n",
    "authors": [
      "Karthik Desingu",
      "Mirunalini P.",
      "Aravindan Chandrabose"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16954"
  },
  {
    "id": "arXiv:2210.16956",
    "title": "Reward Shaping Using Convolutional Neural Network",
    "abstract": "In this paper, we propose Value Iteration Network for Reward Shaping\n(VIN-RS), a potential-based reward shaping mechanism using Convolutional Neural\nNetwork (CNN). The proposed VIN-RS embeds a CNN trained on computed labels\nusing the message passing mechanism of the Hidden Markov Model. The CNN\nprocesses images or graphs of the environment to predict the shaping values.\nRecent work on reward shaping still has limitations towards training on a\nrepresentation of the Markov Decision Process (MDP) and building an estimate of\nthe transition matrix. The advantage of VIN-RS is to construct an effective\npotential function from an estimated MDP while automatically inferring the\nenvironment transition matrix. The proposed VIN-RS estimates the transition\nmatrix through a self-learned convolution filter while extracting environment\ndetails from the input frames or sampled graphs. Due to (1) the previous\nsuccess of using message passing for reward shaping; and (2) the CNN planning\nbehavior, we use these messages to train the CNN of VIN-RS. Experiments are\nperformed on tabular games, Atari 2600 and MuJoCo, for discrete and continuous\naction space. Our results illustrate promising improvements in the learning\nspeed and maximum cumulative reward compared to the state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Hani Sami",
      "Hadi Otrok",
      "Jamal Bentahar",
      "Azzam Mourad",
      "Ernesto Damiani"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16956"
  },
  {
    "id": "arXiv:2210.16960",
    "title": "Multilingual Multimodality: A Taxonomical Survey of Datasets,  Techniques, Challenges and Opportunities",
    "abstract": "Contextualizing language technologies beyond a single language kindled\nembracing multiple modalities and languages. Individually, each of these\ndirections undoubtedly proliferated into several NLP tasks. Despite this\nmomentum, most of the multimodal research is primarily centered around English\nand multilingual research is primarily centered around contexts from text\nmodality. Challenging this conventional setup, researchers studied the\nunification of multilingual and multimodal (MultiX) streams. The main goal of\nthis work is to catalogue and characterize these works by charting out the\ncategories of tasks, datasets and methods to address MultiX scenarios. To this\nend, we review the languages studied, gold or silver data with parallel\nannotations, and understand how these modalities and languages interact in\nmodeling. We present an account of the modeling approaches along with their\nstrengths and weaknesses to better understand what scenarios they can be used\nreliably. Following this, we present the high-level trends in the overall\nparadigm of the field. Finally, we conclude by presenting a road map of\nchallenges and promising research directions.",
    "descriptor": "",
    "authors": [
      "Khyathi Raghavi Chandu",
      "Alborz Geramifard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16960"
  },
  {
    "id": "arXiv:2210.16963",
    "title": "Learning Heuristics for the Maximum Clique Enumeration Problem Using Low  Dimensional Representations",
    "abstract": "Approximate solutions to various NP-hard combinatorial optimization problems\nhave been found by learned heuristics using complex learning models. In\nparticular, vertex (node) classification in graphs has been a helpful method\ntowards finding the decision boundary to distinguish vertices in an optimal set\nfrom the rest. By following this approach, we use a learning framework for a\npruning process of the input graph towards reducing the runtime of the maximum\nclique enumeration problem. We extensively study the role of using different\nvertex representations on the performance of this heuristic method, using graph\nembedding algorithms, such as Node2vec and DeepWalk, and representations using\nhigher-order graph features comprising local subgraph counts. Our results show\nthat Node2Vec and DeepWalk are promising embedding methods in representing\nnodes towards classification purposes. We observe that using local graph\nfeatures in the classification process produce more accurate results when\ncombined with a feature elimination process. Finally, we provide tests on\nrandom graphs to show the robustness and scalability of our method.",
    "descriptor": "",
    "authors": [
      "Ali Baran Ta\u015fdemir",
      "Tuna Karacan",
      "Emir Kaan K\u0131rmac\u0131",
      "Lale \u00d6zkahya"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16963"
  },
  {
    "id": "arXiv:2210.16966",
    "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
    "abstract": "Point cloud data is ubiquitous in scientific fields. Recently, geometric deep\nlearning (GDL) has been widely applied to solve prediction tasks with such\ndata. However, GDL models are often complicated and hardly interpretable, which\nposes concerns to scientists when deploying these models in scientific analysis\nand experiments. This work proposes a general mechanism named learnable\nrandomness injection (LRI), which allows building inherently interpretable\nmodels based on general GDL backbones. LRI-induced models, once being trained,\ncan detect the points in the point cloud data that carry information indicative\nof the prediction label. We also propose four datasets from real scientific\napplications that cover the domains of high-energy physics and biochemistry to\nevaluate the LRI mechanism. Compared with previous post-hoc interpretation\nmethods, the points detected by LRI align much better and stabler with the\nground-truth patterns that have actual scientific meanings. LRI is grounded by\nthe information bottleneck principle. LRI-induced models also show more\nrobustness to the distribution shifts between training and test scenarios. Our\ncode and datasets are available at \\url{https://github.com/Graph-COM/LRI}.",
    "descriptor": "",
    "authors": [
      "Siqi Miao",
      "Yunan Luo",
      "Mia Liu",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16966"
  },
  {
    "id": "arXiv:2210.16976",
    "title": "Representation Learning for General-sum Low-rank Markov Games",
    "abstract": "We study multi-agent general-sum Markov games with nonlinear function\napproximation. We focus on low-rank Markov games whose transition matrix admits\na hidden low-rank structure on top of an unknown non-linear representation. The\ngoal is to design an algorithm that (1) finds an $\\varepsilon$-equilibrium\npolicy sample efficiently without prior knowledge of the environment or the\nrepresentation, and (2) permits a deep-learning friendly implementation. We\nleverage representation learning and present a model-based and a model-free\napproach to construct an effective representation from the collected data. For\nboth approaches, the algorithm achieves a sample complexity of\npoly$(H,d,A,1/\\varepsilon)$, where $H$ is the game horizon, $d$ is the\ndimension of the feature vector, $A$ is the size of the joint action space and\n$\\varepsilon$ is the optimality gap. When the number of players is large, the\nabove sample complexity can scale exponentially with the number of players in\nthe worst case. To address this challenge, we consider Markov games with a\nfactorized transition structure and present an algorithm that escapes such\nexponential scaling. To our best knowledge, this is the first sample-efficient\nalgorithm for multi-agent general-sum Markov games that incorporates\n(non-linear) function approximation. We accompany our theoretical result with a\nneural network-based implementation of our algorithm and evaluate it against\nthe widely used deep RL baseline, DQN with fictitious play.",
    "descriptor": "",
    "authors": [
      "Chengzhuo Ni",
      "Yuda Song",
      "Xuezhou Zhang",
      "Chi Jin",
      "Mengdi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.16976"
  },
  {
    "id": "arXiv:2210.16978",
    "title": "XMD: An End-to-End Framework for Interactive Explanation-Based Debugging  of NLP Models",
    "abstract": "NLP models are susceptible to learning spurious biases (i.e., bugs) that work\non some datasets but do not properly reflect the underlying task.\nExplanation-based model debugging aims to resolve spurious biases by showing\nhuman users explanations of model behavior, asking users to give feedback on\nthe behavior, then using the feedback to update the model. While existing model\ndebugging methods have shown promise, their prototype-level implementations\nprovide limited practical utility. Thus, we propose XMD: the first open-source,\nend-to-end framework for explanation-based model debugging. Given task- or\ninstance-level explanations, users can flexibly provide various forms of\nfeedback via an intuitive, web-based UI. After receiving user feedback, XMD\nautomatically updates the model in real time, by regularizing the model so that\nits explanations align with the user feedback. The new model can then be easily\ndeployed into real-world applications via Hugging Face. Using XMD, we can\nimprove the model's OOD performance on text classification tasks by up to 18%.",
    "descriptor": "\nComments: 6 pages, 7 figures. Project page: this https URL\n",
    "authors": [
      "Dong-Ho Lee",
      "Akshen Kadakia",
      "Brihi Joshi",
      "Aaron Chan",
      "Ziyi Liu",
      "Kiran Narahari",
      "Takashi Shibuya",
      "Ryosuke Mitani",
      "Toshiyuki Sekiya",
      "Jay Pujara",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16978"
  },
  {
    "id": "arXiv:2210.16979",
    "title": "When Do We Need GNN for Node Classification?",
    "abstract": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by\nadditionally making use of graph structure based on the relational inductive\nbias (edge bias), rather than treating the nodes as collections of independent\nand identically distributed (\\iid) samples. Though GNNs are believed to\noutperform basic NNs in real-world tasks, it is found that in some cases, GNNs\nhave little performance gain or even underperform graph-agnostic NNs. To\nidentify these cases, based on graph signal processing and statistical\nhypothesis testing, we propose two measures which analyze the cases in which\nthe edge bias in features and labels does not provide advantages. Based on the\nmeasures, a threshold value can be given to predict the potential performance\nadvantages of graph-aware models over graph-agnostic models.",
    "descriptor": "",
    "authors": [
      "Sitao Luan",
      "Chenqing Hua",
      "Qincheng Lu",
      "Jiaqi Zhu",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16979"
  },
  {
    "id": "arXiv:2210.16981",
    "title": "Monitoring and Detection of Low-current High-Impedance Faults in  Distribution Networks",
    "abstract": "Faults in electricity distribution networks have the potential to ignite\nfires, cause electrocution, and damage the system itself. High current Low\nImpedance Faults (LIF) are typically detected and mitigated via over-current,\ndistance, directional relays, fuses, etc. In contrast, while High Impedance\nFaults (HIF) are equally hazardous, they are much more challenging to detect\ndue to the fault current being much lower than load currents and their\ntime-varying and nonlinear behaviour. Moreover, New Zealand distribution\nnetworks are extensive and largely unmonitored beyond the substation, and\nsuitable HIF detection schemes are still an ongoing research challenge.\nTo date, we have built a physical test facility for power system fault\nanalysis and developing and evaluating our sensing and fault detection system.\nWe have simulated LIF and HIF with different fault surface materials and\nload-switching events. From the data collected, we have characterized the\nunique fault behaviour for both LIF and HIF in 400V networks and trained a Deep\nLearning classifier to recognize the type of fault present from its unique\nsignature. We have developed an outdoor pole mountable sensing system and have\ninstalled this in Wellington Electricity's network for ongoing data collection\nand evaluation.\nThis paper will describe the test facility and our experience developing and\nimplementing the sensing system. The widest range of HIF phenomena observed was\nin the fault experiments involving the tree branch. For brevity, therefore,\nthis paper reports on the results of just these tree-branch experiments. HIF\nfaults on other surface materials will be reported elsewhere. Finally, we will\ndetail the pole-mountable sensing system installed in Wellington Electricity's\nnetwork and the outcomes thus far.",
    "descriptor": "\nComments: 10 pages, 8 figures, EEA Conference and Exhibition 2021, 30 June to 1 July, Wellington, New Zealand\n",
    "authors": [
      "Anwarul Islam Sifat",
      "Fiona J. Stevens McFadden",
      "Ramesh Rayudu",
      "Joseph Bailey"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16981"
  },
  {
    "id": "arXiv:2210.16982",
    "title": "Computation of parabolic cylinder functions having complex argument",
    "abstract": "Numerical methods for the computation of the parabolic cylinder $U(a,z)$ for\nreal $a$ and complex $z$ are presented. The main tools are recent asymptotic\nexpansions involving exponential and Airy functions, with slowly varying\nanalytic coefficient functions involving simple coefficients, and stable\nintegral representations; these two main main methods can be complemented with\nMaclaurin series and a Poincar\\'e asymptotic expansion. We provide numerical\nevidence showing that the combination of these methods is enough for computing\nthe function with $5\\times 10^{-13}$ relative accuracy in double precision\nfloating point arithmetic.",
    "descriptor": "",
    "authors": [
      "T. M. Dunster",
      "A. Gil",
      "J. Segura"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Analysis and ODEs (math.CA)"
    ],
    "url": "https://arxiv.org/abs/2210.16982"
  },
  {
    "id": "arXiv:2210.16984",
    "title": "Synthesizer Preset Interpolation using Transformer Auto-Encoders",
    "abstract": "Sound synthesizers are widespread in modern music production but they\nincreasingly require expert skills to be mastered. This work focuses on\ninterpolation between presets, i.e., sets of values of all sound synthesis\nparameters, to enable the intuitive creation of new sounds from existing ones.\nWe introduce a bimodal auto-encoder neural network, which simultaneously\nprocesses presets using multi-head attention blocks, and audio using\nconvolutions. This model has been tested on a popular frequency modulation\nsynthesizer with more than one hundred parameters. Experiments have compared\nthe model to related architectures and methods, and have demonstrated that it\nperforms smoother interpolations. After training, the proposed model can be\nintegrated into commercial synthesizers for live interpolation or sound design\ntasks.",
    "descriptor": "",
    "authors": [
      "Gwendal Le Vaillant",
      "Thierry Dutoit"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.16984"
  },
  {
    "id": "arXiv:2210.16986",
    "title": "A Practical Distributed ADMM Solver for Billion-Scale Generalized  Assignment Problems",
    "abstract": "Assigning items to owners is a common problem found in various real-world\napplications, for example, audience-channel matching in marketing campaigns,\nborrower-lender matching in loan management, and shopper-merchant matching in\ne-commerce. Given an objective and multiple constraints, an assignment problem\ncan be formulated as a constrained optimization problem. Such assignment\nproblems are usually NP-hard, so when the number of items or the number of\nowners is large, solving for exact solutions becomes challenging. In this\npaper, we are interested in solving constrained assignment problems with\nhundreds of millions of items. Thus, with just tens of owners, the number of\ndecision variables is at billion-scale. This scale is usually seen in the\ninternet industry, which makes decisions for large groups of users. We relax\nthe possible integer constraint, and formulate a general optimization problem\nthat covers commonly seen assignment problems. Its objective function is\nconvex. Its constraints are either linear, or convex and separable by items. We\nstudy to solve our generalized assignment problems in the Bregman Alternating\nDirection Method of Multipliers (BADMM) framework where we exploit Bregman\ndivergence to transform the Augmented Lagrangian into a separable form, and\nsolve many subproblems in parallel. The entire solution can thus be implemented\nusing a MapReduce-style distributed computation framework. We present\nexperiment results on both synthetic and real-world datasets to verify its\naccuracy and scalability.",
    "descriptor": "",
    "authors": [
      "Jun Zhou",
      "Feng Qi",
      "Zhigang Hua",
      "Daohong Jian",
      "Ziqi Liu",
      "Hua Wu",
      "Xingwen Zhang",
      "Shuang Yang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.16986"
  },
  {
    "id": "arXiv:2210.16987",
    "title": "Symbolic Distillation for Learned TCP Congestion Control",
    "abstract": "Recent advances in TCP congestion control (CC) have achieved tremendous\nsuccess with deep reinforcement learning (RL) approaches, which use feedforward\nneural networks (NN) to learn complex environment conditions and make better\ndecisions. However, such \"black-box\" policies lack interpretability and\nreliability, and often, they need to operate outside the traditional TCP\ndatapath due to the use of complex NNs. This paper proposes a novel two-stage\nsolution to achieve the best of both worlds: first to train a deep RL agent,\nthen distill its (over-)parameterized NN policy into white-box, light-weight\nrules in the form of symbolic expressions that are much easier to understand\nand to implement in constrained environments. At the core of our proposal is a\nnovel symbolic branching algorithm that enables the rule to be aware of the\ncontext in terms of various network conditions, eventually converting the NN\npolicy into a symbolic tree. The distilled symbolic rules preserve and often\nimprove performance over state-of-the-art NN policies while being faster and\nsimpler than a standard neural network. We validate the performance of our\ndistilled symbolic rules on both simulation and emulation environments. Our\ncode is available at https://github.com/VITA-Group/SymbolicPCC.",
    "descriptor": "\nComments: Accepted in Advances in Neural Information Processing Systems (NeurIPS), 2022\n",
    "authors": [
      "S P Sharan",
      "Wenqing Zheng",
      "Kuo-Feng Hsu",
      "Jiarong Xing",
      "Ang Chen",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.16987"
  },
  {
    "id": "arXiv:2210.16989",
    "title": "Validity Assessment of Legal Will Statements as Natural Language  Inference",
    "abstract": "This work introduces a natural language inference (NLI) dataset that focuses\non the validity of statements in legal wills. This dataset is unique because:\n(a) each entailment decision requires three inputs: the statement from the\nwill, the law, and the conditions that hold at the time of the testator's\ndeath; and (b) the included texts are longer than the ones in current NLI\ndatasets. We trained eight neural NLI models in this dataset. All the models\nachieve more than 80% macro F1 and accuracy, which indicates that neural\napproaches can handle this task reasonably well. However, group accuracy, a\nstricter evaluation measure that is calculated with a group of positive and\nnegative examples generated from the same statement as a unit, is in mid 80s at\nbest, which suggests that the models' understanding of the task remains\nsuperficial. Further ablative analyses and explanation experiments indicate\nthat all three text segments are used for prediction, but some decisions rely\non semantically irrelevant tokens. This indicates that overfitting on these\nlonger texts likely happens, and that additional research is required for this\ntask to be solved.",
    "descriptor": "\nComments: 10 pages, 4 figures; To be published in the Findings of the Association for Computational Linguistics: EMNLP 2022\n",
    "authors": [
      "Alice Saebom Kwak",
      "Jacob O. Israelsen",
      "Clayton T. Morrison",
      "Derek E. Bambauer",
      "Mihai Surdeanu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16989"
  },
  {
    "id": "arXiv:2210.16998",
    "title": "TPGen: A Self-Stabilizing GPU-Based Method for Prime and Test Paths  Generation",
    "abstract": "This paper presents a novel scalable GPU-based method for Test Paths (TPs)\nand Prime Paths (PPs) Generation, called TPGen, used in structural testing and\nin test data generation. TPGen outperforms existing methods for PPs and TPs\ngeneration in several orders of magnitude, both in time and space efficiency.\nImproving both time and space efficiency is made possible through devising a\nnew non-contiguous and hierarchical memory allocation method, called\nThree-level Path Access Method (TPAM), that enables efficient storage of\nmaximal simple paths in memory. In addition to its high time and space\nefficiency, a major significance of TPGen includes its self-stabilizing design\nwhere threads execute in a fully asynchronous and order-oblivious way without\nusing any atomic instructions. TPGen can generate PPs and TPs of structurally\ncomplex programs that have an extremely high cyclomatic and Npath complexity.",
    "descriptor": "",
    "authors": [
      "Ebrahim Fazli",
      "Ali Ebnenasir"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.16998"
  },
  {
    "id": "arXiv:2210.17004",
    "title": "Character-level White-Box Adversarial Attacks against Transformers via  Attachable Subwords Substitution",
    "abstract": "We propose the first character-level white-box adversarial attack method\nagainst transformer models. The intuition of our method comes from the\nobservation that words are split into subtokens before being fed into the\ntransformer models and the substitution between two close subtokens has a\nsimilar effect to the character modification. Our method mainly contains three\nsteps. First, a gradient-based method is adopted to find the most vulnerable\nwords in the sentence. Then we split the selected words into subtokens to\nreplace the origin tokenization result from the transformer tokenizer. Finally,\nwe utilize an adversarial loss to guide the substitution of attachable\nsubtokens in which the Gumbel-softmax trick is introduced to ensure gradient\npropagation. Meanwhile, we introduce the visual and length constraint in the\noptimization process to achieve minimum character modifications. Extensive\nexperiments on both sentence-level and token-level tasks demonstrate that our\nmethod could outperform the previous attack methods in terms of success rate\nand edit distance. Furthermore, human evaluation verifies our adversarial\nexamples could preserve their origin labels.",
    "descriptor": "\nComments: 13 pages, 3 figures. EMNLP 2022\n",
    "authors": [
      "Aiwei Liu",
      "Honghai Yu",
      "Xuming Hu",
      "Shu'ang Li",
      "Li Lin",
      "Fukun Ma",
      "Yawen Yang",
      "Lijie Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17004"
  },
  {
    "id": "arXiv:2210.17008",
    "title": "Stokes's theorem in R",
    "abstract": "In this short article I introduce the stokes package which provides\nfunctionality for working with tensors, alternating forms, wedge products, and\nrelated concepts from the exterior calculus. Notation and spirit follow Spivak.\nStokes's generalized integral theorem, viz $\\int_{\\partial X}\\phi=\\int_Xd\\phi$,\nis demonstrated here using the package; it is available on CRAN\nathttps://CRAN.R-project.org/package=stokes.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Robin K. S. Hankin"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.17008"
  },
  {
    "id": "arXiv:2210.17009",
    "title": "Point-Syn2Real: Semi-Supervised Synthetic-to-Real Cross-Domain Learning  for Object Classification in 3D Point Clouds",
    "abstract": "Object classification using LiDAR 3D point cloud data is critical for modern\napplications such as autonomous driving. However, labeling point cloud data is\nlabor-intensive as it requires human annotators to visualize and inspect the 3D\ndata from different perspectives. In this paper, we propose a semi-supervised\ncross-domain learning approach that does not rely on manual annotations of\npoint clouds and performs similar to fully-supervised approaches. We utilize\navailable 3D object models to train classifiers that can generalize to\nreal-world point clouds. We simulate the acquisition of point clouds by\nsampling 3D object models from multiple viewpoints and with arbitrary partial\nocclusions. We then augment the resulting set of point clouds through random\nrotations and adding Gaussian noise to better emulate the real-world scenarios.\nWe then train point cloud encoding models, e.g., DGCNN, PointNet++, on the\nsynthesized and augmented datasets and evaluate their cross-domain\nclassification performance on corresponding real-world datasets. We also\nintroduce Point-Syn2Real, a new benchmark dataset for cross-domain learning on\npoint clouds. The results of our extensive experiments with this dataset\ndemonstrate that the proposed cross-domain learning approach for point clouds\noutperforms the related baseline and state-of-the-art approaches in both indoor\nand outdoor settings in terms of cross-domain generalizability. The code and\ndata will be available upon publishing.",
    "descriptor": "",
    "authors": [
      "Ziwei Wang",
      "Reza Arablouei",
      "Jiajun Liu",
      "Paulo Borges",
      "Greg Bishop-Hurley",
      "Nicholas Heaney"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.17009"
  },
  {
    "id": "arXiv:2210.17011",
    "title": "A picture of the space of typical learnable tasks",
    "abstract": "We develop a technique to analyze representations learned by deep networks\nwhen they are trained on different tasks using supervised, meta- and\ncontrastive learning. We develop a technique to visualize such representations\nusing an isometric embedding of the space of probabilistic models into a\nlower-dimensional space, i.e., one that preserves pairwise distances. We\ndiscover the following surprising phenomena that shed light upon the structure\nin the space of learnable tasks: (1) the manifold of probabilistic models\ntrained on different tasks using different representation learning methods is\neffectively low-dimensional; (2) supervised learning on one task results in a\nsurprising amount of progress on seemingly dissimilar tasks; progress on other\ntasks is larger if the training task has diverse classes; (3) the structure of\nthe space of tasks indicated by our analysis is consistent with parts of the\nWordnet phylogenetic tree; (4) fine-tuning a model upon a sub-task does not\nchange the representation much if the model was trained for a large number of\nepochs; (5) episodic meta-learning algorithms fit similar models eventually as\nthat of supervised learning, even if the two traverse different trajectories\nduring training; (6) contrastive learning methods trained on different datasets\nlearn similar representations. We use classification tasks constructed from the\nCIFAR-10 and Imagenet datasets to study these phenomena.",
    "descriptor": "",
    "authors": [
      "Rahul Ramesh",
      "Jialin Mao",
      "Itay Griniasty",
      "Rubing Yang",
      "Han Kheng Teoh",
      "Mark Transtrum",
      "James P. Sethna",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17011"
  },
  {
    "id": "arXiv:2210.17013",
    "title": "Embedding Space Augmentation for Weakly Supervised Learning in  Whole-Slide Images",
    "abstract": "Multiple Instance Learning (MIL) is a widely employed framework for learning\non gigapixel whole-slide images (WSIs) from WSI-level annotations. In most MIL\nbased analytical pipelines for WSI-level analysis, the WSIs are often divided\ninto patches and deep features for patches (i.e., patch embeddings) are\nextracted prior to training to reduce the overall computational cost and cope\nwith the GPUs' limited RAM. To overcome this limitation, we present\nEmbAugmenter, a data augmentation generative adversarial network (DA-GAN) that\ncan synthesize data augmentations in the embedding space rather than in the\npixel space, thereby significantly reducing the computational requirements.\nExperiments on the SICAPv2 dataset show that our approach outperforms MIL\nwithout augmentation and is on par with traditional patch-level augmentation\nfor MIL training while being substantially faster.",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table, ISBI 2023\n",
    "authors": [
      "Imaad Zaffar",
      "Guillaume Jaume",
      "Nasir Rajpoot",
      "Faisal Mahmood"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17013"
  },
  {
    "id": "arXiv:2210.17014",
    "title": "Parametrically driven inertial sensing in chip-scale optomechanical  cavities at the thermodynamical limits with extended dynamic range",
    "abstract": "Recent scientific and technological advances have enabled the detection of\ngravitational waves, autonomous driving, and the proposal of a communications\nnetwork on the Moon (Lunar Internet or LunaNet). These efforts are based on the\nmeasurement of minute displacements and correspondingly the forces or fields\ntransduction, which translate to acceleration, velocity, and position\ndetermination for navigation. State-of-the-art accelerometers use capacitive or\npiezo resistive techniques, and micro-electromechanical systems (MEMS) via\nintegrated circuit (IC) technologies in order to drive the transducer and\nconvert its output for electric readout. In recent years, laser optomechanical\ntransduction and readout have enabled highly sensitive detection of motional\ndisplacement. Here we further examine the theoretical framework for the novel\nmechanical frequency readout technique of optomechanical transduction when the\nsensor is driven into oscillation mode [8]. We demonstrate theoretical and\nphysical agreement and characterize the most relevant performance parameters\nwith a device with 1.5mg/Hz acceleration sensitivity, a 2.5 fm/Hz1/2\ndisplacement resolution corresponding to a 17.02 ug/Hz1/2 force-equivalent\nacceleration, and a 5.91 Hz/nW power sensitivity, at the thermodynamical\nlimits. In addition, we present a novel technique for dynamic range extension\nwhile maintaining the precision sensing sensitivity. Our inertial accelerometer\nis integrated on-chip, and enabled for packaging, with a laser-detuning-enabled\napproach.",
    "descriptor": "",
    "authors": [
      "Jaime Gonzalo Flor Flores",
      "Talha Yerebakan",
      "Wenting Wang",
      "Mingbin Yu",
      "Dim-Lee Kwong",
      "Andrey Matsko",
      "Chee Wei Wong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.17014"
  },
  {
    "id": "arXiv:2210.17016",
    "title": "Wespeaker: A Research and Production oriented Speaker Embedding Learning  Toolkit",
    "abstract": "Speaker modeling is essential for many related tasks, such as speaker\nrecognition and speaker diarization. The dominant modeling approach is\nfixed-dimensional vector representation, i.e., speaker embedding. This paper\nintroduces a research and production oriented speaker embedding learning\ntoolkit, Wespeaker. Wespeaker contains the implementation of scalable data\nmanagement, state-of-the-art speaker embedding models, loss functions, and\nscoring back-ends, with highly competitive results achieved by structured\nrecipes which were adopted in the winning systems in several speaker\nverification challenges. The application to other downstream tasks such as\nspeaker diarization is also exhibited in the related recipe. Moreover, CPU- and\nGPU-compatible deployment codes are integrated for production-oriented\ndevelopment. The toolkit is publicly available at\nhttps://github.com/wenet-e2e/wespeaker.",
    "descriptor": "",
    "authors": [
      "Hongji Wang",
      "Chengdong Liang",
      "Shuai Wang",
      "Zhengyang Chen",
      "Binbin Zhang",
      "Xu Xiang",
      "Yanlei Deng",
      "Yanmin Qian"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17016"
  },
  {
    "id": "arXiv:2210.17017",
    "title": "Blank Collapse: Compressing CTC emission for the faster decoding",
    "abstract": "Connectionist Temporal Classification (CTC) model is a very efficient method\nfor modeling sequences, especially for speech data. In order to use CTC model\nas an Automatic Speech Recognition (ASR) task, the beam search decoding with an\nexternal language model like n-gram LM is necessary to obtain reasonable\nresults. In this paper we analyze the blank label in CTC beam search deeply and\npropose a very simple method to reduce the amount of calculation resulting in\nfaster beam search decoding speed. With this method, we can get up to 78%\nfaster decoding speed than ordinary beam search decoding with a very small loss\nof accuracy in LibriSpeech datasets. We prove this method is effective not only\npractically by experiments but also theoretically by mathematical reasoning. We\nalso observe that this reduction is more obvious if the accuracy of the model\nis higher.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Minkyu Jung",
      "Ohhyeok Kwon",
      "Seunghyun Seo",
      "Soonshin Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17017"
  },
  {
    "id": "arXiv:2210.17020",
    "title": "A Law of Data Separation in Deep Learning",
    "abstract": "Multilayer neural networks have achieved superhuman performance in many\nartificial intelligence applications. However, their black-box nature obscures\nthe underlying mechanism for transforming input data into labels throughout all\nlayers, thus hindering architecture design for new tasks and interpretation for\nhigh-stakes decision makings. We addressed this problem by introducing a\nprecise law that governs how real-world deep neural networks separate data\naccording to their class membership from the bottom layers to the top layers in\nclassification problems. This law shows that each layer roughly improves a\ncertain measure of data separation by an \\textit{equal} multiplicative factor.\nThis law manifests in modern architectures such as AlexNet, VGGNet, and ResNet\nin the late phase of training. This law together with the perspective of data\nseparation offers practical guidelines for designing network architectures,\nimproving model robustness and out-of-sample performance during training, as\nwell as interpreting deep learning predictions.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Hangfeng He",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17020"
  },
  {
    "id": "arXiv:2210.17025",
    "title": "Joint Optimization of Sensing and Computation for Status Update in  Mobile Edge Computing Systems",
    "abstract": "IoT devices recently are utilized to detect the state transition in the\nsurrounding environment and then transmit the status updates to the base\nstation for future system operations. To satisfy the stringent timeliness\nrequirement of the status updates for the accurate system control, age of\ninformation (AoI) is introduced to quantify the freshness of the sensory data.\nDue to the limited computing resources, the status update can be offloaded to\nthe mobile edge computing (MEC) server for execution to ensure the information\nfreshness. Since the status updates generated by insufficient sensing\noperations may be invalid and cause additional processing time, the data\nsensing and processing operations need to be considered simultaneously. In this\nwork, we formulate the joint data sensing and processing optimization problem\nto ensure the freshness of the status updates and reduce the energy consumption\nof IoT devices. Then, the formulated NP-hard problem is decomposed into the\nsampling, sensing and computation offloading optimization problems. Afterwards,\nwe propose a multi-variable iterative system cost minimization algorithm to\noptimize the system overhead. Simulation results show the efficiency of our\nmethod in decreasing the system cost and dominance of sensing and processing\nunder different scenarios.",
    "descriptor": "",
    "authors": [
      "Yi Chen",
      "Zheng Chang",
      "Geyong Min",
      "Shiwen Mao",
      "Timo H\u00e4m\u00e4l\u00e4inen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.17025"
  },
  {
    "id": "arXiv:2210.17027",
    "title": "Joint Pre-Training with Speech and Bilingual Text for Direct Speech to  Speech Translation",
    "abstract": "Direct speech-to-speech translation (S2ST) is an attractive research topic\nwith many advantages compared to cascaded S2ST. However, direct S2ST suffers\nfrom the data scarcity problem because the corpora from speech of the source\nlanguage to speech of the target language are very rare. To address this issue,\nwe propose in this paper a Speech2S model, which is jointly pre-trained with\nunpaired speech and bilingual text data for direct speech-to-speech translation\ntasks. By effectively leveraging the paired text data, Speech2S is capable of\nmodeling the cross-lingual speech conversion from source to target language. We\nverify the performance of the proposed Speech2S on Europarl-ST and VoxPopuli\ndatasets. Experimental results demonstrate that Speech2S gets an improvement of\nabout 5 BLEU scores compared to encoder-only pre-training models, and achieves\na competitive or even better performance than existing state-of-the-art\nmodels1.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Kun Wei",
      "Long Zhou",
      "Ziqiang Zhang",
      "Liping Chen",
      "Shujie Liu",
      "Lei He",
      "Jinyu Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17027"
  },
  {
    "id": "arXiv:2210.17028",
    "title": "Improved Learning-augmented Algorithms for k-means and k-medians  Clustering",
    "abstract": "We consider the problem of clustering in the learning-augmented setting,\nwhere we are given a data set in $d$-dimensional Euclidean space, and a label\nfor each data point given by an oracle indicating what subsets of points should\nbe clustered together. This setting captures situations where we have access to\nsome auxiliary information about the data set relevant for our clustering\nobjective, for instance the labels output by a neural network. Following prior\nwork, we assume that there are at most an $\\alpha \\in (0,c)$ for some $c<1$\nfraction of false positives and false negatives in each predicted cluster, in\nthe absence of which the labels would attain the optimal clustering cost\n$\\mathrm{OPT}$.\nFor a dataset of size $m$, we propose a deterministic $k$-means algorithm\nthat produces centers with improved bound on clustering cost compared to the\nprevious randomized algorithm while preserving the $O( d m \\log m)$ runtime.\nFurthermore, our algorithm works even when the predictions are not very\naccurate, i.e. our bound holds for $\\alpha$ up to $1/2$, an improvement over\n$\\alpha$ being at most $1/7$ in the previous work. For the $k$-medians problem\nwe improve upon prior work by achieving a biquadratic improvement in the\ndependence of the approximation factor on the accuracy parameter $\\alpha$ to\nget a cost of $(1+O(\\alpha))\\mathrm{OPT}$, while requiring essentially just\n$O(md \\log^3 m/\\alpha)$ runtime.",
    "descriptor": "",
    "authors": [
      "Thy Nguyen",
      "Anamay Chaturvedi",
      "Huy L\u00ea Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17028"
  },
  {
    "id": "arXiv:2210.17029",
    "title": "Poison Attack and Defense on Deep Source Code Processing Models",
    "abstract": "In the software engineering community, deep learning (DL) has recently been\napplied to many source code processing tasks. Due to the poor interpretability\nof DL models, their security vulnerabilities require scrutiny. Recently,\nresearchers have identified an emergent security threat, namely poison attack.\nThe attackers aim to inject insidious backdoors into models by poisoning the\ntraining data with poison samples. Poisoned models work normally with clean\ninputs but produce targeted erroneous results with poisoned inputs embedded\nwith triggers. By activating backdoors, attackers can manipulate the poisoned\nmodels in security-related scenarios.\nTo verify the vulnerability of existing deep source code processing models to\nthe poison attack, we present a poison attack framework for source code named\nCodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable\neven human-imperceptible poison samples and attack models by poisoning the\ntraining data with poison samples. To defend against the poison attack, we\nfurther propose an effective defense approach named CodeDetector to detect\npoison samples in the training data. CodeDetector can be applied to many model\narchitectures and effectively defend against multiple poison attack approaches.\nWe apply our CodePoisoner and CodeDetector to three tasks, including defect\ndetection, clone detection, and code repair. The results show that (1)\nCodePoisoner achieves a high attack success rate (max: 100%) in misleading\nmodels to targeted erroneous behaviors. It validates that existing deep source\ncode processing models have a strong vulnerability to the poison attack. (2)\nCodeDetector effectively defends against multiple poison attack approaches by\ndetecting (max: 100%) poison samples in the training data. We hope this work\ncan help practitioners notice the poison attack and inspire the design of more\nadvanced defense techniques.",
    "descriptor": "\nComments: 23 pages, 9 figures\n",
    "authors": [
      "Jia Li",
      "Zhuo Li",
      "Huangzhao Zhang",
      "Ge Li",
      "Zhi Jin",
      "Xing Hu",
      "Xin Xia"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17029"
  },
  {
    "id": "arXiv:2210.17035",
    "title": "Evaluation of large-scale synthetic data for Grammar Error Correction",
    "abstract": "Grammar Error Correction(GEC) mainly relies on the availability of high\nquality of large amount of synthetic parallel data of grammatically correct and\nerroneous sentence pairs. The quality of the synthetic data is evaluated on how\nwell the GEC system performs when pre-trained using it. But this does not\nprovide much insight into what are the necessary factors which define the\nquality of these data. So this work aims to introduce 3 metrics - reliability,\ndiversity and distribution match to provide more insight into the quality of\nlarge-scale synthetic data generated for the GEC task, as well as automatically\nevaluate them. Evaluating these three metrics automatically can also help in\nproviding feedback to the data generation systems and thereby improve the\nquality of the synthetic data generated dynamically",
    "descriptor": "",
    "authors": [
      "Vanya Bannihatti Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17035"
  },
  {
    "id": "arXiv:2210.17036",
    "title": "Adaptive Population-based Simulated Annealing for Uncertain Resource  Constrained Job Scheduling",
    "abstract": "Transporting ore from mines to ports is of significant interest in mining\nsupply chains. These operations are commonly associated with growing costs and\na lack of resources. Large mining companies are interested in optimally\nallocating their resources to reduce operational costs. This problem has been\npreviously investigated in the literature as resource constrained job\nscheduling (RCJS). While a number of optimisation methods have been proposed to\ntackle the deterministic problem, the uncertainty associated with resource\navailability, an inevitable challenge in mining operations, has received less\nattention. RCJS with uncertainty is a hard combinatorial optimisation problem\nthat cannot be solved efficiently with existing optimisation methods. This\nstudy proposes an adaptive population-based simulated annealing algorithm that\ncan overcome the limitations of existing methods for RCJS with uncertainty\nincluding the premature convergence, the excessive number of hyper-parameters,\nand the inefficiency in coping with different uncertainty levels. This new\nalgorithm is designed to effectively balance exploration and exploitation, by\nusing a population, modifying the cooling schedule in the Metropolis-Hastings\nalgorithm, and using an adaptive mechanism to select perturbation operators.\nThe results show that the proposed algorithm outperforms existing methods\nacross a wide range of benchmark RCJS instances and uncertainty levels.\nMoreover, new best known solutions are discovered for all but one problem\ninstance across all uncertainty levels.",
    "descriptor": "",
    "authors": [
      "Dhananjay Thiruvady",
      "Su Nguyen",
      "Yuan Sun",
      "Fatemeh Shiri",
      "Nayyar Zaidi",
      "Xiaodong Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.17036"
  },
  {
    "id": "arXiv:2210.17039",
    "title": "Improving Multi-generation Robustness of Learned Image Compression",
    "abstract": "Benefit from flexible network designs and end-to-end joint optimization\napproach, learned image compression (LIC) has demonstrated excellent coding\nperformance and practical feasibility in recent years. However, existing\ncompression models suffer from serious multi-generation loss, which always\noccurs during image editing and transcoding. During the process of repeatedly\nencoding and decoding, the quality of the image will rapidly degrade, resulting\nin various types of distortion, which significantly limits the practical\napplication of LIC. In this paper, a thorough analysis is carried out to\ndetermine the source of generative loss in successive image compression (SIC).\nWe point out and solve the quantization drift problem that affects SIC,\nreversibility loss function as well as channel relaxation method are proposed\nto further reduce the generation loss. Experiments show that by using our\nproposed solutions, LIC can achieve comparable performance to the first\ncompression of BPG even after 50 times reencoding without any change of the\nnetwork structure.",
    "descriptor": "",
    "authors": [
      "Litian Li",
      "Zheng Yang",
      "Ronggang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.17039"
  },
  {
    "id": "arXiv:2210.17040",
    "title": "CodeEditor: Learning to Edit Source Code with Pre-trained Models",
    "abstract": "Developers often perform repetitive code editing activities for various\nreasons (e.g., code refactor) during software development. Many deep learning\nmodels are applied to automate code editing by learning from the code editing\nhistory. Recently, pre-trained code editing models have achieved the\nstate-of-the-art (SOTA) results. Pre-trained models are first pre-trained with\npre-training tasks and fine-tuned with the code editing task. Existing\npre-training tasks mainly are code infilling tasks (e.g., masked language\nmodeling), which are derived from the natural language processing field and are\nnot designed for code editing.\nIn this paper, we propose a pre-training task specialized in code editing and\npresent an effective pre-trained code editing model named CodeEditor. Our\npre-training task further improves the performance and generalization ability\nof code editing models. Specifically, we collect real-world code snippets as\nthe ground truth and use a generator to rewrite them into natural but inferior\nversions. Then, we pre-train our CodeEditor to edit inferior versions into the\nground truth, to learn edit patterns. We conduct experiments on four datasets\nand evaluate models in three settings. (1) In the fine-tuning setting, we\nfine-tune the pre-trained CodeEditor with four datasets. CodeEditor outperforms\nSOTA baselines by 15%, 25.5%, and 9.4% and 26.6% on four datasets. (2) In the\nfew-shot setting, we fine-tune the pre-trained CodeEditor with limited data.\nCodeEditor substantially performs better than all baselines, even outperforming\nbaselines that are fine-tuned with all data. (3) In the zero-shot setting, we\nevaluate the pre-trained CodeEditor without fine-tuning. CodeEditor correctly\nedits 1,113 programs while SOTA baselines can not work. The results prove that\nthe superiority of our pre-training task and the pre-trained CodeEditor is more\neffective in automatic code editing.",
    "descriptor": "\nComments: 18 pages, 6 figures\n",
    "authors": [
      "Jia Li",
      "Ge Li",
      "Zhuo Li",
      "Zhi Jin",
      "Xing Hu",
      "Kechi Zhang",
      "Zhiyi Fu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17040"
  },
  {
    "id": "arXiv:2210.17041",
    "title": "GPS: Genetic Prompt Search for Efficient Few-shot Learning",
    "abstract": "Prompt-based techniques have demostrated great potential for improving the\nfew-shot generalization of pretrained language models. However, their\nperformance heavily relies on the manual design of prompts and thus requires a\nlot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS)\nto improve few-shot learning with prompts, which utilizes a genetic algorithm\nto automatically search for high-performing prompts. GPS is gradient-free and\nrequires no update of model parameters but only a small validation set.\nExperiments on diverse datasets proved the effectiveness of GPS, which\noutperforms manual prompts by a large margin of 2.6 points. Our method is also\nbetter than other parameter-efficient tuning methods such as prompt tuning.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Hanwei Xu",
      "Yujun Chen",
      "Yulun Du",
      "Nan Shao",
      "Yanggang Wang",
      "Haiyu Li",
      "Zhilin Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17041"
  },
  {
    "id": "arXiv:2210.17043",
    "title": "Evaluating Point-Prediction Uncertainties in Neural Networks for Drug  Discovery",
    "abstract": "Neural Network (NN) models provide potential to speed up the drug discovery\nprocess and reduce its failure rates. The success of NN models require\nuncertainty quantification (UQ) as drug discovery explores chemical space\nbeyond the training data distribution. Standard NN models do not provide\nuncertainty information. Methods that combine Bayesian models with NN models\naddress this issue, but are difficult to implement and more expensive to train.\nSome methods require changing the NN architecture or training procedure,\nlimiting the selection of NN models. Moreover, predictive uncertainty can come\nfrom different sources. It is important to have the ability to separately model\ndifferent types of predictive uncertainty, as the model can take assorted\nactions depending on the source of uncertainty. In this paper, we examine UQ\nmethods that estimate different sources of predictive uncertainty for NN models\naiming at drug discovery. We use our prior knowledge on chemical compounds to\ndesign the experiments. By utilizing a visualization method we create\nnon-overlapping and chemically diverse partitions from a collection of chemical\ncompounds. These partitions are used as training and test set splits to explore\nNN model uncertainty. We demonstrate how the uncertainties estimated by the\nselected methods describe different sources of uncertainty under different\npartitions and featurization schemes and the relationship to prediction error.",
    "descriptor": "",
    "authors": [
      "Ya Ju Fan",
      "Jonathan E. Allen",
      "Kevin S. McLoughlin",
      "Da Shi",
      "Brian J. Bennion",
      "Xiaohua Zhang",
      "Felice C. Lightstone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.17043"
  },
  {
    "id": "arXiv:2210.17047",
    "title": "Block-Wise Dynamic-Precision Neural Network Training Acceleration via  Online Quantization Sensitivity Analytics",
    "abstract": "Data quantization is an effective method to accelerate neural network\ntraining and reduce power consumption. However, it is challenging to perform\nlow-bit quantized training: the conventional equal-precision quantization will\nlead to either high accuracy loss or limited bit-width reduction, while\nexisting mixed-precision methods offer high compression potential but failed to\nperform accurate and efficient bit-width assignment. In this work, we propose\nDYNASTY, a block-wise dynamic-precision neural network training framework.\nDYNASTY provides accurate data sensitivity information through fast online\nanalytics, and maintains stable training convergence with an adaptive bit-width\nmap generator. Network training experiments on CIFAR-100 and ImageNet dataset\nare carried out, and compared to 8-bit quantization baseline, DYNASTY brings up\nto $5.1\\times$ speedup and $4.7\\times$ energy consumption reduction with no\naccuracy drop and negligible hardware overhead.",
    "descriptor": "\nComments: 7 pages, to be published in 28th Asia and South Pacific Design Automation Conference (ASP-DAC 2023)\n",
    "authors": [
      "Ruoyang Liu",
      "Chenhan Wei",
      "Yixiong Yang",
      "Wenxun Wang",
      "Huazhong Yang",
      "Yongpan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.17047"
  },
  {
    "id": "arXiv:2210.17048",
    "title": "A replica exchange preconditioned Crank-Nicolson Langevin dynamic MCMC  method for Bayesian inverse problems",
    "abstract": "This paper proposes a replica exchange preconditioned Langevin diffusion\ndiscretized by the Crank-Nicolson scheme (repCNLD) to handle high-dimensional\nand multi-modal distribution problems. Sampling from high-dimensional and\nmulti-modal distributions is a challenging question. The performance of many\nstandard MCMC chains deteriorates as the dimension of parameters increases, and\nmany MCMC algorithms cannot capture all modes if the energy function is not\nconvex. The proposed repCNLD can accelerate the convergence of the single-chain\npCNLD, and can capture all modes of the multi-modal distributions. We proposed\nthe Crank-Nicolson discretization, which is robust. Moreover, the\ndiscretization error grows linearly with respect to the time step size. We\nextend repCNLD to the multi-variance setting to further accelerate the\nconvergence and save computation costs. Additionally, we derive an unbiased\nestimator of the swapping rate for the multi-variance repCNLD method, providing\na guide for the choice of the low-fidelity model used in the second chain. We\ntest our methods with high-dimensional Gaussian mixture models and\nhigh-dimensional nonlinear PDE inverse problems. Particularly, we employ the\ndiscrete adjoint method to efficiently calculate gradients for nonlinear PDE\ninverse problems.",
    "descriptor": "",
    "authors": [
      "Ou Na",
      "Zecheng Zhang",
      "Guang Lin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17048"
  },
  {
    "id": "arXiv:2210.17049",
    "title": "Modular Hybrid Autoregressive Transducer",
    "abstract": "Text-only adaptation of a transducer model remains challenging for end-to-end\nspeech recognition since the transducer has no clearly separated acoustic model\n(AM), language model (LM) or blank model. In this work, we propose a modular\nhybrid autoregressive transducer (MHAT) that has structurally separated label\nand blank decoders to predict label and blank distributions, respectively,\nalong with a shared acoustic encoder. The encoder and label decoder outputs are\ndirectly projected to AM and internal LM scores and then added to compute label\nposteriors. We train MHAT with an internal LM loss and a HAT loss to ensure\nthat its internal LM becomes a standalone neural LM that can be effectively\nadapted to text. Moreover, text adaptation of MHAT fosters a much better LM\nfusion than internal LM subtraction-based methods. On Google's large-scale\nproduction data, a multi-domain MHAT adapted with 100B sentences achieves\nrelative WER reductions of up to 12.4% without LM fusion and 21.5% with LM\nfusion from 400K-hour trained HAT.",
    "descriptor": "\nComments: 8 pages, 1 figure, SLT 2022\n",
    "authors": [
      "Zhong Meng",
      "Tongzhou Chen",
      "Rohit Prabhavalkar",
      "Yu Zhang",
      "Gary Wang",
      "Kartik Audhkhasi",
      "Jesse Emond",
      "Trevor Strohman",
      "Bhuvana Ramabhadran",
      "W. Ronny Huang",
      "Ehsan Variani",
      "Yinghui Huang",
      "Pedro J. Moreno"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17049"
  },
  {
    "id": "arXiv:2210.17051",
    "title": "Accelerating Carbon Capture and Storage Modeling using Fourier Neural  Operators",
    "abstract": "Carbon capture and storage (CCS) is an important strategy for reducing carbon\ndioxide emissions and mitigating climate change. We consider the storage aspect\nof CCS, which involves injecting carbon dioxide into underground reservoirs.\nThis requires accurate and high-resolution predictions of carbon dioxide plume\nmigration and reservoir pressure buildup. However, such modeling is challenging\nat scale due to the high computational costs of existing numerical methods. We\nintroduce a novel machine learning approach for four-dimensional\nspatial-temporal modeling, which speeds up predictions nearly 700,000 times\ncompared to existing methods. It provides highly accurate predictions under\ndiverse reservoir conditions, geological heterogeneity, and injection schemes.\nOur framework, Nested Fourier Neural Operator (FNO), learns the solution\noperator for the family of partial differential equations governing the carbon\ndioxide-water multiphase flow. It uses a hierarchy of FNO models to produce\noutputs at different refinement levels. Thus, our approach enables\nunprecedented real-time high-resolution modeling for carbon dioxide storage.",
    "descriptor": "",
    "authors": [
      "Gege Wen",
      "Zongyi Li",
      "Qirui Long",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar",
      "Sally M. Benson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2210.17051"
  },
  {
    "id": "arXiv:2210.17052",
    "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for  Self-Supervised Learning",
    "abstract": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of\nthe collision in which target data and its negative samples share the same\nclass can decrease performance. Especially in real-world data such as crawled\ndata or robot-gathered observations, collisions may occur more often due to the\nduplicates in the data. To deal with this problem, we claim that sampling\nnegative samples from the adaptively debiased distribution in the memory makes\nthe model more stable than sampling from a biased dataset directly. In this\npaper, we introduce a novel SSL framework with adaptive Duplicate Elimination\n(DUEL) inspired by the human working memory. The proposed framework\nsuccessfully prevents the downstream task performance from degradation due to a\ndramatic inter-class imbalance.",
    "descriptor": "\nComments: 9 pages, 5 figures, submitted to NeurIPS 2022 Workshop on Self-supervised Learning: Theory and Practice.(Accepted, this https URL)\n",
    "authors": [
      "Won-Seok Choi",
      "Dong-Sig Han",
      "Hyundo Lee",
      "Junseok Park",
      "Byoung-Tak Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17052"
  },
  {
    "id": "arXiv:2210.17053",
    "title": "Control and Simulation of Motion of Constrained Multibody Systems Based  on Projection Matrix Formulation",
    "abstract": "This paper presents a unified approach for inverse and direct dynamics of\nconstrained multibody systems that can serve as a basis for analysis,\nsimulation, and control. The main advantage of the formulation of the dynamic\nis that it does not require the constraint equations to be linearly\nindependent. Thus, a simulation may proceed even in the presence of redundant\nconstraints or singular configurations and a controller does not need to change\nits structure whenever the mechanical system changes its topology or number of\ndegrees of freedom. A motion control scheme is proposed based on a projected\ninverse-dynamics scheme which proves to be stable and minimizes the weighted\nEuclidean norm of the actuation force. The projection-based control scheme is\nfurther developed for constrained systems, e.g. parallel manipulators, which\nhave some joints with no actuators (passive joints). This is complemented by\nthe development of constraint force control. A condition on the inertia matrix\nresulting in a decoupled mechanical system is analytically derived which\nsimplifies the implementation of the force control.",
    "descriptor": "",
    "authors": [
      "Farhad Aghili"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17053"
  },
  {
    "id": "arXiv:2210.17057",
    "title": "Fault diagnosis for open-circuit faults in NPC inverter based on  knowledge-driven and data-driven approaches",
    "abstract": "In this study, the open-circuit faults diagnosis and location issue of the\nneutral-point-clamped (NPC) inverters are analysed. A novel fault diagnosis\napproach based on knowledge driven and data driven was presented for the\nopen-circuit faults in insulated-gate bipolar transistors (IGBTs) of NPC\ninverter, and Concordia transform (knowledge driven) and random forests (RFs)\ntechnique (data driven) are employed to improve the robustness performance of\nthe fault diagnosis classifier. First, the fault feature data of AC in either\nnormal state or open-circuit faults states of NPC inverter are analysed and\nextracted. Second, the Concordia transform is used to process the fault\nsamples, and it has been verified that the slopes of current trajectories are\nnot affected by different loads in this study, which can help the proposed\nmethod to reduce overdependence on fault data. Moreover, then the transformed\nfault samples are adopted to train the RFs fault diagnosis classifier, and the\nfault diagnosis results show that the classification accuracy and robustness\nperformance of the fault diagnosis classifier are improved. Finally, the\ndiagnosis results of online fault diagnosis experiments show that the proposed\nclassifier can locate the open-circuit fault of IGBTs in NPC inverter under the\nconditions of different loads.",
    "descriptor": "\nComments: IET Power Electronics\n",
    "authors": [
      "Lei Kou",
      "Chuang Liu",
      "Guo-wei Cai",
      "Jia-ning Zhou",
      "Quan-de Yuan",
      "Si-miao Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17057"
  },
  {
    "id": "arXiv:2210.17060",
    "title": "MambaNet: A Hybrid Neural Network for Predicting the NBA Playoffs",
    "abstract": "In this paper, we present Mambanet: a hybrid neural network for predicting\nthe outcomes of Basketball games. Contrary to other studies, which focus\nprimarily on season games, this study investigates playoff games. MambaNet is a\nhybrid neural network architecture that processes a time series of teams' and\nplayers' game statistics and generates the probability of a team winning or\nlosing an NBA playoff match. In our approach, we utilize Feature Imitating\nNetworks to provide latent signal-processing feature representations of game\nstatistics to further process with convolutional, recurrent, and dense neural\nlayers. Three experiments using six different datasets are conducted to\nevaluate the performance and generalizability of our architecture against a\nwide range of previous studies. Our final method successfully predicted the AUC\nfrom 0.72 to 0.82, beating the best-performing baseline models by a\nconsiderable margin.",
    "descriptor": "",
    "authors": [
      "Reza Khanmohammadi",
      "Sari Saba-Sadiya",
      "Sina Esfandiarpour",
      "Tuka Alhanai",
      "Mohammad M. Ghassemi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17060"
  },
  {
    "id": "arXiv:2210.17067",
    "title": "Unified Optimal Transport Framework for Universal Domain Adaptation",
    "abstract": "Universal Domain Adaptation (UniDA) aims to transfer knowledge from a source\ndomain to a target domain without any constraints on label sets. Since both\ndomains may hold private classes, identifying target common samples for domain\nalignment is an essential issue in UniDA. Most existing methods require\nmanually specified or hand-tuned threshold values to detect common samples thus\nthey are hard to extend to more realistic UniDA because of the diverse ratios\nof common classes. Moreover, they cannot recognize different categories among\ntarget-private samples as these private samples are treated as a whole. In this\npaper, we propose to use Optimal Transport (OT) to handle these issues under a\nunified framework, namely UniOT. First, an OT-based partial alignment with\nadaptive filling is designed to detect common classes without any predefined\nthreshold values for realistic UniDA. It can automatically discover the\nintrinsic difference between common and private classes based on the\nstatistical information of the assignment matrix obtained from OT. Second, we\npropose an OT-based target representation learning that encourages both global\ndiscrimination and local consistency of samples to avoid the over-reliance on\nthe source. Notably, UniOT is the first method with the capability to\nautomatically discover and recognize private categories in the target domain\nfor UniDA. Accordingly, we introduce a new metric H^3-score to evaluate the\nperformance in terms of both accuracy of common samples and clustering\nperformance of private ones. Extensive experiments clearly demonstrate the\nadvantages of UniOT over a wide range of state-of-the-art methods in UniDA.",
    "descriptor": "\nComments: Accepted by NeurIPS2022\n",
    "authors": [
      "Wanxing Chang",
      "Ye Shi",
      "Hoang Duong Tuan",
      "Jingya Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17067"
  },
  {
    "id": "arXiv:2210.17070",
    "title": "Private optimization in the interpolation regime: faster rates and  hardness results",
    "abstract": "In non-private stochastic convex optimization, stochastic gradient methods\nconverge much faster on interpolation problems -- problems where there exists a\nsolution that simultaneously minimizes all of the sample losses -- than on\nnon-interpolating ones; we show that generally similar improvements are\nimpossible in the private setting. However, when the functions exhibit\nquadratic growth around the optimum, we show (near) exponential improvements in\nthe private sample complexity. In particular, we propose an adaptive algorithm\nthat improves the sample complexity to achieve expected error $\\alpha$ from\n$\\frac{d}{\\varepsilon \\sqrt{\\alpha}}$ to $\\frac{1}{\\alpha^\\rho} +\n\\frac{d}{\\varepsilon} \\log\\left(\\frac{1}{\\alpha}\\right)$ for any fixed $\\rho\n>0$, while retaining the standard minimax-optimal sample complexity for\nnon-interpolation problems. We prove a lower bound that shows the\ndimension-dependent term is tight. Furthermore, we provide a superefficiency\nresult which demonstrates the necessity of the polynomial term for adaptive\nalgorithms: any algorithm that has a polylogarithmic sample complexity for\ninterpolation problems cannot achieve the minimax-optimal rates for the family\nof non-interpolation problems.",
    "descriptor": "\nComments: published at ICML 2022; 25 pages\n",
    "authors": [
      "Hilal Asi",
      "Karan Chadha",
      "Gary Cheng",
      "John Duchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17070"
  },
  {
    "id": "arXiv:2210.17071",
    "title": "Computing Rule-Based Explanations by Leveraging Counterfactuals",
    "abstract": "Sophisticated machine models are increasingly used for high-stakes decisions\nin everyday life. There is an urgent need to develop effective explanation\ntechniques for such automated decisions. Rule-Based Explanations have been\nproposed for high-stake decisions like loan applications, because they increase\nthe users' trust in the decision. However, rule-based explanations are very\ninefficient to compute, and existing systems sacrifice their quality in order\nto achieve reasonable performance. We propose a novel approach to compute\nrule-based explanations, by using a different type of explanation,\nCounterfactual Explanations, for which several efficient systems have already\nbeen developed. We prove a Duality Theorem, showing that rule-based and\ncounterfactual-based explanations are dual to each other, then use this\nobservation to develop an efficient algorithm for computing rule-based\nexplanations, which uses the counterfactual-based explanation as an oracle. We\nconduct extensive experiments showing that our system computes rule-based\nexplanations of higher quality, and with the same or better performance, than\ntwo previous systems, MinSetCover and Anchor.",
    "descriptor": "",
    "authors": [
      "Zixuan Geng",
      "Maximilian Schleich",
      "Dan Suciu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.17071"
  },
  {
    "id": "arXiv:2210.17073",
    "title": "Communication-Efficient Local SGD with Age-Based Worker Selection",
    "abstract": "A major bottleneck of distributed learning under parameter-server (PS)\nframework is communication cost due to frequent bidirectional transmissions\nbetween the PS and workers. To address this issue, local stochastic gradient\ndescent (SGD) and worker selection have been exploited by reducing the\ncommunication frequency and the number of participating workers at each round,\nrespectively. However, partial participation can be detrimental to convergence\nrate, especially for heterogeneous local datasets. In this paper, to improve\ncommunication efficiency and speed up the training process, we develop a novel\nworker selection strategy named AgeSel. The key enabler of AgeSel is\nutilization of the ages of workers to balance their participation frequencies.\nThe convergence of local SGD with the proposed age-based partial worker\nparticipation is rigorously established. Simulation results demonstrate that\nthe proposed AgeSel strategy can significantly reduce the communication cost,\nas well as the number of training rounds needed to achieve a targeted accuracy.\nThe influence of the algorithm hyper-parameter is also explored to manifest the\nbenefit of age-based worker selection.",
    "descriptor": "",
    "authors": [
      "Feng Zhu",
      "Jingjing Zhang",
      "Xin Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17073"
  },
  {
    "id": "arXiv:2210.17079",
    "title": "FusionFormer: Fusing Operations in Transformer for Efficient Streaming  Speech Recognition",
    "abstract": "The recently proposed Conformer architecture which combines convolution with\nattention to capture both local and global dependencies has become the\n\\textit{de facto} backbone model for Automatic Speech Recognition~(ASR).\nInherited from the Natural Language Processing (NLP) tasks, the architecture\ntakes Layer Normalization~(LN) as a default normalization technique. However,\nthrough a series of systematic studies, we find that LN might take 10\\% of the\ninference time despite that it only contributes to 0.1\\% of the FLOPs. This\nmotivates us to replace LN with other normalization techniques, e.g., Batch\nNormalization~(BN), to speed up inference with the help of operator fusion\nmethods and the avoidance of calculating the mean and variance statistics\nduring inference. After examining several plain attempts which directly remove\nall LN layers or replace them with BN in the same place, we find that the\ndivergence issue is mainly caused by the unstable layer output. We therefore\npropose to append a BN layer to each linear or convolution layer where\nstabilized training results are observed. We also propose to simplify the\nactivations in Conformer, such as Swish and GLU, by replacing them with ReLU.\nAll these exchanged modules can be fused into the weights of the adjacent\nlinear/convolution layers and hence have zero inference cost. Therefore, we\nname it FusionFormer. Our experiments indicate that FusionFormer is as\neffective as the LN-based Conformer and is about 10\\% faster.",
    "descriptor": "\nComments: 8 pages, plus 3 appendix\n",
    "authors": [
      "Xingchen Song",
      "Di Wu",
      "Binbin Zhang",
      "Zhiyong Wu",
      "Wenpeng Li",
      "Dongfang Li",
      "Pengshen Zhang",
      "Zhendong Peng",
      "Fuping Pan",
      "Changbao Zhu",
      "Zhongqin Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17079"
  },
  {
    "id": "arXiv:2210.17084",
    "title": "Secure Outage Analysis of RIS-Assisted Communications with Discrete  Phase Control",
    "abstract": "This correspondence investigates a reconfigurable intelligent surface\n(RIS)-assisted wireless communication system with security threats. The RIS is\ndeployed to enhance the secrecy outage probability (SOP) of the data sent to a\nlegitimate user. By deriving the distributions of the received\nsignal-to-noise-ratios (SNRs) at the legitimate user and the eavesdropper, we\nformulate, in a closed-form expression, a tight bound for the SOP under the\nconstraint of discrete phase control at the RIS. The SOP is characterized as a\nfunction of the number of antenna elements, $N$, and the number of discrete\nphase choices, $2^b$. It is revealed that the performance loss in terms of SOP\ndue to the discrete phase control is ignorable for large $N$ when $b\\!\\geq\\!3$.\nIn addition, we explicitly quantify this SOP loss when binary phase shifts with\n$b\\!=\\!1$ is utilized. It is identified that increasing the RIS antenna\nelements by $1.6$ times can achieve the same SOP with binary phase shifts as\nthat by the RIS with ideally continuous phase shifts. Numerical simulations are\nconducted to verify the accuracy of these theoretical observations.",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Wei Shi",
      "Jindan Xu",
      "Wei Xu",
      "Marco Di Renzo",
      "Chunming Zhao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17084"
  },
  {
    "id": "arXiv:2210.17086",
    "title": "EEMARQ: Efficient Lock-Free Range Queries with Memory Reclamation",
    "abstract": "Multi-Version Concurrency Control (MVCC) is a common mechanism for achieving\nlinearizable range queries in database systems and concurrent data-structures.\nThe core idea is to keep previous versions of nodes to serve range queries,\nwhile still providing atomic reads and updates. Existing concurrent\ndata-structure implementations, that support linearizable range queries, are\neither slow, use locks, or rely on blocking reclamation schemes. We present\nEEMARQ, the first scheme that uses MVCC with lock-free memory reclamation to\nobtain a fully lock-free data-structure supporting linearizable inserts,\ndeletes, contains, and range queries. Evaluation shows that EEMARQ outperforms\nexisting solutions across most workloads, with lower space overhead and while\nproviding full lock freedom.",
    "descriptor": "",
    "authors": [
      "Gali Sheffi",
      "Pedro Ramalhete",
      "Erez Petrank"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17086"
  },
  {
    "id": "arXiv:2210.17087",
    "title": "DanZero: Mastering GuanDan Game with Reinforcement Learning",
    "abstract": "Card game AI has always been a hot topic in the research of artificial\nintelligence. In recent years, complex card games such as Mahjong, DouDizhu and\nTexas Hold'em have been solved and the corresponding AI programs have reached\nthe level of human experts. In this paper, we are devoted to developing an AI\nprogram for a more complex card game, GuanDan, whose rules are similar to\nDouDizhu but much more complicated. To be specific, the characteristics of\nlarge state and action space, long length of one episode and the unsure number\nof players in the GuanDan pose great challenges for the development of the AI\nprogram. To address these issues, we propose the first AI program DanZero for\nGuanDan using reinforcement learning technique. Specifically, we utilize a\ndistributed framework to train our AI system. In the actor processes, we\ncarefully design the state features and agents generate samples by self-play.\nIn the learner process, the model is updated by Deep Monte-Carlo Method. After\ntraining for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We\ncompare it with 8 baseline AI programs which are based on heuristic rules and\nthe results reveal the outstanding performance of DanZero. We also test DanZero\nwith human players and demonstrate its human-level performance.",
    "descriptor": "",
    "authors": [
      "Yudong Lu",
      "Jian Zhao",
      "Youpeng Zhao",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17087"
  },
  {
    "id": "arXiv:2210.17092",
    "title": "Confidence-Nets: A Step Towards better Prediction Intervals for  regression Neural Networks on small datasets",
    "abstract": "The recent decade has seen an enormous rise in the popularity of deep\nlearning and neural networks. These algorithms have broken many previous\nrecords and achieved remarkable results. Their outstanding performance has\nsignificantly sped up the progress of AI, and so far various milestones have\nbeen achieved earlier than expected. However, in the case of relatively small\ndatasets, the performance of Deep Neural Networks (DNN) may suffer from reduced\naccuracy compared to other Machine Learning models. Furthermore, it is\ndifficult to construct prediction intervals or evaluate the uncertainty of\npredictions when dealing with regression tasks. In this paper, we propose an\nensemble method that attempts to estimate the uncertainty of predictions,\nincrease their accuracy and provide an interval for the expected variation.\nCompared with traditional DNNs that only provide a prediction, our proposed\nmethod can output a prediction interval by combining DNNs, extreme gradient\nboosting (XGBoost) and dissimilarity computation techniques. Albeit the simple\ndesign, this approach significantly increases accuracy on small datasets and\ndoes not introduce much complexity to the architecture of the neural network.\nThe proposed method is tested on various datasets, and a significant\nimprovement in the performance of the neural network model is seen. The model's\nprediction interval can include the ground truth value at an average rate of\n71% and 78% across training sizes of 90% and 55%, respectively. Finally, we\nhighlight other aspects and applications of the approach in experimental error\nestimation, and the application of transfer learning.",
    "descriptor": "",
    "authors": [
      "Mohamedelmujtaba Altayeb",
      "Abdelrahman M. Elamin",
      "Hozaifa Ahmed",
      "Eithar Elfatih Elfadil Ibrahim",
      "Omer Haydar",
      "Saba Abdulaziz",
      "Najlaa H. M. Mohamed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17092"
  },
  {
    "id": "arXiv:2210.17095",
    "title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with  Entailment Trees",
    "abstract": "Interpreting the reasoning process from questions to answers poses a\nchallenge in approaching explainable QA. A recently proposed structured\nreasoning format, entailment tree, manages to offer explicit logical deductions\nwith entailment steps in a tree structure. To generate entailment trees, prior\nsingle pass sequence-to-sequence models lack visible internal decision\nprobability, while stepwise approaches are supervised with extracted single\nstep data and cannot model the tree as a whole. In this work, we propose RLET,\na Reinforcement Learning based Entailment Tree generation framework, which is\ntrained utilising the cumulative signals across the whole tree. RLET\niteratively performs single step reasoning with sentence selection and\ndeduction generation modules, from which the training signal is accumulated\nacross the tree with elaborately designed aligned reward function that is\nconsistent with the evaluation. To the best of our knowledge, we are the first\nto introduce RL into the entailment tree generation task. Experiments on three\nsettings of the EntailmentBank dataset demonstrate the strength of using RL\nframework.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Tengxiao Liu",
      "Qipeng Guo",
      "Xiangkun Hu",
      "Yue Zhang",
      "Xipeng Qiu",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17095"
  },
  {
    "id": "arXiv:2210.17098",
    "title": "Structured State Space Decoder for Speech Recognition and Synthesis",
    "abstract": "Automatic speech recognition (ASR) systems developed in recent years have\nshown promising results with self-attention models (e.g., Transformer and\nConformer), which are replacing conventional recurrent neural networks.\nMeanwhile, a structured state space model (S4) has been recently proposed,\nproducing promising results for various long-sequence modeling tasks, including\nraw speech classification. The S4 model can be trained in parallel, same as the\nTransformer model. In this study, we applied S4 as a decoder for ASR and\ntext-to-speech (TTS) tasks by comparing it with the Transformer decoder. For\nthe ASR task, our experimental results demonstrate that the proposed model\nachieves a competitive word error rate (WER) of 1.88%/4.25% on LibriSpeech\ntest-clean/test-other set and a character error rate (CER) of 3.80%/2.63%/2.98%\non the CSJ eval1/eval2/eval3 set. Furthermore, the proposed model is more\nrobust than the standard Transformer model, particularly for long-form speech\non both the datasets. For the TTS task, the proposed method outperforms the\nTransformer baseline.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Koichi Miyazaki",
      "Masato Murata",
      "Tomoki Koriyama"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17098"
  },
  {
    "id": "arXiv:2210.17101",
    "title": "Unrolled Graph Learning for Multi-Agent Collaboration",
    "abstract": "Multi-agent learning has gained increasing attention to tackle distributed\nmachine learning scenarios under constrictions of data exchanging. However,\nexisting multi-agent learning models usually consider data fusion under fixed\nand compulsory collaborative relations among agents, which is not as flexible\nand autonomous as human collaboration. To fill this gap, we propose a\ndistributed multi-agent learning model inspired by human collaboration, in\nwhich the agents can autonomously detect suitable collaborators and refer to\ncollaborators' model for better performance. To implement such adaptive\ncollaboration, we use a collaboration graph to indicate the pairwise\ncollaborative relation. The collaboration graph can be obtained by graph\nlearning techniques based on model similarity between different agents. Since\nmodel similarity can not be formulated by a fixed graphical optimization, we\ndesign a graph learning network by unrolling, which can learn underlying\nsimilar features among potential collaborators. By testing on both regression\nand classification tasks, we validate that our proposed collaboration model can\nfigure out accurate collaborative relationship and greatly improve agents'\nlearning performance.",
    "descriptor": "",
    "authors": [
      "Enpei Zhang",
      "Shuo Tang",
      "Xiaowen Dong",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.17101"
  },
  {
    "id": "arXiv:2210.17106",
    "title": "Intelligent Painter: Picture Composition With Resampling Diffusion Model",
    "abstract": "Have you ever thought that you can be an intelligent painter? This means that\nyou can paint a picture with a few expected objects in mind, or with a\ndesirable scene. This is different from normal inpainting approaches for which\nthe location of specific objects cannot be determined. In this paper, we\npresent an intelligent painter that generate a person's imaginary scene in one\ngo, given explicit hints. We propose a resampling strategy for Denoising\nDiffusion Probabilistic Model (DDPM) to intelligently compose harmonized\nscenery images by injecting explicit landmark inputs at specific locations. By\nexploiting the diffusion property, we resample efficiently to produce realistic\nimages. Experimental results show that our resampling method favors the\nsemantic meaning of the generated output efficiently and generate less blurry\noutput. Quantitative analysis of image quality assessment shows that our method\nproduces higher perceptual quality images compared with the state-of-the-art\nmethods.",
    "descriptor": "",
    "authors": [
      "Wing-Fung Ku",
      "Wan-Chi Siu",
      "Xi Cheng",
      "H. Anthony Chan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17106"
  },
  {
    "id": "arXiv:2210.17107",
    "title": "An adaptive damped Newton method for strongly monotone and Lipschitz  continuous operator equations",
    "abstract": "We will consider the damped Newton method for strongly monotone and Lipschitz\ncontinuous operator equations in a variational setting. We will provide a very\naccessible justification why the undamped Newton method performs better than\nits damped counterparts in a vicinity of a solution. Moreover, in the given\nsetting, an adaptive step-size strategy will be presented, which guarantees the\nglobal convergence and favours an undamped update if admissible.",
    "descriptor": "",
    "authors": [
      "Pascal Heid"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17107"
  },
  {
    "id": "arXiv:2210.17108",
    "title": "Do Charge Prediction Models Learn Legal Theory?",
    "abstract": "The charge prediction task aims to predict the charge for a case given its\nfact description. Recent models have already achieved impressive accuracy in\nthis task, however, little is understood about the mechanisms they use to\nperform the judgment.For practical applications, a charge prediction model\nshould conform to the certain legal theory in civil law countries, as under the\nframework of civil law, all cases are judged according to certain local legal\ntheories. In China, for example, nearly all criminal judges make decisions\nbased on the Four Elements Theory (FET).In this paper, we argue that\ntrustworthy charge prediction models should take legal theories into\nconsideration, and standing on prior studies in model interpretation, we\npropose three principles for trustworthy models should follow in this task,\nwhich are sensitive, selective, and presumption of innocence.We further design\na new framework to evaluate whether existing charge prediction models learn\nlegal theories. Our findings indicate that, while existing charge prediction\nmodels meet the selective principle on a benchmark dataset, most of them are\nstill not sensitive enough and do not satisfy the presumption of innocence. Our\ncode and dataset are released at https://github.com/ZhenweiAn/EXP_LJP.",
    "descriptor": "\nComments: findings of emnlp2022\n",
    "authors": [
      "Zhenwei An",
      "Quzhe Huang",
      "Cong Jiang",
      "Yansong Feng",
      "Dongyan Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17108"
  },
  {
    "id": "arXiv:2210.17111",
    "title": "SEVGGNet-LSTM: a fused deep learning model for ECG classification",
    "abstract": "This paper presents a fused deep learning algorithm for ECG classification.\nIt takes advantages of the combined convolutional and recurrent neural network\nfor ECG classification, and the weight allocation capability of attention\nmechanism. The input ECG signals are firstly segmented and normalized, and then\nfed into the combined VGG and LSTM network for feature extraction and\nclassification. An attention mechanism (SE block) is embedded into the core\nnetwork for increasing the weight of important features. Two databases from\ndifferent sources and devices are employed for performance validation, and the\nresults well demonstrate the effectiveness and robustness of the proposed\nalgorithm.",
    "descriptor": "",
    "authors": [
      "Tongyue He",
      "Yiming Chen",
      "Junxin Chen",
      "Wei Wang",
      "Yicong Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17111"
  },
  {
    "id": "arXiv:2210.17114",
    "title": "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM",
    "abstract": "Limited computational budgets often prevent transformers from being used in\nproduction and from having their high accuracy utilized. A knowledge\ndistillation approach addresses the computational efficiency by self-distilling\nBERT into a smaller transformer representation having fewer layers and smaller\ninternal embedding. However, the performance of these models drops as we reduce\nthe number of layers, notably in advanced NLP tasks such as span question\nanswering. In addition, a separate model must be trained for each inference\nscenario with its distinct computational budget. Dynamic-TinyBERT tackles both\nlimitations by partially implementing the Length Adaptive Transformer (LAT)\ntechnique onto TinyBERT, achieving x3 speedup over BERT-base with minimal\naccuracy loss. In this work, we expand the Dynamic-TinyBERT approach to\ngenerate a much more highly efficient model. We use MiniLM distillation jointly\nwith the LAT method, and we further enhance the efficiency by applying low-bit\nquantization. Our quantized length-adaptive MiniLM model (QuaLA-MiniLM) is\ntrained only once, dynamically fits any inference scenario, and achieves an\naccuracy-efficiency trade-off superior to any other efficient approaches per\nany computational budget on the SQuAD1.1 dataset (up to x8.8 speedup with <1%\naccuracy loss). The code to reproduce this work will be publicly released on\nGithub soon.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2111.09645\n",
    "authors": [
      "Shira Guskin",
      "Moshe Wasserblat",
      "Chang Wang",
      "Haihao Shen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17114"
  },
  {
    "id": "arXiv:2210.17115",
    "title": "ViT-LSLA: Vision Transformer with Light Self-Limited-Attention",
    "abstract": "Transformers have demonstrated a competitive performance across a wide range\nof vision tasks, while it is very expensive to compute the global\nself-attention. Many methods limit the range of attention within a local window\nto reduce computation complexity. However, their approaches cannot save the\nnumber of parameters; meanwhile, the self-attention and inner position bias\n(inside the softmax function) cause each query to focus on similar and close\npatches. Consequently, this paper presents a light self-limited-attention\n(LSLA) consisting of a light self-attention mechanism (LSA) to save the\ncomputation cost and the number of parameters, and a self-limited-attention\nmechanism (SLA) to improve the performance. Firstly, the LSA replaces the K\n(Key) and V (Value) of self-attention with the X(origin input). Applying it in\nvision Transformers which have encoder architecture and self-attention\nmechanism, can simplify the computation. Secondly, the SLA has a positional\ninformation module and a limited-attention module. The former contains a\ndynamic scale and an inner position bias to adjust the distribution of the\nself-attention scores and enhance the positional information. The latter uses\nan outer position bias after the softmax function to limit some large values of\nattention weights. Finally, a hierarchical Vision Transformer with Light\nself-Limited-attention (ViT-LSLA) is presented. The experiments show that\nViT-LSLA achieves 71.6% top-1 accuracy on IP102 (2.4% absolute improvement of\nSwin-T); 87.2% top-1 accuracy on Mini-ImageNet (3.7% absolute improvement of\nSwin-T). Furthermore, it greatly reduces FLOPs (3.5GFLOPs vs. 4.5GFLOPs of\nSwin-T) and parameters (18.9M vs. 27.6M of Swin-T).",
    "descriptor": "",
    "authors": [
      "Zhenzhe Hechen",
      "Wei Huang",
      "Yixin Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17115"
  },
  {
    "id": "arXiv:2210.17117",
    "title": "Size-effects of metamaterial beams subjected to pure bending: on  boundary conditions and parameter identification in the relaxed micromorphic  model",
    "abstract": "We devote this paper to model the size-effects of metamaterial beams under\nbending with the aid of the relaxed micromorphic continuum. We analyze first\nthe size-dependent bending stiffness of heterogeneous fully discretized\nmetamaterial beams subjected to pure bending loads. Two equivalent loading\nschemes are introduced which lead to a constant moment along the beam length\nwith no shear force. The relaxed micromorphic model is employed then to\nretrieve the size-effects. We present a procedure for the determination of the\nmaterial parameters of the relaxed micromorphic model based on the fact that\nthe model operates between two well-defined scales. These scales are given by\nlinear elasticity with micro and macro elasticity tensors which bound the\nrelaxed micromorphic continuum from above and below, respectively. The micro\nelasticity tensor is specified as the maximum possible stiffness that is\nexhibited by the assumed metamaterial while the macro elasticity tensor is\ngiven by standard periodic first-order homogenization. For the identification\nof the micro elasticity tensor, two different approaches are shown which rely\non affine and non-affine Dirichlet boundary conditions of candidate unit cell\nvariants with the possible stiffest response. The consistent coupling condition\nis shown to allow the model to act on the whole intended range between macro\nand micro elasticity tensors for both loading cases. Finally, we fit the\nrelaxed micromorphic model against the fully resolved metamaterial solution by\ncontrolling the curvature magnitude after linking it with the specimen's size.",
    "descriptor": "",
    "authors": [
      "Mohammad Sarhil",
      "Lisa Scheunemann",
      "J\u00f6rg Schr\u00f6der",
      "Patrizio Neff"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17117"
  },
  {
    "id": "arXiv:2210.17119",
    "title": "VocabulARy replicated: comparing teenagers to young adults",
    "abstract": "A critical component of user studies is gaining access to a representative\nsample of the population researches intend to investigate. Nevertheless, the\nvast majority of human-computer interaction (HCI)studies, including augmented\nreality (AR) studies, rely on convenience sampling. The outcomes of these\nstudies are often based on results obtained from university students aged\nbetween 19 and 26 years. In order to investigate how the results from one of\nour studies are affected by convenience sampling, we replicated the\nAR-supported language learning study called VocabulARy with 24 teenagers, aged\nbetween 14 and 19 years. The results verified most of the outcomes from the\noriginal study. In addition, it also revealed that teenagers found learning\nsignificantly less mentally demanding compared to young adults, and completed\nthe study in a significantly shorter time. All this at no cost to learning\noutcomes.",
    "descriptor": "",
    "authors": [
      "Maheshya Weerasinghe",
      "Verena Biener",
      "Jens Grubert",
      "Jordan Aiko Deja",
      "Nuwan T. Attygalle",
      "Karolina Trajkovska",
      "Matja\u017e Kljun",
      "Klen \u010copi\u010d Pucihar"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.17119"
  },
  {
    "id": "arXiv:2210.17122",
    "title": "Mining Word Boundaries in Speech as Naturally Annotated Word  Segmentation Data",
    "abstract": "Chinese word segmentation (CWS) models have achieved very high performance\nwhen the training data is sufficient and in-domain. However, the performance\ndrops drastically when shifting to cross-domain and low-resource scenarios due\nto data sparseness issues. Considering that constructing large-scale manually\nannotated data is time-consuming and labor-intensive, in this work, we for the\nfirst time propose to mine word boundary information from pauses in speech to\nefficiently obtain large-scale CWS naturally annotated data. We present a\nsimple yet effective complete-then-train method to utilize these natural\nannotations from speech for CWS model training. Extensive experiments\ndemonstrate that the CWS performance in cross-domain and low-resource scenarios\ncan be significantly improved by leveraging our naturally annotated data\nextracted from speech.",
    "descriptor": "\nComments: Submitted to ICASSP2023\n",
    "authors": [
      "Lei Zhang",
      "Shilin Zhou",
      "Chen Gong",
      "Zhenghua Li",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17122"
  },
  {
    "id": "arXiv:2210.17127",
    "title": "Improving Temporal Generalization of Pre-trained Language Models with  Lexical Semantic Change",
    "abstract": "Recent research has revealed that neural language models at scale suffer from\npoor temporal generalization capability, i.e., the language model pre-trained\non static data from past years performs worse over time on emerging data.\nExisting methods mainly perform continual training to mitigate such a\nmisalignment. While effective to some extent but is far from being addressed on\nboth the language modeling and downstream tasks. In this paper, we empirically\nobserve that temporal generalization is closely affiliated with lexical\nsemantic change, which is one of the essential phenomena of natural languages.\nBased on this observation, we propose a simple yet effective lexical-level\nmasking strategy to post-train a converged language model. Experiments on two\npre-trained language models, two different classification tasks, and four\nbenchmark datasets demonstrate the effectiveness of our proposed method over\nexisting temporal adaptation methods, i.e., continual training with new data.\nOur code is available at \\url{https://github.com/zhaochen0110/LMLM}.",
    "descriptor": "\nComments: EMNLP 2022, Long paper\n",
    "authors": [
      "Zhaochen Su",
      "Zecheng Tang",
      "Xinyan Guan",
      "Juntao Li",
      "Lijun Wu",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17127"
  },
  {
    "id": "arXiv:2210.17128",
    "title": "Diffusion models for missing value imputation in tabular data",
    "abstract": "Missing value imputation in machine learning is the task of estimating the\nmissing values in the dataset accurately using available information. In this\ntask, several deep generative modeling methods have been proposed and\ndemonstrated their usefulness, e.g., generative adversarial imputation\nnetworks. Recently, diffusion models have gained popularity because of their\neffectiveness in the generative modeling task in images, texts, audio, etc. To\nour knowledge, less attention has been paid to the investigation of the\neffectiveness of diffusion models for missing value imputation in tabular data.\nBased on recent development of diffusion models for time-series data\nimputation, we propose a diffusion model approach called \"Conditional\nScore-based Diffusion Models for Tabular data\" (CSDI_T). To effectively handle\ncategorical variables and numerical variables simultaneously, we investigate\nthree techniques: one-hot encoding, analog bits encoding, and feature\ntokenization. Experimental results on benchmark datasets demonstrated the\neffectiveness of CSDI_T compared with well-known existing methods, and also\nemphasized the importance of the categorical embedding techniques.",
    "descriptor": "\nComments: Accepted to Table Representation Learning Workshop at NeurIPS 2022\n",
    "authors": [
      "Shuhan Zheng",
      "Nontawat Charoenphakdee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17128"
  },
  {
    "id": "arXiv:2210.17130",
    "title": "BOREx: Bayesian-Optimization--Based Refinement of Saliency Map for  Image- and Video-Classification Models",
    "abstract": "Explaining a classification result produced by an image- and\nvideo-classification model is one of the important but challenging issues in\ncomputer vision. Many methods have been proposed for producing heat-map--based\nexplanations for this purpose, including ones based on the white-box approach\nthat uses the internal information of a model (e.g., LRP, Grad-CAM, and\nGrad-CAM++) and ones based on the black-box approach that does not use any\ninternal information (e.g., LIME, SHAP, and RISE). We propose a new black-box\nmethod BOREx (Bayesian Optimization for Refinement of visual model Explanation)\nto refine a heat map produced by any method. Our observation is that a\nheat-map--based explanation can be seen as a prior for an explanation method\nbased on Bayesian optimization. Based on this observation, BOREx conducts\nGaussian process regression (GPR) to estimate the saliency of each pixel in a\ngiven image starting from the one produced by another explanation method. Our\nexperiments statistically demonstrate that the refinement by BOREx improves\nlow-quality heat maps for image- and video-classification results.",
    "descriptor": "\nComments: 32 pages. To appear in ACCV 2022\n",
    "authors": [
      "Atsushi Kikuchi",
      "Kotaro Uchida",
      "Masaki Waga",
      "Kohei Suenaga"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17130"
  },
  {
    "id": "arXiv:2210.17138",
    "title": "Reinforcement Learning for Solving Robotic Reaching Tasks in the  Neurorobotics Platform",
    "abstract": "In recent years, reinforcement learning (RL) has shown great potential for\nsolving tasks in well-defined environments like games or robotics. This paper\naims to solve the robotic reaching task in a simulation run on the\nNeurorobotics Platform (NRP). The target position is initialized randomly and\nthe robot has 6 degrees of freedom. We compare the performance of various\nstate-of-the-art model-free algorithms. At first, the agent is trained on\nground truth data from the simulation to reach the target position in only one\ncontinuous movement. Later the complexity of the task is increased by using\nimage data as input from the simulation environment. Experimental results show\nthat training efficiency and results can be improved with appropriate dynamic\ntraining schedule function for curriculum learning.",
    "descriptor": "\nComments: Poster presentation at 6th HBP Student Conference 2022, 10 pages, 7 figures\n",
    "authors": [
      "M\u00e1rton Szep",
      "Leander Lauenburg",
      "Kevin Farkas",
      "Xiyan Su",
      "Chuanlong Zang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.17138"
  },
  {
    "id": "arXiv:2210.17139",
    "title": "Nested Sequents for Intuitionistic Grammar Logics via Structural  Refinement",
    "abstract": "Intuitionistic grammar logics fuse constructive and multi-modal reasoning\nwhile permitting the use of converse modalities, serving as a generalization of\nstandard intuitionistic modal logics. In this paper, we provide definitions of\nthese logics as well as establish a suitable proof theory thereof. In\nparticular, we show how to apply the structural refinement methodology to\nextract cut-free nested sequent calculi for intuitionistic grammar logics from\ntheir semantics. This method proceeds by first transforming the semantics of\nthese logics into sound and complete labeled sequent systems, which we prove\nhave favorable proof-theoretic properties such as syntactic cut-elimination. We\nthen transform these labeled systems into nested sequent systems via the\nintroduction of propagation rules and the elimination of structural rules. Our\nderived proof systems are then put to use, whereby we prove the conservativity\nof intuitionistic grammar logics over their modal counterparts, establish the\ngeneral undecidability of these logics, and recognize a decidable subclass,\nreferred to as \"simple\" intuitionistic grammar logics.",
    "descriptor": "\nComments: This paper is currently under review. arXiv admin note: text overlap with arXiv:2107.01998\n",
    "authors": [
      "Tim S. Lyon"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Discrete Mathematics (cs.DM)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.17139"
  },
  {
    "id": "arXiv:2210.17140",
    "title": "Scoring Black-Box Models for Adversarial Robustness",
    "abstract": "Deep neural networks are susceptible to adversarial inputs and various\nmethods have been proposed to defend these models against adversarial attacks\nunder different perturbation models. The robustness of models to adversarial\nattacks has been analyzed by first constructing adversarial inputs for the\nmodel, and then testing the model performance on the constructed adversarial\ninputs. Most of these attacks require the model to be white-box, need access to\ndata labels, and finding adversarial inputs can be computationally expensive.\nWe propose a simple scoring method for black-box models which indicates their\nrobustness to adversarial input. We show that adversarially more robust models\nhave a smaller $l_1$-norm of LIME weights and sharper explanations.",
    "descriptor": "",
    "authors": [
      "Jian Vora",
      "Pranay Reddy Samala"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17140"
  },
  {
    "id": "arXiv:2210.17141",
    "title": "Studying inductive biases in image classification task",
    "abstract": "Recently, self-attention (SA) structures became popular in computer vision\nfields. They have locally independent filters and can use large kernels, which\ncontradicts the previously popular convolutional neural networks (CNNs). CNNs\nsuccess was attributed to the hard-coded inductive biases of locality and\nspatial invariance. However, recent studies have shown that inductive biases in\nCNNs are too restrictive. On the other hand, the relative position encodings,\nsimilar to depthwise (DW) convolution, are necessary for the local SA networks,\nwhich indicates that the SA structures are not entirely spatially variant.\nHence, we would like to determine which part of inductive biases contributes to\nthe success of the local SA structures. To do so, we introduced context-aware\ndecomposed attention (CADA), which decomposes attention maps into multiple\ntrainable base kernels and accumulates them using context-aware (CA)\nparameters. This way, we could identify the link between the CNNs and SA\nnetworks. We conducted ablation studies using the ResNet50 applied to the\nImageNet classification task. DW convolution could have a large locality\nwithout increasing computational costs compared to CNNs, but the accuracy\nsaturates with larger kernels. CADA follows this characteristic of locality. We\nshowed that context awareness was the crucial property; however, large local\ninformation was not necessary to construct CA parameters. Even though no\nspatial invariance makes training difficult, more relaxed spatial invariance\ngave better accuracy than strict spatial invariance. Also, additional strong\nspatial invariance through relative position encoding was preferable. We\nextended these experiments to filters for downsampling and showed that locality\nbias is more critical for downsampling but can remove the strong locality bias\nusing relaxed spatial invariance.",
    "descriptor": "",
    "authors": [
      "Nana Arizumi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17141"
  },
  {
    "id": "arXiv:2210.17142",
    "title": "Towards Relation-centered Pooling and Convolution for Heterogeneous  Graph Learning Networks",
    "abstract": "Heterogeneous graph neural network has unleashed great potential on graph\nrepresentation learning and shown superior performance on downstream tasks such\nas node classification and clustering. Existing heterogeneous graph learning\nnetworks are primarily designed to either rely on pre-defined meta-paths or use\nattention mechanisms for type-specific attentive message propagation on\ndifferent nodes/edges, incurring many customization efforts and computational\ncosts. To this end, we design a relation-centered Pooling and Convolution for\nHeterogeneous Graph learning Network, namely PC-HGN, to enable\nrelation-specific sampling and cross-relation convolutions, from which the\nstructural heterogeneity of the graph can be better encoded into the embedding\nspace through the adaptive training process. We evaluate the performance of the\nproposed model by comparing with state-of-the-art graph learning models on\nthree different real-world datasets, and the results show that PC-HGN\nconsistently outperforms all the baseline and improves the performance\nmaximumly up by 17.8%.",
    "descriptor": "",
    "authors": [
      "Tiehua Zhang",
      "Yuze Liu",
      "Yao Yao",
      "Youhua Xia",
      "Xin Chen",
      "Xiaowei Huang",
      "Jiong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17142"
  },
  {
    "id": "arXiv:2210.17143",
    "title": "Improving Audio-Language Learning with MixGen and Multi-Level Test-Time  Augmentation",
    "abstract": "In this paper, we propose two novel augmentation methods 1) audio-language\nMixGen (AL-MixGen) and 2) multi-level test-time augmentation (Multi-TTA) for\naudio-language learning. Inspired by MixGen, which is originally applied to\nvision-language learning, we introduce an augmentation method for the\naudio-language domain. We also explore the impact of test-time augmentations\nand present Multi-TTA which generalizes test-time augmentation over multiple\nlayers of a deep learning model. Incorporating AL-MixGen and Multi-TTA into the\nbaseline achieves 47.5 SPIDEr on audio captioning, which is an +18.2% over the\nbaseline and outperforms the state-of-the-art approach with a 5x smaller model.\nIn audio-text retrieval, the proposed methods surpass the baseline performance\nas well.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Eungbeom Kim",
      "Jinhee Kim",
      "Yoori Oh",
      "Kyungsu Kim",
      "Minju Park",
      "Jaeheon Sim",
      "Jinwoo Lee",
      "Kyogu Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17143"
  },
  {
    "id": "arXiv:2210.17146",
    "title": "LAD-RCNN:A Powerful Tool for Livestock Face Detection and Normalization",
    "abstract": "With the demand for standardized large-scale livestock farming and the\ndevelopment of artificial intelligence technology, a lot of research in area of\nanimal face recognition were carried on pigs, cattle, sheep and other\nlivestock. Face recognition consists of three sub-task: face detection, face\nnormalizing and face identification. Most of animal face recognition study\nfocuses on face detection and face identification. Animals are often\nuncooperative when taking photos, so the collected animal face images are often\nin arbitrary directions. The use of non-standard images may significantly\nreduce the performance of face recognition system. However, there is no study\non normalizing of the animal face image with arbitrary directions. In this\nstudy, we developed a light-weight angle detection and region-based\nconvolutional network (LAD-RCNN) containing a new rotation angle coding method\nthat can detect the rotation angle and the location of animal face in\none-stage. LAD-RCNN has a frame rate of 72.74 FPS (including all steps) on a\nsingle GeForce RTX 2080 Ti GPU. LAD-RCNN has been evaluated on multiple dataset\nincluding goat dataset and gaot infrared image. Evaluation result show that the\nAP of face detection was more than 95% and the deviation between the detected\nrotation angle and the ground-truth rotation angle were less than 0.036 (i.e.\n6.48{\\deg}) on all the test dataset. This shows that LAD-RCNN has excellent\nperformance on livestock face and its direction detection, and therefore it is\nvery suitable for livestock face detection and Normalizing. Code is available\nat https://github.com/SheepBreedingLab-HZAU/LAD-RCNN/",
    "descriptor": "\nComments: 8 figures, 5 tables\n",
    "authors": [
      "Ling Sun",
      "Guiqiong Liu",
      "Junrui Liu",
      "Xunping Jiang",
      "Xu Wang",
      "Han Yang",
      "Shiping Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17146"
  },
  {
    "id": "arXiv:2210.17149",
    "title": "Exploring the effectiveness of surrogate-assisted evolutionary  algorithms on the batch processing problem",
    "abstract": "Real-world optimisation problems typically have objective functions which\ncannot be expressed analytically. These optimisation problems are evaluated\nthrough expensive physical experiments or simulations. Cheap approximations of\nthe objective function can reduce the computational requirements for solving\nthese expensive optimisation problems. These cheap approximations may be\nmachine learning or statistical models and are known as surrogate models. This\npaper introduces a simulation of a well-known batch processing problem in the\nliterature. Evolutionary algorithms such as Genetic Algorithm (GA),\nDifferential Evolution (DE) are used to find the optimal schedule for the\nsimulation. We then compare the quality of solutions obtained by the\nsurrogate-assisted versions of the algorithms against the baseline algorithms.\nSurrogate-assistance is achieved through Probablistic Surrogate-Assisted\nFramework (PSAF). The results highlight the potential for improving baseline\nevolutionary algorithms through surrogates. For different time horizons, the\nsolutions are evaluated with respect to several quality indicators. It is shown\nthat the PSAF assisted GA (PSAF-GA) and PSAF-assisted DE (PSAF-DE) provided\nimprovement in some time horizons. In others, they either maintained the\nsolutions or showed some deterioration. The results also highlight the need to\ntune the hyper-parameters used by the surrogate-assisted framework, as the\nsurrogate, in some instances, shows some deterioration over the baseline\nalgorithm.",
    "descriptor": "\nComments: Accepted for publication in SACAIR2022\n",
    "authors": [
      "Mohamed Z. Variawa",
      "Terence L. Van Zyl",
      "Matthew Woolway"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17149"
  },
  {
    "id": "arXiv:2210.17150",
    "title": "Monotonic Mechanisms for Selling Multiple Goods",
    "abstract": "Maximizing the revenue from selling two or more goods has been shown to\nrequire the use of $nonmonotonic$ mechanisms, where a higher-valuation buyer\nmay pay less than a lower-valuation one. Here we show that the restriction to\n$monotonic$ mechanisms may not just lower the revenue, but may in fact yield\nonly a $negligible$ $fraction$ of the maximal revenue; more precisely, the\nrevenue from monotonic mechanisms is no more than k times the simple revenue\nobtainable by selling the goods separately, or bundled (where k is the number\nof goods), whereas the maximal revenue may be arbitrarily larger. We then study\nthe class of monotonic mechanisms and its subclass of allocation-monotonic\nmechanisms, and obtain useful characterizations and revenue bounds.",
    "descriptor": "\nComments: this http URL\n",
    "authors": [
      "Ran Ben Moshe",
      "Sergiu Hart",
      "Noam Nisan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2210.17150"
  },
  {
    "id": "arXiv:2210.17151",
    "title": "Tech Report: One-stage Lightweight Object Detectors",
    "abstract": "This work is for designing one-stage lightweight detectors which perform well\nin terms of mAP and latency. With baseline models each of which targets on GPU\nand CPU respectively, various operations are applied instead of the main\noperations in backbone networks of baseline models. In addition to experiments\nabout backbone networks and operations, several feature pyramid network (FPN)\narchitectures are investigated. Benchmarks and proposed detectors are analyzed\nin terms of the number of parameters, Gflops, GPU latency, CPU latency and mAP,\non MS COCO dataset which is a benchmark dataset in object detection. This work\npropose similar or better network architectures considering the trade-off\nbetween accuracy and latency. For example, our proposed GPU-target backbone\nnetwork outperforms that of YOLOX-tiny which is selected as the benchmark by\n1.43x in speed and 0.5 mAP in accuracy on NVIDIA GeForce RTX 2080 Ti GPU.",
    "descriptor": "",
    "authors": [
      "Deokki Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.17151"
  },
  {
    "id": "arXiv:2210.17152",
    "title": "Audio Time-Scale Modification with Temporal Compressing Networks",
    "abstract": "We proposed a novel approach in the field of time-scale modification on audio\nsignals. While traditional methods use the framing technique, spectral approach\nuses the short-time Fourier transform to preserve the frequency during temporal\nstretching. TSM-Net, our neural-network model encodes the raw audio into a\nhigh-level latent representation. We call it Neuralgram, in which one vector\nrepresents 1024 audio samples. It is inspired by the framing technique but\naddresses the clipping artifacts. The Neuralgram is a two-dimensional matrix\nwith real values, we can apply some existing image resizing techniques on the\nNeuralgram and decode it using our neural decoder to obtain the time-scaled\naudio. Both the encoder and decoder are trained with GANs, which shows fair\ngeneralization ability on the scaled Neuralgrams. Our method yields little\nartifacts and opens a new possibility in the research of modern time-scale\nmodification. The audio samples can be found on\nhttps://ernestchu.github.io/tsm-net-demo/",
    "descriptor": "",
    "authors": [
      "Ernie Chu",
      "Ju-Ting Chen",
      "Chia-Ping Chen"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17152"
  },
  {
    "id": "arXiv:2210.17155",
    "title": "Mahiru: a federated, policy-driven data processing and exchange system",
    "abstract": "Secure, privacy-preserving sharing of scientific or business data is\ncurrently a popular topic for research and development, both in academia and\noutside of it. Systems have been proposed for sharing individual facts about\nindividuals and sharing entire data sets, for sharing data through trusted\nthird parties, for obfuscating sensitive data by anonymisation and homomorphic\nencryption, for distributed processing as in federated machine learning and\nsecure multiparty computation, and for trading data access or ownership.\nHowever, these systems typically support only one of these solutions, while\norganisations often have a variety of data and use cases for which different\nsolutions are appropriate. If a single system could be built that is flexible\nenough to support a variety of solutions, then administration would be greatly\nsimplified and attack surfaces reduced. In this paper we present Mahiru, a\ndesign for a data exchange and processing system in which owners of data and\nsoftware fully control their assets, users may submit a wide variety of\nprocessing requests including most of the above applications, and all parties\ncollaborate to execute those requests in a distributed fashion, while ensuring\nthat the policies are adhered to at all times. This is achieved through a\nfederated, mostly decentralised architecture and a powerful policy mechanism\ndesigned to be easy to understand and simple to implement. We have created a\nproof-of-concept implementation of the system which is openly available and in\ncontinuous development, and which we aim to continue to extend with new\nfunctionality.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Lourens E. Veen",
      "Sara Shakeri",
      "Paola Grosso"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17155"
  },
  {
    "id": "arXiv:2210.17157",
    "title": "1Cademy @ Causal News Corpus 2022: Enhance Causal Span Detection via  Beam-Search-based Position Selector",
    "abstract": "In this paper, we present our approach and empirical observations for\nCause-Effect Signal Span Detection -- Subtask 2 of Shared task\n3~\\cite{tan-etal-2022-event} at CASE 2022. The shared task aims to extract the\ncause, effect, and signal spans from a given causal sentence. We model the task\nas a reading comprehension (RC) problem and apply a token-level RC-based span\nprediction paradigm to the task as the baseline. We explore different training\nobjectives to fine-tune the model, as well as data augmentation (DA) tricks\nbased on the language model (LM) for performance improvement. Additionally, we\npropose an efficient beam-search post-processing strategy to due with the\ndrawbacks of span detection to obtain a further performance gain. Our approach\nachieves an average $F_1$ score of 54.15 and ranks \\textbf{$1^{st}$} in the\nCASE competition. Our code is available at\n\\url{https://github.com/Gzhang-umich/1CademyTeamOfCASE}.",
    "descriptor": "\nComments: paper of CASE workshop in EMNLP 2022\n",
    "authors": [
      "Xingran Chen",
      "Ge Zhang",
      "Adam Nik",
      "Mingyu Li",
      "Jie Fu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17157"
  },
  {
    "id": "arXiv:2210.17159",
    "title": "PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks",
    "abstract": "Aside from graph neural networks (GNNs) catching significant attention as a\npowerful framework revolutionizing graph representation learning, there has\nbeen an increasing demand for explaining GNN models. Although various\nexplanation methods for GNNs have been developed, most studies have focused on\ninstance-level explanations, which produce explanations tailored to a given\ngraph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),\na novel model-level GNN explanation method that explains what the underlying\nGNN model has learned for graph classification by discovering\nhuman-interpretable prototype graphs. Our method produces explanations for a\ngiven class, thus being capable of offering more concise and comprehensive\nexplanations than those of instance-level explanations. First, PAGE selects\nembeddings of class-discriminative input graphs on the graph-level embedding\nspace after clustering them. Then, PAGE discovers a common subgraph pattern by\niteratively searching for high matching node tuples using node-level embeddings\nvia a prototype scoring function, thereby yielding a prototype graph as our\nexplanation. Using five graph classification datasets, we demonstrate that PAGE\nqualitatively and quantitatively outperforms the state-of-the-art model-level\nexplanation method. We also carry out experimental studies systematically by\nshowing the relationship between PAGE and instance-level explanation methods,\nthe robustness of PAGE to input data scarce environments, and the computational\nefficiency of the proposed prototype scoring function in PAGE.",
    "descriptor": "\nComments: 18 pages, 10 figures, 5 tables; its two-page extended summary was presented in the AAAI-22 Student Abstract and Poster Program\n",
    "authors": [
      "Yong-Min Shin",
      "Sun-Woo Kim",
      "Won-Yong Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.17159"
  },
  {
    "id": "arXiv:2210.17161",
    "title": "Improving Cause-of-Death Classification from Verbal Autopsy Reports",
    "abstract": "In many lower-and-middle income countries including South Africa, data access\nin health facilities is restricted due to patient privacy and confidentiality\npolicies. Further, since clinical data is unique to individual institutions and\nlaboratories, there are insufficient data annotation standards and conventions.\nAs a result of the scarcity of textual data, natural language processing (NLP)\ntechniques have fared poorly in the health sector. A cause of death (COD) is\noften determined by a verbal autopsy (VA) report in places without reliable\ndeath registration systems. A non-clinician field worker does a VA report using\na set of standardized questions as a guide to uncover symptoms of a COD. This\nanalysis focuses on the textual part of the VA report as a case study to\naddress the challenge of adapting NLP techniques in the health domain. We\npresent a system that relies on two transfer learning paradigms of monolingual\nlearning and multi-source domain adaptation to improve VA narratives for the\ntarget task of the COD classification. We use the Bidirectional Encoder\nRepresentations from Transformers (BERT) and Embeddings from Language Models\n(ELMo) models pre-trained on the general English and health domains to extract\nfeatures from the VA narratives. Our findings suggest that this transfer\nlearning system improves the COD classification tasks and that the narrative\ntext contains valuable information for figuring out a COD. Our results further\nshow that combining binary VA features and narrative text features learned via\nthis framework boosts the classification task of COD.",
    "descriptor": "\nComments: Southern African Conference for Artificial Intelligence Research\n",
    "authors": [
      "Thokozile Manaka",
      "Terence van Zyl",
      "Deepak Kar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17161"
  },
  {
    "id": "arXiv:2210.17163",
    "title": "HHLPy: Practical Verification of Hybrid Systems using Hoare Logic",
    "abstract": "We present a tool for verification of hybrid systems expressed in the\nsequential fragment of HCSP (Hybrid Communicating Sequential Processes). The\ntool permits annotating HCSP programs with pre- and postconditions, invariants,\nand proof rules for reasoning about ordinary differential equations.\nVerification conditions are generated from the annotations following the rules\nof hybrid Hoare logic. We designed labeling and highlighting mechanisms to\ndistinguish and visualize different verification conditions. The tool is\nimplemented in Python and has a web-based user interface. We evaluated the\neffectiveness of the tool on translations of Simulink/Stateflow models and on\nKeYmaera X benchmarks.",
    "descriptor": "",
    "authors": [
      "Huanhuan Sheng",
      "Alexander Bentkamp",
      "Bohua Zhan"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.17163"
  },
  {
    "id": "arXiv:2210.17166",
    "title": "Listen to what they say: Better understand and detect online  misinformation with user feedback",
    "abstract": "Social media users who report content are key allies in the management of\nonline misinformation, however, no research has been conducted yet to\nunderstand their role and the different trends underlying their reporting\nactivity. We suggest an original approach to studying misinformation: examining\nit from the reporting users perspective at the content-level and comparatively\nacross regions and platforms. We propose the first classification of reported\ncontent pieces, resulting from a review of c. 9,000 items reported on Facebook\nand Instagram in France, the UK, and the US in June 2020. This allows us to\nobserve meaningful distinctions regarding reporting content between countries\nand platforms as it significantly varies in volume, type, topic, and\nmanipulation technique. Examining six of these techniques, we identify a novel\none that is specific to Instagram US and significantly more sophisticated than\nothers, potentially presenting a concrete challenge for algorithmic detection\nand human moderation. We also identify four reporting behaviours, from which we\nderive four types of noise capable of explaining half of the inaccuracy found\nin content reported as misinformation. We finally show that breaking down the\nuser reporting signal into a plurality of behaviours allows to train a simple,\nalthough competitive, classifier on a small dataset with a combination of basic\nusers-reports to classify the different types of reported content pieces.",
    "descriptor": "",
    "authors": [
      "Hubert Etienne",
      "Onur \u00c7elebi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17166"
  },
  {
    "id": "arXiv:2210.17167",
    "title": "Reduce Catastrophic Forgetting of Dense Retrieval Training with  Teleportation Negatives",
    "abstract": "In this paper, we investigate the instability in the standard dense retrieval\ntraining, which iterates between model training and hard negative selection\nusing the being-trained model. We show the catastrophic forgetting phenomena\nbehind the training instability, where models learn and forget different\nnegative groups during training iterations. We then propose ANCE-Tele, which\naccumulates momentum negatives from past iterations and approximates future\niterations using lookahead negatives, as \"teleportations\" along the time axis\nto smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms\nprevious state-of-the-art systems of similar size, eliminates the dependency on\nsparse retrieval negatives, and is competitive among systems using\nsignificantly more (50x) parameters. Our analysis demonstrates that\nteleportation negatives reduce catastrophic forgetting and improve convergence\nspeed for dense retrieval training. Our code is available at\nhttps://github.com/OpenMatch/ANCE-Tele.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference\n",
    "authors": [
      "Si Sun",
      "Chenyan Xiong",
      "Yue Yu",
      "Arnold Overwijk",
      "Zhiyuan Liu",
      "Jie Bao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17167"
  },
  {
    "id": "arXiv:2210.17168",
    "title": "SDCL: Self-Distillation Contrastive Learning for Chinese Spell Checking",
    "abstract": "Due to the ambiguity of homophones, Chinese Spell Checking (CSC) has\nwidespread applications. Existing systems typically utilize BERT for text\nencoding. However, CSC requires the model to account for both phonetic and\ngraphemic information. To adapt BERT to the CSC task, we propose a token-level\nself-distillation contrastive learning method. We employ BERT to encode both\nthe corrupted and corresponding correct sentence. Then, we use contrastive\nlearning loss to regularize corrupted tokens' hidden states to be closer to\ncounterparts in the correct sentence. On three CSC datasets, we confirmed our\nmethod provides a significant improvement above baselines.",
    "descriptor": "",
    "authors": [
      "Xiaotian Zhang",
      "Hang Yan",
      "Sun Yu",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17168"
  },
  {
    "id": "arXiv:2210.17170",
    "title": "Efficient Document Retrieval by End-to-End Refining and Quantizing BERT  Embedding with Contrastive Product Quantization",
    "abstract": "Efficient document retrieval heavily relies on the technique of semantic\nhashing, which learns a binary code for every document and employs Hamming\ndistance to evaluate document distances. However, existing semantic hashing\nmethods are mostly established on outdated TFIDF features, which obviously do\nnot contain lots of important semantic information about documents.\nFurthermore, the Hamming distance can only be equal to one of several integer\nvalues, significantly limiting its representational ability for document\ndistances. To address these issues, in this paper, we propose to leverage BERT\nembeddings to perform efficient retrieval based on the product quantization\ntechnique, which will assign for every document a real-valued codeword from the\ncodebook, instead of a binary code as in semantic hashing. Specifically, we\nfirst transform the original BERT embeddings via a learnable mapping and feed\nthe transformed embedding into a probabilistic product quantization module to\noutput the assigned codeword. The refining and quantizing modules can be\noptimized in an end-to-end manner by minimizing the probabilistic contrastive\nloss. A mutual information maximization based method is further proposed to\nimprove the representativeness of codewords, so that documents can be quantized\nmore accurately. Extensive experiments conducted on three benchmarks\ndemonstrate that our proposed method significantly outperforms current\nstate-of-the-art baselines.",
    "descriptor": "",
    "authors": [
      "Zexuan Qiu",
      "Qinliang Su",
      "Jianxing Yu",
      "Shijing Si"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.17170"
  },
  {
    "id": "arXiv:2210.17174",
    "title": "uBFT: Microsecond-scale BFT using Disaggregated Memory",
    "abstract": "We propose uBFT, the first State Machine Replication (SMR) system to achieve\nmicrosecond-scale latency in data centers, while using only $2f{+}1$ replicas\nto tolerate $f$ Byzantine failures. The Byzantine Fault Tolerance (BFT)\nprovided by uBFT is essential as pure crashes appear to be a mere illusion with\nreal-life systems reportedly failing in many unexpected ways. uBFT relies on a\nsmall non-tailored trusted computing base -- disaggregated memory -- and\nconsumes a practically bounded amount of memory. uBFT is based on a novel\nabstraction called Consistent Tail Broadcast, which we use to prevent\nequivocation while bounding memory. We implement uBFT using RDMA-based\ndisaggregated memory and obtain an end-to-end latency of as little as 10us.\nThis is at least 50$\\times$ faster than MinBFT , a state of the art $2f{+}1$\nBFT SMR based on Intel's SGX. We use uBFT to replicate two KV-stores (Memcached\nand Redis), as well as a financial order matching engine (Liquibook). These\napplications have low latency (up to 20us) and become Byzantine tolerant with\nas little as 10us more. The price for uBFT is a small amount of reliable\ndisaggregated memory (less than 1 MiB), which in our prototype consists of a\nsmall number of memory servers connected through RDMA and replicated for fault\ntolerance.",
    "descriptor": "",
    "authors": [
      "Marcos K. Aguilera",
      "Naama Ben-David",
      "Rachid Guerraoui",
      "Antoine Murat",
      "Athanasios Xygkis",
      "Igor Zablotchi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17174"
  },
  {
    "id": "arXiv:2210.17175",
    "title": "Low-Latency, High-Throughput Garbage Collection (Extended Version)",
    "abstract": "Production garbage collectors make substantial compromises in pursuit of\nreduced pause times. They require far more CPU cycles and memory than prior\nsimpler collectors. concurrent copying collectors (C4, ZGC, and Shenandoah)\nsuffer from the following design limitations. 1) Concurrent copying. They only\nreclaim memory by copying, which is inherently expensive with high memory\nbandwidth demands. Concurrent copying also requires expensive read and write\nbarriers. 2) Scalability. They depend on tracing, which in the limit and in\npractice does not scale. 3) Immediacy. They do not reclaim older objects\npromptly, incurring high memory overheads.\nWe present LXR, which takes a very different approach to optimizing\nresponsiveness and throughput by minimizing concurrent collection work and\noverheads. 1) LXR reclaims most memory without any copying by using the Immix\nheap structure. It then combats fragmentation with limited judicious\nstop-the-world copying. 2) LXR uses reference counting to achieve both\nscalability and immediacy, promptly reclaiming young and old objects. It uses\nconcurrent tracing as needed for identifying cyclic garbage. 3) To minimize\npause times while allowing judicious copying of mature objects, LXR introduces\nremembered sets for reference counting and concurrent decrement processing. 4)\nLXR introduces a novel low-overhead write barrier that combines coalescing\nreference counting, concurrent tracing, and remembered set maintenance.\nThe result is a collector with excellent responsiveness and throughput. On\nthe widely-used Lucene search engine with a generously sized heap, LXR has 6x\nhigher throughput while delivering 30x lower 99.9 percentile tail latency than\nthe popular Shenandoah production collector in its default configuration.",
    "descriptor": "\nComments: 17 pages, 7 Figures. This extends the original publication with an LBO analysis (Section 5.5)\n",
    "authors": [
      "Wenyu Zhao",
      "Stephen M. Blackburn",
      "Kathryn S. McKinley"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.17175"
  },
  {
    "id": "arXiv:2210.17178",
    "title": "Learning to Optimize Permutation Flow Shop Scheduling via Graph-based  Imitation Learning",
    "abstract": "The permutation flow shop scheduling (PFSS), aiming at finding the optimal\npermutation of jobs, is widely used in manufacturing systems. When solving the\nlarge-scale PFSS problems, traditional optimization algorithms such as\nheuristics could hardly meet the demands of both solution accuracy and\ncomputational efficiency. Thus learning-based methods have recently garnered\nmore attention. Some work attempts to solve the problems by reinforcement\nlearning methods, which suffer from slow convergence issues during training and\nare still not accurate enough regarding the solutions. To that end, we train\nthe model via expert-driven imitation learning, which accelerates the\nconvergence more stably and accurately. Moreover, in order to extract better\nfeature representations of input jobs, we incorporate the graph structure as\nthe encoder. The extensive experiments reveal that our proposed model obtains\nsignificant promotion and presents excellent generalizability in large-scale\nproblems with up to 1000 jobs. Compared to the state-of-the-art reinforcement\nlearning method, our model's network parameters are reduced to only 37\\% of\ntheirs, and the solution gap of our model towards the expert solutions\ndecreases from 6.8\\% to 1.3\\% on average.",
    "descriptor": "\nComments: 11 pages, 5 figures, 11 tables\n",
    "authors": [
      "Longkang Li",
      "Siyuan Liang",
      "Zihao Zhu",
      "Xiaochun Cao",
      "Chris Ding",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.17178"
  },
  {
    "id": "arXiv:2210.17180",
    "title": "Automatic Subspace Evoking for Efficient Neural Architecture Search",
    "abstract": "Neural Architecture Search (NAS) aims to automatically find effective\narchitectures from a predefined search space. However, the search space is\noften extremely large. As a result, directly searching in such a large search\nspace is non-trivial and also very time-consuming. To address the above issues,\nin each search step, we seek to limit the search space to a small but effective\nsubspace to boost both the search performance and search efficiency. To this\nend, we propose a novel Neural Architecture Search method via Automatic\nSubspace Evoking (ASE-NAS) that finds promising architectures in automatically\nevoked subspaces. Specifically, we first perform a global search, i.e.,\nautomatic subspace evoking, to evoke/find a good subspace from a set of\ncandidates. Then, we perform a local search within the evoked subspace to find\nan effective architecture. More critically, we further boost search performance\nby taking well-designed/searched architectures as the initial candidate\nsubspaces. Extensive experiments show that our ASE-NAS not only greatly reduces\nthe search cost but also finds better architectures than state-of-the-art\nmethods in various benchmark search spaces.",
    "descriptor": "",
    "authors": [
      "Yaofo Chen",
      "Yong Guo",
      "Daihai Liao",
      "Fanbing Lv",
      "Hengjie Song",
      "Mingkui Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17180"
  },
  {
    "id": "arXiv:2210.17183",
    "title": "Self-Supervised Hierarchical Metrical Structure Modeling",
    "abstract": "We propose a novel method to model hierarchical metrical structures for both\nsymbolic music and audio signals in a self-supervised manner with minimal\ndomain knowledge. The model trains and inferences on beat-aligned music signals\nand predicts an 8-layer hierarchical metrical tree from beat, measure to the\nsection level. The training procedural does not require any hierarchical\nmetrical labeling except for beats, purely relying on the nature of metrical\nregularity and inter-voice consistency as inductive biases. We show in\nexperiments that the method achieves comparable performance with supervised\nbaselines on multiple metrical structure analysis tasks on both symbolic music\nand audio signals. All demos, source code and pre-trained models are publicly\navailable on GitHub.",
    "descriptor": "",
    "authors": [
      "Junyan Jiang",
      "Gus Xia"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17183"
  },
  {
    "id": "arXiv:2210.17185",
    "title": "SurfMyoAiR: A surface Electromyography based framework for Airwriting  Recognition",
    "abstract": "Airwriting Recognition is the task of identifying letters written in free\nspace with finger movement. Electromyography (EMG) is a technique used to\nrecord electrical activity during muscle contraction and relaxation as a result\nof movement and is widely used for gesture recognition. Most of the current\nresearch in gesture recognition is focused on identifying static gestures.\nHowever, dynamic gestures are natural and user-friendly for being used as\nalternate input methods in Human-Computer Interaction applications. Airwriting\nrecognition using EMG signals recorded from forearm muscles is therefore a\nviable solution. Since the user does not need to learn any new gestures and a\nlarge range of words can be formed by concatenating these letters, it is\ngeneralizable to a wider population. There has been limited work in recognition\nof airwriting using EMG signals and forms the core idea of the current work.\nThe SurfMyoAiR dataset comprising of EMG signals recorded during writing\nEnglish uppercase alphabets is constructed. Several different time-domain\nfeatures to construct EMG envelope and two different time-frequency image\nrepresentations: Short-Time Fourier Transform and Continuous Wavelet Transform\nwere explored to form the input to a deep learning model for airwriting\nrecognition. Several different deep learning architectures were exploited for\nthis task. Additionally, the effect of various parameters such as signal\nlength, window length and interpolation techniques on the recognition\nperformance is comprehensively explored. The best-achieved accuracy was 78.50%\nand 62.19% in user-dependent and independent scenarios respectively by using\nShort-Time Fourier Transform in conjunction with a 2D Convolutional Neural\nNetwork based classifier. Airwriting has great potential as a user-friendly\nmodality to be used as an alternate input method in Human-Computer Interaction\napplications.",
    "descriptor": "",
    "authors": [
      "Ayush Tripathi",
      "Lalan Kumar",
      "Prathosh A.P.",
      "Suriya Prakash Muthukrishnan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.17185"
  },
  {
    "id": "arXiv:2210.17190",
    "title": "IITD at the WANLP 2022 Shared Task: Multilingual Multi-Granularity  Network for Propaganda Detection",
    "abstract": "We present our system for the two subtasks of the shared task on propaganda\ndetection in Arabic, part of WANLP'2022. Subtask 1 is a multi-label\nclassification problem to find the propaganda techniques used in a given tweet.\nOur system for this task uses XLM-R to predict probabilities for the target\ntweet to use each of the techniques. In addition to finding the techniques,\nSubtask 2 further asks to identify the textual span for each instance of each\ntechnique that is present in the tweet; the task can be modeled as a sequence\ntagging problem. We use a multi-granularity network with mBERT encoder for\nSubtask 2. Overall, our system ranks second for both subtasks (out of 14 and 3\nparticipants, respectively). Our empirical analysis show that it does not help\nto use a much larger English corpus annotated with propaganda techniques,\nregardless of whether used in English or after translation to Arabic.",
    "descriptor": "",
    "authors": [
      "Shubham Mittal",
      "Preslav Nakov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17190"
  },
  {
    "id": "arXiv:2210.17203",
    "title": "Using Locality-sensitive Hashing for Rendezvous Search",
    "abstract": "The multichannel rendezvous problem is a fundamental problem for neighbor\ndiscovery in many IoT applications. The existing works in the literature focus\nmostly on improving the worst-case performance, and the average-case\nperformance is often not as good as that of the random algorithm. As IoT\ndevices (users) are close to each other, their available channel sets, though\nthey might be different, are similar. Using the locality-sensitive hashing\n(LSH) technique in data mining, we propose channel hopping algorithms that\nexploit the similarity between the two available channel sets to increase the\nrendezvous probability. For the synchronous setting, our algorithms have the\nexpected time-to-rendezvous (ETTR) inversely proportional to a well-known\nsimilarity measure called the Jaccard index. For the asynchronous setting, we\nuse dimensionality reduction to speed up the rendezvous process. Our numerical\nresults show that our algorithms can outperform the random algorithm in terms\nof ETTR.",
    "descriptor": "",
    "authors": [
      "Guann-Yng Jiang",
      "Cheng-Shang Chang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.17203"
  },
  {
    "id": "arXiv:2210.17207",
    "title": "Mapping Extended Landmarks for Radar SLAM",
    "abstract": "Simultaneous localization and mapping (SLAM) using automotive radar sensors\ncan provide enhanced sensing capabilities for autonomous systems. In SLAM\napplications, with a greater requirement for the environment map, information\non the extent of landmarks is vital for precise navigation and path planning.\nAlthough object extent estimation has been successfully applied in target\ntracking, its adaption to SLAM remains unaddressed due to the additional\nuncertainty of the sensor platform, bias in the odometer reading, as well as\nthe measurement non-linearity. In this paper, we propose to incorporate the\nBayesian random matrix approach to estimate the extent of landmarks in radar\nSLAM. We describe the details for implementation of landmark extent\ninitialization, prediction and update. To validate the performance of our\nproposed approach we compare with the model-free ellipse fitting algorithm with\nresults showing more consistent extent estimation. We also demonstrate that\nexploiting the landmark extent in the state update can improve localization\naccuracy.",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table\n",
    "authors": [
      "Shuai Sun",
      "Christopher Gilliam",
      "Kamran Ghorbani",
      "Glenn Matthews",
      "Beth Jelfs"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17207"
  },
  {
    "id": "arXiv:2210.17209",
    "title": "The Effect of Multiple Replies for Natural Language Generation Chatbots",
    "abstract": "In this research, by responding to users' utterances with multiple replies to\ncreate a group chat atmosphere, we alleviate the problem that Natural Language\nGeneration chatbots might reply with inappropriate content, thus causing a bad\nuser experience. Because according to our findings, users tend to pay attention\nto appropriate replies and ignore inappropriate replies. We conducted a 2\n(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)\nrepeated measures experiment to compare the chatting experience in different\nconditions. The result shows that users will have a better chatting experience\nwhen receiving multiple replies at once from the NLG model compared to the\nsingle reply. Furthermore, according to the effect size of our result, to\nimprove the chatting experience for NLG chatbots which is single reply and\nanonymous avatar, providing five replies will have more benefits than setting\nan anime avatar.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Eason Chen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17209"
  },
  {
    "id": "arXiv:2210.17213",
    "title": "Deep Gaussian Process-based Multi-fidelity Bayesian Optimization for  Simulated Chemical Reactors",
    "abstract": "New manufacturing techniques such as 3D printing have recently enabled the\ncreation of previously infeasible chemical reactor designs. Optimizing the\ngeometry of the next generation of chemical reactors is important to understand\nthe underlying physics and to ensure reactor feasibility in the real world.\nThis optimization problem is computationally expensive, nonlinear, and\nderivative-free making it challenging to solve. In this work, we apply deep\nGaussian processes (DGPs) to model multi-fidelity coiled-tube reactor\nsimulations in a Bayesian optimization setting. By applying a multi-fidelity\nBayesian optimization method, the search space of reactor geometries is\nexplored through an amalgam of different fidelity simulations which are chosen\nbased on prediction uncertainty and simulation cost, maximizing the use of\ncomputational budget. The use of DGPs provides an end-to-end model for five\ndiscrete mesh fidelities, enabling less computational effort to gain good\nsolutions during optimization. The accuracy of simulations for these five\nfidelities is determined against experimental data obtained from a 3D printed\nreactor configuration, providing insights into appropriate hyper-parameters. We\nhope this work provides interesting insight into the practical use of DGP-based\nmulti-fidelity Bayesian optimization for engineering discovery.",
    "descriptor": "\nComments: 4 pages, 2022 NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems\n",
    "authors": [
      "Tom Savage",
      "Nausheen Basha",
      "Omar Matar",
      "Ehecatl Antonio Del-Rio Chanona"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17213"
  },
  {
    "id": "arXiv:2210.17215",
    "title": "Mutation Testing Optimisations using the Clang Front-end",
    "abstract": "Mutation testing is the state-of-the-art technique for assessing the fault\ndetection capacity of a test suite. Unfortunately, a full mutation analysis is\noften prohibitively expensive. The CppCheck project for instance, demands a\nbuild time of 5.8 minutes and a test execution time of 17 seconds on our\ndesktop computer. An unoptimised mutation analysis, for 55,000 generated\nmutants took 11.8 days in total, of which 4.3 days is spent on (re)compiling\nthe project. In this paper we present a feasibility study, investigating how a\nnumber of optimisation strategies can be implemented based on the Clang\nfront-end. These optimisation strategies allow to eliminate the compilation and\nexecution overhead in order to support efficient mutation testing for the C\nlanguage family. We provide a proof-of-concept tool that achieves a speedup of\nbetween 2x and 30x. We make a detailed analysis of the speedup induced by the\noptimisations, elaborate on the lessons learned and point out avenues for\nfurther improvements.",
    "descriptor": "\nComments: Submitted to STVR 2022\n",
    "authors": [
      "Sten Vercammen",
      "Serge Demeyer",
      "Markus Borg",
      "Niklas Pettersson",
      "G\u00f6rel Hedin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.17215"
  },
  {
    "id": "arXiv:2210.17216",
    "title": "Symmetries, flat minima, and the conserved quantities of gradient flow",
    "abstract": "Empirical studies of the loss landscape of deep networks have revealed that\nmany local minima are connected through low-loss valleys. Ensemble models\nsampling different parts of a low-loss valley have reached SOTA performance.\nYet, little is known about the theoretical origin of such valleys. We present a\ngeneral framework for finding continuous symmetries in the parameter space,\nwhich carve out low-loss valleys. Importantly, we introduce a novel set of\nnonlinear, data-dependent symmetries for neural networks. These symmetries can\ntransform a trained model such that it performs similarly on new samples. We\nthen show that conserved quantities associated with linear symmetries can be\nused to define coordinates along low-loss valleys. The conserved quantities\nhelp reveal that using common initialization methods, gradient flow only\nexplores a small part of the global minimum. By relating conserved quantities\nto convergence rate and sharpness of the minimum, we provide insights on how\ninitialization impacts convergence and generalizability. We also find the\nnonlinear action to be viable for ensemble building to improve robustness under\ncertain adversarial attacks.",
    "descriptor": "\nComments: Preliminary version; comments welcome\n",
    "authors": [
      "Bo Zhao",
      "Iordan Ganev",
      "Robin Walters",
      "Rose Yu",
      "Nima Dehmamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Representation Theory (math.RT)"
    ],
    "url": "https://arxiv.org/abs/2210.17216"
  },
  {
    "id": "arXiv:2210.17217",
    "title": "AutoBag: Learning to Open Plastic Bags and Insert Objects",
    "abstract": "Thin plastic bags are ubiquitous in retail stores, healthcare, food handling,\nrecycling, homes, and school lunchrooms. They are challenging both for\nperception (due to specularities and occlusions) and for manipulation (due to\nthe dynamics of their 3D deformable structure). We formulate the task of\nmanipulating common plastic shopping bags with two handles from an unstructured\ninitial state to a state where solid objects can be inserted into the bag for\ntransport. We propose a self-supervised learning framework where a dual-arm\nrobot learns to recognize the handles and rim of plastic bags using\nUV-fluorescent markings; at execution time, the robot does not use UV markings\nor UV light. We propose Autonomous Bagging (AutoBag), where the robot uses the\nlearned perception model to open plastic bags through iterative manipulation.\nWe present novel metrics to evaluate the quality of a bag state and new motion\nprimitives for reorienting and opening bags from visual observations. In\nphysical experiments, a YuMi robot using AutoBag is able to open bags and\nachieve a success rate of 16/30 for inserting at least one item across a\nvariety of initial bag configurations. Supplementary material is available at\nhttps://sites.google.com/view/autobag .",
    "descriptor": "",
    "authors": [
      "Lawrence Yunliang Chen",
      "Baiyu Shi",
      "Daniel Seita",
      "Richard Cheng",
      "Thomas Kollar",
      "David Held",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.17217"
  },
  {
    "id": "arXiv:2210.17218",
    "title": "Artificial intelligence in government: Concepts, standards, and a  unified framework",
    "abstract": "Recent advances in artificial intelligence (AI) and machine learning (ML)\nhold the promise of improving government. Given the advanced capabilities of AI\napplications, it is critical that these are embedded using standard operational\nprocedures, clear epistemic criteria, and behave in alignment with the\nnormative expectations of society. Scholars in multiple domains have\nsubsequently begun to conceptualize the different forms that AI systems may\ntake, highlighting both their potential benefits and pitfalls. However, the\nliterature remains fragmented, with researchers in social science disciplines\nlike public administration and political science, and the fast-moving fields of\nAI, ML, and robotics, all developing concepts in relative isolation. Although\nthere are calls to formalize the emerging study of AI in government, a balanced\naccount that captures the full breadth of theoretical perspectives needed to\nunderstand the consequences of embedding AI into a public sector context is\nlacking. Here, we unify efforts across social and technical disciplines by\nusing concept mapping to identify 107 different terms used in the\nmultidisciplinary study of AI. We inductively sort these into three distinct\nsemantic groups, which we label the (a) operational, (b) epistemic, and (c)\nnormative domains. We then build on the results of this mapping exercise by\nproposing three new multifaceted concepts to study AI-based systems for\ngovernment (AI-GOV) in an integrated, forward-looking way, which we call (1)\noperational fitness, (2) epistemic completeness, and (3) normative salience.\nFinally, we put these concepts to work by using them as dimensions in a\nconceptual typology of AI-GOV and connecting each with emerging AI technical\nmeasurement standards to encourage operationalization, foster\ncross-disciplinary dialogue, and stimulate debate among those aiming to reshape\npublic administration with AI.",
    "descriptor": "\nComments: 35 pages with references and appendix, 3 tables, 2 figures\n",
    "authors": [
      "Vincent J. Straub",
      "Deborah Morgan",
      "Jonathan Bright",
      "Helen Margetts"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17218"
  },
  {
    "id": "arXiv:2210.17222",
    "title": "Combining Automatic Speaker Verification and Prosody Analysis for  Synthetic Speech Detection",
    "abstract": "The rapid spread of media content synthesis technology and the potentially\ndamaging impact of audio and video deepfakes on people's lives have raised the\nneed to implement systems able to detect these forgeries automatically. In this\nwork we present a novel approach for synthetic speech detection, exploiting the\ncombination of two high-level semantic properties of the human voice. On one\nside, we focus on speaker identity cues and represent them as speaker\nembeddings extracted using a state-of-the-art method for the automatic speaker\nverification task. On the other side, voice prosody, intended as variations in\nrhythm, pitch or accent in speech, is extracted through a specialized encoder.\nWe show that the combination of these two embeddings fed to a supervised binary\nclassifier allows the detection of deepfake speech generated with both\nText-to-Speech and Voice Conversion techniques. Our results show improvements\nover the considered baselines, good generalization properties over multiple\ndatasets and robustness to audio compression.",
    "descriptor": "",
    "authors": [
      "Luigi Attorresi",
      "Davide Salvi",
      "Clara Borrelli",
      "Paolo Bestagini",
      "Stefano Tubaro"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17222"
  },
  {
    "id": "arXiv:2210.17223",
    "title": "Lita: Accelerating Distributed Training of Sparsely Activated Models",
    "abstract": "Scaling model parameters usually improves model quality, but at the price of\nhigh computation overhead. Sparsely activated models, usually in the form of\nMixture of Experts (MoE) architecture, have constant computation cost over\ntheir dense counterparts, thus providing opportunities to train and serve a\nlarge model at a reasonable cost. However, the distributed training of an MoE\nmodel is prone to low efficiency, mainly due to the interleaved all-to-all\ncommunication during model computation.\nThis paper makes three main contributions. First, we systematically analyze\nthe all-to-all overhead in distributed training of MoE. Second, we propose a\nnew communication scheduling scheme based on tensor partitioning that\nprioritizes the all-to-all operations over other communication, due to its\nblocking nature. Third, we introduce expert packing that reduces the all-to-all\ntransfer size and incorporates optimizations to mitigate its overheads. Both\ntechniques effectively tackle the all-to-all bottleneck, and we integrate them\ninto a new system called Lina. Experiments on an A100 GPU testbed show that\nLina improves the training step time of popular NLP models by up to 1.73x over\nthe state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Jiamin Li",
      "Yimin Jiang",
      "Yibo Zhu",
      "Cong Wang",
      "Hong Xu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17223"
  },
  {
    "id": "arXiv:2210.17224",
    "title": "$\u03c9$GNNs: Deep Graph Neural Networks Enhanced by Multiple  Propagation Operators",
    "abstract": "Graph Neural Networks (GNNs) are limited in their propagation operators.\nThese operators often contain non-negative elements only and are shared across\nchannels and layers, limiting the expressiveness of GNNs. Moreover, some GNNs\nsuffer from over-smoothing, limiting their depth. On the other hand,\nConvolutional Neural Networks (CNNs) can learn diverse propagation filters, and\nphenomena like over-smoothing are typically not apparent in CNNs. In this\npaper, we bridge this gap by incorporating trainable channel-wise weighting\nfactors $\\omega$ to learn and mix multiple smoothing and sharpening propagation\noperators at each layer. Our generic method is called $\\omega$GNN, and we study\ntwo variants: $\\omega$GCN and $\\omega$GAT. For $\\omega$GCN, we theoretically\nanalyse its behaviour and the impact of $\\omega$ on the obtained node features.\nOur experiments confirm these findings, demonstrating and explaining how both\nvariants do not over-smooth. Additionally, we experiment with 15 real-world\ndatasets on node- and graph-classification tasks, where our $\\omega$GCN and\n$\\omega$GAT perform better or on par with state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Moshe Eliasof",
      "Lars Ruthotto",
      "Eran Treister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17224"
  },
  {
    "id": "arXiv:2210.17227",
    "title": "Modelling M/M/R-JSQ-PS Sojourn Time Distributions for URLLC Services",
    "abstract": "The future of networking promises to support time-sensitive applications that\nrequire ultra low latencies and reliabilities of 99.99%. Recent advances in\ncellular and WiFi connections enhance the network to meet high reliability and\nultra low latencies. However, the aforementioned services require that the\nserver processing time ensures low latencies with high reliability, otherwise\nthe end-to-end performance is not met. To that end, in this paper we use\nqueuing theory to model the sojourn time distribution for ultra reliable low\nlatency constrained services of M/M/R-JSQ-PS systems: Markovian queues with R\nCPUs following a join shortest queue processor sharing discipline (for example\nLinux systems). We develop open-source simulation software, and develop and\ncompare six analytical approximations for the sojourn time distribution. The\nproposed approximations yield Wasserstein distances below 2 time units, and\nupon medium loads incur into errors of less than 1.75 time units (e.g.,\nmilliseconds) for the 99.99th percentile sojourn time. Moreover, the proposed\napproximations are stable regardless the number of CPUs and stay close to the\nsimulations regardless the service time distribution. To show the applicability\nof our approximations, we leverage on a real world vehicular dataset to scale a\n99.99% reliable vehicular service and achieve accuracies above a 90%.",
    "descriptor": "\nComments: 16 Pages, 14 figures, submitted to IEEE Transactions on Services Computing\n",
    "authors": [
      "Geraint I. Palmer",
      "Jorge Mart\u00edn-P\u00e9rez"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.17227"
  },
  {
    "id": "arXiv:2210.17228",
    "title": "VertiBayes: Learning Bayesian network parameters from vertically  partitioned data with missing values",
    "abstract": "Federated learning makes it possible to train a machine learning model on\ndecentralized data. Bayesian networks are probabilistic graphical models that\nhave been widely used in artificial intelligence applications. Their popularity\nstems from the fact they can be built by combining existing expert knowledge\nwith data and are highly interpretable, which makes them useful for decision\nsupport, e.g. in healthcare. While some research has been published on the\nfederated learning of Bayesian networks, publications on Bayesian networks in a\nvertically partitioned or heterogeneous data setting (where different variables\nare located in different datasets) are limited, and suffer from important\nomissions, such as the handling of missing data. In this article, we propose a\nnovel method called VertiBayes to train Bayesian networks (structure and\nparameters) on vertically partitioned data, which can handle missing values as\nwell as an arbitrary number of parties. For structure learning we adapted the\nwidely used K2 algorithm with a privacy-preserving scalar product protocol. For\nparameter learning, we use a two-step approach: first, we learn an intermediate\nmodel using maximum likelihood by treating missing values as a special value\nand then we train a model on synthetic data generated by the intermediate model\nusing the EM algorithm. The privacy guarantees of our approach are equivalent\nto the ones provided by the privacy preserving scalar product protocol used. We\nexperimentally show our approach produces models comparable to those learnt\nusing traditional algorithms and we estimate the increase in complexity in\nterms of samples, network size, and complexity. Finally, we propose two\nalternative approaches to estimate the performance of the model using\nvertically partitioned data and we show in experiments that they lead to\nreasonably accurate estimates.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Florian van Daalen",
      "Lianne Ippel",
      "Andre Dekker",
      "Inigo Bermejo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.17228"
  },
  {
    "id": "arXiv:2210.17233",
    "title": "CorrLoss: Integrating Co-Occurrence Domain Knowledge for Affect  Recognition",
    "abstract": "Neural networks are widely adopted, yet the integration of domain knowledge\nis still underutilized. We propose to integrate domain knowledge about\nco-occurring facial movements as a constraint in the loss function to enhance\nthe training of neural networks for affect recognition. As the co-ccurrence\npatterns tend to be similar across datasets, applying our method can lead to a\nhigher generalizability of models and a lower risk of overfitting. We\ndemonstrate this by showing performance increases in cross-dataset testing for\nvarious datasets. We also show the applicability of our method for calibrating\nneural networks to different facial expressions.",
    "descriptor": "\nComments: This paper is accepted at IEEE 26TH International Conference on Pattern Recognition (ICPR) 2022\n",
    "authors": [
      "Ines Rieger",
      "Jaspar Pahl",
      "Bettina Finzel",
      "Ute Schmid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17233"
  },
  {
    "id": "arXiv:2210.17234",
    "title": "The language of opinion change on social media under the lens of  communicative action",
    "abstract": "Which messages are more effective at inducing a change of opinion in the\nlistener? We approach this question within the frame of Habermas' theory of\ncommunicative action, which posits that the illocutionary intent of the message\n(its pragmatic meaning) is the key. Thanks to recent advances in natural\nlanguage processing, we are able to operationalize this theory by extracting\nthe latent social dimensions of a message, namely archetypes of social intent\nof language, that come from social exchange theory. We identify key ingredients\nto opinion change by looking at more than 46k posts and more than 3.5M comments\non Reddit's r/ChangeMyView, a debate forum where people try to change each\nother's opinion and explicitly mark opinion-changing comments with a special\nflag called \"delta\". Comments that express no intent are about 77% less likely\nto change the mind of the recipient, compared to comments that convey at least\none social dimension. Among the various social dimensions, the ones that are\nmost likely to produce an opinion change are knowledge, similarity, and trust,\nwhich resonates with Habermas' theory of communicative action. We also find\nother new important dimensions, such as appeals to power or empathetic\nexpressions of support. Finally, in line with theories of constructive\nconflict, yet contrary to the popular characterization of conflict as the bane\nof modern social media, our findings show that voicing conflict in the context\nof a structured public debate can promote integration, especially when it is\nused to counter another conflictive stance. By leveraging recent advances in\nnatural language processing, our work provides an empirical framework for\nHabermas' theory, finds concrete examples of its effects in the wild, and\nsuggests its possible extension with a more faceted understanding of intent\ninterpreted as social dimensions of language.",
    "descriptor": "\nComments: Main paper: 13 pages, 1 figure, 3 tables. Supplementary material: 9 pages, 6 figures, 8 tables\n",
    "authors": [
      "Corrado Monti",
      "Luca Maria Aiello",
      "Gianmarco De Francisci Morales",
      "Francesco Bonchi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.17234"
  },
  {
    "id": "arXiv:2210.17235",
    "title": "50 Ways to Bake a Cookie: Mapping the Landscape of Procedural Texts",
    "abstract": "The web is full of guidance on a wide variety of tasks, from changing the oil\nin your car to baking an apple pie. However, as content is created\nindependently, a single task could have thousands of corresponding procedural\ntexts. This makes it difficult for users to view the bigger picture and\nunderstand the multiple ways the task could be accomplished. In this work we\npropose an unsupervised learning approach for summarizing multiple procedural\ntexts into an intuitive graph representation, allowing users to easily explore\ncommonalities and differences. We demonstrate our approach on recipes, a\nprominent example of procedural texts. User studies show that our\nrepresentation is intuitive and coherent and that it has the potential to help\nusers with several sensemaking tasks, including adapting recipes for a novice\ncook and finding creative ways to spice up a dish.",
    "descriptor": "\nComments: 11 pages, 6 figures, Accepted to CIKM 2021\n",
    "authors": [
      "Moran Mizrahi",
      "Dafna Shahaf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17235"
  },
  {
    "id": "arXiv:2210.17236",
    "title": "When Language Model Meets Private Library",
    "abstract": "With the rapid development of pre-training techniques, a number of language\nmodels have been pre-trained on large-scale code corpora and perform well in\ncode generation. In this paper, we investigate how to equip pre-trained\nlanguage models with the ability of code generation for private libraries. In\npractice, it is common for programmers to write code using private libraries.\nHowever, this is a challenge for language models since they have never seen\nprivate APIs during training. Motivated by the fact that private libraries\nusually come with elaborate API documentation, we propose a novel framework\nwith two modules: the APIRetriever finds useful APIs, and then the APICoder\ngenerates code using these APIs. For APIRetriever, we present a dense retrieval\nsystem and also design a friendly interaction to involve uses. For APICoder, we\ncan directly use off-the-shelf language models, or continually pre-train the\nbase model on a code corpus containing API information. Both modules are\ntrained with data from public libraries and can be generalized to private ones.\nFurthermore, we craft three benchmarks for private libraries, named\nTorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate\nthe impressive performance of our framework.",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Daoguang Zan",
      "Bei Chen",
      "Zeqi Lin",
      "Bei Guan",
      "Yongji Wang",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.17236"
  },
  {
    "id": "arXiv:2210.17238",
    "title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response  Selection Task",
    "abstract": "In retrieval-based dialogue systems, a response selection model acts as a\nranker to select the most appropriate response among several candidates.\nHowever, such selection models tend to rely on context-response content\nsimilarity, which makes models vulnerable to adversarial responses that are\nsemantically similar but not relevant to the dialogue context. Recent studies\nhave shown that leveraging these adversarial responses as negative training\nsamples is useful for improving the discriminating power of the selection\nmodel. Nevertheless, collecting human-written adversarial responses is\nexpensive, and existing synthesizing methods often have limited scalability. To\novercome these limitations, this paper proposes a simple but efficient method\nfor generating adversarial negative responses leveraging a large-scale language\nmodel. Experimental results on dialogue selection tasks show that our method\noutperforms other methods of synthesizing adversarial negative responses. These\nresults suggest that our method can be an effective alternative to human\nannotators in generating adversarial responses. Our dataset and generation code\nis available at https://github.com/leenw23/generating-negatives-by-gpt3.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Nyoungwoo Lee",
      "ChaeHun Park",
      "Ho-Jin Choi",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17238"
  },
  {
    "id": "arXiv:2210.17242",
    "title": "Analysis and numerical approximation of energy-variational solutions to  the Ericksen--Leslie equations",
    "abstract": "We define the concept of energy-variational solutions for the\nEricksen--Leslie equations in three spatial dimensions. This solution concept\nis finer than dissipative solutions and satisfies the weak-strong uniqueness\nproperty. For a certain choice of the regularity weight, the existence of\nenergy-variational solutions implies the existence of measure-valued solutions\nand for a different choice, we construct an energy-variational solution with\nthe help of an implementable, structure-inheriting space-time discretization.\nComputational studies are performed in order to provide some evidence of the\napplicability of the proposed algorithm.",
    "descriptor": "",
    "authors": [
      "Robert Lasarzik",
      "Maximilian E.V. Reiter"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.17242"
  },
  {
    "id": "arXiv:2210.17246",
    "title": "Tables to LaTeX: structure and content extraction from scientific tables",
    "abstract": "Scientific documents contain tables that list important information in a\nconcise fashion. Structure and content extraction from tables embedded within\nPDF research documents is a very challenging task due to the existence of\nvisual features like spanning cells and content features like mathematical\nsymbols and equations. Most existing table structure identification methods\ntend to ignore these academic writing features. In this paper, we adapt the\ntransformer-based language modeling paradigm for scientific table structure and\ncontent extraction. Specifically, the proposed model converts a tabular image\nto its corresponding LaTeX source code. Overall, we outperform the current\nstate-of-the-art baselines and achieve an exact match accuracy of 70.35 and\n49.69% on table structure and content extraction, respectively. Further\nanalysis demonstrates that the proposed models efficiently identify the number\nof rows and columns, the alphanumeric characters, the LaTeX tokens, and\nsymbols.",
    "descriptor": "\nComments: 10 pages, published in IJDAR'22. arXiv admin note: text overlap with arXiv:2105.14426\n",
    "authors": [
      "Pratik Kayal",
      "Mrinal Anand",
      "Harsh Desai",
      "Mayank Singh"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17246"
  },
  {
    "id": "arXiv:2210.17252",
    "title": "Multi-Camera Calibration Free BEV Representation for 3D Object Detection",
    "abstract": "In advanced paradigms of autonomous driving, learning Bird's Eye View (BEV)\nrepresentation from surrounding views is crucial for multi-task framework.\nHowever, existing methods based on depth estimation or camera-driven attention\nare not stable to obtain transformation under noisy camera parameters, mainly\nwith two challenges, accurate depth prediction and calibration. In this work,\nwe present a completely Multi-Camera Calibration Free Transformer (CFT) for\nrobust BEV representation, which focuses on exploring implicit mapping, not\nrelied on camera intrinsics and extrinsics. To guide better feature learning\nfrom image views to BEV, CFT mines potential 3D information in BEV via our\ndesigned position-aware enhancement (PA). Instead of camera-driven point-wise\nor global transformation, for interaction within more effective region and\nlower computation cost, we propose a view-aware attention which also reduces\nredundant computation and promotes converge. CFT achieves 49.7% NDS on the\nnuScenes detection task leaderboard, which is the first work removing camera\nparameters, comparable to other geometry-guided methods. Without temporal input\nand other modal information, CFT achieves second highest performance with a\nsmaller image input 1600 * 640. Thanks to view-attention variant, CFT reduces\nmemory and transformer FLOPs for vanilla attention by about 12% and 60%,\nrespectively, with improved NDS by 1.0%. Moreover, its natural robustness to\nnoisy camera parameters makes CFT more competitive.",
    "descriptor": "\nComments: 15 pages, 7 figures\n",
    "authors": [
      "Hongxiang Jiang",
      "Wenming Meng",
      "Hongmei Zhu",
      "Qian Zhang",
      "Jihao Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17252"
  },
  {
    "id": "arXiv:2210.17258",
    "title": "Teacher-Student Network for 3D Point Cloud Anomaly Detection with Few  Normal Samples",
    "abstract": "Anomaly detection, which is a critical and popular topic in computer vision,\naims to detect anomalous samples that are different from the normal (i.e.,\nnon-anomalous) ones. The current mainstream methods focus on anomaly detection\nfor images, whereas little attention has been paid to 3D point cloud. In this\npaper, drawing inspiration from the knowledge transfer ability of\nteacher-student architecture and the impressive feature extraction capability\nof recent neural networks, we design a teacher-student structured model for 3D\nanomaly detection. Specifically, we use feature space alignment, dimension\nzoom, and max pooling to extract the features of the point cloud and then\nminimize a multi-scale loss between the feature vectors produced by the teacher\nand the student networks. Moreover, our method only requires very few normal\nsamples to train the student network due to the teacher-student distillation\nmechanism. Once trained, the teacher-student network pair can be leveraged\njointly to fulfill 3D point cloud anomaly detection based on the calculated\nanomaly score. For evaluation, we compare our method against the\nreconstruction-based method on the ShapeNet-Part dataset. The experimental\nresults and ablation studies quantitatively and qualitatively confirm that our\nmodel can achieve higher performance compared with the state of the arts in 3D\nanomaly detection with very few training samples.",
    "descriptor": "",
    "authors": [
      "Jianjian Qin",
      "Chunzhi Gu",
      "Jun Yu",
      "Chao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17258"
  },
  {
    "id": "arXiv:2210.17260",
    "title": "Joint BS-RIS-User Association and Beamforming Design for RIS-assisted  Cellular Networks",
    "abstract": "Reconfigurable intelligent surface (RIS) is a revolutionary technology for\nsixth-generation (6G) networks owing to its ability to manipulate wireless\nenvironments. As a frequency-selective device, RIS can only effectively shape\nthe propagation of signals within a certain frequency band. Due to this\nfrequency-selective property, the deployment of RIS in cellular networks will\nintroduce a complicated base station (BS)-RIS-user association issue since\nadjacent BSs operate at different frequency bands. In this paper, with the\nconsideration of the frequency-selective characteristics of RIS, we aim to\njointly optimize BS-RIS-user association, active beamforming at BSs, and\npassive beamforming of RIS to maximize the sum-rate of a RIS-assisted cellular\nnetwork. We first leverage $l_0$-norm to efficiently integrate BS-RIS-user\nassociation with active and passive beamforming. Then, we adopt fractional\nprogramming (FP) and block coordinate descent (BCD) methods to deal with\nlogarithmic and fractional parts and decouple the joint association and\nbeamforming design problem into several sub-problems. Efficient algorithms\nwhich combine $l_0$-norm approximation, majorization-minimization (MM), and\nalternating direction method of multipliers (ADMM) are developed to alternately\nsolve the sub-problems. Extensive simulation results illustrate the importance\nof BS-RIS-user association optimization in RIS-assisted cellular networks and\nverify the effectiveness of the proposed joint association and beamforming\ndesign algorithm.",
    "descriptor": "\nComments: Submitted to IEEE Journal\n",
    "authors": [
      "Sifan Liu",
      "Rang Liu",
      "Ming Li",
      "Yang Liu",
      "Qian Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17260"
  },
  {
    "id": "arXiv:2210.17262",
    "title": "QNet: A Quantum-native Sequence Encoder Architecture",
    "abstract": "This work investigates how current quantum computers can improve the\nperformance of natural language processing tasks. To achieve this goal, we\nproposed QNet, a novel sequence encoder model entirely inferences on the\nquantum computer using a minimum number of qubits. QNet is inspired by\nTransformer, the state-of-the-art neural network model based on the attention\nmechanism to relate the tokens. While the attention mechanism requires time\ncomplexity of $O(n^2 \\cdot d)$ to perform matrix multiplication operations,\nQNet has merely $O(n+d)$ quantum circuit depth, where $n$ and $d$ represent the\nlength of the sequence and the embedding size, respectively. To employ QNet on\nthe NISQ devices, ResQNet, a quantum-classical hybrid model composed of several\nQNet blocks linked by residual connections, is introduced. We evaluate ResQNet\non various natural language processing tasks, including text classification,\nrating score prediction, and named entity recognition. ResQNet exhibits a 6% to\n818% performance gain on all these tasks over classical state-of-the-art models\nusing the exact embedding dimensions. In summary, this work demonstrates the\nadvantage of quantum computing in natural language processing tasks.",
    "descriptor": "",
    "authors": [
      "Wei Day",
      "Hao-Sheng Chen",
      "Min-Te Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17262"
  },
  {
    "id": "arXiv:2210.17264",
    "title": "Cross-lingual Text-To-Speech with Flow-based Voice Conversion for  Improved Pronunciation",
    "abstract": "This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Nikolaos Ellinas",
      "Georgios Vamvoukakis",
      "Konstantinos Markopoulos",
      "Georgia Maniati",
      "Panos Kakoulidis",
      "June Sig Sung",
      "Inchul Hwang",
      "Spyros Raptis",
      "Aimilios Chalamandaris",
      "Pirros Tsiakoulis"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17264"
  },
  {
    "id": "arXiv:2210.17272",
    "title": "Reinforcement Learning-based Defect Mitigation for Quality Assurance of  Additive Manufacturing",
    "abstract": "Additive Manufacturing (AM) is a powerful technology that produces complex 3D\ngeometries using various materials in a layer-by-layer fashion. However,\nquality assurance is the main challenge in AM industry due to the possible\ntime-varying processing conditions during AM process. Notably, new defects may\noccur during printing, which cannot be mitigated by offline analysis tools that\nfocus on existing defects. This challenge motivates this work to develop online\nlearning-based methods to deal with the new defects during printing. Since AM\ntypically fabricates a small number of customized products, this paper aims to\ncreate an online learning-based strategy to mitigate the new defects in AM\nprocess while minimizing the number of samples needed. The proposed method is\nbased on model-free Reinforcement Learning (RL). It is called Continual\nG-learning since it transfers several sources of prior knowledge to reduce the\nneeded training samples in the AM process. Offline knowledge is obtained from\nliterature, while online knowledge is learned during printing. The proposed\nmethod develops a new algorithm for learning the optimal defect mitigation\nstrategies proven the best performance when utilizing both knowledge sources.\nNumerical and real-world case studies in a fused filament fabrication (FFF)\nplatform are performed and demonstrate the effectiveness of the proposed\nmethod.",
    "descriptor": "",
    "authors": [
      "Jihoon Chung",
      "Bo Shen",
      "Andrew Chung Chee Law",
      "Zhenyu",
      "Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.17272"
  },
  {
    "id": "arXiv:2210.17274",
    "title": "Imbalanced Data Classification via Generative Adversarial Network with  Application to Anomaly Detection in Additive Manufacturing Process",
    "abstract": "Supervised classification methods have been widely utilized for the quality\nassurance of the advanced manufacturing process, such as additive manufacturing\n(AM) for anomaly (defects) detection. However, since abnormal states (with\ndefects) occur much less frequently than normal ones (without defects) in the\nmanufacturing process, the number of sensor data samples collected from a\nnormal state outweighs that from an abnormal state. This issue causes\nimbalanced training data for classification models, thus deteriorating the\nperformance of detecting abnormal states in the process. It is beneficial to\ngenerate effective artificial sample data for the abnormal states to make a\nmore balanced training set. To achieve this goal, this paper proposes a novel\ndata augmentation method based on a generative adversarial network (GAN) using\nadditive manufacturing process image sensor data. The novelty of our approach\nis that a standard GAN and classifier are jointly optimized with techniques to\nstabilize the learning process of standard GAN. The diverse and high-quality\ngenerated samples provide balanced training data to the classifier. The\niterative optimization between GAN and classifier provides the high-performance\nclassifier. The effectiveness of the proposed method is validated by both\nopen-source data and real-world case studies in polymer and metal AM processes.",
    "descriptor": "",
    "authors": [
      "Jihoon Chung",
      "Bo Shen",
      "Zhenyu",
      "Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.17274"
  },
  {
    "id": "arXiv:2210.17281",
    "title": "GNN at the Edge: Cost-Efficient Graph Neural Network Processing over  Distributed Edge Servers",
    "abstract": "Edge intelligence has arisen as a promising computing paradigm for supporting\nmiscellaneous smart applications that rely on machine learning techniques.\nWhile the community has extensively investigated multi-tier edge deployment for\ntraditional deep learning models (e.g. CNNs, RNNs), the emerging Graph Neural\nNetworks (GNNs) are still under exploration, presenting a stark disparity to\nits broad edge adoptions such as traffic flow forecasting and location-based\nsocial recommendation. To bridge this gap, this paper formally studies the cost\noptimization for distributed GNN processing over a multi-tier heterogeneous\nedge network. We build a comprehensive modeling framework that can capture a\nvariety of different cost factors, based on which we formulate a cost-efficient\ngraph layout optimization problem that is proved to be NP-hard. Instead of\ntrivially applying traditional data placement wisdom, we theoretically reveal\nthe structural property of quadratic submodularity implicated in GNN's unique\ncomputing pattern, which motivates our design of an efficient iterative\nsolution exploiting graph cuts. Rigorous analysis shows that it provides\nparameterized constant approximation ratio, guaranteed convergence, and exact\nfeasibility. To tackle potential graph topological evolution in GNN processing,\nwe further devise an incremental update strategy and an adaptive scheduling\nalgorithm for lightweight dynamic layout optimization. Evaluations with\nreal-world datasets and various GNN benchmarks demonstrate that our approach\nachieves superior performance over de facto baselines with more than 95.8% cost\neduction in a fast convergence speed.",
    "descriptor": "\nComments: 19 pages, 20 figures, accepted by IEEE Journal of Selected Areas in Communications\n",
    "authors": [
      "Liekang Zeng",
      "Chongyu Yang",
      "Peng Huang",
      "Zhi Zhou",
      "Shuai Yu",
      "Xu Chen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.17281"
  },
  {
    "id": "arXiv:2210.17283",
    "title": "CausalBench: A Large-scale Benchmark for Network Inference from  Single-cell Perturbation Data",
    "abstract": "Mapping biological mechanisms in cellular systems is a fundamental step in\nearly-stage drug discovery that serves to generate hypotheses on what\ndisease-relevant molecular targets may effectively be modulated by\npharmacological interventions. With the advent of high-throughput methods for\nmeasuring single-cell gene expression under genetic perturbations, we now have\neffective means for generating evidence for causal gene-gene interactions at\nscale. However, inferring graphical networks of the size typically encountered\nin real-world gene-gene interaction networks is difficult in terms of both\nachieving and evaluating faithfulness to the true underlying causal graph.\nMoreover, standardised benchmarks for comparing methods for causal discovery in\nperturbational single-cell data do not yet exist. Here, we introduce\nCausalBench - a comprehensive benchmark suite for evaluating network inference\nmethods on large-scale perturbational single-cell gene expression data.\nCausalBench introduces several biologically meaningful performance metrics and\noperates on two large, curated and openly available benchmark data sets for\nevaluating methods on the inference of gene regulatory networks from\nsingle-cell data generated under perturbations. With real-world datasets\nconsisting of over \\numprint{200000} training samples under interventions,\nCausalBench could potentially help facilitate advances in causal network\ninference by providing what is - to the best of our knowledge - the largest\nopenly available test bed for causal discovery from real-world perturbation\ndata to date.",
    "descriptor": "",
    "authors": [
      "Mathieu Chevalley",
      "Yusuf Roohani",
      "Arash Mehrjou",
      "Jure Leskovec",
      "Patrick Schwab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17283"
  },
  {
    "id": "arXiv:2210.17284",
    "title": "Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3",
    "abstract": "We present very early results on using GPT-3 to perform question answering on\ntabular data. We find that stock pre-trained GPT-3 is able to zero-shot learn\nthe table structure from a serialized JSON array-of-arrays representation, and\nable to answer lookup queries and simple comparison questions in natural\nlanguage without any fine-tuning. We further find that simple prompt\nengineering to include few-shot static Q&A examples significantly improves\naccuracy. Lastly, we find that intermixing passage text improves accuracy even\nfurther on heterogeneous data. We apply our approach on a novel dataset of\nsimple tables in newspaper infographics with promising results. Overall, we\nfind much cause for optimism in this basic approach.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Pragya Srivastava",
      "Tanuja Ganu",
      "Saikat Guha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17284"
  },
  {
    "id": "arXiv:2210.17289",
    "title": "Forecasting local behavior of multi-agent system and its application to  forest fire model",
    "abstract": "In this paper, we study a CNN-LSTM model to forecast the state of a specific\nagent in a large multi-agent system. The proposed model consists of a CNN\nencoder to represent the system into a low-dimensional vector, a LSTM module to\nlearn the agent dynamics in the vector space, and a MLP decoder to predict the\nfuture state of an agent. A forest fire model is considered as an example where\nwe need to predict when a specific tree agent will be burning. We observe that\nthe proposed model achieves higher AUC with less computation than a frame-based\nmodel and significantly saves computational costs such as the activation than\nConvLSTM.",
    "descriptor": "\nComments: submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n",
    "authors": [
      "Beomseok Kang",
      "Minah Lee",
      "Harshit Kumar",
      "Saibal Mukhopadhyay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17289"
  },
  {
    "id": "arXiv:2210.17291",
    "title": "SIX-Trust for 6G: Towards a Secure and Trustworthy 6G Network",
    "abstract": "Recent years have witnessed a digital explosion with the deployment of 5G and\nproliferation of 5G-enabled innovations. Compared with 5G, 6G is envisioned to\nachieve much higher performance in terms of latency, data rate, connectivity,\nenergy efficiency, coverage and mobility. To fulfil these expectations, 6G will\nexperience a number of paradigm shifts, such as exploiting new spectrum,\napplying ubiquitous ML/AI technologies and building a space-air-ground-sea\nintegrated network. However, these paradigm shifts may lead to numerous new\nsecurity and privacy issues, which traditional security measures may not be\nable to deal with. To tackle these issues and build a trustworthy 6G network,\nwe introduce a novel trust framework named as SIX-Trust, which composes of 3\nlayers: sustainable trust (S-Trust), infrastructure trust (I-Trust) and\nxenogenesis trust (X-Trust). Each layer plays a different role, and the\nimportance of each layer varies for different application scenarios of 6G. For\neach layer, we briefly introduce its related enabling technologies, and\ndemonstrate how these technologies can be applied to enhance trust and security\nof the 6G network. In general, SIX-Trust provides a holistic framework for\ndefining and modeling trust of 6G, which can facilitate establishing a\ntrustworthy 6G network.",
    "descriptor": "\nComments: 7 pages, 3 figures, under review\n",
    "authors": [
      "Yiying Wang",
      "Xin Kang",
      "Tieyan Li",
      "Haiguang Wang",
      "Cheng-Kang Chu",
      "Zhongding Lei"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.17291"
  },
  {
    "id": "arXiv:2210.17292",
    "title": "M$^3$Care: Learning with Missing Modalities in Multimodal Healthcare  Data",
    "abstract": "Multimodal electronic health record (EHR) data are widely used in clinical\napplications. Conventional methods usually assume that each sample (patient) is\nassociated with the unified observed modalities, and all modalities are\navailable for each sample. However, missing modality caused by various clinical\nand social reasons is a common issue in real-world clinical scenarios. Existing\nmethods mostly rely on solving a generative model that learns a mapping from\nthe latent space to the original input space, which is an unstable ill-posed\ninverse problem. To relieve the underdetermined system, we propose a model\nsolving a direct problem, dubbed learning with Missing Modalities in Multimodal\nhealthcare data (M3Care). M3Care is an end-to-end model compensating the\nmissing information of the patients with missing modalities to perform clinical\nanalysis. Instead of generating raw missing data, M3Care imputes the\ntask-related information of the missing modalities in the latent space by the\nauxiliary information from each patient's similar neighbors, measured by a\ntask-guided modality-adaptive similarity metric, and thence conducts the\nclinical tasks. The task-guided modality-adaptive similarity metric utilizes\nthe uncensored modalities of the patient and the other patients who also have\nthe same uncensored modalities to find similar patients. Experiments on\nreal-world datasets show that M3Care outperforms the state-of-the-art\nbaselines. Moreover, the findings discovered by M3Care are consistent with\nexperts and medical knowledge, demonstrating the capability and the potential\nof providing useful insights and explanations.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Chaohe Zhang",
      "Xu Chu",
      "Liantao Ma",
      "Yinghao Zhu",
      "Yasha Wang",
      "Jiangtao Wang",
      "Junfeng Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17292"
  },
  {
    "id": "arXiv:2210.17294",
    "title": "In-Flight Energy-Driven Composition of Drone Swarm Services",
    "abstract": "We propose a novel framework for swarm-based drone delivery services with\nin-flight energy recharging. The framework aims to enhance the delivery time of\nmultiple packages by reducing the number of stops and recharging times at\nintermediate stations. The proposed framework considers various intrinsic and\nextrinsic delivery constraints. We propose to use support drones whose sole\npurpose is to recharge other drones in the swarm during their flight. In this\nrespect, we compute the optimal set of optimal support drones to minimize the\nprobability of delivery services and recharging time at the next stations. We\nalso use two settings to position the support drones in a flight formation for\ncomparative purposes. Two novel energy sharing methods are proposed, namely,\nPriority-based and Fairness-based methods. A re-ordering method of the delivery\ndrones is presented to facilitate the in-flight energy composition process. An\nenhanced A* algorithm is implemented to compose the optimal services in terms\nof delivery time. Experimental results prove the efficiency of our proposed\napproach.",
    "descriptor": "\nComments: 15 pages, 12 figures. This is an accepted paper appearing in the IEEE Transactions on Services Computing (IEEE TSC)\n",
    "authors": [
      "Balsam Alkouz",
      "Amani Abusafia",
      "Abdallah Lakhdari",
      "Athman Bouguettaya"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.17294"
  },
  {
    "id": "arXiv:2210.17296",
    "title": "Using Contrastive Samples for Identifying and Leveraging Possible Causal  Relationships in Reinforcement Learning",
    "abstract": "A significant challenge in reinforcement learning is quantifying the complex\nrelationship between actions and long-term rewards. The effects may manifest\nthemselves over a long sequence of state-action pairs, making them hard to\npinpoint. In this paper, we propose a method to link transitions with\nsignificant deviations in state with unusually large variations in subsequent\nrewards. Such transitions are marked as possible causal effects, and the\ncorresponding state-action pairs are added to a separate replay buffer. In\naddition, we include \\textit{contrastive} samples corresponding to transitions\nfrom a similar state but with differing actions. Including this Contrastive\nExperience Replay (CER) during training is shown to outperform standard\nvalue-based methods on 2D navigation tasks. We believe that CER can be useful\nfor a broad class of learning tasks, including for any off-policy reinforcement\nlearning algorithm.",
    "descriptor": "",
    "authors": [
      "Harshad Khadilkar",
      "Hardik Meisheri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17296"
  },
  {
    "id": "arXiv:2210.17298",
    "title": "A Long-term Dependent and Trustworthy Approach to Reactor Accident  Prognosis based on Temporal Fusion Transformer",
    "abstract": "Prognosis of the reactor accident is a crucial way to ensure appropriate\nstrategies are adopted to avoid radioactive releases. However, there is very\nlimited research in the field of nuclear industry. In this paper, we propose a\nmethod for accident prognosis based on the Temporal Fusion Transformer (TFT)\nmodel with multi-headed self-attention and gating mechanisms. The method\nutilizes multiple covariates to improve prediction accuracy on the one hand,\nand quantile regression methods for uncertainty assessment on the other. The\nmethod proposed in this paper is applied to the prognosis after loss of coolant\naccidents (LOCAs) in HPR1000 reactor. Extensive experimental results show that\nthe method surpasses novel deep learning-based prediction methods in terms of\nprediction accuracy and confidence. Furthermore, the interference experiments\nwith different signal-to-noise ratios and the ablation experiments for static\ncovariates further illustrate that the robustness comes from the ability to\nextract the features of static and historical covariates. In summary, this work\nfor the first time applies the novel composite deep learning model TFT to the\nprognosis of key parameters after a reactor accident, and makes a positive\ncontribution to the establishment of a more intelligent and staff-light\nmaintenance method for reactor systems.",
    "descriptor": "",
    "authors": [
      "Chengyuan Li",
      "Zhifang Qiu",
      "Yugao Ma",
      "Meifu Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17298"
  },
  {
    "id": "arXiv:2210.17301",
    "title": "Effective Cross-Task Transfer Learning for Explainable Natural Language  Inference with T5",
    "abstract": "We compare sequential fine-tuning with a model for multi-task learning in the\ncontext where we are interested in boosting performance on two tasks, one of\nwhich depends on the other. We test these models on the FigLang2022 shared task\nwhich requires participants to predict language inference labels on figurative\nlanguage along with corresponding textual explanations of the inference\npredictions. Our results show that while sequential multi-task learning can be\ntuned to be good at the first of two target tasks, it performs less well on the\nsecond and additionally struggles with overfitting. Our findings show that\nsimple sequential fine-tuning of text-to-text models is an extraordinarily\npowerful method for cross-task knowledge transfer while simultaneously\npredicting multiple interdependent targets. So much so, that our best model\nachieved the (tied) highest score on the task.",
    "descriptor": "\nComments: Accepted for publication in the Proceedings of the Second Workshop on Figurative Language Processing (colocated with EMNLP 2022). Code and models at this https URL\n",
    "authors": [
      "Irina Bigoulaeva",
      "Rachneet Sachdeva",
      "Harish Tayyar Madabushi",
      "Aline Villavicencio",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17301"
  },
  {
    "id": "arXiv:2210.17302",
    "title": "Design, Field Evaluation, and Traffic Analysis of a Competitive  Autonomous Driving Model in the a Congested Environment",
    "abstract": "Recently, numerous studies have investigated cooperative traffic systems\nusing the communication between vehicle-to-everything (V2X), which includes\nboth vehicle-to-vehicle and vehicle-to-infrastructures. Unfortunately, if\ncooperative driving using V2X communication is disabled, there can be a\nconflict of optimal conditions between various autonomous vehicles. This study\nassumes a rather pessimistic approach for the transportation system, that is,\nracing in an urban environment. In South Korea, virtual and live urban\nautonomous multi-vehicle races were held in March and November of 2021,\nrespectively. In these competitions, each car drove in the congested urban\nenvironment while minimizing the transversal time and obeying traffic laws. In\nthis study, we propose a full autonomous driving software stack to deploy a\ncompetitive driving model covering module-wise autonomous driving modules.\nAfter developing the module-level navigation, perception, and planning systems\nfor the autonomous vehicle, we performed a traffic analysis. Finally, we\nvalidated the proposed system at the module level. In addition, we analyzed a\nmodel consisting of competitive driving models to determine the similarity of\neach team's driving log data.",
    "descriptor": "",
    "authors": [
      "Daegyu Lee",
      "Hyunki Seong",
      "Seungil Han",
      "Gyuree Kang",
      "D.Hyunchul Shim",
      "Yoonjin Yoon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17302"
  },
  {
    "id": "arXiv:2210.17305",
    "title": "AccentSpeech: Learning Accent from Crowd-sourced Data for Target Speaker  TTS with Accents",
    "abstract": "Learning accent from crowd-sourced data is a feasible way to achieve a target\nspeaker TTS system that can synthesize accent speech. To this end, there are\ntwo challenging problems to be solved. First, direct use of the poor acoustic\nquality crowd-sourced data and the target speaker data in accent transfer will\napparently lead to synthetic speech with degraded quality. To mitigate this\nproblem, we take a bottleneck feature (BN) based TTS approach, in which TTS is\ndecomposed into a Text-to-BN (T2BN) module to learn accent and a BN-to-Mel\n(BN2Mel) module to learn speaker timbre, where neural network based BN feature\nserves as the intermediate representation that are robust to noise\ninterference. Second, direct training T2BN using the crowd-sourced data in the\ntwo-stage system will produce accent speech of target speaker with poor\nprosody. This is because the the crowd-sourced recordings are contributed from\nthe ordinary unprofessional speakers. To tackle this problem, we update the\ntwo-stage approach to a novel three-stage approach, where T2BN and BN2Mel are\ntrained using the high-quality target speaker data and a new BN-to-BN module is\nplugged in between the two modules to perform accent transfer. To train the\nBN2BN module, the parallel unaccented and accented BN features are obtained by\na proposed data augmentation procedure. Finally the proposed three-stage\napproach manages to produce accent speech for the target speaker with good\nprosody, as the prosody pattern is inherited from the professional target\nspeaker and accent transfer is achieved by the BN2BN module at the same time.\nThe proposed approach, named as AccentSpeech, is validated in a Mandarin TTS\naccent transfer task.",
    "descriptor": "\nComments: Accepted by ISCSLP2022\n",
    "authors": [
      "Yongmao Zhang",
      "Zhichao Wang",
      "Peiji Yang",
      "Hongshen Sun",
      "Zhisheng Wang",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17305"
  },
  {
    "id": "arXiv:2210.17309",
    "title": "Spontaneous emergence of groups and signaling diversity in dynamic  networks",
    "abstract": "Sending and receiving signals is ubiquitous in the living world. It includes\neverything from individual molecules triggering complex metabolic cascades, to\nanimals using signals to alert their group to the presence of predators. When\ncommunication involves common interest, simple sender-receiver games show how\nreliable signaling can emerge and evolve to transmit information effectively.\nThese games have been analyzed extensively, with some work investigating the\nrole of static network structure on information transfer. However, no existing\nwork has examined the coevolution of strategy and network structure in\nsender-receiver games. Here we show that coevolution is sufficient to generate\nthe endogenous formation of distinct groups from an initially homogeneous\npopulation. It also allows for the emergence of novel ``hybrid'' signaling\ngroups that have not previously been considered or demonstrated in theory or\nnature. Hybrid groups are composed of different complementary signaling\nbehaviors that rely on evolved network structure to achieve effective\ncommunication. Without this structure, such groups would normally fail to\neffectively communicate. Our findings pertain to all common interest signaling\ngames, are robust across many parameters, and mitigate known problems of\ninefficient communication. Our work generates new insights for the theory of\nadaptive behavior, signaling, and group formation in natural and social systems\nacross a wide range of environments in which changing network structure is\ncommon. We discuss implications for research on metabolic networks, among\nneurons, proteins, and social organisms.",
    "descriptor": "",
    "authors": [
      "Zachary Fulker",
      "Patrick Forber",
      "Rory Smead",
      "Christoph Riedl"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Theoretical Economics (econ.TH)",
      "Physics and Society (physics.soc-ph)",
      "Molecular Networks (q-bio.MN)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2210.17309"
  },
  {
    "id": "arXiv:2210.17311",
    "title": "Shared Manifold Learning Using a Triplet Network for Multiple Sensor  Translation and Fusion with Missing Data",
    "abstract": "Heterogeneous data fusion can enhance the robustness and accuracy of an\nalgorithm on a given task. However, due to the difference in various\nmodalities, aligning the sensors and embedding their information into\ndiscriminative and compact representations is challenging. In this paper, we\npropose a Contrastive learning based MultiModal Alignment Network (CoMMANet) to\nalign data from different sensors into a shared and discriminative manifold\nwhere class information is preserved. The proposed architecture uses a\nmultimodal triplet autoencoder to cluster the latent space in such a way that\nsamples of the same classes from each heterogeneous modality are mapped close\nto each other. Since all the modalities exist in a shared manifold, a unified\nclassification framework is proposed. The resulting latent space\nrepresentations are fused to perform more robust and accurate classification.\nIn a missing sensor scenario, the latent space of one sensor is easily and\nefficiently predicted using another sensor's latent space, thereby allowing\nsensor translation. We conducted extensive experiments on a manually labeled\nmultimodal dataset containing hyperspectral data from AVIRIS-NG and NEON, and\nLiDAR (light detection and ranging) data from NEON. Lastly, the model is\nvalidated on two benchmark datasets: Berlin Dataset (hyperspectral and\nsynthetic aperture radar) and MUUFL Gulfport Dataset (hyperspectral and LiDAR).\nA comparison made with other methods demonstrates the superiority of this\nmethod. We achieved a mean overall accuracy of 94.3% on the MUUFL dataset and\nthe best overall accuracy of 71.26% on the Berlin dataset, which is better than\nother state-of-the-art approaches.",
    "descriptor": "\nComments: 19 pages, 16 figures; Accepted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\n",
    "authors": [
      "Aditya Dutt",
      "Alina Zare",
      "Paul Gader"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17311"
  },
  {
    "id": "arXiv:2210.17312",
    "title": "Training Neural Networks for Sequential Change-point Detection",
    "abstract": "Detecting an abrupt distributional shift of the data stream, known as\nchange-point detection, is a fundamental problem in statistics and signal\nprocessing. We present a new approach for online change-point detection by\ntraining neural networks (NN), and sequentially cumulating the detection\nstatistics by evaluating the trained discriminating function on test samples by\na CUSUM recursion. The idea is based on the observation that training neural\nnetworks through logistic loss may lead to the log-likelihood function. We\ndemonstrated the good performance of NN-CUSUM on detecting change-point in\nhigh-dimensional data using both synthetic and real-world data.",
    "descriptor": "",
    "authors": [
      "Junghwan Lee",
      "Yao Xie",
      "Xiuyuan Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17312"
  },
  {
    "id": "arXiv:2210.17313",
    "title": "DiscreteCommunication and ControlUpdating in Event-Triggered Consensus",
    "abstract": "This paper studies the consensus control problem faced with three essential\ndemands, namely, discrete control updating for each agent, discrete-time\ncommunications among neighboring agents, and the fully distributed fashion of\nthe controller implementation without requiring any global information of the\nwhole network topology. Noting that the existing related results only meeting\none or two demands at most are essentially not applicable, in this paper we\nestablish a novel framework to solve the problem of fully distributed consensus\nwith discrete communication and control. The first key point in this framework\nis the design of controllers that are only updated at discrete event instants\nand do not depend on global information by introducing time-varying gains\ninspired by the adaptive control technique. Another key point is the invention\nof novel dynamic triggering functions that are independent of relative\ninformation among neighboring agents. Under the established framework, we\npropose fully distributed state-feedback event-triggered protocols for\nundirected graphs and also further study the more complexed cases of\noutput-feedback control and directed graphs. Finally, numerical examples are\nprovided to verify the effectiveness of the proposed event-triggered protocols.",
    "descriptor": "",
    "authors": [
      "Bin Cheng",
      "Yuezu Lv",
      "Zhongkui Li",
      "Zhisheng Duan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.17313"
  },
  {
    "id": "arXiv:2210.17314",
    "title": "A deep scalable neural architecture for soil properties estimation from  spectral information",
    "abstract": "In this paper we propose an adaptive deep neural architecture for the\nprediction of multiple soil characteristics from the analysis of hyperspectral\nsignatures. The proposed method overcomes the limitations of previous methods\nin the state of art: (i) it allows to predict multiple soil variables at once;\n(ii) it permits to backtrace the spectral bands that most contribute to the\nestimation of a given variable; (iii) it is based on a flexible neural\narchitecture capable of automatically adapting to the spectral library under\nanalysis. The proposed architecture is experimented on LUCAS, a large\nlaboratory dataset and on a dataset achieved by simulating PRISMA hyperspectral\nsensor. 'Results, compared with other state-of-the-art methods confirm the\neffectiveness of the proposed solution.",
    "descriptor": "\nComments: 14 pages + 13 of appendix. Journal paper\n",
    "authors": [
      "Flavio Piccoli",
      "Micol Rossini",
      "Roberto Colombo",
      "Raimondo Schettini",
      "Paolo Napoletano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17314"
  },
  {
    "id": "arXiv:2210.17321",
    "title": "Dominator Coloring Parameterized by Cluster Vertex Deletion Number",
    "abstract": "The Dominator Coloring (DC) problem borrows properties of two classical\nproblems in graph theory - Graph Coloring and Dominating Set. A dominator\ncoloring $\\chi_d$ of a graph $G$ is a proper coloring of its vertices such that\neach vertex dominates a color class - that is, for each $v \\in V(G)$, there\nexists a color $c$ such that $\\emptyset \\subset \\chi^{-1}_d(c) \\subseteq\nN_G[v]$. Given a graph $G$ and a natural number $\\ell$, DC asks if there is a\ndominator coloring of $G$ which uses at most $\\ell$-many colors. DC, which was\nfirst described in 2006 and studied in several papers since then, still hosts\nseveral important open questions. While it is known that DC is FPT when\nparameterized by $(t,\\ell)$ where $\\ell$ is the number of colors used and $t$\nthe treewidth of $G$, the structural parameterized landscape of the problem\nremains unexplored. We initiate the study of DC through the lens of structural\nparameterization.\nOur first result in this paper is a randomized $O^*(c^k)$ algorithm for DC\nwhere $c$ is a constant and $k$ is the size of a graph's Clique Modulator, a\nset of vertices whose deletion results in a clique. This algorithm is obtained\nby a non-trivial adaptation of the recent work by Gutin et al. for List\nColoring (LC) parameterized by the clique modulator that uses an\ninclusion-exclusion based polynomial sieving technique, and in addition uses a\nDP based exact algorithm we develop for DC. Later, we prove the main result of\nthe paper - DC is FPT when parameterized by the size of a graph's Cluster\nVertex Deletion (CVD) set; in contrast to the W[i]-hardness result for LC\nparameterized by the CVD set size. En route, we design a simpler and faster\ndeterministic FPT algorithm when the problem is parameterized by the size of a\ngraph's Twin Cover. We believe that this algorithm's approach, which uses a\nrelationship between DC and LC that we establish, is of independent interest.",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Aritra Banik",
      "Prahlad Narasimhan Kasthurirangan",
      "Venkatesh Raman"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.17321"
  },
  {
    "id": "arXiv:2210.17322",
    "title": "Generative Negative Text Replay for Continual Vision-Language  Pretraining",
    "abstract": "Vision-language pre-training (VLP) has attracted increasing attention\nrecently. With a large amount of image-text pairs, VLP models trained with\ncontrastive loss have achieved impressive performance in various tasks,\nespecially the zero-shot generalization on downstream datasets. In practical\napplications, however, massive data are usually collected in a streaming\nfashion, requiring VLP models to continuously integrate novel knowledge from\nincoming data and retain learned knowledge. In this work, we focus on learning\na VLP model with sequential chunks of image-text pair data. To tackle the\ncatastrophic forgetting issue in this multi-modal continual learning setting,\nwe first introduce pseudo text replay that generates hard negative texts\nconditioned on the training images in memory, which not only better preserves\nlearned knowledge but also improves the diversity of negative samples in the\ncontrastive loss. Moreover, we propose multi-modal knowledge distillation\nbetween images and texts to align the instance-wise prediction between old and\nnew models. We incrementally pre-train our model on both the instance and class\nincremental splits of the Conceptual Caption dataset, and evaluate the model on\nzero-shot image classification and image-text retrieval tasks. Our method\nconsistently outperforms the existing baselines with a large margin, which\ndemonstrates its superiority. Notably, we realize an average performance boost\nof $4.60\\%$ on image-classification downstream datasets for the class\nincremental split.",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Shipeng Yan",
      "Lanqing Hong",
      "Hang Xu",
      "Jianhua Han",
      "Tinne Tuytelaars",
      "Zhenguo Li",
      "Xuming He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17322"
  },
  {
    "id": "arXiv:2210.17323",
    "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained  Transformers",
    "abstract": "Generative Pre-trained Transformer (GPT) models set themselves apart through\nbreakthrough performance across complex language modelling tasks, but also by\ntheir extremely high computational and storage costs. Specifically, due to\ntheir massive size, even inference for large, highly-accurate GPT models may\nrequire multiple performant GPUs to execute, which limits the usability of such\nmodels. While there is emerging work on relieving this pressure via model\ncompression, the applicability and performance of existing compression\ntechniques is limited by the scale and complexity of GPT models. In this paper,\nwe address this challenge, and propose GPTQ, a new one-shot weight quantization\nmethod based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU. We show experimentally that these\nimprovements can be leveraged for end-to-end inference speedups over FP16, of\naround 2x when using high-end GPUs (NVIDIA A100) and 4x when using more\ncost-effective ones (NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq.",
    "descriptor": "",
    "authors": [
      "Elias Frantar",
      "Saleh Ashkboos",
      "Torsten Hoefler",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17323"
  },
  {
    "id": "arXiv:2210.17325",
    "title": "Real-time Mapping of Physical Scene Properties with an Autonomous Robot  Experimenter",
    "abstract": "Neural fields can be trained from scratch to represent the shape and\nappearance of 3D scenes efficiently. It has also been shown that they can\ndensely map correlated properties such as semantics, via sparse interactions\nfrom a human labeller. In this work, we show that a robot can densely annotate\na scene with arbitrary discrete or continuous physical properties via its own\nfully-autonomous experimental interactions, as it simultaneously scans and maps\nit with an RGB-D camera. A variety of scene interactions are possible,\nincluding poking with force sensing to determine rigidity, measuring local\nmaterial type with single-pixel spectroscopy or predicting force distributions\nby pushing. Sparse experimental interactions are guided by entropy to enable\nhigh efficiency, with tabletop scene properties densely mapped from scratch in\na few minutes from a few tens of interactions.",
    "descriptor": "",
    "authors": [
      "Iain Haughton",
      "Edgar Sucar",
      "Andre Mouton",
      "Edward Johns",
      "Andrew J. Davison"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17325"
  },
  {
    "id": "arXiv:2210.17329",
    "title": "Uncertainty quantification for random domains using periodic random  variables",
    "abstract": "We consider uncertainty quantification for the Poisson problem subject to\ndomain uncertainty. For the stochastic parameterization of the random domain,\nwe use the model recently introduced by Kaarnioja, Kuo, and Sloan (SIAM J.\nNumer. Anal., 2020) in which a countably infinite number of independent random\nvariables enter the random field as periodic functions. We develop lattice\nquasi-Monte Carlo (QMC) cubature rules for computing the expected value of the\nsolution to the Poisson problem subject to domain uncertainty. These QMC rules\ncan be shown to exhibit higher order cubature convergence rates permitted by\nthe periodic setting independently of the stochastic dimension of the problem.\nIn addition, we present a complete error analysis for the problem by taking\ninto account the approximation errors incurred by truncating the input random\nfield to a finite number of terms and discretizing the spatial domain using\nfinite elements. The paper concludes with numerical experiments demonstrating\nthe theoretical error estimates.",
    "descriptor": "\nComments: 38 pages, 3 figures\n",
    "authors": [
      "Harri Hakula",
      "Helmut Harbrecht",
      "Vesa Kaarnioja",
      "Frances Y. Kuo",
      "Ian H. Sloan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17329"
  },
  {
    "id": "arXiv:2210.17330",
    "title": "Flexible categorization for auditing using formal concept analysis and  Dempster-Shafer theory",
    "abstract": "Categorization of business processes is an important part of auditing. Large\namounts of transnational data in auditing can be represented as transactions\nbetween financial accounts using weighted bipartite graphs. We view such\nbipartite graphs as many-valued formal contexts, which we use to obtain\nexplainable categorization of these business processes in terms of financial\naccounts involved in a business process by using methods in formal concept\nanalysis. The specific explainability feature of the methodology introduced in\nthe present paper provides several advantages over e.g.~non-explainable machine\nlearning techniques, and in fact, it can be taken as a basis for the\ndevelopment of algorithms which perform the task of clustering on transparent\nand accountable principles. Here, we focus on obtaining and studying different\nways to categorize according to different extents of interest in different\nfinancial accounts, or interrogative agendas, of various agents or sub-tasks in\naudit. We use Dempster-Shafer mass functions to represent agendas showing\ndifferent interest in different set of financial accounts. We propose two new\nmethods to obtain categorizations from these agendas. We also model some\npossible deliberation scenarios between agents with different interrogative\nagendas to reach an aggregated agenda and categorization. The framework\ndeveloped in this paper provides a formal ground to obtain and study\nexplainable categorizations from the data represented as bipartite graphs\naccording to the agendas of different agents in an organization (e.g.~an audit\nfirm), and interaction between these through deliberation.",
    "descriptor": "",
    "authors": [
      "Marcel Boersma",
      "Krishna Manoorkar",
      "Alessandra Palmigiano",
      "Mattia Panettiere",
      "Apostolos Tzimoulis",
      "Nachoem Wijnberg"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.17330"
  },
  {
    "id": "arXiv:2210.17331",
    "title": "Trends in Energy Estimates for Computing in AI/Machine Learning  Accelerators, Supercomputers, and Compute-Intensive Applications",
    "abstract": "We examine the computational energy requirements of different systems driven\nby the geometrical scaling law, and increasing use of Artificial Intelligence\nor Machine Learning (AI-ML) over the last decade. With more scientific and\ntechnology applications based on data-driven discovery, machine learning\nmethods, especially deep neural networks, have become widely used. In order to\nenable such applications, both hardware accelerators and advanced AI-ML methods\nhave led to the introduction of new architectures, system designs, algorithms,\nand software. Our analysis of energy trends indicates three important\nobservations: 1) Energy efficiency due to geometrical scaling is slowing down;\n2) The energy efficiency at the bit-level does not translate into efficiency at\nthe instruction-level, or at the system-level for a variety of systems,\nespecially for large-scale AI-ML accelerators or supercomputers; 3) At the\napplication level, general-purpose AI-ML methods can be computationally energy\nintensive, off-setting the gains in energy from geometrical scaling and special\npurpose accelerators. Further, our analysis provides specific pointers for\nintegrating energy efficiency with performance analysis for enabling\nhigh-performance and sustainable computing in the future.",
    "descriptor": "\nComments: 8 pages, 9 figures, Submitted to Proceedings of IEEE Conference on High Performance Extreme Computing (HPEC) 2022\n",
    "authors": [
      "Sadasivan Shankar",
      "Albert Reuther"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.17331"
  },
  {
    "id": "arXiv:2210.17332",
    "title": "Teacher-Student Architecture for Knowledge Learning: A Survey",
    "abstract": "Although Deep Neural Networks (DNNs) have shown a strong capacity to solve\nlarge-scale problems in many areas, such DNNs with voluminous parameters are\nhard to be deployed in a real-time system. To tackle this issue,\nTeacher-Student architectures were first utilized in knowledge distillation,\nwhere simple student networks can achieve comparable performance to deep\nteacher networks. Recently, Teacher-Student architectures have been effectively\nand widely embraced on various knowledge learning objectives, including\nknowledge distillation, knowledge expansion, knowledge adaption, and multi-task\nlearning. With the help of Teacher-Student architectures, current studies are\nable to achieve multiple knowledge-learning objectives through lightweight and\neffective student networks. Different from the existing knowledge distillation\nsurveys, this survey detailedly discusses Teacher-Student architectures with\nmultiple knowledge learning objectives. In addition, we systematically\nintroduce the knowledge construction and optimization process during the\nknowledge learning and then analyze various Teacher-Student architectures and\neffective learning schemes that have been leveraged to learn representative and\nrobust knowledge. This paper also summarizes the latest applications of\nTeacher-Student architectures based on different purposes (i.e.,\nclassification, recognition, and generation). Finally, the potential research\ndirections of knowledge learning are investigated on the Teacher-Student\narchitecture design, the quality of knowledge, and the theoretical studies of\nregression-based learning, respectively. With this comprehensive survey, both\nindustry practitioners and the academic community can learn insightful\nguidelines about Teacher-Student architectures on multiple knowledge learning\nobjectives.",
    "descriptor": "",
    "authors": [
      "Chengming Hu",
      "Xuan Li",
      "Dan Liu",
      "Xi Chen",
      "Ju Wang",
      "Xue Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17332"
  },
  {
    "id": "arXiv:2210.17335",
    "title": "Polymorphic Typestate for Session Types",
    "abstract": "Session types provide a principled approach to typed communication protocols\nthat guarantee type safety and protocol fidelity. Formalizations of\nsession-typed communication are typically based on process calculi, concurrent\nlambda calculi, or linear logic. An alternative model based on\ncontext-sensitive typing and typestate has not received much attention due to\nits apparent restrictions. However, this model is attractive because it does\nnot force programmers into particular patterns like continuation-passing style\nor channel-passing style, but rather enables them to treat communication\nchannels like mutable variables. Polymorphic typestate is the key that enables\na full treatment of session-typed communication. Previous work in this\ndirection was hampered by its setting in a simply-typed lambda calculus. We\nshow that higher-order polymorphism and existential types enable us to lift the\nrestrictions imposed by the previous work, thus bringing the expressivity of\nthe typestate-based approach on par with the competition. On this basis, we\ndefine PolyVGR, the system of polymorphic typestate for session types,\nestablish its basic metatheory, type preservation and progress, and present a\nprototype implementation.",
    "descriptor": "\nComments: 45 papges in submission\n",
    "authors": [
      "Hannes Saffrich",
      "Peter Thiemann"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.17335"
  },
  {
    "id": "arXiv:2210.17341",
    "title": "HARRIS: Hybrid Ranking and Regression Forests for Algorithm Selection",
    "abstract": "It is well known that different algorithms perform differently well on an\ninstance of an algorithmic problem, motivating algorithm selection (AS): Given\nan instance of an algorithmic problem, which is the most suitable algorithm to\nsolve it? As such, the AS problem has received considerable attention resulting\nin various approaches - many of which either solve a regression or ranking\nproblem under the hood. Although both of these formulations yield very natural\nways to tackle AS, they have considerable weaknesses. On the one hand,\ncorrectly predicting the performance of an algorithm on an instance is a\nsufficient, but not a necessary condition to produce a correct ranking over\nalgorithms and in particular ranking the best algorithm first. On the other\nhand, classical ranking approaches often do not account for concrete\nperformance values available in the training data, but only leverage rankings\ncomposed from such data. We propose HARRIS- Hybrid rAnking and RegRessIon\nforeSts - a new algorithm selector leveraging special forests, combining the\nstrengths of both approaches while alleviating their weaknesses. HARRIS'\ndecisions are based on a forest model, whose trees are created based on splits\noptimized on a hybrid ranking and regression loss function. As our preliminary\nexperimental study on ASLib shows, HARRIS improves over standard algorithm\nselection approaches on some scenarios showing that combining ranking and\nregression in trees is indeed promising for AS.",
    "descriptor": "\nComments: 4 pages, 4 figures, 2 tables\n",
    "authors": [
      "Lukas Fehring",
      "Jonas Hanselle",
      "Alexander Tornede"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17341"
  },
  {
    "id": "arXiv:2210.17344",
    "title": "gCoRF: Generative Compositional Radiance Fields",
    "abstract": "3D generative models of objects enable photorealistic image synthesis with 3D\ncontrol. Existing methods model the scene as a global scene representation,\nignoring the compositional aspect of the scene. Compositional reasoning can\nenable a wide variety of editing applications, in addition to enabling\ngeneralizable 3D reasoning. In this paper, we present a compositional\ngenerative model, where each semantic part of the object is represented as an\nindependent 3D representation learned from only in-the-wild 2D data. We start\nwith a global generative model (GAN) and learn to decompose it into different\nsemantic parts using supervision from 2D segmentation masks. We then learn to\ncomposite independently sampled parts in order to create coherent global\nscenes. Different parts can be independently sampled while keeping the rest of\nthe object fixed. We evaluate our method on a wide variety of objects and parts\nand demonstrate editing applications.",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Mallikarjun BR",
      "Ayush Tewari",
      "Xingang Pan",
      "Mohamed Elgharib",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17344"
  },
  {
    "id": "arXiv:2210.17349",
    "title": "Robust MelGAN: A robust universal neural vocoder for high-fidelity TTS",
    "abstract": "In current two-stage neural text-to-speech (TTS) paradigm, it is ideal to\nhave a universal neural vocoder, once trained, which is robust to imperfect\nmel-spectrogram predicted from the acoustic model. To this end, we propose\nRobust MelGAN vocoder by solving the original multi-band MelGAN's metallic\nsound problem and increasing its generalization ability. Specifically, we\nintroduce a fine-grained network dropout strategy to the generator. With a\nspecifically designed over-smooth handler which separates speech signal intro\nperiodic and aperiodic components, we only perform network dropout to the\naperodic components, which alleviates metallic sounding and maintains good\nspeaker similarity. To further improve generalization ability, we introduce\nseveral data augmentation methods to augment fake data in the discriminator,\nincluding harmonic shift, harmonic noise and phase noise. Experiments show that\nRobust MelGAN can be used as a universal vocoder, significantly improving sound\nquality in TTS systems built on various types of data.",
    "descriptor": "",
    "authors": [
      "Kun Song",
      "Jian Cong",
      "Yongmao Zhang",
      "Lei Xie",
      "Ning Jiang",
      "Haiying Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17349"
  },
  {
    "id": "arXiv:2210.17356",
    "title": "IoT System Case Study: Personal Office Energy Monitor (POEM)",
    "abstract": "This paper describes the design, implementation, and user evaluation of an\nIoT project focused on monitoring and management of user comfort and energy\nusage in office buildings. The objective is to depict an instructive use case\nand to illustrate experiences with all major phases of designing and running a\nfairly complex IoT system. The design part includes motivation and outline of\nthe problem statement, the resulting definition of data to be collected, system\nimplementation, and subsequent changes resulting from the additional insights\nthat it provided. The user experience part describes quantitative findings as\nwell as key results of the extensive human factors study with over 70 office\nusers participating in two major pilots in France and Japan. The original idea\nfor this project came out from a diverse group of companies exploring\nchallenges of designing and operating smart buildings with net-positive energy\nbalance. Members included companies involved in the design and construction of\nsmart buildings, building-management and automation systems, computer design,\nenergy systems, and office furniture and space design. One of the early\ninsights was that maximum energy efficiency in office buildings cannot be\nachieved and sustained without the awareness and active participation of\nbuilding occupants. The resulting project explored and evaluated several ways\nto engage and empower users in ways that benefit them and makes them the\nwilling and active participants.",
    "descriptor": "\nComments: 20 pages, 8 figures, 1 table\n",
    "authors": [
      "Milan Milenkovic"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17356"
  },
  {
    "id": "arXiv:2210.17357",
    "title": "L-GreCo: An Efficient and General Framework for Layerwise-Adaptive  Gradient Compression",
    "abstract": "Data-parallel distributed training of deep neural networks (DNN) has gained\nvery widespread adoption, but can still experience communication bottlenecks\ndue to gradient transmission. To address this issue, entire families of lossy\ngradient compression mechanisms have been developed, including quantization,\nsparsification, and low-rank approximation, some of which are seeing\nsignificant practical adoption. Despite this progress, almost all known\ncompression schemes apply compression uniformly across DNN layers, although\nlayers are heterogeneous in terms of parameter count and their impact on model\naccuracy. In this work, we provide a general framework for adapting the degree\nof compression across the model's layers dynamically during training,\nsignificantly improving the overall compression without sacrificing accuracy.\nOur framework, called L-GreCo, is based on an efficient adaptive algorithm,\nwhich automatically picks the optimal compression parameters for model layers\nguaranteeing the best compression ratio while respecting a\ntheoretically-justified error constraint. Our extensive experimental study over\nimage classification and language modeling tasks shows that L-GreCo is\neffective across all three compression families, and achieves up to 2.5$\\times$\ntraining speedup and up to 5$\\times$ compression improvement over efficient\nimplementations of standard approaches while recovering full accuracy.\nMoreover, we show that L-GreCo is complementary to existing adaptive algorithms\nimproving their compression ratio by 50% and practical throughput by 66%.",
    "descriptor": "",
    "authors": [
      "Mohammadreza Alimohammadi",
      "Ilia Markov",
      "Elias Frantar",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.17357"
  },
  {
    "id": "arXiv:2210.17358",
    "title": "A Faster Sampler for Discrete Determinantal Point Processes",
    "abstract": "Discrete Determinantal Point Processes (DPPs) have a wide array of potential\napplications for subsampling datasets. They are however held back in some cases\nby the high cost of sampling. In the worst-case scenario, the sampling cost\nscales as $O(n^3)$ where n is the number of elements of the ground set. A\npopular workaround to this prohibitive cost is to sample DPPs defined by\nlow-rank kernels. In such cases, the cost of standard sampling algorithms\nscales as $O(np^2 + nm^2)$ where m is the (average) number of samples of the\nDPP (usually $m \\ll n$) and p ($m \\leq p \\leq n$) the rank of the kernel used\nto define the DPP. The first term, $O(np^2)$, comes from a SVD-like step. We\nfocus here on the second term of this cost, $O(nm^2)$, and show that it can be\nbrought down to $O(nm + m^3 log m)$ without loss on the sampling's exactness.\nIn practice, we observe extremely substantial speedups compared to the\nclassical algorithm as soon as $n > 1, 000$. The algorithm described here is a\nclose variant of the standard algorithm for sampling continuous DPPs, and uses\nrejection sampling. In the specific case of projection DPPs, we also show that\nany additional sample can be drawn in time $O(m^3 log m)$. Finally, an\ninteresting by-product of the analysis is that a realisation from a DPP is\ntypically contained in a subset of size $O(m log m)$ formed using leverage\nscore i.i.d. sampling.",
    "descriptor": "",
    "authors": [
      "Simon Barthelm\u00e9",
      "Nicolas Tremblay",
      "Pierre-Olivier Amblard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17358"
  },
  {
    "id": "arXiv:2210.17360",
    "title": "Explainable Deep Learning to Profile Mitochondrial Disease Using High  Dimensional Protein Expression Data",
    "abstract": "Mitochondrial diseases are currently untreatable due to our limited\nunderstanding of their pathology. We study the expression of various\nmitochondrial proteins in skeletal myofibres (SM) in order to discover\nprocesses involved in mitochondrial pathology using Imaging Mass Cytometry\n(IMC). IMC produces high dimensional multichannel pseudo-images representing\nspatial variation in the expression of a panel of proteins within a tissue,\nincluding subcellular variation. Statistical analysis of these images requires\nsemi-automated annotation of thousands of SMs in IMC images of patient muscle\nbiopsies. In this paper we investigate the use of deep learning (DL) on raw IMC\ndata to analyse it without any manual pre-processing steps, statistical\nsummaries or statistical models. For this we first train state-of-art computer\nvision DL models on all available image channels, both combined and\nindividually. We observed better than expected accuracy for many of these\nmodels. We then apply state-of-the-art explainable techniques relevant to\ncomputer vision DL to find the basis of the predictions of these models. Some\nof the resulting visual explainable maps highlight features in the images that\nappear consistent with the latest hypotheses about mitochondrial disease\nprogression within myofibres.",
    "descriptor": "\nComments: 10 pages, 11 figures\n",
    "authors": [
      "Atif Khan",
      "Conor Lawless",
      "Amy E Vincent",
      "Satish Pilla",
      "Sushanth Ramesh",
      "A. Stephen McGough"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17360"
  },
  {
    "id": "arXiv:2210.17366",
    "title": "Guided Conditional Diffusion for Controllable Traffic Simulation",
    "abstract": "Controllable and realistic traffic simulation is critical for developing and\nverifying autonomous vehicles. Typical heuristic-based traffic models offer\nflexible control to make vehicles follow specific trajectories and traffic\nrules. On the other hand, data-driven approaches generate realistic and\nhuman-like behaviors, improving transfer from simulated to real-world traffic.\nHowever, to the best of our knowledge, no traffic model offers both\ncontrollability and realism. In this work, we develop a conditional diffusion\nmodel for controllable traffic generation (CTG) that allows users to control\ndesired properties of trajectories at test time (e.g., reach a goal or follow a\nspeed limit) while maintaining realism and physical feasibility through\nenforced dynamics. The key technical idea is to leverage recent advances from\ndiffusion modeling and differentiable logic to guide generated trajectories to\nmeet rules defined using signal temporal logic (STL). We further extend\nguidance to multi-agent settings and enable interaction-based rules like\ncollision avoidance. CTG is extensively evaluated on the nuScenes dataset for\ndiverse and composite rules, demonstrating improvement over strong baselines in\nterms of the controllability-realism tradeoff.",
    "descriptor": "",
    "authors": [
      "Ziyuan Zhong",
      "Davis Rempe",
      "Danfei Xu",
      "Yuxiao Chen",
      "Sushant Veer",
      "Tong Che",
      "Baishakhi Ray",
      "Marco Pavone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17366"
  },
  {
    "id": "arXiv:2210.17367",
    "title": "Analysis and Detection of Singing Techniques in Repertoires of J-POP  Solo Singers",
    "abstract": "In this paper, we focus on singing techniques within the scope of music\ninformation retrieval research. We investigate how singers use singing\ntechniques using real-world recordings of famous solo singers in Japanese\npopular music songs (J-POP). First, we built a new dataset of singing\ntechniques. The dataset consists of 168 commercial J-POP songs, and each song\nis annotated using various singing techniques with timestamps and vocal pitch\ncontours. We also present descriptive statistics of singing techniques on the\ndataset to clarify what and how often singing techniques appear. We further\nexplored the difficulty of the automatic detection of singing techniques using\npreviously proposed machine learning techniques. In the detection, we also\ninvestigate the effectiveness of auxiliary information (i.e., pitch and\ndistribution of label duration), not only providing the baseline. The best\nresult achieves 40.4% at macro-average F-measure on nine-way multi-class\ndetection. We provide the annotation of the dataset and its detail on the\nappendix website 0 .",
    "descriptor": "\nComments: Accepted at ISMIR 2022\n",
    "authors": [
      "Yuya Yamamoto",
      "Juhan Nam",
      "Hiroko Terasawa"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17367"
  },
  {
    "id": "arXiv:2210.17368",
    "title": "Teacher-student curriculum learning for reinforcement learning",
    "abstract": "Reinforcement learning (rl) is a popular paradigm for sequential decision\nmaking problems. The past decade's advances in rl have led to breakthroughs in\nmany challenging domains such as video games, board games, robotics, and chip\ndesign. The sample inefficiency of deep reinforcement learning methods is a\nsignificant obstacle when applying rl to real-world problems. Transfer learning\nhas been applied to reinforcement learning such that the knowledge gained in\none task can be applied when training in a new task. Curriculum learning is\nconcerned with sequencing tasks or data samples such that knowledge can be\ntransferred between those tasks to learn a target task that would otherwise be\ntoo difficult to solve. Designing a curriculum that improves sample efficiency\nis a complex problem. In this thesis, we propose a teacher-student curriculum\nlearning setting where we simultaneously train a teacher that selects tasks for\nthe student while the student learns how to solve the selected task. Our method\nis independent of human domain knowledge and manual curriculum design. We\nevaluated our methods on two reinforcement learning benchmarks: grid world and\nthe challenging Google Football environment. With our method, we can improve\nthe sample efficiency and generality of the student compared to tabula-rasa\nreinforcement learning.",
    "descriptor": "",
    "authors": [
      "Yanick Schraner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17368"
  },
  {
    "id": "arXiv:2210.17373",
    "title": "Assignment games with population monotonic allocation schemes",
    "abstract": "We characterize the assignment games which admit a population monotonic\nallocation scheme (PMAS) in terms of efficiently verifiable structural\nproperties of the nonnegative matrix that induces the game. We prove that an\nassignment game is PMAS-admissible if and only if the positive elements of the\nunderlying nonnegative matrix form orthogonal submatrices of three special\ntypes. In game theoretic terms it means that an assignment game is\nPMAS-admissible if and only if it contains a veto player or a dominant veto\nmixed pair or is composed of from these two types of special assignment games.\nWe also show that in a PMAS-admissible assignment game all core allocations can\nbe extended to a PMAS, and the nucleolus coincides with the tau-value.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Tam\u00e1s Solymosi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2210.17373"
  },
  {
    "id": "arXiv:2210.17375",
    "title": "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared  State Representation and Individual Policy Representation",
    "abstract": "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two\nmajor paradigms of policy optimization with distinct learning principles, i.e.,\ngradient-based v.s. gradient free. An appealing research direction is\nintegrating Deep RL and EA to devise new methods by fusing their complementary\nadvantages. However, existing works on combining Deep RL and EA have two common\ndrawbacks: 1) the RL agent and EA agents learn their policies individually,\nneglecting efficient sharing of useful common knowledge; 2) parameter-level\npolicy optimization guarantees no semantic level of behavior evolution for the\nEA side. In this paper, we propose Evolutionary Reinforcement Learning with\nTwo-scale State Representation and Policy Representation (ERL-Re2), a novel\nsolution to the aforementioned two drawbacks. The key idea of ERL-Re2 is\ntwo-scale representation: all EA and RL policies share the same nonlinear state\nrepresentation while maintaining individual linear policy representations. The\nstate representation conveys expressive common features of the environment\nlearned by all the agents collectively; the linear policy representation\nprovides a favorable space for efficient policy optimization, where novel\nbehavior-level crossover and mutation operations can be performed. Moreover,\nthe linear policy representation allows convenient generalization of policy\nfitness with the help of Policy-extended Value Function Approximator (PeVFA),\nfurther improving the sample efficiency of fitness estimation. The experiments\non a range of continuous control tasks show that ERL-Re2 consistently\noutperforms strong baselines and achieves significant improvement over both its\nDeep RL and EA components.",
    "descriptor": "\nComments: The paper has been accpeted by Deep Reinforcement Learning Workshop, NeurIPS 2022\n",
    "authors": [
      "Pengyi Li",
      "Hongyao Tang",
      "Jianye Hao",
      "Yan Zheng",
      "Xian Fu",
      "Zhaopeng Meng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17375"
  },
  {
    "id": "arXiv:2210.17376",
    "title": "SoK: Modeling Explainability in Security Monitoring for Trust, Privacy,  and Interpretability",
    "abstract": "Trust, privacy, and interpretability have emerged as significant concerns for\nexperts deploying deep learning models for security monitoring. Due to their\nback-box nature, these models cannot provide an intuitive understanding of the\nmachine learning predictions, which are crucial in several decision-making\napplications, like anomaly detection. Security operations centers have a number\nof security monitoring tools that analyze logs and generate threat alerts which\nsecurity analysts inspect. The alerts lack sufficient explanation on why it was\nraised or the context in which they occurred. Existing explanation methods for\nsecurity also suffer from low fidelity and low stability and ignore privacy\nconcerns. However, explanations are highly desirable; therefore, we systematize\nthis knowledge on explanation models so they can ensure trust and privacy in\nsecurity monitoring. Through our collaborative study of security operation\ncenters, security monitoring tools, and explanation techniques, we discuss the\nstrengths of existing methods and concerns vis-a-vis applications, such as\nsecurity log analysis. We present a pipeline to design interpretable and\nprivacy-preserving system monitoring tools. Additionally, we define and propose\nquantitative metrics to evaluate methods in explainable security. Finally, we\ndiscuss challenges and enlist exciting research directions for explorations.",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Dipkamal Bhusal",
      "Nidhi Rastogi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17376"
  },
  {
    "id": "arXiv:2210.17377",
    "title": "Enabling Atomic Durability for Persistent Memory with Transiently  Persistent CPU Cache",
    "abstract": "Persistent memory (pmem) products bring the persistence domain up to the\nmemory level. Intel recently introduced the eADR feature that guarantees to\nflush data buffered in CPU cache to pmem on a power outage, thereby making the\nCPU cache a transient persistence domain. Researchers have explored how to\nenable the atomic durability for applications' in-pmem data. In this paper, we\nexploit the eADR-supported CPU cache to do so. A modified cache line, until\nwritten back to pmem, is a natural redo log copy of the in-pmem data. However,\na write-back due to cache replacement or eADR on a crash overwrites the\noriginal copy. We accordingly develop Hercules, a hardware logging design for\nthe transaction-level atomic durability, with supportive components installed\nin CPU cache, memory controller (MC), and pmem. When a transaction commits,\nHercules commits on-chip its data staying in cache lines. For cache lines\nevicted before the commit, Hercules asks the MC to redirect and persist them\ninto in-pmem log entries and commits them off-chip upon committing the\ntransaction. Hercules lazily conducts pmem writes only for cache replacements\nat runtime. On a crash, Hercules saves metadata and data for active\ntransactions into pmem for recovery. Experiments show that, by using CPU cache\nfor both buffering and logging, Hercules yields much higher throughput and\nincurs significantly fewer pmem writes than state-of-the-art designs.",
    "descriptor": "",
    "authors": [
      "Chongnan Ye",
      "Meng Chen",
      "Qisheng Jiang",
      "Chundong Wang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.17377"
  },
  {
    "id": "arXiv:2210.17378",
    "title": "Questioning the Validity of Summarization Datasets and Improving Their  Factual Consistency",
    "abstract": "The topic of summarization evaluation has recently attracted a surge of\nattention due to the rapid development of abstractive summarization systems.\nHowever, the formulation of the task is rather ambiguous, neither the\nlinguistic nor the natural language processing community has succeeded in\ngiving a mutually agreed-upon definition. Due to this lack of well-defined\nformulation, a large number of popular abstractive summarization datasets are\nconstructed in a manner that neither guarantees validity nor meets one of the\nmost essential criteria of summarization: factual consistency. In this paper,\nwe address this issue by combining state-of-the-art factual consistency models\nto identify the problematic instances present in popular summarization\ndatasets. We release SummFC, a filtered summarization dataset with improved\nfactual consistency, and demonstrate that models trained on this dataset\nachieve improved performance in nearly all quality aspects. We argue that our\ndataset should become a valid benchmark for developing and evaluating\nsummarization systems.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Yanzhu Guo",
      "Chlo\u00e9 Clavel",
      "Moussa Kamal Eddine",
      "Michalis Vazirgiannis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17378"
  },
  {
    "id": "arXiv:2210.17381",
    "title": "Safe and Efficient Manoeuvring for Emergency Vehicles in Autonomous  Traffic using Multi-Agent Proximal Policy Optimisation",
    "abstract": "Manoeuvring in the presence of emergency vehicles is still a major issue for\nvehicle autonomy systems. Most studies that address this topic are based on\nrule-based methods, which cannot cover all possible scenarios that can take\nplace in autonomous traffic. Multi-Agent Proximal Policy Optimisation (MAPPO)\nhas recently emerged as a powerful method for autonomous systems because it\nallows for training in thousands of different situations. In this study, we\npresent an approach based on MAPPO to guarantee the safe and efficient\nmanoeuvring of autonomous vehicles in the presence of an emergency vehicle. We\nintroduce a risk metric that summarises the potential risk of collision in a\nsingle index. The proposed method generates cooperative policies allowing the\nemergency vehicle to go at $15 \\%$ higher average speed while maintaining high\nsafety distances. Moreover, we explore the trade-off between safety and traffic\nefficiency and assess the performance in a competitive scenario.",
    "descriptor": "",
    "authors": [
      "Leandro Parada",
      "Eduardo Candela",
      "Luis Marques",
      "Panagiotis Angeloudis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.17381"
  },
  {
    "id": "arXiv:2210.17386",
    "title": "Enabling Digital Twin in Vehicular Edge Computing: A Multi-Agent  Multi-Objective Deep Reinforcement Learning Solution",
    "abstract": "With recent advances in sensing technologies, wireless communications, and\ncomputing paradigms, traditional vehicles are evolving to electronic consumer\nproducts, driving the research on digital twins in vehicular edge computing\n(DT-VEC). This paper makes the first attempt to achieve the quality-cost\ntradeoff in DT-VEC. First, a DT-VEC architecture is presented, where the\nheterogeneous information can be sensed by vehicles and uploaded to the edge\nnode via vehicle-to-infrastructure (V2I) communications. The DT-VEC are modeled\nat the edge node, forming a logical view to reflect the physical vehicular\nenvironment. Second, we model the DT-VEC by deriving an ISAC (integrated\nsensing and communication)-assisted sensing model and a reliability-guaranteed\nuploading model. Third, we define the quality of DT-VEC by considering the\ntimeliness and consistency, and define the cost of DT-VEC by considering the\nredundancy, sensing cost, and transmission cost. Then, a bi-objective problem\nis formulated to maximize the quality and minimize the cost. Fourth, we propose\na multi-agent multi-objective (MAMO) deep reinforcement learning solution\nimplemented distributedly in the vehicles and the edge nodes. Specifically, a\ndueling critic network is proposed to evaluate the advantage of action over the\naverage of random actions. Finally, we give a comprehensive performance\nevaluation, demonstrating the superiority of the proposed MAMO.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2209.12265\n",
    "authors": [
      "Xincao Xu",
      "Kai Liu",
      "Penglin Dai",
      "Biwen Chen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17386"
  },
  {
    "id": "arXiv:2210.17388",
    "title": "Combining noisy well data and expert knowledge in a Bayesian calibration  of a flow model under uncertainties: an application to solute transport in  the Ticino basin",
    "abstract": "Groundwater flow modeling is commonly used to calculate groundwater heads,\nestimate groundwater flow paths and travel times, and provide insights into\nsolute transport processes within an aquifer. However, the values of input\nparameters that drive groundwater flow models are often highly uncertain due to\nsubsurface heterogeneity and geologic complexity in combination with lack of\nmeasurements/unreliable measurements. This uncertainty affects the accuracy and\nreliability of model outputs. Therefore, parameters' uncertainty must be\nquantified before adopting the model as an engineering tool. In this study, we\nmodel the uncertain parameters as random variables and use a Bayesian inversion\napproach to obtain a posterior,data-informed, probability density function\n(pdf) for them: in particular, the likelihood function we consider takes into\naccount both well measurements and \"expert knowledge\" about the extent of the\nsprings in the domain under study. To keep the modelistic and computational\ncomplexities under control, we assume Gaussianity of the posterior pdf of the\nparameters. To corroborate this assumption, we run an identifiability analysis\nof the model: we apply the inversion procedure to several sets of synthetic\ndata polluted by increasing levels of noise, and we determine at which levels\nof noise we can effectively recover the \"true value\" of the parameters. We then\nmove to real well data (coming from the Ticino River basin, in northern Italy,\nand spanning a month in summer 2014), and use the posterior pdf of the\nparameters as a starting point to perform an Uncertainty Quantification\nanalysis on groundwater travel-time distributions.",
    "descriptor": "\nComments: First submission\n",
    "authors": [
      "Emily A. Baker",
      "Sauro Manenti",
      "Alessandro Reali",
      "Giancarlo Sangalli",
      "Lorenzo Tamellini",
      "Sara Todeschini"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.17388"
  },
  {
    "id": "arXiv:2210.17390",
    "title": "Latent Semantic Structure in Malicious Programs",
    "abstract": "Latent Semantic Analysis is a method of matrix decomposition used for\ndiscovering topics and topic weights in natural language documents. This study\nuses Latent Semantic Analysis to analyze the composition of binaries of\nmalicious programs. The semantic representation of the term frequency vector\nrepresentation yields a set of topics, each topic being a composition of terms.\nThe vectors and topics were evaluated quantitatively using a spatial\nrepresentation. This semantic analysis provides a more abstract representation\nof the program derived from its term frequency analysis. We use a metric space\nto represent a program as a collection of vectors, and a distance metric to\nevaluate their similarity within a topic. The segmentation of the vectors in\nthis dataset provides increased resolution into the program structure.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "John Musgrave",
      "Temesguen Messay-Kebede",
      "David Kapp",
      "Anca Ralescu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.17390"
  },
  {
    "id": "arXiv:2210.17392",
    "title": "On the Geometry Transferability of the Hybrid Iterative Numerical Solver  for Differential Equations",
    "abstract": "The discovery of fast numerical solvers prompted a clear and rapid shift\ntowards iterative techniques in many applications, especially in computational\nmechanics, due to the increased necessity for solving very large linear\nsystems. Most numerical solvers are highly dependent on the problem geometry\nand discretization, facing issues when any of these properties change. The\nnewly developed Hybrid Iterative Numerical Transferable Solver (HINTS) combines\na standard solver with a neural operator to achieve better performance,\nfocusing on a single geometry at a time. In this work, we explore the \"T\" in\nHINTS, i.e., the geometry transferability properties of HINTS. We first propose\nto directly employ HINTS built for a specific geometry to a different but\nrelated geometry without any adjustments. In addition, we propose the\nintegration of an operator level transfer learning with HINTS to even further\nimprove the convergence of HINTS on new geometries and discretizations. We\nconduct numerical experiments for a Darcy flow problem and a plane-strain\nelasticity problem. The results show that both the direct application of HINTS\nand the transfer learning enhanced HINTS are able to accurately solve these\nproblems on different geometries. In addition, using transfer learning, HINTS\nis able to converge to machine zero even faster than the direct application of\nHINTS.",
    "descriptor": "\nComments: 12 pages, 8 figures, 1 table\n",
    "authors": [
      "Adar Kahana",
      "Enrui Zhang",
      "Somdatta Goswami",
      "George EM Karniadakis",
      "Rishikesh Ranade",
      "Jay Pathak"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17392"
  },
  {
    "id": "arXiv:2210.17393",
    "title": "Probabilistic Decomposition Transformer for Time Series Forecasting",
    "abstract": "Time series forecasting is crucial for many fields, such as disaster warning,\nweather prediction, and energy consumption. The Transformer-based models are\nconsidered to have revolutionized the field of sequence modeling. However, the\ncomplex temporal patterns of the time series hinder the model from mining\nreliable temporal dependencies. Furthermore, the autoregressive form of the\nTransformer introduces cumulative errors in the inference step. In this paper,\nwe propose the probabilistic decomposition Transformer model that combines the\nTransformer with a conditional generative model, which provides hierarchical\nand interpretable probabilistic forecasts for intricate time series. The\nTransformer is employed to learn temporal patterns and implement primary\nprobabilistic forecasts, while the conditional generative model is used to\nachieve non-autoregressive hierarchical probabilistic forecasts by introducing\nlatent space feature representations. In addition, the conditional generative\nmodel reconstructs typical features of the series, such as seasonality and\ntrend terms, from probability distributions in the latent space to enable\ncomplex pattern separation and provide interpretable forecasts. Extensive\nexperiments on several datasets demonstrate the effectiveness and robustness of\nthe proposed model, indicating that it compares favorably with the state of the\nart.",
    "descriptor": "",
    "authors": [
      "Junlong Tong",
      "Liping Xie",
      "Wankou Yang",
      "Kanjian Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17393"
  },
  {
    "id": "arXiv:2210.17395",
    "title": "User Manual of Automatic Data Curation Tool(ADCT): A bulk data curator  software in Library and Information Science",
    "abstract": "In library and information science, document storage and user-specific\ndocument retrieval are the main aspects of digital library services. To\npreserve the cultural heritage, documents, and literature, we need a common\nplatform where all types of documents are available in a specific format. Our\nproposed software tool, ADCT can handle a bulk amount of data and transforms\ndifferent types of raw data into specified metadata information. It generalizes\nmultiple forms of curation logic applied to the source data. As state of the\nart, many research activities is done in various university to manage their\nresearch data in digital library services. The author provides descriptive\nstatistics of library and information service (LIS) activities for information\nstorage and retrieval. The paper shows research on methodology, information\ngathering, and scientific communication done on library data as an activity of\nthe LIS community. The analysis of LIS data and the change of interest in\ninformation storage and retrieval from classification and indexing to retrieval\nare much important. For Building these types of digitized educational services,\nautomatic data curation is an important stepping stone. The paper shows that\nautomation can complement expertise and knowledge. The author builds an\nautomated data transformation and curation tool that assists users in the\nanalysis of metadata information and effort towards conforming to generic\nstandards using a catalog when a librarian wants to see the adjacent related\nmetadata for an archival collection of congressional correspondence.",
    "descriptor": "",
    "authors": [
      "A. Banerjee",
      "B. Sutradhar"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.17395"
  },
  {
    "id": "arXiv:2210.17398",
    "title": "Rethinking Generalization: The Impact of Annotation Style on Medical  Image Segmentation",
    "abstract": "Generalization is an important attribute of machine learning models,\nparticularly for those that are to be deployed in a medical context, where\nunreliable predictions can have real world consequences. While the failure of\nmodels to generalize across datasets is typically attributed to a mismatch in\nthe data distributions, performance gaps are often a consequence of biases in\nthe ``ground-truth\" label annotations. This is particularly important in the\ncontext of medical image segmentation of pathological structures (e.g.\nlesions), where the annotation process is much more subjective, and affected by\na number underlying factors, including the annotation protocol, rater\neducation/experience, and clinical aims, among others. In this paper, we show\nthat modeling annotation biases, rather than ignoring them, poses a promising\nway of accounting for differences in annotation style across datasets. To this\nend, we propose a generalized conditioning framework to (1) learn and account\nfor different annotation styles across multiple datasets using a single model,\n(2) identify similar annotation styles across different datasets in order to\npermit their effective aggregation, and (3) fine-tune a fully trained model to\na new annotation style with just a few samples. Next, we present an\nimage-conditioning approach to model annotation styles that correlate with\nspecific image features, potentially enabling detection biases to be more\neasily identified.",
    "descriptor": "\nComments: Accepted to MELBA\n",
    "authors": [
      "Brennan Nichyporuk",
      "Jillian Cardinell",
      "Justin Szeto",
      "Raghav Mehta",
      "Jean-Pierre R. Falet",
      "Douglas L. Arnold",
      "Sotirios A. Tsaftaris",
      "Tal Arbel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.17398"
  },
  {
    "id": "arXiv:2210.17399",
    "title": "Do You Really Need to Disguise Normal Servers as Honeypots?",
    "abstract": "A honeypot, which is a kind of deception strategy, has been widely used for\nat least 20 years to mitigate cyber threats. Decision-makers have believed that\nhoneypot strategies are intuitive and effective, since honeypots have\nsuccessfully protected systems from Denial-of-Service (DoS) attacks to Advanced\nPersistent Threats (APT) in real-world cases. Nonetheless, there is a lack of\nresearch on the appropriate level of honeypot technique application to choose\nreal-world operations. We examine and contrast three attack-defense games with\nrespect to honeypot detection techniques in this paper. In particular, we\nspecifically design and contrast two stages of honeypot technology one by one,\nstarting with a game without deception. We demonstrate that the return for a\ndefender using honeypots is higher than for a defender without them, albeit the\ndefender may not always benefit financially from using more honeypot deception\nstrategies. Particularly, disguising regular servers as honeypots does not\nprovide defenders with a better reward. Furthermore, we take in consideration\nthat fake honeypots can make maintaining normal nodes more costly. Our research\noffers a theoretical foundation for the real-world operator's decision of\nhoneypot deception tactics and the required number of honeypot nodes.",
    "descriptor": "\nComments: This paper is accepted to the IEEE Military Communications Conference (MILCOM) 2022\n",
    "authors": [
      "Suhyeon Lee",
      "Kwangsoo Cho",
      "Seungjoo Kim"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.17399"
  },
  {
    "id": "arXiv:2210.17400",
    "title": "Max Pooling with Vision Transformers reconciles class and shape in  weakly supervised semantic segmentation",
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) research has explored many\ndirections to improve the typical pipeline CNN plus class activation maps (CAM)\nplus refinements, given the image-class label as the only supervision. Though\nthe gap with the fully supervised methods is reduced, further abating the\nspread seems unlikely within this framework. On the other hand, WSSS methods\nbased on Vision Transformers (ViT) have not yet explored valid alternatives to\nCAM. ViT features have been shown to retain a scene layout, and object\nboundaries in self-supervised learning. To confirm these findings, we prove\nthat the advantages of transformers in self-supervised methods are further\nstrengthened by Global Max Pooling (GMP), which can leverage patch features to\nnegotiate pixel-label probability with class probability. This work proposes a\nnew WSSS method dubbed ViT-PCM (ViT Patch-Class Mapping), not based on CAM. The\nend-to-end presented network learns with a single optimization process, refined\nshape and proper localization for segmentation masks. Our model outperforms the\nstate-of-the-art on baseline pseudo-masks (BPM), where we achieve $69.3\\%$ mIoU\non PascalVOC 2012 $val$ set. We show that our approach has the least set of\nparameters, though obtaining higher accuracy than all other approaches. In a\nsentence, quantitative and qualitative results of our method reveal that\nViT-PCM is an excellent alternative to CNN-CAM based architectures.",
    "descriptor": "\nComments: 28 pages, 9 images, ECCV 2022 conference\n",
    "authors": [
      "Simone Rossetti",
      "Damiano Zappia",
      "Marta Sanzari",
      "Marco Schaerf",
      "Fiora Pirri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17400"
  },
  {
    "id": "arXiv:2210.17402",
    "title": "The Executable Digital Twin: merging the digital and the physics worlds",
    "abstract": "While the digital twin has become an intrinsic part of the product creation\nprocess, its true power lies in the connectivity of the digital representation\nwith its physical counterpart. Data acquired on the physical asset can\nvalidate, update and enrich the digital twin. The knowledge contained in the\ndigital representation brings value to the physical asset itself. When a\ndedicated encapsulation is extracted from the digital twin to model a specific\nset of behaviors in a specific context, delivering a stand-alone executable\nrepresentation, such instantiated and self-contained model is referred to as an\nExecutable Digital Twin. In this contribution, key building blocks such as\nmodel order reduction, real-time models, state estimation and co-simulation are\nreviewed, and a number of characteristic use cases are presented. These include\nvirtual sensing, hybrid testing and hardware-in-the loop, model-based control\nand model-based diagnostics.",
    "descriptor": "",
    "authors": [
      "Dirk Hartmann",
      "Herman Van der Auweraer"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.17402"
  },
  {
    "id": "arXiv:2210.17403",
    "title": "Random Walk-based Community Key-members Search over Large Graphs",
    "abstract": "Given a graph $G$, a query node $q$, and an integer $k$, community search\n(CS) seeks a cohesive subgraph (measured by community models such as $k$-core\nor $k$-truss) from $G$ that contains $q$. It is difficult for ordinary users\nwith less knowledge of graphs' complexity to set an appropriate $k$. Even if we\ndefine quite a large $k$, the community size returned by CS is often too large\nfor users to gain much insight about it. Compared against the entire community,\nkey-members in the community appear more valuable than others. To contend with\nthis, we focus on a new problem, that is \\textbf{C}ommunity\n\\textbf{K}ey-members \\textbf{S}earch problem (CKS). We turn our perspective to\nthe key-members in the community containing $q$ instead of the entire\ncommunity. To solve CKS problem, we first propose an exact algorithm based on\ntruss decomposition as the baseline. Then, we present four random walk-based\noptimized algorithms to achieve a trade-off between effectiveness and\nefficiency, by carefully considering some important cohesiveness features in\nthe design of transition matrix. We return the top-$n$ key-members according to\nthe stationary distribution when random walk converges. Moreover, we propose a\nlightweight refinement method following an \"expand-replace\" manner to further\noptimize the top-$n$ result with little overhead, and we extend our solution to\nsupport CKS with multiple query nodes. We also analyze our solution's\neffectiveness theoretically. Comprehensive experimental studies on various\nreal-world datasets demonstrate our method's superiority.",
    "descriptor": "",
    "authors": [
      "Yuxiang Wang",
      "Yue Wu",
      "Xiaoliang Xu",
      "Tianxing Wu",
      "Xiangyu Ke",
      "Yuyang Zhao"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.17403"
  },
  {
    "id": "arXiv:2210.17406",
    "title": "Emergent Linguistic Structures in Neural Networks are Fragile",
    "abstract": "Large language models (LLMs) have been reported to have strong performance on\nnatural language processing tasks. However, performance metrics such as\naccuracy do not measure the quality of the model in terms of its ability to\nrobustly represent complex linguistic structure. Further, the sheer size of\nLLMs makes it difficult to analyse them using standard robustness evaluation\nmethods. In this work, we propose a framework to evaluate the robustness of\nlinguistic representations using probing tasks. We argue that a robust\nlinguistic model is one that is able to robustly and efficiently represent\ncomplex syntactic structure underlying the data distribution and propose\nappropriate robustness measures. We leverage recent advances in extracting\nemergent linguistic constructs from LLMs and apply syntax-preserving\nperturbations to test the stability of these constructs in order to better\nunderstand the representations learned by LLMs. Empirically, we study the\nperformance of four LLMs across six different corpora on the proposed\nrobustness measures. We provide evidence that context-free representation\n(e.g., GloVE) are in some cases competitive with context-dependent\nrepresentations from modern LLMs (e.g., BERT), yet equally brittle to\nsyntax-preserving manipulations. Emergent syntactic representations in neural\nnetworks are brittle, thus our work poses the attention on the risk of\ncomparing such structures to those that are object of a long lasting debate in\nlinguistics.",
    "descriptor": "",
    "authors": [
      "Emanuele La Malfa",
      "Matthew Wicker",
      "Marta Kiatkowska"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17406"
  },
  {
    "id": "arXiv:2210.17407",
    "title": "Circuit Solutions towards Broadband Piezoelectric Energy Harvesting: An  Impedance Analysis",
    "abstract": "In the studies of piezoelectric energy harvesting (PEH) systems, literature\nhas shown that circuit advancement has a significant effect on the enhancement\nof energy harvesting capability in resonance. On the other hand, some recent\nstudies using the phase-variable (PV) synchronized switch technologies have\nfound that the advanced circuit solutions can also broaden the harvesting\nbandwidth. However, the available span of the electrically induced dynamics by\nthe existing energy harvesting circuits was not properly defined and\ndemonstrated. Performance comparison among different circuits cannot be fairly\nachieved without using a common theoretical language. Given these, this paper\nprovides an impedance-based analysis and comparison on the electromechanical\njoint dynamics of the PEH systems using different interface circuits. Given\nthat the resonance tunability by circuit solutions has received no attention in\nthe conventional ideal model of kinetic energy harvester, we firstly propose a\nmore inclusive ideal model for better generalization. In practice, it was\nproven that the attainable dynamic ranges of the practical energy harvesting\ncircuits are only some subsets of the ideal realm. A detailed quantitative\nstudy on the attainable ranges of the PV circuit solutions is provided after\nthe introduction of the ideal target. Simulation and experimental results of\ndifferent interface circuits show good agreement with the theoretical analysis.\nIt can be concluded that the resonance tunability strongly depends on the\nachievable extent in the reactive direction of the equivalent impedance plane.\nIn practice, the electromechanical coupling conditions and dielectric loss\nmight also influence the resonance tunability. The general ideal model and\nquantitative impedance analysis provided in this paper help guide the future\ndesign effort towards high-capability and broadband PEH systems.",
    "descriptor": "",
    "authors": [
      "Bao Zhao",
      "Junrui Liang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.17407"
  },
  {
    "id": "arXiv:2210.17409",
    "title": "Deep Model Reassembly",
    "abstract": "In this paper, we explore a novel knowledge-transfer task, termed as Deep\nModel Reassembly (DeRy), for general-purpose model reuse. Given a collection of\nheterogeneous models pre-trained from distinct sources and with diverse\narchitectures, the goal of DeRy, as its name implies, is to first dissect each\nmodel into distinctive building blocks, and then selectively reassemble the\nderived blocks to produce customized networks under both the hardware resource\nand performance constraints. Such ambitious nature of DeRy inevitably imposes\nsignificant challenges, including, in the first place, the feasibility of its\nsolution. We strive to showcase that, through a dedicated paradigm proposed in\nthis paper, DeRy can be made not only possibly but practically efficiently.\nSpecifically, we conduct the partitions of all pre-trained networks jointly via\na cover set optimization, and derive a number of equivalence set, within each\nof which the network blocks are treated as functionally equivalent and hence\ninterchangeable. The equivalence sets learned in this way, in turn, enable\npicking and assembling blocks to customize networks subject to certain\nconstraints, which is achieved via solving an integer program backed up with a\ntraining-free proxy to estimate the task performance. The reassembled models,\ngive rise to gratifying performances with the user-specified constraints\nsatisfied. We demonstrate that on ImageNet, the best reassemble model achieves\n78.6% top-1 accuracy without fine-tuning, which could be further elevated to\n83.2% with end-to-end training. Our code is available at\nhttps://github.com/Adamdad/DeRy",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Xingyi Yang",
      "Zhou Daquan",
      "Songhua Liu",
      "Jingwen Ye",
      "Xinchao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17409"
  },
  {
    "id": "arXiv:2210.17410",
    "title": "A Python Framework for SPICE Circuit Simulation of In-Memory Analog  Computing Circuits",
    "abstract": "With the increased attention to memristive-based in-memory analog computing\n(IMAC) architectures as an alternative for energy-hungry computer systems for\ndata-intensive applications, a tool that enables exploring their device- and\ncircuit-level design space can significantly boost the research and development\nin this area. Thus, in this paper, we develop IMAC-Sim, a circuit-level\nsimulator for the design space exploration and multi-objective optimization of\nIMAC architectures. IMAC-Sim is a Python-based simulation framework, which\ncreates the SPICE netlist of the IMAC circuit based on various device- and\ncircuit-level hyperparameters selected by the user, and automatically evaluates\nthe accuracy, power consumption and latency of the developed circuit using a\nuser-specified dataset. IMAC-Sim simulates the interconnect parasitic\nresistance and capacitance in the IMAC architectures, and is also equipped with\nhorizontal and vertical partitioning techniques to surmount these reliability\nchallenges. In this abstract, we perform controlled experiments to exhibit some\nof the important capabilities of the IMAC-Sim.",
    "descriptor": "",
    "authors": [
      "Md Hasibul Amin",
      "Mohammed Elbtity",
      "Ramtin Zand"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.17410"
  },
  {
    "id": "arXiv:2210.17411",
    "title": "Offset-Guided Attention Network for Room-Level Aware Floor Plan  Segmentation",
    "abstract": "Recognition of floor plans has been a challenging and popular task. Despite\nthat many recent approaches have been proposed for this task, they typically\nfail to make the room-level unified prediction. Specifically, multiple semantic\ncategories can be assigned in a single room, which seriously limits their\nvisual quality and applicability. In this paper, we propose a novel approach to\nrecognize the floor plan layouts with a newly proposed Offset-Guided Attention\nmechanism to improve the semantic consistency within a room. In addition, we\npresent a Feature Fusion Attention module that leverages the channel-wise\nattention to encourage the consistency of the room, wall, and door predictions,\nfurther enhancing the room-level semantic consistency. Experimental results\nmanifest our approach is able to improve the room-level semantic consistency\nand outperforms the existing works both qualitatively and quantitatively.",
    "descriptor": "\nComments: Under review of IEEE Access(3 accepts and 1 reject)\n",
    "authors": [
      "Zhangyu Wang",
      "Ningyuan Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17411"
  },
  {
    "id": "arXiv:2210.17412",
    "title": "Adversarial Domain Adaptation for Action Recognition Around the Clock",
    "abstract": "Due to the numerous potential applications in visual surveillance and\nnighttime driving, recognizing human action in low-light conditions remains a\ndifficult problem in computer vision. Existing methods separate action\nrecognition and dark enhancement into two distinct steps to accomplish this\ntask. However, isolating the recognition and enhancement impedes end-to-end\nlearning of the space-time representation for video action classification. This\npaper presents a domain adaptation-based action recognition approach that uses\nadversarial learning in cross-domain settings to learn cross-domain action\nrecognition. Supervised learning can train it on a large amount of labeled data\nfrom the source domain (daytime action sequences). However, it uses deep domain\ninvariant features to perform unsupervised learning on many unlabelled data\nfrom the target domain (night-time action sequences). The resulting augmented\nmodel, named 3D-DiNet can be trained using standard backpropagation with an\nadditional layer. It achieves SOTA performance on InFAR and XD145 actions\ndatasets.",
    "descriptor": "\nComments: 6 Pages, 6 Figures\n",
    "authors": [
      "Anwaar Ulhaq"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17412"
  },
  {
    "id": "arXiv:2210.17414",
    "title": "An Industrial Workplace Alerting and Monitoring Platform to Prevent  Workplace Injury and Accidents",
    "abstract": "Workplace accidents are a critical problem that causes many deaths, injuries,\nand financial losses. Climate change has a severe impact on industrial workers,\npartially caused by global warming. To reduce such casualties, it is important\nto proactively find unsafe environments where injuries could occur by detecting\nthe use of personal protective equipment (PPE) and identifying unsafe\nactivities. Thus, we propose an industrial workplace alerting and monitoring\nplatform to detect PPE use and classify unsafe activity in group settings\ninvolving multiple humans and objects over a long period of time. Our proposed\nmethod is the first to analyze prolonged actions involving multiple people or\nobjects. It benefits from combining pose estimation with PPE detection in one\nplatform. Additionally, we propose the first open source annotated data set\nwith video data from industrial workplaces annotated with the action\nclassifications and detected PPE. The proposed system can be implemented within\nthe surveillance cameras already present in industrial settings, making it a\npractical and effective solution.",
    "descriptor": "",
    "authors": [
      "Sanjay Adhikesaven"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17414"
  },
  {
    "id": "arXiv:2210.17415",
    "title": "ProbNeRF: Uncertainty-Aware Inference of 3D Shapes from 2D Images",
    "abstract": "The problem of inferring object shape from a single 2D image is\nunderconstrained. Prior knowledge about what objects are plausible can help,\nbut even given such prior knowledge there may still be uncertainty about the\nshapes of occluded parts of objects. Recently, conditional neural radiance\nfield (NeRF) models have been developed that can learn to infer good point\nestimates of 3D models from single 2D images. The problem of inferring\nuncertainty estimates for these models has received less attention. In this\nwork, we propose probabilistic NeRF (ProbNeRF), a model and inference strategy\nfor learning probabilistic generative models of 3D objects' shapes and\nappearances, and for doing posterior inference to recover those properties from\n2D images. ProbNeRF is trained as a variational autoencoder, but at test time\nwe use Hamiltonian Monte Carlo (HMC) for inference. Given one or a few 2D\nimages of an object (which may be partially occluded), ProbNeRF is able not\nonly to accurately model the parts it sees, but also to propose realistic and\ndiverse hypotheses about the parts it does not see. We show that key to the\nsuccess of ProbNeRF are (i) a deterministic rendering scheme, (ii) an\nannealed-HMC strategy, (iii) a hypernetwork-based decoder architecture, and\n(iv) doing inference over a full set of NeRF weights, rather than just a\nlow-dimensional code.",
    "descriptor": "\nComments: 18 pages, 18 figures, 1 table; submitted to the 26th International Conference on Artificial Intelligence and Statistics (AISTATS 2023)\n",
    "authors": [
      "Matthew D. Hoffman",
      "Tuan Anh Le",
      "Pavel Sountsov",
      "Christopher Suter",
      "Ben Lee",
      "Vikash K. Mansinghka",
      "Rif A. Saurous"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17415"
  },
  {
    "id": "arXiv:2210.17416",
    "title": "Efficient Similarity-based Passive Filter Pruning for Compressing CNNs",
    "abstract": "Convolution neural networks (CNNs) have shown great success in various\napplications. However, the computational complexity and memory storage of CNNs\nis a bottleneck for their deployment on resource-constrained devices. Recent\nefforts towards reducing the computation cost and the memory overhead of CNNs\ninvolve similarity-based passive filter pruning methods. Similarity-based\npassive filter pruning methods compute a pairwise similarity matrix for the\nfilters and eliminate a few similar filters to obtain a small pruned CNN.\nHowever, the computational complexity of computing the pairwise similarity\nmatrix is high, particularly when a convolutional layer has many filters. To\nreduce the computational complexity in obtaining the pairwise similarity\nmatrix, we propose to use an efficient method where the complete pairwise\nsimilarity matrix is approximated from only a few of its columns by using a\nNystr\\\"om approximation method. The proposed efficient similarity-based passive\nfilter pruning method is 3 times faster and gives same accuracy at the same\nreduction in computations for CNNs compared to that of the similarity-based\npruning method that computes a complete pairwise similarity matrix. Apart from\nthis, the proposed efficient similarity-based pruning method performs similarly\nor better than the existing norm-based pruning methods. The efficacy of the\nproposed pruning method is evaluated on CNNs such as DCASE 2021 Task 1A\nbaseline network and a VGGish network designed for acoustic scene\nclassification.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Arshdeep Singh",
      "Mark D. Plumbley"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.17416"
  },
  {
    "id": "arXiv:2210.17417",
    "title": "Fashion-Specific Attributes Interpretation via Dual Gaussian  Visual-Semantic Embedding",
    "abstract": "Several techniques to map various types of components, such as words,\nattributes, and images, into the embedded space have been studied. Most of them\nestimate the embedded representation of target entity as a point in the\nprojective space. Some models, such as Word2Gauss, assume a probability\ndistribution behind the embedded representation, which enables the spread or\nvariance of the meaning of embedded target components to be captured and\nconsidered in more detail. We examine the method of estimating embedded\nrepresentations as probability distributions for the interpretation of\nfashion-specific abstract and difficult-to-understand terms. Terms, such as\n\"casual,\" \"adult-casual,'' \"beauty-casual,\" and \"formal,\" are extremely\nsubjective and abstract and are difficult for both experts and non-experts to\nunderstand, which discourages users from trying new fashion. We propose an\nend-to-end model called dual Gaussian visual-semantic embedding, which maps\nimages and attributes in the same projective space and enables the\ninterpretation of the meaning of these terms by its broad applications. We\ndemonstrate the effectiveness of the proposed method through multifaceted\nexperiments involving image and attribute mapping, image retrieval and\nre-ordering techniques, and a detailed theoretical/analytical discussion of the\ndistance measure included in the loss function.",
    "descriptor": "",
    "authors": [
      "Ryotaro Shimizu",
      "Masanari Kimura",
      "Masayuki Goto"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17417"
  },
  {
    "id": "arXiv:2210.17418",
    "title": "Controllable Factuality in Document-Grounded Dialog Systems Using a  Noisy Channel Model",
    "abstract": "In this work, we present a model for document-grounded response generation in\ndialog that is decomposed into two components according to Bayes theorem. One\ncomponent is a traditional ungrounded response generation model and the other\ncomponent models the reconstruction of the grounding document based on the\ndialog context and generated response. We propose different approximate\ndecoding schemes and evaluate our approach on multiple open-domain and\ntask-oriented document-grounded dialog datasets. Our experiments show that the\nmodel is more factual in terms of automatic factuality metrics than the\nbaseline model. Furthermore, we outline how introducing scaling factors between\nthe components allows for controlling the tradeoff between factuality and\nfluency in the model output. Finally, we compare our approach to a recently\nproposed method to control factuality in grounded dialog, CTRL\n(arXiv:2107.06963), and show that both approaches can be combined to achieve\nadditional improvements.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Nico Daheim",
      "David Thulke",
      "Christian Dugast",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17418"
  },
  {
    "id": "arXiv:2210.17419",
    "title": "Impact of PolSAR pre-processing and balancing methods on complex-valued  neural networks segmentation tasks",
    "abstract": "In this paper, we investigated the semantic segmentation of Polarimetric\nSynthetic Aperture Radar (PolSAR) using Complex-Valued Neural Network (CVNN).\nAlthough the coherency matrix is more widely used as the input of CVNN, the\nPauli vector has recently been shown to be a valid alternative. We exhaustively\ncompare both methods for six model architectures, three complex-valued, and\ntheir respective real-equivalent models. We are comparing, therefore, not only\nthe input representation impact but also the complex- against the real-valued\nmodels. We then argue that the dataset splitting produces a high correlation\nbetween training and validation sets, saturating the task and thus achieving\nvery high performance. We, therefore, use a different data pre-processing\ntechnique designed to reduce this effect and reproduce the results with the\nsame configurations as before (input representation and model architectures).\nAfter seeing that the performance per class is highly different according to\nclass occurrences, we propose two methods for reducing this gap and performing\nthe results for all input representations, models, and dataset pre-processing.",
    "descriptor": "\nComments: 9 pages, 4 figures, 5 tables\n",
    "authors": [
      "Jos\u00e9 Agustin Barrachina",
      "Chengfang Ren",
      "Christ\u00e8le Morisseau",
      "Gilles Vieillard",
      "Jean-Philippe Ovarlez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.17419"
  },
  {
    "id": "arXiv:2210.17420",
    "title": "Digital twins of physical printing-imaging channel",
    "abstract": "In this paper, we address the problem of modeling a printing-imaging channel\nbuilt on a machine learning approach a.k.a. digital twin for\nanti-counterfeiting applications based on copy detection patterns (CDP). The\ndigital twin is formulated on an information-theoretic framework called Turbo\nthat uses variational approximations of mutual information developed for both\nencoder and decoder in a two-directional information passage. The proposed\nmodel generalizes several state-of-the-art architectures such as adversarial\nautoencoder (AAE), CycleGAN and adversarial latent space autoencoder (ALAE).\nThis model can be applied to any type of printing and imaging and it only\nrequires training data consisting of digital templates or artworks that are\nsent to a printing device and data acquired by an imaging device. Moreover,\nthese data can be paired, unpaired or hybrid paired-unpaired which makes the\nproposed architecture very flexible and scalable to many practical setups. We\ndemonstrate the impact of various architectural factors, metrics and\ndiscriminators on the overall system performance in the task of\ngeneration/prediction of printed CDP from their digital counterparts and vice\nversa. We also compare the proposed system with several state-of-the-art\nmethods used for image-to-image translation applications.",
    "descriptor": "\nComments: Paper accepted at the IEEE International Workshop on Information Forensics and Security (WIFS) 2022\n",
    "authors": [
      "Yury Belousov",
      "Brian Pulfer",
      "Roman Chaban",
      "Joakim Tutt",
      "Olga Taran",
      "Taras Holotyak",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17420"
  },
  {
    "id": "arXiv:2210.17421",
    "title": "I am Only Happy When There is Light: The Impact of Environmental Changes  on Affective Facial Expressions Recognition",
    "abstract": "Human-robot interaction (HRI) benefits greatly from advances in the machine\nlearning field as it allows researchers to employ high-performance models for\nperceptual tasks like detection and recognition. Especially deep learning\nmodels, either pre-trained for feature extraction or used for classification,\nare now established methods to characterize human behaviors in HRI scenarios\nand to have social robots that understand better those behaviors. As HRI\nexperiments are usually small-scale and constrained to particular lab\nenvironments, the questions are how well can deep learning models generalize to\nspecific interaction scenarios, and further, how good is their robustness\ntowards environmental changes? These questions are important to address if the\nHRI field wishes to put social robotic companions into real environments acting\nconsistently, i.e. changing lighting conditions or moving people should still\nproduce the same recognition results. In this paper, we study the impact of\ndifferent image conditions on the recognition of arousal and valence from human\nfacial expressions using the FaceChannel framework \\cite{Barro20}. Our results\nshow how the interpretation of human affective states can differ greatly in\neither the positive or negative direction even when changing only slightly the\nimage properties. We conclude the paper with important points to consider when\nemploying deep learning models to ensure sound interpretation of HRI\nexperiments.",
    "descriptor": "\nComments: Paper contribution to the Social and Cognitive Interactions for Assistive Robotics Workshop (SCIAR) In Conjunction with IROS 2022\n",
    "authors": [
      "Doreen Jirak",
      "Alessandra Sciutti",
      "Pablo Barros",
      "Francesco Rea"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.17421"
  },
  {
    "id": "arXiv:2210.17424",
    "title": "Tree Detection and Diameter Estimation Based on Deep Learning",
    "abstract": "Tree perception is an essential building block toward autonomous forestry\noperations. Current developments generally consider input data from lidar\nsensors to solve forest navigation, tree detection and diameter estimation\nproblems. Whereas cameras paired with deep learning algorithms usually address\nspecies classification or forest anomaly detection. In either of these cases,\ndata unavailability and forest diversity restrain deep learning developments\nfor autonomous systems. So, we propose two densely annotated image datasets -\n43k synthetic, 100 real - for bounding box, segmentation mask and keypoint\ndetections to assess the potential of vision-based methods. Deep neural network\nmodels trained on our datasets achieve a precision of 90.4% for tree detection,\n87.2% for tree segmentation, and centimeter accurate keypoint estimations. We\nmeasure our models' generalizability when testing it on other forest datasets,\nand their scalability with different dataset sizes and architectural\nimprovements. Overall, the experimental results offer promising avenues toward\nautonomous tree felling operations and other applied forestry problems. The\ndatasets and pre-trained models in this article are publicly available on\n\\href{https://github.com/norlab-ulaval/PercepTreeV1}{GitHub}\n(https://github.com/norlab-ulaval/PercepTreeV1).",
    "descriptor": "",
    "authors": [
      "Vincent Grondin",
      "Jean-Michel Fortin",
      "Fran\u00e7ois Pomerleau",
      "Philippe Gigu\u00e8re"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17424"
  },
  {
    "id": "arXiv:2210.17426",
    "title": "Consistent and Truthful Interpretation with Fourier Analysis",
    "abstract": "For many interdisciplinary fields, ML interpretations need to be consistent\nwith what-if scenarios related to the current case, i.e., if one factor\nchanges, how does the model react? Although the attribution methods are\nsupported by the elegant axiomatic systems, they mainly focus on individual\ninputs, and are generally inconsistent. To support what-if scenarios, we\nintroduce a new notion called truthful interpretation, and apply Fourier\nanalysis of Boolean functions to get rigorous guarantees. Experimental results\nshow that for neighborhoods with various radii, our method achieves 2x - 50x\nlower interpretation error compared with the other methods.",
    "descriptor": "",
    "authors": [
      "Yifan Zhang",
      "Haowei He",
      "Yang Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17426"
  },
  {
    "id": "arXiv:2210.17429",
    "title": "The power of the Binary Value Principle",
    "abstract": "The (extended) Binary Value Principle (eBVP: $\\sum_{i=1}^n x_i2^{i-1} = -k$\nfor $k>0$ and $x^2_i=x_i$) has received a lot of attention recently: several\nlower bounds have been proved for it (Alekseev et al 2020, Alekseev 2021, Part\nand Tzameret 2021), and a polynomial simulation of a strong semialgebraic proof\nsystem in IPS+eBVP has been shown (Alekseev et al 2020). In this paper we\nconsider Ext-PC: Polynomial Calculus with the algebraic version of Tseitin's\nextension rule. Contrary to IPS, this is a Cook--Reckhow proof system. We show\nthat in this context eBVP still allows to simulate similar semialgebraic\nsystems. We also prove that it allows to simulate the Square Root Rule\n(Grigoriev and Hirsch 2003), which is absolutely unclear in the context of\nordinary Polynomial Calculus. On the other hand, we demonstrate that eBVP\nprobably does not help in proving exponential lower bounds for Boolean\ntautologies: we show that an Ext-PC (even with the Square Root Rule) derivation\nof any such tautology from eBVP must be of exponential size.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Yaroslav Alekseev",
      "Edward A. Hirsch"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.17429"
  },
  {
    "id": "arXiv:2210.17432",
    "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for  Text Generation and Modular Control",
    "abstract": "Despite the growing success of diffusion models in continuous-valued domains\n(e.g., images), diffusion-based language models on discrete text have yet to\nmatch autoregressive language models on text generation benchmarks. In this\nwork, we present SSD-LM -- a diffusion language model with two key design\nchoices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of\ntext, allowing for flexible output length at decoding time while enabling local\nbidirectional context updates. Second, it is simplex-based, performing\ndiffusion on the natural vocabulary space rather than a learned latent space,\nallowing us to incorporate classifier guidance and modular control without any\nadaptation of off-the-shelf classifiers. We evaluate SSD-LM on unconstrained as\nwell as controlled text generation benchmarks, and show that it matches or\noutperforms strong autoregressive GPT-2 baselines across standard quality and\ndiversity metrics.",
    "descriptor": "",
    "authors": [
      "Xiaochuang Han",
      "Sachin Kumar",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17432"
  },
  {
    "id": "arXiv:2210.17437",
    "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes",
    "abstract": "It has been experimentally demonstrated that humans are able to learn in a\nmanner that allows them to make predictions on categories for which they have\nnot seen any examples (Malaviya et al., 2022). Sucholutsky and Schonlau (2020)\nhave recently presented a machine learning approach that aims to do the same.\nThey utilise synthetically generated data and demonstrate that it is possible\nto achieve sub-linear scaling and develop models that can learn to recognise N\nclasses from M training samples where M is less than N - aka less-than-one shot\nlearning. Their method was, however, defined for univariate or simple\nmultivariate data (Sucholutsky et al., 2021). We extend it to work on large,\nhigh-dimensional and real-world datasets and empirically validate it in this\nnew and challenging setting. We apply this method to learn previously unseen\nNLP tasks from very few examples (4, 8 or 16). We first generate compact,\nsophisticated less-than-one shot representations called soft-label prototypes\nwhich are fitted on training data, capturing the distribution of different\nclasses across the input domain space. We then use a modified k-Nearest\nNeighbours classifier to demonstrate that soft-label prototypes can classify\ndata competitively, even outperforming much more computationally complex\nfew-shot learning methods.",
    "descriptor": "\nComments: 13 pages, 2 figures\n",
    "authors": [
      "Avyav Kumar Singh",
      "Ekaterina Shutova",
      "Helen Yannakoudakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17437"
  },
  {
    "id": "arXiv:2210.17440",
    "title": "Semantic Novelty Detection and Characterization in Factual Text  Involving Named Entities",
    "abstract": "Much of the existing work on text novelty detection has been studied at the\ntopic level, i.e., identifying whether the topic of a document or a sentence is\nnovel or not. Little work has been done at the fine-grained semantic level (or\ncontextual level). For example, given that we know Elon Musk is the CEO of a\ntechnology company, the sentence \"Elon Musk acted in the sitcom The Big Bang\nTheory\" is novel and surprising because normally a CEO would not be an actor.\nExisting topic-based novelty detection methods work poorly on this problem\nbecause they do not perform semantic reasoning involving relations between\nnamed entities in the text and their background knowledge. This paper proposes\nan effective model (called PAT-SND) to solve the problem, which can also\ncharacterize the novelty. An annotated dataset is also created. Evaluation\nshows that PAT-SND outperforms 10 baselines by large margins.",
    "descriptor": "\nComments: 28 pages, 2 figures\n",
    "authors": [
      "Nianzu Ma",
      "Sahisnu Mazumder",
      "Alexander Politowicz",
      "Bing Liu",
      "Eric Robertson",
      "Scott Grigsby"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17440"
  },
  {
    "id": "arXiv:2210.17442",
    "title": "A Faster Approach to Spiking Deep Convolutional Neural Networks",
    "abstract": "Spiking neural networks (SNNs) have closer dynamics to the brain than current\ndeep neural networks. Their low power consumption and sample efficiency make\nthese networks interesting. Recently, several deep convolutional spiking neural\nnetworks have been proposed. These networks aim to increase biological\nplausibility while creating powerful tools to be applied to machine learning\ntasks. Here, we suggest a network structure based on previous work to improve\nnetwork runtime and accuracy. Improvements to the network include reducing\ntraining iterations to only once, effectively using principal component\nanalysis (PCA) dimension reduction, weight quantization, timed outputs for\nclassification, and better hyperparameter tuning. Furthermore, the\npreprocessing step is changed to allow the processing of colored images instead\nof only black and white to improve accuracy. The proposed structure\nfractionalizes runtime and introduces an efficient approach to deep\nconvolutional SNNs.",
    "descriptor": "\nComments: 6 pages, 7 figures, to be published in the Asilomar 2022 conference\n",
    "authors": [
      "Shahriar Rezghi Shirsavar",
      "Mohammad-Reza A. Dehaqani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.17442"
  },
  {
    "id": "arXiv:2210.17444",
    "title": "Multimodal Information Bottleneck: Learning Minimal Sufficient Unimodal  and Multimodal Representations",
    "abstract": "Learning effective joint embedding for cross-modal data has always been a\nfocus in the field of multimodal machine learning. We argue that during\nmultimodal fusion, the generated multimodal embedding may be redundant, and the\ndiscriminative unimodal information may be ignored, which often interferes with\naccurate prediction and leads to a higher risk of overfitting. Moreover,\nunimodal representations also contain noisy information that negatively\ninfluences the learning of cross-modal dynamics. To this end, we introduce the\nmultimodal information bottleneck (MIB), aiming to learn a powerful and\nsufficient multimodal representation that is free of redundancy and to filter\nout noisy information in unimodal representations. Specifically, inheriting\nfrom the general information bottleneck (IB), MIB aims to learn the minimal\nsufficient representation for a given task by maximizing the mutual information\nbetween the representation and the target and simultaneously constraining the\nmutual information between the representation and the input data. Different\nfrom general IB, our MIB regularizes both the multimodal and unimodal\nrepresentations, which is a comprehensive and flexible framework that is\ncompatible with any fusion methods. We develop three MIB variants, namely,\nearly-fusion MIB, late-fusion MIB, and complete MIB, to focus on different\nperspectives of information constraints. Experimental results suggest that the\nproposed method reaches state-of-the-art performance on the tasks of multimodal\nsentiment analysis and multimodal emotion recognition across three widely used\ndatasets. The codes are available at\n\\url{https://github.com/TmacMai/Multimodal-Information-Bottleneck}.",
    "descriptor": "\nComments: This paper is accepted by IEEE Transactions on Multimedia. This version addresses some mistakes and typos in the original paper. The appendix is available at this https URL\n",
    "authors": [
      "Sijie Mai",
      "Ying Zeng",
      "Haifeng Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17444"
  },
  {
    "id": "arXiv:2210.17449",
    "title": "Globally Gated Deep Linear Networks",
    "abstract": "Recently proposed Gated Linear Networks present a tractable nonlinear network\narchitecture, and exhibit interesting capabilities such as learning with local\nerror signals and reduced forgetting in sequential learning. In this work, we\nintroduce a novel gating architecture, named Globally Gated Deep Linear\nNetworks (GGDLNs) where gating units are shared among all processing units in\neach layer, thereby decoupling the architectures of the nonlinear but unlearned\ngatings and the learned linear processing motifs. We derive exact equations for\nthe generalization properties in these networks in the finite-width\nthermodynamic limit, defined by $P,N\\rightarrow\\infty, P/N\\sim O(1)$, where P\nand N are the training sample size and the network width respectively. We find\nthat the statistics of the network predictor can be expressed in terms of\nkernels that undergo shape renormalization through a data-dependent matrix\ncompared to the GP kernels. Our theory accurately captures the behavior of\nfinite width GGDLNs trained with gradient descent dynamics. We show that kernel\nshape renormalization gives rise to rich generalization properties w.r.t.\nnetwork width, depth and L2 regularization amplitude. Interestingly, networks\nwith sufficient gating units behave similarly to standard ReLU networks.\nAlthough gatings in the model do not participate in supervised learning, we\nshow the utility of unsupervised learning of the gating parameters.\nAdditionally, our theory allows the evaluation of the network's ability for\nlearning multiple tasks by incorporating task-relevant information into the\ngating units. In summary, our work is the first exact theoretical solution of\nlearning in a family of nonlinear networks with finite width. The rich and\ndiverse behavior of the GGDLNs suggests that they are helpful analytically\ntractable models of learning single and multiple tasks, in finite-width\nnonlinear deep networks.",
    "descriptor": "",
    "authors": [
      "Qianyi Li",
      "Haim Sompolinsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.17449"
  },
  {
    "id": "arXiv:2210.17451",
    "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning",
    "abstract": "Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.",
    "descriptor": "\nComments: Accepted by EMNLP 2022. arXiv admin note: substantial text overlap with arXiv:2205.12410\n",
    "authors": [
      "Yaqing Wang",
      "Sahaj Agarwal",
      "Subhabrata Mukherjee",
      "Xiaodong Liu",
      "Jing Gao",
      "Ahmed Hassan Awadallah",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17451"
  },
  {
    "id": "arXiv:2210.17452",
    "title": "A Case Study of Chinese Sentiment Analysis on Social Media Reviews Based  on LSTM",
    "abstract": "Network public opinion analysis is obtained by a combination of natural\nlanguage processing (NLP) and public opinion supervision, and is crucial for\nmonitoring public mood and trends. Therefore, network public opinion analysis\ncan identify and solve potential and budding social problems. This study aims\nto realize an analysis of Chinese sentiment in social media reviews using a\nlong short-term memory network (LSTM) model. The dataset was obtained from Sina\nWeibo using a web crawler and was cleaned with Pandas. First, Chinese comments\nregarding the legal sentencing in of Tangshan attack and Jiang Ge Case were\nsegmented and vectorized. Then, a binary LSTM model was trained and tested.\nFinally, sentiment analysis results were obtained by analyzing the comments\nwith the LSTM model. The accuracy of the proposed model has reached\napproximately 92%.",
    "descriptor": "",
    "authors": [
      "Lukai Wang",
      "Lei Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17452"
  },
  {
    "id": "arXiv:2210.17457",
    "title": "Agglomeration of Polygonal Grids using Graph Neural Networks with  applications to Multigrid solvers",
    "abstract": "Agglomeration-based strategies are important both within adaptive refinement\nalgorithms and to construct scalable multilevel algebraic solvers. In order to\nautomatically perform agglomeration of polygonal grids, we propose the use of\nGraph Neural Networks (GNNs) to partition the connectivity graph of a\ncomputational mesh. GNNs have the advantage to process naturally and\nsimultaneously both the graph structure of mesh and the geometrical\ninformation, such as the areas of the elements or their barycentric\ncoordinates. This is not the case with other approaches such as METIS, a\nstandard algorithm for graph partitioning which is meant to process only the\ngraph information, or the k-means clustering algorithm, which can process only\nthe geometrical information. Performance in terms of quality metrics is\nenhanced for Machine Learning (ML) strategies, with GNNs featuring a lower\ncomputational cost online. Such models also show a good degree of\ngeneralization when applied to more complex geometries, such as brain MRI\nscans, and the capability of preserving the quality of the grid. The\neffectiveness of these strategies is demonstrated also when applied to\nMultiGrid (MG) solvers in a Polygonal Discontinuous Galerkin (PolyDG)\nframework.",
    "descriptor": "",
    "authors": [
      "P. F. Antonietti",
      "N. Farenga",
      "E. Manuzzi",
      "G. Martinelli",
      "L. Saverio"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17457"
  },
  {
    "id": "arXiv:2210.17463",
    "title": "Domain Curricula for Code-Switched MT at MixMT 2022",
    "abstract": "In multilingual colloquial settings, it is a habitual occurrence to compose\nexpressions of text or speech containing tokens or phrases of different\nlanguages, a phenomenon popularly known as code-switching or code-mixing (CMX).\nWe present our approach and results for the Code-mixed Machine Translation\n(MixMT) shared task at WMT 2022: the task consists of two subtasks, monolingual\nto code-mixed machine translation (Subtask-1) and code-mixed to monolingual\nmachine translation (Subtask-2). Most non-synthetic code-mixed data are from\nsocial media but gathering a significant amount of this kind of data would be\nlaborious and this form of data has more writing variation than other domains,\nso for both subtasks, we experimented with data schedules for out-of-domain\ndata. We jointly learn multiple domains of text by pretraining and fine-tuning,\ncombined with a sentence alignment objective. We found that switching between\ndomains caused improved performance in the domains seen earliest during\ntraining, but depleted the performance on the remaining domains. A continuous\ntraining run with strategically dispensed data of different domains showed a\nsignificantly improved performance over fine-tuning.",
    "descriptor": "\nComments: The paper composed of 6 pages, it contains 3 figures and 4 tables, it has been accepted at the EMNLP 2022, SEVENTH CONFERENCE ON MACHINE TRANSLATION (WMT22)\n",
    "authors": [
      "Lekan Raheem",
      "Maab Elrashid"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17463"
  },
  {
    "id": "arXiv:2210.17464",
    "title": "Visualising Generative Spaces Using Convolutional Neural Network  Embeddings",
    "abstract": "As academic interest in procedural content generation (PCG) for games has\nincreased, so has the need for methodologies for comparing and contrasting the\noutput spaces of alternative PCG systems. In this paper we introduce and\nevaluate a novel approach for visualising the generative spaces of level\ngeneration systems, using embeddings extracted from a trained convolutional\nneural network. We evaluate the approach in terms of its ability to produce 2D\nvisualisations of encoded game levels that correlate with their behavioural\ncharacteristics. The results across two alternative game domains, Super Mario\nand Boxoban, indicate that this approach is powerful in certain settings and\nthat it has the potential to supersede alternative methods for visually\ncomparing generative spaces. However its performance was also inconsistent\nacross the domains investigated in this work, as well as it being susceptible\nto intermittent failure. We conclude that this method is worthy of further\nevaluation, but that future implementations of it would benefit from\nsignificant refinement.",
    "descriptor": "\nComments: 8 Pages, 4 Figures, To be published in the proceedings of the Experimental Games Workshop 2022 (EXAG) at AIIDE 2022\n",
    "authors": [
      "Oliver Withington",
      "Laurissa Tokarchuk"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.17464"
  },
  {
    "id": "arXiv:2210.17467",
    "title": "Iterative Teaching by Data Hallucination",
    "abstract": "We consider the problem of iterative machine teaching, where a teacher\nsequentially provides examples based on the status of a learner under a\ndiscrete input space (i.e., a pool of finite samples), which greatly limits the\nteacher's capability. To address this issue, we study iterative teaching under\na continuous input space where the input example (i.e., image) can be either\ngenerated by solving an optimization problem or drawn directly from a\ncontinuous distribution. Specifically, we propose data hallucination teaching\n(DHT) where the teacher can generate input data intelligently based on labels,\nthe learner's status and the target concept. We study a number of challenging\nteaching setups (e.g., linear/neural learners in omniscient and black-box\nsettings). Extensive empirical results verify the effectiveness of DHT.",
    "descriptor": "\nComments: Technical Report (21 pages, 24 figures)\n",
    "authors": [
      "Zeju Qiu",
      "Weiyang Liu",
      "Tim Z. Xiao",
      "Zhen Liu",
      "Umang Bhatt",
      "Yucen Luo",
      "Adrian Weller",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17467"
  },
  {
    "id": "arXiv:2210.17469",
    "title": "Blind Asynchronous Over-the-Air Federated Edge Learning",
    "abstract": "Federated Edge Learning (FEEL) is a distributed machine learning technique\nwhere each device contributes to training a global inference model by\nindependently performing local computations with their data. More recently,\nFEEL has been merged with over-the-air computation (OAC), where the global\nmodel is calculated over the air by leveraging the superposition of analog\nsignals. However, when implementing FEEL with OAC, there is the challenge on\nhow to precode the analog signals to overcome any time misalignment at the\nreceiver. In this work, we propose a novel synchronization-free method to\nrecover the parameters of the global model over the air without requiring any\nprior information about the time misalignments. For that, we construct a convex\noptimization based on the norm minimization problem to directly recover the\nglobal model by solving a convex semi-definite program. The performance of the\nproposed method is evaluated in terms of accuracy and convergence via numerical\nexperiments. We show that our proposed algorithm is close to the ideal\nsynchronized scenario by $10\\%$, and performs $4\\times$ better than the simple\ncase where no recovering method is used.",
    "descriptor": "",
    "authors": [
      "Saeed Razavikia",
      "Jaume Anguera Peris",
      "Jose Mairton B. da Silva Jr",
      "Carlo Fischione"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17469"
  },
  {
    "id": "arXiv:2210.17471",
    "title": "Optimal Antenna Placement for Two-Antenna Near-Field Wireless Power  Transfer",
    "abstract": "Current trends in communication system design precipitate a change in the\noperating regime from the traditional far-field to the radiating near-field\n(Fresnel) region. We investigate the optimal transmit antenna placement for a\nmultiple-input single-output (MISO) wireless power transfer (WPT) system\ndesigned for a three-dimensional cuboid room under line-of-sight (LoS)\nconditions in the Fresnel region. We formulate an optimisation problem for\nmaximising the received power at the worst possible receiver location by\nconsidering the spherical nature of the electromagnetic (EM) wavefronts in the\nFresnel region while assuming perfect knowledge of the channel at the\ntransmitter. For the case of two transmit antennas, we derive a closed-form\nexpression for the optimal positioning of the antennas which is purely\ndetermined by the geometry of the environment. If the room contains locations\nwhere the far-field approximation holds, the proposed positioning is shown to\nreduce to the far-field solution. The analytical solution is validated through\nsimulation. Furthermore, the maximum received power at the locations yielding\nthe worst performance is quantified and the power gain over the optimal\nfar-field solution is presented. For the considered cuboid environment, we show\nthat a distributed antenna system is optimal in the Fresnel region, whereas a\nco-located antenna architecture is ideal for the far-field.",
    "descriptor": "\nComments: 7 pages, 3 figures, six page version of this paper has been submitted to IEEE ICC 2023\n",
    "authors": [
      "Kenneth MacSporran Mayer",
      "Laura Cottatellucci",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17471"
  },
  {
    "id": "arXiv:2210.17473",
    "title": "Chronic pain patient narratives allow for the estimation of current pain  intensity",
    "abstract": "Chronic pain is a multi-dimensional experience, and pain intensity plays an\nimportant part, impacting the patients emotional balance, psychology, and\nbehaviour. Standard self-reporting tools, such as the Visual Analogue Scale for\npain, fail to capture these impacts. Moreover, these tools are susceptible to a\ndegree of subjectivity, dependent on the patients clear understanding of how to\nuse them, social biases, and their ability to translate a complex experience to\na scale. To overcome these and other self-reporting challenges, pain intensity\nestimation has been previously studied based on facial expressions,\nelectroencephalograms, brain imaging, and autonomic features. However, to the\nbest of our knowledge, it has never been attempted to base this estimation on\nthe patient narratives of the personal experience of chronic pain, which is\nwhat we propose in this work. Indeed, in the clinical assessment and management\nof chronic pain, verbal communication is essential to convey information to\nphysicians that would otherwise not be easily accessible through standard\nreporting tools, since language, sociocultural, and psychosocial variables are\nintertwined. We show that language features from patient narratives indeed\nconvey information relevant for pain intensity estimation, and that our\ncomputational models can take advantage of that. Specifically, our results show\nthat patients with mild pain focus more on the use of verbs, whilst moderate\nand severe pain patients focus on adverbs, and nouns and adjectives,\nrespectively, and that these differences allow for the distinction between\nthese three pain classes.",
    "descriptor": "\nComments: 23 pages, 6 figures, 6 tables\n",
    "authors": [
      "Diogo A.P. Nunes",
      "Joana Ferreira-Gomes",
      "Carlos Vaz",
      "Daniela Oliveira",
      "Sofia Pimenta",
      "Fani Neto",
      "David Martins de Matos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17473"
  },
  {
    "id": "arXiv:2210.17474",
    "title": "A-LAQ: Adaptive Lazily Aggregated Quantized Gradient",
    "abstract": "Federated Learning (FL) plays a prominent role in solving machine learning\nproblems with data distributed across clients. In FL, to reduce the\ncommunication overhead of data between clients and the server, each client\ncommunicates the local FL parameters instead of the local data. However, when a\nwireless network connects clients and the server, the communication resource\nlimitations of the clients may prevent completing the training of the FL\niterations. Therefore, communication-efficient variants of FL have been widely\ninvestigated. Lazily Aggregated Quantized Gradient (LAQ) is one of the\npromising communication-efficient approaches to lower resource usage in FL.\nHowever, LAQ assigns a fixed number of bits for all iterations, which may be\ncommunication-inefficient when the number of iterations is medium to high or\nconvergence is approaching. This paper proposes Adaptive Lazily Aggregated\nQuantized Gradient (A-LAQ), which is a method that significantly extends LAQ by\nassigning an adaptive number of communication bits during the FL iterations. We\ntrain FL in an energy-constraint condition and investigate the convergence\nanalysis for A-LAQ. The experimental results highlight that A-LAQ outperforms\nLAQ by up to a $50$% reduction in spent communication energy and an $11$%\nincrease in test accuracy.",
    "descriptor": "",
    "authors": [
      "Afsaneh Mahmoudi",
      "Jos\u00e9 Mairton Barros Da Silva J\u00fanior",
      "Hossein S. Ghadikolaei",
      "Carlo Fischione"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17474"
  },
  {
    "id": "arXiv:2210.17475",
    "title": "Study of Manifold Geometry using Multiscale Non-Negative Kernel Graphs",
    "abstract": "Modern machine learning systems are increasingly trained on large amounts of\ndata embedded in high-dimensional spaces. Often this is done without analyzing\nthe structure of the dataset. In this work, we propose a framework to study the\ngeometric structure of the data. We make use of our recently introduced\nnon-negative kernel (NNK) regression graphs to estimate the point density,\nintrinsic dimension, and the linearity of the data manifold (curvature). We\nfurther generalize the graph construction and geometric estimation to multiple\nscale by iteratively merging neighborhoods in the input data. Our experiments\ndemonstrate the effectiveness of our proposed approach over other baselines in\nestimating the local geometry of the data manifolds on synthetic and real\ndatasets.",
    "descriptor": "\nComments: 7 pages (5+2), Under review\n",
    "authors": [
      "Carlos Hurtado",
      "Sarath Shekkizhar",
      "Javier Ruiz-Hidalgo",
      "Antonio Ortega"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17475"
  },
  {
    "id": "arXiv:2210.17479",
    "title": "kt-Safety: Graph Release via k-Anonymity and t-Closeness (Technical  Report)",
    "abstract": "In a wide spectrum of real-world applications, it is very important to\nanalyze and mine graph data such as social networks, communication networks,\ncitation networks, and so on. However, the release of such graph data often\nraises privacy issue, and the graph privacy preservation has recently drawn\nmuch attention from the database community. While prior works on graph privacy\npreservation mainly focused on protecting the privacy of either the graph\nstructure only or vertex attributes only, in this paper, we propose a novel\nmechanism for graph privacy preservation by considering attacks from both graph\nstructures and vertex attributes, which transforms the original graph to a\nso-called kt-safe graph, via k-anonymity and t-closeness. We prove that the\ngeneration of a kt-safe graph is NP-hard, therefore, we propose a feasible\nframework for effectively and efficiently anonymizing a graph with low\nanonymization cost. In particular, we design a cost-model-based graph\npartitioning approach to enable our proposed divide-and-conquer strategy for\nthe graph anonymization, and propose effective optimization techniques such as\npruning method and a tree synopsis to improve the anonymization efficiency over\nlarge-scale graphs. Extensive experiments have been conducted to verify the\nefficiency and effectiveness of our proposed kt-safe graph generation approach\non both real and synthetic data sets.",
    "descriptor": "\nComments: 22 pages, 31 figures, the technical report of a TKDE paper entitled \"kt-Safety: Graph Release via k-Anonymity and t-Closeness\"\n",
    "authors": [
      "Weilong Ren",
      "Kambiz Ghazinour",
      "Xiang Lian"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.17479"
  },
  {
    "id": "arXiv:2210.17482",
    "title": "Low Complexity Detect and Avoid for Autonomous Agents in Cluttered  Environments",
    "abstract": "In this paper, we propose a fast and low complexity detect and avoid (DAA)\nalgorithm for unknown cluttered environments. Examples of such scenarios\ninclude but are not limited to various terrestrial missions e.g., search and\nrescue missions in jungles, and in space-exploration e.g., navigation of rovers\non the Moon. The proposed method is based on the concept of artificial\npotential fields (APF) in which the target is attractive while the obstacles\nare repulsive to the mobile agent. To the original APF algorithm (by Khatib),\nwe propose two major updates which significantly improve the performance of DAA\nusing APF. First, we propose to improve an existing classical method that\nreplaces the gradient descent optimization of the potential field cost function\non a continuous domain with a combinatorial optimization on a set of predefined\npoints (called bacteria points) around the agent's current location. Our\nproposition includes an adaptive hyperparameter that changes the value of the\npotential function associated to each bacteria point based on the current\nenvironmental measurements. Our proposed solution improves the navigation\nperformance in terms of convergence to the target at the expense of minimal\nincrease in computational complexity. Second, we propose an improved potential\nfield cost function of the bacteria points by introducing a new branching cost\nfunction which further improves the navigation performance. The algorithms were\ntested on a set of Monte Carlo simulation trials where the environment changes\nfor each trial. Our simulation results show 25% lower navigation time and\naround 200% higher success rate compared to the conventional potential field\nmethod.",
    "descriptor": "",
    "authors": [
      "Mosab Diab",
      "Mostafa Mohammadkarimi",
      "Raj Thilak Rajan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17482"
  },
  {
    "id": "arXiv:2210.17484",
    "title": "The Open MatSci ML Toolkit: A Flexible Framework for Machine Learning in  Materials Science",
    "abstract": "We present the Open MatSci ML Toolkit: a flexible, self-contained, and\nscalable Python-based framework to apply deep learning models and methods on\nscientific data with a specific focus on materials science and the OpenCatalyst\nDataset. Our toolkit provides: 1. A scalable machine learning workflow for\nmaterials science leveraging PyTorch Lightning, which enables seamless scaling\nacross different computation capabilities (laptop, server, cluster) and\nhardware platforms (CPU, GPU, XPU). 2. Deep Graph Library (DGL) support for\nrapid graph neural network prototyping and development. By publishing and\nsharing this toolkit with the research community via open-source release, we\nhope to: 1. Lower the entry barrier for new machine learning researchers and\npractitioners that want to get started with the OpenCatalyst dataset, which\npresently comprises the largest computational materials science dataset. 2.\nEnable the scientific community to apply advanced machine learning tools to\nhigh-impact scientific challenges, such as modeling of materials behavior for\nclean energy applications. We demonstrate the capabilities of our framework by\nenabling three new equivariant neural network models for multiple OpenCatalyst\ntasks and arrive at promising results for compute scaling and model\nperformance.",
    "descriptor": "\nComments: Paper accompanying Open-Source Software from this https URL\n",
    "authors": [
      "Santiago Miret",
      "Kin Long Kelvin Lee",
      "Carmelo Gonzales",
      "Marcel Nassar",
      "Matthew Spellings"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17484"
  },
  {
    "id": "arXiv:2210.17486",
    "title": "Learning Modular Robot Visual-motor Locomotion Policies",
    "abstract": "Control policy learning for modular robot locomotion has previously been\nlimited to proprioceptive feedback and flat terrain. This paper develops\npolicies for modular systems with vision traversing more challenging\nenvironments. These modular robots can be reconfigured to form many different\ndesigns, where each design needs a controller to function. Though one could\ncreate a policy for individual designs and environments, such an approach is\nnot scalable given the wide range of potential designs and environments. To\naddress this challenge, we create a visual-motor policy that can generalize to\nboth new designs and environments. The policy itself is modular, in that it is\ndivided into components, each of which corresponds to a type of module (e.g., a\nleg, wheel, or body). The policy components can be recombined during training\nto learn to control multiple designs. We develop a deep reinforcement learning\nalgorithm where visual observations are input to a modular policy interacting\nwith multiple environments at once. We apply this algorithm to train robots\nwith combinations of legs and wheels, then demonstrate the policy controlling\nreal robots climbing stairs and curbs.",
    "descriptor": "",
    "authors": [
      "Julian Whitman",
      "Howie Choset"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.17486"
  },
  {
    "id": "arXiv:2210.17490",
    "title": "Quantum-Inspired Edge Detection Algorithms Implementation using New  Dynamic Visual Data Representation and Short-Length Convolution Computation",
    "abstract": "As the availability of imagery data continues to swell, so do the demands on\ntransmission, storage and processing power. Processing requirements to handle\nthis plethora of data is quickly outpacing the utility of conventional\nprocessing techniques. Transitioning to quantum processing and algorithms that\noffer promising efficiencies over conventional methods can address some of\nthese issues. However, to make this transformation possible, fundamental issues\nof implementing real time Quantum algorithms must be overcome for crucial\nprocesses needed for intelligent analysis applications. For example, consider\nedge detection tasks which require time-consuming acquisition processes and are\nfurther hindered by the complexity of the devices used thus limiting\nfeasibility for implementation in real-time applications. Convolution is\nanother example of an operation that is essential for signal and image\nprocessing applications, where the mathematical operations consist of an\nintelligent mixture of multiplication and addition that require considerable\ncomputational resources. This paper studies a new paired transform-based\nquantum representation and computation of one-dimensional and 2-D signals\nconvolutions and gradients. A new visual data representation is defined to\nsimplify convolution calculations making it feasible to parallelize convolution\nand gradient operations for more efficient performance. The new data\nrepresentation is demonstrated on multiple illustrative examples for quantum\nedge detection, gradients, and convolution. Furthermore, the efficiency of the\nproposed approach is shown on real-world images.",
    "descriptor": "\nComments: 11 pages, 11 figures\n",
    "authors": [
      "Artyom M. Grigoryan",
      "Sos S. Agaian",
      "Karen Panetta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantum Algebra (math.QA)"
    ],
    "url": "https://arxiv.org/abs/2210.17490"
  },
  {
    "id": "arXiv:2210.17491",
    "title": "Learning Modular Robot Locomotion from Demonstrations",
    "abstract": "Modular robots can be reconfigured to create a variety of designs from a\nsmall set of components. But constructing a robot's hardware on its own is not\nenough -- each robot needs a controller. One could create controllers for some\ndesigns individually, but developing policies for additional designs can be\ntime consuming. This work presents a method that uses demonstrations from one\nset of designs to accelerate policy learning for additional designs. We\nleverage a learning framework in which a graph neural network is made up of\nmodular components, each component corresponds to a type of module (e.g., a\nleg, wheel, or body) and these components can be recombined to learn from\nmultiple designs at once. In this paper we develop a combined reinforcement and\nimitation learning algorithm. Our method is novel because the policy is\noptimized to both maximize a reward for one design, and simultaneously imitate\ndemonstrations from different designs, within one objective function. We show\nthat when the modular policy is optimized with this combined objective,\ndemonstrations from one set of designs influence how the policy behaves on a\ndifferent design, decreasing the number of training iterations needed.",
    "descriptor": "",
    "authors": [
      "Julian Whitman",
      "Howie Choset"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17491"
  },
  {
    "id": "arXiv:2210.17495",
    "title": "Automated Code Extraction from Discussion Board Text Dataset",
    "abstract": "This study introduces and investigates the capabilities of three different\ntext mining approaches, namely Latent Semantic Analysis, Latent Dirichlet\nAnalysis, and Clustering Word Vectors, for automating code extraction from a\nrelatively small discussion board dataset. We compare the outputs of each\nalgorithm with a previous dataset that was manually coded by two human raters.\nThe results show that even with a relatively small dataset, automated\napproaches can be an asset to course instructors by extracting some of the\ndiscussion codes, which can be used in Epistemic Network Analysis.",
    "descriptor": "",
    "authors": [
      "Sina Mahdipour Saravani",
      "Sadaf Ghaffari",
      "Yanye Luther",
      "James Folkestad",
      "Marcia Moraes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17495"
  },
  {
    "id": "arXiv:2210.17497",
    "title": "Leveraging Pre-trained Models for Failure Analysis Triplets Generation",
    "abstract": "Pre-trained Language Models recently gained traction in the Natural Language\nProcessing (NLP) domain for text summarization, generation and\nquestion-answering tasks. This stems from the innovation introduced in\nTransformer models and their overwhelming performance compared with Recurrent\nNeural Network Models (Long Short Term Memory (LSTM)). In this paper, we\nleverage the attention mechanism of pre-trained causal language models such as\nTransformer model for the downstream task of generating Failure Analysis\nTriplets (FATs) - a sequence of steps for analyzing defected components in the\nsemiconductor industry. We compare different transformer models for this\ngenerative task and observe that Generative Pre-trained Transformer 2 (GPT2)\noutperformed other transformer model for the failure analysis triplet\ngeneration (FATG) task. In particular, we observe that GPT2 (trained on 1.5B\nparameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on\nROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric\n(LESE) for better evaluation of the structured FAT data and show that it\ncompares exactly with human judgment than existing metrics.",
    "descriptor": "\nComments: 33 pages, 11 figures, 9 tables\n",
    "authors": [
      "Kenneth Ezukwoke",
      "Anis Hoayek",
      "Mireille Batton-Hubert",
      "Xavier Boucher",
      "Pascal Gounet",
      "Jerome Adrian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.17497"
  },
  {
    "id": "arXiv:2210.17501",
    "title": "Fast Principal Component Analysis for Cryo-EM Images",
    "abstract": "Principal component analysis (PCA) plays an important role in the analysis of\ncryo-EM images for various tasks such as classification, denoising,\ncompression, and ab-initio modeling. We introduce a fast method for estimating\na compressed representation of the 2-D covariance matrix of noisy cryo-electron\nmicroscopy projection images that enables fast PCA computation. Our method is\nbased on a new algorithm for expanding images in the Fourier-Bessel basis (the\nharmonics on the disk), which provides a convenient way to handle the effect of\nthe contrast transfer functions. For $N$ images of size $L\\times L$, our method\nhas time complexity $O(N L^3 + L^4)$ and space complexity $O(NL^2 + L^3)$. In\ncontrast to previous work, these complexities are independent of the number of\ndifferent contrast transfer functions of the images. We demonstrate our\napproach on synthetic and experimental data and show acceleration by factors of\nup to two orders of magnitude.",
    "descriptor": "\nComments: 16 pages, 7 figures, 2 tables\n",
    "authors": [
      "Nicholas F. Marshall",
      "Oscar Mickelin",
      "Yunpeng Shi",
      "Amit Singer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.17501"
  },
  {
    "id": "arXiv:2210.17505",
    "title": "Space-fluid Adaptive Sampling by Self-Organisation",
    "abstract": "A recurrent task in coordinated systems is managing (estimating, predicting,\nor controlling) signals that vary in space, such as distributed sensed data or\ncomputation outcomes. Especially in large-scale settings, the problem can be\naddressed through decentralised and situated computing systems: nodes can\nlocally sense, process, and act upon signals, and coordinate with neighbours to\nimplement collective strategies. Accordingly, in this work we devise\ndistributed coordination strategies for the estimation of a spatial phenomenon\nthrough collaborative adaptive sampling. Our design is based on the idea of\ndynamically partitioning space into regions that compete and grow/shrink to\nprovide accurate aggregate sampling. Such regions hence define a sort of\nvirtualised space that is \"fluid\", since its structure adapts in response to\npressure forces exerted by the underlying phenomenon. We provide an adaptive\nsampling algorithm in the field-based coordination framework, and prove it is\nself-stabilising and locally optimal. Finally, we verify by simulation that the\nproposed algorithm effectively carries out a spatially adaptive sampling while\nmaintaining a tuneable trade-off between accuracy and efficiency.",
    "descriptor": "\nComments: 27 pages, 11 figures\n",
    "authors": [
      "Roberto Casadei",
      "Stefano Mariani",
      "Danilo Pianini",
      "Mirko Viroli",
      "Franco Zambonelli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17505"
  },
  {
    "id": "arXiv:2210.17508",
    "title": "A model of actors and grey failures",
    "abstract": "Existing models for the analysis of concurrent processes tend to focus on\nfail-stop failures, where processes are either working or permanently stopped,\nand their state (working/stopped) is known. In fact, systems are often affected\nby grey failures: failures that are latent, possibly transient, and may affect\nthe system in subtle ways that later lead to major issues (such as crashes,\nlimited availability, overload). We introduce a model of actor-based systems\nwith grey failures, based on two interlinked layers: an actor model, given as\nan asynchronous process calculus with discrete time, and a failure model that\nrepresents failure patterns to inject in the system. Our failure model captures\nnot only fail-stop node and link failures, but also grey failures (e.g.,\npartial, transient). We give a behavioural equivalence relation based on weak\nbarbed bisimulation to compare systems on the basis of their ability to recover\nfrom failures, and on this basis we define some desirable properties of\nreliable systems. By doing so, we reduce the problem of checking reliability\nproperties of systems to the problem of checking bisimulation.",
    "descriptor": "",
    "authors": [
      "Laura Bocchi",
      "Julien Lange",
      "Simon Thompson",
      "A. Laura Voinea"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2210.17508"
  },
  {
    "id": "arXiv:2210.17511",
    "title": "Examining the Landscape of Digital Safety and Privacy Assistance for  Black Communities",
    "abstract": "Recent events have placed a renewed focus on the issue of racial justice in\nthe United States and other countries. One dimension of this issue that has\nreceived considerable attention is the security and privacy threats and\nvulnerabilities faced by the communities of color.\nOur study focuses on community-level advocates who organize workshops,\nclinics, and other initiatives that inform Black communities about existing\ndigital safety and privacy threats and ways to mitigate against them.\nAdditionally, we aim to understand the online security and privacy needs and\nattitudes of participants who partake in these initiatives. We hope that by\nunderstanding how advocates work in different contexts and what teaching\nmethods are effective, we can help other digital safety experts and activists\nbecome advocates within their communities.",
    "descriptor": "\nComments: Appears in the Workshop on Inclusive Privacy and Security (WIPS) 2021 co-located with Symposium on Usable Privacy and Security (SOUPS)\n",
    "authors": [
      "Nikita Samarin",
      "Aparna Krishnan",
      "Moses Namara",
      "Joanne Ma",
      "Elissa M. Redmiles"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.17511"
  },
  {
    "id": "arXiv:2210.17514",
    "title": "Cost-aware Generalized $\u03b1$-investing for Multiple Hypothesis  Testing",
    "abstract": "We consider the problem of sequential multiple hypothesis testing with\nnontrivial data collection cost. This problem appears, for example, when\nconducting biological experiments to identify differentially expressed genes in\na disease process. This work builds on the generalized $\\alpha$-investing\nframework that enables control of the false discovery rate in a sequential\ntesting setting. We make a theoretical analysis of the long term asymptotic\nbehavior of $\\alpha$-wealth which motivates a consideration of sample size in\nthe $\\alpha$-investing decision rule. Using the game theoretic principle of\nindifference, we construct a decision rule that optimizes the expected return\n(ERO) of $\\alpha$-wealth and provides an optimal sample size for the test. We\nshow empirical results that a cost-aware ERO decision rule correctly rejects\nmore false null hypotheses than other methods. We extend cost-aware ERO\ninvesting to finite-horizon testing which enables the decision rule to hedge\nagainst the risk of unproductive tests. Finally, empirical tests on a real data\nset from a biological experiment show that cost-aware ERO produces actionable\ndecisions as to which tests to conduct and if so at what sample size.",
    "descriptor": "\nComments: 21 pages, 4 figures, 7 tables\n",
    "authors": [
      "Thomas Cook",
      "Harsh Vardhan Dubey",
      "Ji Ah Lee",
      "Guangyu Zhu",
      "Tingting Zhao",
      "Patrick Flaherty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.17514"
  },
  {
    "id": "arXiv:2210.17515",
    "title": "Beating $(1-1/e)$-Approximation for Weighted Stochastic Matching",
    "abstract": "In the stochastic weighted matching problem, the goal is to find a\nlarge-weight matching of a graph when we are uncertain about the existence of\nits edges. In particular, each edge $e$ has a known weight $w_e$ but is\nrealized independently with some probability $p_e$. The algorithm may query an\nedge to see whether it is realized. We consider the well-studied query commit\nversion of the problem, in which any queried edge that happens to be realized\nmust be included in the solution.\nGamlath, Kale, and Svensson showed that when the input graph is bipartite,\nthe problem admits a $(1-1/e)$-approximation. In this paper, we give an\nalgorithm that for an absolute constant $\\delta > 0.0014$ obtains a\n$(1-1/e+\\delta)$-approximation, therefore breaking this prevalent bound.",
    "descriptor": "",
    "authors": [
      "Mahsa Derakhshan",
      "Alireza Farhadi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.17515"
  },
  {
    "id": "arXiv:2210.17517",
    "title": "Lila: A Unified Benchmark for Mathematical Reasoning",
    "abstract": "Mathematical reasoning skills are essential for general-purpose intelligent\nsystems to perform tasks from grocery shopping to climate modeling. Towards\nevaluating and improving AI systems in this domain, we propose LILA, a unified\nmathematical reasoning benchmark consisting of 23 diverse tasks along four\ndimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language\nformat e.g., question-answering, fill-in-the-blanks (iii) language diversity\ne.g., no language, simple language (iv) external knowledge e.g., commonsense,\nphysics. We construct our benchmark by extending 20 datasets benchmark by\ncollecting task instructions and solutions in the form of Python programs,\nthereby obtaining explainable solutions in addition to the correct answer. We\nadditionally introduce two evaluation datasets to measure out-of-distribution\nperformance and robustness to language perturbation. Finally, we introduce\nBHASKARA, a general-purpose mathematical reasoning model trained on LILA.\nImportantly, we find that multi-tasking leads to significant improvements\n(average relative improvement of 21.83% F1 score vs. single-task models), while\nthe best performing model only obtains 60.40%, indicating the room for\nimprovement in general mathematical reasoning and understanding.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Swaroop Mishra",
      "Matthew Finlayson",
      "Pan Lu",
      "Leonard Tang",
      "Sean Welleck",
      "Chitta Baral",
      "Tanmay Rajpurohit",
      "Oyvind Tafjord",
      "Ashish Sabharwal",
      "Peter Clark",
      "Ashwin Kalyan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17517"
  },
  {
    "id": "arXiv:2210.17518",
    "title": "Weak Galerkin methods based Morley elements on general polytopal  partitions",
    "abstract": "A new weak Galerkin method based on the weak tangential derivative and weak\nsecond order partial derivative is proposed to extend the well-known Morley\nelement for the biharmonic equation from triangular elements to general\npolytopal elements. The Schur complement of the weak Galerkin scheme not only\nenjoys the same degrees of freedom as the Morley element on the triangular\nelement but also extends the Morley element to any general polytopal element.\nThe error estimates for the numerical approximation are established in the\nenergy norm and the usual $L^2$ norms. Several numerical experiments are\ndemonstrated to validate the theory developed in this article.",
    "descriptor": "\nComments: 24 pages, 4 tables, 1 figure\n",
    "authors": [
      "Dan Li",
      "Chunmei Wang",
      "Junping Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.17518"
  },
  {
    "id": "arXiv:2210.17520",
    "title": "Fully Adaptive Composition for Gaussian Differential Privacy",
    "abstract": "We show that Gaussian Differential Privacy, a variant of differential privacy\ntailored to the analysis of Gaussian noise addition, composes gracefully even\nin the presence of a fully adaptive analyst. Such an analyst selects mechanisms\n(to be run on a sensitive data set) and their privacy budgets adaptively, that\nis, based on the answers from other mechanisms run previously on the same data\nset. In the language of Rogers, Roth, Ullman and Vadhan, this gives a filter\nfor GDP with the same parameters as for nonadaptive composition.",
    "descriptor": "",
    "authors": [
      "Adam Smith",
      "Abhradeep Thakurta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17520"
  },
  {
    "id": "arXiv:2210.17525",
    "title": "Query Refinement Prompts for Closed-Book Long-Form Question Answering",
    "abstract": "Large language models (LLMs) have been shown to perform well in answering\nquestions and in producing long-form texts, both in few-shot closed-book\nsettings. While the former can be validated using well-known evaluation\nmetrics, the latter is difficult to evaluate. We resolve the difficulties to\nevaluate long-form output by doing both tasks at once -- to do question\nanswering that requires long-form answers. Such questions tend to be\nmultifaceted, i.e., they may have ambiguities and/or require information from\nmultiple sources. To this end, we define query refinement prompts that\nencourage LLMs to explicitly express the multifacetedness in questions and\ngenerate long-form answers covering multiple facets of the question. Our\nexperiments on two long-form question answering datasets, ASQA and AQuAMuSe,\nshow that using our prompts allows us to outperform fully finetuned models in\nthe closed book setting, as well as achieve results comparable to\nretrieve-then-generate open-book models.",
    "descriptor": "",
    "authors": [
      "Reinald Kim Amplayo",
      "Kellie Webster",
      "Michael Collins",
      "Dipanjan Das",
      "Shashi Narayan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17525"
  },
  {
    "id": "arXiv:2210.17540",
    "title": "Agent-Time Attention for Sparse Rewards Multi-Agent Reinforcement  Learning",
    "abstract": "Sparse and delayed rewards pose a challenge to single agent reinforcement\nlearning. This challenge is amplified in multi-agent reinforcement learning\n(MARL) where credit assignment of these rewards needs to happen not only across\ntime, but also across agents. We propose Agent-Time Attention (ATA), a neural\nnetwork model with auxiliary losses for redistributing sparse and delayed\nrewards in collaborative MARL. We provide a simple example that demonstrates\nhow providing agents with their own local redistributed rewards and shared\nglobal redistributed rewards motivate different policies. We extend several\nMiniGrid environments, specifically MultiRoom and DoorKey, to the multi-agent\nsparse delayed rewards setting. We demonstrate that ATA outperforms various\nbaselines on many instances of these environments. Source code of the\nexperiments is available at https://github.com/jshe/agent-time-attention.",
    "descriptor": "\nComments: Full version of the Extended Abstract accepted at the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2022\n",
    "authors": [
      "Jennifer She",
      "Jayesh K. Gupta",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.17540"
  },
  {
    "id": "arXiv:2210.17541",
    "title": "Zero-Shot Text Classification with Self-Training",
    "abstract": "Recent advances in large pretrained language models have increased attention\nto zero-shot text classification. In particular, models finetuned on natural\nlanguage inference datasets have been widely adopted as zero-shot classifiers\ndue to their promising results and off-the-shelf availability. However, the\nfact that such models are unfamiliar with the target task can lead to\ninstability and performance issues. We propose a plug-and-play method to bridge\nthis gap using a simple self-training approach, requiring only the class names\nalong with an unlabeled dataset, and without the need for domain expertise or\ntrial and error. We show that fine-tuning the zero-shot classifier on its most\nconfident predictions leads to significant performance gains across a wide\nrange of text classification tasks, presumably since self-training adapts the\nzero-shot model to the task at hand.",
    "descriptor": "\nComments: 9 pages, 5 figures; To be published in EMNLP 2022\n",
    "authors": [
      "Ariel Gera",
      "Alon Halfon",
      "Eyal Shnarch",
      "Yotam Perlitz",
      "Liat Ein-Dor",
      "Noam Slonim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17541"
  },
  {
    "id": "arXiv:2210.17543",
    "title": "High order splitting methods for SDEs satisfying a commutativity  condition",
    "abstract": "In this paper, we introduce a new simple approach to developing and\nestablishing the convergence of splitting methods for a large class of\nstochastic differential equations (SDEs), including additive, diagonal and\nscalar noise types. The central idea is to view the splitting method as a\nreplacement of the driving signal of an SDE, namely Brownian motion and time,\nwith a piecewise linear path that yields a sequence of ODEs -- which can be\ndiscretised to produce a numerical scheme. This new way of understanding\nsplitting methods is inspired by, but does not use, rough path theory. We show\nthat when the driving piecewise linear path matches certain iterated stochastic\nintegrals of Brownian motion, then a high order splitting method can be\nobtained. We propose a general proof methodology for establishing the strong\nconvergence of these approximations that is akin to the general framework of\nMilstein and Tretyakov. That is, once local error estimates are obtained for\nthe splitting method, then a global rate of convergence follows. This approach\ncan then be readily applied in future research on SDE splitting methods. By\nincorporating recently developed approximations for iterated integrals of\nBrownian motion into these piecewise linear paths, we propose several high\norder splitting methods for SDEs satisfying a certain commutativity condition.\nIn our experiments, which include the Cox-Ingersoll-Ross model and additive\nnoise SDEs (noisy anharmonic oscillator, stochastic FitzHugh-Nagumo model,\nunderdamped Langevin dynamics), the new splitting methods exhibit convergence\nrates of $O(h^{3/2})$ and outperform schemes previously proposed in the\nliterature.",
    "descriptor": "\nComments: 47 pages, 10 figures\n",
    "authors": [
      "James Foster",
      "Goncalo dos Reis",
      "Calum Strange"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.17543"
  },
  {
    "id": "arXiv:2210.17544",
    "title": "Compressed IF-TEM: Time Encoding Analog-To-Digital Compression",
    "abstract": "An integrate-and-fire time-encoding-machine (IF-TEM) is an energy-efficient\nasynchronous sampler. Utilizing the IF-TEM sampler for bandlimited signals, we\nintroduce designs for time encoding and decoding with analog compression prior\nto the quantization phase. Before the quantizer, efficient analog compression\nis conducted based on the stationarity of the encoded signal, which is a\nfundamental characteristic of IF-TEM processing. Low-bit-rate reconstruction is\nachieved by subdividing the known IF-TEM dynamic range into tighter windows,\nwhich can be either fixed size or dynamically changed, and detecting in which\nwindow the sample resides. We demonstrate empirically that employing the same\nnumber of samples and up to 7% additional bits than the conventional IF-TEM\nresults in a 5-20dB improvement in MSE. Fixing the reconstruction MSE target\nand the number of samples, using the compressed IF-TEM enables the use of 1-2\nfewer bits compared to the classical IF-TEM.",
    "descriptor": "",
    "authors": [
      "Saar Tarnopolsky",
      "Hila Naaman",
      "Yonina C. Eldar",
      "Alejandro Cohen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.17544"
  },
  {
    "id": "arXiv:2210.17546",
    "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense  of Privacy",
    "abstract": "Studying data memorization in neural language models helps us understand the\nrisks (e.g., to privacy or copyright) associated with models regurgitating\ntraining data, and aids in the evaluation of potential countermeasures. Many\nprior works -- and some recently deployed defenses -- focus on \"verbatim\nmemorization\", defined as a model generation that exactly matches a substring\nfrom the training set. We argue that verbatim memorization definitions are too\nrestrictive and fail to capture more subtle forms of memorization.\nSpecifically, we design and implement an efficient defense based on Bloom\nfilters that perfectly prevents all verbatim memorization. And yet, we\ndemonstrate that this \"perfect\" filter does not prevent the leakage of training\ndata. Indeed, it is easily circumvented by plausible and minimally modified\n\"style-transfer\" prompts -- and in some cases even the non-modified original\nprompts -- to extract memorized information. For example, instructing the model\nto output ALL-CAPITAL texts bypasses memorization checks based on verbatim\nmatching. We conclude by discussing potential alternative definitions and why\ndefining memorization is a difficult yet crucial open question for neural\nlanguage models.",
    "descriptor": "",
    "authors": [
      "Daphne Ippolito",
      "Florian Tram\u00e8r",
      "Milad Nasr",
      "Chiyuan Zhang",
      "Matthew Jagielski",
      "Katherine Lee",
      "Christopher A. Choquette-Choo",
      "Nicholas Carlini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.17546"
  },
  {
    "id": "arXiv:2210.16307",
    "title": "Investigation of chemical structure recognition by encoder-decoder  models in learning progress",
    "abstract": "Descriptor generation methods using latent representations of\nencoder$-$decoder (ED) models with SMILES as input are useful because of the\ncontinuity of descriptor and restorability to the structure. However, it is not\nclear how the structure is recognized in the learning progress of ED models. In\nthis work, we created ED models of various learning progress and investigated\nthe relationship between structural information and learning progress. We\nshowed that compound substructures were learned early in ED models by\nmonitoring the accuracy of downstream tasks and input$-$output substructure\nsimilarity using substructure$-$based descriptors, which suggests that existing\nevaluation methods based on the accuracy of downstream tasks may not be\nsensitive enough to evaluate the performance of ED models with SMILES as\ndescriptor generation methods. On the other hand, we showed that structure\nrestoration was time$-$consuming, and in particular, insufficient learning led\nto the estimation of a larger structure than the actual one. It can be inferred\nthat determining the endpoint of the structure is a difficult task for the\nmodel. To our knowledge, this is the first study to link the learning progress\nof SMILES by ED model to chemical structures for a wide range of chemicals.",
    "descriptor": "\nComments: 17 pages, 4 figures\n",
    "authors": [
      "Shumpei Nemoto",
      "Tadahaya Mizuno",
      "Hiroyuki Kusuhara"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2210.16307"
  },
  {
    "id": "arXiv:2210.16311",
    "title": "Simultaneous off-the-grid learning of mixtures issued from a continuous  dictionary",
    "abstract": "In this paper we observe a set, possibly a continuum, of signals corrupted by\nnoise. Each signal is a finite mixture of an unknown number of features\nbelonging to a continuous dictionary. The continuous dictionary is parametrized\nby a real non-linear parameter. We shall assume that the signals share an\nunderlying structure by saying that the union of active features in the whole\ndataset is finite. We formulate regularized optimization problems to estimate\nsimultaneously the linear coefficients in the mixtures and the non-linear\nparameters of the features. The optimization problems are composed of a data\nfidelity term and a (l1 , Lp)-penalty. We prove high probability bounds on the\nprediction errors associated to our estimators. The proof is based on the\nexistence of certificate functions. Following recent works on the geometry of\noff-the-grid methods, we show that such functions can be constructed provided\nthe parameters of the active features are pairwise separated by a constant with\nrespect to a Riemannian metric. When the number of signals is finite and the\nnoise is assumed Gaussian, we give refinements of our results for p = 1 and p =\n2 using tail bounds on suprema of Gaussian and $\\chi$2 random processes. When p\n= 2, our prediction error reaches the rates obtained by the Group-Lasso\nestimator in the multi-task linear regression model.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2207.00171\n",
    "authors": [
      "Cristina Butucea",
      "Jean-Fran\u00e7ois Delmas",
      "Anne Dutfoy",
      "Cl\u00e9ment Hardy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.16311"
  },
  {
    "id": "arXiv:2210.16373",
    "title": "Continuous Attribution of Episodical Outcomes for More Efficient and  Targeted Online Measurement",
    "abstract": "Online experimentation platforms collect user feedback at low cost and large\nscale. Some systems even support real-time or near real-time data processing,\nand can update metrics and statistics continuously. Many commonly used metrics,\nsuch as clicks and page views, can be observed without much delay. However,\nmany important signals can only be observed after several hours or days, with\nnoise adding up over the duration of the episode. When episodical outcomes\nfollow a complex sequence of user-product interactions, it is difficult to\nunderstand which interactions lead to the final outcome. There is no obvious\nattribution logic for us to associate a positive or negative outcome back to\nthe actions and choices we made at different times. This attribution logic is\ncritical to unlocking more targeted and efficient measurement at a finer\ngranularity that could eventually lead to the full capability of reinforcement\nlearning. In this paper, we borrow the idea of Causal Surrogacy to model a\nlong-term outcome using leading indicators that are incrementally observed and\napply it as the value function to track the progress towards the final outcome\nand attribute incrementally to various user-product interaction steps. Applying\nthis approach to the guest booking metric at Airbnb resulted in significant\nvariance reductions of 50% to 85%, while aligning well with the booking metric\nitself. Continuous attribution allows us to assign a utility score to each\nproduct page-view, and this score can be flexibly further aggregated to a\nvariety of units of interest, such as searches and listings. We provide\nmultiple real-world applications of attribution to illustrate its versatility.",
    "descriptor": "",
    "authors": [
      "Alex Deng",
      "Michelle Du",
      "Anna Matlin"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16373"
  },
  {
    "id": "arXiv:2210.16399",
    "title": "U-Net-based Models for Skin Lesion Segmentation: More Attention and  Augmentation",
    "abstract": "According to WHO[1], since the 1970s, diagnosis of melanoma skin cancer has\nbeen more frequent. However, if detected early, the 5-year survival rate for\nmelanoma can increase to 99 percent. In this regard, skin lesion segmentation\ncan be pivotal in monitoring and treatment planning. In this work, ten models\nand four augmentation configurations are trained on the ISIC 2016 dataset. The\nperformance and overfitting are compared utilizing five metrics. Our results\nshow that the U-Net-Resnet50 and the R2U-Net have the highest metrics value,\nalong with two data augmentation scenarios. We also investigate CBAM and AG\nblocks in the U-Net architecture, which enhances segmentation performance at a\nmeager computational cost. In addition, we propose using pyramid, AG, and CBAM\nblocks in a sequence, which significantly surpasses the results of using the\ntwo individually. Finally, our experiments show that models that have exploited\nattention modules successfully overcome common skin lesion segmentation\nproblems. Lastly, in the spirit of reproducible research, we implement models\nand codes publicly available.",
    "descriptor": "\nComments: 27 pages, 12 figures\n",
    "authors": [
      "Pooya Mohammadi Kazaj",
      "MohammadHossein Koosheshi",
      "Ali Shahedi",
      "Alireza Vafaei Sadr"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16399"
  },
  {
    "id": "arXiv:2210.16414",
    "title": "Meta-Learning Biologically Plausible Plasticity Rules with Random  Feedback Pathways",
    "abstract": "Backpropagation is widely used to train artificial neural networks, but its\nrelationship to synaptic plasticity in the brain is unknown. Some biological\nmodels of backpropagation rely on feedback projections that are symmetric with\nfeedforward connections, but experiments do not corroborate the existence of\nsuch symmetric backward connectivity. Random feedback alignment offers an\nalternative model in which errors are propagated backward through fixed, random\nbackward connections. This approach successfully trains shallow models, but\nlearns slowly and does not perform well with deeper models or online learning.\nIn this study, we develop a novel meta-plasticity approach to discover\ninterpretable, biologically plausible plasticity rules that improve online\nlearning performance with fixed random feedback connections. The resulting\nplasticity rules show improved online training of deep models in the low data\nregime. Our results highlight the potential of meta-plasticity to discover\neffective, interpretable learning rules satisfying biological constraints.",
    "descriptor": "",
    "authors": [
      "Navid Shervani-Tabar",
      "Robert Rosenbaum"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.16414"
  },
  {
    "id": "arXiv:2210.16428",
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention",
    "abstract": "Audio captioning is the task of generating captions that describe the content\nof audio clips. In the real world, many objects produce similar sounds. It is\ndifficult to identify these auditory ambiguous sound events with access to\naudio information only. How to accurately recognize ambiguous sounds is a major\nchallenge for audio captioning systems. In this work, inspired by the\naudio-visual multi-modal perception of human beings, we propose visually-aware\naudio captioning, which makes use of visual information to help the recognition\nof ambiguous sounding objects. Specifically, we introduce an off-the-shelf\nvisual encoder to process the video inputs, and incorporate the extracted\nvisual features into an audio captioning system. Furthermore, to better exploit\ncomplementary contexts from redundant audio-visual streams, we propose an\naudio-visual attention mechanism that integrates audio and visual information\nadaptively according to their confidence levels. Experimental results on\nAudioCaps, the largest publicly available audio captioning dataset, show that\nthe proposed method achieves significant improvement over a strong baseline\naudio captioning system and is on par with the state-of-the-art result.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Xubo Liu",
      "Qiushi Huang",
      "Xinhao Mei",
      "Haohe Liu",
      "Qiuqiang Kong",
      "Jianyuan Sun",
      "Shengchen Li",
      "Tom Ko",
      "Yu Zhang",
      "Lilian H. Tang",
      "Mark D. Plumbley",
      "Volkan K\u0131l\u0131\u00e7",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16428"
  },
  {
    "id": "arXiv:2210.16440",
    "title": "ODNet: A Convolutional Neural Network for Asteroid Occultation Detection",
    "abstract": "We propose to design and build an algorithm that will use a Convolutional\nNeural Network (CNN) and observations from the Unistellar network to reliably\ndetect asteroid occultations. The Unistellar Network, made of more than 10,000\ndigital telescopes owned by citizen scientists, and is regularly used to record\nasteroid occultations. In order to process the increasing amount of\nobservational produced by this network, we need a quick and reliable way to\nanalyze occultations. In an effort to solve this problem, we trained a CNN with\nartificial images of stars with twenty different types of photometric signals.\nInputs to the network consists of two stacks of snippet images of stars, one\naround the star that is supposed to be occulted and a reference star used for\ncomparison. We need the reference star to distinguish between a true\noccultation and artefacts introduced by poor atmospheric condition. Our\nOccultation Detection Neural Network (ODNet), can analyze three sequence of\nstars per second with 91\\% of precision and 87\\% of recall. The algorithm is\nsufficiently fast and robust so we can envision incorporating onboard the\neVscopes to deliver real-time results. We conclude that citizen science\nrepresents an important opportunity for the future studies and discoveries in\nthe occultations, and that application of artificial intelligence will permit\nus to to take better advantage of the ever-growing quantity of data to\ncategorize asteroids.",
    "descriptor": "\nComments: 17 pages, 10 figures, 3 tables, accepted for publication in Astrophysical Journal\n",
    "authors": [
      "Dorian Cazeneuve",
      "Franck Marchis",
      "Guillaume Blaclard",
      "Paul A. Dalba",
      "Victor Martin",
      "Jo\u00e9 Asencioa"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16440"
  },
  {
    "id": "arXiv:2210.16450",
    "title": "The Secret Source : Incorporating Source Features to Improve  Acoustic-to-Articulatory Speech Inversion",
    "abstract": "In this work, we incorporated acoustically derived source features,\naperiodicity, periodicity and pitch as additional targets to an\nacoustic-to-articulatory speech inversion (SI) system. We also propose a\nTemporal Convolution based SI system, which uses auditory spectrograms as the\ninput speech representation, to learn long-range dependencies and complex\ninteractions between the source and vocal tract, to improve the SI task. The\nexperiments are conducted with both the Wisconsin X-ray microbeam (XRMB) and\nHaskins Production Rate Comparison (HPRC) datasets, with comparisons done with\nrespect to three baseline SI model architectures. The proposed SI system with\nthe HPRC dataset gains an improvement of close to 28% when the source features\nare used as additional targets. The same SI system outperforms the current best\nperforming SI models by around 9% on the XRMB dataset.",
    "descriptor": "",
    "authors": [
      "Yashish M. Siriwardena",
      "Carol Espy-Wilson"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16450"
  },
  {
    "id": "arXiv:2210.16454",
    "title": "Learning to Compute the Articulatory Representations of Speech with the  MIRRORNET",
    "abstract": "Most organisms including humans function by coordinating and integrating\nsensory signals with motor actions to survive and accomplish desired tasks.\nLearning these complex sensorimotor mappings proceeds simultaneously and often\nin an unsupervised or semi-supervised fashion. An autoencoder architecture\n(MirrorNet) inspired by this sensorimotor learning paradigm is explored in this\nwork to learn how to control an articulatory synthesizer. The synthesizer takes\nas input control signals consisting of six vocal Tract Variables (TVs) and\nsource features (voicing indicators and pitch), and generates the corresponding\nauditory spectrograms. Due to the non-linear structure of the synthesizer, the\ncontrol parameters that produce a target speech signal are not readily\ncomputable nor are they always unique. Here we demonstrate how to initialize\nthe MirrorNet learning so as to produce a meaningful range of articulatory\nvalues. Once trained, the MirrorNet successfully estimates the TVs and source\nfeatures needed to synthesize any arbitrary speech utterance. This approach\noutperforms the best previously designed `speech inversion' systems on the\nWisconsin X-ray microbeam (XRMB) dataset.",
    "descriptor": "",
    "authors": [
      "Yashish M. Siriwardena",
      "Carol Espy-Wilson",
      "Shihab Shamma"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16454"
  },
  {
    "id": "arXiv:2210.16456",
    "title": "Flows, Scaling, and Entropy Revisited: a Unified Perspective via  Optimizing Joint Distributions",
    "abstract": "In this short expository note, we describe a unified algorithmic perspective\non several classical problems which have traditionally been studied in\ndifferent communities. This perspective views the main characters -- the\nproblems of Optimal Transport, Minimum Mean Cycle, Matrix Scaling, and Matrix\nBalancing -- through the same lens of optimization problems over joint\nprobability distributions P(x,y) with constrained marginals. While this is how\nOptimal Transport is typically introduced, this lens is markedly less\nconventional for the other three problems. This perspective leads to a simple\nand unified framework spanning problem formulation, algorithm development, and\nruntime analysis.",
    "descriptor": "\nComments: Invited expository article for the SIAM Group on Optimization's Views and News\n",
    "authors": [
      "Jason M. Altschuler"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.16456"
  },
  {
    "id": "arXiv:2210.16458",
    "title": "Reformulating van Rijsbergen's $F_\u03b2$ metric for weighted binary  cross-entropy",
    "abstract": "The separation of performance metrics from gradient based loss functions may\nnot always give optimal results and may miss vital aggregate information. This\npaper investigates incorporating a performance metric alongside differentiable\nloss functions to inform training outcomes. The goal is to guide model\nperformance and interpretation by assuming statistical distributions on this\nperformance metric for dynamic weighting. The focus is on van Rijsbergens\n$F_{\\beta}$ metric -- a popular choice for gauging classification performance.\nThrough distributional assumptions on the $F_{\\beta}$, an intermediary link can\nbe established to the standard binary cross-entropy via dynamic penalty\nweights. First, the $F_{\\beta}$ metric is reformulated to facilitate assuming\nstatistical distributions with accompanying proofs for the cumulative density\nfunction. These probabilities are used within a knee curve algorithm to find an\noptimal $\\beta$ or $\\beta_{opt}$. This $\\beta_{opt}$ is used as a weight or\npenalty in the proposed weighted binary cross-entropy. Experimentation on\npublicly available data with imbalanced classes mostly yields better and\ninterpretable results as compared to the baseline. For example, for the IMDB\ntext data with known labeling errors, a 14% boost is shown. This methodology\ncan accelerate training and provide better interpretation.",
    "descriptor": "",
    "authors": [
      "Satesh Ramdhani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16458"
  },
  {
    "id": "arXiv:2210.16460",
    "title": "The Vector Balancing Constant for Zonotopes",
    "abstract": "The vector balancing constant $\\mathrm{vb}(K,Q)$ of two symmetric convex\nbodies $K,Q$ is the minimum $r \\geq 0$ so that any number of vectors from $K$\ncan be balanced into an $r$-scaling of $Q$. A question raised by Schechtman is\nwhether for any zonotope $K \\subseteq \\mathbb{R}^d$ one has $\\mathrm{vb}(K,K)\n\\lesssim \\sqrt{d}$. Intuitively, this asks whether a natural geometric\ngeneralization of Spencer's Theorem (for which $K = B^d_\\infty$) holds. We\nprove that for any zonotope $K \\subseteq \\mathbb{R}^d$ one has\n$\\mathrm{vb}(K,K) \\lesssim \\sqrt{d} \\log \\log \\log d$. Our main technical\ncontribution is a tight lower bound on the Gaussian measure of any section of a\nnormalized zonotope, generalizing Vaaler's Theorem for cubes. We also prove\nthat for two different normalized zonotopes $K$ and $Q$ one has\n$\\mathrm{vb}(K,Q) \\lesssim \\sqrt{d \\log d}$. All the bounds are constructive\nand the corresponding colorings can be computed in polynomial time.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Laurel Heck",
      "Victor Reis",
      "Thomas Rothvoss"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.16460"
  },
  {
    "id": "arXiv:2210.16481",
    "title": "Accelerating RNN-T Training and Inference Using CTC guidance",
    "abstract": "We propose a novel method to accelerate training and inference process of\nrecurrent neural network transducer (RNN-T) based on the guidance from a\nco-trained connectionist temporal classification (CTC) model. We made a key\nassumption that if an encoder embedding frame is classified as a blank frame by\nthe CTC model, it is likely that this frame will be aligned to blank for all\nthe partial alignments or hypotheses in RNN-T and it can be discarded from the\ndecoder input. We also show that this frame reduction operation can be applied\nin the middle of the encoder, which result in significant speed up for the\ntraining and inference in RNN-T. We further show that the CTC alignment, a\nby-product of the CTC decoder, can also be used to perform lattice reduction\nfor RNN-T during training. Our method is evaluated on the Librispeech and\nSpeechStew tasks. We demonstrate that the proposed method is able to accelerate\nthe RNN-T inference by 2.2 times with similar or slightly better word error\nrates (WER).",
    "descriptor": "\nComments: submitted to ICASSP 2023\n",
    "authors": [
      "Yongqiang Wang",
      "Zhehuai Chen",
      "Chengjian Zheng",
      "Yu Zhang",
      "Wei Han",
      "Parisa Haghani"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16481"
  },
  {
    "id": "arXiv:2210.16490",
    "title": "Harmonic Tutte polynomials of matroids II",
    "abstract": "In this work, we introduce the harmonic generalization of the $m$-tuple\nweight enumerators of codes over finite Frobenius rings. A harmonic version of\nthe MacWilliams-type identity for $m$-tuple weight enumerators of codes over\nfinite Frobenius ring is also given. Moreover, we define the demi-matroid\nanalogue of well-known polynomials from matroid theory, namely Tutte\npolynomials and coboundary polynomials, and associate them with a harmonic\nfunction. We also prove the Greene-type identity relating these polynomials to\nthe harmonic $m$-tuple weight enumerators of codes over finite Frobenius rings.\nAs an application of this Greene-type identity, we provide a simple\ncombinatorial proof of the MacWilliams-type identity for harmonic $m$-tuple\nweight enumerators over finite Frobenius rings. Finally, we provide the\nstructure of the relative invariant spaces containing the harmonic $m$-tuple\nweight enumerators of self-dual codes over finite fields.",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Thomas Britz",
      "Himadri Shekhar Chakraborty",
      "Reina Ishikawa",
      "Tsuyoshi Miezaki"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)",
      "Group Theory (math.GR)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2210.16490"
  },
  {
    "id": "arXiv:2210.16493",
    "title": "Neural network quantum state with proximal optimization: a ground-state  searching scheme based on variational Monte Carlo",
    "abstract": "Neural network quantum states (NQS), incorporating with variational Monte\nCarlo (VMC) method, are shown to be a promising way to investigate quantum\nmany-body physics. Whereas vanilla VMC methods perform one gradient update per\nsample, we introduce a novel objective function with proximal optimization (PO)\nthat enables multiple updates via reusing the mismatched samples. Our VMC-PO\nmethod keeps the advantage of the previous importance sampling gradient\noptimization algorithm [L. Yang, {\\it et al}, Phys. Rev. Research {\\bf 2},\n012039(R)(2020)] that efficiently uses sampled states. PO mitigates the\nnumerical instabilities during network updates, which is similar to stochastic\nreconfiguration (SR) methods, but achieves an alternative and simpler implement\nwith lower computational complexity. We investigate the performance of our\nVMC-PO algorithm for ground-state searching with a 1-dimensional\ntransverse-field Ising model and 2-dimensional Heisenberg antiferromagnet on a\nsquare lattice, and demonstrate that the reached ground-state energies are\ncomparable to state-of-the-art results.",
    "descriptor": "\nComments: developement log of a NQS optimization method, comments are welcome\n",
    "authors": [
      "Feng Chen",
      "Ming Xue"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.16493"
  },
  {
    "id": "arXiv:2210.16497",
    "title": "Fiber Organization has Little Effect on Electrical Activation Patterns  during Focal Arrhythmias in the Left Atrium",
    "abstract": "Over the past two decades there has been a steady trend towards the\ndevelopment of realistic models of cardiac conduction with increasing levels of\ndetail. However, making models more realistic complicates their personalization\nand use in clinical practice due to limited availability of tissue and cellular\nscale data. One such limitation is obtaining information about myocardial fiber\norganization in the clinical setting. In this study, we investigated a chimeric\nmodel of the left atrium utilizing clinically derived patient-specific atrial\ngeometry and a realistic, yet foreign for a given patient fiber organization.\nWe discovered that even significant variability of fiber organization had a\nrelatively small effect on the spatio-temporal activation pattern during\nregular pacing. For a given pacing site, the activation maps were very similar\nacross all fiber organizations tested.",
    "descriptor": "",
    "authors": [
      "Jiyue He",
      "Arkady M. Pertsov",
      "Elizabeth M. Cherry",
      "Flavio H. Fenton",
      "Caroline H. Roney",
      "Steven A. Niederer",
      "Zirui Zang",
      "Rahul Mangharam"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16497"
  },
  {
    "id": "arXiv:2210.16498",
    "title": "Articulatory Representation Learning Via Joint Factor Analysis and  Neural Matrix Factorization",
    "abstract": "Articulatory representation learning is the fundamental research in modeling\nneural speech production system. Our previous work has established a deep\nparadigm to decompose the articulatory kinematics data into gestures, which\nexplicitly model the phonological and linguistic structure encoded with human\nspeech production mechanism, and corresponding gestural scores. We continue\nwith this line of work by raising two concerns: (1) The articulators are\nentangled together in the original algorithm such that some of the articulators\ndo not leverage effective moving patterns, which limits the interpretability of\nboth gestures and gestural scores; (2) The EMA data is sparsely sampled from\narticulators, which limits the intelligibility of learned representations. In\nthis work, we propose a novel articulatory representation decomposition\nalgorithm that takes the advantage of guided factor analysis to derive the\narticulatory-specific factors and factor scores. A neural convolutive matrix\nfactorization algorithm is then employed on the factor scores to derive the new\ngestures and gestural scores. We experiment with the rtMRI corpus that captures\nthe fine-grained vocal tract contours. Both subjective and objective evaluation\nresults suggest that the newly proposed system delivers the articulatory\nrepresentations that are intelligible, generalizable, efficient and\ninterpretable.",
    "descriptor": "\nComments: Submitted to 2023 ICASSP\n",
    "authors": [
      "Jiachen Lian",
      "Alan W Black",
      "Yijing Lu",
      "Louis Goldstein",
      "Shinji Watanabe",
      "Gopala K. Anumanchipalli"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16498"
  },
  {
    "id": "arXiv:2210.16513",
    "title": "Mapping state transition susceptibility in reverse annealing",
    "abstract": "Quantum annealing is a novel type of analog computation that aims to use\nquantum mechanical fluctuations to search for optimal solutions of Ising\nproblems. Quantum annealing in the transverse field Ising model, implemented on\nD-Wave devices, works by applying a time dependent transverse field, which puts\nall qubits into a uniform state of superposition, and then applying a\nHamiltonian over time which describes a user programmed Ising problem. We\npresent a method which utilizes two control features of D-Wave quantum\nannealers, reverse annealing and an h-gain schedule, to quantify the\nsusceptibility, or the distance, between two classical states of an Ising\nproblem. The starting state is encoded using reverse annealing, and the second\nstate is encoded on the linear terms of problem Hamiltonian. An h-gain schedule\nis specified which incrementally increases the strength of the linear terms,\nthus allowing a quantification of the h-gain strength required to transition\nthe anneal into a specific state at the final measurement. By the nature of\nquantum annealing, the state tends towards global minima and therefore we\nrestrict the second classical state to a minimum solution of the given Ising\nproblem. This susceptibility mapping, when enumerated across all initial\nstates, shows in detail the behavior of the quantum annealer during reverse\nannealing. The procedure is experimentally demonstrated on three small test\nIsing's which were embedded in parallel on the D-Wave Advantage_system4.1.\nAnalysis of the state transition mapping shows detailed characteristics of the\nreverse annealing process including intermediate state transition paths, which\nare visually represented as state transition networks.",
    "descriptor": "",
    "authors": [
      "Elijah Pelofske"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Discrete Mathematics (cs.DM)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.16513"
  },
  {
    "id": "arXiv:2210.16525",
    "title": "Spectral Representation Learning for Conditional Moment Models",
    "abstract": "Many problems in causal inference and economics can be formulated in the\nframework of conditional moment models, which characterize the target function\nthrough a collection of conditional moment restrictions. For nonparametric\nconditional moment models, efficient estimation has always relied on preimposed\nconditions on various measures of ill-posedness of the hypothesis space, which\nare hard to validate when flexible models are used. In this work, we address\nthis issue by proposing a procedure that automatically learns representations\nwith controlled measures of ill-posedness. Our method approximates a linear\nrepresentation defined by the spectral decomposition of a conditional\nexpectation operator, which can be used for kernelized estimators and is known\nto facilitate minimax optimal estimation in certain settings. We show this\nrepresentation can be efficiently estimated from data, and establish L2\nconsistency for the resulting estimator. We evaluate the proposed method on\nproximal causal inference tasks, exhibiting promising performance on\nhigh-dimensional, semi-synthetic data.",
    "descriptor": "",
    "authors": [
      "Ziyu Wang",
      "Yucen Luo",
      "Yueru Li",
      "Jun Zhu",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2210.16525"
  },
  {
    "id": "arXiv:2210.16568",
    "title": "Ice Core Dating using Probabilistic Programming",
    "abstract": "Ice cores record crucial information about past climate. However, before ice\ncore data can have scientific value, the chronology must be inferred by\nestimating the age as a function of depth. Under certain conditions, chemicals\nlocked in the ice display quasi-periodic cycles that delineate annual layers.\nManually counting these noisy seasonal patterns to infer the chronology can be\nan imperfect and time-consuming process, and does not capture uncertainty in a\nprincipled fashion. In addition, several ice cores may be collected from a\nregion, introducing an aspect of spatial correlation between them. We present\nan exploration of the use of probabilistic models for automatic dating of ice\ncores, using probabilistic programming to showcase its use for prototyping,\nautomatic inference and maintainability, and demonstrate common failure modes\nof these tools.",
    "descriptor": "",
    "authors": [
      "Aditya Ravuri",
      "Tom R. Andersson",
      "Ieva Kazlauskaite",
      "Will Tebbutt",
      "Richard E. Turner",
      "J. Scott Hosking",
      "Neil D. Lawrence",
      "Markus Kaiser"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16568"
  },
  {
    "id": "arXiv:2210.16584",
    "title": "CMT: Interpretable Model for Rapid Recognition Pneumonia from Chest  X-Ray Images by Fusing Low Complexity Multilevel Attention Mechanism",
    "abstract": "Chest imaging plays an essential role in diagnosing and predicting patients\nwith COVID-19 with evidence of worsening respiratory status. Many deep\nlearning-based diagnostic models for pneumonia have been developed to enable\ncomputer-aided diagnosis. However, the long training and inference time make\nthem inflexible. In addition, the lack of interpretability reduces their\ncredibility in clinical medical practice. This paper presents CMT, a model with\ninterpretability and rapid recognition of pneumonia, especially COVID-19\npositive. Multiple convolutional layers in CMT are first used to extract\nfeatures in CXR images, and then Transformer is applied to calculate the\npossibility of each symptom. To improve the model's generalization performance\nand to address the problem of sparse medical image data, we propose Feature\nFusion Augmentation (FFA), a plug-and-play method for image augmentation. It\nfuses the features of the two images to varying degrees to produce a new image\nthat does not deviate from the original distribution. Furthermore, to reduce\nthe computational complexity and accelerate the convergence, we propose\nMultilevel Multi-Head Self-Attention (MMSA), which computes attention on\ndifferent levels to establish the relationship between global and local\nfeatures. It significantly improves the model performance while substantially\nreducing its training and inference time. Experimental results on the largest\nCOVID-19 dataset show the proposed CMT has state-of-the-art performance. The\neffectiveness of FFA and MMSA is demonstrated in the ablation experiments. In\naddition, the weights and feature activation maps of the model inference\nprocess are visualized to show the CMT's interpretability.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "Shengchao Chen",
      "Sufen Ren",
      "Guanjun Wang",
      "Mengxing Huang",
      "Chenyang Xue"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16584"
  },
  {
    "id": "arXiv:2210.16592",
    "title": "Cram\u00e9r-Rao Bound Minimization for IRS-Enabled Multiuser Integrated  Sensing and Communication with Extended Target",
    "abstract": "This paper investigates an intelligent reflecting surface (IRS) enabled\nmultiuser integrated sensing and communication (ISAC) system, which consists of\none multi-antenna base station (BS), one IRS, multiple single-antenna\ncommunication users (CUs), and one extended target at the non-line-of-sight\n(NLoS) region of the BS. The IRS is deployed to not only assist the\ncommunication from the BS to the CUs, but also enable the BS's NLoS target\nsensing based on the echo signals from the BS-IRS-target-IRS-BS link. To\nprovide full degrees of freedom for sensing, we suppose that the BS sends\nadditional dedicated sensing signals combined with the information signals.\nAccordingly, we consider two types of CU receivers, namely Type-I and Type-II\nreceivers, which do not have and have the capability of cancelling the\ninterference from the sensing signals, respectively. Under this setup, we\njointly optimize the transmit beamforming at the BS and the reflective\nbeamforming at the IRS to minimize the Cram\\'er-Rao bound (CRB) for estimating\nthe target response matrix with respect to the IRS, subject to the minimum\nsignal-to-interference-plus-noise ratio (SINR) constraints at the CUs and the\nmaximum transmit power constraint at the BS. We present efficient algorithms to\nsolve the highly non-convex SINR-constrained CRB minimization problems, by\nusing the techniques of alternating optimization and semi-definite relaxation.\nNumerical results show that the proposed design achieves lower estimation CRB\nthan other benchmark schemes, and the sensing signal interference\npre-cancellation is beneficial when the number of CUs is greater than one.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Xianxin Song",
      "Tony Xiao Han",
      "Jie Xu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16592"
  },
  {
    "id": "arXiv:2210.16611",
    "title": "Application of Knowledge Distillation to Multi-task Speech  Representation Learning",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to\nlearn speech representations from audio waveforms in a self-supervised manner.\nWhen these models are combined with downstream tasks such as speech\nrecognition, they have been shown to provide state-of-the-art performance.\nHowever, these models use a large number of parameters, the smallest version of\nwhich has about 95 million parameters. This constitutes a challenge for edge AI\ndevice deployments. In this paper, we use knowledge distillation to reduce the\noriginal model size by about 75% while maintaining similar performance levels.\nMoreover, we use wav2vec 2.0 and HuBERT models for distillation and present a\ncomprehensive performance analysis through our experiments where we fine-tune\nthe distilled models on single task and multi-task frameworks separately. In\nparticular, our experiments show that fine-tuning the distilled models on\nkeyword spotting and speaker verification tasks result in only 0.1% accuracy\nand 0.9% equal error rate degradations, respectively.",
    "descriptor": "\nComments: Speech representation learning, multitask learning, wav2vec, HuBERT, knowledge distillation\n",
    "authors": [
      "Mine Kerpicci",
      "Van Nguyen",
      "Shuhua Zhang",
      "Erik Visser"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16611"
  },
  {
    "id": "arXiv:2210.16622",
    "title": "Discriminative Speaker Representation via Contrastive Learning with  Class-Aware Attention in Angular Space",
    "abstract": "The challenges in applying contrastive learning to speaker verification (SV)\nare that the softmax-based contrastive loss lacks discriminative power and that\nthe hard negative pairs can easily influence learning. To overcome these\nchallenges, we propose a contrastive learning SV framework incorporating an\nadditive angular margin into the supervised contrastive loss. The margin\nimproves the speaker representation's discrimination ability. We introduce a\nclass-aware attention mechanism through which hard negative samples contribute\nless significantly to the supervised contrastive loss. We also employed a\ngradient-based multi-objective optimization approach to balance the\nclassification and contrastive loss. Experimental results on CN-Celeb and\nVoxceleb1 show that this new learning objective can cause the encoder to find\nan embedding space that exhibits great speaker discrimination across languages.",
    "descriptor": "\nComments: Submitted to ICASSP 2023, 5 pages, 2 figures\n",
    "authors": [
      "Zhe Li",
      "Man-Wai Mak",
      "Helen Mei-Ling Meng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16622"
  },
  {
    "id": "arXiv:2210.16636",
    "title": "Speaker Representation Learning via Contrastive Loss with Maximal  Speaker Separability",
    "abstract": "A great challenge in speaker representation learning using deep models is to\ndesign learning objectives that can enhance the discrimination of unseen\nspeakers under unseen domains. This work proposes a supervised contrastive\nlearning objective to learn a speaker embedding space by effectively leveraging\nthe label information in the training data. In such a space, utterance pairs\nspoken by the same or similar speakers will stay close, while utterance pairs\nspoken by different speakers will be far apart. For each training speaker, we\nperform random data augmentation on their utterances to form positive pairs,\nand utterances from different speakers form negative pairs. To maximize speaker\nseparability in the embedding space, we incorporate the additive angular-margin\nloss into the contrastive learning objective. Experimental results on CN-Celeb\nshow that this new learning objective can cause ECAPA-TDNN to find an embedding\nspace that exhibits great speaker discrimination. The contrastive learning\nobjective is easy to implement, and we provide PyTorch code at\nhttps://github.com/shanmon110/AAMSupCon.",
    "descriptor": "\nComments: Accept by APSIPA ASC 2022, 6 pages, 2 figures\n",
    "authors": [
      "Zhe Li",
      "Man-Wai Mak"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16636"
  },
  {
    "id": "arXiv:2210.16640",
    "title": "2D and 3D CT Radiomic Features Performance Comparison in  Characterization of Gastric Cancer: A Multi-center Study",
    "abstract": "Objective: Radiomics, an emerging tool for medical image analysis, is\npotential towards precisely characterizing gastric cancer (GC). Whether using\none-slice 2D annotation or whole-volume 3D annotation remains a long-time\ndebate, especially for heterogeneous GC. We comprehensively compared 2D and 3D\nradiomic features' representation and discrimination capacity regarding GC, via\nthree tasks.\nMethods: Four-center 539 GC patients were retrospectively enrolled and\ndivided into the training and validation cohorts. From 2D or 3D regions of\ninterest (ROIs) annotated by radiologists, radiomic features were extracted\nrespectively. Feature selection and model construction procedures were customed\nfor each combination of two modalities (2D or 3D) and three tasks.\nSubsequently, six machine learning models (Model_2D^LNM, Model_3D^LNM;\nModel_2D^LVI, Model_3D^LVI; Model_2D^pT, Model_3D^pT) were derived and\nevaluated to reflect modalities' performances in characterizing GC.\nFurthermore, we performed an auxiliary experiment to assess modalities'\nperformances when resampling spacing is different.\nResults: Regarding three tasks, the yielded areas under the curve (AUCs)\nwere: Model_2D^LNM's 0.712 (95% confidence interval, 0.613-0.811),\nModel_3D^LNM's 0.680 (0.584-0.775); Model_2D^LVI's 0.677 (0.595-0.761),\nModel_3D^LVI's 0.615 (0.528-0.703); Model_2D^pT's 0.840 (0.779-0.901),\nModel_3D^pT's 0.813 (0.747-0.879). Moreover, the auxiliary experiment indicated\nthat Models_2D are statistically more advantageous than Models3D with different\nresampling spacings.\nConclusion: Models constructed with 2D radiomic features revealed comparable\nperformances with those constructed with 3D features in characterizing GC.\nSignificance: Our work indicated that time-saving 2D annotation would be the\nbetter choice in GC, and provided a related reference to further\nradiomics-based researches.",
    "descriptor": "\nComments: Published in IEEE Journal of Biomedical and Health Informatics\n",
    "authors": [
      "Lingwei Meng",
      "Di Dong",
      "Xin Chen",
      "Mengjie Fang",
      "Rongpin Wang",
      "Jing Li",
      "Zaiyi Liu",
      "Jie Tian"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.16640"
  },
  {
    "id": "arXiv:2210.16662",
    "title": "Global Optimization of Energy Efficiency in IRS-Aided Communication  Systems via Robust IRS-Element Activation",
    "abstract": "In this paper, we study an intelligent reflecting surface (IRS) assisted\ncommunication system with single-antenna transmitter and receiver, under\nimperfect channel state information (CSI). More specifically, we deal with the\nrobust selection of binary (on/off) states of the IRS elements in order to\nmaximize the worst-case energy efficiency (EE), given a bounded CSI\nuncertainty, while satisfying a minimum signal-to-noise ratio (SNR). The IRS\nphase shifts are adjusted so as to maximize the ideal SNR (i.e., without CSI\nerror), based only on the estimated channels. First, we derive a closed-form\nexpression of the worst-case SNR, and then formulate the robust (discrete)\noptimization problem. Moreover, we design and analyze a dynamic programming\n(DP) algorithm that is theoretically guaranteed to achieve the global maximum\nwith polynomial complexity $O(L \\log L)$, where $L$ is the number of IRS\nelements. Finally, numerical simulations confirm the theoretical results. In\nparticular, the proposed algorithm shows identical performance with the\nexhaustive search, and significantly outperforms a baseline scheme, namely, the\nactivation of all IRS elements.",
    "descriptor": "\nComments: 7 pages, 3 figures\n",
    "authors": [
      "Christos N. Efrem",
      "Ioannis Krikidis"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16662"
  },
  {
    "id": "arXiv:2210.16663",
    "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with  Pre-trained Masked Language Model",
    "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech\nrecognition that adapts BERT for connectionist temporal classification (CTC).\nOur formulation relaxes the conditional independence assumptions used in\nconventional CTC and incorporates linguistic knowledge through the explicit\noutput dependency obtained by BERT contextual embedding. BERT-CTC attends to\nthe full contexts of the input and hypothesized output sequences via the\nself-attention mechanism. This mechanism encourages a model to learn\ninner/inter-dependencies between the audio and token representations while\nmaintaining CTC's training efficiency. During inference, BERT-CTC combines a\nmask-predict algorithm with CTC decoding, which iteratively refines an output\nsequence. The experimental results reveal that BERT-CTC improves over\nconventional approaches across variations in speaking styles and languages.\nFinally, we show that the semantic representations in BERT-CTC are beneficial\ntowards downstream spoken language understanding tasks.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP2022\n",
    "authors": [
      "Yosuke Higuchi",
      "Brian Yan",
      "Siddhant Arora",
      "Tetsuji Ogawa",
      "Tetsunori Kobayashi",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16663"
  },
  {
    "id": "arXiv:2210.16667",
    "title": "IRS-User Association in IRS-Aided MISO Wireless Networks: Convex  Optimization and Machine Learning Approaches",
    "abstract": "This paper concentrates on the problem of associating an intelligent\nreflecting surface (IRS) to multiple users in a multiple-input single-output\n(MISO) downlink wireless communication network. The main objective of the paper\nis to maximize the sum-rate of all users by solving the joint optimization\nproblem of the IRS-user association, IRS reflection, and BS beamforming,\nformulated as a non-convex mixed-integer optimization problem. The variable\nseparation and relaxation are used to transform the problem into three convex\nsub-problems, which are alternatively solved through the convex optimization\n(CO) method. The major drawback of the proposed CO-based algorithm is high\ncomputational complexity. Thus, we make use of machine learning (ML) to tackle\nthis problem. To this end, first, we convert the optimization problem into a\nregression problem. Then, we solve it with feed-forward neural networks (FNNs),\ntrained by CO-based generated data. Simulation results show that the proposed\nML-based algorithm has a performance equivalent to the CO-based algorithm, but\nwith less computation complexity due to its offline training procedure.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Hamid Amiriara",
      "Farid Ashtiani",
      "Mahtab Mirmohseni",
      "Masoumeh Nasiri-Kenari"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.16667"
  },
  {
    "id": "arXiv:2210.16674",
    "title": "Semantic-SuPer: A Semantic-aware Surgical Perception Framework for  Endoscopic Tissue Classification, Reconstruction, and Tracking",
    "abstract": "Accurate and robust tracking and reconstruction of the surgical scene is a\ncritical enabling technology toward autonomous robotic surgery. Existing\nalgorithms for 3D perception in surgery mainly rely on geometric information,\nwhile we propose to also leverage semantic information inferred from the\nendoscopic video using image segmentation algorithms. In this paper, we present\na novel, comprehensive surgical perception framework, Semantic-SuPer, that\nintegrates geometric and semantic information to facilitate data association,\n3D reconstruction, and tracking of endoscopic scenes, benefiting downstream\ntasks like surgical navigation. The proposed framework is demonstrated on\nchallenging endoscopic data with deforming tissue, showing its advantages over\nour baseline and several other state-of the-art approaches. Our code and\ndataset will be available at https://github.com/ucsdarclab/Python-SuPer.",
    "descriptor": "\nComments: Under review for ICRA 2023\n",
    "authors": [
      "Shan Lin",
      "Albert J. Miao",
      "Jingpei Lu",
      "Shunkai Yu",
      "Zih-Yun Chiu",
      "Florian Richter",
      "Michael C. Yip"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16674"
  },
  {
    "id": "arXiv:2210.16679",
    "title": "Monitoring the Dynamic Networks of Stock Returns",
    "abstract": "In this paper, we study the connection between the companies in the Swedish\ncapital market. We consider 28 companies included in the determination of the\nmarket index OMX30. The network structure of the market is constructed using\ndifferent methods to determine the distance between the companies. We use\nhierarchical clustering methods to find the relation among the companies in\neach window. Next, we obtain one-dimensional time series of the distances\nbetween the clustering trees that reflect the changes in the relationship\nbetween the companies in the market over time. The method of statistical\nprocess control, namely the Shewhart control chart, is applied to those time\nseries to detect abnormal changes in the financial market.",
    "descriptor": "",
    "authors": [
      "Elena Farahbakhsh Touli",
      "Hoang Nguyen",
      "Olha Bodnar"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.16679"
  },
  {
    "id": "arXiv:2210.16704",
    "title": "Multi-Scale Fusion Methodologies for Head and Neck Tumor Segmentation",
    "abstract": "Head and Neck (H\\&N) organ-at-risk (OAR) and tumor segmentations are\nessential components of radiation therapy planning. The varying anatomic\nlocations and dimensions of H\\&N nodal Gross Tumor Volumes (GTVn) and H\\&N\nprimary gross tumor volume (GTVp) are difficult to obtain due to lack of\naccurate and reliable delineation methods. The downstream effect of incorrect\nsegmentation can result in unnecessary irradiation of normal organs. Towards a\nfully automated radiation therapy planning algorithm, we explore the efficacy\nof multi-scale fusion based deep learning architectures for accurately\nsegmenting H\\&N tumors from medical scans.",
    "descriptor": "",
    "authors": [
      "Abhishek Srivastava",
      "Debesh Jha",
      "Bulent Aydogan",
      "Mohamed E.Abazeed",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16704"
  },
  {
    "id": "arXiv:2210.16724",
    "title": "QuEst: Graph Transformer for Quantum Circuit Reliability Estimation",
    "abstract": "Among different quantum algorithms, PQC for QML show promises on near-term\ndevices. To facilitate the QML and PQC research, a recent python library called\nTorchQuantum has been released. It can construct, simulate, and train PQC for\nmachine learning tasks with high speed and convenient debugging supports.\nBesides quantum for ML, we want to raise the community's attention on the\nreversed direction: ML for quantum. Specifically, the TorchQuantum library also\nsupports using data-driven ML models to solve problems in quantum system\nresearch, such as predicting the impact of quantum noise on circuit fidelity\nand improving the quantum circuit compilation efficiency.\nThis paper presents a case study of the ML for quantum part. Since estimating\nthe noise impact on circuit reliability is an essential step toward\nunderstanding and mitigating noise, we propose to leverage classical ML to\npredict noise impact on circuit fidelity. Inspired by the natural graph\nrepresentation of quantum circuits, we propose to leverage a graph transformer\nmodel to predict the noisy circuit fidelity. We firstly collect a large dataset\nwith a variety of quantum circuits and obtain their fidelity on noisy\nsimulators and real machines. Then we embed each circuit into a graph with gate\nand noise properties as node features, and adopt a graph transformer to predict\nthe fidelity.\nEvaluated on 5 thousand random and algorithm circuits, the graph transformer\npredictor can provide accurate fidelity estimation with RMSE error 0.04 and\noutperform a simple neural network-based model by 0.02 on average. It can\nachieve 0.99 and 0.95 R$^2$ scores for random and algorithm circuits,\nrespectively. Compared with circuit simulators, the predictor has over 200X\nspeedup for estimating the fidelity.",
    "descriptor": "\nComments: ICCAD 2022; 10 pages, 10 figures; code at this https URL\n",
    "authors": [
      "Hanrui Wang",
      "Pengyu Liu",
      "Jinglei Cheng",
      "Zhiding Liang",
      "Jiaqi Gu",
      "Zirui Li",
      "Yongshan Ding",
      "Weiwen Jiang",
      "Yiyu Shi",
      "Xuehai Qian",
      "David Z. Pan",
      "Frederic T. Chong",
      "Song Han"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16724"
  },
  {
    "id": "arXiv:2210.16726",
    "title": "Improvements to Embedding-Matching Acoustic-to-Word ASR Using  Multiple-Hypothesis Pronunciation-Based Embeddings",
    "abstract": "In embedding-matching acoustic-to-word (A2W) ASR, every word in the\nvocabulary is represented by a fixed-dimension embedding vector that can be\nadded or removed independently of the rest of the system. The approach is\npotentially an elegant solution for the dynamic out-of-vocabulary (OOV) words\nproblem, where speaker- and context-dependent named entities like contact names\nmust be incorporated into the ASR on-the-fly for every speech utterance at\ntesting time. Challenges still remain, however, in improving the overall\naccuracy of embedding-matching A2W. In this paper, we contribute two methods\nthat improve the accuracy of embedding-matching A2W. First, we propose\ninternally producing multiple embeddings, instead of a single embedding, at\neach instance in time, which allows the A2W model to propose a richer set of\nhypotheses over multiple time segments in the audio. Second, we propose using\nword pronunciation embeddings rather than word orthography embeddings to reduce\nambiguities introduced by words that have more than one sound. We show that the\nabove ideas give significant accuracy improvement, with the same training data\nand nearly identical model size, in scenarios where dynamic OOV words play a\ncrucial role. On a dataset of various queries to a speech-based digital\nassistant that include many user-dependent contact names, we observe up to 18%\ndecrease in word error rate using the proposed improvements.",
    "descriptor": "",
    "authors": [
      "Hao Yen",
      "Woojay Jeon"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16726"
  },
  {
    "id": "arXiv:2210.16731",
    "title": "Projection Valued Measure-based Quantum Machine Learning for Multi-Class  Classification",
    "abstract": "In recent years, quantum machine learning (QML) has been actively used for\nvarious tasks, e.g., classification, reinforcement learning, and adversarial\nlearning. However, these QML studies do not achieve complex tasks due to\nscalability issues on input and output are the biggest hurdle in QML. To cope\nwith this problem, we aim to solve the output scalability issue. Motivated by\nthis challenge, we focus on projection-valued measure (PVM) which utilizes the\nnature of probability amplitude in quantum statistical mechanics. By leveraging\nPVM, the output dimension is expanded from the number of qubits $q$ to\n$\\mathcal{O}(2^q)$. We propose a novel QML framework for multi-class\nclassification. We corroborate that our framework outperforms the\nstate-of-theart (SOTA) with various datasets using no more than 6 qubits.\nFurthermore, our PVM-based QML outperforms 42.2% SOTA.",
    "descriptor": "",
    "authors": [
      "Won Joon Yun",
      "Hankyul Baek",
      "Joongheon Kim"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16731"
  },
  {
    "id": "arXiv:2210.16739",
    "title": "DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common  Label Set",
    "abstract": "In a multilingual country like India, multilingual Automatic Speech\nRecognition (ASR) systems have much scope. Multilingual ASR systems exhibit\nmany advantages like scalability, maintainability, and improved performance\nover the monolingual ASR systems. However, building multilingual systems for\nIndian languages is challenging since different languages use different scripts\nfor writing. On the other hand, Indian languages share a lot of common sounds.\nCommon Label Set (CLS) exploits this idea and maps graphemes of various\nlanguages with similar sounds to common labels. Since Indian languages are\nmostly phonetic, building a parser to convert from native script to CLS is\neasy. In this paper, we explore various approaches to build multilingual ASR\nmodels. We also propose a novel architecture called Encoder-Decoder-Decoder for\nbuilding multilingual systems that use both CLS and native script labels. We\nalso analyzed the effectiveness of CLS-based multilingual systems combined with\nmachine transliteration.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Arunkumar A",
      "Mudit Batra",
      "Umesh S"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16739"
  },
  {
    "id": "arXiv:2210.16743",
    "title": "WeKws: A production first small-footprint end-to-end Keyword Spotting  Toolkit",
    "abstract": "Keyword spotting (KWS) enables speech-based user interaction and gradually\nbecomes an indispensable component of smart devices. Recently, end-to-end (E2E)\nmethods have become the most popular approach for on-device KWS tasks. However,\nthere is still a gap between the research and deployment of E2E KWS methods. In\nthis paper, we introduce WeKws, a production-quality, easy-to-build, and\nconvenient-to-be-applied E2E KWS toolkit. WeKws contains the implementations of\nseveral state-of-the-art backbone networks, making it achieve highly\ncompetitive results on three publicly available datasets. To make WeKws a pure\nE2E toolkit, we utilize a refined max-pooling loss to make the model learn the\nending position of the keyword by itself, which significantly simplifies the\ntraining pipeline and makes WeKws very efficient to be applied in real-world\nscenarios. The toolkit is publicly available at\nhttps://github.com/wenet-e2e/wekws.",
    "descriptor": "",
    "authors": [
      "Jie Wang",
      "Menglong Xu",
      "Jingyong Hou",
      "Binbin Zhang",
      "Xiao-Lei Zhang",
      "Lei Xie",
      "Fuping Pan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16743"
  },
  {
    "id": "arXiv:2210.16775",
    "title": "Nonlinear Causal Discovery via Kernel Anchor Regression",
    "abstract": "Learning causal relationships is a fundamental problem in science. Anchor\nregression has been developed to address this problem for a large class of\ncausal graphical models, though the relationships between the variables are\nassumed to be linear. In this work, we tackle the nonlinear setting by\nproposing kernel anchor regression (KAR). Beyond the natural formulation using\na classic two-stage least square estimator, we also study an improved variant\nthat involves nonparametric regression in three separate stages. We provide\nconvergence results for the proposed KAR estimators and the identifiability\nconditions for KAR to learn the nonlinear structural equation models (SEM).\nExperimental results demonstrate the superior performances of the proposed KAR\nestimators over existing baselines.",
    "descriptor": "",
    "authors": [
      "Wenqi Shi",
      "Wenkai Xu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.16775"
  },
  {
    "id": "arXiv:2210.16793",
    "title": "Approximation on hexagonal domains by Taylor-Abel-Poisson means",
    "abstract": "Approximative properties of the Taylor-Abel-Poisson linear summation me\\-thod\nof Fourier series are considered for functions of several variables, periodic\nwith respect to the hexagonal domain, in the integral metric. In particular,\ndirect and inverse theorems are proved in terms of approximations of functions\nby the Taylor-Abel-Poisson means and $K$-functionals generated by radial\nderivatives. Bernstein type inequalities for $L_1$-norm of high-order radial\nderivatives of the Poisson kernel are also obtained.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1609.09615, arXiv:1901.06275\n",
    "authors": [
      "J\u00fcrgen Prestin",
      "Viktor Savchuk",
      "Andrii Shidlich"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.16793"
  },
  {
    "id": "arXiv:2210.16835",
    "title": "Robust Data Valuation via Variance Reduced Data Shapley",
    "abstract": "Data valuation, especially quantifying data value in algorithmic prediction\nand decision-making, is a fundamental problem in data trading scenarios. The\nmost widely used method is to define the data Shapley and approximate it by\nmeans of the permutation sampling algorithm. To make up for the large\nestimation variance of the permutation sampling that hinders the development of\nthe data marketplace, we propose a more robust data valuation method using\nstratified sampling, named variance reduced data Shapley (VRDS for short). We\ntheoretically show how to stratify, how many samples are taken at each stratum,\nand the sample complexity analysis of VRDS. Finally, the effectiveness of VRDS\nis illustrated in different types of datasets and data removal applications.",
    "descriptor": "",
    "authors": [
      "Mengmeng Wu",
      "Ruoxi Jia",
      "Changle lin",
      "Wei Huang",
      "Xiangyu Chang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16835"
  },
  {
    "id": "arXiv:2210.16871",
    "title": "Improved acoustic-to-articulatory inversion using representations from  pretrained self-supervised learning models",
    "abstract": "In this work, we investigate the effectiveness of pretrained Self-Supervised\nLearning (SSL) features for learning the mapping for acoustic to articulatory\ninversion (AAI). Signal processing-based acoustic features such as MFCCs have\nbeen predominantly used for the AAI task with deep neural networks. With SSL\nfeatures working well for various other speech tasks such as speech\nrecognition, emotion classification, etc., we experiment with its efficacy for\nAAI. We train on SSL features with transformer neural networks-based AAI models\nof 3 different model complexities and compare its performance with MFCCs in\nsubject-specific (SS), pooled and fine-tuned (FT) configurations with data from\n10 subjects, and evaluate with correlation coefficient (CC) score on the unseen\nsentence test set. We find that acoustic feature reconstruction objective-based\nSSL features such as TERA and DeCoAR work well for AAI, with SS CCs of these\nSSL features reaching close to the best FT CCs of MFCC. We also find the\nresults consistent across different model sizes.",
    "descriptor": "\nComments: submitted to ICASSP 2023\n",
    "authors": [
      "Sathvik Udupa",
      "Siddarth C",
      "Prasanta Kumar Ghosh"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16871"
  },
  {
    "id": "arXiv:2210.16881",
    "title": "Real-Time MRI Video synthesis from time aligned phonemes with  sequence-to-sequence networks",
    "abstract": "Real-Time Magnetic resonance imaging (rtMRI) of the midsagittal plane of the\nmouth is of interest for speech production research. In this work, we focus on\nestimating utterance level rtMRI video from the spoken phoneme sequence. We\nobtain time-aligned phonemes from forced alignment, to obtain frame-level\nphoneme sequences which are aligned with rtMRI frames. We propose a\nsequence-to-sequence learning model with a transformer phoneme encoder and\nconvolutional frame decoder. We then modify the learning by using intermediary\nfeatures obtained from sampling from a pretrained phoneme-conditioned\nvariational autoencoder (CVAE). We train on 8 subjects in a subject-specific\nmanner and demonstrate the performance with a subjective test. We also use an\nauxiliary task of air tissue boundary (ATB) segmentation to obtain the\nobjective scores on the proposed models. We show that the proposed method is\nable to generate realistic rtMRI video for unseen utterances, and adding CVAE\nis beneficial for learning the sequence-to-sequence mapping for subjects where\nthe mapping is hard to learn.",
    "descriptor": "\nComments: submitted to ICASSP 2023\n",
    "authors": [
      "Sathvik Udupa",
      "Prasanta Kumar Ghosh"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.16881"
  },
  {
    "id": "arXiv:2210.16894",
    "title": "Distributionally Robust Domain Adaptation",
    "abstract": "Domain Adaptation (DA) has recently received significant attention due to its\npotential to adapt a learning model across source and target domains with\nmismatched distributions. Since DA methods rely exclusively on the given source\nand target domain samples, they generally yield models that are vulnerable to\nnoise and unable to adapt to unseen samples from the target domain, which calls\nfor DA methods that guarantee the robustness and generalization of the learned\nmodels. In this paper, we propose DRDA, a distributionally robust domain\nadaptation method. DRDA leverages a distributionally robust optimization (DRO)\nframework to learn a robust decision function that minimizes the worst-case\ntarget domain risk and generalizes to any sample from the target domain by\ntransferring knowledge from a given labeled source domain sample. We utilize\nthe Maximum Mean Discrepancy (MMD) metric to construct an ambiguity set of\ndistributions that provably contains the source and target domain distributions\nwith high probability. Hence, the risk is shown to upper bound the\nout-of-sample target domain loss. Our experimental results demonstrate that our\nformulation outperforms existing robust learning approaches.",
    "descriptor": "",
    "authors": [
      "Akram S. Awad",
      "George K. Atia"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16894"
  },
  {
    "id": "arXiv:2210.16898",
    "title": "Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin  Lesion Segmentation",
    "abstract": "Melanoma is caused by the abnormal growth of melanocytes in human skin. Like\nother cancers, this life-threatening skin cancer can be treated with early\ndiagnosis. To support a diagnosis by automatic skin lesion segmentation,\nseveral Fully Convolutional Network (FCN) approaches, specifically the U-Net\narchitecture, have been proposed. The U-Net model with a symmetrical\narchitecture has exhibited superior performance in the segmentation task.\nHowever, the locality restriction of the convolutional operation incorporated\nin the U-Net architecture limits its performance in capturing long-range\ndependency, which is crucial for the segmentation task in medical images. To\naddress this limitation, recently a Transformer based U-Net architecture that\nreplaces the CNN blocks with the Swin Transformer module has been proposed to\ncapture both local and global representation. In this paper, we propose\nAtt-SwinU-Net, an attention-based Swin U-Net extension, for medical image\nsegmentation. In our design, we seek to enhance the feature re-usability of the\nnetwork by carefully designing the skip connection path. We argue that the\nclassical concatenation operation utilized in the skip connection path can be\nfurther improved by incorporating an attention mechanism. By performing a\ncomprehensive ablation study on several skin lesion segmentation datasets, we\ndemonstrate the effectiveness of our proposed attention mechanism.",
    "descriptor": "",
    "authors": [
      "Ehsan Khodapanah Aghdam",
      "Reza Azad",
      "Maral Zarvani",
      "Dorit Merhof"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16898"
  },
  {
    "id": "arXiv:2210.16935",
    "title": "Scalable and self-correcting photonic computation using balanced  photonic binary tree cascades",
    "abstract": "Programmable unitary photonic networks that interfere hundreds of modes are\nemerging as a key technology in energy-efficient sensing, machine learning,\ncryptography, and linear optical quantum computing applications. In this work,\nwe establish a theoretical framework to quantify error tolerance and\nscalability in a more general class of \"binary tree cascade'' programmable\nphotonic networks that accept up to tens of thousands of discrete input modes\n$N$. To justify this scalability claim, we derive error tolerance and\nconfiguration time that scale with $\\log_2 N$ for balanced trees versus $N$ in\nunbalanced trees, despite the same number of total components. Specifically, we\nuse second-order perturbation theory to compute phase sensitivity in each\nwaveguide of balanced and unbalanced networks, and we compute the statistics of\nthe sensitivity given random input vectors. We also evaluate such networks\nafter they self-correct, or self-configure, themselves for errors in the\ncircuit due to fabrication error and environmental drift. Our findings have\nimportant implications for scaling photonic circuits to much larger circuit\nsizes; this scaling is particularly critical for applications such as principal\ncomponent analysis and fast Fourier transforms, which are important algorithms\nfor machine learning and signal processing.",
    "descriptor": "\nComments: 32 pages, 12 figures\n",
    "authors": [
      "Sunil Pai",
      "Olav Solgaard",
      "Shanhui Fan",
      "David A.B. Miller"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Emerging Technologies (cs.ET)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.16935"
  },
  {
    "id": "arXiv:2210.16955",
    "title": "Learning to Defer to Multiple Experts: Consistent Surrogate Losses,  Confidence Calibration, and Conformal Ensembles",
    "abstract": "We study the statistical properties of learning to defer (L2D) to multiple\nexperts. In particular, we address the open problems of deriving a consistent\nsurrogate loss, confidence calibration, and principled ensembling of experts.\nFirstly, we derive two consistent surrogates -- one based on a softmax\nparameterization, the other on a one-vs-all (OvA) parameterization -- that are\nanalogous to the single expert losses proposed by Mozannar and Sontag (2020)\nand Verma and Nalisnick (2022), respectively. We then study the frameworks'\nability to estimate P( m_j = y | x ), the probability that the jth expert will\ncorrectly predict the label for x. Theory shows the softmax-based loss causes\nmis-calibration to propagate between the estimates while the OvA-based loss\ndoes not (though in practice, we find there are trade offs). Lastly, we propose\na conformal inference technique that chooses a subset of experts to query when\nthe system defers. We perform empirical validation on tasks for galaxy, skin\nlesion, and hate speech classification.",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Rajeev Verma",
      "Daniel Barrej\u00f3n",
      "Eric Nalisnick"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16955"
  },
  {
    "id": "arXiv:2210.16970",
    "title": "Semantic-Native Communication: A Simplicial Complex Perspective",
    "abstract": "Semantic communication enables intelligent agents to extract meaning (or\nsemantics) of information via interaction, to carry out collaborative tasks. In\nthis paper, we study semantic communication from a topological space\nperspective, in which higher-order data semantics live in a simplicial complex.\nSpecifically, a transmitter first maps its data into a $k$-order simplicial\ncomplex and then learns its high-order correlations. The simplicial structure\nand corresponding features are encoded into semantic embeddings in latent space\nfor transmission. Subsequently, the receiver decodes the structure and infers\nthe missing or distorted data. The transmitter and receiver collaboratively\ntrain a simplicial convolutional autoencoder to accomplish the semantic\ncommunication task. Experiments are carried out on a real dataset of Semantic\nScholar Open Research Corpus, where one part of the semantic embedding is\nmissing or distorted during communication. Numerical results show that the\nsimplicial convolutional autoencoder enabled semantic communication effectively\nrebuilds the simplicial features and infer the missing data with $95\\%$\naccuracy, while achieving stable performance under channel noise. In contrast,\nthe conventional autoencoder enabled communication fails to infer any missing\ndata. Moreover, our approach is shown to effectively infer the distorted data\nwithout prior simplicial structure knowledge at the receiver, by learning\nextracted semantic information during communications. Leveraging the\ntopological nature of information, the proposed method is also shown to be more\nreliable and efficient compared to several baselines, notably at low\nsignal-to-noise (SNR) levels.",
    "descriptor": "",
    "authors": [
      "Qiyang Zhao",
      "Mehdi Bennis",
      "Merouane Debbah",
      "Daniel Benevides da Costa"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16970"
  },
  {
    "id": "arXiv:2210.16985",
    "title": "Space-time design for deep joint source channel coding of images Over  MIMO channels",
    "abstract": "We propose novel deep joint source-channel coding (DeepJSCC) algorithms for\nwireless image transmission over multi-input multi-output (MIMO) Rayleigh\nfading channels, when channel state information (CSI) is available only at the\nreceiver. We consider two different transmission schemes; one exploiting\nspatial diversity and the other one exploiting spatial multiplexing of the MIMO\nchannel. In the diversity scheme, we utilize an orthogonal space-time block\ncode (OSTBC) to achieve full diversity which increases the robustness of\ntransmission against channel variations. The multiplexing scheme, on the other\nhand, allows the user to directly map the codeword to the antennas, where the\nadditional degree-of-freedom is used to send more information about the source\nsignal. Simulation results show that the diversity scheme outperforms the\nmultiplexing scheme at lower signal-to-noise ratio (SNR) values and smaller\nnumber of receive antennas at the AP. When the number of transmit antennas is\ngreater than two, however, the full-diversity scheme becomes less beneficial.\nWe also show that both the diversity and multiplexing scheme can achieve\ncomparable performance with the state-of-the-art BPG algorithm delivered at the\nMIMO capacity in the considered scenarios.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Chenghong Bian",
      "Yulin Shao",
      "Haotian Wu",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16985"
  },
  {
    "id": "arXiv:2210.16993",
    "title": "STN: a new tensor network method to identify stimulus category from  brain activity pattern",
    "abstract": "\\begin{abstract}\nNeural decoding is still a challenge and hot topic in neurocomputing science.\nRecently, many studies have shown that brain network pattern containing rich\nspatial and temporal structure information, which represented the activation\ninformation of brain under external stimuli. %Therefore, the research of\ndecoding stimuli from brain network received extensive more attention. The\ntraditional method is to extract brain network features directly from the\ncommon machine learning method, then put these features into the classifier,\nand realize to decode external stimuli. However, this method cannot effectively\nextract the multi-dimensional structural information, which is hidden in the\nbrain network. The tensor researchers show that the tensor decomposition model\ncan fully mine unique spatio-temporal structure characteristics in\nmulti-dimensional structure data. This research proposed a stimulus constrain\ntensor brain model(STN), which involved the tensor decomposition idea and\nstimulus category constraint information. The model was verified on the real\nneuroimaging data sets (MEG and fMRI). The experimental results show that the\nSTN model achieved more $11.06\\%$ and $18.46\\%$ compared with others methods on\ntwo modal data sets. These results imply the superiority of extracting\ndiscriminative characteristics about STN model, especially for decoding object\nstimuli with semantic information.\n\\end{abstract}",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Chunyu Liu",
      "Jiacai Zhang"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.16993"
  },
  {
    "id": "arXiv:2210.16994",
    "title": "Comparison of two artificial neural networks trained for the surrogate  modeling of stress in materially heterogeneous elastoplastic solids",
    "abstract": "The purpose of this work is the systematic comparison of the application of\ntwo artificial neural networks (ANNs) to the surrogate modeling of the stress\nfield in materially heterogeneous periodic polycrystalline microstructures. The\nfirst ANN is a UNet-based convolutional neural network (CNN) for periodic data,\nand the second is based on Fourier neural operators (FNO). Both of these were\ntrained, validated, and tested with results from the numerical solution of the\nboundary-value problem (BVP) for quasi-static mechanical equilibrium in\nperiodic grain microstructures with square domains. More specifically, these\nANNs were trained to correlate the spatial distribution of material properties\nwith the equilibrium stress field under uniaxial tensile loading. The resulting\ntrained ANNs (tANNs) calculate the stress field for a given microstructure on\nthe order of 1000 (UNet) to 2500 (FNO) times faster than the numerical solution\nof the corresponding BVP.\nFor microstructures in the test dataset, the FNO-based tANN, or simply FNO,\nis more accurate than its UNet-based counterpart; the normalized mean absolute\nerror of different stress components for the former is 0.25-0.40% as compared\nto 1.41-2.15% for the latter. Errors in FNO are restricted to grain boundary\nregions, whereas the error in U-Net also comes from within the grain. In\ncomparison to U-Net, errors in FNO are more robust to large variations in\nspatial resolution as well as small variations in grain density. On other hand,\nerrors in U-Net are robust to variations in boundary box aspect ratio, whereas\nerrors in FNO increase as the domain becomes rectangular. Both tANNs are\nhowever unable to reproduce strong stress gradients, especially around regions\nof stress concentration.",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Sarthak Kapoor",
      "Jaber Rezaei Mianroodi",
      "Mohammad Khorrami",
      "Nima S. Siboni",
      "Bob Svendsen"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16994"
  },
  {
    "id": "arXiv:2210.16997",
    "title": "Almost Sure Convergence Rates of Stochastic Zeroth-order Gradient  Descent for \u0141ojasiewicz Functions",
    "abstract": "We prove \\emph{almost sure convergence rates} of Zeroth-order Gradient\nDescent (SZGD) algorithms for \\L ojasiewicz functions. The SZGD algorithm\niterates as \\begin{align*}\nx_{t+1} = x_t - \\eta_t \\widehat{\\nabla} f (x_t), \\qquad t = 0,1,2,3,\\cdots ,\n\\end{align*} where $f$ is the objective function that satisfies the \\L\nojasiewicz inequality with \\L ojasiewicz exponent $\\theta$, $\\eta_t$ is the\nstep size (learning rate), and $ \\widehat{\\nabla} f (x_t) $ is the approximate\ngradient estimated using zeroth-order information. We show that, for {smooth}\n\\L ojasiewicz functions, the sequence $\\{ x_t \\}_{t\\in\\mathbb{N}}$ governed by\nSZGD converges to a bounded point $x_\\infty$ almost surely, and $x_\\infty$ is a\ncritical point of $f$. If $\\theta \\in (0,\\frac{1}{2}]$, $ f (x_t) - f\n(x_\\infty) $, $ \\sum_{s=t}^\\infty \\| x_s - x_\\infty \\|^2$ and $ \\| x_t -\nx_\\infty \\| $ ($\\| \\cdot \\|$ is the Euclidean norm) converge to zero\n\\emph{linearly almost surely}. If $\\theta \\in (\\frac{1}{2}, 1)$, then $ f (x_t)\n- f (x_\\infty) $ (and $ \\sum_{s=t}^\\infty \\| x_{s+1} - x_s \\|^2 $) converges to\nzero at rate $o \\left( t^{\\frac{1}{1 - 2\\theta}} \\log t \\right) $ almost\nsurely; $ \\| x_{t} - x_\\infty \\| $ converges to zero at rate $o \\left(\nt^{\\frac{1-\\theta}{1-2\\theta}} \\log t \\right) $ almost surely. To the best of\nour knowledge, this paper provides the first \\emph{almost sure convergence\nrate} guarantee for stochastic zeroth order algorithms for \\L ojasiewicz\nfunctions.",
    "descriptor": "",
    "authors": [
      "Tianyu Wang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.16997"
  },
  {
    "id": "arXiv:2210.17012",
    "title": "GotFlow3D: Recurrent Graph Optimal Transport for Learning 3D Flow Motion  in Particle Tracking",
    "abstract": "Flow visualization technologies such as particle tracking velocimetry (PTV)\nare broadly used in understanding the all-pervasiveness three-dimensional (3D)\nturbulent flow from nature and industrial processes. Despite the advances in 3D\nacquisition techniques, the developed motion estimation algorithms in particle\ntracking remain great challenges of large particle displacements, dense\nparticle distributions and high computational cost. By introducing a novel deep\nneural network based on recurrent Graph Optimal Transport, called GotFlow3D, we\npresent an end-to-end solution to learn the 3D fluid flow motion from\ndouble-frame particle sets. The proposed network constructs two graphs in the\ngeometric and feature space and further enriches the original particle\nrepresentations with the fused intrinsic and extrinsic features learnt from a\ngraph neural network. The extracted deep features are subsequently utilized to\nmake optimal transport plans indicating the correspondences of particle pairs,\nwhich are then iteratively and adaptively retrieved to guide the recurrent flow\nlearning. Experimental evaluations, including assessments on numerical\nexperiments and validations on real-world experiments, demonstrate that the\nproposed GotFlow3D achieves state-of-the-art performance against both\nrecently-developed scene flow learners and particle tracking algorithms, with\nimpressive accuracy, robustness and generalization ability, which can provide\ndeeper insight into the complex dynamics of broad physical and biological\nsystems.",
    "descriptor": "",
    "authors": [
      "Jiaming Liang",
      "Chao Xu",
      "Shengze Cai"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.17012"
  },
  {
    "id": "arXiv:2210.17015",
    "title": "Emotional Brain State Classification on fMRI Data Using Deep Residual  and Convolutional Networks",
    "abstract": "The goal of emotional brain state classification on functional MRI (fMRI)\ndata is to recognize brain activity patterns related to specific emotion tasks\nperformed by subjects during an experiment. Distinguishing emotional brain\nstates from other brain states using fMRI data has proven to be challenging due\nto two factors: a difficulty to generate fast yet accurate predictions in short\ntime frames, and a difficulty to extract emotion features which generalize to\nunseen subjects. To address these challenges, we conducted an experiment in\nwhich 22 subjects viewed pictures designed to stimulate either negative,\nneutral or rest emotional responses while their brain activity was measured\nusing fMRI. We then developed two distinct Convolution-based approaches to\ndecode emotional brain states using only spatial information from single,\nminimally pre-processed (slice timing and realignment) fMRI volumes. In our\nfirst approach, we trained a 1D Convolutional Network (84.9% accuracy; chance\nlevel 33%) to classify 3 emotion conditions using One-way Analysis of Variance\n(ANOVA) voxel selection combined with hyperalignment. In our second approach,\nwe trained a 3D ResNet-50 model (78.0% accuracy; chance level 50%) to classify\n2 emotion conditions from single 3D fMRI volumes directly. Our Convolutional\nand Residual classifiers successfully learned group-level emotion features and\ncould decode emotion conditions from fMRI volumes in milliseconds. These\napproaches could potentially be used in brain computer interfaces and real-time\nfMRI neurofeedback research.",
    "descriptor": "",
    "authors": [
      "Maxime Tchibozo",
      "Donggeun Kim",
      "Zijing Wang",
      "Xiaofu He"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17015"
  },
  {
    "id": "arXiv:2210.17030",
    "title": "Uncertainty Aware Trader-Company Method: Interpretable Stock Price  Prediction Capturing Uncertainty",
    "abstract": "Machine learning is an increasingly popular tool with some success in\npredicting stock prices. One promising method is the Trader-Company~(TC)\nmethod, which takes into account the dynamism of the stock market and has both\nhigh predictive power and interpretability. Machine learning-based stock\nprediction methods including the TC method have been concentrating on point\nprediction. However, point prediction in the absence of uncertainty estimates\nlacks credibility quantification and raises concerns about safety. The\nchallenge in this paper is to make an investment strategy that combines high\npredictive power and the ability to quantify uncertainty. We propose a novel\napproach called Uncertainty Aware Trader-Company Method~(UTC) method. The core\nidea of this approach is to combine the strengths of both frameworks by merging\nthe TC method with the probabilistic modeling, which provides probabilistic\npredictions and uncertainty estimations. We expect this to retain the\npredictive power and interpretability of the TC method while capturing the\nuncertainty. We theoretically prove that the proposed method estimates the\nposterior variance and does not introduce additional biases from the original\nTC method. We conduct a comprehensive evaluation of our approach based on the\nsynthetic and real market datasets. We confirm with synthetic data that the UTC\nmethod can detect situations where the uncertainty increases and the prediction\nis difficult. We also confirmed that the UTC method can detect abrupt changes\nin data generating distributions. We demonstrate with real market data that the\nUTC method can achieve higher returns and lower risks than baselines.",
    "descriptor": "\nComments: IEEE BIGDATA 2022 Accepted\n",
    "authors": [
      "Yugo Fujimotol",
      "Kei Nakagawa",
      "Kentaro Imajo",
      "Kentaro Minami"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17030"
  },
  {
    "id": "arXiv:2210.17037",
    "title": "FrozenQubits: Boosting Fidelity of QAOA by Skipping Hotspot Nodes",
    "abstract": "Quantum Approximate Optimization Algorithm (QAOA) is one of the leading\ncandidates for demonstrating the quantum advantage using near-term quantum\ncomputers. Unfortunately, high device error rates limit us from reliably\nrunning QAOA circuits for problems with more than a few qubits. In QAOA, the\nproblem graph is translated into a quantum circuit such that every edge\ncorresponds to two 2-qubit CNOT operations in each layer of the circuit. As\nCNOTs are extremely error-prone, the fidelity of QAOA circuits is dictated by\nthe number of edges in the problem graph.\nWe observe that majority of graphs corresponding to real-world applications\nfollow the ``power-law`` distribution, where some hotspot nodes have\nsignificantly higher number of connections. We leverage this insight and\npropose ``FrozenQubits`` that freezes the hotspot nodes or qubits and\nintelligently partitions the state-space of the given problem into several\nsmaller sub-spaces which are then solved independently. The corresponding QAOA\nsub-circuits are significantly less vulnerable to gate and decoherence errors\ndue to the reduced number of CNOT operations in each sub-circuit. Unlike prior\ncircuit-cutting approaches, FrozenQubits does not require any exponentially\ncomplex post-processing step. Our evaluations with 5,300 QAOA circuits on eight\ndifferent quantum computers from IBM shows that FrozenQubits can improve the\nquality of solutions by 8.73x on average (and by up to 57x), albeit utilizing\n2x more quantum resources.",
    "descriptor": "",
    "authors": [
      "Ramin Ayanzadeh",
      "Narges Alavisamani",
      "Poulami Das",
      "Moinuddin Qureshi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.17037"
  },
  {
    "id": "arXiv:2210.17076",
    "title": "TW-BAG: Tensor-wise Brain-aware Gate Network for Inpainting Disrupted  Diffusion Tensor Imaging",
    "abstract": "Diffusion Weighted Imaging (DWI) is an advanced imaging technique commonly\nused in neuroscience and neurological clinical research through a Diffusion\nTensor Imaging (DTI) model. Volumetric scalar metrics including fractional\nanisotropy, mean diffusivity, and axial diffusivity can be derived from the DTI\nmodel to summarise water diffusivity and other quantitative microstructural\ninformation for clinical studies. However, clinical practice constraints can\nlead to sub-optimal DWI acquisitions with missing slices (either due to a\nlimited field of view or the acquisition of disrupted slices). To avoid\ndiscarding valuable subjects for group-wise studies, we propose a novel 3D\nTensor-Wise Brain-Aware Gate network (TW-BAG) for inpainting disrupted DTIs.\nThe proposed method is tailored to the problem with a dynamic gate mechanism\nand independent tensor-wise decoders. We evaluated the proposed method on the\npublicly available Human Connectome Project (HCP) dataset using common image\nsimilarity metrics derived from the predicted tensors and scalar DTI metrics.\nOur experimental results show that the proposed approach can reconstruct the\noriginal brain DTI volume and recover relevant clinical imaging information.",
    "descriptor": "\nComments: Accepted by The 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2022)\n",
    "authors": [
      "Zihao Tang",
      "Xinyi Wang",
      "Lihaowen Zhu",
      "Mariano Cabezas",
      "Dongnan Liu",
      "Michael Barnett",
      "Weidong Cai",
      "Chengyu Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.17076"
  },
  {
    "id": "arXiv:2210.17105",
    "title": "Reconfiguration of colorings in triangulations of the sphere",
    "abstract": "In 1973, Fisk proved that any $4$-coloring of a $3$-colorable triangulation\nof the $2$-sphere can be obtained from any $3$-coloring by a sequence of\nKempe-changes. On the other hand, in the case where we are only allowed to\nrecolor a single vertex in each step, which is a special case of a\nKempe-change, there exists a $4$-coloring that cannot be obtained from any\n$3$-coloring. In this paper, we present a characterization of a $4$-coloring of\na $3$-colorable triangulation of the $2$-sphere that can be obtained from a\n$3$-coloring by a sequence of recoloring operations at single vertices, and a\ncriterion for a $3$-colorable triangulation of the $2$-sphere that all\n$4$-colorings can be obtained from a $3$-coloring by such a sequence. Moreover,\nour first result can be generalized to a high-dimensional case, in which\n``$4$-coloring,'' ``$3$-colorable,'' and ``$2$-sphere'' above are replaced with\n``$k$-coloring,'' ``$(k-1)$-colorable,'' and ``$(k-2)$-sphere'' for $k \\geq 4$,\nrespectively. In addition, we show that the problem of deciding whether, for\ngiven two $(k+1)$-colorings, one can be obtained from the other by such a\nsequence is PSPACE-complete for any fixed $k \\geq 4$. Our results above can be\nrephrased as new results on the computational problems named {\\sc\n$k$-Recoloring} and {\\sc Connectedness of $k$-Coloring Reconfiguration Graph},\nwhich are fundamental problems in the field of combinatorial reconfiguration.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Takehiro Ito",
      "Yuni Iwamasa",
      "Yusuke Kobayashi",
      "Shun-ichi Maezawa",
      "Yuta Nozaki",
      "Yoshio Okamoto",
      "Kenta Ozeki"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.17105"
  },
  {
    "id": "arXiv:2210.17145",
    "title": "Probability-Dependent Gradient Decay in Large Margin Softmax",
    "abstract": "In the past few years, Softmax has become a common component in neural\nnetwork frameworks. In this paper, a gradient decay hyperparameter is\nintroduced in Softmax to control the probability-dependent gradient decay rate\nduring training. By following the theoretical analysis and empirical results of\na variety of model architectures trained on MNIST, CIFAR-10/100 and SVHN, we\nfind that the generalization performance depends significantly on the gradient\ndecay rate as the confidence probability rises, i.e., the gradient decreases\nconvexly or concavely as the sample probability increases. Moreover,\noptimization with the small gradient decay shows a similar curriculum learning\nsequence where hard samples are in the spotlight only after easy samples are\nconvinced sufficiently, and well-separated samples gain a higher gradient to\nreduce intra-class distance. Based on the analysis results, we can provide\nevidence that the large margin Softmax will affect the local Lipschitz\nconstraint of the loss function by regulating the probability-dependent\ngradient decay rate. This paper provides a new perspective and understanding of\nthe relationship among concepts of large margin Softmax, local Lipschitz\nconstraint and curriculum learning by analyzing the gradient decay rate.\nBesides, we propose a warm-up strategy to dynamically adjust Softmax loss in\ntraining, where the gradient decay rate increases from over-small to speed up\nthe convergence rate.",
    "descriptor": "",
    "authors": [
      "Siyuan Zhang",
      "Linbo Xie",
      "Ying Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17145"
  },
  {
    "id": "arXiv:2210.17153",
    "title": "The Importance of Accurate Alignments in End-to-End Speech Synthesis",
    "abstract": "Unit selection synthesis systems required accurate segmentation and labeling\nof the speech signal owing to the concatenative nature. Hidden Markov\nmodel-based speech synthesis accommodates some transcription errors, but it was\nlater shown that accurate transcriptions yield highly intelligible speech with\nsmaller amounts of training data. With the arrival of end-to-end (E2E) systems,\nit was observed that very good quality speech could be synthesised with large\namounts of data. As end-to-end synthesis progressed from Tacotron to\nFastSpeech2, it has become imminent that features that represent prosody are\nimportant for good-quality synthesis. In particular, durations of the sub-word\nunits are important. Variants of FastSpeech use a teacher model or forced\nalignments to obtain good-quality synthesis. In this paper, we focus on\nduration prediction, using signal processing cues in tandem with forced\nalignment to produce accurate phone durations during training.\nThe current work aims to highlight the importance of accurate alignments for\ngood-quality synthesis. An attempt is made to train the E2E systems with\naccurately labeled data, and compare the same with approximately labeled data.",
    "descriptor": "\nComments: Version 1 uploaded\n",
    "authors": [
      "Anusha Prakash",
      "Hema A Murthy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17153"
  },
  {
    "id": "arXiv:2210.17154",
    "title": "Minimum Processing Near-end Listening Enhancement",
    "abstract": "The intelligibility and quality of speech from a mobile phone or public\nannouncement system are often affected by background noise in the listening\nenvironment. By pre-processing the speech signal it is possible to improve the\nspeech intelligibility and quality -- this is known as near-end listening\nenhancement (NLE). Although, existing NLE techniques are able to greatly\nincrease intelligibility in harsh noise environments, in favorable noise\nconditions the intelligibility of speech reaches a ceiling where it cannot be\nfurther enhanced. Actually, the focus of existing methods solely on improving\nthe intelligibility causes unnecessary processing of the speech signal and\nleads to speech distortions and quality degradations. In this paper, we provide\na new rationale for NLE, where the target speech is minimally processed in\nterms of a processing penalty, provided that a certain performance constraint,\ne.g., intelligibility, is satisfied. We present a closed-form solution for the\ncase where the performance criterion is an intelligibility estimator based on\nthe approximated speech intelligibility index and the processing penalty is the\nmean-square error between the processed and the clean speech. This produces an\nNLE method that adapts to changing noise conditions via a simple gain rule by\nlimiting the processing to the minimum necessary to achieve a desired\nintelligibility, while at the same time focusing on quality in favorable noise\nsituations by minimizing the amount of speech distortions. Through simulation\nstudies, we show the proposed method attains speech quality on par or better\nthan existing methods in both objective measurements and subjective listening\ntests, whilst still sustaining objective speech intelligibility performance on\npar with existing methods.",
    "descriptor": "",
    "authors": [
      "Andreas Jonas Fuglsig",
      "Jesper Jensen",
      "Zheng-Hua Tan",
      "Lars S\u00f8ndergaard Bertelsen",
      "Jens Christian Lindof",
      "Jan \u00d8stergaard"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.17154"
  },
  {
    "id": "arXiv:2210.17156",
    "title": "Bifurcation analysis of the Microscopic Markov Chain Approach to  contact-based epidemic spreading in networks",
    "abstract": "The dynamics of many epidemic compartmental models for infectious diseases\nthat spread in a single host population present a second-order phase\ntransition. This transition occurs as a function of the infectivity parameter,\nfrom the absence of infected individuals to an endemic state. Here, we study\nthis transition, from the perspective of dynamical systems, for a discrete-time\ncompartmental epidemic model known as Microscopic Markov Chain Approach, whose\napplicability for forecasting future scenarios of epidemic spreading has been\nproved very useful during the COVID-19 pandemic. We show that there is an\nendemic state which is stable and a global attractor and that its existence is\na consequence of a transcritical bifurcation. This mathematical analysis\ngrounds the results of the model in practical applications.",
    "descriptor": "\nComments: 15 pages, 3 figures\n",
    "authors": [
      "Alex Arenas",
      "Antonio Garijo",
      "Sergio G\u00f3mez",
      "Jordi Villadelprat"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.17156"
  },
  {
    "id": "arXiv:2210.17177",
    "title": "Variational Inference Aided Estimation of Time Varying Channels",
    "abstract": "One way to improve the estimation of time varying channels is to incorporate\nknowledge of previous observations. In this context, Dynamical VAEs (DVAEs)\nbuild a promising deep learning (DL) framework which is well suited to learn\nthe distribution of time series data. We introduce a new DVAE architecture,\ncalled k-MemoryMarkovVAE (k-MMVAE), whose sparsity can be controlled by an\nadditional memory parameter. Following the approach in [1] we derive a k-MMVAE\naided channel estimator which takes temporal correlations of successive\nobservations into account. The results are evaluated on simulated channels by\nQuaDRiGa and show that the k-MMVAE aided channel estimator clearly outperforms\nother machine learning (ML) aided estimators which are either memoryless or\nnaively extended to time varying channels without major adaptions.",
    "descriptor": "",
    "authors": [
      "Benedikt B\u00f6ck",
      "Michael Baur",
      "Valentina Rizzello",
      "Wolfgang Utschick"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17177"
  },
  {
    "id": "arXiv:2210.17189",
    "title": "DiaCorrect: End-to-end error correction for speaker diarization",
    "abstract": "In recent years, speaker diarization has attracted widespread attention. To\nachieve better performance, some studies propose to diarize speech in multiple\nstages. Although these methods might bring additional benefits, most of them\nare quite complex. Motivated by spelling correction in automatic speech\nrecognition (ASR), in this paper, we propose an end-to-end error correction\nframework, termed DiaCorrect, to refine the initial diarization results in a\nsimple but efficient way. By exploiting the acoustic interactions between input\nmixture and its corresponding speaker activity, DiaCorrect could automatically\nadapt the initial speaker activity to minimize the diarization errors. Without\nbells and whistles, experiments on LibriSpeech based 2-speaker meeting-like\ndata show that, the self-attentitive end-to-end neural diarization (SA-EEND)\nbaseline with DiaCorrect could reduce its diarization error rate (DER) by over\n62.4% from 12.31% to 4.63%. Our source code is available online at\nhttps://github.com/jyhan03/diacorrect.",
    "descriptor": "\nComments: submitted to ICASSP\n",
    "authors": [
      "Jiangyu Han",
      "Yuhang Cao",
      "Heng Lu",
      "Yanhua Long"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17189"
  },
  {
    "id": "arXiv:2210.17196",
    "title": "Optimal Trajectory Planning and Task Assignment for UAV-assisted Fog  Computing",
    "abstract": "Fog computing is an emerging distributed computing model for the Internet of\nThings (IoT). It extends computing and caching functions to the edge of\nwireless networks. Uncrewed Aerial Vehicles (UAVs) provide adequate support for\nfog computing. UAVs can not only act as a relay between mobile users and\nphysically remote edge devices to avoid costly long-range wireless\ncommunications but also are equipped with computing facilities that can take\nover specific tasks. In this paper, we aim to optimize the energy efficiency of\na fog computing system assisted by a single UAV by planning the trajectories of\nthe UAV and assigning computing tasks to different devices, including the UAV\nitself. We propose two algorithms based on the classical Ant Colony and\nParticle Swarm Optimization techniques and solve the problem by continuous\nconvex approximation. Unlike most existing studies where the trajectories are\nassumed to be straight lines, we account for the effect of obstacles, such as\nbuildings, and deliberately avoid them during the trajectory planning phase.\nThrough extensive simulation experiments, we demonstrate that our proposed\napproach can achieve significantly better energy efficiency than existing\nbenchmark algorithms.",
    "descriptor": "",
    "authors": [
      "Shuaijun Liu",
      "Jiaying Yin",
      "Zishu Zeng",
      "Jingjin Wu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.17196"
  },
  {
    "id": "arXiv:2210.17230",
    "title": "Lipschitz regularized gradient flows and latent generative particles",
    "abstract": "Lipschitz regularized f-divergences are constructed by imposing a bound on\nthe Lipschitz constant of the discriminator in the variational representation.\nThey interpolate between the Wasserstein metric and f-divergences and provide a\nflexible family of loss functions for non-absolutely continuous (e.g.\nempirical) distributions, possibly with heavy tails. We construct Lipschitz\nregularized gradient flows on the space of probability measures based on these\ndivergences. Examples of such gradient flows are Lipschitz regularized\nFokker-Planck and porous medium partial differential equations (PDEs) for the\nKullback-Leibler and alpha-divergences, respectively. The regularization\ncorresponds to imposing a Courant-Friedrichs-Lewy numerical stability condition\non the PDEs. For empirical measures, the Lipschitz regularization on gradient\nflows induces a numerically stable transporter/discriminator particle\nalgorithm, where the generative particles are transported along the gradient of\nthe discriminator. The gradient structure leads to a regularized Fisher\ninformation (particle kinetic energy) used to track the convergence of the\nalgorithm. The Lipschitz regularized discriminator can be implemented via\nneural network spectral normalization and the particle algorithm generates\napproximate samples from possibly high-dimensional distributions known only\nfrom data. Notably, our particle algorithm can generate synthetic data even in\nsmall sample size regimes. A new data processing inequality for the regularized\ndivergence allows us to combine our particle algorithm with representation\nlearning, e.g. autoencoder architectures. The resulting algorithm yields\nmarkedly improved generative properties in terms of efficiency and quality of\nthe synthetic samples. From a statistical mechanics perspective the encoding\ncan be interpreted dynamically as learning a better mobility for the generative\nparticles.",
    "descriptor": "",
    "authors": [
      "Hyemin Gu",
      "Panagiota Birmpa",
      "Yiannis Pantazis",
      "Luc Rey-Bellet",
      "Markos A. Katsoulakis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17230"
  },
  {
    "id": "arXiv:2210.17237",
    "title": "Latent Multimodal Functional Graphical Model Estimation",
    "abstract": "Joint multimodal functional data acquisition, where functional data from\nmultiple modes are measured simultaneously from the same subject, has emerged\nas an exciting modern approach enabled by recent engineering breakthroughs in\nthe neurological and biological sciences. One prominent motivation to acquire\nsuch data is to enable new discoveries of the underlying connectivity by\ncombining multimodal signals. Despite the scientific interest, there remains a\ngap in principled statistical methods for estimating the graph underlying\nmultimodal functional data. To this end, we propose a new integrative framework\nthat models the data generation process and identifies operators mapping from\nthe observation space to the latent space. We then develop an estimator that\nsimultaneously estimates the transformation operators and the latent graph.\nThis estimator is based on the partial correlation operator, which we\nrigorously extend from the multivariate to the functional setting. Our\nprocedure is provably efficient, with the estimator converging to a stationary\npoint with quantifiable statistical error. Furthermore, we show recovery of the\nlatent graph under mild conditions. Our work is applied to analyze\nsimultaneously acquired multimodal brain imaging data where the graph indicates\nfunctional connectivity of the brain. We present simulation and empirical\nresults that support the benefits of joint estimation.",
    "descriptor": "",
    "authors": [
      "Katherine Tsai",
      "Boxin Zhao",
      "Oluwasanmi Koyejo",
      "Mladen Kolar"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17237"
  },
  {
    "id": "arXiv:2210.17239",
    "title": "RIS-Based Steerable Beamforming Antenna with Near-Field Eigenmode Feeder",
    "abstract": "We present a novel, power and hardware efficient, ``reconfigurable'' antenna\nsystem leveraging the eigenmodes of the over-the-air propagation matrix T from\nan active multi-antenna feeder (AMAF) placed in the near-field of a large\nreflective intelligent surface (RIS) to the RIS. We consider directive antenna\nelements with finite non-zero aperture constituting the two standard linear\narrays (SLAs) - the AMAF and the RIS. We demonstrate the flexibility of the\nproposed architecture by showing that it is capable of generating radiation\npatterns for multiple applications such as very narrow beams with low side\nlobes for space-division multiple access communications, wide-angle beams for\nshort range automotive sensing and sectorial beaconing, and monopulse patterns\nfor radar angular tracking. A key parameter in our design is the AMAF-RIS\ndistance which must be optimized and it is generally much less than the\nRayleigh distance. Active RF amplification is done at the AMAF only with almost\nno signal splitting unlike conventional RF beamforming for massive MIMO systems\nin millimeter-wave (mmWave) and sub-teraHertz (sub-THz) bands. For a given AMAF\naperture, the optimal AMAF-RIS distance increases as a function of the RIS\nsize. The AMAF-RIS loss is compensated almost exactly by the larger aperture\ngain of the RIS leading to almost constant RIS gain with increasing RIS sizes.\nThis allows to choose different beam angular selectivities with the same center\nbeam gain.",
    "descriptor": "",
    "authors": [
      "Krishan K. Tiwari",
      "Giuseppe Caire"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.17239"
  },
  {
    "id": "arXiv:2210.17253",
    "title": "House of Graphs 2.0: a database of interesting graphs and more",
    "abstract": "In 2012 we announced the House of Graphs (https://houseofgraphs.org)\n[Discrete Appl. Math. 161 (2013), 311-314], which was a new database of graphs.\nThe House of Graphs hosts complete lists of graphs of various graph classes,\nbut its main feature is a searchable database of so called \"interesting\"\ngraphs, which includes graphs that already occurred as extremal graphs or as\ncounterexamples to conjectures. An important aspect of this database is that it\ncan be extended by users of the website.\nOver the years, several new features and graph invariants were added to the\nHouse of Graphs and users uploaded many interesting graphs to the website. But\nas the development of the original House of Graphs website started in 2010, the\nunderlying frameworks and technologies of the website became outdated. This is\nwhy we completely rebuilt the House of Graphs using modern frameworks to build\na maintainable and expandable web application that is future-proof. On top of\nthis, several new functionalities were added to improve the application and the\nuser experience.\nThis article describes the changes and new features of the new House of\nGraphs website.",
    "descriptor": "\nComments: 16 pages; to appear in Discrete Applied Mathematics\n",
    "authors": [
      "Kris Coolsaet",
      "Sven D'hondt",
      "Jan Goedgebeur"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.17253"
  },
  {
    "id": "arXiv:2210.17269",
    "title": "Scoliosis Detection using Deep Neural Network",
    "abstract": "Scoliosis is a sideways curvature of the spine that most often is diagnosed\namong young teenagers. It dramatically affects the quality of life, which can\ncause complications from heart and lung injuries in severe cases. The current\ngold standard to detect and estimate scoliosis is to manually examine the\nspinal anterior-posterior X-ray images. This process is time-consuming,\nobserver-dependent, and has high inter-rater variability. Consequently, there\nhas been increasing interest in automatic scoliosis estimation from spinal\nX-ray images, and the development of deep learning has shown amazing\nachievements in automatic spinal curvature estimation. The main target of this\nthesis is to review the fundamental concepts of deep learning, analyze how deep\nlearning is applied to detect spinal curvature, explore the practical deep\nlearning-based models that have been employed. It aims to improve the accuracy\nof scoliosis detection and implement the most successful one for automated Cobb\nangle prediction. Keywords: Scoliosis Detection, Spinal Curvature Estimation,\nDeep Learning. i",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1911.03723 by other authors\n",
    "authors": [
      "Yen Hoang Nguyen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17269"
  },
  {
    "id": "arXiv:2210.17287",
    "title": "A Versatile Diffusion-based Generative Refiner for Speech Enhancement",
    "abstract": "Although deep neural network (DNN)-based speech enhancement (SE) methods\noutperform the previous non-DNN-based ones, they often degrade the perceptual\nquality of generated outputs. To tackle this problem, We introduce a DNN-based\ngenerative refiner aiming to improve perceptual speech quality pre-processed by\nan SE method. As the refiner, we train a diffusion-based generative model by\nutilizing a dataset consisting of clean speech only. Then, the model replaces\nthe degraded and distorted parts caused by a preceding SE method with newly\ngenerated clean parts by denoising diffusion restoration. Once our refiner is\ntrained on a set of clean speech, it can be applied to various SE methods\nwithout additional training specialized for each SE module. Therefore, our\nrefiner can be a versatile post-processing module w.r.t. SE methods and has\nhigh potential in terms of modularity. Experimental results show that our\nmethod improved perceptual speech quality regardless of the preceding SE\nmethods used.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Ryosuke Sawata",
      "Naoki Murata",
      "Yuhta Takida",
      "Toshimitsu Uesaka",
      "Takashi Shibuya",
      "Shusuke Takahashi",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17287"
  },
  {
    "id": "arXiv:2210.17299",
    "title": "Bayesian Model Selection of Lithium-Ion Battery Models via Bayesian  Quadrature",
    "abstract": "This paper presents a Bayesian model selection approach via Bayesian\nquadrature and sensitivity analysis of the selection criterion for a\nlithium-ion battery model. The Bayesian model evidence is adopted as the\nmetric, which can select the simplest but well-describing model based on\nOccam's razor principle. While the model evidence requires prohibitive integral\ncomputations over parameter space, Bayesian quadrature offers sample-efficient\nintegration via model-based inference to minimise the number of battery model\nevaluations. The posterior distribution of battery model parameters can also be\ninferred as a byproduct in one go, which is also beneficial in creating a\ndigital twin. The simplest lithium-ion battery models, equivalent circuit\nmodels, were used to analyse the sensitivity of the selection criterion at\ngiven different datasets and model configurations. We show that popular\nselection criteria, such as root-mean-square error, and Bayesian information\ncriterion, can fail to select a correct model in a multimodal posterior case.\nThe model evidence can spot the true model in such cases, simultaneously\nproviding the variance of evidence inference itself as an indication of\nconfidence. Bayesian quadrature can compute the evidence faster than popular\nMCMC solvers.",
    "descriptor": "\nComments: 8 pages, 2 figures\n",
    "authors": [
      "Masaki Adachi",
      "Yannick Kuhn",
      "Birger Horstmann",
      "Michael A. Osborne",
      "David A. Howey"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.17299"
  },
  {
    "id": "arXiv:2210.17310",
    "title": "Convolution-Based Channel-Frequency Attention for Text-Independent  Speaker Verification",
    "abstract": "Deep convolutional neural networks (CNNs) have been applied to extracting\nspeaker embeddings with significant success in speaker verification.\nIncorporating the attention mechanism has shown to be effective in improving\nthe model performance. This paper presents an efficient two-dimensional\nconvolution-based attention module, namely C2D-Att. The interaction between the\nconvolution channel and frequency is involved in the attention calculation by\nlightweight convolution layers. This requires only a small number of\nparameters. Fine-grained attention weights are produced to represent channel\nand frequency-specific information. The weights are imposed on the input\nfeatures to improve the representation ability for speaker modeling. The\nC2D-Att is integrated into a modified version of ResNet for speaker embedding\nextraction. Experiments are conducted on VoxCeleb datasets. The results show\nthat C2DAtt is effective in generating discriminative attention maps and\noutperforms other attention methods. The proposed model shows robust\nperformance with different scales of model size and achieves state-of-the-art\nresults.",
    "descriptor": "",
    "authors": [
      "Jingyu Li",
      "Yusheng Tian",
      "Tan Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17310"
  },
  {
    "id": "arXiv:2210.17316",
    "title": "There is more than one kind of robustness: Fooling Whisper with  adversarial examples",
    "abstract": "Whisper is a recent Automatic Speech Recognition (ASR) model displaying\nimpressive robustness to both out-of-distribution inputs and random noise. In\nthis work, we show that this robustness does not carry over to adversarial\nnoise. We generate very small input perturbations with Signal Noise Ratio of up\nto 45dB, with which we can degrade Whisper performance dramatically, or even\ntranscribe a target sentence of our choice. We also show that by fooling the\nWhisper language detector we can very easily degrade the performance of\nmultilingual models. These vulnerabilities of a widely popular open-source\nmodel have practical security implications, and emphasize the need for\nadversarially robust ASR.",
    "descriptor": "",
    "authors": [
      "Raphael Olivier",
      "Bhiksha Raj"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17316"
  },
  {
    "id": "arXiv:2210.17319",
    "title": "Physics-Informed CNNs for Super-Resolution of Sparse Observations on  Dynamical Systems",
    "abstract": "In the absence of high-resolution samples, super-resolution of sparse\nobservations on dynamical systems is a challenging problem with wide-reaching\napplications in experimental settings. We showcase the application of\nphysics-informed convolutional neural networks for super-resolution of sparse\nobservations on grids. Results are shown for the chaotic-turbulent Kolmogorov\nflow, demonstrating the potential of this method for resolving finer scales of\nturbulence when compared with classic interpolation methods, and thus\neffectively reconstructing missing physics.",
    "descriptor": "\nComments: Published in NeurIPS 2022: Machine Learning and the Physical Sciences Workshop. Code at this https URL arXiv admin note: text overlap with arXiv:2210.16215\n",
    "authors": [
      "Daniel Kelshaw",
      "Georgios Rigas",
      "Luca Magri"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17319"
  },
  {
    "id": "arXiv:2210.17326",
    "title": "Model Compression for DNN-Based Text-Independent Speaker Verification  Using Weight Quantization",
    "abstract": "DNN-based models achieve high performance in the speaker verification (SV)\ntask with substantial computation costs. The model size is an essential concern\nin applying models on resource-constrained devices, while model compression for\nSV models has not been studied extensively in previous works. Weight\nquantization is exploited to compress DNN-based speaker embedding extraction\nmodels in this paper. Uniform and Powers-of-Two quantization are utilized in\nthe experiments. The results on VoxCeleb show that the weight quantization can\ndecrease the size of ECAPA-TDNN and ResNet by 4 times with insignificant\nperformance decline. The quantized 4-bit ResNet achieves similar performance to\nthe original model with an 8 times smaller size. We empirically show that the\nperformance of ECAPA-TDNN is more sensitive than ResNet to quantization due to\nthe difference in weight distribution. The experiments on CN-Celeb also\ndemonstrate that quantized models are robust for SV in the language mismatch\nscenario.",
    "descriptor": "",
    "authors": [
      "Jingyu Li",
      "Zhaoyang Zhang",
      "Jiong Wang",
      "Tan Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17326"
  },
  {
    "id": "arXiv:2210.17327",
    "title": "Diffusion-based Generative Speech Source Separation",
    "abstract": "We propose a new single channel source separation method based on\nscore-matching of a stochastic differential equation (SDE). We craft a tailored\ncontinuous time diffusion-mixing process starting from the separated sources\nand converging to a Gaussian distribution centered on their mixture. This\nformulation lets us apply the machinery of score-based generative modelling.\nFirst, we train a neural network to approximate the score function of the\nmarginal probabilities or the diffusion-mixing process. Then, we use it to\nsolve the reverse time SDE that progressively separates the sources starting\nfrom their mixture. We propose a modified training strategy to handle model\nmismatch and source permutation ambiguity. Experiments on the WSJ0 2mix dataset\ndemonstrate the potential of the method. Furthermore, the method is also\nsuitable for speech enhancement and shows performance competitive with prior\nwork on the VoiceBank-DEMAND dataset.",
    "descriptor": "\nComments: 5 pages, 3 figures, 2 tables. Submitted to ICASSP 2023\n",
    "authors": [
      "Robin Scheibler",
      "Youna Ji",
      "Soo-Whan Chung",
      "Jaeuk Byun",
      "Soyeon Choe",
      "Min-Seok Choi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17327"
  },
  {
    "id": "arXiv:2210.17338",
    "title": "VoicePrivacy 2022 System Description: Speaker Anonymization with  Feature-matched F0 Trajectories",
    "abstract": "We introduce a novel method to improve the performance of the VoicePrivacy\nChallenge 2022 baseline B1 variants. Among the known deficiencies of\nx-vector-based anonymization systems is the insufficient disentangling of the\ninput features. In particular, the fundamental frequency (F0) trajectories,\nwhich are used for voice synthesis without any modifications. Especially in\ncross-gender conversion, this situation causes unnatural sounding voices,\nincreases word error rates (WERs), and personal information leakage. Our\nsubmission overcomes this problem by synthesizing an F0 trajectory, which\nbetter harmonizes with the anonymized x-vector. We utilized a low-complexity\ndeep neural network to estimate an appropriate F0 value per frame, using the\nlinguistic content from the bottleneck features (BN) and the anonymized\nx-vector. Our approach results in a significantly improved anonymization system\nand increased naturalness of the synthesized voice. Consequently, our results\nsuggest that F0 extraction is not required for voice anonymization.",
    "descriptor": "\nComments: 4 pages, 4 figures, 2 tables, submitted to VoicePrivacy Challenge 2022\n",
    "authors": [
      "\u00dcnal Ege Gaznepoglu",
      "Anna Leschanowsky",
      "Nils Peters"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17338"
  },
  {
    "id": "arXiv:2210.17347",
    "title": "Optimal Control for Wind Turbine Wake Mixing on Floating Platforms",
    "abstract": "Dynamic induction control is a wind farm flow control strategy that utilises\nwind turbine thrust variations to accelerate breakdown of the aerodynamic wake\nand improve downstream turbine performance. However, when floating wind\nturbines are considered, additional dynamics and challenges appear that make\noptimal control difficult. In this work, we propose an adjoint optimisation\nframework for non-linear economic model-predictive control, which utilises a\nnovel coupling of an existing aerodynamic wake model to floating platform\nhydrodynamics. Analysis of the frequency response for the coupled model shows\nthat it is possible to achieve wind turbine thrust variations without inducing\nlarge motion of the rotor. Using economic model-predictive control, we find\ndynamic induction results that lead to an improvement of 7% over static\ninduction control, where the dynamic controller stimulates wake breakdown with\nonly small variations in rotor displacement. This novel model formulation\nprovides a starting point for the adaptation of dynamic wind farm flow control\nstrategies for floating wind turbines.",
    "descriptor": "\nComments: 6 pages, 8 figures, submitted to IFAC World Congress 2023\n",
    "authors": [
      "Maarten J. van den Broek",
      "Daniel van den Berg",
      "Benjamin Sanderse",
      "Jan-Willem van Wingerden"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2210.17347"
  },
  {
    "id": "arXiv:2210.17401",
    "title": "TransEDRP: Dual Transformer model with Edge Emdedded for Drug Respond  Prediction",
    "abstract": "GNN-based methods have achieved excellent results as a mainstream task in\ndrug response prediction tasks in recent years. Traditional GNN methods use\nonly the atoms in a drug molecule as nodes to obtain the representation of the\nmolecular graph through node information passing, whereas the method using the\ntransformer can only extract information about the nodes. However, the covalent\nbonding and chirality of a drug molecule have a great influence on the\npharmacological properties of the molecule, and these information are implied\nin the chemical bonds formed by the edges between the atoms. In addition, CNN\nmethods for modelling cell lines genomics sequences can only perceive local\nrather than global information about the sequence. In order to solve the above\nproblems, we propose the decoupled dual transformer structure with edge\nembedded for drug respond prediction (TransEDRP), which is used for the\nrepresentation of cell line genomics and drug respectively. For the drug\nbranch, we encoded the chemical bond information within the molecule as the\nembedding of the edge in the molecular graph, extracted the global structural\nand biochemical information of the drug molecule using graph transformer. For\nthe branch of cell lines genomics, we use the multi-headed attention mechanism\nto globally represent the genomics sequence. Finally, the drug and genomics\nbranches are fused to predict IC50 values through the transformer layer and the\nfully connected layer, which two branches are different modalities. Extensive\nexperiments have shown that our method is better than the current mainstream\napproach in all evaluation indicators.",
    "descriptor": "\nComments: 8 pages, 5 figures, 4 tables\n",
    "authors": [
      "Li Kun",
      "Hu Wenbin"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17401"
  },
  {
    "id": "arXiv:2210.17405",
    "title": "Exact and Approximate Conformal Inference in Multiple Dimensions",
    "abstract": "It is common in machine learning to estimate a response y given covariate\ninformation x. However, these predictions alone do not quantify any uncertainty\nassociated with said predictions. One way to overcome this deficiency is with\nconformal inference methods, which construct a set containing the unobserved\nresponse y with a prescribed probability. Unfortunately, even with\none-dimensional responses, conformal inference is computationally expensive\ndespite recent encouraging advances. In this paper, we explore the\nmultidimensional response case within a regression setting, delivering exact\nderivations of conformal inference p-values when the predictive model can be\ndescribed as a linear function of y. Additionally, we propose different\nefficient ways of approximating the conformal prediction region for non-linear\npredictors while preserving computational advantages. We also provide empirical\njustification for these approaches using a real-world data example.",
    "descriptor": "\nComments: 18 pages with supplemental material, 10 figures\n",
    "authors": [
      "Chancellor Johnstone",
      "Eugene Ndiaye"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Computation (stat.CO)",
      "Other Statistics (stat.OT)"
    ],
    "url": "https://arxiv.org/abs/2210.17405"
  },
  {
    "id": "arXiv:2210.17408",
    "title": "Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling  for Medical Image Segmentation",
    "abstract": "Based on the Denoising Diffusion Probabilistic Model (DDPM), medical image\nsegmentation can be described as a conditional image generation task, which\nallows to compute pixel-wise uncertainty maps of the segmentation and allows an\nimplicit ensemble of segmentations to boost the segmentation performance.\nHowever, DDPM requires many iterative denoising steps to generate segmentations\nfrom Gaussian noise, resulting in extremely inefficient inference. To mitigate\nthe issue, we propose a principled acceleration strategy, called\npre-segmentation diffusion sampling DDPM (PD-DDPM), which is specially used for\nmedical image segmentation. The key idea is to obtain pre-segmentation results\nbased on a separately trained segmentation network, and construct noise\npredictions (non-Gaussian distribution) according to the forward diffusion\nrule. We can then start with noisy predictions and use fewer reverse steps to\ngenerate segmentation results. Experiments show that PD-DDPM yields better\nsegmentation results over representative baseline methods even if the number of\nreverse steps is significantly reduced. Moreover, PD-DDPM is orthogonal to\nexisting advanced segmentation models, which can be combined to further improve\nthe segmentation performance.",
    "descriptor": "",
    "authors": [
      "Xutao Guo",
      "Yanwu Yang",
      "Chenfei Ye",
      "Shang Lu",
      "Yang Xiang",
      "Ting Ma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17408"
  },
  {
    "id": "arXiv:2210.17456",
    "title": "Audio-Visual Speech Enhancement and Separation by Leveraging Multi-Modal  Self-Supervised Embeddings",
    "abstract": "AV-HuBERT, a multi-modal self-supervised learning model, has been shown to be\neffective for categorical problems such as automatic speech recognition and\nlip-reading. This suggests that useful audio-visual speech representations can\nbe obtained via utilizing multi-modal self-supervised embeddings. Nevertheless,\nit is unclear if such representations can be generalized to solve real-world\nmulti-modal AV regression tasks, such as audio-visual speech enhancement (AVSE)\nand audio-visual speech separation (AVSS). In this study, we leveraged the\npre-trained AV-HuBERT model followed by an SE module for AVSE and AVSS.\nComparative experimental results demonstrate that our proposed model performs\nbetter than the state-of-the-art AVSE and traditional audio-only SE models. In\nsummary, our results confirm the effectiveness of our proposed model for the\nAVSS task with proper fine-tuning strategies, demonstrating that multi-modal\nself-supervised embeddings obtained from AV-HUBERT can be generalized to\naudio-visual regression tasks.",
    "descriptor": "\nComments: Under peer review\n",
    "authors": [
      "I-Chun Chern",
      "Kuo-Hsuan Hung",
      "Yi-Ting Chen",
      "Tassadaq Hussain",
      "Mandar Gogate",
      "Amir Hussain",
      "Yu Tsao",
      "Jen-Cheng Hou"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.17456"
  },
  {
    "id": "arXiv:2210.17465",
    "title": "Convergence Guarantees of a Distributed Network Equivalence Algorithm  for Distribution-OPF",
    "abstract": "The massive integration of distributed energy resources changes the\noperational demands of the electric power distribution system, motivating\noptimization-based approaches. The added computational complexities of the\nresulting optimal power flow (OPF) problem have generally been managed by\napproximated or relaxed models; however, they may lead to infeasible or\ninaccurate solutions. Decomposition-based methods have also been used to solve\nthe OPF problems. But the existing methods require several message passing\nrounds for relatively small systems, causing significant delays in decision\nmaking; related feedback-based methods also suffer from slow tracking of the\noptimal solutions. In this paper, we propose a provably convergent distributed\nalgorithm to solve the nonlinear OPF problem for power distribution systems.\nOur method is based on a previously developed decomposition-based optimization\nmethod that employs the network equivalence method. We present a thorough\nmathematical analysis that includes sufficient conditions that guarantee\nconvergence of the method. We also present simulation results using the\nIEEE-123 bus test system to demonstrate the algorithm's effectiveness and\nprovide additional insights into theoretical results.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Yunqi Luo",
      "Rabayet Sadnan",
      "Bala Krishnamoorthy",
      "Anamika Dubey"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.17465"
  },
  {
    "id": "arXiv:2210.17478",
    "title": "TiAda: A Time-scale Adaptive Algorithm for Nonconvex Minimax  Optimization",
    "abstract": "Adaptive gradient methods have shown their ability to adjust the stepsizes on\nthe fly in a parameter-agnostic manner, and empirically achieve faster\nconvergence for solving minimization problems. When it comes to nonconvex\nminimax optimization, however, current convergence analyses of gradient descent\nascent (GDA) combined with adaptive stepsizes require careful tuning of\nhyper-parameters and the knowledge of problem-dependent parameters. Such a\ndiscrepancy arises from the primal-dual nature of minimax problems and the\nnecessity of delicate time-scale separation between the primal and dual updates\nin attaining convergence. In this work, we propose a single-loop adaptive GDA\nalgorithm called TiAda for nonconvex minimax optimization that automatically\nadapts to the time-scale separation. Our algorithm is fully parameter-agnostic\nand can achieve near-optimal complexities simultaneously in deterministic and\nstochastic settings of nonconvex-strongly-concave minimax problems. The\neffectiveness of the proposed method is further justified numerically for a\nnumber of machine learning applications.",
    "descriptor": "",
    "authors": [
      "Xiang Li",
      "Junchi Yang",
      "Niao He"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17478"
  },
  {
    "id": "arXiv:2210.17545",
    "title": "Unclonability and Quantum Cryptanalysis: From Foundations to  Applications",
    "abstract": "The impossibility of creating perfect identical copies of unknown quantum\nsystems is a fundamental concept in quantum theory and one of the main\nnon-classical properties of quantum information. This limitation imposed by\nquantum mechanics, famously known as the no-cloning theorem, has played a\ncentral role in quantum cryptography as a key component in the security of\nquantum protocols. In this thesis, we look at Unclonability in a broader\ncontext in physics and computer science and more specifically through the lens\nof cryptography, learnability and hardware assumptions. We introduce new\nnotions of unclonability in the quantum world, namely quantum physical\nunclonability, and study the relationship with cryptographic properties and\nassumptions such as unforgeability, and quantum pseudorandomness. The purpose\nof this study is to bring new insights into the field of quantum cryptanalysis\nand into the notion of unclonability itself. We also discuss several\napplications of this new type of unclonability as a cryptographic resource for\ndesigning provably secure quantum protocols. Furthermore, we present a new\npractical cryptanalysis technique concerning the problem of approximate cloning\nof quantum states. We design a quantum machine learning-based cryptanalysis\nalgorithm to demonstrate the power of quantum learning tools as both attack\nstrategies and powerful tools for the practical study of quantum unclonability.",
    "descriptor": "\nComments: PhD Thesis. Defended 17th August 2022, University of Edinburgh\n",
    "authors": [
      "Mina Doosti"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.17545"
  },
  {
    "id": "arXiv:2210.17550",
    "title": "Nesterov Meets Optimism: Rate-Optimal Optimistic-Gradient-Based Method  for Stochastic Bilinearly-Coupled Minimax Optimization",
    "abstract": "We provide a novel first-order optimization algorithm for bilinearly-coupled\nstrongly-convex-concave minimax optimization called the AcceleratedGradient\nOptimisticGradient (AG-OG). The main idea of our algorithm is to leverage the\nstructure of the considered minimax problem and operates Nesterov's\nacceleration on the individual part and optimistic gradient on the coupling\npart of the objective. We motivate our method by showing that its\ncontinuous-time dynamics corresponds to an organic combination of the dynamics\nof optimistic gradient and of Nesterov's acceleration. By discretizing the\ndynamics we conclude polynomial convergence behavior in discrete time. Further\nenhancement of AG-OG with proper restarting allows us to achieve rate-optimal\n(up to a constant) convergence rates with respect to the conditioning of the\ncoupling and individual parts, which results in the first single-call algorithm\nachieving improved convergence in the deterministic setting and rate-optimality\nin the stochastic setting under bilinearly coupled minimax problem sets.",
    "descriptor": "\nComments: 37 pages\n",
    "authors": [
      "Chris Junchi Li",
      "Angela Yuan",
      "Gauthier Gidel",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.17550"
  },
  {
    "id": "arXiv:1907.02919",
    "title": "Hitting Topological Minor Models in Planar Graphs is Fixed Parameter  Tractable",
    "abstract": "Comments: A preliminary version of these results appeared in [Petr A. Golovach, Giannos Stamoulis, Dimitrios M. Thilikos: Hitting Topological Minor Models in Planar Graphs is Fixed Parameter Tractable. SODA 2020: 931-950]",
    "descriptor": "\nComments: A preliminary version of these results appeared in [Petr A. Golovach, Giannos Stamoulis, Dimitrios M. Thilikos: Hitting Topological Minor Models in Planar Graphs is Fixed Parameter Tractable. SODA 2020: 931-950]\n",
    "authors": [
      "Petr A. Golovach",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1907.02919"
  },
  {
    "id": "arXiv:1910.02791",
    "title": "Small Youden Rectangles, Near Youden Rectangles, and Their Connections  to Other Row-Column Designs",
    "abstract": "Small Youden Rectangles, Near Youden Rectangles, and Their Connections  to Other Row-Column Designs",
    "descriptor": "",
    "authors": [
      "Gerold J\u00e4ger",
      "Klas Markstr\u00f6m",
      "Denys Shcherbak",
      "Lars-Daniel \u00d6hman"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1910.02791"
  },
  {
    "id": "arXiv:1911.11747",
    "title": "Proportionality and the Limits of Welfarism",
    "abstract": "Comments: 34 pages, improved clarity of presentation",
    "descriptor": "\nComments: 34 pages, improved clarity of presentation\n",
    "authors": [
      "Dominik Peters",
      "Piotr Skowron"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/1911.11747"
  },
  {
    "id": "arXiv:2005.10696",
    "title": "Novel Policy Seeking with Constrained Optimization",
    "abstract": "Novel Policy Seeking with Constrained Optimization",
    "descriptor": "",
    "authors": [
      "Hao Sun",
      "Zhenghao Peng",
      "Bo Dai",
      "Jian Guo",
      "Dahua Lin",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.10696"
  },
  {
    "id": "arXiv:2006.11684",
    "title": "To Explain or Not to Explain: A Study on the Necessity of Explanations  for Autonomous Vehicles",
    "abstract": "Comments: Won Best Paper Award at NeurIPS 2022 Progress and Challenges in Building Trustworthy Embodied AI Workshop (TEA 2022)",
    "descriptor": "\nComments: Won Best Paper Award at NeurIPS 2022 Progress and Challenges in Building Trustworthy Embodied AI Workshop (TEA 2022)\n",
    "authors": [
      "Yuan Shen",
      "Shanduojiao Jiang",
      "Yanlin Chen",
      "Katie Driggs Campbell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2006.11684"
  },
  {
    "id": "arXiv:2006.13309",
    "title": "Fast Deep Mixtures of Gaussian Process Experts",
    "abstract": "Comments: 19 pages, 19 figures",
    "descriptor": "\nComments: 19 pages, 19 figures\n",
    "authors": [
      "Clement Etienam",
      "Kody Law",
      "Sara Wade",
      "Vitaly Zankin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.13309"
  },
  {
    "id": "arXiv:2009.05175",
    "title": "Denoising Large-Scale Image Captioning from Alt-text Data using Content  Selection Models",
    "abstract": "Denoising Large-Scale Image Captioning from Alt-text Data using Content  Selection Models",
    "descriptor": "",
    "authors": [
      "Khyathi Raghavi Chandu",
      "Piyush Sharma",
      "Soravit Changpinyo",
      "Ashish Thapliyal",
      "Radu Soricut"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.05175"
  },
  {
    "id": "arXiv:2009.07669",
    "title": "Universality Laws for High-Dimensional Learning with Random Features",
    "abstract": "Universality Laws for High-Dimensional Learning with Random Features",
    "descriptor": "",
    "authors": [
      "Hong Hu",
      "Yue M. Lu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.07669"
  },
  {
    "id": "arXiv:2010.11060",
    "title": "A Critical Study on Data Leakage in Recommender System Offline  Evaluation",
    "abstract": "Comments: Accepted by TOIS",
    "descriptor": "\nComments: Accepted by TOIS\n",
    "authors": [
      "Yitong Ji",
      "Aixin Sun",
      "Jie Zhang",
      "Chenliang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2010.11060"
  },
  {
    "id": "arXiv:2011.00130",
    "title": "Approximability results for the $p$-centdian and the converse centdian  problems",
    "abstract": "Approximability results for the $p$-centdian and the converse centdian  problems",
    "descriptor": "",
    "authors": [
      "Yen Hung Chen"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2011.00130"
  },
  {
    "id": "arXiv:2011.04267",
    "title": "A Broad Dataset is All You Need for One-Shot Object Detection",
    "abstract": "A Broad Dataset is All You Need for One-Shot Object Detection",
    "descriptor": "",
    "authors": [
      "Claudio Michaelis",
      "Matthias Bethge",
      "Alexander S. Ecker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.04267"
  },
  {
    "id": "arXiv:2011.14848",
    "title": "A Framework for Output-Feedback Symbolic Control",
    "abstract": "A Framework for Output-Feedback Symbolic Control",
    "descriptor": "",
    "authors": [
      "Mahmoud Khaled",
      "Kuize Zhang",
      "Majid Zamani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.14848"
  },
  {
    "id": "arXiv:2012.06128",
    "title": "SoK: Diving into DAG-based Blockchain Systems",
    "abstract": "Comments: Accepted by ACM Computing Survey by the title \"SoK: DAG-based Blockchain Systems\"",
    "descriptor": "\nComments: Accepted by ACM Computing Survey by the title \"SoK: DAG-based Blockchain Systems\"\n",
    "authors": [
      "Qin Wang",
      "Jiangshan Yu",
      "Shiping Chen",
      "Yang Xiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2012.06128"
  },
  {
    "id": "arXiv:2012.10469",
    "title": "On the Efficient Implementation of the Matrix Exponentiated Gradient  Algorithm for Low-Rank Matrix Optimization",
    "abstract": "Comments: Accepted for publication in Mathematics of Operations Research",
    "descriptor": "\nComments: Accepted for publication in Mathematics of Operations Research\n",
    "authors": [
      "Dan Garber",
      "Atara Kaplan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.10469"
  },
  {
    "id": "arXiv:2012.15691",
    "title": "Quantum error-correcting codes from matrix-product codes related to  quasi-orthogonal and quasi-unitary matrices",
    "abstract": "Quantum error-correcting codes from matrix-product codes related to  quasi-orthogonal and quasi-unitary matrices",
    "descriptor": "",
    "authors": [
      "Meng Cao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2012.15691"
  },
  {
    "id": "arXiv:2101.07701",
    "title": "Computing the exact number of periodic orbits for planar flows",
    "abstract": "Computing the exact number of periodic orbits for planar flows",
    "descriptor": "",
    "authors": [
      "Daniel S. Gra\u00e7a",
      "Ning Zhong"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.07701"
  },
  {
    "id": "arXiv:2101.11282",
    "title": "Deep Learning for Instance Retrieval: A Survey",
    "abstract": "Comments: IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "descriptor": "\nComments: IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
    "authors": [
      "Wei Chen",
      "Yu Liu",
      "Weiping Wang",
      "Erwin Bakker",
      "Theodoros Georgiou",
      "Paul Fieguth",
      "Li Liu",
      "Michael S. Lew"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.11282"
  },
  {
    "id": "arXiv:2102.11469",
    "title": "Analysis of Evolutionary Diversity Optimisation for Permutation Problems",
    "abstract": "Comments: 20 pages, 4 figures, 1 table",
    "descriptor": "\nComments: 20 pages, 4 figures, 1 table\n",
    "authors": [
      "Anh Viet Do",
      "Mingyu Guo",
      "Aneta Neumann",
      "Frank Neumann"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2102.11469"
  },
  {
    "id": "arXiv:2102.11761",
    "title": "SBI: A Simulation-Based Test of Identifiability for Bayesian Causal  Inference",
    "abstract": "Comments: 17 pages, 3 figures",
    "descriptor": "\nComments: 17 pages, 3 figures\n",
    "authors": [
      "Sam Witty",
      "David Jensen",
      "Vikash Mansinghka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2102.11761"
  },
  {
    "id": "arXiv:2103.12390",
    "title": "Saddle-Type Blow-Up Solutions with Computer-Assisted Proofs: Validation  and Extraction of Global Nature",
    "abstract": "Comments: 71 pages, 15 figures. The title is changed and the contents are arranged for readability in v2",
    "descriptor": "\nComments: 71 pages, 15 figures. The title is changed and the contents are arranged for readability in v2\n",
    "authors": [
      "Jean-Philippe Lessard",
      "Kaname Matsue",
      "Akitoshi Takayasu"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Classical Analysis and ODEs (math.CA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.12390"
  },
  {
    "id": "arXiv:2103.16262",
    "title": "Is Image-to-Image Translation the Panacea for Multimodal Image  Registration? A Comparative Study",
    "abstract": "Comments: 37 pages, 10 figures, 2 tables",
    "descriptor": "\nComments: 37 pages, 10 figures, 2 tables\n",
    "authors": [
      "Jiahao Lu",
      "Johan \u00d6fverstedt",
      "Joakim Lindblad",
      "Nata\u0161a Sladoje"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.16262"
  },
  {
    "id": "arXiv:2104.04594",
    "title": "Benchmarking preconditioned boundary integral formulations for acoustics",
    "abstract": "Benchmarking preconditioned boundary integral formulations for acoustics",
    "descriptor": "",
    "authors": [
      "Elwin van 't Wout",
      "Seyyed R. Haqshenas",
      "Pierre G\u00e9lat",
      "Timo Betcke",
      "Nader Saffari"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.04594"
  },
  {
    "id": "arXiv:2104.05225",
    "title": "Edgeless-GNN: Unsupervised Representation Learning for Edgeless Nodes",
    "abstract": "Comments: 13 pages, 5 figures, 6 tables",
    "descriptor": "\nComments: 13 pages, 5 figures, 6 tables\n",
    "authors": [
      "Yong-Min Shin",
      "Cong Tran",
      "Won-Yong Shin",
      "Xin Cao"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2104.05225"
  },
  {
    "id": "arXiv:2104.07454",
    "title": "Memory Capacity of Recurrent Neural Networks with Matrix Representation",
    "abstract": "Memory Capacity of Recurrent Neural Networks with Matrix Representation",
    "descriptor": "",
    "authors": [
      "Animesh Renanse",
      "Alok Sharma",
      "Rohitash Chandra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2104.07454"
  },
  {
    "id": "arXiv:2104.08194",
    "title": "Spatiotemporal Deformable Scene Graphs for Complex Activity Detection",
    "abstract": "Comments: This paper is published at BMVC 2021",
    "descriptor": "\nComments: This paper is published at BMVC 2021\n",
    "authors": [
      "Salman Khan",
      "Fabio Cuzzolin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.08194"
  },
  {
    "id": "arXiv:2104.12577",
    "title": "Optimal controller synthesis for timed systems",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1812.01062 author note: indeed, this is a journal paper that contains and expands on our previous conference papers",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1812.01062 author note: indeed, this is a journal paper that contains and expands on our previous conference papers\n",
    "authors": [
      "Damien Busatto-Gaston",
      "Benjamin Monmege",
      "Pierre-Alain Reynier"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2104.12577"
  },
  {
    "id": "arXiv:2104.13281",
    "title": "Complete Deterministic Dynamics and Spectral Decomposition of the Linear  Ensemble Kalman Inversion",
    "abstract": "Comments: Version as accepted for publication in SIAM/ASA Journal on Uncertainty Quantification",
    "descriptor": "\nComments: Version as accepted for publication in SIAM/ASA Journal on Uncertainty Quantification\n",
    "authors": [
      "Leon Bungert",
      "Philipp Wacker"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2104.13281"
  },
  {
    "id": "arXiv:2105.14708",
    "title": "Blockchain Assisted Federated Learning over Wireless Channels: Dynamic  Resource Allocation and Client Scheduling",
    "abstract": "Comments: Accepted by IEEE TWC",
    "descriptor": "\nComments: Accepted by IEEE TWC\n",
    "authors": [
      "Xiumei Deng",
      "Jun Li",
      "Chuan Ma",
      "Kang Wei",
      "Long Shi",
      "Ming Ding",
      "Wen Chen",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2105.14708"
  },
  {
    "id": "arXiv:2106.00834",
    "title": "Autonomous Low Power IoT System Architecture for Cybersecurity  Monitoring",
    "abstract": "Comments: Cybersecurity, IoT, NSM, packet capture, sensor, green systems, oil and gas, Network Security Monitoring. Accepted in IEEE WF-IoT 2022",
    "descriptor": "\nComments: Cybersecurity, IoT, NSM, packet capture, sensor, green systems, oil and gas, Network Security Monitoring. Accepted in IEEE WF-IoT 2022\n",
    "authors": [
      "Zag ElSayed",
      "Nelly Elsayed",
      "Chengcheng Li",
      "Magdy Bayoumi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.00834"
  },
  {
    "id": "arXiv:2106.05533",
    "title": "Perturbation Theory for Quantum Information",
    "abstract": "Comments: 21 pages",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Michael R Grace",
      "Saikat Guha"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.05533"
  },
  {
    "id": "arXiv:2106.05969",
    "title": "Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation",
    "abstract": "Comments: NeurIPS 2021. Project page: this https URL",
    "descriptor": "\nComments: NeurIPS 2021. Project page: this https URL\n",
    "authors": [
      "Zhengyi Luo",
      "Ryo Hachiuma",
      "Ye Yuan",
      "Kris Kitani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.05969"
  },
  {
    "id": "arXiv:2106.07075",
    "title": "Revisiting consistency for semi-supervised semantic segmentation",
    "abstract": "Revisiting consistency for semi-supervised semantic segmentation",
    "descriptor": "",
    "authors": [
      "Ivan Grubi\u0161i\u0107",
      "Marin Or\u0161i\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07075"
  },
  {
    "id": "arXiv:2107.01693",
    "title": "A precise bare simulation approach to the minimization of some  distances. Foundations",
    "abstract": "Comments: v2: typos-corrected version of v1; 93 pages",
    "descriptor": "\nComments: v2: typos-corrected version of v1; 93 pages\n",
    "authors": [
      "Michel Broniatowski",
      "Wolfgang Stummer"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.01693"
  },
  {
    "id": "arXiv:2107.12003",
    "title": "Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal  Latent Representations",
    "abstract": "Comments: 5 pages (including references), 1 figure",
    "descriptor": "\nComments: 5 pages (including references), 1 figure\n",
    "authors": [
      "Se-Yun Um",
      "Jihyun Kim",
      "Jihyun Lee",
      "Hong-Goo Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2107.12003"
  },
  {
    "id": "arXiv:2108.03517",
    "title": "Upfront Commitment in Online Resource Allocation with Patient Customers",
    "abstract": "Upfront Commitment in Online Resource Allocation with Patient Customers",
    "descriptor": "",
    "authors": [
      "Negin Golrezaei",
      "Evan Yao"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.03517"
  },
  {
    "id": "arXiv:2108.04520",
    "title": "Fast and Fair Randomized Wait-Free Locks",
    "abstract": "Fast and Fair Randomized Wait-Free Locks",
    "descriptor": "",
    "authors": [
      "Naama Ben-David",
      "Guy E. Blelloch"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.04520"
  },
  {
    "id": "arXiv:2108.10566",
    "title": "sigmoidF1: A Smooth F1 Score Surrogate Loss for Multilabel  Classification",
    "abstract": "Comments: Published at TMLR",
    "descriptor": "\nComments: Published at TMLR\n",
    "authors": [
      "Gabriel B\u00e9n\u00e9dict",
      "Vincent Koops",
      "Daan Odijk",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.10566"
  },
  {
    "id": "arXiv:2108.13732",
    "title": "Deep Learning on Edge TPUs",
    "abstract": "Comments: 10 pages, 4 figures, 3 tables",
    "descriptor": "\nComments: 10 pages, 4 figures, 3 tables\n",
    "authors": [
      "Yipeng Sun",
      "Andreas M Kist"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.13732"
  },
  {
    "id": "arXiv:2109.00210",
    "title": "EventPoint: Self-Supervised Interest Point Detection and Description for  Event-based Camera",
    "abstract": "Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n",
    "authors": [
      "Ze Huang",
      "Li Sun",
      "Cheng Zhao",
      "Song Li",
      "Songzhi Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.00210"
  },
  {
    "id": "arXiv:2109.03010",
    "title": "Exploring the Accuracy Potential of IMU Preintegration in Factor Graph  Optimization",
    "abstract": "Comments: 8 pages, 3 figures",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Hailiang Tang",
      "Xiaoji Niu",
      "Tisheng Zhang",
      "Jing Fan",
      "Jingnan Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.03010"
  },
  {
    "id": "arXiv:2109.04569",
    "title": "Highly Compressive Visual Self-localization Using Sequential Semantic  Scene Graph and Graph Convolutional Neural Network",
    "abstract": "Comments: 6 pages, 5 figures, Draft version of a paper presented at the 13th IROS Workshop on Planning, Perception, Navigation for Intelligent Vehicle (PPNIV2022)",
    "descriptor": "\nComments: 6 pages, 5 figures, Draft version of a paper presented at the 13th IROS Workshop on Planning, Perception, Navigation for Intelligent Vehicle (PPNIV2022)\n",
    "authors": [
      "Mitsuki Yoshida",
      "Ryogo Yamamoto",
      "Kanji Tanaka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.04569"
  },
  {
    "id": "arXiv:2109.05455",
    "title": "Competitive Driving of Autonomous Vehicles",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Gabriel Hartmann",
      "Zvi Shiller",
      "Amos Azaria"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.05455"
  },
  {
    "id": "arXiv:2109.07983",
    "title": "Let the CAT out of the bag: Contrastive Attributed explanations for Text",
    "abstract": "Let the CAT out of the bag: Contrastive Attributed explanations for Text",
    "descriptor": "",
    "authors": [
      "Saneem Chemmengath",
      "Amar Prakash Azad",
      "Ronny Luss",
      "Amit Dhurandhar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.07983"
  },
  {
    "id": "arXiv:2109.10380",
    "title": "Deep Policies for Online Bipartite Matching: A Reinforcement Learning  Approach",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Mohammad Ali Alomrani",
      "Reza Moravej",
      "Elias B. Khalil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.10380"
  },
  {
    "id": "arXiv:2109.14493",
    "title": "Automatic Discovery and Description of Human Planning Strategies",
    "abstract": "Comments: Submitted to the Behavior Research Methods journal. Code available at this https URL",
    "descriptor": "\nComments: Submitted to the Behavior Research Methods journal. Code available at this https URL\n",
    "authors": [
      "Julian Skirzynski",
      "Yash Raj Jain",
      "Falk Lieder"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2109.14493"
  },
  {
    "id": "arXiv:2109.15254",
    "title": "SlovakBERT: Slovak Masked Language Model",
    "abstract": "Comments: 12 pages, 2 figures",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "Mat\u00fa\u0161 Pikuliak",
      "\u0160tefan Grivalsk\u00fd",
      "Martin Kon\u00f4pka",
      "Miroslav Bl\u0161t\u00e1k",
      "Martin Tamajka",
      "Viktor Bachrat\u00fd",
      "Mari\u00e1n \u0160imko",
      "Pavol Bal\u00e1\u017eik",
      "Michal Trnka",
      "Filip Uhl\u00e1rik"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.15254"
  },
  {
    "id": "arXiv:2110.01661",
    "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and  Enhancement Prediction",
    "abstract": "Comments: Journal of Data Mining and Digital Humanities; Minor revision",
    "descriptor": "\nComments: Journal of Data Mining and Digital Humanities; Minor revision\n",
    "authors": [
      "Pit Schneider",
      "Yves Maurer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01661"
  },
  {
    "id": "arXiv:2110.03611",
    "title": "Adversarial Retriever-Ranker for dense text retrieval",
    "abstract": "Comments: ICLR 2022",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Hang Zhang",
      "Yeyun Gong",
      "Yelong Shen",
      "Jiancheng Lv",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03611"
  },
  {
    "id": "arXiv:2110.05917",
    "title": "On complexity of structure and substructure connectivity, component  connectivity and restricted connectivity of graphs",
    "abstract": "On complexity of structure and substructure connectivity, component  connectivity and restricted connectivity of graphs",
    "descriptor": "",
    "authors": [
      "Huazhong L\u00fc",
      "Tingzeng Wu"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.05917"
  },
  {
    "id": "arXiv:2110.12179",
    "title": "MisMatch: Calibrated Segmentation via Consistency on Differential  Morphological Feature Perturbations with Limited Labels",
    "abstract": "MisMatch: Calibrated Segmentation via Consistency on Differential  Morphological Feature Perturbations with Limited Labels",
    "descriptor": "",
    "authors": [
      "Mou-Cheng Xu",
      "Yukun Zhou",
      "Chen Jin",
      "Marius De Groot",
      "Neil P. Oxtoby",
      "Daniel C. Alexander",
      "Joseph Jacob"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.12179"
  },
  {
    "id": "arXiv:2110.14518",
    "title": "NIDA-CLIFGAN: Natural Infrastructure Damage Assessment through Efficient  Classification Combining Contrastive Learning, Information Fusion and  Generative Adversarial Networks",
    "abstract": "NIDA-CLIFGAN: Natural Infrastructure Damage Assessment through Efficient  Classification Combining Contrastive Learning, Information Fusion and  Generative Adversarial Networks",
    "descriptor": "",
    "authors": [
      "Jie Wei",
      "Zhigang Zhu",
      "Erik Blasch",
      "Bilal Abdulrahman",
      "Billy Davila",
      "Shuoxin Liu",
      "Jed Magracia",
      "Ling Fang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14518"
  },
  {
    "id": "arXiv:2111.01457",
    "title": "Synthesizing Speech from Intracranial Depth Electrodes using an  Encoder-Decoder Framework",
    "abstract": "Synthesizing Speech from Intracranial Depth Electrodes using an  Encoder-Decoder Framework",
    "descriptor": "",
    "authors": [
      "Jonas Kohler",
      "Maarten C. Ottenhoff",
      "Sophocles Goulis",
      "Miguel Angrick",
      "Albert J. Colon",
      "Louis Wagner",
      "Simon Tousseyn",
      "Pieter L. Kubben",
      "Christian Herff"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.01457"
  },
  {
    "id": "arXiv:2111.02295",
    "title": "The Parameterized Complexity of the Survivable Network Design Problem",
    "abstract": "The Parameterized Complexity of the Survivable Network Design Problem",
    "descriptor": "",
    "authors": [
      "Andreas Emil Feldmann",
      "Anish Mukherjee",
      "Erik Jan van Leeuwen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.02295"
  },
  {
    "id": "arXiv:2111.07243",
    "title": "Simulating Diffusion Bridges with Score Matching",
    "abstract": "Comments: Revised with new numerical examples",
    "descriptor": "\nComments: Revised with new numerical examples\n",
    "authors": [
      "Jeremy Heng",
      "Valentin De Bortoli",
      "Arnaud Doucet",
      "James Thornton"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.07243"
  },
  {
    "id": "arXiv:2111.09254",
    "title": "Universal Inference Meets Random Projections: A Scalable Test for  Log-concavity",
    "abstract": "Universal Inference Meets Random Projections: A Scalable Test for  Log-concavity",
    "descriptor": "",
    "authors": [
      "Robin Dunn",
      "Aditya Gangrade",
      "Larry Wasserman",
      "Aaditya Ramdas"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2111.09254"
  },
  {
    "id": "arXiv:2111.10822",
    "title": "New Clocks, Optimal Line Formation and Self-Replication Population  Protocols",
    "abstract": "New Clocks, Optimal Line Formation and Self-Replication Population  Protocols",
    "descriptor": "",
    "authors": [
      "Leszek Gasieniec",
      "Paul Spirakis",
      "Grzegorz Stachowiak"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.10822"
  },
  {
    "id": "arXiv:2111.12255",
    "title": "A Family of Independent Variable Eddington Factor Methods with Efficient  Preconditioned Iterative Solvers",
    "abstract": "A Family of Independent Variable Eddington Factor Methods with Efficient  Preconditioned Iterative Solvers",
    "descriptor": "",
    "authors": [
      "Samuel Olivier",
      "Will Pazner",
      "Terry S. Haut",
      "Ben C. Yee"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2111.12255"
  },
  {
    "id": "arXiv:2111.15041",
    "title": "Online Learning for Predictive Control with Provable Regret Guarantees",
    "abstract": "Online Learning for Predictive Control with Provable Regret Guarantees",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Jianjun Yuan",
      "Dileep Kalathil",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.15041"
  },
  {
    "id": "arXiv:2111.15063",
    "title": "Online Robust Control of Linear Dynamical Systems with Limited  Prediction",
    "abstract": "Online Robust Control of Linear Dynamical Systems with Limited  Prediction",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Dileep Kalathil",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.15063"
  },
  {
    "id": "arXiv:2112.01771",
    "title": "Understanding Performance Problems in Deep Learning Systems",
    "abstract": "Comments: Has been accepted by ESEC/FSE 2022",
    "descriptor": "\nComments: Has been accepted by ESEC/FSE 2022\n",
    "authors": [
      "Junming Cao",
      "Bihuan Chen",
      "Chao Sun",
      "Longjie Hu",
      "Shuaihong Wu",
      "Xin Peng"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.01771"
  },
  {
    "id": "arXiv:2112.03244",
    "title": "Projection methods for Neural Field equations",
    "abstract": "Projection methods for Neural Field equations",
    "descriptor": "",
    "authors": [
      "Daniele Avitabile"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.03244"
  },
  {
    "id": "arXiv:2112.04017",
    "title": "fastball: A fast algorithm to sample bipartite graphs with fixed degree  sequences",
    "abstract": "Comments: Journal of Complex Networks (2022)",
    "descriptor": "\nComments: Journal of Complex Networks (2022)\n",
    "authors": [
      "Karl Godard",
      "Zachary P. Neal"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.04017"
  },
  {
    "id": "arXiv:2112.05871",
    "title": "A Comparative Study of Adversarial Attacks against Point Cloud Semantic  Segmentation",
    "abstract": "A Comparative Study of Adversarial Attacks against Point Cloud Semantic  Segmentation",
    "descriptor": "",
    "authors": [
      "Jiacen Xu",
      "Zhe Zhou",
      "Boyuan Feng",
      "Yufei Ding",
      "Zhou Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.05871"
  },
  {
    "id": "arXiv:2112.06316",
    "title": "IFGF-accelerated integral equation solvers for acoustic scattering",
    "abstract": "IFGF-accelerated integral equation solvers for acoustic scattering",
    "descriptor": "",
    "authors": [
      "Edwin Jimenez",
      "Christoph Bauinger",
      "Oscar P. Bruno"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2112.06316"
  },
  {
    "id": "arXiv:2112.06793",
    "title": "Modeling the debonding process of osseointegrated implants due to  coupled adhesion and friction",
    "abstract": "Comments: Biomech Model Mechanobiol (2022)",
    "descriptor": "\nComments: Biomech Model Mechanobiol (2022)\n",
    "authors": [
      "Katharina Immel",
      "Vu-Hieu Nguyen",
      "Guillaume Haiat",
      "Roger A. Sauer"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2112.06793"
  },
  {
    "id": "arXiv:2112.08759",
    "title": "KnAC: an approach for enhancing cluster analysis with background  knowledge and explanations",
    "abstract": "Comments: Accepted to Applied Intelligence",
    "descriptor": "\nComments: Accepted to Applied Intelligence\n",
    "authors": [
      "Szymon Bobek",
      "Micha\u0142 Kuk",
      "Jakub Brzegowski",
      "Edyta Brzychczy",
      "Grzegorz J. Nalepa"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.08759"
  },
  {
    "id": "arXiv:2112.14252",
    "title": "Inferring Symbolic Automata",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2011.05389",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2011.05389\n",
    "authors": [
      "Dana Fisman",
      "Hadar Frenkel",
      "Sandra Zilles"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.14252"
  },
  {
    "id": "arXiv:2112.15400",
    "title": "A Theoretical Understanding of Gradient Bias in Meta-Reinforcement  Learning",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Bo Liu",
      "Xidong Feng",
      "Jie Ren",
      "Luo Mai",
      "Rui Zhu",
      "Haifeng Zhang",
      "Jun Wang",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.15400"
  },
  {
    "id": "arXiv:2201.00354",
    "title": "Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference",
    "abstract": "Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference",
    "descriptor": "",
    "authors": [
      "Hao Sun",
      "Taiyi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.00354"
  },
  {
    "id": "arXiv:2201.01863",
    "title": "CFU Playground: Full-Stack Open-Source Framework for Tiny Machine  Learning (tinyML) Acceleration on FPGAs",
    "abstract": "CFU Playground: Full-Stack Open-Source Framework for Tiny Machine  Learning (tinyML) Acceleration on FPGAs",
    "descriptor": "",
    "authors": [
      "Shvetank Prakash",
      "Tim Callahan",
      "Joseph Bushagour",
      "Colby Banbury",
      "Alan V. Green",
      "Pete Warden",
      "Tim Ansell",
      "Vijay Janapa Reddi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.01863"
  },
  {
    "id": "arXiv:2201.05706",
    "title": "Perspective Transformation Layer",
    "abstract": "Comments: This paper has been accepted for publication by the 2022 International Conference on Computational Science & Computational Intelligence (CSCI'22), Research Track on Signal & Image Processing, Computer Vision & Pattern Recognition",
    "descriptor": "\nComments: This paper has been accepted for publication by the 2022 International Conference on Computational Science & Computational Intelligence (CSCI'22), Research Track on Signal & Image Processing, Computer Vision & Pattern Recognition\n",
    "authors": [
      "Nishan Khatri",
      "Agnibh Dasgupta",
      "Yucong Shen",
      "Xin Zhong",
      "Frank Y. Shih"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.05706"
  },
  {
    "id": "arXiv:2201.06206",
    "title": "SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph  Reasoning",
    "abstract": "Comments: EMNLP 2022. Code is available at this https URL",
    "descriptor": "\nComments: EMNLP 2022. Code is available at this https URL\n",
    "authors": [
      "Yushi Bai",
      "Xin Lv",
      "Juanzi Li",
      "Lei Hou",
      "Yincen Qu",
      "Zelin Dai",
      "Feiyu Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.06206"
  },
  {
    "id": "arXiv:2201.06910",
    "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves  Zero-Shot Generalization",
    "abstract": "Comments: 18 pages",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Hanwei Xu",
      "Yujun Chen",
      "Yulun Du",
      "Nan Shao",
      "Yanggang Wang",
      "Haiyu Li",
      "Zhilin Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.06910"
  },
  {
    "id": "arXiv:2201.07436",
    "title": "Global-Local Path Networks for Monocular Depth Estimation with Vertical  CutDepth",
    "abstract": "Comments: 11pages, 5 figures",
    "descriptor": "\nComments: 11pages, 5 figures\n",
    "authors": [
      "Doyeon Kim",
      "Woonghyun Ka",
      "Pyungwhan Ahn",
      "Donggyu Joo",
      "Sehwan Chun",
      "Junmo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.07436"
  },
  {
    "id": "arXiv:2201.09023",
    "title": "Content-aware Warping for View Synthesis",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2108.07408",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2108.07408\n",
    "authors": [
      "Mantang Guo",
      "Junhui Hou",
      "Jing Jin",
      "Hui Liu",
      "Huanqiang Zeng",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.09023"
  },
  {
    "id": "arXiv:2201.10459",
    "title": "FRAMED: An AutoML Approach for Structural Performance Prediction of  Bicycle Frames",
    "abstract": "FRAMED: An AutoML Approach for Structural Performance Prediction of  Bicycle Frames",
    "descriptor": "",
    "authors": [
      "Lyle Regenwetter",
      "Colin Weaver",
      "Faez Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.10459"
  },
  {
    "id": "arXiv:2202.00872",
    "title": "On the Global Convergence Rates of Decentralized Softmax Gradient Play  in Markov Potential Games",
    "abstract": "On the Global Convergence Rates of Decentralized Softmax Gradient Play  in Markov Potential Games",
    "descriptor": "",
    "authors": [
      "Runyu Zhang",
      "Jincheng Mei",
      "Bo Dai",
      "Dale Schuurmans",
      "Na Li"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2202.00872"
  },
  {
    "id": "arXiv:2202.01771",
    "title": "Pre-Trained Language Models for Interactive Decision-Making",
    "abstract": "Pre-Trained Language Models for Interactive Decision-Making",
    "descriptor": "",
    "authors": [
      "Shuang Li",
      "Xavier Puig",
      "Chris Paxton",
      "Yilun Du",
      "Clinton Wang",
      "Linxi Fan",
      "Tao Chen",
      "De-An Huang",
      "Ekin Aky\u00fcrek",
      "Anima Anandkumar",
      "Jacob Andreas",
      "Igor Mordatch",
      "Antonio Torralba",
      "Yuke Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.01771"
  },
  {
    "id": "arXiv:2202.02904",
    "title": "A Compressed Sensing Based Least Squares Approach to Semi-supervised  Local Cluster Extraction",
    "abstract": "A Compressed Sensing Based Least Squares Approach to Semi-supervised  Local Cluster Extraction",
    "descriptor": "",
    "authors": [
      "Ming-Jun Lai",
      "Zhaiming Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.02904"
  },
  {
    "id": "arXiv:2202.03233",
    "title": "A Variational Edge Partition Model for Supervised Graph Representation  Learning",
    "abstract": "Comments: 10 pages, 5 figures, 14 pages of appendix, accepted to NeurIPS 2022",
    "descriptor": "\nComments: 10 pages, 5 figures, 14 pages of appendix, accepted to NeurIPS 2022\n",
    "authors": [
      "Yilin He",
      "Chaojie Wang",
      "Hao Zhang",
      "Bo Chen",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2202.03233"
  },
  {
    "id": "arXiv:2202.04108",
    "title": "A Lagrangian Duality Approach to Active Learning",
    "abstract": "A Lagrangian Duality Approach to Active Learning",
    "descriptor": "",
    "authors": [
      "Juan Elenter",
      "Navid NaderiAlizadeh",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.04108"
  },
  {
    "id": "arXiv:2202.06817",
    "title": "CATs++: Boosting Cost Aggregation with Convolutions and Transformers",
    "abstract": "Comments: Accepted to TPAMI. Project page:this https URL arXiv admin note: text overlap with arXiv:2106.02520",
    "descriptor": "\nComments: Accepted to TPAMI. Project page:this https URL arXiv admin note: text overlap with arXiv:2106.02520\n",
    "authors": [
      "Seokju Cho",
      "Sunghwan Hong",
      "Seungryong Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.06817"
  },
  {
    "id": "arXiv:2202.08469",
    "title": "An analysis of citing and referencing habits across all scholarly  disciplines: approaches and trends in bibliographic metadata errors",
    "abstract": "An analysis of citing and referencing habits across all scholarly  disciplines: approaches and trends in bibliographic metadata errors",
    "descriptor": "",
    "authors": [
      "Erika Alves dos Santos",
      "Silvio Peroni",
      "Marcos Luiz Mucheroni"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2202.08469"
  },
  {
    "id": "arXiv:2202.08832",
    "title": "Universality of empirical risk minimization",
    "abstract": "Comments: 74 pages",
    "descriptor": "\nComments: 74 pages\n",
    "authors": [
      "Andrea Montanari",
      "Basil Saeed"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.08832"
  },
  {
    "id": "arXiv:2202.09778",
    "title": "Pseudo Numerical Methods for Diffusion Models on Manifolds",
    "abstract": "Comments: ICLR 2022",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Luping Liu",
      "Yi Ren",
      "Zhijie Lin",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.09778"
  },
  {
    "id": "arXiv:2202.10550",
    "title": "Imbalanced Classification via Explicit Gradient Learning From Augmented  Data",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1906.05591",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1906.05591\n",
    "authors": [
      "Bronislav Yasinnik",
      "Moshe Salhov",
      "Ofir Lindenbaum",
      "Amir Averbuch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10550"
  },
  {
    "id": "arXiv:2202.10793",
    "title": "PyTorch Geometric Signed Directed: A Software Package on Graph Neural  Networks for Signed and Directed Graphs",
    "abstract": "PyTorch Geometric Signed Directed: A Software Package on Graph Neural  Networks for Signed and Directed Graphs",
    "descriptor": "",
    "authors": [
      "Yixuan He",
      "Xitong Zhang",
      "Junjie Huang",
      "Benedek Rozemberczki",
      "Mihai Cucuringu",
      "Gesine Reinert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.10793"
  },
  {
    "id": "arXiv:2202.11389",
    "title": "Fast Sparse Classification for Generalized Linear and Additive Models",
    "abstract": "Comments: AISTATS 2022",
    "descriptor": "\nComments: AISTATS 2022\n",
    "authors": [
      "Jiachang Liu",
      "Chudi Zhong",
      "Margo Seltzer",
      "Cynthia Rudin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.11389"
  },
  {
    "id": "arXiv:2202.11508",
    "title": "AI-enabled mm-Waveform Configuration for Autonomous Vehicles with  Integrated Communication and Sensing",
    "abstract": "Comments: Typos, channel model updates",
    "descriptor": "\nComments: Typos, channel model updates\n",
    "authors": [
      "Nam H. Chu",
      "Diep N. Nguyen",
      "Dinh Thai Hoang",
      "Quoc-Viet Pham",
      "Khoa T. Phan",
      "Won-Joo Hwang",
      "Eryk Dutkiewicz"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.11508"
  },
  {
    "id": "arXiv:2202.13084",
    "title": "Visual Speech Recognition for Multiple Languages in the Wild",
    "abstract": "Comments: Published in Nature Machine Intelligence",
    "descriptor": "\nComments: Published in Nature Machine Intelligence\n",
    "authors": [
      "Pingchuan Ma",
      "Stavros Petridis",
      "Maja Pantic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2202.13084"
  },
  {
    "id": "arXiv:2202.13110",
    "title": "Optimal-er Auctions through Attention",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Dmitry Ivanov",
      "Iskander Safiulin",
      "Igor Filippov",
      "Ksenia Balabaeva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2202.13110"
  },
  {
    "id": "arXiv:2202.13328",
    "title": "Thinking Outside the Ball: Optimal Learning with Gradient Descent for  Generalized Linear Stochastic Convex Optimization",
    "abstract": "Thinking Outside the Ball: Optimal Learning with Gradient Descent for  Generalized Linear Stochastic Convex Optimization",
    "descriptor": "",
    "authors": [
      "Idan Amir",
      "Roi Livni",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.13328"
  },
  {
    "id": "arXiv:2202.13363",
    "title": "Variational Autoencoder with Disentanglement Priors for Low-Resource  Task-Specific Natural Language Generation",
    "abstract": "Comments: 22 pages, EMNLP 2022",
    "descriptor": "\nComments: 22 pages, EMNLP 2022\n",
    "authors": [
      "Zhuang Li",
      "Lizhen Qu",
      "Qiongkai Xu",
      "Tongtong Wu",
      "Tianyang Zhan",
      "Gholamreza Haffari"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.13363"
  },
  {
    "id": "arXiv:2202.14014",
    "title": "Distributed-MPC with Data-Driven Estimation of Bus Admittance Matrix in  Voltage Control",
    "abstract": "Distributed-MPC with Data-Driven Estimation of Bus Admittance Matrix in  Voltage Control",
    "descriptor": "",
    "authors": [
      "Ramij R. Hossain",
      "Ratnesh Kumar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.14014"
  },
  {
    "id": "arXiv:2203.00592",
    "title": "Tiny Autoscalers for Tiny Workloads: Dynamic CPU Allocation for  Serverless Functions",
    "abstract": "Comments: Published in 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid), 2022",
    "descriptor": "\nComments: Published in 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid), 2022\n",
    "authors": [
      "Yuxuan Zhao",
      "Alexandru Uta"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2203.00592"
  },
  {
    "id": "arXiv:2203.03456",
    "title": "Negative-Weight Single-Source Shortest Paths in Near-linear Time",
    "abstract": "Comments: Simplified algorithm for Low-Diameter Decomposition and minor corrections",
    "descriptor": "\nComments: Simplified algorithm for Low-Diameter Decomposition and minor corrections\n",
    "authors": [
      "Aaron Bernstein",
      "Danupon Nanongkai",
      "Christian Wulff-Nilsen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.03456"
  },
  {
    "id": "arXiv:2203.04313",
    "title": "Multi-Scale Adaptive Network for Single Image Denoising",
    "abstract": "Multi-Scale Adaptive Network for Single Image Denoising",
    "descriptor": "",
    "authors": [
      "Yuanbiao Gou",
      "Peng Hu",
      "Jiancheng Lv",
      "Joey Tianyi Zhou",
      "Xi Peng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04313"
  },
  {
    "id": "arXiv:2203.04946",
    "title": "Do better ImageNet classifiers assess perceptual similarity better?",
    "abstract": "Comments: TMLR 2022 (this https URL)",
    "descriptor": "\nComments: TMLR 2022 (this https URL)\n",
    "authors": [
      "Manoj Kumar",
      "Neil Houlsby",
      "Nal Kalchbrenner",
      "Ekin D. Cubuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04946"
  },
  {
    "id": "arXiv:2203.08060",
    "title": "Seeking Commonness and Inconsistencies: A Jointly Smoothed Approach to  Multi-view Subspace Clustering",
    "abstract": "Comments: To appear in Information Fusion",
    "descriptor": "\nComments: To appear in Information Fusion\n",
    "authors": [
      "Xiaosha Cai",
      "Dong Huang",
      "Guang-Yu Zhang",
      "Chang-Dong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.08060"
  },
  {
    "id": "arXiv:2203.08098",
    "title": "RB2: Robotic Manipulation Benchmarking with a Twist",
    "abstract": "Comments: accepted at the NeurIPS 2021 Datasets and Benchmarks Track",
    "descriptor": "\nComments: accepted at the NeurIPS 2021 Datasets and Benchmarks Track\n",
    "authors": [
      "Sudeep Dasari",
      "Jianren Wang",
      "Joyce Hong",
      "Shikhar Bahl",
      "Yixin Lin",
      "Austin Wang",
      "Abitha Thankaraj",
      "Karanbir Chahal",
      "Berk Calli",
      "Saurabh Gupta",
      "David Held",
      "Lerrel Pinto",
      "Deepak Pathak",
      "Vikash Kumar",
      "Abhinav Gupta"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.08098"
  },
  {
    "id": "arXiv:2203.08504",
    "title": "A Survey of Historical Document Image Datasets",
    "abstract": "Comments: 42 pages, 2 figures",
    "descriptor": "\nComments: 42 pages, 2 figures\n",
    "authors": [
      "Konstantina Nikolaidou",
      "Mathias Seuret",
      "Hamam Mokayed",
      "Marcus Liwicki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.08504"
  },
  {
    "id": "arXiv:2203.08615",
    "title": "Scientific and Technological Information Oriented Semantics-adversarial  and Media-adversarial Cross-media Retrieval",
    "abstract": "Comments: There are some problems in our algorithm and we want to withdraw this paper",
    "descriptor": "\nComments: There are some problems in our algorithm and we want to withdraw this paper\n",
    "authors": [
      "Ang Li",
      "Junping Du",
      "Feifei Kou",
      "Zhe Xue",
      "Xin Xu",
      "Mingying Xu",
      "Yang Jiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.08615"
  },
  {
    "id": "arXiv:2203.09553",
    "title": "Efficient Federated Learning on Knowledge Graphs via Privacy-preserving  Relation Embedding Aggregation",
    "abstract": "Comments: Accepted to Findings of EMNLP 2022",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Kai Zhang",
      "Yu Wang",
      "Hongyi Wang",
      "Lifu Huang",
      "Carl Yang",
      "Xun Chen",
      "Lichao Sun"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.09553"
  },
  {
    "id": "arXiv:2203.10197",
    "title": "Cost Function Learning in Memorized Social Networks with Cognitive  Behavioral Asymmetry",
    "abstract": "Comments: 15 pages",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Yanbing Mao",
      "Jining Li",
      "Naira Hovakimyan",
      "Tarek Abdelzaher",
      "Christian Lebiere"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2203.10197"
  },
  {
    "id": "arXiv:2203.10731",
    "title": "Research Scholar Interest Mining Method based on Load Centrality",
    "abstract": "Comments: There are some probiems in our algorithm and we want to withdraw this paper",
    "descriptor": "\nComments: There are some probiems in our algorithm and we want to withdraw this paper\n",
    "authors": [
      "Yang Jiang",
      "Zhe Xue",
      "Ang Li"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.10731"
  },
  {
    "id": "arXiv:2203.10788",
    "title": "Computing the least action ground state of the nonlinear Schr\u00f6dinger  equation by a normalized gradient flow",
    "abstract": "Comments: 30 pages, 14 figures",
    "descriptor": "\nComments: 30 pages, 14 figures\n",
    "authors": [
      "Chushan Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2203.10788"
  },
  {
    "id": "arXiv:2203.11537",
    "title": "Convolutional Neural Network-based Efficient Dense Point Cloud  Generation using Unsigned Distance Fields",
    "abstract": "Convolutional Neural Network-based Efficient Dense Point Cloud  Generation using Unsigned Distance Fields",
    "descriptor": "",
    "authors": [
      "Abol Basher",
      "Jani Boutellier"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.11537"
  },
  {
    "id": "arXiv:2203.12610",
    "title": "AI Poincar\u00e9 2.0: Machine Learning Conservation Laws from  Differential Equations",
    "abstract": "Comments: 15 pages, 12 figures",
    "descriptor": "\nComments: 15 pages, 12 figures\n",
    "authors": [
      "Ziming Liu",
      "Varun Madhavan",
      "Max Tegmark"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Exactly Solvable and Integrable Systems (nlin.SI)",
      "Classical Physics (physics.class-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2203.12610"
  },
  {
    "id": "arXiv:2203.13537",
    "title": "Efficient Visual Tracking via Hierarchical Cross-Attention Transformer",
    "abstract": "Efficient Visual Tracking via Hierarchical Cross-Attention Transformer",
    "descriptor": "",
    "authors": [
      "Xin Chen",
      "Ben Kang",
      "Dong Wang",
      "Dongdong Li",
      "Huchuan Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.13537"
  },
  {
    "id": "arXiv:2203.15065",
    "title": "DeepShadow: Neural Shape from Shadow",
    "abstract": "Comments: ECCV 2022. Project page available at this https URL",
    "descriptor": "\nComments: ECCV 2022. Project page available at this https URL\n",
    "authors": [
      "Asaf Karnieli",
      "Ohad Fried",
      "Yacov Hel-Or"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.15065"
  },
  {
    "id": "arXiv:2203.15840",
    "title": "Autoregressive Co-Training for Learning Discrete Speech Representations",
    "abstract": "Autoregressive Co-Training for Learning Discrete Speech Representations",
    "descriptor": "",
    "authors": [
      "Sung-Lin Yeh",
      "Hao Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.15840"
  },
  {
    "id": "arXiv:2203.15927",
    "title": "The DESC Stellarator Code Suite Part II: Perturbation and continuation  methods",
    "abstract": "The DESC Stellarator Code Suite Part II: Perturbation and continuation  methods",
    "descriptor": "",
    "authors": [
      "Rory Conlin",
      "Daniel W. Dudt",
      "Dario Panici",
      "Egemen Kolemen"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.15927"
  },
  {
    "id": "arXiv:2203.17118",
    "title": "Doubly-Robust Estimation for Correcting Position-Bias in Click Feedback  for Unbiased Learning to Rank",
    "abstract": "Doubly-Robust Estimation for Correcting Position-Bias in Click Feedback  for Unbiased Learning to Rank",
    "descriptor": "",
    "authors": [
      "Harrie Oosterhuis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2203.17118"
  },
  {
    "id": "arXiv:2204.00170",
    "title": "Universal Adaptor: Converting Mel-Spectrograms Between Different  Configurations for Speech Synthesis",
    "abstract": "Universal Adaptor: Converting Mel-Spectrograms Between Different  Configurations for Speech Synthesis",
    "descriptor": "",
    "authors": [
      "Fan-Lin Wang",
      "Po-chun Hsu",
      "Da-rong Liu",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.00170"
  },
  {
    "id": "arXiv:2204.02787",
    "title": "DiffSearch: A Scalable and Precise Search Engine for Code Changes",
    "abstract": "DiffSearch: A Scalable and Precise Search Engine for Code Changes",
    "descriptor": "",
    "authors": [
      "Luca Di Grazia",
      "Paul Bredl",
      "Michael Pradel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2204.02787"
  },
  {
    "id": "arXiv:2204.04416",
    "title": "E^2TAD: An Energy-Efficient Tracking-based Action Detector",
    "abstract": "E^2TAD: An Energy-Efficient Tracking-based Action Detector",
    "descriptor": "",
    "authors": [
      "Xin Hu",
      "Zhenyu Wu",
      "Hao-Yu Miao",
      "Siqi Fan",
      "Taiyu Long",
      "Zhenyu Hu",
      "Pengcheng Pi",
      "Yi Wu",
      "Zhou Ren",
      "Zhangyang Wang",
      "Gang Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.04416"
  },
  {
    "id": "arXiv:2204.04900",
    "title": "Confusing Image Quality Assessment: Towards Better Augmented Reality  Experience",
    "abstract": "Confusing Image Quality Assessment: Towards Better Augmented Reality  Experience",
    "descriptor": "",
    "authors": [
      "Huiyu Duan",
      "Xiongkuo Min",
      "Yucheng Zhu",
      "Guangtao Zhai",
      "Xiaokang Yang",
      "Patrick Le Callet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2204.04900"
  },
  {
    "id": "arXiv:2204.07234",
    "title": "PARC: Physics-Aware Recurrent Convolutional Neural Networks to  Assimilate Meso-scale Reactive Mechanics of Energetic Materials",
    "abstract": "PARC: Physics-Aware Recurrent Convolutional Neural Networks to  Assimilate Meso-scale Reactive Mechanics of Energetic Materials",
    "descriptor": "",
    "authors": [
      "Phong C.H. Nguyen",
      "Yen-Thi Nguyen",
      "Joseph B. Choi",
      "Pradeep K. Seshadri",
      "H.S. Udaykumar",
      "Stephen Baek"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.07234"
  },
  {
    "id": "arXiv:2204.08189",
    "title": "Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile  Edge",
    "abstract": "Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile  Edge",
    "descriptor": "",
    "authors": [
      "Qun Song",
      "Zhenyu Yan",
      "Wenjie Luo",
      "Rui Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.08189"
  },
  {
    "id": "arXiv:2204.08646",
    "title": "Label Efficient Regularization and Propagation for Graph Node  Classification",
    "abstract": "Label Efficient Regularization and Propagation for Graph Node  Classification",
    "descriptor": "",
    "authors": [
      "Tian Xie",
      "Rajgopal Kannan",
      "C.-C. Jay Kuo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.08646"
  },
  {
    "id": "arXiv:2204.10070",
    "title": "Multi-UAV trajectory planning for 3D visual inspection of complex  structures",
    "abstract": "Comments: Revised version, 17 pages",
    "descriptor": "\nComments: Revised version, 17 pages\n",
    "authors": [
      "Stefan Ivi\u0107",
      "Bojan Crnkovi\u0107",
      "Luka Grb\u010di\u0107",
      "Lea Matlekovi\u0107"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.10070"
  },
  {
    "id": "arXiv:2204.10268",
    "title": "Out-of-distribution generalization for learning quantum dynamics",
    "abstract": "Comments: 8 pages (main body) + 14 pages (references and appendix); 5+1 figures; V2 includes additional numerical experiments",
    "descriptor": "\nComments: 8 pages (main body) + 14 pages (references and appendix); 5+1 figures; V2 includes additional numerical experiments\n",
    "authors": [
      "Matthias C. Caro",
      "Hsin-Yuan Huang",
      "Nicholas Ezzell",
      "Joe Gibbs",
      "Andrew T. Sornborger",
      "Lukasz Cincio",
      "Patrick J. Coles",
      "Zo\u00eb Holmes"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.10268"
  },
  {
    "id": "arXiv:2204.10472",
    "title": "Sparse dynamical system identification with simultaneous structural  parameters and initial condition estimation",
    "abstract": "Sparse dynamical system identification with simultaneous structural  parameters and initial condition estimation",
    "descriptor": "",
    "authors": [
      "Baolei Wei"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Systems and Control (eess.SY)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2204.10472"
  },
  {
    "id": "arXiv:2204.12077",
    "title": "AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in  Ultrasound Images",
    "abstract": "Comments: Breast cancer segmentation, Ultrasound images, Hybrid attention, Adaptive learning, Deep learning",
    "descriptor": "\nComments: Breast cancer segmentation, Ultrasound images, Hybrid attention, Adaptive learning, Deep learning\n",
    "authors": [
      "Gongping Chen",
      "Yu Dai",
      "Jianxun Zhang",
      "Moi Hoon Yap"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.12077"
  },
  {
    "id": "arXiv:2204.13031",
    "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for  Dialog Response Generation",
    "abstract": "Comments: 13 pages, 1 figures, 9 tables",
    "descriptor": "\nComments: 13 pages, 1 figures, 9 tables\n",
    "authors": [
      "Wei Chen",
      "Yeyun Gong",
      "Song Wang",
      "Bolun Yao",
      "Weizhen Qi",
      "Zhongyu Wei",
      "Xiaowu Hu",
      "Bartuer Zhou",
      "Yi Mao",
      "Weizhu Chen",
      "Biao Cheng",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.13031"
  },
  {
    "id": "arXiv:2205.00922",
    "title": "ARK: Fully Homomorphic Encryption Accelerator with Runtime Data  Generation and Inter-Operation Key Reuse",
    "abstract": "Comments: 18 pages, 9 figures",
    "descriptor": "\nComments: 18 pages, 9 figures\n",
    "authors": [
      "Jongmin Kim",
      "Gwangho Lee",
      "Sangpyo Kim",
      "Gina Sohn",
      "John Kim",
      "Minsoo Rhu",
      "Jung Ho Ahn"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2205.00922"
  },
  {
    "id": "arXiv:2205.00953",
    "title": "BERTops: Studying BERT Representations under a Topological Lens",
    "abstract": "BERTops: Studying BERT Representations under a Topological Lens",
    "descriptor": "",
    "authors": [
      "Jatin Chauhan",
      "Manohar Kaul"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.00953"
  },
  {
    "id": "arXiv:2205.02332",
    "title": "Learning Individual Interactions from Population Dynamics with  Discrete-Event Simulation Model",
    "abstract": "Comments: for further modification",
    "descriptor": "\nComments: for further modification\n",
    "authors": [
      "Yan Shen",
      "Fan Yang",
      "Mingchen Gao",
      "Wen Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02332"
  },
  {
    "id": "arXiv:2205.02357",
    "title": "Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge  Graph Completion",
    "abstract": "Comments: Accepted by SIGIR 2022. Fix a severe bug (part done)",
    "descriptor": "\nComments: Accepted by SIGIR 2022. Fix a severe bug (part done)\n",
    "authors": [
      "Xiang Chen",
      "Ningyu Zhang",
      "Lei Li",
      "Shumin Deng",
      "Chuanqi Tan",
      "Changliang Xu",
      "Fei Huang",
      "Luo Si",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.02357"
  },
  {
    "id": "arXiv:2205.03040",
    "title": "Fusion: Efficient and Secure Inference Resilient to Malicious Servers",
    "abstract": "Comments: 18 pages, 6 figures",
    "descriptor": "\nComments: 18 pages, 6 figures\n",
    "authors": [
      "Caiqin Dong",
      "Jian Weng",
      "Jia-Nan Liu",
      "Yue Zhang",
      "Yao Tong",
      "Anjia Yang",
      "Yudan Cheng",
      "Shun Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.03040"
  },
  {
    "id": "arXiv:2205.03578",
    "title": "Automatic Detection of Interplanetary Coronal Mass Ejections in Solar  Wind In Situ Data",
    "abstract": "Automatic Detection of Interplanetary Coronal Mass Ejections in Solar  Wind In Situ Data",
    "descriptor": "",
    "authors": [
      "Hannah T. R\u00fcdisser",
      "Andreas Windisch",
      "Ute V. Amerstorfer",
      "Christian M\u00f6stl",
      "Tanja Amerstorfer",
      "Rachel L. Bailey",
      "Martin A. Reiss"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Space Physics (physics.space-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.03578"
  },
  {
    "id": "arXiv:2205.03743",
    "title": "End-to-End Rubbing Restoration Using Generative Adversarial Networks",
    "abstract": "Comments: 8 pages, 11 figures, the work has been accepted to the AI for content creation workshop at CVPR 2022",
    "descriptor": "\nComments: 8 pages, 11 figures, the work has been accepted to the AI for content creation workshop at CVPR 2022\n",
    "authors": [
      "Gongbo Sun",
      "Zijie Zheng",
      "Ming Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.03743"
  },
  {
    "id": "arXiv:2205.05011",
    "title": "Bike Share's Impact on COVID-19 Transmission and Bike Share's Responses  to COVID-19: A case study of Washington DC",
    "abstract": "Bike Share's Impact on COVID-19 Transmission and Bike Share's Responses  to COVID-19: A case study of Washington DC",
    "descriptor": "",
    "authors": [
      "Pedram Beigi",
      "Mohaiminul Haque",
      "Mohammad Sadra Rajabi",
      "Samer Hamdar"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Systems and Control (eess.SY)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.05011"
  },
  {
    "id": "arXiv:2205.06938",
    "title": "Generating Literal and Implied Subquestions to Fact-check Complex Claims",
    "abstract": "Generating Literal and Implied Subquestions to Fact-check Complex Claims",
    "descriptor": "",
    "authors": [
      "Jifan Chen",
      "Aniruddh Sriram",
      "Eunsol Choi",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.06938"
  },
  {
    "id": "arXiv:2205.08821",
    "title": "Lessons Learned: How (Not) to Defend Against Property Inference Attacks",
    "abstract": "Lessons Learned: How (Not) to Defend Against Property Inference Attacks",
    "descriptor": "",
    "authors": [
      "Joshua Stock",
      "Jens Wettlaufer",
      "Daniel Demmler",
      "Hannes Federrath"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.08821"
  },
  {
    "id": "arXiv:2205.09522",
    "title": "Defending Against Adversarial Attacks by Energy Storage Facility",
    "abstract": "Comments: 5 pages, 5 main figures. Published in PESGM 2022",
    "descriptor": "\nComments: 5 pages, 5 main figures. Published in PESGM 2022\n",
    "authors": [
      "Jiawei Li",
      "Jianxiao Wang",
      "Lin Chen",
      "Yang Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.09522"
  },
  {
    "id": "arXiv:2205.10488",
    "title": "Cryptanalysis of Three Quantum Money Schemes",
    "abstract": "Cryptanalysis of Three Quantum Money Schemes",
    "descriptor": "",
    "authors": [
      "Andriyan Bilyk",
      "Javad Doliskani",
      "Zhiyong Gong"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10488"
  },
  {
    "id": "arXiv:2205.11018",
    "title": "LexiconNet: An End-to-End Handwritten Paragraph Text Recognition System",
    "abstract": "LexiconNet: An End-to-End Handwritten Paragraph Text Recognition System",
    "descriptor": "",
    "authors": [
      "Lalita Kumari",
      "Sukhdeep Singh",
      "Vaibhav Varish Singh Rathore",
      "Anuj Sharma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11018"
  },
  {
    "id": "arXiv:2205.11485",
    "title": "Conditional Supervised Contrastive Learning for Fair Text Classification",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jianfeng Chi",
      "William Shand",
      "Yaodong Yu",
      "Kai-Wei Chang",
      "Han Zhao",
      "Yuan Tian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11485"
  },
  {
    "id": "arXiv:2205.11799",
    "title": "Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A  Pilot Study on Named Entity Recognition",
    "abstract": "Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A  Pilot Study on Named Entity Recognition",
    "descriptor": "",
    "authors": [
      "Zihan Wang",
      "Kewen Zhao",
      "Zilong Wang",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11799"
  },
  {
    "id": "arXiv:2205.11803",
    "title": "WeDef: Weakly Supervised Backdoor Defense for Text Classification",
    "abstract": "WeDef: Weakly Supervised Backdoor Defense for Text Classification",
    "descriptor": "",
    "authors": [
      "Lesheng Jin",
      "Zihan Wang",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11803"
  },
  {
    "id": "arXiv:2205.12393",
    "title": "Fine-tuned Language Models are Continual Learners",
    "abstract": "Fine-tuned Language Models are Continual Learners",
    "descriptor": "",
    "authors": [
      "Thomas Scialom",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12393"
  },
  {
    "id": "arXiv:2205.12476",
    "title": "Leveraging Locality in Abstractive Text Summarization",
    "abstract": "Comments: Accepted to EMNLP 2022",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Yixin Liu",
      "Ansong Ni",
      "Linyong Nan",
      "Budhaditya Deb",
      "Chenguang Zhu",
      "Ahmed H. Awadallah",
      "Dragomir Radev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12476"
  },
  {
    "id": "arXiv:2205.12672",
    "title": "Discovering Language-neutral Sub-networks in Multilingual Language  Models",
    "abstract": "Discovering Language-neutral Sub-networks in Multilingual Language  Models",
    "descriptor": "",
    "authors": [
      "Negar Foroutan",
      "Mohammadreza Banaei",
      "Remi Lebret",
      "Antoine Bosselut",
      "Karl Aberer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12672"
  },
  {
    "id": "arXiv:2205.14365",
    "title": "Granular Generalized Variable Precision Rough Sets and Rational  Approximations",
    "abstract": "Comments: 50 Pages",
    "descriptor": "\nComments: 50 Pages\n",
    "authors": [
      "A Mani",
      "Sushmita Mitra"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.14365"
  },
  {
    "id": "arXiv:2205.15031",
    "title": "Neural Copula: A unified framework for estimating generic  high-dimensional Copula functions",
    "abstract": "Neural Copula: A unified framework for estimating generic  high-dimensional Copula functions",
    "descriptor": "",
    "authors": [
      "Zhi Zeng",
      "Ting Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15031"
  },
  {
    "id": "arXiv:2205.15268",
    "title": "Federated X-Armed Bandit",
    "abstract": "Federated X-Armed Bandit",
    "descriptor": "",
    "authors": [
      "Wenjie Li",
      "Qifan Song",
      "Jean Honorio",
      "Guang Lin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15268"
  },
  {
    "id": "arXiv:2206.00128",
    "title": "ForestPrune: Compact Depth-Controlled Tree Ensembles",
    "abstract": "ForestPrune: Compact Depth-Controlled Tree Ensembles",
    "descriptor": "",
    "authors": [
      "Brian Liu",
      "Rahul Mazumder"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00128"
  },
  {
    "id": "arXiv:2206.00160",
    "title": "On an Information and Control Architecture for Future Electric Energy  Systems",
    "abstract": "On an Information and Control Architecture for Future Electric Energy  Systems",
    "descriptor": "",
    "authors": [
      "Le Xie",
      "Tong Huang",
      "P. R. Kumar",
      "Anupam A. Thatte",
      "Sanjoy K. Mitter"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00160"
  },
  {
    "id": "arXiv:2206.00208",
    "title": "AdaVITS: Tiny VITS for Low Computing Resource Speaker Adaptation",
    "abstract": "AdaVITS: Tiny VITS for Low Computing Resource Speaker Adaptation",
    "descriptor": "",
    "authors": [
      "Kun Song",
      "Heyang Xue",
      "Xinsheng Wang",
      "Jian Cong",
      "Yongmao Zhang",
      "Lei Xie",
      "Bing Yang",
      "Xiong Zhang",
      "Dan Su"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.00208"
  },
  {
    "id": "arXiv:2206.00939",
    "title": "Gradient flow dynamics of shallow ReLU networks for square loss and  orthogonal inputs",
    "abstract": "Gradient flow dynamics of shallow ReLU networks for square loss and  orthogonal inputs",
    "descriptor": "",
    "authors": [
      "Etienne Boursier",
      "Loucas Pillaud-Vivien",
      "Nicolas Flammarion"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00939"
  },
  {
    "id": "arXiv:2206.01272",
    "title": "Data-Driven Linear Koopman Embedding for Networked Systems:  Model-Predictive Grid Control",
    "abstract": "Data-Driven Linear Koopman Embedding for Networked Systems:  Model-Predictive Grid Control",
    "descriptor": "",
    "authors": [
      "Ramij R. Hossain",
      "Rahmat Adesunkanmi",
      "Ratnesh Kumar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.01272"
  },
  {
    "id": "arXiv:2206.01883",
    "title": "A symmetrized parametric finite element method for anisotropic surface  diffusion in 3D",
    "abstract": "A symmetrized parametric finite element method for anisotropic surface  diffusion in 3D",
    "descriptor": "",
    "authors": [
      "Weizhu Bao",
      "Yifei Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.01883"
  },
  {
    "id": "arXiv:2206.02568",
    "title": "A Deep Reinforcement Learning Framework For Column Generation",
    "abstract": "A Deep Reinforcement Learning Framework For Column Generation",
    "descriptor": "",
    "authors": [
      "Cheng Chi",
      "Amine Mohamed Aboussalah",
      "Elias B. Khalil",
      "Juyoung Wang",
      "Zoha Sherkat-Masoumi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02568"
  },
  {
    "id": "arXiv:2206.02904",
    "title": "The Creativity of Text-to-Image Generation",
    "abstract": "The Creativity of Text-to-Image Generation",
    "descriptor": "",
    "authors": [
      "Jonas Oppenlaender"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.02904"
  },
  {
    "id": "arXiv:2206.03200",
    "title": "FairVFL: A Fair Vertical Federated Learning Framework with Contrastive  Adversarial Learning",
    "abstract": "FairVFL: A Fair Vertical Federated Learning Framework with Contrastive  Adversarial Learning",
    "descriptor": "",
    "authors": [
      "Tao Qi",
      "Fangzhao Wu",
      "Chuhan Wu",
      "Lingjuan Lyu",
      "Tong Xu",
      "Zhongliang Yang",
      "Yongfeng Huang",
      "Xing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.03200"
  },
  {
    "id": "arXiv:2206.05437",
    "title": "ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle  Phase Transition",
    "abstract": "Comments: 26 pages, 5 figures, NeurIPS 2022 Workshop on GLFrontiers (Oral)",
    "descriptor": "\nComments: 26 pages, 5 figures, NeurIPS 2022 Workshop on GLFrontiers (Oral)\n",
    "authors": [
      "Yuelin Wang",
      "Kai Yi",
      "Xinliang Liu",
      "Yu Guang Wang",
      "Shi Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.05437"
  },
  {
    "id": "arXiv:2206.05522",
    "title": "The Effects of Spatial Configuration on Relative Translation Gain  Thresholds in Redirected Walking",
    "abstract": "Comments: 21 pages, 11 figures, Under review in the Springer VR Journal",
    "descriptor": "\nComments: 21 pages, 11 figures, Under review in the Springer VR Journal\n",
    "authors": [
      "Dooyoung Kim",
      "Seonji Kim",
      "Jae-eun Shin",
      "Boram Yoon",
      "Jinwook Kim",
      "Jeongmi Lee",
      "Woontack Woo"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.05522"
  },
  {
    "id": "arXiv:2206.06417",
    "title": "Image-based Treatment Effect Heterogeneity",
    "abstract": "Image-based Treatment Effect Heterogeneity",
    "descriptor": "",
    "authors": [
      "Connor T. Jerzak",
      "Fredrik Johansson",
      "Adel Daoud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.06417"
  },
  {
    "id": "arXiv:2206.06522",
    "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer  Learning",
    "abstract": "Comments: NeurIPS 2022 (our code is available at: this https URL)",
    "descriptor": "\nComments: NeurIPS 2022 (our code is available at: this https URL)\n",
    "authors": [
      "Yi-Lin Sung",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06522"
  },
  {
    "id": "arXiv:2206.06565",
    "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning  Tasks",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Tuan Dinh",
      "Yuchen Zeng",
      "Ruisu Zhang",
      "Ziqian Lin",
      "Michael Gira",
      "Shashank Rajput",
      "Jy-yong Sohn",
      "Dimitris Papailiopoulos",
      "Kangwook Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.06565"
  },
  {
    "id": "arXiv:2206.08771",
    "title": "Downlink Massive MU-MIMO with Successively-Regularized Zero Forcing  Precoding",
    "abstract": "Comments: 5 pages (main paper) + 1 page (MATLAB test), 2 figures. Accepted to the IEEE Wireless Communications Letters",
    "descriptor": "\nComments: 5 pages (main paper) + 1 page (MATLAB test), 2 figures. Accepted to the IEEE Wireless Communications Letters\n",
    "authors": [
      "Aravindh Krishnamoorthy",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08771"
  },
  {
    "id": "arXiv:2206.08839",
    "title": "Decentralized adaptive clustering of deep nets is beneficial for client  collaboration",
    "abstract": "Decentralized adaptive clustering of deep nets is beneficial for client  collaboration",
    "descriptor": "",
    "authors": [
      "Edvin Listo Zec",
      "Ebba Ekblom",
      "Martin Willbo",
      "Olof Mogren",
      "Sarunas Girdzijauskas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08839"
  },
  {
    "id": "arXiv:2206.08998",
    "title": "A review of machine learning concepts and methods for addressing  challenges in probabilistic hydrological post-processing and forecasting",
    "abstract": "A review of machine learning concepts and methods for addressing  challenges in probabilistic hydrological post-processing and forecasting",
    "descriptor": "",
    "authors": [
      "Georgia Papacharalampous",
      "Hristos Tyralis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08998"
  },
  {
    "id": "arXiv:2206.09123",
    "title": "POD-ROMs for incompressible flows including snapshots of the temporal  derivative of the full order solution",
    "abstract": "POD-ROMs for incompressible flows including snapshots of the temporal  derivative of the full order solution",
    "descriptor": "",
    "authors": [
      "Bosco Garc\u00eda-Archilla",
      "Volker John",
      "Julia Novo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.09123"
  },
  {
    "id": "arXiv:2206.09140",
    "title": "Certified Graph Unlearning",
    "abstract": "Comments: NeurIPS 2022 New Frontiers in Graph Learning Workshop (NeurIPS GLFrontiers 2022)",
    "descriptor": "\nComments: NeurIPS 2022 New Frontiers in Graph Learning Workshop (NeurIPS GLFrontiers 2022)\n",
    "authors": [
      "Eli Chien",
      "Chao Pan",
      "Olgica Milenkovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09140"
  },
  {
    "id": "arXiv:2206.09522",
    "title": "Multiple Testing Framework for Out-of-Distribution Detection",
    "abstract": "Multiple Testing Framework for Out-of-Distribution Detection",
    "descriptor": "",
    "authors": [
      "Akshayaa Magesh",
      "Venugopal V. Veeravalli",
      "Anirban Roy",
      "Susmit Jha"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09522"
  },
  {
    "id": "arXiv:2206.09682",
    "title": "SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous  Vehicles",
    "abstract": "Comments: Published as a conference paper at NeurIPS 2022 (Track on Datasets and Benchmarks)",
    "descriptor": "\nComments: Published as a conference paper at NeurIPS 2022 (Track on Datasets and Benchmarks)\n",
    "authors": [
      "Chejian Xu",
      "Wenhao Ding",
      "Weijie Lyu",
      "Zuxin Liu",
      "Shuai Wang",
      "Yihan He",
      "Hanjiang Hu",
      "Ding Zhao",
      "Bo Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.09682"
  },
  {
    "id": "arXiv:2206.10498",
    "title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning  and Reasoning about Change)",
    "abstract": "Comments: Accepted at Foundation Models for Decision Making Workshop at Neural Information Processing Systems, 2022",
    "descriptor": "\nComments: Accepted at Foundation Models for Decision Making Workshop at Neural Information Processing Systems, 2022\n",
    "authors": [
      "Karthik Valmeekam",
      "Alberto Olmo",
      "Sarath Sreedharan",
      "Subbarao Kambhampati"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.10498"
  },
  {
    "id": "arXiv:2206.11078",
    "title": "Traffic-Twitter Transformer: A Nature Language Processing-joined  Framework For Network-wide Traffic Forecasting",
    "abstract": "Traffic-Twitter Transformer: A Nature Language Processing-joined  Framework For Network-wide Traffic Forecasting",
    "descriptor": "",
    "authors": [
      "Meng-Ju Tsai",
      "Zhiyong Cui",
      "Hao Yang",
      "Cole Kopca",
      "Sophie Tien",
      "Yinhai Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.11078"
  },
  {
    "id": "arXiv:2206.11461",
    "title": "Towards Better User Studies in Computer Graphics and Vision",
    "abstract": "Comments: 15 pages of text, 6 pages of references, 2 figures, 1 table",
    "descriptor": "\nComments: 15 pages of text, 6 pages of references, 2 figures, 1 table\n",
    "authors": [
      "Zoya Bylinskii",
      "Laura Herman",
      "Aaron Hertzmann",
      "Stefanie Hutka",
      "Yile Zhang"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.11461"
  },
  {
    "id": "arXiv:2206.11682",
    "title": "EFFGAN: Ensembles of fine-tuned federated GANs",
    "abstract": "EFFGAN: Ensembles of fine-tuned federated GANs",
    "descriptor": "",
    "authors": [
      "Ebba Ekblom",
      "Edvin Listo Zec",
      "Olof Mogren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.11682"
  },
  {
    "id": "arXiv:2206.12458",
    "title": "Bag of Tricks for Long-Tail Visual Recognition of Animal Species in  Camera-Trap Images",
    "abstract": "Bag of Tricks for Long-Tail Visual Recognition of Animal Species in  Camera-Trap Images",
    "descriptor": "",
    "authors": [
      "Fagner Cunha",
      "Eulanda M. dos Santos",
      "Juan G. Colonna"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.12458"
  },
  {
    "id": "arXiv:2206.12490",
    "title": "Arithmetic Circuits, Structured Matrices and (not so) Deep Learning",
    "abstract": "Arithmetic Circuits, Structured Matrices and (not so) Deep Learning",
    "descriptor": "",
    "authors": [
      "Atri Rudra"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.12490"
  },
  {
    "id": "arXiv:2206.12728",
    "title": "Learning Preconditions of Hybrid Force-Velocity Controllers for  Contact-Rich Manipulation",
    "abstract": "Learning Preconditions of Hybrid Force-Velocity Controllers for  Contact-Rich Manipulation",
    "descriptor": "",
    "authors": [
      "Jacky Liang",
      "Xianyi Cheng",
      "Oliver Kroemer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.12728"
  },
  {
    "id": "arXiv:2206.13748",
    "title": "Principal Phrase Mining",
    "abstract": "Comments: 15 pages, 4 tables",
    "descriptor": "\nComments: 15 pages, 4 tables\n",
    "authors": [
      "Ellie Small",
      "Javier Cabrera"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.13748"
  },
  {
    "id": "arXiv:2206.13891",
    "title": "Feature Learning for Dimensionality Reduction toward Maximal Extraction  of Hidden Patterns",
    "abstract": "Comments: This manuscript is currently under review",
    "descriptor": "\nComments: This manuscript is currently under review\n",
    "authors": [
      "Takanori Fujiwara",
      "Yun-Hsin Kuo",
      "Anders Ynnerman",
      "Kwan-Liu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.13891"
  },
  {
    "id": "arXiv:2206.13903",
    "title": "AS-IntroVAE: Adversarial Similarity Distance Makes Robust IntroVAE",
    "abstract": "Comments: ACML conference paper",
    "descriptor": "\nComments: ACML conference paper\n",
    "authors": [
      "Changjie Lu",
      "Shen Zheng",
      "Zirui Wang",
      "Omar Dib",
      "Gaurav Gupta"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.13903"
  },
  {
    "id": "arXiv:2206.14314",
    "title": "Generative Neural Articulated Radiance Fields",
    "abstract": "Comments: Project website: this http URL",
    "descriptor": "\nComments: Project website: this http URL\n",
    "authors": [
      "Alexander W. Bergman",
      "Petr Kellnhofer",
      "Wang Yifan",
      "Eric R. Chan",
      "David B. Lindell",
      "Gordon Wetzstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.14314"
  },
  {
    "id": "arXiv:2206.14566",
    "title": "Using Interpretable Machine Learning to Massively Increase the Number of  Antibody-Virus Interactions Across Studies",
    "abstract": "Using Interpretable Machine Learning to Massively Increase the Number of  Antibody-Virus Interactions Across Studies",
    "descriptor": "",
    "authors": [
      "Tal Einav",
      "Rong Ma"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2206.14566"
  },
  {
    "id": "arXiv:2206.14683",
    "title": "Computer-aided diagnosis and prediction in brain disorders",
    "abstract": "Computer-aided diagnosis and prediction in brain disorders",
    "descriptor": "",
    "authors": [
      "Vikram Venkatraghavan",
      "Sebastian R. van der Voort",
      "Daniel Bos",
      "Marion Smits",
      "Frederik Barkhof",
      "Wiro J. Niessen",
      "Stefan Klein",
      "Esther E. Bron"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2206.14683"
  },
  {
    "id": "arXiv:2206.15000",
    "title": "Grounded Copilot: How Programmers Interact with Code-Generating Models",
    "abstract": "Grounded Copilot: How Programmers Interact with Code-Generating Models",
    "descriptor": "",
    "authors": [
      "Shraddha Barke",
      "Michael B. James",
      "Nadia Polikarpova"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.15000"
  },
  {
    "id": "arXiv:2207.01332",
    "title": "The least-control principle for local learning at equilibrium",
    "abstract": "Comments: Published at NeurIPS 2022. 56 pages",
    "descriptor": "\nComments: Published at NeurIPS 2022. 56 pages\n",
    "authors": [
      "Alexander Meulemans",
      "Nicolas Zucchet",
      "Seijin Kobayashi",
      "Johannes von Oswald",
      "Jo\u00e3o Sacramento"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2207.01332"
  },
  {
    "id": "arXiv:2207.02754",
    "title": "Tensor Neural Network and Its Numerical Integration",
    "abstract": "Comments: 18 pages, 20 figures",
    "descriptor": "\nComments: 18 pages, 20 figures\n",
    "authors": [
      "Yifan Wang",
      "Pengzhan Jin",
      "Hehu Xie"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2207.02754"
  },
  {
    "id": "arXiv:2207.03364",
    "title": "Group Equality in Adaptive Submodular Maximization",
    "abstract": "Group Equality in Adaptive Submodular Maximization",
    "descriptor": "",
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2207.03364"
  },
  {
    "id": "arXiv:2207.04194",
    "title": "Online algorithms for finding distinct substrings with length and  multiple prefix and suffix conditions",
    "abstract": "Comments: 14 pages (including references and appendix), 3 figures, 1 table",
    "descriptor": "\nComments: 14 pages (including references and appendix), 3 figures, 1 table\n",
    "authors": [
      "Laurentius Leonard",
      "Shunsuke Inenaga",
      "Hideo Bannai",
      "Takuya Mieno"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2207.04194"
  },
  {
    "id": "arXiv:2207.04507",
    "title": "Closing the Gap Between Directed Hopsets and Shortcut Sets",
    "abstract": "Comments: Abstract shortened to meet arXiv requirements, v2: fixed a typo, v3: implemented reviewer comments",
    "descriptor": "\nComments: Abstract shortened to meet arXiv requirements, v2: fixed a typo, v3: implemented reviewer comments\n",
    "authors": [
      "Aaron Bernstein",
      "Nicole Wein"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2207.04507"
  },
  {
    "id": "arXiv:2207.05127",
    "title": "RUSH: Robust Contrastive Learning via Randomized Smoothing",
    "abstract": "Comments: incomplete validation, the defense strategy will fail when considering Expectation Over Test (EOT)",
    "descriptor": "\nComments: incomplete validation, the defense strategy will fail when considering Expectation Over Test (EOT)\n",
    "authors": [
      "Yijiang Pang",
      "Boyang Liu",
      "Jiayu Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.05127"
  },
  {
    "id": "arXiv:2207.06144",
    "title": "A Beyond-5G Authentication and Key Agreement Protocol",
    "abstract": "A Beyond-5G Authentication and Key Agreement Protocol",
    "descriptor": "",
    "authors": [
      "Mohamed Taoufiq Damir",
      "Tommi Meskanen",
      "Sara Ramezanian",
      "Valtteri Niemi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2207.06144"
  },
  {
    "id": "arXiv:2207.06993",
    "title": "A fundamental non-classical logic",
    "abstract": "Comments: Added Definition 6.1, Propositions 2.3, 4.33, and 6.2, Remarks 1.1 and 1.2, Theorems 2.4, 4.28, 5.1, 6.3, and 6.4, and references",
    "descriptor": "\nComments: Added Definition 6.1, Propositions 2.3, 4.33, and 6.2, Remarks 1.1 and 1.2, Theorems 2.4, 4.28, 5.1, 6.3, and 6.4, and references\n",
    "authors": [
      "Wesley H. Holliday"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2207.06993"
  },
  {
    "id": "arXiv:2207.08455",
    "title": "Open-world Semantic Segmentation via Contrasting and Clustering  Vision-Language Embedding",
    "abstract": "Comments: Accepted to ECCV 2022 (revise acknowledgement)",
    "descriptor": "\nComments: Accepted to ECCV 2022 (revise acknowledgement)\n",
    "authors": [
      "Quande Liu",
      "Youpeng Wen",
      "Jianhua Han",
      "Chunjing Xu",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.08455"
  },
  {
    "id": "arXiv:2207.09927",
    "title": "ViGAT: Bottom-up event recognition and explanation in video using  factorized graph attention network",
    "abstract": "ViGAT: Bottom-up event recognition and explanation in video using  factorized graph attention network",
    "descriptor": "",
    "authors": [
      "Nikolaos Gkalelis",
      "Dimitrios Daskalakis",
      "Vasileios Mezaris"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2207.09927"
  },
  {
    "id": "arXiv:2207.10345",
    "title": "CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution",
    "abstract": "Comments: ECCV 2022",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Cheeun Hong",
      "Sungyong Baik",
      "Heewon Kim",
      "Seungjun Nah",
      "Kyoung Mu Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2207.10345"
  },
  {
    "id": "arXiv:2207.11388",
    "title": "Low-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering",
    "abstract": "Low-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering",
    "descriptor": "",
    "authors": [
      "Dong Yang",
      "Fei Jiang",
      "Wei Wu",
      "Xuefei Fang",
      "Muyong Cao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2207.11388"
  },
  {
    "id": "arXiv:2207.12230",
    "title": "Mind the hubris: complexity can misfire",
    "abstract": "Comments: This is a draft of a chapter that has been accepted for publication by Oxford University Press in the forthcoming book \"Views on Mathematical Modelling\", edited by Andrea Saltelli and Monica Di Fiore and due for publication in 2023",
    "descriptor": "\nComments: This is a draft of a chapter that has been accepted for publication by Oxford University Press in the forthcoming book \"Views on Mathematical Modelling\", edited by Andrea Saltelli and Monica Di Fiore and due for publication in 2023\n",
    "authors": [
      "Arnald Puy",
      "Andrea Saltelli"
    ],
    "subjectives": [
      "General Literature (cs.GL)",
      "History and Overview (math.HO)"
    ],
    "url": "https://arxiv.org/abs/2207.12230"
  },
  {
    "id": "arXiv:2207.12644",
    "title": "Learning Bipedal Walking On Planned Footsteps For Humanoid Robots",
    "abstract": "Comments: GitHub code: this https URL",
    "descriptor": "\nComments: GitHub code: this https URL\n",
    "authors": [
      "Rohan Pratap Singh",
      "Mehdi Benallegue",
      "Mitsuharu Morisawa",
      "Rafael Cisneros",
      "Fumio Kanehiro"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.12644"
  },
  {
    "id": "arXiv:2207.14200",
    "title": "CrAM: A Compression-Aware Minimizer",
    "abstract": "Comments: 28 pages, 2 figures. Contains important improvements to the results from the first version, experiments on language models and comparison with other methods. Added minor corrections to the second version",
    "descriptor": "\nComments: 28 pages, 2 figures. Contains important improvements to the results from the first version, experiments on language models and comparison with other methods. Added minor corrections to the second version\n",
    "authors": [
      "Alexandra Peste",
      "Adrian Vladu",
      "Eldar Kurtic",
      "Christoph H. Lampert",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.14200"
  },
  {
    "id": "arXiv:2207.14690",
    "title": "Renting Edge Computing Resources for Service Hosting",
    "abstract": "Comments: 31 pages, 8 figures",
    "descriptor": "\nComments: 31 pages, 8 figures\n",
    "authors": [
      "Aadesh Madnaik",
      "Sharayu Moharir",
      "Nikhil Karamchandani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2207.14690"
  },
  {
    "id": "arXiv:2208.00344",
    "title": "Towards Intercultural Affect Recognition: Audio-Visual Affect  Recognition in the Wild Across Six Cultures",
    "abstract": "Comments: Accepted at IEEE International Conference on Automatic Face and Gesture Recognition (FG 2023), publication and presentation at refereed IEEE workshop",
    "descriptor": "\nComments: Accepted at IEEE International Conference on Automatic Face and Gesture Recognition (FG 2023), publication and presentation at refereed IEEE workshop\n",
    "authors": [
      "Leena Mathur",
      "Ralph Adolphs",
      "Maja J Matari\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.00344"
  },
  {
    "id": "arXiv:2208.00685",
    "title": "Distortion element in the automorphism group of a full shift",
    "abstract": "Comments: 58 pages, 6 figures; exposition improved, typos corrected",
    "descriptor": "\nComments: 58 pages, 6 figures; exposition improved, typos corrected\n",
    "authors": [
      "Antonin Callard",
      "Ville Salo"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Computational Complexity (cs.CC)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2208.00685"
  },
  {
    "id": "arXiv:2208.00795",
    "title": "An Approximate Generalization of the Okamura-Seymour Theorem",
    "abstract": "An Approximate Generalization of the Okamura-Seymour Theorem",
    "descriptor": "",
    "authors": [
      "Nikhil Kumar"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2208.00795"
  },
  {
    "id": "arXiv:2208.01483",
    "title": "Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours",
    "abstract": "Comments: 7 pages, 2 figures To be published at EMNLP 2022",
    "descriptor": "\nComments: 7 pages, 2 figures To be published at EMNLP 2022\n",
    "authors": [
      "Eyal Shnarch",
      "Alon Halfon",
      "Ariel Gera",
      "Marina Danilevsky",
      "Yannis Katsis",
      "Leshem Choshen",
      "Martin Santillan Cooper",
      "Dina Epelboim",
      "Zheng Zhang",
      "Dakuo Wang",
      "Lucy Yip",
      "Liat Ein-Dor",
      "Lena Dankin",
      "Ilya Shnayderman",
      "Ranit Aharonov",
      "Yunyao Li",
      "Naftali Liberman",
      "Philip Levin Slesarev",
      "Gwilym Newton",
      "Shila Ofek-Koifman",
      "Noam Slonim",
      "Yoav Katz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2208.01483"
  },
  {
    "id": "arXiv:2208.05605",
    "title": "Symbolic Music Loop Generation with Neural Discrete Representations",
    "abstract": "Comments: Accepted at ISMIR 2022",
    "descriptor": "\nComments: Accepted at ISMIR 2022\n",
    "authors": [
      "Sangjun Han",
      "Hyeongrae Ihm",
      "Moontae Lee",
      "Woohyung Lim"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2208.05605"
  },
  {
    "id": "arXiv:2208.06882",
    "title": "CoShNet: A Hybrid Complex Valued Neural Network using Shearlets",
    "abstract": "Comments: 16 pages, 11 figures",
    "descriptor": "\nComments: 16 pages, 11 figures\n",
    "authors": [
      "Manny Ko",
      "Ujjawal K. Panchal",
      "H\u00e9ctor Andrade-Loarca",
      "Andres Mendez-Vazquez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.06882"
  },
  {
    "id": "arXiv:2208.07106",
    "title": "Degree of Convexity and Expected Distances in Polygons",
    "abstract": "Comments: 29 pages, 17 figures. This version corrects an error in the running time in Theorem 2 and Corollary 3 from the first version",
    "descriptor": "\nComments: 29 pages, 17 figures. This version corrects an error in the running time in Theorem 2 and Corollary 3 from the first version\n",
    "authors": [
      "Mikkel Abrahamsen",
      "Viktor Fredslund-Hansen"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2208.07106"
  },
  {
    "id": "arXiv:2208.07341",
    "title": "Fair Assortment Planning",
    "abstract": "Comments: 79 pages, 6 figures",
    "descriptor": "\nComments: 79 pages, 6 figures\n",
    "authors": [
      "Qinyi Chen",
      "Negin Golrezaei",
      "Fransisca Susan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2208.07341"
  },
  {
    "id": "arXiv:2208.09632",
    "title": "Adam Can Converge Without Any Modification On Update Rules",
    "abstract": "Comments: 66 pages",
    "descriptor": "\nComments: 66 pages\n",
    "authors": [
      "Yushun Zhang",
      "Congliang Chen",
      "Naichen Shi",
      "Ruoyu Sun",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2208.09632"
  },
  {
    "id": "arXiv:2208.09711",
    "title": "Improving Multilayer-Perceptron(MLP)-based Network Anomaly Detection  with Birch Clustering on CICIDS-2017 Dataset",
    "abstract": "Improving Multilayer-Perceptron(MLP)-based Network Anomaly Detection  with Birch Clustering on CICIDS-2017 Dataset",
    "descriptor": "",
    "authors": [
      "Yuhua Yin",
      "Julian Jang-Jaccard",
      "Fariza Sabrina",
      "Jin Kwak"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.09711"
  },
  {
    "id": "arXiv:2208.10367",
    "title": "Multi-View Attention Transfer for Efficient Speech Enhancement",
    "abstract": "Comments: Proceedings of Interspeech 2022",
    "descriptor": "\nComments: Proceedings of Interspeech 2022\n",
    "authors": [
      "Wooseok Shin",
      "Hyun Joon Park",
      "Jin Sob Kim",
      "Byung Hoon Lee",
      "Sung Won Han"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2208.10367"
  },
  {
    "id": "arXiv:2208.10461",
    "title": "Scale invariant process regression: Towards Bayesian ML with minimal  assumptions",
    "abstract": "Scale invariant process regression: Towards Bayesian ML with minimal  assumptions",
    "descriptor": "",
    "authors": [
      "Matthias Wieler"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2208.10461"
  },
  {
    "id": "arXiv:2208.11460",
    "title": "Improving Natural-Language-based Audio Retrieval with Transfer Learning  and Audio & Text Augmentations",
    "abstract": "Comments: accepted at DCASE Workshop 2022",
    "descriptor": "\nComments: accepted at DCASE Workshop 2022\n",
    "authors": [
      "Paul Primus",
      "Gerhard Widmer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2208.11460"
  },
  {
    "id": "arXiv:2208.12046",
    "title": "A Platform-Free Proof of Federated Learning Consensus Mechanism for  Sustainable Blockchains",
    "abstract": "Comments: Accepted by IEEE Journal on Selected Areas in Communications",
    "descriptor": "\nComments: Accepted by IEEE Journal on Selected Areas in Communications\n",
    "authors": [
      "Yuntao Wang",
      "Haixia Peng",
      "Zhou Su",
      "Tom H Luan",
      "Abderrahim Benslimane",
      "Yuan Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2208.12046"
  },
  {
    "id": "arXiv:2208.12079",
    "title": "Bridging the View Disparity Between Radar and Camera Features for  Multi-modal Fusion 3D Object Detection",
    "abstract": "Comments: 12 pages,6 figures",
    "descriptor": "\nComments: 12 pages,6 figures\n",
    "authors": [
      "Taohua Zhou",
      "Yining Shi",
      "Junjie Chen",
      "Kun Jiang",
      "Mengmeng Yang",
      "Diange Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.12079"
  },
  {
    "id": "arXiv:2208.12437",
    "title": "Detecting Mitoses with a Convolutional Neural Network for MIDOG 2022  Challenge",
    "abstract": "Comments: 3 pages, 2 figures",
    "descriptor": "\nComments: 3 pages, 2 figures\n",
    "authors": [
      "Hongyan Gu",
      "Mohammad Haeri",
      "Shuo Ni",
      "Christopher Kazu Williams",
      "Neda Zarrin-Khameh",
      "Shino Magaki",
      "Xiang 'Anthony' Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.12437"
  },
  {
    "id": "arXiv:2208.12967",
    "title": "Anti-Retroactive Interference for Lifelong Learning",
    "abstract": "Anti-Retroactive Interference for Lifelong Learning",
    "descriptor": "",
    "authors": [
      "Runqi Wang",
      "Yuxiang Bao",
      "Baochang Zhang",
      "Jianzhuang Liu",
      "Wentao Zhu",
      "Guodong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.12967"
  },
  {
    "id": "arXiv:2208.13077",
    "title": "SupervisorBot: NLP-Annotated Real-Time Recommendations of Psychotherapy  Treatment Strategies with Deep Reinforcement Learning",
    "abstract": "Comments: This work extends our work series in interactive speech or text systems for psychotherapy (e.g. arXiv:2006.04376, arXiv:2204.05522 and arXiv:2204.10189) and proposes a novel recommendation setting",
    "descriptor": "\nComments: This work extends our work series in interactive speech or text systems for psychotherapy (e.g. arXiv:2006.04376, arXiv:2204.05522 and arXiv:2204.10189) and proposes a novel recommendation setting\n",
    "authors": [
      "Baihan Lin",
      "Guillermo Cecchi",
      "Djallel Bouneffouf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2208.13077"
  },
  {
    "id": "arXiv:2208.13848",
    "title": "ProspectNet: Weighted Conditional Attention for Future Interaction  Modeling in Behavior Prediction",
    "abstract": "ProspectNet: Weighted Conditional Attention for Future Interaction  Modeling in Behavior Prediction",
    "descriptor": "",
    "authors": [
      "Yutian Pang",
      "Zehua Guo",
      "Binnan Zhuang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2208.13848"
  },
  {
    "id": "arXiv:2208.14311",
    "title": "Modeling Volatility and Dependence of European Carbon and Energy Prices",
    "abstract": "Modeling Volatility and Dependence of European Carbon and Energy Prices",
    "descriptor": "",
    "authors": [
      "Jonathan Berrisch",
      "Sven Pappert",
      "Florian Ziel",
      "Antonia Arsova"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Applications (stat.AP)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2208.14311"
  },
  {
    "id": "arXiv:2209.00546",
    "title": "MSGNN: A Spectral Graph Neural Network Based on a Novel Magnetic Signed  Laplacian",
    "abstract": "MSGNN: A Spectral Graph Neural Network Based on a Novel Magnetic Signed  Laplacian",
    "descriptor": "",
    "authors": [
      "Yixuan He",
      "Michael Permultter",
      "Gesine Reinert",
      "Mihai Cucuringu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.00546"
  },
  {
    "id": "arXiv:2209.02139",
    "title": "Cross-Lingual and Cross-Domain Crisis Classification for Low-Resource  Scenarios",
    "abstract": "Comments: Accepted at the International AAAI Conference on Web and Social Media (ICWSM 2023)",
    "descriptor": "\nComments: Accepted at the International AAAI Conference on Web and Social Media (ICWSM 2023)\n",
    "authors": [
      "Cinthia S\u00e1nchez",
      "Hernan Sarmiento",
      "Andres Abeliuk",
      "Jorge P\u00e9rez",
      "Barbara Poblete"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.02139"
  },
  {
    "id": "arXiv:2209.02577",
    "title": "Avgust: Automating Usage-Based Test Generation from Videos of App  Executions",
    "abstract": "Avgust: Automating Usage-Based Test Generation from Videos of App  Executions",
    "descriptor": "",
    "authors": [
      "Yixue Zhao",
      "Saghar Talebipour",
      "Kesina Baral",
      "Hyojae Park",
      "Leon Yee",
      "Safwat Ali Khan",
      "Yuriy Brun",
      "Nenad Medvidovic",
      "Kevin Moran"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.02577"
  },
  {
    "id": "arXiv:2209.02855",
    "title": "The Role of Vocal Persona in Natural and Synthesized Speech",
    "abstract": "Comments: To be published in the proceedings of the 17th IEEE International Conference on Automatic Face and Gesture Recognition as part of the Workshop on Socially Interactive Human-like Virtual Agents (SIVA '23)",
    "descriptor": "\nComments: To be published in the proceedings of the 17th IEEE International Conference on Automatic Face and Gesture Recognition as part of the Workshop on Socially Interactive Human-like Virtual Agents (SIVA '23)\n",
    "authors": [
      "Camille Noufi",
      "Lloyd May",
      "Jonathan Berger"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Human-Computer Interaction (cs.HC)",
      "Audio and Speech Processing (eess.AS)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.02855"
  },
  {
    "id": "arXiv:2209.03286",
    "title": "Fairly Allocating (Contiguous) Dynamic Indivisible Items with Few  Adjustments",
    "abstract": "Fairly Allocating (Contiguous) Dynamic Indivisible Items with Few  Adjustments",
    "descriptor": "",
    "authors": [
      "Mingwei Yang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.03286"
  },
  {
    "id": "arXiv:2209.04427",
    "title": "Zydeco-Style Spike Sorting Low Power VLSI Architecture for IoT BCI  Implants",
    "abstract": "Comments: 6 pages, 7 Figures",
    "descriptor": "\nComments: 6 pages, 7 Figures\n",
    "authors": [
      "Zag ElSayed",
      "Murat Ozer",
      "Nelly Elsayed",
      "Magdy Bayoumi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.04427"
  },
  {
    "id": "arXiv:2209.04684",
    "title": "A constrained gentlest ascent dynamics and its applications to finding  excited states of Bose-Einstein condensates",
    "abstract": "Comments: 28 pages, 7 figures; Submitted to Journal of Computational Physics on January 30, 2021",
    "descriptor": "\nComments: 28 pages, 7 figures; Submitted to Journal of Computational Physics on January 30, 2021\n",
    "authors": [
      "Wei Liu",
      "Ziqing Xie",
      "Yongjun Yuan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Quantum Gases (cond-mat.quant-gas)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.04684"
  },
  {
    "id": "arXiv:2209.05191",
    "title": "Deep Reinforcement Learning for Online Latency Aware Workload Offloading  in Mobile Edge Computing",
    "abstract": "Comments: 9 pages, 7 figures, This paper has been accepted for the publication at the GLOBECOM' 22",
    "descriptor": "\nComments: 9 pages, 7 figures, This paper has been accepted for the publication at the GLOBECOM' 22\n",
    "authors": [
      "Zeinab Akhavan",
      "Mona Esmaeili",
      "Babak Badnava",
      "Mohammad Yousefi",
      "Xiang Sun",
      "Michael Devetsikiotis",
      "Payman Zarkesh-Ha"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.05191"
  },
  {
    "id": "arXiv:2209.07193",
    "title": "NU-net: An Unpretentious Nested U-net for Breast Tumor Segmentation",
    "abstract": "NU-net: An Unpretentious Nested U-net for Breast Tumor Segmentation",
    "descriptor": "",
    "authors": [
      "Gong-Ping Chen",
      "Lei Li",
      "Yu Dai",
      "Jian-Xun Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.07193"
  },
  {
    "id": "arXiv:2209.07237",
    "title": "Robust Implementation of Foreground Extraction and Vessel Segmentation  for X-ray Coronary Angiography Image Sequence",
    "abstract": "Comments: 32pages, 8figures",
    "descriptor": "\nComments: 32pages, 8figures\n",
    "authors": [
      "Zeyu Fu",
      "Zhuang Fu",
      "Chenzhuo Lu",
      "Jun Yan",
      "Jian Fei",
      "Hui Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.07237"
  },
  {
    "id": "arXiv:2209.07601",
    "title": "Towards Improving Calibration in Object Detection Under Domain Shift",
    "abstract": "Comments: To appear in NeurIPS 2022",
    "descriptor": "\nComments: To appear in NeurIPS 2022\n",
    "authors": [
      "Muhammad Akhtar Munir",
      "Muhammad Haris Khan",
      "M. Saquib Sarfraz",
      "Mohsen Ali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.07601"
  },
  {
    "id": "arXiv:2209.08615",
    "title": "Membership Inference Attacks and Generalization: A Causal Perspective",
    "abstract": "Comments: 26 pages, 15 figures; added CC-license block icons and links, typos corrected, added reference to Github",
    "descriptor": "\nComments: 26 pages, 15 figures; added CC-license block icons and links, typos corrected, added reference to Github\n",
    "authors": [
      "Teodora Baluta",
      "Shiqi Shen",
      "S. Hitarth",
      "Shruti Tople",
      "Prateek Saxena"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.08615"
  },
  {
    "id": "arXiv:2209.09116",
    "title": "Trolley optimisation: An extension of bin packing to load PCB components",
    "abstract": "Trolley optimisation: An extension of bin packing to load PCB components",
    "descriptor": "",
    "authors": [
      "Vinod Kumar Chauhan",
      "Mark Bass",
      "Ajith Kumar Parlikad",
      "Alexandra Brintrup"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.09116"
  },
  {
    "id": "arXiv:2209.09233",
    "title": "Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in  Dynamic Environments",
    "abstract": "Comments: Submitted to ICRA 2023",
    "descriptor": "\nComments: Submitted to ICRA 2023\n",
    "authors": [
      "Mingyo Seo",
      "Ryan Gupta",
      "Yifeng Zhu",
      "Alexy Skoutnev",
      "Luis Sentis",
      "Yuke Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.09233"
  },
  {
    "id": "arXiv:2209.09339",
    "title": "Identifying and Characterizing Behavioral Classes of Radicalization  within the QAnon Conspiracy on Twitter",
    "abstract": "Comments: 12 pages, 10 figures, 2 tables",
    "descriptor": "\nComments: 12 pages, 10 figures, 2 tables\n",
    "authors": [
      "Emily L. Wang",
      "Luca Luceri",
      "Francesco Pierri",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.09339"
  },
  {
    "id": "arXiv:2209.09367",
    "title": "Supporting Multi-Cloud in Serverless Computing",
    "abstract": "Comments: Accepted for the 15th IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC'22 Companion)",
    "descriptor": "\nComments: Accepted for the 15th IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC'22 Companion)\n",
    "authors": [
      "Haidong Zhao",
      "Zakaria Benomar",
      "Tobias Pfandzelter",
      "Nikolaos Georgantas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.09367"
  },
  {
    "id": "arXiv:2209.09478",
    "title": "Guiding vector fields for the distributed motion coordination of mobile  robots",
    "abstract": "Comments: Evolved paper from arXiv:2103.12372. Accepted to IEEE Transactions on Robotics. Supplementary video: this https URL",
    "descriptor": "\nComments: Evolved paper from arXiv:2103.12372. Accepted to IEEE Transactions on Robotics. Supplementary video: this https URL\n",
    "authors": [
      "Weijia Yao",
      "Hector Garcia de Marina",
      "Zhiyong Sun",
      "Ming Cao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.09478"
  },
  {
    "id": "arXiv:2209.09941",
    "title": "Predicting Drug-Drug Interactions using Deep Generative Models on Graphs",
    "abstract": "Predicting Drug-Drug Interactions using Deep Generative Models on Graphs",
    "descriptor": "",
    "authors": [
      "Nhat Khang Ngo",
      "Truong Son Hy",
      "Risi Kondor"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.09941"
  },
  {
    "id": "arXiv:2209.10674",
    "title": "Modeling Perceptual Loudness of Piano Tone: Theory and Applications",
    "abstract": "Comments: Accepted to ISMIR 2022",
    "descriptor": "\nComments: Accepted to ISMIR 2022\n",
    "authors": [
      "Yang Qu",
      "Yutian Qin",
      "Lecheng Chao",
      "Hangkai Qian",
      "Ziyu Wang",
      "Gus Xia"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.10674"
  },
  {
    "id": "arXiv:2209.10707",
    "title": "Gaussian Process Hydrodynamics",
    "abstract": "Comments: 23 pages. See this https URL for animations",
    "descriptor": "\nComments: 23 pages. See this https URL for animations\n",
    "authors": [
      "Houman Owhadi"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.10707"
  },
  {
    "id": "arXiv:2209.11422",
    "title": "LEADER: Learning Attention over Driving Behaviors for Planning under  Uncertainty",
    "abstract": "Comments: CoRL 2022 (oral)",
    "descriptor": "\nComments: CoRL 2022 (oral)\n",
    "authors": [
      "Mohamad H. Danesh",
      "Panpan Cai",
      "David Hsu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.11422"
  },
  {
    "id": "arXiv:2209.12929",
    "title": "Noncommutative Differential Geometry on Infinitesimal Spaces",
    "abstract": "Noncommutative Differential Geometry on Infinitesimal Spaces",
    "descriptor": "",
    "authors": [
      "Damien Tageddine",
      "Jean-Christophe Nave"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.12929"
  },
  {
    "id": "arXiv:2209.13430",
    "title": "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training",
    "abstract": "Comments: Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Janghyeon Lee",
      "Jongsuk Kim",
      "Hyounguk Shon",
      "Bumsoo Kim",
      "Seung Hwan Kim",
      "Honglak Lee",
      "Junmo Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13430"
  },
  {
    "id": "arXiv:2209.13821",
    "title": "Online Multi Camera-IMU Calibration",
    "abstract": "Online Multi Camera-IMU Calibration",
    "descriptor": "",
    "authors": [
      "Jacob Hartzer",
      "Srikanth Saripalli"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.13821"
  },
  {
    "id": "arXiv:2209.15451",
    "title": "Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance  Image Segmentation with High Quality Pseudo Labels",
    "abstract": "Comments: Accepted by International Workshop on Statistical Atlases and Computational Models of the Heart (STACOM2022) of MICCAI2022",
    "descriptor": "\nComments: Accepted by International Workshop on Statistical Atlases and Computational Models of the Heart (STACOM2022) of MICCAI2022\n",
    "authors": [
      "Wanqin Ma",
      "Huifeng Yao",
      "Yiqun Lin",
      "Jiarong Guo",
      "Xiaomeng Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15451"
  },
  {
    "id": "arXiv:2210.00960",
    "title": "Stability Analysis and Generalization Bounds of Adversarial Training",
    "abstract": "Comments: Published as a conference paper in NeurIPS2022",
    "descriptor": "\nComments: Published as a conference paper in NeurIPS2022\n",
    "authors": [
      "Jiancong Xiao",
      "Yanbo Fan",
      "Ruoyu Sun",
      "Jue Wang",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00960"
  },
  {
    "id": "arXiv:2210.01282",
    "title": "Structural Estimation of Markov Decision Processes in High-Dimensional  State Space with Finite-Time Guarantees",
    "abstract": "Comments: This conference version of this paper refers to \"Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees\" in NeurIPS 2022",
    "descriptor": "\nComments: This conference version of this paper refers to \"Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time Guarantees\" in NeurIPS 2022\n",
    "authors": [
      "Siliang Zeng",
      "Mingyi Hong",
      "Alfredo Garcia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01282"
  },
  {
    "id": "arXiv:2210.01307",
    "title": "Beam Management in Ultra-dense mmWave Network via Federated  Reinforcement Learning: An Intelligent and Secure Approach",
    "abstract": "Comments: 12 pages, 12 figures, has been accepted for publication in IEEE Transactions on Cognitive Communications and Networking",
    "descriptor": "\nComments: 12 pages, 12 figures, has been accepted for publication in IEEE Transactions on Cognitive Communications and Networking\n",
    "authors": [
      "Qing Xue",
      "Yi-Jing Liu",
      "Yao Sun",
      "Jian Wang",
      "Li Yan",
      "Gang Feng",
      "Shaodan Ma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01307"
  },
  {
    "id": "arXiv:2210.02631",
    "title": "Data-driven Approaches to Surrogate Machine Learning Model Development",
    "abstract": "Comments: 16 pages, 13 figures",
    "descriptor": "\nComments: 16 pages, 13 figures\n",
    "authors": [
      "H. Rhys Jones",
      "Tingting Mu",
      "Andrei C. Popescu",
      "Yusuf Sulehman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.02631"
  },
  {
    "id": "arXiv:2210.02643",
    "title": "Automatic Scene-based Topic Channel Construction System for E-Commerce",
    "abstract": "Comments: EMNLP2022 Camera-ready",
    "descriptor": "\nComments: EMNLP2022 Camera-ready\n",
    "authors": [
      "Peng Lin",
      "Yanyan Zou",
      "Lingfei Wu",
      "Mian Ma",
      "Zhuoye Ding",
      "Bo Long"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02643"
  },
  {
    "id": "arXiv:2210.03410",
    "title": "The Power of Small Coalitions under Two-Tier Majority on Regular Graphs",
    "abstract": "Comments: 28 pages, 8 figures, submitted",
    "descriptor": "\nComments: 28 pages, 8 figures, submitted\n",
    "authors": [
      "Pavel Chebotarev",
      "David Peleg"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.03410"
  },
  {
    "id": "arXiv:2210.03444",
    "title": "Depersonalized Federated Learning: Tackling Statistical Heterogeneity by  Alternating Stochastic Gradient Descent",
    "abstract": "Depersonalized Federated Learning: Tackling Statistical Heterogeneity by  Alternating Stochastic Gradient Descent",
    "descriptor": "",
    "authors": [
      "Yujie Zhou",
      "Zhidu Li",
      "Tong Tang",
      "Ruyan Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03444"
  },
  {
    "id": "arXiv:2210.03809",
    "title": "Retrieval Augmented Visual Question Answering with Outside Knowledge",
    "abstract": "Comments: Accepted to appear at the main conference of EMNLP 2022",
    "descriptor": "\nComments: Accepted to appear at the main conference of EMNLP 2022\n",
    "authors": [
      "Weizhe Lin",
      "Bill Byrne"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.03809"
  },
  {
    "id": "arXiv:2210.03936",
    "title": "Cloud Native Robotic Applications with GPU Sharing on Kubernetes",
    "abstract": "Comments: Submission accepted at the IROS'22 Cloud Robotics Workshop",
    "descriptor": "\nComments: Submission accepted at the IROS'22 Cloud Robotics Workshop\n",
    "authors": [
      "Giovanni Toffetti",
      "Leonardo Militano",
      "Se\u00e1n Murphy",
      "Remo Maurer",
      "Mark Straub"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.03936"
  },
  {
    "id": "arXiv:2210.04001",
    "title": "Don't Waste Data: Transfer Learning to Leverage All Data for  Machine-Learnt Climate Model Emulation",
    "abstract": "Comments: 8 pages. NeurIPS 2022 Workshop: Tackling Climate Change with Machine Learning. Spotlight talk",
    "descriptor": "\nComments: 8 pages. NeurIPS 2022 Workshop: Tackling Climate Change with Machine Learning. Spotlight talk\n",
    "authors": [
      "Raghul Parthipan",
      "Damon J. Wischik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2210.04001"
  },
  {
    "id": "arXiv:2210.04006",
    "title": "(Fusionformer):Exploiting the Joint Motion Synergy with Fusion Network  Based On Transformer for 3D Human Pose Estimation",
    "abstract": "(Fusionformer):Exploiting the Joint Motion Synergy with Fusion Network  Based On Transformer for 3D Human Pose Estimation",
    "descriptor": "",
    "authors": [
      "Xinwei Yu",
      "Xiaohua Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04006"
  },
  {
    "id": "arXiv:2210.04297",
    "title": "Optimal Control for Platooning in Vehicular Networks",
    "abstract": "Optimal Control for Platooning in Vehicular Networks",
    "descriptor": "",
    "authors": [
      "Thiago S. Gomides",
      "Evangelos Kranakis",
      "Ioannis Lambadaris",
      "Yannis Viniotis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.04297"
  },
  {
    "id": "arXiv:2210.04754",
    "title": "Semantically Enhanced Hard Negatives for Cross-modal Information  Retrieval",
    "abstract": "Semantically Enhanced Hard Negatives for Cross-modal Information  Retrieval",
    "descriptor": "",
    "authors": [
      "Yan Gong",
      "Georgina Cosma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.04754"
  },
  {
    "id": "arXiv:2210.04996",
    "title": "Graph2Vid: Flow graph to Video Grounding for Weakly-supervised  Multi-Step Localization",
    "abstract": "Comments: ECCV'22, oral",
    "descriptor": "\nComments: ECCV'22, oral\n",
    "authors": [
      "Nikita Dvornik",
      "Isma Hadji",
      "Hai Pham",
      "Dhaivat Bhatt",
      "Brais Martinez",
      "Afsaneh Fazly",
      "Allan D. Jepson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04996"
  },
  {
    "id": "arXiv:2210.05371",
    "title": "A global analysis of global optimisation",
    "abstract": "A global analysis of global optimisation",
    "descriptor": "",
    "authors": [
      "Lachlan Ewen MacDonald",
      "Hemanth Saratchandran",
      "Jack Valmadre",
      "Simon Lucey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05371"
  },
  {
    "id": "arXiv:2210.05793",
    "title": "Comparison of Soft and Hard Target RNN-T Distillation for Large-scale  ASR",
    "abstract": "Comments: 8 pages, 2 figures",
    "descriptor": "\nComments: 8 pages, 2 figures\n",
    "authors": [
      "Dongseong Hwang",
      "Khe Chai Sim",
      "Yu Zhang",
      "Trevor Strohman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05793"
  },
  {
    "id": "arXiv:2210.06007",
    "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment  Generation via Transformer VQ-VAE",
    "abstract": "Comments: Accepted at ISMIR 2022",
    "descriptor": "\nComments: Accepted at ISMIR 2022\n",
    "authors": [
      "Yueh-Kao Wu",
      "Ching-Yu Chiu",
      "Yi-Hsuan Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06007"
  },
  {
    "id": "arXiv:2210.06179",
    "title": "Convolutional Neural Network-Based Image Watermarking using Discrete  Wavelet Transform",
    "abstract": "Convolutional Neural Network-Based Image Watermarking using Discrete  Wavelet Transform",
    "descriptor": "",
    "authors": [
      "Alireza Tavakoli",
      "Zahra Honjani",
      "Hedieh Sajedi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06179"
  },
  {
    "id": "arXiv:2210.06692",
    "title": "Model-Based Offline Reinforcement Learning with Pessimism-Modulated  Dynamics Belief",
    "abstract": "Comments: NeurIPS 2022 (Oral)",
    "descriptor": "\nComments: NeurIPS 2022 (Oral)\n",
    "authors": [
      "Kaiyang Guo",
      "Yunfeng Shao",
      "Yanhui Geng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06692"
  },
  {
    "id": "arXiv:2210.06744",
    "title": "Visualizing Multispecies Coalescent Trees: Drawing Gene Trees Inside  Species Trees",
    "abstract": "Comments: Appears in the Proceedings of SOFSEM 2023",
    "descriptor": "\nComments: Appears in the Proceedings of SOFSEM 2023\n",
    "authors": [
      "Jonathan Klawitter",
      "Felix Klesen",
      "Moritz Niederer",
      "Alexander Wolff"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.06744"
  },
  {
    "id": "arXiv:2210.06778",
    "title": "X-Align: Cross-Modal Cross-View Alignment for Bird's-Eye-View  Segmentation",
    "abstract": "Comments: Accepted to WACV 2023",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Shubhankar Borse",
      "Marvin Klingner",
      "Varun Ravi Kumar",
      "Hong Cai",
      "Abdulaziz Almuzairee",
      "Senthil Yogamani",
      "Fatih Porikli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06778"
  },
  {
    "id": "arXiv:2210.07332",
    "title": "Secure Multiparty Computation for Synthetic Data Generation from  Distributed Data",
    "abstract": "Secure Multiparty Computation for Synthetic Data Generation from  Distributed Data",
    "descriptor": "",
    "authors": [
      "Mayana Pereira",
      "Sikha Pentyala",
      "Anderson Nascimento",
      "Rafael T. de Sousa Jr.",
      "Martine De Cock"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07332"
  },
  {
    "id": "arXiv:2210.07789",
    "title": "i13DR: A Real-Time Demand Response Infrastructure for Integrating  Renewable Energy Resources",
    "abstract": "i13DR: A Real-Time Demand Response Infrastructure for Integrating  Renewable Energy Resources",
    "descriptor": "",
    "authors": [
      "Pezhman Nasirifard",
      "Hans-Arno Jacobsen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.07789"
  },
  {
    "id": "arXiv:2210.07904",
    "title": "HashFormers: Towards Vocabulary-independent Pre-trained Transformers",
    "abstract": "Comments: Accepted at EMNLP 2022",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Huiyin Xue",
      "Nikolaos Aletras"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07904"
  },
  {
    "id": "arXiv:2210.08182",
    "title": "Learning Invariant Representation and Risk Minimized for Unsupervised  Accent Domain Adaptation",
    "abstract": "Comments: Accepted to 2022 IEEE Spoken Language Technology Workshop (SLT 2022)",
    "descriptor": "\nComments: Accepted to 2022 IEEE Spoken Language Technology Workshop (SLT 2022)\n",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.08182"
  },
  {
    "id": "arXiv:2210.08210",
    "title": "Providing Error Detection for Deep Learning Image Classifiers Using  Self-Explainability",
    "abstract": "Providing Error Detection for Deep Learning Image Classifiers Using  Self-Explainability",
    "descriptor": "",
    "authors": [
      "Mohammad Mahdi Karimi",
      "Azin Heidarshenas",
      "William W. Edmonson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.08210"
  },
  {
    "id": "arXiv:2210.08634",
    "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of  Self-Supervised Speech Representation Learning",
    "abstract": "Comments: Accepted by 2022 SLT Workshop",
    "descriptor": "\nComments: Accepted by 2022 SLT Workshop\n",
    "authors": [
      "Tzu-hsun Feng",
      "Annie Dong",
      "Ching-Feng Yeh",
      "Shu-wen Yang",
      "Tzu-Quan Lin",
      "Jiatong Shi",
      "Kai-Wei Chang",
      "Zili Huang",
      "Haibin Wu",
      "Xuankai Chang",
      "Shinji Watanabe",
      "Abdelrahman Mohamed",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.08634"
  },
  {
    "id": "arXiv:2210.08786",
    "title": "How \"troll\" are you? Measuring and detecting troll behavior in online  social networks",
    "abstract": "Comments: 15 pages",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Fatima Ezzeddine",
      "Luca Luceri",
      "Omran Ayoub",
      "Ihab Sbeity",
      "Gianluca Nogara",
      "Emilio Ferrara",
      "Silvia Giordano"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.08786"
  },
  {
    "id": "arXiv:2210.08993",
    "title": "When Digital Economy Meets Web3.0: Applications and Challenges",
    "abstract": "Comments: 14 pages, 5 figures",
    "descriptor": "\nComments: 14 pages, 5 figures\n",
    "authors": [
      "Chuan Chen",
      "Lei Zhang",
      "Yihao Li",
      "Tianchi Liao",
      "Siran Zhao",
      "Zibin Zheng",
      "Huawei Huang",
      "Jiajing Wu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.08993"
  },
  {
    "id": "arXiv:2210.09081",
    "title": "Asymptotic-Preserving Neural Networks for hyperbolic systems with  diffusive scaling",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2206.12625",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2206.12625\n",
    "authors": [
      "Giulia Bertaglia"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09081"
  },
  {
    "id": "arXiv:2210.09083",
    "title": "Nish: A Novel Negative Stimulated Hybrid Activation Function",
    "abstract": "Comments: 9 pages, 2 figures, 2 tables",
    "descriptor": "\nComments: 9 pages, 2 figures, 2 tables\n",
    "authors": [
      "Yildiray Anagun",
      "Sahin Isik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.09083"
  },
  {
    "id": "arXiv:2210.09517",
    "title": "Graph neural networks to learn joint representations of disjoint  molecular graphs",
    "abstract": "Comments: 5 pages, 4 figures",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Chen Shao",
      "Zhou Chen",
      "Pascal Friederich"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.09517"
  },
  {
    "id": "arXiv:2210.09840",
    "title": "Graph-Based Multilingual Label Propagation for Low-Resource  Part-of-Speech Tagging",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ayyoob Imani",
      "Silvia Severini",
      "Masoud Jalili Sabet",
      "Fran\u00e7ois Yvon",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.09840"
  },
  {
    "id": "arXiv:2210.09946",
    "title": "MMGA: Multimodal Learning with Graph Alignment",
    "abstract": "Comments: Please contact xuany@zju.edu.cn for the dataset",
    "descriptor": "\nComments: Please contact xuany@zju.edu.cn for the dataset\n",
    "authors": [
      "Xuan Yang",
      "Quanjin Tao",
      "Xiao Feng",
      "Donghong Cai",
      "Xiang Ren",
      "Yang Yang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09946"
  },
  {
    "id": "arXiv:2210.10202",
    "title": "Planning with SiMBA: Motion Planning under Uncertainty for Temporal  Goals using Simplified Belief Guides",
    "abstract": "Comments: 8 pages, submitted to IEEE International Conference on Robotics and Automation (ICRA), 2023",
    "descriptor": "\nComments: 8 pages, submitted to IEEE International Conference on Robotics and Automation (ICRA), 2023\n",
    "authors": [
      "Qi Heng Ho",
      "Zachary N. Sunberg",
      "Morteza Lahijanian"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.10202"
  },
  {
    "id": "arXiv:2210.10349",
    "title": "Museformer: Transformer with Fine- and Coarse-Grained Attention for  Music Generation",
    "abstract": "Comments: Accepted by the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Accepted by the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Botao Yu",
      "Peiling Lu",
      "Rui Wang",
      "Wei Hu",
      "Xu Tan",
      "Wei Ye",
      "Shikun Zhang",
      "Tao Qin",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.10349"
  },
  {
    "id": "arXiv:2210.11479",
    "title": "Exploitation of material consolidation trade-offs in a multi-tier  complex supply networks",
    "abstract": "Comments: (under review)",
    "descriptor": "\nComments: (under review)\n",
    "authors": [
      "Vinod Kumar Chauhan",
      "Muhannad Alomari",
      "James Arney",
      "Ajith Kumar Parlikad",
      "Alexandra Brintrup"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.11479"
  },
  {
    "id": "arXiv:2210.11498",
    "title": "Balanced Adversarial Training: Balancing Tradeoffs between Fickleness  and Obstinacy in NLP Models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Hannah Chen",
      "Yangfeng Ji",
      "David Evans"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11498"
  },
  {
    "id": "arXiv:2210.11714",
    "title": "Design a Sustainable Micro-mobility Future: Trends and Challenges in the  United States and European Union Using Natural Language Processing Techniques",
    "abstract": "Comments: 33 pages, 4 figures",
    "descriptor": "\nComments: 33 pages, 4 figures\n",
    "authors": [
      "Lilit Avetisyan",
      "Chengxin Zhang",
      "Sue Bai",
      "Ehsan Moradi Pari",
      "Fred Feng",
      "Shan Bao",
      "Feng Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.11714"
  },
  {
    "id": "arXiv:2210.11735",
    "title": "Extracted BERT Model Leaks More Information than You Think!",
    "abstract": "Comments: accepted to EMNLP2022 (oral). arXiv admin note: text overlap with arXiv:2105.10909",
    "descriptor": "\nComments: accepted to EMNLP2022 (oral). arXiv admin note: text overlap with arXiv:2105.10909\n",
    "authors": [
      "Xuanli He",
      "Chen Chen",
      "Lingjuan Lyu",
      "Qiongkai Xu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.11735"
  },
  {
    "id": "arXiv:2210.11905",
    "title": "Exploration of the Usage of Color Terms by Color-blind Participants in  Online Discussion Platforms",
    "abstract": "Comments: Accepted at EMNLP 2022 (main conference), 13 pages",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (main conference), 13 pages\n",
    "authors": [
      "Ella Rabinovich",
      "Boaz Carmeli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11905"
  },
  {
    "id": "arXiv:2210.11953",
    "title": "Real-time large-scale supplier order assignments across two-tiers of a  supply chain with penalty and dual-sourcing",
    "abstract": "Comments: (under review)",
    "descriptor": "\nComments: (under review)\n",
    "authors": [
      "Vinod Kumar Chauhan",
      "Stephen Mak",
      "Ajith Kumar Parlikad",
      "Muhannad Alomari",
      "Linus Casassa",
      "Alexandra Brintrup"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.11953"
  },
  {
    "id": "arXiv:2210.11968",
    "title": "CobNet: Cross Attention on Object and Background for Few-Shot  Segmentation",
    "abstract": "Comments: Accepted to ICPR2022",
    "descriptor": "\nComments: Accepted to ICPR2022\n",
    "authors": [
      "Haoyan Guan",
      "Michael Spratling"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.11968"
  },
  {
    "id": "arXiv:2210.12150",
    "title": "Formalizing Chemical Theory using the Lean Theorem Prover",
    "abstract": "Formalizing Chemical Theory using the Lean Theorem Prover",
    "descriptor": "",
    "authors": [
      "Maxwell P. Bobbin",
      "Samiha Sharlin",
      "Parivash Feyzishendi",
      "An Hong Dang",
      "Catherine M. Wraback",
      "Tyler R. Josephson"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.12150"
  },
  {
    "id": "arXiv:2210.12273",
    "title": "Graphemic Normalization of the Perso-Arabic Script",
    "abstract": "Comments: Pre-print to appear in the Proceedings of Grapholinguistics in the 21st Century (G21C), 2022. Telecom Paris, Palaiseau, France, June 8-10, 2022. 41 pages, 38 tables, 3 figures",
    "descriptor": "\nComments: Pre-print to appear in the Proceedings of Grapholinguistics in the 21st Century (G21C), 2022. Telecom Paris, Palaiseau, France, June 8-10, 2022. 41 pages, 38 tables, 3 figures\n",
    "authors": [
      "Raiomond Doctor",
      "Alexander Gutkin",
      "Cibu Johny",
      "Brian Roark",
      "Richard Sproat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12273"
  },
  {
    "id": "arXiv:2210.12378",
    "title": "Correcting Diverse Factual Errors in Abstractive Summarization via  Post-Editing and Language Model Infilling",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Vidhisha Balachandran",
      "Hannaneh Hajishirzi",
      "William W. Cohen",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12378"
  },
  {
    "id": "arXiv:2210.12415",
    "title": "ALT: Boosting Deep Learning Performance by Breaking the Wall between  Graph and Operator Level Optimizations",
    "abstract": "ALT: Boosting Deep Learning Performance by Breaking the Wall between  Graph and Operator Level Optimizations",
    "descriptor": "",
    "authors": [
      "Zhiying Xu",
      "Jiafan Xu",
      "Hongding Peng",
      "Wei Wang",
      "Xiaoliang Wang",
      "Haoran Wan",
      "Haipeng Dai",
      "Yixu Xu",
      "Hao Cheng",
      "Kun Wang",
      "Guihai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12415"
  },
  {
    "id": "arXiv:2210.12513",
    "title": "HAM: Hierarchical Attention Model with High Performance for 3D Visual  Grounding",
    "abstract": "Comments: Champion on ECCV 2022 ScanRefer Challenge",
    "descriptor": "\nComments: Champion on ECCV 2022 ScanRefer Challenge\n",
    "authors": [
      "Jiaming Chen",
      "Weixin Luo",
      "Xiaolin Wei",
      "Lin Ma",
      "Wei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12513"
  },
  {
    "id": "arXiv:2210.12770",
    "title": "On Cross-Domain Pre-Trained Language Models for Clinical Text Mining:  How Do They Perform on Data-Constrained Fine-Tuning?",
    "abstract": "On Cross-Domain Pre-Trained Language Models for Clinical Text Mining:  How Do They Perform on Data-Constrained Fine-Tuning?",
    "descriptor": "",
    "authors": [
      "Yuping Wu",
      "Lifeng Han",
      "Valerio Antonini",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12770"
  },
  {
    "id": "arXiv:2210.12774",
    "title": "Manifold Alignment with Label Information",
    "abstract": "Manifold Alignment with Label Information",
    "descriptor": "",
    "authors": [
      "Andres F. Duque",
      "Myriam Lizotte",
      "Guy Wolf",
      "Kevin R. Moon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12774"
  },
  {
    "id": "arXiv:2210.12786",
    "title": "When Can Transformers Ground and Compose: Insights from Compositional  Generalization Benchmarks",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ankur Sikarwar",
      "Arkil Patel",
      "Navin Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12786"
  },
  {
    "id": "arXiv:2210.12809",
    "title": "Data Augmentation for Automated Essay Scoring using Transformer Models",
    "abstract": "Comments: Accepted at ICCMST 2022",
    "descriptor": "\nComments: Accepted at ICCMST 2022\n",
    "authors": [
      "Kshitij Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12809"
  },
  {
    "id": "arXiv:2210.13109",
    "title": "WDA-Net: Weakly-Supervised Domain Adaptive Segmentation of Electron  Microscopy",
    "abstract": "Comments: Accepted by BIBM 2022: International Conference on Bioinformatics & Biomedicine",
    "descriptor": "\nComments: Accepted by BIBM 2022: International Conference on Bioinformatics & Biomedicine\n",
    "authors": [
      "Dafei Qiu",
      "Jiajin Yi",
      "Jialin Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13109"
  },
  {
    "id": "arXiv:2210.13210",
    "title": "Mutual Information Alleviates Hallucinations in Abstractive  Summarization",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Liam van der Poel",
      "Ryan Cotterell",
      "Clara Meister"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13210"
  },
  {
    "id": "arXiv:2210.13529",
    "title": "Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and  Refinement",
    "abstract": "Comments: Published at ECCV 2022",
    "descriptor": "\nComments: Published at ECCV 2022\n",
    "authors": [
      "Junuk Cha",
      "Muhammad Saqlain",
      "GeonU Kim",
      "Mingyu Shin",
      "Seungryul Baek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13529"
  },
  {
    "id": "arXiv:2210.13572",
    "title": "Sequential Recommendation with Auxiliary Item Relationships via  Multi-Relational Transformer",
    "abstract": "Comments: Accepted to BigData 2022. The code is at this https URL",
    "descriptor": "\nComments: Accepted to BigData 2022. The code is at this https URL\n",
    "authors": [
      "Ziwei Fan",
      "Zhiwei Liu",
      "Chen Wang",
      "Peijie Huang",
      "Hao Peng",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13572"
  },
  {
    "id": "arXiv:2210.13694",
    "title": "Worst-Case Adaptive Submodular Cover",
    "abstract": "Worst-Case Adaptive Submodular Cover",
    "descriptor": "",
    "authors": [
      "Jing Yuan",
      "Shaojie Tang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13694"
  },
  {
    "id": "arXiv:2210.13745",
    "title": "Numerical Analysis for Real-time Nonlinear Model Predictive Control of  Ethanol Steam Reformers",
    "abstract": "Numerical Analysis for Real-time Nonlinear Model Predictive Control of  Ethanol Steam Reformers",
    "descriptor": "",
    "authors": [
      "Robert Joseph George",
      "Xinwei Yu"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13745"
  },
  {
    "id": "arXiv:2210.13832",
    "title": "FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation",
    "abstract": "Comments: EMNLP-2022, 20 pages",
    "descriptor": "\nComments: EMNLP-2022, 20 pages\n",
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Qiquan Zhang",
      "Thomas Friedrichs",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13832"
  },
  {
    "id": "arXiv:2210.13931",
    "title": "An Optimal Stochastic Algorithm for Decentralized Nonconvex Finite-sum  Optimization",
    "abstract": "An Optimal Stochastic Algorithm for Decentralized Nonconvex Finite-sum  Optimization",
    "descriptor": "",
    "authors": [
      "Luo Luo",
      "Haishan Ye"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13931"
  },
  {
    "id": "arXiv:2210.14061",
    "title": "From exemplar to copy: the scribal appropriation of a Hadewijch  manuscript computationally explored",
    "abstract": "From exemplar to copy: the scribal appropriation of a Hadewijch  manuscript computationally explored",
    "descriptor": "",
    "authors": [
      "Wouter Haverals",
      "Mike Kestemont"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14061"
  },
  {
    "id": "arXiv:2210.14077",
    "title": "Eigen Memory Trees",
    "abstract": "Comments: corrected an author name; corrected title plurality",
    "descriptor": "\nComments: corrected an author name; corrected title plurality\n",
    "authors": [
      "Mark Rucker",
      "Jordan T. Ash",
      "John Langford",
      "Paul Mineiro",
      "Ida Momennejad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14077"
  },
  {
    "id": "arXiv:2210.14140",
    "title": "Contrastive Search Is What You Need For Neural Text Generation",
    "abstract": "Comments: 20 pages, 5 figures, 14 tables. Work in progress",
    "descriptor": "\nComments: 20 pages, 5 figures, 14 tables. Work in progress\n",
    "authors": [
      "Yixuan Su",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14140"
  },
  {
    "id": "arXiv:2210.14267",
    "title": "A Survey on 3D-aware Image Synthesis",
    "abstract": "Comments: Project: this https URL",
    "descriptor": "\nComments: Project: this https URL\n",
    "authors": [
      "Weihao Xia",
      "Jing-Hao Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.14267"
  },
  {
    "id": "arXiv:2210.14335",
    "title": "A Noise-aware Transpiler for Optimal Amplitude Amplification",
    "abstract": "A Noise-aware Transpiler for Optimal Amplitude Amplification",
    "descriptor": "",
    "authors": [
      "Debashis Ganguly",
      "Wonsun Ahn"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.14335"
  },
  {
    "id": "arXiv:2210.14799",
    "title": "Segmentation of Bruch's Membrane in retinal OCT with AMD using  anatomical priors and uncertainty quantification",
    "abstract": "Segmentation of Bruch's Membrane in retinal OCT with AMD using  anatomical priors and uncertainty quantification",
    "descriptor": "",
    "authors": [
      "Botond Fazekas",
      "Dmitrii Lachinov",
      "Guilherme Aresta",
      "Julia Mai",
      "Ursula Schmidt-Erfurth",
      "Hrvoje Bogunovic"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14799"
  },
  {
    "id": "arXiv:2210.14880",
    "title": "Integrated Sensing and Communication in Distributed Antenna Networks",
    "abstract": "Comments: 18 pages, 5 figures",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Dongfang Xu",
      "Ata Khalili",
      "Xianghao Yu",
      "Derrick Wing Kwan Ng",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14880"
  },
  {
    "id": "arXiv:2210.15067",
    "title": "arXivEdits: Understanding the Human Revision Process in Scientific  Writing",
    "abstract": "Comments: This paper has been accepted to EMNLP 2022",
    "descriptor": "\nComments: This paper has been accepted to EMNLP 2022\n",
    "authors": [
      "Chao Jiang",
      "Wei Xu",
      "Samuel Stevens"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.15067"
  },
  {
    "id": "arXiv:2210.15143",
    "title": "Audio Signal Enhancement with Learning from Positive and Unlabelled Data",
    "abstract": "Audio Signal Enhancement with Learning from Positive and Unlabelled Data",
    "descriptor": "",
    "authors": [
      "Nobutaka Ito",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.15143"
  },
  {
    "id": "arXiv:2210.15196",
    "title": "HRTF Field: Unifying Measured HRTF Magnitude Representation with Neural  Fields",
    "abstract": "Comments: 5 pages, submitted to ICASSP 2023",
    "descriptor": "\nComments: 5 pages, submitted to ICASSP 2023\n",
    "authors": [
      "You Zhang",
      "Yuxiang Wang",
      "Zhiyao Duan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Graphics (cs.GR)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.15196"
  },
  {
    "id": "arXiv:2210.15327",
    "title": "Towards Language-driven Scientific AI",
    "abstract": "Towards Language-driven Scientific AI",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.15327"
  },
  {
    "id": "arXiv:2210.15491",
    "title": "GaitMixer: Skeleton-based Gait Representation Learning via Wide-spectrum  Multi-axial Mixer",
    "abstract": "Comments: Submitted to ICASSP 2023",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Ekkasit Pinyoanuntapong",
      "Ayman Ali",
      "Pu Wang",
      "Minwoo Lee",
      "Chen Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.15491"
  },
  {
    "id": "arXiv:2210.15514",
    "title": "Point-Voxel Adaptive Feature Abstraction for Robust Point Cloud  Classification",
    "abstract": "Comments: Technical report",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Lifa Zhu",
      "Changwei Lin",
      "Chen Zheng",
      "Ninghua Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.15514"
  },
  {
    "id": "arXiv:2210.15533",
    "title": "Source-Filter HiFi-GAN: Fast and Pitch Controllable High-Fidelity Neural  Vocoder",
    "abstract": "Comments: Submitted to ICASSP 2023",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Reo Yoneyama",
      "Yi-Chiao Wu",
      "Tomoki Toda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.15533"
  },
  {
    "id": "arXiv:2210.15669",
    "title": "On Catalan Constant Continued Fractions",
    "abstract": "On Catalan Constant Continued Fractions",
    "descriptor": "",
    "authors": [
      "David Naccache",
      "Ofer Yifrach-Stav"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Discrete Mathematics (cs.DM)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2210.15669"
  },
  {
    "id": "arXiv:2210.15707",
    "title": "FedAudio: A Federated Learning Benchmark for Audio Tasks",
    "abstract": "FedAudio: A Federated Learning Benchmark for Audio Tasks",
    "descriptor": "",
    "authors": [
      "Tuo Zhang",
      "Tiantian Feng",
      "Samiul Alam",
      "Sunwoo Lee",
      "Mi Zhang",
      "Shrikanth S. Narayanan",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.15707"
  },
  {
    "id": "arXiv:2210.15973",
    "title": "A Deep Dive into VirusTotal: Characterizing and Clustering a Massive  File Feed",
    "abstract": "Comments: 16 pages, 4 figures",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Kevin van Liebergen",
      "Juan Caballero",
      "Platon Kotzias",
      "Chris Gates"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.15973"
  },
  {
    "id": "arXiv:2210.16031",
    "title": "UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal  Guidance",
    "abstract": "Comments: First Version, 16 pages",
    "descriptor": "\nComments: First Version, 16 pages\n",
    "authors": [
      "Wei Li",
      "Xue Xu",
      "Xinyan Xiao",
      "Jiachen Liu",
      "Hu Yang",
      "Guohao Li",
      "Zhanpeng Wang",
      "Zhifan Feng",
      "Qiaoqiao She",
      "Yajuan Lyu",
      "Hua Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.16031"
  },
  {
    "id": "arXiv:2210.16187",
    "title": "GRAND-assisted Optimal Modulation",
    "abstract": "Comments: Presented at IEEE Globecom 2022",
    "descriptor": "\nComments: Presented at IEEE Globecom 2022\n",
    "authors": [
      "Basak Ozaydin",
      "Muriel M\u00e9dard",
      "Ken Duffy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.16187"
  }
]