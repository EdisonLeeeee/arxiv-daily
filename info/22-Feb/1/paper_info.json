[
  {
    "id": "arXiv:2201.12340",
    "title": "A low-rank power iteration scheme for neutron transport criticality  problems",
    "abstract": "Computing effective eigenvalues for neutron transport often requires a fine\nnumerical resolution. The main challenge of such computations is the high\nmemory effort of classical solvers, which limits the accuracy of chosen\ndiscretizations. In this work, we derive a method for the computation of\neffective eigenvalues when the underlying solution has a low-rank structure.\nThis is accomplished by utilizing dynamical low-rank approximation (DLRA),\nwhich is an efficient strategy to derive time evolution equations for low-rank\nsolution representations. The main idea is to interpret the iterates of the\nclassical inverse power iteration as pseudo-time steps and apply the DLRA\nconcepts in this framework. In our numerical experiment, we demonstrate that\nour method significantly reduces memory requirements while achieving the\ndesired accuracy. Analytic investigations show that the proposed iteration\nscheme inherits the convergence speed of the inverse power iteration, at least\nfor a simplified setting.",
    "descriptor": "",
    "authors": [
      "Jonas Kusch",
      "Benjamin Whewell",
      "Ryan McClarren",
      "Martin Frank"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12340"
  },
  {
    "id": "arXiv:2201.12341",
    "title": "VarRCWA: An Adaptive High-Order Rigorous Coupled Wave Analysis Method",
    "abstract": "Semi-analytical methods, such as rigorous coupled wave analysis, have been\npivotal for numerical analysis of photonic structures. In comparison to other\nmethods, they offer much faster computation, especially for structures with\nconstant cross-sectional shapes (such as metasurface units). However, when the\ncross-sectional shape varies even mildly (such as a taper), existing\nsemi-analytical methods suffer from high computational cost. We show that the\nexisting methods can be viewed as a zeroth-order approximation with respect to\nthe structure's cross-sectional variation. We instead derive a high-order\nperturbative expansion with respect to the cross-sectional variation. Based on\nthis expansion, we propose a new semi-analytical method that is fast to compute\neven in presence of large cross-sectional shape variation. Furthermore, we\ndesign an algorithm that automatically discretizes the structure in a way that\nachieves a user specified accuracy level while at the same time reducing the\ncomputational cost.",
    "descriptor": "",
    "authors": [
      "Ziwei Zhu",
      "Changxi Zheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2201.12341"
  },
  {
    "id": "arXiv:2201.12342",
    "title": "Error-Correcting Neural Networks for Two-Dimensional Curvature  Computation in the Level-Set Method",
    "abstract": "We present an error-neural-modeling-based strategy for approximating\ntwo-dimensional curvature in the level-set method. Our main contribution is a\nredesigned hybrid solver (Larios-C\\'{a}rdenas and Gibou (2021)[1]) that relies\non numerical schemes to enable machine-learning operations on demand. In\nparticular, our routine features double predicting to harness curvature\nsymmetry invariance in favor of precision and stability. As in [1], the core of\nthis solver is a multilayer perceptron trained on circular- and\nsinusoidal-interface samples. Its role is to quantify the error in numerical\ncurvature approximations and emit corrected estimates for select grid vertices\nalong the free boundary. These corrections arise in response to preprocessed\ncontext level-set, curvature, and gradient data. To promote neural capacity, we\nhave adopted sample negative-curvature normalization, reorientation, and\nreflection-based augmentation. In the same manner, our system incorporates\ndimensionality reduction, well-balancedness, and regularization to minimize\noutlying effects. Our training approach is likewise scalable across mesh sizes.\nFor this purpose, we have introduced dimensionless parametrization and\nprobabilistic subsampling during data production. Together, all these elements\nhave improved the accuracy and efficiency of curvature calculations around\nunder-resolved regions. In most experiments, our strategy has outperformed the\nnumerical baseline at twice the number of redistancing steps while requiring\nonly a fraction of the cost.",
    "descriptor": "",
    "authors": [
      "Luis \u00c1ngel Larios-C\u00e1rdenas",
      "Fr\u00e9d\u00e9ric Gibou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12342"
  },
  {
    "id": "arXiv:2201.12343",
    "title": "Accelerated numerical algorithms for steady states of Gross-Pitaevskii  equations coupled with microwaves",
    "abstract": "We present two accelerated numerical algorithms for single-component and\nbinary Gross-Pitaevskii (GP) equations coupled with microwaves (electromagnetic\nfields) in steady state. One is based on a normalized gradient flow\nformulation, called the ASGF method, while the other on a perturbed, projected\nconjugate gradient approach for the nonlinear constrained optimization, called\nthe PPNCG method. The coupled GP equations are nonlocal in space, describing\npseudo-spinor Bose-Einstein condensates (BECs) interacting with an\nelectromagnetic field. Our interest in this study is to develop efficient,\niterative numerical methods for steady symmetric and central vortex states of\nthe nonlocal GP equation systems. In the algorithms, the GP equations are\ndiscretized by a Legendre-Galerkin spectral method in a polar coordinate in\ntwo-dimensional (2D) space. The new algorithms are shown to outperform the\nexisting ones through a host of benchmark examples, among which the PPNCG\nmethod performs the best. Additional numerical simulations of the central\nvortex states are provided to demonstrate the usefulness and efficiency of the\nnew algorithms.",
    "descriptor": "",
    "authors": [
      "Di Wang",
      "Qi Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12343"
  },
  {
    "id": "arXiv:2201.12344",
    "title": "Socioergonomics: A few clarifications on the  Technology-Organizations-People Tryptic",
    "abstract": "This position paper introduces and coins the term socioergonomics, considered\nas a sociological, ontological, and methodological support to human systems\nintegration (HSI). It describes the evolution of ergonomics from early\nphysiological to psychological to contemporary social sciences approaches\nsupporting Industry 4.0 sociotechnical systems engineering. It presents a\nTechnology Readiness Levels (TRLs) extension to Organizational Readiness Levels\n(ORLs) and a departure toward a socioergonomics approach that includes systemic\nproperties such as flexibility, separability, and emergent social facts.",
    "descriptor": "\nComments: INCOSE HSI2021 International Conference Proceedings, 2021, San Diego, California, USA (virtual), United States\n",
    "authors": [
      "Guy Andre Boy"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.12344"
  },
  {
    "id": "arXiv:2201.12345",
    "title": "On Stability and Convergence of Three-layer Semi-discrete Scheme for an  Abstract Analogue of the Ball Integro-differential Equation",
    "abstract": "Cauchy problem for second-order nonlinear evolution equation is considered.\nThis equation represents the abstract generalization of the Ball\nintegro-differential equation. The general nonlinear case concerning terms of\nthe equation which include a square of a norm of a gradient is considered.\nThree-layer semi-discrete scheme is proposed for numerical computations. In\nthis scheme, approximation of nonlinear terms that are dependent on the\ngradient is done using integral averaging. Here is proved that solution of the\nnonlinear discrete problem and its corresponding first-order difference is\nuniformly bounded. For the solution of corresponding linearized discrete\nproblem high-order, a priori estimation is obtained using two-variable\nChebyshev polynomials. Based on this estimation stability of the nonlinear\ndiscrete problem is shown. For smooth solutions, it is obtained estimation for\nthe error of the approximate solution. An approximate solution for each time\nstep we apply the iteration method. The convergence of the iteration method is\nproved.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Jemal Rogava",
      "Mikheil Tsiklauri",
      "Zurab Vashakidze"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)",
      "Analysis of PDEs (math.AP)",
      "Functional Analysis (math.FA)"
    ],
    "url": "https://arxiv.org/abs/2201.12345"
  },
  {
    "id": "arXiv:2201.12346",
    "title": "DiriNet: A network to estimate the spatial and spectral degradation  functions",
    "abstract": "The spatial and spectral degradation functions are critical to hyper- and\nmulti-spectral image fusion. However, few work has been payed on the estimation\nof the degradation functions. To learn the spatial response function and the\npoint spread function from the image pairs to be fused, we propose a Dirichlet\nnetwork, where both functions are properly constrained. Specifically, the\nspatial response function is constrained with positivity, while the Dirichlet\ndistribution along with a total variation is imposed on the point spread\nfunction. To the best of our knowledge, the neural netwrok and the Dirichlet\nregularization are exclusively investigated, for the first time, to estimate\nthe degradation functions. Both image degradation and fusion experiments\ndemonstrate the effectiveness and superiority of the proposed Dirichlet\nnetwork.",
    "descriptor": "",
    "authors": [
      "Ting Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12346"
  },
  {
    "id": "arXiv:2201.12347",
    "title": "Adversarial Robustness in Deep Learning: Attacks on Fragile Neurons",
    "abstract": "We identify fragile and robust neurons of deep learning architectures using\nnodal dropouts of the first convolutional layer. Using an adversarial targeting\nalgorithm, we correlate these neurons with the distribution of adversarial\nattacks on the network. Adversarial robustness of neural networks has gained\nsignificant attention in recent times and highlights intrinsic weaknesses of\ndeep learning networks against carefully constructed distortion applied to\ninput images. In this paper, we evaluate the robustness of state-of-the-art\nimage classification models trained on the MNIST and CIFAR10 datasets against\nthe fast gradient sign method attack, a simple yet effective method of\ndeceiving neural networks. Our method identifies the specific neurons of a\nnetwork that are most affected by the adversarial attack being applied. We,\ntherefore, propose to make fragile neurons more robust against these attacks by\ncompressing features within robust neurons and amplifying the fragile neurons\nproportionally.",
    "descriptor": "",
    "authors": [
      "Chandresh Pravin",
      "Ivan Martino",
      "Giuseppe Nicosia",
      "Varun Ojha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12347"
  },
  {
    "id": "arXiv:2201.12351",
    "title": "Low-rank features based double transformation matrices learning for  image classification",
    "abstract": "Linear regression is a supervised method that has been widely used in\nclassification tasks. In order to apply linear regression to classification\ntasks, a technique for relaxing regression targets was proposed. However,\nmethods based on this technique ignore the pressure on a single transformation\nmatrix due to the complex information contained in the data. A single\ntransformation matrix in this case is too strict to provide a flexible\nprojection, thus it is necessary to adopt relaxation on transformation matrix.\nThis paper proposes a double transformation matrices learning method based on\nlatent low-rank feature extraction. The core idea is to use double\ntransformation matrices for relaxation, and jointly projecting the learned\nprincipal and salient features from two directions into the label space, which\ncan share the pressure of a single transformation matrix. Firstly, the low-rank\nfeatures are learned by the latent low rank representation (LatLRR) method\nwhich processes the original data from two directions. In this process, sparse\nnoise is also separated, which alleviates its interference on projection\nlearning to some extent. Then, two transformation matrices are introduced to\nprocess the two features separately, and the information useful for the\nclassification is extracted. Finally, the two transformation matrices can be\neasily obtained by alternate optimization methods. Through such processing,\neven when a large amount of redundant information is contained in samples, our\nmethod can also obtain projection results that are easy to classify.\nExperiments on multiple data sets demonstrate the effectiveness of our approach\nfor classification, especially for complex scenarios.",
    "descriptor": "",
    "authors": [
      "Yu-Hong Cai",
      "Xiao-Jun Wu",
      "Zhe Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12351"
  },
  {
    "id": "arXiv:2201.12352",
    "title": "Automatic Audio Captioning using Attention weighted Event based  Embeddings",
    "abstract": "Automatic Audio Captioning (AAC) refers to the task of translating audio into\na natural language that describes the audio events, source of the events and\ntheir relationships. The limited samples in AAC datasets at present, has set up\na trend to incorporate transfer learning with Audio Event Detection (AED) as a\nparent task. Towards this direction, in this paper, we propose an\nencoder-decoder architecture with light-weight (i.e. with lesser learnable\nparameters) Bi-LSTM recurrent layers for AAC and compare the performance of two\nstate-of-the-art pre-trained AED models as embedding extractors. Our results\nshow that an efficient AED based embedding extractor combined with temporal\nattention and augmentation techniques is able to surpass existing literature\nwith computationally intensive architectures. Further, we provide evidence of\nthe ability of the non-uniform attention weighted encoding generated as a part\nof our model to facilitate the decoder glance over specific sections of the\naudio while generating each token.",
    "descriptor": "",
    "authors": [
      "Swapnil Bhosale",
      "Rupayan Chakraborty",
      "Sunil Kumar Kopparapu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.12352"
  },
  {
    "id": "arXiv:2201.12354",
    "title": "Discovering Nonlinear PDEs from Scarce Data with Physics-encoded  Learning",
    "abstract": "There have been growing interests in leveraging experimental measurements to\ndiscover the underlying partial differential equations (PDEs) that govern\ncomplex physical phenomena. Although past research attempts have achieved great\nsuccess in data-driven PDE discovery, the robustness of the existing methods\ncannot be guaranteed when dealing with low-quality measurement data. To\novercome this challenge, we propose a novel physics-encoded discrete learning\nframework for discovering spatiotemporal PDEs from scarce and noisy data. The\ngeneral idea is to (1) firstly introduce a novel deep convolutional-recurrent\nnetwork, which can encode prior physics knowledge (e.g., known PDE terms,\nassumed PDE structure, initial/boundary conditions, etc.) while remaining\nflexible on representation capability, to accurately reconstruct high-fidelity\ndata, and (2) perform sparse regression with the reconstructed data to identify\nthe explicit form of the governing PDEs. We validate our method on three\nnonlinear PDE systems. The effectiveness and superiority of the proposed method\nover baseline models are demonstrated.",
    "descriptor": "",
    "authors": [
      "Chengping Rao",
      "Pu Ren",
      "Yang Liu",
      "Hao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.12354"
  },
  {
    "id": "arXiv:2201.12355",
    "title": "Adversarial Decisions on Complex Dynamical Systems using Game Theory",
    "abstract": "We apply computational Game Theory to a unification of physics-based models\nthat represent decision-making across a number of agents within both\ncooperative and competitive processes. Here the competitors try to both\npositively influence their own returns, while negatively affecting those of\ntheir competitors. Modelling these interactions with the so-called\nBoyd-Kuramoto-Lanchester (BKL) complex dynamical system model yields results\nthat can be applied to business, gaming and security contexts. This paper\nstudies a class of decision problems on the BKL model, where a large set of\ncoupled, switching dynamical systems are analysed using game-theoretic methods.\nDue to their size, the computational cost of solving these BKL games becomes\nthe dominant factor in the solution process. To resolve this, we introduce a\nnovel Nash Dominant solver, which is both numerically efficient and exact. The\nperformance of this new solution technique is compared to traditional exact\nsolvers, which traverse the entire game tree, as well as to approximate solvers\nsuch as Myopic and Monte Carlo Tree Search (MCTS). These techniques are\nassessed, and used to gain insights into both nonlinear dynamical systems and\nstrategic decision making in adversarial environments.",
    "descriptor": "",
    "authors": [
      "Andrew C. Cullen",
      "Tansu Alpcan",
      "Alexander C. Kalloniatis"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2201.12355"
  },
  {
    "id": "arXiv:2201.12356",
    "title": "Adversarial Examples for Good: Adversarial Examples Guided Imbalanced  Learning",
    "abstract": "Adversarial examples are inputs for machine learning models that have been\ndesigned by attackers to cause the model to make mistakes. In this paper, we\ndemonstrate that adversarial examples can also be utilized for good to improve\nthe performance of imbalanced learning. We provide a new perspective on how to\ndeal with imbalanced data: adjust the biased decision boundary by training with\nGuiding Adversarial Examples (GAEs). Our method can effectively increase the\naccuracy of minority classes while sacrificing little accuracy on majority\nclasses. We empirically show, on several benchmark datasets, our proposed\nmethod is comparable to the state-of-the-art method. To our best knowledge, we\nare the first to deal with imbalanced learning with adversarial examples.",
    "descriptor": "\nComments: 5 pages, 6 figures\n",
    "authors": [
      "Jie Zhang",
      "Lei Zhang",
      "Gang Li",
      "Chao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12356"
  },
  {
    "id": "arXiv:2201.12358",
    "title": "Detecting Electric Vehicle Battery Failure via Dynamic-VAE",
    "abstract": "In this note, we describe a battery failure detection pipeline backed up by\ndeep learning models. We first introduce a large-scale Electric vehicle (EV)\nbattery dataset including cleaned battery-charging data from hundreds of\nvehicles. We then formulate battery failure detection as an outlier detection\nproblem, and propose a new algorithm named Dynamic-VAE based on dynamic system\nand variational autoencoders. We validate the performance of our proposed\nalgorithm against several baselines on our released dataset and demonstrated\nthe effectiveness of Dynamic-VAE.",
    "descriptor": "\nComments: 5 pages, 1 figures\n",
    "authors": [
      "Haowei He",
      "Jingzhao Zhang",
      "Yanan Wang",
      "Shaobo Huang",
      "Chen Wang",
      "Yang Zhang",
      "Dongxu Guo",
      "Guannan He",
      "Minggao Ouyang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12358"
  },
  {
    "id": "arXiv:2201.12360",
    "title": "Variational Neural Cellular Automata",
    "abstract": "In nature, the process of cellular growth and differentiation has lead to an\namazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades,\nand orcas are all created by the same generative process. Inspired by the\nincredible diversity of this biological generative process, we propose a\ngenerative model, the Variational Neural Cellular Automata (VNCA), which is\nloosely inspired by the biological processes of cellular growth and\ndifferentiation. Unlike previous related works, the VNCA is a proper\nprobabilistic generative model, and we evaluate it according to best practices.\nWe find that the VNCA learns to reconstruct samples well and that despite its\nrelatively few parameters and simple local-only communication, the VNCA can\nlearn to generate a large variety of output from information encoded in a\ncommon vector format. While there is a significant gap to the current\nstate-of-the-art in terms of generative modeling performance, we show that the\nVNCA can learn a purely self-organizing generative process of data.\nAdditionally, we show that the VNCA can learn a distribution of stable\nattractors that can recover from significant damage.",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Rasmus Berg Palm",
      "Miguel Gonz\u00e1lez-Duque",
      "Shyam Sudhakaran",
      "Sebastian Risi"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.12360"
  },
  {
    "id": "arXiv:2201.12374",
    "title": "On the Kolmogorov Complexity of Binary Classifiers",
    "abstract": "We provide tight upper and lower bounds on the expected minimum Kolmogorov\ncomplexity of binary classifiers that are consistent with labeled samples. The\nexpected size is not more than complexity of the target concept plus the\nconditional entropy of the labels given the sample.",
    "descriptor": "",
    "authors": [
      "Samuel Epstein"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2201.12374"
  },
  {
    "id": "arXiv:2201.12376",
    "title": "Probably Reasonable Search in eDiscovery",
    "abstract": "In eDiscovery, a party to a lawsuit or similar action must search through\navailable information to identify those documents and files that are relevant\nto the suit. Search efforts tend to identify less than 100% of the relevant\ndocuments and courts are frequently asked to adjudicate whether the search\neffort has been reasonable, or whether additional effort to find more of the\nrelevant documents is justified. This article provides a method for estimating\nthe probability that significant additional information will be found from\nextended effort. Modeling and two data sets indicate that the probability that\nfacts/topics exist among the so-far unidentified documents that have not been\nobserved in the identified documents is low for even moderate levels of Recall.",
    "descriptor": "",
    "authors": [
      "Herbert L. Roitblat"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.12376"
  },
  {
    "id": "arXiv:2201.12380",
    "title": "Explaining Graph-level Predictions with Communication Structure-Aware  Cooperative Games",
    "abstract": "Explaining predictions made by machine learning models is important and have\nattracted an increased interest. The Shapley value from cooperative game theory\nhas been proposed as a prime approach to compute feature importances towards\npredictions, especially for images, text, tabular data, and recently graph\nneural networks (GNNs) on graphs. In this work, we revisit the appropriateness\nof the Shapley value for graph explanation, where the task is to identify the\nmost important subgraph and constituent nodes for graph-level predictions. We\npurport that the Shapley value is a no-ideal choice for graph data because it\nis by definition not structure-aware. We propose a Graph Structure-aware\neXplanation (GStarX) method to leverage the critical graph structure\ninformation to improve the explanation. Specifically, we propose a scoring\nfunction based on a new structure-aware value from the cooperative game theory\ncalled the HN value. When used to score node importance, the HN value utilizes\ngraph structures to attribute cooperation surplus between neighbor nodes,\nresembling message passing in GNNs, so that node importance scores reflect not\nonly the node feature importance, but also the structural roles. We demonstrate\nthat GstarX produces qualitatively more intuitive explanations, and\nquantitatively improves over strong baselines on chemical graph property\nprediction and text graph sentiment classification.",
    "descriptor": "",
    "authors": [
      "Shichang Zhang",
      "Neil Shah",
      "Yozen Liu",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12380"
  },
  {
    "id": "arXiv:2201.12382",
    "title": "Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's  Progressive Matrices",
    "abstract": "Abstract visual reasoning (AVR) domain encompasses problems solving which\nrequires the ability to reason about relations among entities present in a\ngiven scene. While humans, generally, solve AVR tasks in a ``natural'' way,\neven without prior experience, this type of problems has proven difficult for\ncurrent machine learning systems. The paper summarises recent progress in\napplying deep learning methods to solving AVR problems, as a proxy for studying\nmachine intelligence. We focus on the most common type of AVR tasks -- the\nRaven's Progressive Matrices (RPMs) -- and provide a comprehensive review of\nthe learning methods and deep neural models applied to solve RPMs, as well as,\nthe RPM benchmark sets. Performance analysis of the state-of-the-art approaches\nto solving RPMs leads to formulation of certain insights and remarks on the\ncurrent and future trends in this area. We conclude the paper by demonstrating\nhow real-world problems can benefit from the discoveries of RPM studies.",
    "descriptor": "",
    "authors": [
      "Miko\u0142aj Ma\u0142ki\u0144ski",
      "Jacek Ma\u0144dziuk"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12382"
  },
  {
    "id": "arXiv:2201.12383",
    "title": "Bounding Training Data Reconstruction in Private (Deep) Learning",
    "abstract": "Differential privacy is widely accepted as the de facto method for preventing\ndata leakage in ML, and conventional wisdom suggests that it offers strong\nprotection against privacy attacks. However, existing semantic guarantees for\nDP focus on membership inference, which may overestimate the adversary's\ncapabilities and is not applicable when membership status itself is\nnon-sensitive. In this paper, we derive the first semantic guarantees for DP\nmechanisms against training data reconstruction attacks under a formal threat\nmodel. We show that two distinct privacy accounting methods -- Renyi\ndifferential privacy and Fisher information leakage -- both offer strong\nsemantic protection against data reconstruction attacks.",
    "descriptor": "",
    "authors": [
      "Chuan Guo",
      "Brian Karrer",
      "Kamalika Chaudhuri",
      "Laurens van der Maaten"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12383"
  },
  {
    "id": "arXiv:2201.12384",
    "title": "Developing a Machine-Learning Algorithm to Diagnose Age-Related Macular  Degeneration",
    "abstract": "Today, more than 12 million people over the age of 40 suffer from ocular\ndiseases. Most commonly, older patients are susceptible to age related macular\ndegeneration, an eye disease that causes blurring of the central vision due to\nthe deterioration of the retina. The former can only be detected through\ncomplex and expensive imaging software, markedly a visual field test; this\nleaves a significant population with untreated eye disease and holds them at\nrisk for complete vision loss. The use of machine learning algorithms has been\nproposed for treating eye disease. However, the development of these models is\nlimited by a lack of understanding regarding appropriate model and training\nparameters to maximize model performance. In our study, we address these points\nby generating 6 models, each with a learning rate of 1 * 10^n where n is 0, -1,\n-2, ... -6, and calculated a f1 score for each of the models. Our analysis\nshows that sample imbalance is a key challenge in training of machine learning\nmodels and can result in deceptive improvements in training cost which does not\ntranslate to true improvements in model predictive performance. Considering the\nwide ranging impact of the disease and its adverse effects, we developed a\nmachine learning algorithm to treat the same. We trained our model on varying\neye disease datasets consisting of over 5000 patients, and the pictures of\ntheir infected eyes. In the future, we hope this model is used extensively,\nespecially in areas that are under-resourced, to better diagnose eye disease\nand improve well being for humanity.",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "Ananya Dua",
      "Pham Hung Minh",
      "Sajid Fahmid",
      "Shikhar Gupta",
      "Sophia Zheng",
      "Vanessa Moyo",
      "Yanran Elisa Xue"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12384"
  },
  {
    "id": "arXiv:2201.12385",
    "title": "A deep Q-learning method for optimizing visual search strategies in  backgrounds of dynamic noise",
    "abstract": "Humans process visual information with varying resolution (foveated visual\nsystem) and explore images by orienting through eye movements the\nhigh-resolution fovea to points of interest. The Bayesian ideal searcher (IS)\nthat employs complete knowledge of task-relevant information optimizes eye\nmovement strategy and achieves the optimal search performance. The IS can be\nemployed as an important tool to evaluate the optimality of human eye\nmovements, and potentially provide guidance to improve human observer visual\nsearch strategies. Najemnik and Geisler (2005) derived an IS for backgrounds of\nspatial 1/f noise. The corresponding template responses follow Gaussian\ndistributions and the optimal search strategy can be analytically determined.\nHowever, the computation of the IS can be intractable when considering more\nrealistic and complex backgrounds such as medical images. Modern reinforcement\nlearning methods, successfully applied to obtain optimal policy for a variety\nof tasks, do not require complete knowledge of the background generating\nfunctions and can be potentially applied to anatomical backgrounds. An\nimportant first step is to validate the optimality of the reinforcement\nlearning method. In this study, we investigate the ability of a reinforcement\nlearning method that employs Q-network to approximate the IS. We demonstrate\nthat the search strategy corresponding to the Q-network is consistent with the\nIS search strategy. The findings show the potential of the reinforcement\nlearning with Q-network approach to estimate optimal eye movement planning with\nreal anatomical backgrounds.",
    "descriptor": "\nComments: SPIE Medical Imaging 2022\n",
    "authors": [
      "Weimin Zhou",
      "Miguel P. Eckstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12385"
  },
  {
    "id": "arXiv:2201.12386",
    "title": "Few-shot Unsupervised Domain Adaptation for Multi-modal Cardiac Image  Segmentation",
    "abstract": "Unsupervised domain adaptation (UDA) methods intend to reduce the gap between\nsource and target domains by using unlabeled target domain and labeled source\ndomain data, however, in the medical domain, target domain data may not always\nbe easily available, and acquiring new samples is generally time-consuming.\nThis restricts the development of UDA methods for new domains. In this paper,\nwe explore the potential of UDA in a more challenging while realistic scenario\nwhere only one unlabeled target patient sample is available. We call it\nFew-shot Unsupervised Domain adaptation (FUDA). We first generate target-style\nimages from source images and explore diverse target styles from a single\ntarget patient with Random Adaptive Instance Normalization (RAIN). Then, a\nsegmentation network is trained in a supervised manner with the generated\ntarget images. Our experiments demonstrate that FUDA improves the segmentation\nperformance by 0.33 of Dice score on the target domain compared with the\nbaseline, and it also gives 0.28 of Dice score improvement in a more rigorous\none-shot setting. Our code is available at\n\\url{https://github.com/MingxuanGu/Few-shot-UDA}.",
    "descriptor": "\nComments: Accepted t0 BVM2022\n",
    "authors": [
      "Mingxuan Gu",
      "Sulaiman Vesal",
      "Ronak Kosti",
      "Andreas Maier"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12386"
  },
  {
    "id": "arXiv:2201.12391",
    "title": "Efficient optimization-based quadrature for variational discretization  of nonlocal problems",
    "abstract": "Casting nonlocal problems in variational form and discretizing them with the\nfinite element (FE) method facilitates the use of nonlocal vector calculus to\nprove well-posedeness, convergence, and stability of such schemes. Employing an\nFE method also facilitates meshing of complicated domain geometries and\ncoupling with FE methods for local problems. However, nonlocal weak problems\ninvolve the computation of a double-integral, which is computationally\nexpensive and presents several challenges. In particular, the inner integral of\nthe variational form associated with the stiffness matrix is defined over the\nintersections of FE mesh elements with a ball of radius $\\delta$, where\n$\\delta$ is the range of nonlocal interaction. Identifying and parameterizing\nthese intersections is a nontrivial computational geometry problem. In this\nwork, we propose a quadrature technique where the inner integration is\nperformed using quadrature points distributed over the full ball, without\nregard for how it intersects elements, and weights are computed based on the\ngeneralized moving least squares method. Thus, as opposed to all previously\nemployed methods, our technique does not require element-by-element integration\nand fully circumvents the computation of element-ball intersections. This paper\nconsiders one- and two-dimensional implementations of piecewise linear\ncontinuous FE approximations, focusing on the case where the element size h and\nthe nonlocal radius $\\delta$ are proportional, as is typical of practical\ncomputations. When boundary conditions are treated carefully and the outer\nintegral of the variational form is computed accurately, the proposed method is\nasymptotically compatible in the limit of $h \\sim \\delta \\to 0$, featuring at\nleast first-order convergence in L^2 for all dimensions, using both uniform and\nnonuniform grids.",
    "descriptor": "\nComments: 59 pages, 21 figures\n",
    "authors": [
      "Marco Pasetto",
      "Zhaoxiang Shen",
      "Marta D'Elia",
      "Xiaochuan Tian",
      "Nathaniel Trask",
      "David Kamensky"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2201.12391"
  },
  {
    "id": "arXiv:2201.12393",
    "title": "Better Algorithms for Online Bin Stretching via Computer Search",
    "abstract": "We present an algorithm for computing upper bounds for the Online Bin\nStretching Problem with a small number of bins and the resulting upper bounds\nfor 4, 5 and 6 bins. This both demonstrates the possibility of using computer\nsearch for upper bounds on a fundamentally real-valued online problem and\nimproves upon the best bounds know so far, some of which have remained\nunchanged since 2001.",
    "descriptor": "",
    "authors": [
      "Matej Lieskovsk\u00fd"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12393"
  },
  {
    "id": "arXiv:2201.12394",
    "title": "Constellation: An Edge-Based Semantic Runtime System for Internet of  Things Applications",
    "abstract": "With the global Internet of Things IoT market size predicted to grow to over\n1 trillion dollars in the next 5 years, many large corporations are scrambling\nto solidify their product line as the defacto device suite for consumers. This\nhas led to each corporation developing their devices in a siloed environment\nwith unique protocols and runtime frameworks that explicitly exclude the\nability to work with the competitions devices. This development silo has\ncreated problems with programming complexity for application developers as well\nas concurrency and scalability limitations for applications that involve a\nnetwork of IoT devices. The Constellation project is a distributed IoT runtime\nsystem that attempts to address these challenges by creating an operating\nsystem layer that decouples applications from devices. This layer provides\nmechanisms designed to allow applications to interface with an underlying\nsubstrate of IoT devices while abstracting away the complexities of application\nconcurrency, device interoperability, and system scalability. This paper\nprovides an overview of the Constellation system as well as details four new\nproject expansions to improve system scalability.",
    "descriptor": "\nComments: 15 pages, 11 figures, 2 tables\n",
    "authors": [
      "Mitch Terrell",
      "Yixuan Wang",
      "Matt Dorow",
      "Soumya Agrawal",
      "Bhaargav Sriraman",
      "Zach Leidall",
      "Abhishek Chandra",
      "Jon Weissman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12394"
  },
  {
    "id": "arXiv:2201.12395",
    "title": "Competitive Algorithms and Reinforcement Learning for NOMA in IoT  Networks",
    "abstract": "This paper studies the problem of massive Internet of things (IoT) access in\nbeyond fifth generation (B5G) networks using non-orthogonal multiple access\n(NOMA) technique. The problem involves massive IoT devices grouping and power\nallocation in order to respect the low latency as well as the limited operating\nenergy of the IoT devices. The considered objective function, maximizing the\nnumber of successfully received IoT packets, is different from the classical\nsum-rate-related objective functions. The problem is first divided into\nmultiple NOMA grouping subproblems. Then, using competitive analysis, an\nefficient online competitive algorithm (CA) is proposed to solve each\nsubproblem. Next, to solve the power allocation problem, we propose a new\nreinforcement learning (RL) framework in which a RL agent learns to use the CA\nas a black box and combines the obtained solutions to each subproblem to\ndetermine the power allocation for each NOMA group. Our simulations results\nreveal that the proposed innovative RL framework outperforms deep-Q-learning\nmethods and is close-to-optimal.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2002.07957\n",
    "authors": [
      "Zoubeir Mlika",
      "Soumaya Cherkaoui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12395"
  },
  {
    "id": "arXiv:2201.12403",
    "title": "Planning and Learning with Adaptive Lookahead",
    "abstract": "The classical Policy Iteration (PI) algorithm alternates between greedy\none-step policy improvement and policy evaluation. Recent literature shows that\nmulti-step lookahead policy improvement leads to a better convergence rate at\nthe expense of increased complexity per iteration. However, prior to running\nthe algorithm, one cannot tell what is the best fixed lookahead horizon.\nMoreover, per a given run, using a lookahead of horizon larger than one is\noften wasteful. In this work, we propose for the first time to dynamically\nadapt the multi-step lookahead horizon as a function of the state and of the\nvalue estimate. We devise two PI variants and analyze the trade-off between\niteration count and computational complexity per iteration. The first variant\ntakes the desired contraction factor as the objective and minimizes the\nper-iteration complexity. The second variant takes as input the computational\ncomplexity per iteration and minimizes the overall contraction factor. We then\ndevise a corresponding DQN-based algorithm with an adaptive tree search\nhorizon. We also include a novel enhancement for on-policy learning: per-depth\nvalue function estimator. Lastly, we demonstrate the efficacy of our adaptive\nlookahead method in a maze environment and in Atari.",
    "descriptor": "",
    "authors": [
      "Aviv Rosenberg",
      "Assaf Hallak",
      "Shie Mannor",
      "Gal Chechik",
      "Gal Dalal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12403"
  },
  {
    "id": "arXiv:2201.12406",
    "title": "Syfer: Neural Obfuscation for Private Data Release",
    "abstract": "Balancing privacy and predictive utility remains a central challenge for\nmachine learning in healthcare. In this paper, we develop Syfer, a neural\nobfuscation method to protect against re-identification attacks. Syfer composes\ntrained layers with random neural networks to encode the original data (e.g.\nX-rays) while maintaining the ability to predict diagnoses from the encoded\ndata. The randomness in the encoder acts as the private key for the data owner.\nWe quantify privacy as the number of attacker guesses required to re-identify a\nsingle image (guesswork). We propose a contrastive learning algorithm to\nestimate guesswork. We show empirically that differentially private methods,\nsuch as DP-Image, obtain privacy at a significant loss of utility. In contrast,\nSyfer achieves strong privacy while preserving utility. For example, X-ray\nclassifiers built with DP-image, Syfer, and original data achieve average AUCs\nof 0.53, 0.78, and 0.86, respectively.",
    "descriptor": "",
    "authors": [
      "Adam Yala",
      "Victor Quach",
      "Homa Esfahanizadeh",
      "Rafael G. L. D'Oliveira",
      "Ken R. Duffy",
      "Muriel M\u00e9dard",
      "Tommi S. Jaakkola",
      "Regina Barzilay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12406"
  },
  {
    "id": "arXiv:2201.12407",
    "title": "Schema-Free Dependency Parsing via Sequence Generation",
    "abstract": "Dependency parsing aims to extract syntactic dependency structure or semantic\ndependency structure for sentences. Existing methods suffer the drawbacks of\nlacking universality or highly relying on the auxiliary decoder. To remedy\nthese drawbacks, we propose to achieve universal and schema-free Dependency\nParsing (DP) via Sequence Generation (SG) DPSG by utilizing only the\npre-trained language model (PLM) without any auxiliary structures or parsing\nalgorithms. We first explore different serialization designing strategies for\nconverting parsing structures into sequences. Then we design dependency units\nand concatenate these units into the sequence for DPSG. Thanks to the high\nflexibility of the sequence generation, our DPSG can achieve both syntactic DP\nand semantic DP using a single model. By concatenating the prefix to indicate\nthe specific schema with the sequence, our DPSG can even accomplish\nmulti-schemata parsing. The effectiveness of our DPSG is demonstrated by the\nexperiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and\nSemEval16. DPSG achieves comparable results with the first-tier methods on all\nthe benchmarks and even the state-of-the-art (SOTA) performance in CODT and\nSemEval16. This paper demonstrates our DPSG has the potential to be a new\nparsing paradigm. We will release our codes upon acceptance.",
    "descriptor": "",
    "authors": [
      "Boda Lin",
      "Zijun Yao",
      "Jiaxin Shi",
      "Shulin Cao",
      "Binghao Tang",
      "Si Li",
      "Yong Luo",
      "Juanzi Li",
      "Lei Hou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12407"
  },
  {
    "id": "arXiv:2201.12408",
    "title": "Networked Restless Multi-Armed Bandits for Mobile Interventions",
    "abstract": "Motivated by a broad class of mobile intervention problems, we propose and\nstudy restless multi-armed bandits (RMABs) with network effects. In our model,\narms are partially recharging and connected through a graph, so that pulling\none arm also improves the state of neighboring arms, significantly extending\nthe previously studied setting of fully recharging bandits with no network\neffects. In mobile interventions, network effects may arise due to regular\npopulation movements (such as commuting between home and work). We show that\nnetwork effects in RMABs induce strong reward coupling that is not accounted\nfor by existing solution methods. We propose a new solution approach for\nnetworked RMABs, exploiting concavity properties which arise under natural\nassumptions on the structure of intervention effects. We provide sufficient\nconditions for optimality of our approach in idealized settings and demonstrate\nthat it empirically outperforms state-of-the art baselines in three mobile\nintervention domains using real-world graphs.",
    "descriptor": "",
    "authors": [
      "Han-Ching Ou",
      "Christoph Siebenbrunner",
      "Jackson Killian",
      "Meredith B Brooks",
      "David Kempe",
      "Yevgeniy Vorobeychik",
      "Milind Tambe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12408"
  },
  {
    "id": "arXiv:2201.12409",
    "title": "A Unified Approach to Entity-Centric Context Tracking in Social  Conversations",
    "abstract": "In human-human conversations, Context Tracking deals with identifying\nimportant entities and keeping track of their properties and relationships.\nThis is a challenging problem that encompasses several subtasks such as slot\ntagging, coreference resolution, resolving plural mentions and entity linking.\nWe approach this problem as an end-to-end modeling task where the\nconversational context is represented by an entity repository containing the\nentity references mentioned so far, their properties and the relationships\nbetween them. The repository is updated turn-by-turn, thus making training and\ninference computationally efficient even for long conversations. This paper\nlays the groundwork for an investigation of this framework in two ways. First,\nwe release Contrack, a large scale human-human conversation corpus for context\ntracking with people and location annotations. It contains over 7000\nconversations with an average of 11.8 turns, 5.8 entities and 15.2 references\nper conversation. Second, we open-source a neural network architecture for\ncontext tracking. Finally we compare this network to state-of-the-art\napproaches for the subtasks it subsumes and report results on the involved\ntradeoffs.",
    "descriptor": "",
    "authors": [
      "Ulrich R\u00fcckert",
      "Srinivas Sunkara",
      "Abhinav Rastogi",
      "Sushant Prakash",
      "Pranav Khaitan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12409"
  },
  {
    "id": "arXiv:2201.12414",
    "title": "Any Variational Autoencoder Can Do Arbitrary Conditioning",
    "abstract": "Arbitrary conditioning is an important problem in unsupervised learning,\nwhere we seek to model the conditional densities $p(\\mathbf{x}_u \\mid\n\\mathbf{x}_o)$ that underly some data, for all possible non-intersecting\nsubsets $o, u \\subset \\{1, \\dots , d\\}$. However, the vast majority of density\nestimation only focuses on modeling the joint distribution $p(\\mathbf{x})$, in\nwhich important conditional dependencies between features are opaque. We\npropose a simple and general framework, coined Posterior Matching, that enables\nany Variational Autoencoder (VAE) to perform arbitrary conditioning, without\nmodification to the VAE itself. Posterior Matching applies to the numerous\nexisting VAE-based approaches to joint density estimation, thereby\ncircumventing the specialized models required by previous approaches to\narbitrary conditioning. We find that Posterior Matching achieves performance\nthat is comparable or superior to current state-of-the-art methods for a\nvariety of tasks.",
    "descriptor": "",
    "authors": [
      "Ryan R. Strauss",
      "Junier B. Oliva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12414"
  },
  {
    "id": "arXiv:2201.12416",
    "title": "Discovering Exfiltration Paths Using Reinforcement Learning with Attack  Graphs",
    "abstract": "Reinforcement learning (RL), in conjunction with attack graphs and cyber\nterrain, are used to develop reward and state associated with determination of\noptimal paths for exfiltration of data in enterprise networks. This work builds\non previous crown jewels (CJ) identification that focused on the target goal of\ncomputing optimal paths that adversaries may traverse toward compromising CJs\nor hosts within their proximity. This work inverts the previous CJ approach\nbased on the assumption that data has been stolen and now must be quietly\nexfiltrated from the network. RL is utilized to support the development of a\nreward function based on the identification of those paths where adversaries\ndesire reduced detection. Results demonstrate promising performance for a\nsizable network environment.",
    "descriptor": "",
    "authors": [
      "Tyler Cody",
      "Abdul Rahman",
      "Christopher Redino",
      "Lanxiao Huang",
      "Ryan Clark",
      "Akshay Kakkar",
      "Deepak Kushwaha",
      "Paul Park",
      "Peter Beling",
      "Edward Bowen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.12416"
  },
  {
    "id": "arXiv:2201.12417",
    "title": "Why Should I Trust You, Bellman? The Bellman Error is a Poor Replacement  for Value Error",
    "abstract": "In this work, we study the use of the Bellman equation as a surrogate\nobjective for value prediction accuracy. While the Bellman equation is uniquely\nsolved by the true value function over all state-action pairs, we find that the\nBellman error (the difference between both sides of the equation) is a poor\nproxy for the accuracy of the value function. In particular, we show that (1)\ndue to cancellations from both sides of the Bellman equation, the magnitude of\nthe Bellman error is only weakly related to the distance to the true value\nfunction, even when considering all state-action pairs, and (2) in the finite\ndata regime, the Bellman equation can be satisfied exactly by infinitely many\nsuboptimal solutions. This means that the Bellman error can be minimized\nwithout improving the accuracy of the value function. We demonstrate these\nphenomena through a series of propositions, illustrative toy examples, and\nempirical analysis in standard benchmark domains.",
    "descriptor": "",
    "authors": [
      "Scott Fujimoto",
      "David Meger",
      "Doina Precup",
      "Ofir Nachum",
      "Shixiang Shane Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12417"
  },
  {
    "id": "arXiv:2201.12420",
    "title": "Electra: Conditional Generative Model based Predicate-Aware Query  Approximation",
    "abstract": "The goal of Approximate Query Processing (AQP) is to provide very fast but\n\"accurate enough\" results for costly aggregate queries thereby improving user\nexperience in interactive exploration of large datasets. Recently proposed\nMachine-Learning based AQP techniques can provide very low latency as query\nexecution only involves model inference as compared to traditional query\nprocessing on database clusters. However, with increase in the number of\nfiltering predicates(WHERE clauses), the approximation error significantly\nincreases for these methods. Analysts often use queries with a large number of\npredicates for insights discovery. Thus, maintaining low approximation error is\nimportant to prevent analysts from drawing misleading conclusions. In this\npaper, we propose ELECTRA, a predicate-aware AQP system that can answer\nanalytics-style queries with a large number of predicates with much smaller\napproximation errors. ELECTRA uses a conditional generative model that learns\nthe conditional distribution of the data and at runtime generates a small\n(~1000 rows) but representative sample, on which the query is executed to\ncompute the approximate result. Our evaluations with four different baselines\non three real-world datasets show that ELECTRA provides lower AQP error for\nlarge number of predicates compared to baselines.",
    "descriptor": "\nComments: To appear in Proceedings of AAAI 2022\n",
    "authors": [
      "Nikhil Sheoran",
      "Subrata Mitra",
      "Vibhor Porwal",
      "Siddharth Ghetia",
      "Jatin Varshney",
      "Tung Mai",
      "Anup Rao",
      "Vikas Maddukuri"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12420"
  },
  {
    "id": "arXiv:2201.12423",
    "title": "Benchmarking Resource Usage for Efficient Distributed Deep Learning",
    "abstract": "Deep learning (DL) workflows demand an ever-increasing budget of compute and\nenergy in order to achieve outsized gains. Neural architecture searches,\nhyperparameter sweeps, and rapid prototyping consume immense resources that can\nprevent resource-constrained researchers from experimenting with large models\nand carry considerable environmental impact. As such, it becomes essential to\nunderstand how different deep neural networks (DNNs) and training leverage\nincreasing compute and energy resources -- especially specialized\ncomputationally-intensive models across different domains and applications.\nIn this paper, we conduct over 3,400 experiments training an array of deep\nnetworks representing various domains/tasks -- natural language processing,\ncomputer vision, and chemistry -- on up to 424 graphics processing units\n(GPUs). During training, our experiments systematically vary compute resource\ncharacteristics and energy-saving mechanisms such as power utilization and GPU\nclock rate limits to capture and illustrate the different trade-offs and\nscaling behaviors each representative model exhibits under various resource and\nenergy-constrained regimes. We fit power law models that describe how training\ntime scales with available compute resources and energy constraints. We\nanticipate that these findings will help inform and guide high-performance\ncomputing providers in optimizing resource utilization, by selectively reducing\nenergy consumption for different deep learning tasks/workflows with minimal\nimpact on training.",
    "descriptor": "\nComments: 14 pages, 17 figures\n",
    "authors": [
      "Nathan C. Frey",
      "Baolin Li",
      "Joseph McDonald",
      "Dan Zhao",
      "Michael Jones",
      "David Bestor",
      "Devesh Tiwari",
      "Vijay Gadepally",
      "Siddharth Samsi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12423"
  },
  {
    "id": "arXiv:2201.12425",
    "title": "CoordX: Accelerating Implicit Neural Representation with a Split MLP  Architecture",
    "abstract": "Implicit neural representations with multi-layer perceptrons (MLPs) have\nrecently gained prominence for a wide variety of tasks such as novel view\nsynthesis and 3D object representation and rendering. However, a significant\nchallenge with these representations is that both training and inference with\nan MLP over a large number of input coordinates to learn and represent an\nimage, video, or 3D object, require large amounts of computation and incur long\nprocessing times. In this work, we aim to accelerate inference and training of\ncoordinate-based MLPs for implicit neural representations by proposing a new\nsplit MLP architecture, CoordX. With CoordX, the initial layers are split to\nlearn each dimension of the input coordinates separately. The intermediate\nfeatures are then fused by the last layers to generate the learned signal at\nthe corresponding coordinate point. This significantly reduces the amount of\ncomputation required and leads to large speedups in training and inference,\nwhile achieving similar accuracy as the baseline MLP. This approach thus aims\nat first learning functions that are a decomposition of the original signal and\nthen fusing them to generate the learned signal. Our proposed architecture can\nbe generally used for many implicit neural representation tasks with no\nadditional memory overheads. We demonstrate a speedup of up to 2.92x compared\nto the baseline model for image, video, and 3D shape representation and\nrendering tasks.",
    "descriptor": "",
    "authors": [
      "Ruofan Liang",
      "Hongyi Sun",
      "Nandita Vijaykumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12425"
  },
  {
    "id": "arXiv:2201.12426",
    "title": "A Simple Guard for Learned Optimizers",
    "abstract": "If the trend of learned components eventually outperforming their\nhand-crafted version continues, learned optimizers will eventually outperform\nhand-crafted optimizers like SGD or Adam. Even if learned optimizers (L2Os)\neventually outpace hand-crafted ones in practice however, they are still not\nprovably convergent and might fail out of distribution. These are the questions\naddressed here. Currently, learned optimizers frequently outperform generic\nhand-crafted optimizers (such as gradient descent) at the beginning of learning\nbut they generally plateau after some time while the generic algorithms\ncontinue to make progress and often overtake the learned algorithm as Aesop's\ntortoise which overtakes the hare and are not. L2Os also still have a difficult\ntime generalizing out of distribution. (Heaton et al., 2020) proposed\nSafeguarded L2O (GL2O) which can take a learned optimizer and safeguard it with\na generic learning algorithm so that by conditionally switching between the\ntwo, the resulting algorithm is provably convergent.\nWe propose a new class of Safeguarded L2O, called Loss-Guarded L2O (LGL2O),\nwhich is both conceptually simpler and computationally less expensive. The\nguarding mechanism decides solely based on the expected future loss value of\nboth optimizers. Furthermore, we show theoretical proof of LGL2O's convergence\nguarantee and empirical results comparing to GL2O and other baselines showing\nthat it combines the best of both L2O and SGD and and in practice converges\nmuch better than GL2O.",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Isabeau Pr\u00e9mont-Schwarz",
      "Jaroslav V\u00edtk\u016f",
      "Jan Feyereisl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12426"
  },
  {
    "id": "arXiv:2201.12427",
    "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
    "abstract": "We consider the safe reinforcement learning (RL) problem of maximizing\nutility while satisfying provided constraints. Since we do not assume any prior\nknowledge or pre-training of the safety concept, we are interested in\nasymptotic constraint satisfaction. A popular approach in this line of research\nis to combine the Lagrangian method with a model-free RL algorithm to adjust\nthe weight of the constraint reward dynamically. It relies on a single policy\nto handle the conflict between utility and constraint rewards, which is often\nchallenging. Inspired by the safety layer design (Dalal et al., 2018), we\npropose to separately learn a safety editor policy that transforms potentially\nunsafe actions output by a utility maximizer policy into safe ones. The safety\neditor is trained to maximize the constraint reward while minimizing a hinge\nloss of the utility Q values of actions before and after the edit. On 12 custom\nSafety Gym (Ray et al., 2019) tasks and 2 safe racing tasks with very harsh\nconstraint thresholds, our approach demonstrates outstanding utility\nperformance while complying with the constraints. Ablation studies reveal that\nour two-policy design is critical. Simply doubling the model capacity of\ntypical single-policy approaches will not lead to comparable results. The Q\nhinge loss is also important in certain circumstances, and replacing it with\nthe usual L2 distance could fail badly.",
    "descriptor": "",
    "authors": [
      "Haonan Yu",
      "Wei Xu",
      "Haichao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12427"
  },
  {
    "id": "arXiv:2201.12428",
    "title": "Systematic Training and Testing for Machine Learning Using Combinatorial  Interaction Testing",
    "abstract": "This paper demonstrates the systematic use of combinatorial coverage for\nselecting and characterizing test and training sets for machine learning\nmodels. The presented work adapts combinatorial interaction testing, which has\nbeen successfully leveraged in identifying faults in software testing, to\ncharacterize data used in machine learning. The MNIST hand-written digits data\nis used to demonstrate that combinatorial coverage can be used to select test\nsets that stress machine learning model performance, to select training sets\nthat lead to robust model performance, and to select data for fine-tuning\nmodels to new domains. Thus, the results posit combinatorial coverage as a\nholistic approach to training and testing for machine learning. In contrast to\nprior work which has focused on the use of coverage in regard to the internal\nof neural networks, this paper considers coverage over simple features derived\nfrom inputs and outputs. Thus, this paper addresses the case where the supplier\nof test and training sets for machine learning models does not have\nintellectual property rights to the models themselves. Finally, the paper\naddresses prior criticism of combinatorial coverage and provides a rebuttal\nwhich advocates the use of coverage metrics in machine learning applications.",
    "descriptor": "",
    "authors": [
      "Tyler Cody",
      "Erin Lanus",
      "Daniel D. Doyle",
      "Laura Freeman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12428"
  },
  {
    "id": "arXiv:2201.12431",
    "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
    "abstract": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton -- retrieval automaton -- which approximates\nthe datastore search, based on (1) clustering of entries into \"states\", and (2)\nstate transitions from previous entries. This effectively results in a weighted\nfinite automaton built on top of the datastore, instead of representing the\ndatastore as a flat list. The creation of the automaton is unsupervised, and a\nRetoMaton can be constructed from any text collection: either the original\ntraining corpus or from another domain. Traversing this automaton at inference\ntime, in parallel to the LM inference, reduces its perplexity, or alternatively\nsaves up to 83% of the nearest neighbor searches over kNN-LM (Khandelwal et\nal., 2020), without hurting perplexity.",
    "descriptor": "",
    "authors": [
      "Uri Alon",
      "Frank F. Xu",
      "Junxian He",
      "Sudipta Sengupta",
      "Dan Roth",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12431"
  },
  {
    "id": "arXiv:2201.12433",
    "title": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of  Graph Convolutional Networks",
    "abstract": "Distributed methods for training models on graph datasets have recently grown\nin popularity, due to the size of graph datasets as well as the private nature\nof graphical data like social networks. However, the graphical structure of\nthis data means that it cannot be disjointly partitioned between different\nlearning clients, leading to either significant communication overhead between\nclients or a loss of information available to the training method. We introduce\nFederated Graph Convolutional Network (FedGCN), which uses federated learning\nto train GCN models with optimized convergence rate and communication cost.\nCompared to prior methods that require communication among clients at each\niteration, FedGCN preserves the privacy of client data and only needs\ncommunication at the initial step, which greatly reduces communication cost and\nspeeds up the convergence rate. We theoretically analyze the tradeoff between\nFedGCN's convergence rate and communication cost under different data\ndistributions, introducing a general framework can be generally used for the\nanalysis of all edge-completion-based GCN training algorithms. Experimental\nresults demonstrate the effectiveness of our algorithm and validate our\ntheoretical analysis.",
    "descriptor": "",
    "authors": [
      "Yuhang Yao",
      "Carlee Joe-Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12433"
  },
  {
    "id": "arXiv:2201.12434",
    "title": "Do You Need the Entropy Reward (in Practice)?",
    "abstract": "Maximum entropy (MaxEnt) RL maximizes a combination of the original task\nreward and an entropy reward. It is believed that the regularization imposed by\nentropy, on both policy improvement and policy evaluation, together contributes\nto good exploration, training convergence, and robustness of learned policies.\nThis paper takes a closer look at entropy as an intrinsic reward, by conducting\nvarious ablation studies on soft actor-critic (SAC), a popular representative\nof MaxEnt RL. Our findings reveal that in general, entropy rewards should be\napplied with caution to policy evaluation. On one hand, the entropy reward,\nlike any other intrinsic reward, could obscure the main task reward if it is\nnot properly managed. We identify some failure cases of the entropy reward\nespecially in episodic Markov decision processes (MDPs), where it could cause\nthe policy to be overly optimistic or pessimistic. On the other hand, our\nlarge-scale empirical study shows that using entropy regularization alone in\npolicy improvement, leads to comparable or even better performance and\nrobustness than using it in both policy improvement and policy evaluation.\nBased on these observations, we recommend either normalizing the entropy reward\nto a zero mean (SACZero), or simply removing it from policy evaluation\n(SACLite) for better practical results.",
    "descriptor": "",
    "authors": [
      "Haonan Yu",
      "Haichao Zhang",
      "Wei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12434"
  },
  {
    "id": "arXiv:2201.12436",
    "title": "Any-Play: An Intrinsic Augmentation for Zero-Shot Coordination",
    "abstract": "Cooperative artificial intelligence with human or superhuman proficiency in\ncollaborative tasks stands at the frontier of machine learning research. Prior\nwork has tended to evaluate cooperative AI performance under the restrictive\nparadigms of self-play (teams composed of agents trained together) and\ncross-play (teams of agents trained independently but using the same\nalgorithm). Recent work has indicated that AI optimized for these narrow\nsettings may make for undesirable collaborators in the real-world. We formalize\nan alternative criteria for evaluating cooperative AI, referred to as\ninter-algorithm cross-play, where agents are evaluated on teaming performance\nwith all other agents within an experiment pool with no assumption of\nalgorithmic similarities between agents. We show that existing state-of-the-art\ncooperative AI algorithms, such as Other-Play and Off-Belief Learning,\nunder-perform in this paradigm. We propose the Any-Play learning augmentation\n-- a multi-agent extension of diversity-based intrinsic rewards for zero-shot\ncoordination (ZSC) -- for generalizing self-play-based algorithms to the\ninter-algorithm cross-play setting. We apply the Any-Play learning augmentation\nto the Simplified Action Decoder (SAD) and demonstrate state-of-the-art\nperformance in the collaborative card game Hanabi.",
    "descriptor": "\nComments: Accepted to AAMAS 2022. Code will be made available at this https URL (may take several weeks after posting of this pre-print)\n",
    "authors": [
      "Keane Lucas",
      "Ross E. Allen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12436"
  },
  {
    "id": "arXiv:2201.12437",
    "title": "Task-Focused Few-Shot Object Detection for Robot Manipulation",
    "abstract": "This paper addresses the problem of mobile robot manipulation of novel\nobjects via detection. Our approach uses vision and control as complementary\nfunctions that learn from real-world tasks. We develop a manipulation method\nbased solely on detection then introduce task-focused few-shot object detection\nto learn new objects and settings. The current paradigm for few-shot object\ndetection uses existing annotated examples. In contrast, we extend this\nparadigm by using active data collection and annotation selection that improves\nperformance for specific downstream tasks (e.g., depth estimation and\ngrasping). In experiments for our interactive approach to few-shot learning, we\ntrain a robot to manipulate objects directly from detection (ClickBot).\nClickBot learns visual servo control from a single click of annotation, grasps\nnovel objects in clutter and other settings, and achieves state-of-the-art\nresults on an existing visual servo control and depth estimation benchmark.\nFinally, we establish a task-focused few-shot object detection benchmark to\nsupport future research: https://github.com/griffbr/TFOD.",
    "descriptor": "",
    "authors": [
      "Brent Griffin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12437"
  },
  {
    "id": "arXiv:2201.12438",
    "title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language  Models: A Survey",
    "abstract": "While commonsense knowledge acquisition and reasoning has traditionally been\na core research topic in the knowledge representation and reasoning community,\nrecent years have seen a surge of interest in the natural language processing\ncommunity in developing pre-trained models and testing their ability to address\na variety of newly designed commonsense knowledge reasoning and generation\ntasks. This paper presents a survey of these tasks, discusses the strengths and\nweaknesses of state-of-the-art pre-trained models for commonsense reasoning and\ngeneration as revealed by these tasks, and reflects on future research\ndirections.",
    "descriptor": "\nComments: AAAI 2022\n",
    "authors": [
      "Prajjwal Bhargava",
      "Vincent Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12438"
  },
  {
    "id": "arXiv:2201.12439",
    "title": "Discriminating Defense Against DDoS Attacks; a Novel Approach",
    "abstract": "A recent paper (circa 2020) by Osterwile et al., entitled \"21 Years of\nDistributed Denial of Service: A Call to Action\", states: \"We are falling\nbehind in the war against distributed denial-of-service attacks. Unless we act\nnow, the future of the Internet could be at stake.\" And an earlier (circa 2007)\npaper by Peng et al. states: \"a key challenge for the defense [against DDoS\nattacks] is how to discriminate legitimate requests for service from malicious\naccess attempts.\" This challenge has not been met yet, which is, arguably, a\nmajor reason for the dire situation described by Osterwile et al. -- thirteen\nyears later. This paper attempts to meet an approximation to this challenge, by\nenabling a a site to define the kind of messages that it considers important,\nand by introducing an unambiguous criterion of discrimination between messages\nthat a given site considers important, and all other messages sent to it. Two\nanti-DDoS mechanisms based on this criterion are introduced in this paper. One\nof these relies on lightweight support by routers; and the other one does not.",
    "descriptor": "\nComments: 21 pages 0 figures\n",
    "authors": [
      "Naftaly H. Minsky"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12439"
  },
  {
    "id": "arXiv:2201.12440",
    "title": "Certifying Model Accuracy under Distribution Shifts",
    "abstract": "Certified robustness in machine learning has primarily focused on adversarial\nperturbations of the input with a fixed attack budget for each point in the\ndata distribution. In this work, we present provable robustness guarantees on\nthe accuracy of a model under bounded Wasserstein shifts of the data\ndistribution. We show that a simple procedure that randomizes the input of the\nmodel within a transformation space is provably robust to distributional shifts\nunder the transformation. Our framework allows the datum-specific perturbation\nsize to vary across different points in the input distribution and is general\nenough to include fixed-sized perturbations as well. Our certificates produce\nguaranteed lower bounds on the performance of the model for any (natural or\nadversarial) shift of the input distribution within a Wasserstein ball around\nthe original distribution. We apply our technique to: (i) certify robustness\nagainst natural (non-adversarial) transformations of images such as color\nshifts, hue shifts and changes in brightness and saturation, (ii) certify\nrobustness against adversarial shifts of the input distribution, and (iii) show\nprovable lower bounds (hardness results) on the performance of models trained\non so-called \"unlearnable\" datasets that have been poisoned to interfere with\nmodel training.",
    "descriptor": "",
    "authors": [
      "Aounon Kumar",
      "Alexander Levine",
      "Tom Goldstein",
      "Soheil Feizi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12440"
  },
  {
    "id": "arXiv:2201.12451",
    "title": "Extracting Finite Automata from RNNs Using State Merging",
    "abstract": "One way to interpret the behavior of a blackbox recurrent neural network\n(RNN) is to extract from it a more interpretable discrete computational model,\nlike a finite state machine, that captures its behavior. In this work, we\npropose a new method for extracting finite automata from RNNs inspired by the\nstate merging paradigm from grammatical inference. We demonstrate the\neffectiveness of our method on the Tomita languages benchmark, where we find\nthat it is able to extract faithful automata from RNNs trained on all languages\nin the benchmark. We find that extraction performance is aided by the number of\ndata provided during the extraction process, as well as, curiously, whether the\nRNN model is trained for additional epochs after perfectly learning its target\nlanguage. We use our method to analyze this phenomenon, finding that training\nbeyond convergence is useful because it leads to compression of the internal\nstate space of the RNN. This finding demonstrates how our method can be used\nfor interpretability and analysis of trained RNN models.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "William Merrill",
      "Nikolaos Tsilivis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12451"
  },
  {
    "id": "arXiv:2201.12452",
    "title": "Unfolding Orthotubes with a Dual Hamiltonian Path",
    "abstract": "An orthotube consists of orthogonal boxes (e.g., unit cubes) glued\nface-to-face to form a path. In 1998, Biedl et al. showed that every orthotube\nhas a grid unfolding: a cutting along edges of the boxes so that the surface\nunfolds into a connected planar shape without overlap. We give a new\nalgorithmic grid unfolding of orthotubes with the additional property that the\nrectangular faces are attached in a single path -- a Hamiltonian path on the\nrectangular faces of the orthotube surface.",
    "descriptor": "",
    "authors": [
      "Erik D. Demaine",
      "Kritkorn Karntikoon"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2201.12452"
  },
  {
    "id": "arXiv:2201.12454",
    "title": "The Complexity of Approximate Pattern Matching on De Bruijn Graphs",
    "abstract": "Aligning a sequence to a walk in a labeled graph is a problem of fundamental\nimportance to Computational Biology. For finding a walk in an arbitrary graph\nwith $|E|$ edges that exactly matches a pattern of length $m$, a lower bound\nbased on the Strong Exponential Time Hypothesis (SETH) implies an algorithm\nsignificantly faster than $O(|E|m)$ time is unlikely [Equi et al., ICALP 2019].\nHowever, for many special graphs, such as de Bruijn graphs, the problem can be\nsolved in linear time [Bowe et al., WABI 2012]. For approximate matching, the\npicture is more complex. When edits (substitutions, insertions, and deletions)\nare only allowed to the pattern, or when the graph is acyclic, the problem is\nagain solvable in $O(|E|m)$ time. When edits are allowed to arbitrary cyclic\ngraphs, the problem becomes NP-complete, even on binary alphabets [Jain et al.,\nRECOMB 2019]. These results hold even when edits are restricted to only\nsubstitutions. The complexity of approximate pattern matching on de Bruijn\ngraphs remained open. We investigate this problem and show that the properties\nthat make de Bruijn graphs amenable to efficient exact pattern matching do not\nextend to approximate matching, even when restricted to the substitutions only\ncase with alphabet size four. We prove that determining the existence of a\nmatching walk in a de Bruijn graph is NP-complete when substitutions are\nallowed to the graph. In addition, we demonstrate that an algorithm\nsignificantly faster than $O(|E|m)$ is unlikely for de Bruijn graphs in the\ncase where only substitutions are allowed to the pattern. This stands in\ncontrast to pattern-to-text matching where exact matching is solvable in linear\ntime, like on de Bruijn graphs, but approximate matching under substitutions is\nsolvable in subquadratic $O(n\\sqrt{m})$ time, where $n$ is the text's length\n[Abrahamson, SIAM J. Computing 1987].",
    "descriptor": "",
    "authors": [
      "Daniel Gibney",
      "Sharma V. Thankachan",
      "Srinivas Aluru"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12454"
  },
  {
    "id": "arXiv:2201.12460",
    "title": "On the Global Convergence of Particle Swarm Optimization Methods",
    "abstract": "In this paper we provide a rigorous convergence analysis for the renowned\nParticle Swarm Optimization method using tools from stochastic calculus and the\nanalysis of partial differential equations. Based on a time-continuous\nformulation of the particle dynamics as a system of stochastic differential\nequations, we establish the convergence to a global minimizer in two steps.\nFirst, we prove the consensus formation of the dynamics by analyzing the\ntime-evolution of the variance of the particle distribution. Consecutively, we\nshow that this consensus is close to a global minimizer by employing the\nasymptotic Laplace principle and a tractability condition on the energy\nlandscape of the objective function. Our results allow for the usage of memory\nmechanisms, and hold for a rich class of objectives provided certain conditions\nof well-preparation of the hyperparameters and the initial datum are satisfied.\nTo demonstrate the applicability of the method we propose an efficient and\nparallelizable implementation, which is tested in particular on a competitive\nand well-understood high-dimensional benchmark problem in machine learning.",
    "descriptor": "\nComments: 22 pages, 4 figures\n",
    "authors": [
      "Hui Huang",
      "Jinniao Qiu",
      "Konstantin Riedl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12460"
  },
  {
    "id": "arXiv:2201.12462",
    "title": "Explaining Reinforcement Learning Policies through Counterfactual  Trajectories",
    "abstract": "In order for humans to confidently decide where to employ RL agents for\nreal-world tasks, a human developer must validate that the agent will perform\nwell at test-time. Some policy interpretability methods facilitate this by\ncapturing the policy's decision making in a set of agent rollouts. However,\neven the most informative trajectories of training time behavior may give\nlittle insight into the agent's behavior out of distribution. In contrast, our\nmethod conveys how the agent performs under distribution shifts by showing the\nagent's behavior across a wider trajectory distribution. We generate these\ntrajectories by guiding the agent to more diverse unseen states and showing the\nagent's behavior there. In a user study, we demonstrate that our method enables\nusers to score better than baseline methods on one of two agent validation\ntasks.",
    "descriptor": "\nComments: Accepted at ICML HILL 2021 Workshop\n",
    "authors": [
      "Julius Frost",
      "Olivia Watkins",
      "Eric Weiner",
      "Pieter Abbeel",
      "Trevor Darrell",
      "Bryan Plummer",
      "Kate Saenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12462"
  },
  {
    "id": "arXiv:2201.12464",
    "title": "Using Dynamic Binary Instrumentation to Detect Failures in Robotics  Software",
    "abstract": "Autonomous and Robotics Systems (ARSs) are widespread, complex, and\nincreasingly coming into contact with the public. Many of these systems are\nsafety-critical, and it is vital to detect software errors to protect against\nharm. We propose a family of novel techniques to detect unusual program\nexecutions and incorrect program behavior. We model execution behavior by\ncollecting low-level signals at run time and using those signals to build\nmachine learning models. These models can identify previously-unseen executions\nthat are more likely to exhibit errors. We describe a tractable approach for\ncollecting dynamic binary runtime signals on ARSs, allowing the systems to\nabsorb most of the overhead from dynamic instrumentation. The architecture of\nARSs is particularly well-adapted to hiding the overhead from instrumentation.\nWe demonstrate the efficiency of these approaches on ARDUPILOT -- a popular\nopen-source autopilot software system -- and HUSKY -- an unmanned ground\nvehicle -- in simulation. We instrument executions to gather data from which we\nbuild supervised machine learning models of executions and evaluate the\naccuracy of these models. We also analyze the amount of training data needed to\ndevelop models with various degrees of accuracy, measure the overhead added to\nexecutions that use the analysis tool, and analyze which runtime signals are\nmost useful for detecting unusual behavior on the program under test. In\naddition, we analyze the effects of timing delays on the functional behavior of\nARSs.",
    "descriptor": "",
    "authors": [
      "Deborah S. Katz",
      "Christopher S. Timperley",
      "Claire Le Goues"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.12464"
  },
  {
    "id": "arXiv:2201.12465",
    "title": "Flashlight: Enabling Innovation in Tools for Machine Learning",
    "abstract": "As the computational requirements for machine learning systems and the size\nand complexity of machine learning frameworks increases, essential framework\ninnovation has become challenging. While computational needs have driven recent\ncompiler, networking, and hardware advancements, utilization of those\nadvancements by machine learning tools is occurring at a slower pace. This is\nin part due to the difficulties involved in prototyping new computational\nparadigms with existing frameworks. Large frameworks prioritize machine\nlearning researchers and practitioners as end users and pay comparatively\nlittle attention to systems researchers who can push frameworks forward -- we\nargue that both are equally important stakeholders. We introduce Flashlight, an\nopen-source library built to spur innovation in machine learning tools and\nsystems by prioritizing open, modular, customizable internals and\nstate-of-the-art, research-ready models and training setups across a variety of\ndomains. Flashlight allows systems researchers to rapidly prototype and\nexperiment with novel ideas in machine learning computation and has low\noverhead, competing with and often outperforming other popular machine learning\nframeworks. We see Flashlight as a tool enabling research that can benefit\nwidely used libraries downstream and bring machine learning and systems\nresearchers closer together.",
    "descriptor": "",
    "authors": [
      "Jacob Kahn",
      "Vineel Pratap",
      "Tatiana Likhomanenko",
      "Qiantong Xu",
      "Awni Hannun",
      "Jeff Cai",
      "Paden Tomasello",
      "Ann Lee",
      "Edouard Grave",
      "Gilad Avidov",
      "Benoit Steiner",
      "Vitaliy Liptchinsky",
      "Gabriel Synnaeve",
      "Ronan Collobert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12465"
  },
  {
    "id": "arXiv:2201.12467",
    "title": "Improving Federated Learning Face Recognition via Privacy-Agnostic  Clusters",
    "abstract": "The growing public concerns on data privacy in face recognition can be\ngreatly addressed by the federated learning (FL) paradigm. However,\nconventional FL methods perform poorly due to the uniqueness of the task:\nbroadcasting class centers among clients is crucial for recognition\nperformances but leads to privacy leakage. To resolve the privacy-utility\nparadox, this work proposes PrivacyFace, a framework largely improves the\nfederated learning face recognition via communicating auxiliary and\nprivacy-agnostic information among clients. PrivacyFace mainly consists of two\ncomponents: First, a practical Differentially Private Local Clustering (DPLC)\nmechanism is proposed to distill sanitized clusters from local class centers.\nSecond, a consensus-aware recognition loss subsequently encourages global\nconsensuses among clients, which ergo results in more discriminative features.\nThe proposed framework is mathematically proved to be differentially private,\nintroducing a lightweight overhead as well as yielding prominent performance\nboosts (\\textit{e.g.}, +9.63\\% and +10.26\\% for TAR@FAR=1e-4 on IJB-B and IJB-C\nrespectively). Extensive experiments and ablation studies on a large-scale\ndataset have demonstrated the efficacy and practicability of our method.",
    "descriptor": "\nComments: ICLR2022, Spotlight\n",
    "authors": [
      "Qiang Meng",
      "Feng Zhou",
      "Hainan Ren",
      "Tianshu Feng",
      "Guochao Liu",
      "Yuanqing Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12467"
  },
  {
    "id": "arXiv:2201.12468",
    "title": "Symbolic-Numeric Integration of Univariate Expressions based on Sparse  Regression",
    "abstract": "Most computer algebra systems (CAS) support symbolic integration as core\nfunctionality. The majority of the integration packages use a combination of\nheuristic algebraic and rule-based (integration table) methods. In this paper,\nwe present a hybrid (symbolic-numeric) methodology to calculate the indefinite\nintegrals of univariate expressions. The primary motivation for this work is to\nadd symbolic integration functionality to a modern CAS (the symbolic\nmanipulation packages of SciML, the Scientific Machine Learning ecosystem of\nthe Julia programming language), which is mainly designed toward numerical and\nmachine learning applications and has a different set of features than\ntraditional CAS. The symbolic part of our method is based on the combination of\ncandidate terms generation (borrowed from the Homotopy operators theory) with\nrule-based expression transformations provided by the underlying CAS. The\nnumeric part is based on sparse-regression, a component of Sparse\nIdentification of Nonlinear Dynamics (SINDy) technique. We show that this\nsystem can solve a large variety of common integration problems using only a\nfew dozen basic integration rules.",
    "descriptor": "\nComments: 8 pages. submitted to ISSAC 2022. Code at this https URL\n",
    "authors": [
      "Shahriar Iravanian",
      "Carl Julius Martensen",
      "Alessandro Cheli",
      "Shashi Gowda",
      "Anand Jain",
      "Yingbo Ma",
      "Chris Rackauckas"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2201.12468"
  },
  {
    "id": "arXiv:2201.12469",
    "title": "ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language  Models via Efficient Large-Batch Adversarial Noise",
    "abstract": "In recent years, large pre-trained Transformer-based language models have led\nto dramatic improvements in many natural language understanding tasks. To train\nthese models with increasing sizes, many neural network practitioners attempt\nto increase the batch sizes in order to leverage multiple GPUs to improve\ntraining speed. However, increasing the batch size often makes the optimization\nmore difficult, leading to slow convergence or poor generalization that can\nrequire orders of magnitude more training time to achieve the same model\nquality. In this paper, we explore the steepness of the loss landscape of\nlarge-batch optimization for adapting pre-trained Transformer-based language\nmodels to domain-specific tasks and find that it tends to be highly complex and\nirregular, posing challenges to generalization on downstream tasks.\nTo tackle this challenge, we propose ScaLA, a novel and efficient method to\naccelerate the adaptation speed of pre-trained transformer networks. Different\nfrom prior methods, we take a sequential game-theoretic approach by adding\nlightweight adversarial noise into large-batch optimization, which\nsignificantly improves adaptation speed while preserving model generalization.\nExperiment results show that ScaLA attains 2.7--9.8$\\times$ adaptation speedups\nover the baseline for GLUE on BERT-base and RoBERTa-large, while achieving\ncomparable and sometimes higher accuracy than the state-of-the-art large-batch\noptimization methods. Finally, we also address the theoretical aspect of\nlarge-batch optimization with adversarial noise and provide a theoretical\nconvergence rate analysis for ScaLA using techniques for analyzing non-convex\nsaddle-point problems.",
    "descriptor": "",
    "authors": [
      "Minjia Zhang",
      "Niranjan Uma Naresh",
      "Yuxiong He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12469"
  },
  {
    "id": "arXiv:2201.12470",
    "title": "Distributed Dimension Reduction for Distributed Massive MIMO C-RAN with  Finite Fronthaul Capacity",
    "abstract": "The use of a large excess of service antennas brings a variety of performance\nbenefits to distributed MIMO C-RAN, but the corresponding high fronthaul data\nloads can be problematic in practical systems with limited fronthaul capacity.\nIn this work we propose the use of lossy dimension reduction, applied locally\nat each remote radio head (RRH), to reduce this fronthaul traffic. We first\nconsider the uplink, and the case where each RRH applies a linear dimension\nreduction filter to its multi-antenna received signal vector. It is shown that\nunder a joint mutual information criteria, the optimal dimension reduction\nfilters are given by a variant of the conditional Karhunen-Loeve transform,\nwith a stationary point found using block co-ordinate ascent. These filters are\nthen modified such that each RRH can calculate its own dimension reduction\nfilter in a decentralised manner, using knowledge only of its own instantaneous\nchannel and network slow fading coefficients. We then show that in TDD systems\nthese dimension reduction filters can be re-used as part of a two-stage reduced\ndimension downlink precoding scheme. Analysis and numerical results demonstrate\nthat the proposed approach can significantly reduce both uplink and downlink\nfronthaul traffic whilst incurring very little loss in MIMO performance.",
    "descriptor": "",
    "authors": [
      "Fred Wiffen",
      "Woon Hau Chin",
      "Angela Doufexi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12470"
  },
  {
    "id": "arXiv:2201.12477",
    "title": "An Indirect Rate-Distortion Characterization for Semantic Sources:  General Model and the Case of Gaussian Observation",
    "abstract": "A new source model, which consists of an intrinsic state part and an\nextrinsic observation part, is proposed and its information-theoretic\ncharacterization, namely its rate-distortion function, is defined and analyzed.\nSuch a source model is motivated by the recent surge of interest in the\nsemantic aspect of information: the intrinsic state corresponds to the semantic\nfeature of the source, which in general is not observable but can only be\ninferred from the extrinsic observation. There are two distortion measures, one\nbetween the intrinsic state and its reproduction, and the other between the\nextrinsic observation and its reproduction. Under a given code rate, the\ntradeoff between these two distortion measures is characterized by the\nrate-distortion function, which is solved via the indirect rate-distortion\ntheory and is termed as the semantic rate-distortion function of the source. As\nan application of the general model and its analysis, the case of Gaussian\nextrinsic observation is studied, assuming a linear relationship between the\nintrinsic state and the extrinsic observation, under a quadratic distortion\nstructure. The semantic rate-distortion function is shown to be the solution of\na convex programming programming with respect to an error covariance matrix,\nand a reverse water-filling type of solution is provided when the model further\nsatisfies a diagonalizability condition.",
    "descriptor": "\nComments: Part of the preliminary results appears in arXiv:2105.04278\n",
    "authors": [
      "Jiakun Liu",
      "Shuo Shao",
      "Wenyi Zhang",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12477"
  },
  {
    "id": "arXiv:2201.12480",
    "title": "Interconnect Parasitics and Partitioning in Fully-Analog In-Memory  Computing Architectures",
    "abstract": "Fully-analog in-memory computing (IMC) architectures that implement both\nmatrix-vector multiplication and non-linear vector operations within the same\nmemory array have shown promising performance benefits over conventional IMC\nsystems due to the removal of energy-hungry signal conversion units. However,\nmaintaining the computation in the analog domain for the entire deep neural\nnetwork (DNN) comes with potential sensitivity to interconnect parasitics.\nThus, in this paper, we investigate the effect of wire parasitic resistance and\ncapacitance on the accuracy of DNN models deployed on fully-analog IMC\narchitectures. Moreover, we propose a partitioning mechanism to alleviate the\nimpact of the parasitic while keeping the computation in the analog domain\nthrough dividing large arrays into multiple partitions. The SPICE circuit\nsimulation results for a 400 X 120 X 84 X 10 DNN model deployed on a\nfully-analog IMC circuit show that a 94.84% accuracy could be achieved for\nMNIST classification application with 16, 8, and 8 horizontal partitions, as\nwell as 8, 8, and 1 vertical partitions for first, second, and third layers of\nthe DNN, respectively, which is comparable to the ~97% accuracy realized by\ndigital implementation on CPU. It is shown that accuracy benefits are achieved\nat the cost of higher power consumption due to the extra circuitry required for\nhandling partitioning.",
    "descriptor": "\nComments: 5 pages, 6 figures\n",
    "authors": [
      "Md Hasibul Amin",
      "Mohammed Elbtity",
      "Ramtin Zand"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12480"
  },
  {
    "id": "arXiv:2201.12482",
    "title": "Collaborative Learning in General Graphs with Limited Memorization:  Learnability, Complexity and Reliability",
    "abstract": "We consider K-armed bandit problem in general graphs where agents are\narbitrarily connected and each of them has limited memorization and\ncommunication bandwidth. The goal is to let each of the agents learn the best\narm. Although recent studies show the power of collaboration among the agents\nin improving the efficacy of learning, it is assumed in these studies that the\ncommunication graphs should be complete or well-structured, whereas such an\nassumption is not always valid in practice. Furthermore, limited memorization\nand communication bandwidth also restrict the collaborations of the agents,\nsince very few knowledge can be drawn by each agent from its experiences or the\nones shared by its peers in this case. Additionally, the agents may be\ncorrupted to share falsified experience, while the resource limit may\nconsiderably restrict the reliability of the learning process. To address the\nabove issues, we propose a three-staged collaborative learning algorithm. In\neach step, the agents share their experience with each other through\nlight-weight random walks in the general graphs, and then make decisions on\nwhich arms to pull according to the randomly memorized suggestions. The agents\nfinally update their adoptions (i.e., preferences to the arms) based on the\nreward feedback of the arm pulling. Our theoretical analysis shows that, by\nexploiting the limited memorization and communication resources, all the agents\neventually learn the best arm with high probability. We also reveal in our\ntheoretical analysis the upper-bound on the number of corrupted agents our\nalgorithm can tolerate. The efficacy of our proposed three-staged collaborative\nlearning algorithm is finally verified by extensive experiments on both\nsynthetic and real datasets.",
    "descriptor": "",
    "authors": [
      "Feng Li",
      "Xuyang Yuan",
      "Lina Wang",
      "Huan Yang",
      "Dongxiao Yu",
      "Weifeng Lv",
      "Xiuzhen Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12482"
  },
  {
    "id": "arXiv:2201.12484",
    "title": "Fair Stable Matching Meets Correlated Preferences",
    "abstract": "The stable matching problem sets the economic foundation of several practical\napplications ranging from school choice and medical residency to ridesharing\nand refugee placement. It is concerned with finding a matching between two\ndisjoint sets of agents wherein no pair of agents prefer each other to their\nmatched partners. The Deferred Acceptance (DA) algorithm is an elegant\nprocedure that guarantees a stable matching for any input; however, its outcome\nmay be unfair as it always favors one side by returning a matching that is\noptimal for one side (say men) and pessimal for the other side (say women). A\ndesirable fairness notion is minimizing the sex-equality cost, i.e. the\ndifference between the total rankings of both sides. Computing such stable\nmatchings is a strongly NP-hard problem, which raises the question of what\ntractable algorithms to adopt in practice. We conduct a series of empirical\nevaluations on the properties of sex-equal stable matchings when preferences of\nagents on both sides are correlated. Our empirical results suggest that under\ncorrelated preferences, the DA algorithm returns stable matchings with low\nsex-equality cost, which further confirms its broad use in many practical\napplications.",
    "descriptor": "\nComments: Full version of the paper that appeared at AAMAS 2022\n",
    "authors": [
      "Angelina Brilliantova",
      "Hadi Hosseini"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12484"
  },
  {
    "id": "arXiv:2201.12487",
    "title": "Counterfactual Plans under Distributional Ambiguity",
    "abstract": "Counterfactual explanations are attracting significant attention due to the\nflourishing applications of machine learning models in consequential domains. A\ncounterfactual plan consists of multiple possibilities to modify a given\ninstance so that the model's prediction will be altered. As the predictive\nmodel can be updated subject to the future arrival of new data, a\ncounterfactual plan may become ineffective or infeasible with respect to the\nfuture values of the model parameters. In this work, we study the\ncounterfactual plans under model uncertainty, in which the distribution of the\nmodel parameters is partially prescribed using only the first- and\nsecond-moment information. First, we propose an uncertainty quantification tool\nto compute the lower and upper bounds of the probability of validity for any\ngiven counterfactual plan. We then provide corrective methods to adjust the\ncounterfactual plan to improve the validity measure. The numerical experiments\nvalidate our bounds and demonstrate that our correction increases the\nrobustness of the counterfactual plans in different real-world datasets.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Ngoc Bui",
      "Duy Nguyen",
      "Viet Anh Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12487"
  },
  {
    "id": "arXiv:2201.12488",
    "title": "Achieving Efficient Distributed Machine Learning Using a Novel  Non-Linear Class of Aggregation Functions",
    "abstract": "Distributed machine learning (DML) over time-varying networks can be an\nenabler for emerging decentralized ML applications such as autonomous driving\nand drone fleeting. However, the commonly used weighted arithmetic mean model\naggregation function in existing DML systems can result in high model loss, low\nmodel accuracy, and slow convergence speed over time-varying networks. To\naddress this issue, in this paper, we propose a novel non-linear class of model\naggregation functions to achieve efficient DML over time-varying networks.\nInstead of taking a linear aggregation of neighboring models as most existing\nstudies do, our mechanism uses a nonlinear aggregation, a weighted power-p mean\n(WPM) where p is a positive odd integer, as the aggregation function of local\nmodels from neighbors. The subsequent optimizing steps are taken using mirror\ndescent defined by a Bregman divergence that maintains convergence to\noptimality. In this paper, we analyze properties of the WPM and rigorously\nprove convergence properties of our aggregation mechanism. Additionally,\nthrough extensive experiments, we show that when p > 1, our design\nsignificantly improves the convergence speed of the model and the scalability\nof DML under time-varying networks compared with arithmetic mean aggregation\nfunctions, with little additional 26computation overhead.",
    "descriptor": "\nComments: 13 pages, 26 figures\n",
    "authors": [
      "Haizhou Du",
      "Ryan Yang",
      "Yijian Chen",
      "Qiao Xiang",
      "Andre Wibisono",
      "Wei Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12488"
  },
  {
    "id": "arXiv:2201.12489",
    "title": "A Context-Integrated Transformer-Based Neural Network for Auction Design",
    "abstract": "One of the central problems in auction design is developing an\nincentive-compatible mechanism that maximizes the auctioneer's expected\nrevenue. While theoretical approaches have encountered bottlenecks in\nmulti-item auctions, recently, there has been much progress on finding the\noptimal mechanism through deep learning. However, these works either focus on a\nfixed set of bidders and items, or restrict the auction to be symmetric. In\nthis work, we overcome such limitations by factoring \\emph{public} contextual\ninformation of bidders and items into the auction learning framework. We\npropose $\\mathtt{CITransNet}$, a context-integrated transformer-based neural\nnetwork for optimal auction design, which maintains permutation-equivariance\nover bids and contexts while being able to find asymmetric solutions. We show\nby extensive experiments that $\\mathtt{CITransNet}$ can recover the known\noptimal solutions in single-item settings, outperform strong baselines in\nmulti-item auctions, and generalize well to cases other than those in training.",
    "descriptor": "",
    "authors": [
      "Zhijian Duan",
      "Jingwu Tang",
      "Yutong Yin",
      "Zhe Feng",
      "Xiang Yan",
      "Manzil Zaheer",
      "Xiaotie Deng"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12489"
  },
  {
    "id": "arXiv:2201.12490",
    "title": "Random Orthogonalization for Federated Learning in Massive MIMO Systems",
    "abstract": "We propose a novel uplink communication method, coined random\northogonalization, for federated learning (FL) in a massive multiple-input and\nmultiple-output (MIMO) wireless system. The key novelty of random\northogonalization comes from the tight coupling of FL model aggregation and two\nunique characteristics of massive MIMO - channel hardening and favorable\npropagation. As a result, random orthogonalization can achieve natural\nover-the-air model aggregation without requiring transmitter side channel state\ninformation, while significantly reducing the channel estimation overhead at\nthe receiver. Theoretical analyses with respect to both communication and\nmachine learning performances are carried out. In particular, an explicit\nrelationship among the convergence rate, the number of clients and the number\nof antennas is established. Experimental results validate the effectiveness and\nefficiency of random orthogonalization for FL in massive MIMO.",
    "descriptor": "\nComments: 6 pages, 1 table, 4 figures, accepted to International Conference on Communications (ICC) 2022\n",
    "authors": [
      "Xizixiang Wei",
      "Cong Shen",
      "Jing Yang",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12490"
  },
  {
    "id": "arXiv:2201.12493",
    "title": "A new Sparse Auto-encoder based Framework using Grey Wolf Optimizer for  Data Classification Problem",
    "abstract": "One of the most important properties of deep auto-encoders (DAEs) is their\ncapability to extract high level features from row data. Hence, especially\nrecently, the autoencoders are preferred to be used in various classification\nproblems such as image and voice recognition, computer security, medical data\nanalysis, etc. Despite, its popularity and high performance, the training phase\nof autoencoders is still a challenging task, involving to select best\nparameters that let the model to approach optimal results. Different training\napproaches are applied to train sparse autoencoders. Previous studies and\npreliminary experiments reveal that those approaches may present remarkable\nresults in same problems but also disappointing results can be obtained in\nother complex problems. Metaheuristic algorithms have emerged over the last two\ndecades and are becoming an essential part of contemporary optimization\ntechniques. Gray wolf optimization (GWO) is one of the current of those\nalgorithms and is applied to train sparse auto-encoders for this study. This\nmodel is validated by employing several popular Gene expression databases.\nResults are compared with previous state-of-the art methods studied with the\nsame data sets and also are compared with other popular metaheuristic\nalgorithms, namely, Genetic Algorithms (GA), Particle Swarm Optimization (PSO)\nand Artificial Bee Colony (ABC). Results reveal that the performance of the\ntrained model using GWO outperforms on both conventional models and models\ntrained with most popular metaheuristic algorithms.",
    "descriptor": "",
    "authors": [
      "Ahmad Mozaffer Karim"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12493"
  },
  {
    "id": "arXiv:2201.12498",
    "title": "Investigating Why Contrastive Learning Benefits Robustness Against Label  Noise",
    "abstract": "Self-supervised contrastive learning has recently been shown to be very\neffective in preventing deep networks from overfitting noisy labels. Despite\nits empirical success, the theoretical understanding of the effect of\ncontrastive learning on boosting robustness is very limited. In this work, we\nrigorously prove that the representation matrix learned by contrastive learning\nboosts robustness, by having: (i) one prominent singular value corresponding to\nevery sub-class in the data, and remaining significantly smaller singular\nvalues; and (ii) a large alignment between the prominent singular vector and\nthe clean labels of each sub-class. The above properties allow a linear layer\ntrained on the representations to quickly learn the clean labels, and prevent\nit from overfitting the noise for a large number of training iterations. We\nfurther show that the low-rank structure of the Jacobian of deep networks\npre-trained with contrastive learning allows them to achieve a superior\nperformance initially, when fine-tuned on noisy labels. Finally, we demonstrate\nthat the initial robustness provided by contrastive learning enables robust\ntraining methods to achieve state-of-the-art performance under extreme noise\nlevels, e.g., an average of 27.18\\% and 15.58\\% increase in accuracy on\nCIFAR-10 and CIFAR-100 with 80\\% symmetric noisy labels, and 4.11\\% increase in\naccuracy on WebVision.",
    "descriptor": "",
    "authors": [
      "Yihao Xue",
      "Kyle Whitecross",
      "Baharan Mirzasoleiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12498"
  },
  {
    "id": "arXiv:2201.12499",
    "title": "Reconstruction of Power Lines from Point Clouds",
    "abstract": "This paper proposes a novel solution for constructing line features modeling\neach catenary curve present within a series of points representing multiple\ncatenary curves. The solution can be applied to extract power lines from lidar\npoint clouds, which can then be used in downstream applications like creating\ndigital twin geospatial models and evaluating the encroachment of vegetation.\nThis paper offers an example of how the results obtained by the proposed\nsolution could be used to assess vegetation growth near transmission power\nlines based on freely available lidar data for the City of Utrecht, Netherlands\n[1].",
    "descriptor": "\nComments: 15 pages, 8 figures, 1 table\n",
    "authors": [
      "Alexander Gribov",
      "Khalid Duri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2201.12499"
  },
  {
    "id": "arXiv:2201.12501",
    "title": "Does Transliteration Help Multilingual Language Modeling?",
    "abstract": "As there is a scarcity of large representative corpora for most languages, it\nis important for Multilingual Language Models (MLLM) to extract the most out of\nexisting corpora. In this regard, script diversity presents a challenge to\nMLLMs by reducing lexical overlap among closely related languages. Therefore,\ntransliterating closely related languages that use different writing scripts to\na common script may improve the downstream task performance of MLLMs. In this\npaper, we pretrain two ALBERT models to empirically measure the effect of\ntransliteration on MLLMs. We specifically focus on the Indo-Aryan language\nfamily, which has the highest script diversity in the world. Afterward, we\nevaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test\nto rigorously verify whether the effect of transliteration is significant or\nnot. We find that transliteration benefits the low-resource languages without\nnegatively affecting the comparatively high-resource languages. We also measure\nthe cross-lingual representation similarity (CLRS) of the models using centered\nkernel alignment (CKA) on parallel sentences of eight languages from the\nFLORES-101 dataset. We find that the hidden representations of the\ntransliteration-based model have higher and more stable CLRS scores. Our code\nis available at Github (github.com/ibraheem-moosa/XLM-Indic) and Hugging Face\nHub (huggingface.co/ibraheemmoosa/xlmindic-base-multiscript and\nhuggingface.co/ibraheemmoosa/xlmindic-base-uniscript).",
    "descriptor": "",
    "authors": [
      "Ibraheem Muhammad Moosa",
      "Mahmud Elahi Akhter",
      "Ashfia Binte Habib"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12501"
  },
  {
    "id": "arXiv:2201.12502",
    "title": "Unsupervised Summarization with Customized Granularities",
    "abstract": "Text summarization is a personalized and customized task, i.e., for one\ndocument, users often have different preferences for the summary. As a key\naspect of customization in summarization, granularity is used to measure the\nsemantic coverage between summary and source document. Coarse-grained summaries\ncan only contain the most central event in the original text, while\nfine-grained summaries cover more sub-events and corresponding details.\nHowever, previous studies mostly develop systems in the single-granularity\nscenario. And models that can generate summaries with customizable semantic\ncoverage still remain an under-explored topic. In this paper, we propose the\nfirst unsupervised multi-granularity summarization framework, GranuSum. We take\nevents as the basic semantic units of the source documents and propose to rank\nthese events by their salience. We also develop a model to summarize input\ndocuments with given events as anchors and hints. By inputting different\nnumbers of events, GranuSum is capable of producing multi-granular summaries in\nan unsupervised manner. Meanwhile, to evaluate multi-granularity summarization\nmodels, we annotate a new benchmark GranuDUC, in which we write multiple\nsummaries of different granularities for each document cluster. Experimental\nresults confirm the substantial superiority of GranuSum on multi-granularity\nsummarization over several baseline systems. Furthermore, by experimenting on\nconventional unsupervised abstractive summarization tasks, we find that\nGranuSum, by exploiting the event information, can also achieve new\nstate-of-the-art results under this scenario, outperforming strong baselines.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Ming Zhong",
      "Yang Liu",
      "Suyu Ge",
      "Yuning Mao",
      "Yizhu Jiao",
      "Xingxing Zhang",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12502"
  },
  {
    "id": "arXiv:2201.12506",
    "title": "2D+3D facial expression recognition via embedded tensor manifold  regularization",
    "abstract": "In this paper, a novel approach via embedded tensor manifold regularization\nfor 2D+3D facial expression recognition (FERETMR) is proposed. Firstly, 3D\ntensors are constructed from 2D face images and 3D face shape models to keep\nthe structural information and correlations. To maintain the local structure\n(geometric information) of 3D tensor samples in the low-dimensional tensors\nspace during the dimensionality reduction, the $\\ell_0$-norm of the core\ntensors and a tensor manifold regularization scheme embedded on core tensors\nare adopted via a low-rank truncated Tucker decomposition on the generated\ntensors. As a result, the obtained factor matrices will be used for facial\nexpression classification prediction. To make the resulting tensor optimization\nmore tractable, $\\ell_1$-norm surrogate is employed to relax $\\ell_0$-norm and\nhence the resulting tensor optimization problem has a nonsmooth objective\nfunction due to the $\\ell_1$-norm and orthogonal constraints from the\northogonal Tucker decomposition. To efficiently tackle this tensor optimization\nproblem, we establish the first-order optimality condition in terms of\nstationary points, and then design a block coordinate descent (BCD) algorithm\nwith convergence analysis and the computational complexity. Numerical results\non BU-3DFE database and Bosphorus databases demonstrate the effectiveness of\nour proposed approach.",
    "descriptor": "",
    "authors": [
      "Yunfang Fu",
      "Qiuqi Ruan",
      "Ziyan Luo",
      "Gaoyun An",
      "Yi Jin",
      "Jun Wan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12506"
  },
  {
    "id": "arXiv:2201.12507",
    "title": "AutoDistil: Few-shot Task-agnostic Neural Architecture Search for  Distilling Large Language Models",
    "abstract": "Knowledge distillation (KD) methods compress large models into smaller\nstudents with manually-designed student architectures given pre-specified\ncomputational cost. This requires several trials to find a viable student, and\nfurther repeating the process for each student or computational budget change.\nWe use Neural Architecture Search (NAS) to automatically distill several\ncompressed students with variable cost from a large model. Current works train\na single SuperLM consisting of millions of subnetworks with weight-sharing,\nresulting in interference between subnetworks of different sizes. Our framework\nAutoDistil addresses above challenges with the following steps: (a)\nIncorporates inductive bias and heuristics to partition Transformer search\nspace into K compact sub-spaces (K=3 for typical student sizes of base, small\nand tiny); (b) Trains one SuperLM for each sub-space using task-agnostic\nobjective (e.g., self-attention distillation) with weight-sharing of students;\n(c) Lightweight search for the optimal student without re-training. Fully\ntask-agnostic training and search allow students to be reused for fine-tuning\non any downstream task. Experiments on GLUE benchmark against state-of-the-art\nKD and NAS methods demonstrate AutoDistil to outperform leading compression\ntechniques with upto 2.7x reduction in computational cost and negligible loss\nin task performance.",
    "descriptor": "\nComments: 13 pages, 4 figures, 10 tables\n",
    "authors": [
      "Dongkuan Xu",
      "Subhabrata Mukherjee",
      "Xiaodong Liu",
      "Debadeepta Dey",
      "Wenhui Wang",
      "Xiang Zhang",
      "Ahmed Hassan Awadallah",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12507"
  },
  {
    "id": "arXiv:2201.12513",
    "title": "The Geography of Facebook Groups in the United States",
    "abstract": "We use exploratory factor analysis to investigate the online persistence of\nknown community-level patterns of social capital variance in the U.S. context.\nOur analysis focuses on Facebook groups, specifically those that tend to\nconnect users in the same local area. We investigate the relationship between\nestablished, localized measures of social capital at the county level and\npatterns of participation in Facebook groups in the same areas. We identify\nfour main factors that distinguish Facebook group engagement by county. The\nfirst captures small, private groups, dense with friendship connections. The\nsecond captures very local and small groups. The third captures non-local,\nlarge, public groups, with more age mixing. The fourth captures partially local\ngroups of medium to large size. The first and third factor correlate with\ncommunity level social capital measures, while the second and fourth do not.\nTogether and individually, the factors are predictive of offline social capital\nmeasures, even controlling for various demographic attributes of the counties.\nOur analysis reveals striking patterns of correlation between established\nmeasures of social capital and patterns of online interaction in local Facebook\ngroups. To our knowledge this is the first systematic test of the association\nbetween offline regional social capital and patterns of online community\nengagement in the same regions.",
    "descriptor": "",
    "authors": [
      "Ama\u00e7 Herda\u011fdelen",
      "Lada Adamic",
      "Bogdan State"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12513"
  },
  {
    "id": "arXiv:2201.12514",
    "title": "Composing a surrogate observation operator for sequential data  assimilation",
    "abstract": "In data assimilation, state estimation is not straightforward when the\nobservation operator is unknown. This study proposes a method for composing a\nsurrogate operator for a true operator. The surrogate model is improved\niteratively to decrease the difference between the observations and the results\nof the surrogate model, and a neural network is adopted in the process. A twin\nexperiment suggests that the proposed method outperforms approaches that use a\nspecific operator that is given tentatively throughout the data assimilation\nprocess.",
    "descriptor": "",
    "authors": [
      "Kosuke Akita",
      "Yuto Miyatake",
      "Daisuke Furihata"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2201.12514"
  },
  {
    "id": "arXiv:2201.12515",
    "title": "Towards Fast and Accurate Federated Learning with non-IID Data for  Cloud-Based IoT Applications",
    "abstract": "As a promising method of central model training on decentralized device data\nwhile securing user privacy, Federated Learning (FL)is becoming popular in\nInternet of Things (IoT) design. However, when the data collected by IoT\ndevices are highly skewed in a non-independent and identically distributed\n(non-IID) manner, the accuracy of vanilla FL method cannot be guaranteed.\nAlthough there exist various solutions that try to address the bottleneck of FL\nwith non-IID data, most of them suffer from extra intolerable communication\noverhead and low model accuracy. To enable fast and accurate FL, this paper\nproposes a novel data-based device grouping approach that can effectively\nreduce the disadvantages of weight divergence during the training of non-IID\ndata. However, since our grouping method is based on the similarity of\nextracted feature maps from IoT devices, it may incur additional risks of\nprivacy exposure. To solve this problem, we propose an improved version by\nexploiting similarity information using the Locality-Sensitive Hashing (LSH)\nalgorithm without exposing extracted feature maps. Comprehensive experimental\nresults on well-known benchmarks show that our approach can not only accelerate\nthe convergence rate, but also improve the prediction accuracy for FL with\nnon-IID data.",
    "descriptor": "\nComments: FL\n",
    "authors": [
      "Tian Liu",
      "Jiahao Ding",
      "Ting Wang",
      "Miao Pan",
      "Mingsong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12515"
  },
  {
    "id": "arXiv:2201.12518",
    "title": "Zeroth-Order Actor-Critic",
    "abstract": "Zeroth-order optimization methods and policy gradient based first-order\nmethods are two promising alternatives to solve reinforcement learning (RL)\nproblems with complementary advantages. The former work with arbitrary\npolicies, drive state-dependent and temporally-extended exploration, possess\nrobustness-seeking property, but suffer from high sample complexity, while the\nlatter are more sample efficient but restricted to differentiable policies and\nthe learned policies are less robust. We propose Zeroth-Order Actor-Critic\nalgorithm (ZOAC) that unifies these two methods into an on-policy actor-critic\narchitecture to preserve the advantages from both. ZOAC conducts rollouts\ncollection with timestep-wise perturbation in parameter space, first-order\npolicy evaluation (PEV) and zeroth-order policy improvement (PIM) alternately\nin each iteration. We evaluate our proposed method on a range of challenging\ncontinuous control benchmarks using different types of policies, where ZOAC\noutperforms zeroth-order and first-order baseline algorithms.",
    "descriptor": "",
    "authors": [
      "Yuheng Lei",
      "Jianyu Chen",
      "Shengbo Eben Li",
      "Sifa Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12518"
  },
  {
    "id": "arXiv:2201.12519",
    "title": "It\u00f4Wave: It\u00f4 Stochastic Differential Equation Is All You Need For  Wave Generation",
    "abstract": "In this paper, we propose a vocoder based on a pair of forward and\nreverse-time linear stochastic differential equations (SDE). The solutions of\nthis SDE pair are two stochastic processes, one of which turns the distribution\nof wave, that we want to generate, into a simple and tractable distribution.\nThe other is the generation procedure that turns this tractable simple signal\ninto the target wave. The model is called It\\^oWave. It\\^oWave use the Wiener\nprocess as a driver to gradually subtract the excess signal from the noise\nsignal to generate realistic corresponding meaningful audio respectively, under\nthe conditional inputs of original mel spectrogram. The results of the\nexperiment show that the mean opinion scores (MOS) of It\\^oWave can exceed the\ncurrent state-of-the-art (SOTA) methods, and reached 4.35$\\pm$0.115. The\ngenerated audio samples are available online\\footnotemark[2].",
    "descriptor": "\nComments: ICASSP 2022. arXiv admin note: substantial text overlap with arXiv:2105.07583\n",
    "authors": [
      "Shoule Wu",
      "Ziqiang Shi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.12519"
  },
  {
    "id": "arXiv:2201.12522",
    "title": "Continual Learning with Recursive Gradient Optimization",
    "abstract": "Learning multiple tasks sequentially without forgetting previous knowledge,\ncalled Continual Learning(CL), remains a long-standing challenge for neural\nnetworks. Most existing methods rely on additional network capacity or data\nreplay. In contrast, we introduce a novel approach which we refer to as\nRecursive Gradient Optimization(RGO). RGO is composed of an iteratively updated\noptimizer that modifies the gradient to minimize forgetting without data replay\nand a virtual Feature Encoding Layer(FEL) that represents different long-term\nstructures with only task descriptors. Experiments demonstrate that RGO has\nsignificantly better performance on popular continual classification benchmarks\nwhen compared to the baselines and achieves new state-of-the-art performance on\n20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher\naverage accuracy than Single-Task Learning(STL), this method is flexible and\nreliable to provide continual learning capabilities for learning models that\nrely on gradient descent.",
    "descriptor": "",
    "authors": [
      "Hao Liu",
      "Huaping Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12522"
  },
  {
    "id": "arXiv:2201.12523",
    "title": "Efficient, Out-of-Memory Sparse MTTKRP on Massively Parallel  Architectures",
    "abstract": "Tensor decomposition (TD) is an important method for extracting latent\ninformation from high-dimensional (multi-modal) sparse data. This study\npresents a novel framework for accelerating fundamental TD operations on\nmassively parallel GPU architectures. In contrast to prior work, the proposed\nBlocked Linearized CoOrdinate (BLCO) format enables efficient out-of-memory\ncomputation of tensor algorithms using a unified implementation that works on a\nsingle tensor copy. Our adaptive blocking and linearization strategies not only\nmeet the resource constraints of GPU devices, but also accelerate data\nindexing, eliminate control-flow and memory-access irregularities, and reduce\nkernel launching overhead. To address the substantial synchronization cost on\nGPUs, we introduce an opportunistic conflict resolution algorithm that\ndiscovers and resolves conflicting updates across threads on-the-fly, without\nkeeping any auxiliary information or storing non-zero elements in specific mode\norientations. As a result, our framework delivers superior in-memory\nperformance compared to prior state-of-the-art, and is the only framework\ncapable of processing out-of-memory tensors. On the latest Intel and NVIDIA\nGPUs, BLCO achieves 2.12-2.6X geometric-mean speedup (with up to 33.35X\nspeedup) over the state-of-the-art mixed-mode compressed sparse fiber (MM-CSF)\non a range of real-world sparse tensors.",
    "descriptor": "",
    "authors": [
      "Andy Nguyen",
      "Ahmed E. Helal",
      "Fabio Checconi",
      "Jan Laukemann",
      "Jesmin Jahan Tithi",
      "Yongseok Soh",
      "Teresa Ranadive",
      "Fabrizio Petrini",
      "Jee W. Choi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2201.12523"
  },
  {
    "id": "arXiv:2201.12525",
    "title": "Spherical Convolution empowered FoV Prediction in 360-degree Video  Multicast with Limited FoV Feedback",
    "abstract": "Field of view (FoV) prediction is critical in 360-degree video multicast,\nwhich is a key component of the emerging Virtual Reality (VR) and Augmented\nReality (AR) applications. Most of the current prediction methods combining\nsaliency detection and FoV information neither take into account that the\ndistortion of projected 360-degree videos can invalidate the weight sharing of\ntraditional convolutional networks, nor do they adequately consider the\ndifficulty of obtaining complete multi-user FoV information, which degrades the\nprediction performance. This paper proposes a spherical convolution-empowered\nFoV prediction method, which is a multi-source prediction framework combining\nsalient features extracted from 360-degree video with limited FoV feedback\ninformation. A spherical convolution neural network (CNN) is used instead of a\ntraditional two-dimensional CNN to eliminate the problem of weight sharing\nfailure caused by video projection distortion. Specifically, salient\nspatial-temporal features are extracted through a spherical convolution-based\nsaliency detection model, after which the limited feedback FoV information is\nrepresented as a time-series model based on a spherical convolution-empowered\ngated recurrent unit network. Finally, the extracted salient video features are\ncombined to predict future user FoVs. The experimental results show that the\nperformance of the proposed method is better than other prediction methods.",
    "descriptor": "",
    "authors": [
      "Jie Li",
      "Ling Han",
      "Cong Zhang",
      "Qiyue Li",
      "Zhi Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2201.12525"
  },
  {
    "id": "arXiv:2201.12527",
    "title": "Scale-Invariant Adversarial Attack for Evaluating and Enhancing  Adversarial Defenses",
    "abstract": "Efficient and effective attacks are crucial for reliable evaluation of\ndefenses, and also for developing robust models. Projected Gradient Descent\n(PGD) attack has been demonstrated to be one of the most successful adversarial\nattacks. However, the effect of the standard PGD attack can be easily weakened\nby rescaling the logits, while the original decision of every input will not be\nchanged. To mitigate this issue, in this paper, we propose Scale-Invariant\nAdversarial Attack (SI-PGD), which utilizes the angle between the features in\nthe penultimate layer and the weights in the softmax layer to guide the\ngeneration of adversaries. The cosine angle matrix is used to learn angularly\ndiscriminative representation and will not be changed with the rescaling of\nlogits, thus making SI-PGD attack to be stable and effective. We evaluate our\nattack against multiple defenses and show improved performance when compared\nwith existing attacks. Further, we propose Scale-Invariant (SI) adversarial\ndefense mechanism based on the cosine angle matrix, which can be embedded into\nthe popular adversarial defenses. The experimental results show the defense\nmethod with our SI mechanism achieves state-of-the-art performance among\nmulti-step and single-step defenses.",
    "descriptor": "\nComments: TDSC under review\n",
    "authors": [
      "Mengting Xu",
      "Tao Zhang",
      "Zhongnian Li",
      "Daoqiang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12527"
  },
  {
    "id": "arXiv:2201.12528",
    "title": "SupWMA: Consistent and Efficient Tractography Parcellation of  Superficial White Matter with Deep Learning",
    "abstract": "White matter parcellation classifies tractography streamlines into clusters\nor anatomically meaningful tracts to enable quantification and visualization.\nMost parcellation methods focus on the deep white matter (DWM), while fewer\nmethods address the superficial white matter (SWM) due to its complexity. We\npropose a deep-learning-based framework, Superficial White Matter Analysis\n(SupWMA), that performs an efficient and consistent parcellation of 198 SWM\nclusters from whole-brain tractography. A point-cloud-based network is modified\nfor our SWM parcellation task, and supervised contrastive learning enables more\ndiscriminative representations between plausible streamlines and outliers. We\nperform evaluation on a large tractography dataset with ground truth labels and\non three independently acquired testing datasets from individuals across ages\nand health conditions. Compared to several state-of-the-art methods, SupWMA\nobtains a highly consistent and accurate SWM parcellation result. In addition,\nthe computational speed of SupWMA is much faster than other methods.",
    "descriptor": "\nComments: ISBI 2022 Oral\n",
    "authors": [
      "Tengfei Xue",
      "Fan Zhang",
      "Chaoyi Zhang",
      "Yuqian Chen",
      "Yang Song",
      "Nikos Makris",
      "Yogesh Rathi",
      "Weidong Cai",
      "Lauren J. O'Donnell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12528"
  },
  {
    "id": "arXiv:2201.12532",
    "title": "Rethinking Adjacent Dependency in Session-based Recommendations",
    "abstract": "Session-based recommendations (SBRs) recommend the next item for an anonymous\nuser by modeling the dependencies between items in a session. Benefiting from\nthe superiority of graph neural networks (GNN) in learning complex\ndependencies, GNN-based SBRs have become the main stream of SBRs in recent\nyears. Most GNN-based SBRs are based on a strong assumption of adjacent\ndependency, which means any two adjacent items in a session are necessarily\ndependent here. However, based on our observation, the adjacency does not\nnecessarily indicate dependency due to the uncertainty and complexity of user\nbehaviours. Therefore, the aforementioned assumption does not always hold in\nthe real-world cases and thus easily leads to two deficiencies: (1) the\nintroduction of false dependencies between items which are adjacent in a\nsession but are not really dependent, and (2) the missing of true dependencies\nbetween items which are not adjacent but are actually dependent. Such\ndeficiencies significantly downgrade accurate dependency learning and thus\nreduce the recommendation performance. Aiming to address these deficiencies, we\npropose a novel review-refined inter-item graph neural network (RI-GNN), which\nutilizes the topic information extracted from items' reviews to refine\ndependencies between items. Experiments on two public real-world datasets\ndemonstrate that RI-GNN outperforms the state-of-the-art methods.",
    "descriptor": "\nComments: 12 pages, 4 figures, conference\n",
    "authors": [
      "Qian Zhang",
      "Shoujin Wang",
      "Wenpeng Lu",
      "Chong Feng",
      "Xueping Peng",
      "Qingxiang Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12532"
  },
  {
    "id": "arXiv:2201.12533",
    "title": "Light field Rectification based on relative pose estimation",
    "abstract": "Hand-held light field (LF) cameras have unique advantages in computer vision\nsuch as 3D scene reconstruction and depth estimation. However, the related\napplications are limited by the ultra-small baseline, e.g., leading to the\nextremely low depth resolution in reconstruction. To solve this problem, we\npropose to rectify LF to obtain a large baseline. Specifically, the proposed\nmethod aligns two LFs captured by two hand-held LF cameras with a random\nrelative pose, and extracts the corresponding row-aligned sub-aperture images\n(SAIs) to obtain an LF with a large baseline. For an accurate rectification, a\nmethod for pose estimation is also proposed, where the relative rotation and\ntranslation between the two LF cameras are estimated. The proposed pose\nestimation minimizes the degree of freedom (DoF) in the LF-point-LF-point\ncorrespondence model and explicitly solves this model in a linear way. The\nproposed pose estimation outperforms the state-of-the-art algorithms by\nproviding more accurate results to support rectification. The significantly\nimproved depth resolution in 3D reconstruction demonstrates the effectiveness\nof the proposed LF rectification.",
    "descriptor": "",
    "authors": [
      "Xiao Huo",
      "Dongyang Jin",
      "Saiping Zhang",
      "Fuzheng Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12533"
  },
  {
    "id": "arXiv:2201.12538",
    "title": "Incorporating Commonsense Knowledge into Story Ending Generation via  Heterogeneous Graph Networks",
    "abstract": "Story ending generation is an interesting and challenging task, which aims to\ngenerate a coherent and reasonable ending given a story context. The key\nchallenges of the task lie in how to comprehend the story context sufficiently\nand handle the implicit knowledge behind story clues effectively, which are\nstill under-explored by previous work. In this paper, we propose a Story\nHeterogeneous Graph Network (SHGN) to explicitly model both the information of\nstory context at different granularity levels and the multi-grained interactive\nrelations among them. In detail, we consider commonsense knowledge, words and\nsentences as three types of nodes. To aggregate non-local information, a global\nnode is also introduced. Given this heterogeneous graph network, the node\nrepresentations are updated through graph propagation, which adequately\nutilizes commonsense knowledge to facilitate story comprehension. Moreover, we\ndesign two auxiliary tasks to implicitly capture the sentiment trend and key\nevents lie in the context. The auxiliary tasks are jointly optimized with the\nprimary story ending generation task in a multi-task learning strategy.\nExtensive experiments on the ROCStories Corpus show that the developed model\nachieves new state-of-the-art performances. Human study further demonstrates\nthat our model generates more reasonable story endings.",
    "descriptor": "\nComments: DASFAA 2022\n",
    "authors": [
      "Jiaan Wang",
      "Beiqi Zou",
      "Zhixu Li",
      "Jianfeng Qu",
      "Pengpeng Zhao",
      "An Liu",
      "Lei Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12538"
  },
  {
    "id": "arXiv:2201.12542",
    "title": "Aper: Evolution-Aware Runtime Permission Misuse Detection for Android  Apps",
    "abstract": "The Android platform introduces the runtime permission model in version 6.0.\nThe new model greatly improves data privacy and user experience, but brings new\nchallenges for app developers. First, it allows users to freely revoke granted\npermissions. Hence, developers cannot assume that the permissions granted to an\napp would keep being granted. Instead, they should make their apps carefully\ncheck the permission status before invoking dangerous APIs. Second, the\npermission specification keeps evolving, bringing new types of compatibility\nissues into the ecosystem. To understand the impact of the challenges, we\nconducted an empirical study on 13,352 popular Google Play apps. We found that\n86.0% apps used dangerous APIs asynchronously after permission management and\n61.2% apps used evolving dangerous APIs. If an app does not properly handle\npermission revocations or platform differences, unexpected runtime issues may\nhappen and even cause app crashes. We call such Android Runtime Permission\nissues as ARP bugs. Unfortunately, existing runtime permission issue detection\ntools cannot effectively deal with the ARP bugs induced by asynchronous\npermission management and permission specification evolution. To fill the gap,\nwe designed a static analyzer, Aper, that performs reaching definition and\ndominator analysis on Android apps to detect the two types of ARP bugs. To\ncompare Aper with existing tools, we built a benchmark, ARPfix, from 60 real\nARP bugs. Our experiment results show that Aper significantly outperforms two\nacademic tools, ARPDroid and RevDroid, and an industrial tool, Lint, on ARPfix,\nwith an average improvement of 46.3% on F1-score. In addition, Aper\nsuccessfully found 34 ARP bugs in 214 opensource Android apps, most of which\ncan result in abnormal app behaviors (such as app crashes) according to our\nmanual validation.",
    "descriptor": "",
    "authors": [
      "Sinan Wang",
      "Yibo Wang",
      "Xian Zhan",
      "Ying Wang",
      "Yepang Liu",
      "Xiapu Luo",
      "Shing-Chi Cheung"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.12542"
  },
  {
    "id": "arXiv:2201.12543",
    "title": "Fast Differentiable Matrix Square Root and Inverse Square Root",
    "abstract": "Computing the matrix square root and its inverse in a differentiable manner\nis important in a variety of computer vision tasks. Previous methods either\nadopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix\nor use the Newton-Schulz iteration (NS iteration) to derive the approximate\nsolution. However, both methods are not computationally efficient enough in\neither the forward pass or the backward pass. In this paper, we propose two\nmore efficient variants to compute the differentiable matrix square root and\nthe inverse square root. For the forward propagation, one method is to use\nMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'e\nApproximants (MPA). The backward gradient is computed by iteratively solving\nthe continuous-time Lyapunov equation using the matrix sign function. A series\nof numerical tests show that both methods yield considerable speed-up compared\nwith the SVD or the NS iteration. Moreover, we validate the effectiveness of\nour methods in several real-world applications, including de-correlated batch\nnormalization, second-order vision transformer, global covariance pooling for\nlarge-scale and fine-grained recognition, attentive covariance pooling for\nvideo recognition, and neural style transfer. The experimental results\ndemonstrate that our methods can also achieve competitive and even slightly\nbetter performances. The Pytorch implementation is available at\n\\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}.",
    "descriptor": "\nComments: Under review by T-PAMI. arXiv admin note: substantial text overlap with arXiv:2201.08663\n",
    "authors": [
      "Yue Song",
      "Nicu Sebe",
      "Wei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12543"
  },
  {
    "id": "arXiv:2201.12544",
    "title": "An Open Data and Geo-based Information Systems",
    "abstract": "Barangay is the smallest type of government in the Philippines, and it is\ndriven and represented by its barangay authorities. The barangay officials are\naccountable for keeping the records of citizens health and crime incidents. It\nalso the first-hand source of information of the national government to develop\ngovernment programs, community services, and maintain peace and order. This\npaper presents a developed a web-based information system incorporating open\ndata and geo-based features for a pilot community in the Philippines. This\nsystem serves as a platform for information collection and used for planning,\nanalysis, decision-making and increase effectiveness and efficiency of\ngovernment services in the community.",
    "descriptor": "\nComments: None\n",
    "authors": [
      "Dexter I. Mercurio",
      "Alexander A. Hernandez"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.12544"
  },
  {
    "id": "arXiv:2201.12546",
    "title": "Progressive Continual Learning for Spoken Keyword Spotting",
    "abstract": "Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. To tackle such challenges, we propose a\nprogressive continual learning strategy for small-footprint spoken keyword\nspotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a\nnetwork instantiator to generate the task-specific sub-networks for remembering\npreviously learned keywords. As a result, the PCL-KWS approach incrementally\nlearns new keywords without forgetting prior knowledge. Besides, the\nkeyword-aware network scaling mechanism of PCL-KWS constrains the growth of\nmodel parameters while achieving high performance. Experimental results show\nthat after learning five new tasks sequentially, our proposed PCL-KWS approach\narchives the new state-of-the-art performance of 92.8% average accuracy for all\nthe tasks on Google Speech Command dataset compared with other baselines.",
    "descriptor": "\nComments: ICASSP 2022\n",
    "authors": [
      "Yizheng Huang",
      "Nana Hou",
      "Nancy F. Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.12546"
  },
  {
    "id": "arXiv:2201.12548",
    "title": "Transport Capacity Optimization for Resource Allocation in Tera-IoT  Networks",
    "abstract": "We present a new adaptive resource optimization strategy that jointly\nallocates the subwindow and transmit power in multi-device terahertz (THz) band\nInternet of Things (Tera-IoT) networks. Unlike the prior studies focusing\nmostly on maximizing the sum distance, we incorporate both rate and\ntransmission distance into the objective function of our problem formulation\nwith key features of THz bands, including the spreading and molecular\nabsorption losses. More specifically, as a performance metric of Tera-IoT\nnetworks, we adopt the transport capacity (TC), which is defined as the sum of\nthe rate-distance products over all users. This metric has been widely adopted\nin large-scale ad hoc networks, and would also be appropriate for evaluating\nthe performance of various Tera-IoT applications. We then formulate an\noptimization problem that aims at maximizing the TC. Moreover, motivated by the\nimportance of the transmission distance that is very limited due to the high\npath loss in THz bands, our optimization problem is extended to the case of\nallocating the subwindow, transmit power, and transmission distance. We show\nhow to solve our problems via an effective two-stage resource allocation\nstrategy. We demonstrate the superiority of our adaptive solution over\nbenchmark methods via intensive numerical evaluations for various environmental\nsetups of large-scale Tera-IoT networks.",
    "descriptor": "\nComments: 15 pages, 10 figures; to appear in the IEEE Internet of Things Journal (Please cite our journal version that will appear in an upcoming issue.)\n",
    "authors": [
      "Cheol Jeong",
      "Chang-Jae Chun",
      "Won-Yong Shin",
      "Il-Min Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.12548"
  },
  {
    "id": "arXiv:2201.12549",
    "title": "A Simple Information-Based Approach to Unsupervised Domain-Adaptive  Aspect-Based Sentiment Analysis",
    "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask which aims to extract the aspects from sentences and identify their\ncorresponding sentiments. Aspect term extraction (ATE) is the crucial step for\nABSA. Due to the expensive annotation for aspect terms, we often lack labeled\ntarget domain data for fine-tuning. To address this problem, many approaches\nhave been proposed recently to transfer common knowledge in an unsupervised\nway, but such methods have too many modules and require expensive multi-stage\npreprocessing. In this paper, we propose a simple but effective technique based\non mutual information maximization, which can serve as an additional component\nto enhance any kind of model for cross-domain ABSA and ATE. Furthermore, we\nprovide some analysis of this approach. Experiment results show that our\nproposed method outperforms the state-of-the-art methods for cross-domain ABSA\nby 4.32% Micro-F1 on average over 10 different domain pairs. Apart from that,\nour method can be extended to other sequence labeling tasks, such as named\nentity recognition (NER).",
    "descriptor": "\nComments: 11 pages, 3 figures, 10 tables\n",
    "authors": [
      "Xiang Chen",
      "Xiaojun Wan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12549"
  },
  {
    "id": "arXiv:2201.12558",
    "title": "The KFIoU Loss for Rotated Object Detection",
    "abstract": "Differing from the well-developed horizontal object detection area whereby\nthe computing-friendly IoU based loss is readily adopted and well fits with the\ndetection metrics. In contrast, rotation detectors often involve a more\ncomplicated loss based on SkewIoU which is unfriendly to gradient-based\ntraining. In this paper, we argue that one effective alternative is to devise\nan approximate loss who can achieve trend-level alignment with SkewIoU loss\ninstead of the strict value-level identity. Specifically, we model the objects\nas Gaussian distribution and adopt Kalman filter to inherently mimic the\nmechanism of SkewIoU by its definition, and show its alignment with the SkewIoU\nat trend-level. This is in contrast to recent Gaussian modeling based rotation\ndetectors e.g. GWD, KLD that involves a human-specified distribution distance\nmetric which requires additional hyperparameter tuning. The resulting new loss\ncalled KFIoU is easier to implement and works better compared with exact\nSkewIoU, thanks to its full differentiability and ability to handle the\nnon-overlapping cases. We further extend our technique to the 3-D case which\nalso suffers from the same issues as 2-D detection. Extensive results on\nvarious public datasets (2-D/3-D, aerial/text/face images) with different base\ndetectors show the effectiveness of our approach.",
    "descriptor": "\nComments: 19 pages, 5 figures, 11 tables\n",
    "authors": [
      "Xue Yang",
      "Yue Zhou",
      "Gefan Zhang",
      "Jitui Yang",
      "Wentao Wang",
      "Junchi Yan",
      "Xiaopeng Zhang",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12558"
  },
  {
    "id": "arXiv:2201.12559",
    "title": "Task-Balanced Batch Normalization for Exemplar-based Class-Incremental  Learning",
    "abstract": "Batch Normalization (BN) is an essential layer for training neural network\nmodels in various computer vision tasks. It has been widely used in continual\nlearning scenarios with little discussion, but we find that BN should be\ncarefully applied, particularly for the exemplar memory based class incremental\nlearning (CIL). We first analyze that the empirical mean and variance obtained\nfor normalization in a BN layer become highly biased toward the current task.\nTo tackle its significant problems in training and test phases, we propose\nTask-Balanced Batch Normalization (TBBN). Given each mini-batch imbalanced\nbetween the current and previous tasks, TBBN first reshapes and repeats the\nbatch, calculating near task-balanced mean and variance. Second, we show that\nwhen the affine transformation parameters of BN are learned from a reshaped\nfeature map, they become less-biased toward the current task. Based on our\nextensive CIL experiments with CIFAR-100 and ImageNet-100 datasets, we\ndemonstrate that our TBBN is easily applicable to most of existing\nexemplar-based CIL algorithms, improving their performance by decreasing the\nforgetting on the previous tasks.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Sungmin Cha",
      "Soonwon Hong",
      "Moontae Lee",
      "Taesup Moon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12559"
  },
  {
    "id": "arXiv:2201.12560",
    "title": "Sim-to-Real for Projective Dynamics on Soft Robots via  Differentiability: Meshing, Damping, and Actuation",
    "abstract": "An accurate, physically-based model of soft robots can unlock downstream\napplications in optimal control. The Finite Element Method (FEM) is an\nexpressive approach for modeling highly deformable structures such as dynamic,\nelastomeric soft robots. Recently, Projective Dynamics (PD) has been proposed\nas a fast FEM; however, PD lacks rigorous benchmarking against reality. In this\npaper, we compare virtual robot models simulated using PD with measurements\nfrom their physical counterparts. In particular, we examine several soft\nstructures with different morphologies: a clamped beam under external force, a\npneumatically actuated soft robotic arm, and a soft robotic fish tail. We\nbenchmark and analyze different meshing resolutions and elements (tetrahedra\nand hexahedra), numerical damping, and the differentiability of PD through a\ndifferentiable solution (DiffPD). We also advance PD in application to soft\nrobotics by proposing a predictive model for pneumatic soft robot actuation.\nThrough our case-studies, we provide strategies and algorithms for matching\nreal-world physics in simulation, making PD useful for soft robots.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Mathieu Dubied",
      "Mike Michelis",
      "Andrew Spielberg",
      "Robert Katzschmann"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12560"
  },
  {
    "id": "arXiv:2201.12563",
    "title": "Dissimilar Redundancy in DeFi",
    "abstract": "The meteoric rise of Decentralized Finance (DeFi) has been accompanied by a\nplethora of frequent and often financially devastating attacks on its protocols\nThere have been over 70 exploits of DeFi protocols, with the total of lost\nfunds amounting to approximately 1.5bn USD. In this paper, we introduce a new\napproach to minimizing the frequency and severity of such attacks: dissimilar\nredundancy for smart contracts. In a nutshell, the idea is to implement a\nprogram logic more than once, ideally using different programming languages.\nThen, for each implementation, the results should match before allowing the\nstate of the blockchain to change. This is inspired by and has clear parallels\nto the field of avionics, where on account of the safety-critical environment,\nflight control systems typically feature multiple redundant implementations. We\nargue that the high financial stakes in DeFi protocols merit a conceptually\nsimilar approach, and we provide a novel algorithm for implementing dissimilar\nredundancy for smart contracts.",
    "descriptor": "",
    "authors": [
      "Daniel Perez",
      "Lewis Gudgeon"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12563"
  },
  {
    "id": "arXiv:2201.12565",
    "title": "Active IRS Aided Multiple Access for Energy-Constrained IoT Systems",
    "abstract": "We investigate the fundamental multiple access (MA) scheme in an active\nintelligent reflecting surface (IRS) aided energy-constrained\nInternet-of-Things (IoT) system, where an active IRS is deployed to assist the\nuplink transmission from multiple IoT devices to an access point (AP). Our goal\nis to maximize the sum throughput by optimizing the IRS beamforming vectors\nacross time and resource allocation. To this end, we first study two typical\nactive IRS aided MA schemes, namely time division multiple access (TDMA) and\nnon-orthogonal multiple access (NOMA), by analytically comparing their\nachievable sum throughput and proposing corresponding algorithms.\nInterestingly, we prove that given only one available IRS beamforming vector,\nthe NOMA-based scheme generally achieves a larger throughput than the\nTDMA-based scheme, whereas the latter can potentially outperform the former if\nmultiple IRS beamforming vectors are available to harness the favorable time\nselectivity of the IRS. To strike a flexible balance between the system\nperformance and the associated signaling overhead incurred by more IRS\nbeamforming vectors, we then propose a general hybrid TDMA-NOMA scheme with\nuser grouping, where the devices in the same group transmit simultaneously via\nNOMA while devices in different groups occupy orthogonal time slots. By\ncontrolling the number of groups, the hybrid TDMA-NOMA scheme is applicable for\nany given number of IRS beamforming vectors available. Despite of the\nnon-convexity of the considered optimization problem, we propose an efficient\nalgorithm based on alternating optimization. Simulation results illustrate the\npractical superiorities of the active IRS over the passive IRS in terms of the\ncoverage extension and supporting multiple energy-limited devices, and\ndemonstrate the effectiveness of our proposed hybrid MA scheme for flexibly\nbalancing the performance-cost tradeoff.",
    "descriptor": "",
    "authors": [
      "Guangji Chen",
      "Qingqing Wu",
      "Chong He",
      "Wen Chen",
      "Jie Tang",
      "Shi Jin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12565"
  },
  {
    "id": "arXiv:2201.12567",
    "title": "The HCCL-DKU system for fake audio generation task of the 2022 ICASSP  ADD Challenge",
    "abstract": "The voice conversion task is to modify the speaker identity of continuous\nspeech while preserving the linguistic content. Generally, the naturalness and\nsimilarity are two main metrics for evaluating the conversion quality, which\nhas been improved significantly in recent years. This paper presents the\nHCCL-DKU entry for the fake audio generation task of the 2022 ICASSP ADD\nchallenge. We propose a novel ppg-based voice conversion model that adopts a\nfully end-to-end structure. Experimental results show that the proposed method\noutperforms other conversion models, including Tacotron-based and\nFastspeech-based models, on conversion quality and spoofing performance against\nanti-spoofing systems. In addition, we investigate several post-processing\nmethods for better spoofing power. Finally, we achieve second place with a\ndeception success rate of 0.916 in the ADD challenge.",
    "descriptor": "",
    "authors": [
      "Ziyi Chen",
      "Hua Hua",
      "Yuxiang Zhang",
      "Ming Li",
      "Pengyuan Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.12567"
  },
  {
    "id": "arXiv:2201.12568",
    "title": "Le Processus Powered Dirichlet-Hawkes comme A Priori Flexible pour  Clustering Temporel de Textes",
    "abstract": "The textual content of a document and its publication date are intertwined.\nFor example, the publication of a news article on a topic is influenced by\nprevious publications on similar issues, according to underlying temporal\ndynamics. However, it can be challenging to retrieve meaningful information\nwhen textual information conveys little. Furthermore, the textual content of a\ndocument is not always correlated to its temporal dynamics. We develop a method\nto create clusters of textual documents according to both their content and\npublication time, the Powered Dirichlet-Hawkes process (PDHP). PDHP yields\nsignificantly better results than state-of-the-art models when temporal\ninformation or textual content is weakly informative. PDHP also alleviates the\nhypothesis that textual content and temporal dynamics are perfectly correlated.\nWe demonstrate that PDHP generalizes previous work --such as DHP and UP.\nFinally, we illustrate a possible application using a real-world dataset from\nReddit.",
    "descriptor": "\nComments: in French\n",
    "authors": [
      "Ga\u00ebl Poux-M\u00e9dard",
      "Julien Velcin",
      "Sabine Loudcher"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12568"
  },
  {
    "id": "arXiv:2201.12569",
    "title": "Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal  Point Processes",
    "abstract": "We consider a sequential decision making problem where the agent faces the\nenvironment characterized by the stochastic discrete events and seeks an\noptimal intervention policy such that its long-term reward is maximized. This\nproblem exists ubiquitously in social media, finance and health informatics but\nis rarely investigated by the conventional research in reinforcement learning.\nTo this end, we present a novel framework of the model-based reinforcement\nlearning where the agent's actions and observations are asynchronous stochastic\ndiscrete events occurring in continuous-time. We model the dynamics of the\nenvironment by Hawkes process with external intervention control term and\ndevelop an algorithm to embed such process in the Bellman equation which guides\nthe direction of the value gradient. We demonstrate the superiority of our\nmethod in both synthetic simulator and real-world problem.",
    "descriptor": "",
    "authors": [
      "Chao Qu",
      "Xiaoyu Tan",
      "Siqiao Xue",
      "Xiaoming Shi",
      "James Zhang",
      "Hongyuan Mei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12569"
  },
  {
    "id": "arXiv:2201.12571",
    "title": "Probabilistic Power Flow Calculation of AC/DC Hybrid System Based on  Cumulant Method",
    "abstract": "The operating conditions of the power system have become more complex and\nchangeable. This paper proposes a probabilistic power flow calculation method\nbased on the cumulant method for the voltage sourced converter high voltage\ndirect current (VSC-HVDC) hybrid system containing photovoltaic grid-connected\nsystems. Firstly, the corresponding control mode is set for the converter,\nincluding droop control and master-slave control. The unified iterative method\nis used to calculate the conventional AC/DC power flow. Secondly, on the basis\nof the probability model of load and photovoltaic output, based on the\naforementioned power flow results, use correlation coefficient matrix of this\npaper will change the relevant sample into independent sample, the cumulants of\nthe load and photovoltaic output are obtained; Then, the probability density\nfunction (PDF) and cumulative distribution function (CDF) of state variables\nare obtained by using Gram-Charlie series expansion method. Finally, the mean\nvalue and standard deviation of node voltage and line power are calculated on\nthe modified IEEE 34-bus and IEEE 57-bus transmission systems. The algorithm\ncan reflect the inherent uncertainty of new energy sources, and replace the\ncomplex convolution operation, greatly improving the calculation speed and the\nconvergence.",
    "descriptor": "\nComments: Accepted by International Journal of Electrical Power & Energy Systems\n",
    "authors": [
      "Yinfeng Sun",
      "Dapeng Xia",
      "Zichun Gao",
      "Zhenhao Wang",
      "Guoqing Li",
      "Weihua Lu",
      "Xueguang Wu",
      "Yang Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2201.12571"
  },
  {
    "id": "arXiv:2201.12572",
    "title": "Logical Pseudocode: Connecting Algorithms with Proofs",
    "abstract": "Proofs (sequent calculus, natural deduction) and imperative algorithms\n(pseudocodes) are two well-known coexisting concepts. Then what is their\nrelationship? Our answer is that\n\\[ imperative\\ algorithms\\ =\\ proofs\\ with\\ cuts \\]\nThis observation leads to a generalization to pseudocodes which we call {\\it\nlogical pseudocodes}. It is similar to natural deduction proof of computability\nlogic\\cite{Jap03,Jap08}. Each statement in it corresponds to a proof step in\nnatural deduction. Therefore, the merit over pseudocode is that each statement\nis guaranteed to be correct and safe with respect to the initial\nspecifications. It can also be seen as an extension to computability logic web\n(\\colw) with forward reasoning capability.",
    "descriptor": "\nComments: 4 pages. Logical pseudocode aims at serving as a new programming language and a new deductive system as well. system. arXiv admin note: text overlap with arXiv:2108.10728\n",
    "authors": [
      "Keehang Kwon",
      "Hyung Joon Kwon"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2201.12572"
  },
  {
    "id": "arXiv:2201.12576",
    "title": "Scale-arbitrary Invertible Image Downscaling",
    "abstract": "Downscaling is indispensable when distributing high-resolution (HR) images\nover the Internet to fit the displays of various resolutions, while upscaling\nis also necessary when users want to see details of the distributed images.\nRecent invertible image downscaling methods jointly model these two problems\nand achieve significant improvements. However, they only consider fixed integer\nscale factors that cannot meet the requirement of conveniently fitting the\ndisplays of various resolutions in real-world applications. In this paper, we\npropose a scale-Arbitrary Invertible image Downscaling Network (AIDN), to\nnatively downscale HR images with arbitrary scale factors for fitting various\ntarget resolutions. Meanwhile, the HR information is embedded in the downscaled\nlow-resolution (LR) counterparts in a nearly imperceptible form such that our\nAIDN can also restore the original HR images solely from the LR images. The key\nto supporting arbitrary scale factors is our proposed Conditional Resampling\nModule (CRM) that conditions the downscaling/upscaling kernels and sampling\nlocations on both scale factors and image content. Extensive experimental\nresults demonstrate that our AIDN achieves top performance for invertible\ndownscaling with both arbitrary integer and non-integer scale factors.",
    "descriptor": "",
    "authors": [
      "Jinbo Xing",
      "Wenbo Hu",
      "Tien-Tsin Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12576"
  },
  {
    "id": "arXiv:2201.12577",
    "title": "A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks  (Inference)",
    "abstract": "In this work, we present $\\texttt{Volley Revolver}$, a novel matrix-encoding\nmethod that is particularly convenient for privacy-preserving neural networks\nto make predictions, and use it to implement a CNN for handwritten image\nclassification. Based on this encoding method, we develop several additional\noperations for putting into practice the secure matrix multiplication over\nencrypted data matrices. For two matrices $A$ and $B$ to perform multiplication\n$A \\times B$, the main idea is, in a simple version, to encrypt matrix $A$ and\nthe transposition of the matrix $B$ into two ciphertexts respectively. Along\nwith the additional operations, the homomorphic matrix multiplication $A \\times\nB$ can be calculated over encrypted data matrices efficiently. For the\nconvolution operation in CNN, on the basis of the $\\texttt{Volley Revolver}$\nencoding method, we develop a feasible and efficient evaluation strategy for\nperforming the convolution operation. We in advance span each convolution\nkernel of CNN to a matrix space of the same size as the input image so as to\ngenerate several ciphertexts, each of which is later used together with the\ninput image for calculating some part of the final convolution result. We\naccumulate all these part results of convolution operation and thus obtain the\nfinal convolution result.",
    "descriptor": "\nComments: The encoding method we proposed in this work, $\\texttt{Volley Revolver}$, is particularly tailored for privacy-preserving neural networks and probably can be used to assist the private neural networks training, in which case for the backpropagation algorithm of the fully connected layer the first matrix is revolved while the second matrix is settled to be still\n",
    "authors": [
      "John Chiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12577"
  },
  {
    "id": "arXiv:2201.12581",
    "title": "Integrated Sensing and Over-the-Air Computation: Dual-Functional MIMO  Beamforming Design",
    "abstract": "To support the unprecedented growth of the Internet of Things (IoT)\napplications and the access of tremendous IoT devices, two new technologies\nemerge recently to overcome the shortage of spectrum resources. The first one,\nknown as integrated sensing and communication (ISAC), aims to share the\nspectrum bandwidth for both radar sensing and data communication. The second\none, called over-the-air computation (AirComp), enables simultaneous\ntransmission and computation of data from multiple IoT devices in the same\nfrequency. The promising performance of ISAC and AirComp motivates the current\nwork on developing a framework that combines the merits of both called\nintegrated sensing and AirComp (ISAA). Two schemes are designed to support\nmultiple-input-multiple-output (MIMO) ISAA simultaneously, namely the shared\nand separated schemes. The performance metrics of radar sensing and AirComp are\nevaluated by the mean square errors of the estimated target response matrix and\nthe received computation results, respectively. The design challenge of MIMO\nISAA lies in the joint optimization of radar sensing beamformers and data\ntransmission beamformers at the IoT devices, and data aggregation beamformer at\nthe server, which results in complex non-convex problem. To solve this problem,\nan algorithmic solution based on the technique of semidefinite relaxation is\nproposed. The results reveal that the beamformer at each sensor needs to\naccount for supporting dual-functional signals in the shared scheme, while\ndedicated beamformers for sensing and AirComp are needed to mitigate the mutual\ninterference between the two functionalities in the separated scheme. The use\ncase of target location estimation based on ISAA is demonstrated in simulation\nto show the performance superiority.",
    "descriptor": "",
    "authors": [
      "Xiaoyang Li",
      "Fan Liu",
      "Ziqin Zhou",
      "Guangxu Zhu",
      "Shuai Wang",
      "Kaibin Huang",
      "Yi Gong"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12581"
  },
  {
    "id": "arXiv:2201.12583",
    "title": "Joint Sensing and Communication Rates Control for Energy Efficient  Mobile Crowd Sensing",
    "abstract": "Driven by the fast development of Internet of Things (IoT) applications,\ntremendous data need to be collected by sensors and passed to the servers for\nfurther process. As a promising solution, the mobile crowd sensing (MCS)\nenables controllable sensing and transmission processes for multiple types of\ndata in a single device. To achieve the energy efficient MCS, the data sensing\nand transmission over a long-term time duration should be designed accounting\nfor the differentiated requirements of IoT tasks including data size and delay\ntolerance. The said design is achieved by jointly optimizing the sensing and\ntransmission rates, which leads to a complex optimization problem due to the\nrestraining relationship between the controlling variables as well as the\nexistence of busy time interval during which no data can be sensed. To deal\nwith such problem, a vital concept namely height is introduced, based on which\nthe classical string-pulling algorithms can be applied for obtaining the\ncorresponding optimal sensing and transmission rates. Therefore, the original\nrates optimization problem can be converted to a searching problem for the\noptimal height. Based on the property of the objective function, the upper and\nlower bounds of the area where the optimal height lies in are derived. The\nwhole searching area is further divided into a series of sub-areas due to the\nformat change of the objective function with the varying heights. Finally, the\noptimal height in each sub-area is obtained based on the convexity of the\nobjective function and the global optimal height is further determined by\ncomparing the local optimums. The above solving approach is further extended\nfor the case with limited data buffer capacity of the server. Simulations are\nconducted to evaluate the performance of the proposed design.",
    "descriptor": "",
    "authors": [
      "Ziqin Zhou",
      "Xiaoyang Li",
      "Changsheng You",
      "Kaibing Huang",
      "Yi Gong"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12583"
  },
  {
    "id": "arXiv:2201.12585",
    "title": "LBCF: A Large-Scale Budget-Constrained Causal Forest Algorithm",
    "abstract": "Offering incentives (e.g., coupons at Amazon, discounts at Uber and video\nbonuses at Tiktok) to user is a common strategy used by online platforms to\nincrease user engagement and platform revenue. Despite its proven\neffectiveness, these marketing incentives incur an inevitable cost and might\nresult in a low ROI (Return on Investment) if not used properly. On the other\nhand, different users respond differently to these incentives, for instance,\nsome users never buy certain products without coupons, while others do anyway.\nThus, how to select the right amount of incentives (i.e. treatment) to each\nuser under budget constraints is an important research problem with great\npractical implications. In this paper, we call such problem as a\nbudget-constrained treatment selection (BTS) problem.\nThe challenge is how to efficiently solve BTS problem on a Large-Scale\ndataset and achieve improved results over the existing techniques. We propose a\nnovel tree-based treatment selection technique under budget constraints, called\nLarge-Scale Budget-Constrained Causal Forest (LBCF) algorithm, which is also an\nefficient treatment selection algorithm suitable for modern distributed\ncomputing systems. A novel offline evaluation method is also proposed to\novercome an intrinsic challenge in assessing solutions' performance for BTS\nproblem in randomized control trials (RCT) data. We deploy our approach in a\nreal-world scenario on a large-scale video platform, where the platform gives\naway bonuses in order to increase users' campaign engagement duration. The\nsimulation analysis, offline and online experiments all show that our method\noutperforms various tree-based state-of-the-art baselines. The proposed\napproach is currently serving over hundreds of millions of users on the\nplatform and achieves one of the most tremendous improvements over these\nmonths.",
    "descriptor": "\nComments: 10 pages, 10 figures, to be published in Web Conference 2022 (WWW'22)\n",
    "authors": [
      "Meng Ai",
      "Biao Li",
      "Heyang Gong",
      "Qingwei Yu",
      "Shengjie Xue",
      "Yuan Zhang",
      "Yunzhou Zhang",
      "Peng Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.12585"
  },
  {
    "id": "arXiv:2201.12587",
    "title": "A generalized SAV approach with relaxation for dissipative systems",
    "abstract": "The scalar auxiliary variable (SAV) approach \\cite{shen2018scalar} and its\ngeneralized version GSAV proposed in \\cite{huang2020highly} are very popular\nmethods to construct efficient and accurate energy stable schemes for nonlinear\ndissipative systems. However, the discrete value of the SAV is not directly\nlinked to the free energy of the dissipative system, and may lead to inaccurate\nsolutions if the time step is not sufficiently small. Inspired by the relaxed\nSAV method proposed in \\cite{jiang2022improving} for gradient flows, we propose\nin this paper a generalized SAV approach with relaxation (R-GSAV) for general\ndissipative systems. The R-GSAV approach preserves all the advantages of the\nGSAV appraoch, in addition, it dissipates a modified energy that is directly\nlinked to the original free energy. We prove that the $k$-th order\nimplicit-explicit (IMEX) schemes based on R-GSAV are unconditionally energy\nstable, and we carry out a rigorous error analysis for $k=1,2,3,4,5$. We\npresent ample numerical results to demonstrate the improved accuracy and\neffectiveness of the R-GSAV approach.",
    "descriptor": "",
    "authors": [
      "Yanrong Zhang",
      "Jie Shen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12587"
  },
  {
    "id": "arXiv:2201.12590",
    "title": "Map Equation Centrality: A Community-Aware Centrality Score Based on the  Map Equation",
    "abstract": "To measure node importance, network scientists employ centrality scores that\ntypically take a microscopic or macroscopic perspective, relying on node\nfeatures or global network structure. However, traditional centrality measures,\nsuch as degree centrality and PageRank, neglect the community structure found\nin real-world networks. To study node importance based on network flows from a\nmesoscopic perspective, we analytically derive a community-aware\ninformation-theoretic centrality score based on the coding principles behind\nthe map equation: map equation centrality. Map equation centrality measures how\nmuch further we can compress the network's modular description by not coding\nfor random walker transitions to the respective node, using an adapted coding\nscheme. It can be determined from the local network context alone because\nchanges to the coding scheme affect only the node's module. Applied to\nsynthetic and real-world networks, we highlight how our approach enables a more\nfine-grained differentiation between nodes than node-local or network-global\nmeasures. Map equation centrality tends to outperform other community-aware\ncentrality measures.",
    "descriptor": "",
    "authors": [
      "Christopher Bl\u00f6cker",
      "Juan Carlos Nieves",
      "Martin Rosvall"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.12590"
  },
  {
    "id": "arXiv:2201.12592",
    "title": "Exact Decomposition of Joint Low Rankness and Local Smoothness Plus  Sparse Matrices",
    "abstract": "It is known that the decomposition in low-rank and sparse matrices\n(\\textbf{L+S} for short) can be achieved by several Robust PCA techniques.\nBesides the low rankness, the local smoothness (\\textbf{LSS}) is a vitally\nessential prior for many real-world matrix data such as hyperspectral images\nand surveillance videos, which makes such matrices have low-rankness and local\nsmoothness properties at the same time. This poses an interesting question: Can\nwe make a matrix decomposition in terms of \\textbf{L\\&LSS +S } form exactly? To\naddress this issue, we propose in this paper a new RPCA model based on\nthree-dimensional correlated total variation regularization (3DCTV-RPCA for\nshort) by fully exploiting and encoding the prior expression underlying such\njoint low-rank and local smoothness matrices. Specifically, using a\nmodification of Golfing scheme, we prove that under some mild assumptions, the\nproposed 3DCTV-RPCA model can decompose both components exactly, which should\nbe the first theoretical guarantee among all such related methods combining low\nrankness and local smoothness. In addition, by utilizing Fast Fourier Transform\n(FFT), we propose an efficient ADMM algorithm with a solid convergence\nguarantee for solving the resulting optimization problem. Finally, a series of\nexperiments on both simulations and real applications are carried out to\ndemonstrate the general validity of the proposed 3DCTV-RPCA model.",
    "descriptor": "\nComments: 14 pages, 14 figures, 3 tables\n",
    "authors": [
      "Jiangjun Peng",
      "Yao Wang",
      "Hongying Zhang",
      "Jianjun Wang",
      "Deyu Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12592"
  },
  {
    "id": "arXiv:2201.12594",
    "title": "Robust Imitation Learning from Corrupted Demonstrations",
    "abstract": "We consider offline Imitation Learning from corrupted demonstrations where a\nconstant fraction of data can be noise or even arbitrary outliers. Classical\napproaches such as Behavior Cloning assumes that demonstrations are collected\nby an presumably optimal expert, hence may fail drastically when learning from\ncorrupted demonstrations. We propose a novel robust algorithm by minimizing a\nMedian-of-Means (MOM) objective which guarantees the accurate estimation of\npolicy, even in the presence of constant fraction of outliers. Our theoretical\nanalysis shows that our robust method in the corrupted setting enjoys nearly\nthe same error scaling and sample complexity guarantees as the classical\nBehavior Cloning in the expert demonstration setting. Our experiments on\ncontinuous-control benchmarks validate that our method exhibits the predicted\nrobustness and effectiveness, and achieves competitive results compared to\nexisting imitation learning methods.",
    "descriptor": "",
    "authors": [
      "Liu Liu",
      "Ziyang Tang",
      "Lanqing Li",
      "Dijun Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12594"
  },
  {
    "id": "arXiv:2201.12596",
    "title": "MVP: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic  Alignment",
    "abstract": "In this paper, we propose a Multi-stage Vision-language Pre-training (MVP)\nframework to learn cross-modality representation via multi-level semantic\nalignment. We introduce concepts in both modalities to construct two-level\nsemantic representations for language and vision. Based on the multi-level\ninput, we train the cross-modality model in two stages, namely, uni-modal\nlearning and cross-modal learning. The former stage enforces within-modality\ninteractions to learn multi-level semantics for each single modality. The\nlatter stage enforces interactions across modalities via both coarse-grain and\nfine-grain semantic alignment tasks. Image-text matching and masked language\nmodeling are then used to further optimize the pre-training model. Our model\ngenerates the-state-of-the-art results on several vision and language tasks.",
    "descriptor": "",
    "authors": [
      "Zejun Li",
      "Zhihao Fan",
      "Huaixiao Tou",
      "Zhongyu Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2201.12596"
  },
  {
    "id": "arXiv:2201.12599",
    "title": "Semantic-assisted image compression",
    "abstract": "Conventional image compression methods typically aim at pixel-level\nconsistency while ignoring the performance of downstream AI tasks.To solve this\nproblem, this paper proposes a Semantic-Assisted Image Compression method\n(SAIC), which can maintain semantic-level consistency to enable high\nperformance of downstream AI tasks.To this end, we train the compression\nnetwork using semantic-level loss function. In particular, semantic-level loss\nis measured using gradient-based semantic weights mechanism (GSW). GSW directly\nconsider downstream AI tasks' perceptual results. Then, this paper proposes a\nsemantic-level distortion evaluation metric to quantify the amount of semantic\ninformation retained during the compression process. Experimental results show\nthat the proposed SAIC method can retain more semantic-level information and\nachieve better performance of downstream AI tasks compared to the traditional\ndeep learning-based method and the advanced perceptual method at the same\ncompression ratio.",
    "descriptor": "",
    "authors": [
      "Qizheng Sun",
      "Caili Guo",
      "Yang Yang",
      "Jiujiu Chen",
      "Xijun Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12599"
  },
  {
    "id": "arXiv:2201.12602",
    "title": "DeepRNG: Towards Deep Reinforcement Learning-Assisted Generative Testing  of Software",
    "abstract": "Although machine learning (ML) has been successful in automating various\nsoftware engineering needs, software testing still remains a highly challenging\ntopic. In this paper, we aim to improve the generative testing of software by\ndirectly augmenting the random number generator (RNG) with a deep reinforcement\nlearning (RL) agent using an efficient, automatically extractable state\nrepresentation of the software under test. Using the Cosmos SDK as the testbed,\nwe show that the proposed DeepRNG framework provides a statistically\nsignificant improvement to the testing of the highly complex software library\nwith over 350,000 lines of code. The source code of the DeepRNG framework is\npublicly available online.",
    "descriptor": "\nComments: Workshop on ML for Systems, 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Chuan-Yung Tsai",
      "Graham W. Taylor"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12602"
  },
  {
    "id": "arXiv:2201.12604",
    "title": "Learning Fast, Learning Slow: A General Continual Learning Method based  on Complementary Learning System",
    "abstract": "Humans excel at continually learning from an ever-changing environment\nwhereas it remains a challenge for deep neural networks which exhibit\ncatastrophic forgetting. The complementary learning system (CLS) theory\nsuggests that the interplay between rapid instance-based learning and slow\nstructured learning in the brain is crucial for accumulating and retaining\nknowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER)\nmethod which maintains short-term and long-term semantic memories that interact\nwith the episodic memory. Our method employs an effective replay mechanism\nwhereby new knowledge is acquired while aligning the decision boundaries with\nthe semantic memories. CLS-ER does not utilize the task boundaries or make any\nassumption about the distribution of the data which makes it versatile and\nsuited for \"general continual learning\". Our approach achieves state-of-the-art\nperformance on standard benchmarks as well as more realistic general continual\nlearning settings.",
    "descriptor": "\nComments: Published as a conference paper at ICLR 2022\n",
    "authors": [
      "Elahe Arani",
      "Fahad Sarfraz",
      "Bahram Zonooz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12604"
  },
  {
    "id": "arXiv:2201.12605",
    "title": "Design of Outdoor Autonomous Moble Robot",
    "abstract": "This study presents the design of a six-wheeled outdoor autonomous mobile\nrobot. The main design goal of our robot is to increase its adaptability and\nflexibility when moving outdoors. This six-wheeled robot platform was equipped\nwith some sensors, such as a global positioning system (GPS), high definition\n(HD) webcam, light detection and ranging (LiDAR), and rotary encoders. A\npersonal mobile computer and 86Duino ONE microcontroller were used as the\nalgorithm computing platform. In terms of control, the lateral offset and head\nangle offset of the robot were calculated using a differential GPS or a camera\nto detect structured and unstructured road boundaries. The lateral offset and\nhead angle offset were fed to a fuzzy controller. The control input was\ndesigned by Q-learning of the differential speed between the left and right\nwheels. This made the robot track a reference route so that it could stay in\nits own lane. 2D LiDAR was also used to measure the relative distance from the\nfront obstacle. The robot would immediately stop to avoid a collision when the\ndistance between the robot and obstacle was less than a specific safety\ndistance. A custom-designed rocker arm gave the robot the ability to climb a\nlow step. Body balance could be maintained by controlling the angle of the\nrocker arm when the robot changed its pose. The autonomous mobile robot has\nbeen used for delivery service on our campus road by integrating the above\nsystem functionality.",
    "descriptor": "\nComments: International Conference on Recent Innovations in Biotechnology, System Engineering, Applied Sciences, Space Environment & Aviation Technology\n",
    "authors": [
      "I-Hsi Kao",
      "Jian-An Su",
      "Jau-Woei Perng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12605"
  },
  {
    "id": "arXiv:2201.12609",
    "title": "ApolloRL: a Reinforcement Learning Platform for Autonomous Driving",
    "abstract": "We introduce ApolloRL, an open platform for research in reinforcement\nlearning for autonomous driving. The platform provides a complete closed-loop\npipeline with training, simulation, and evaluation components. It comes with\n300 hours of real-world data in driving scenarios and popular baselines such as\nProximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) agents. We\nelaborate in this paper on the architecture and the environment defined in the\nplatform. In addition, we discuss the performance of the baseline agents in the\nApolloRL environment.",
    "descriptor": "",
    "authors": [
      "Fei Gao",
      "Peng Geng",
      "Jiaqi Guo",
      "Yuan Liu",
      "Dingfeng Guo",
      "Yabo Su",
      "Jie Zhou",
      "Xiao Wei",
      "Jin Li",
      "Xu Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12609"
  },
  {
    "id": "arXiv:2201.12612",
    "title": "On Non-Cooperative Perfect Information Semi-Markov Games",
    "abstract": "We show that an N-person non-cooperative semi-Markov game under limiting\nratio average pay-off has a pure semi-stationary Nash equilibrium. In an\nearlier paper, the zero-sum two person case has been dealt with. The proof\nfollows by reducing such perfect information games to an associated semi-Markov\ndecision process (SMDP) and then using existence results from the theory of\nSMDP. Exploiting this reduction procedure, one gets simple proofs of the\nfollowing: (a) zero-sum two person perfect information stochastic (Markov)\ngames have a value and pure stationary optimal strategies for both the players\nunder discounted as well as undiscounted pay-off criteria. (b) Similar\nconclusions hold for N-person non-cooperative perfect information stochastic\ngames as well. All such games can be solved using any efficient algorithm for\nthe reduced SMDP (MDP for the case of Stochastic games). In this paper we have\nimplemented Mondal's algorithm to solve an SMDP under limiting ratio average\npay-off criterion. To avoid notational complications we took N=2 in our proof.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2201.00179\n",
    "authors": [
      "K. G. Bakshi",
      "S. Sinha"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12612"
  },
  {
    "id": "arXiv:2201.12614",
    "title": "BatteryLab: A Collaborative Platform for Power Monitoring",
    "abstract": "Advances in cloud computing have simplified the way that both software\ndevelopment and testing are performed. This is not true for battery testing for\nwhich state of the art test-beds simply consist of one phone attached to a\npower meter. These test-beds have limited resources, access, and are overall\nhard to maintain; for these reasons, they often sit idle with no experiment to\nrun. In this paper, we propose to share existing battery testbeds and transform\nthem into vantage points of BatteryLab, a power monitoring platform offering\nheterogeneous devices and testing conditions. We have achieved this vision with\na combination of hardware and software which allow to augment existing battery\ntest-beds with remote capabilities. BatteryLab currently counts three vantage\npoints, one in Europe and two in the US, hosting three Android devices and one\niPhone 7. We benchmark BatteryLab with respect to the accuracy of its battery\nreadings, system performance, and platform heterogeneity. Next, we demonstrate\nhow measurements can be run atop of BatteryLab by developing the \"Web Power\nMonitor\" (WPM), a tool which can measure website power consumption at scale. We\nreleased WPM and used it to report on the energy consumption of Alexa's top\n1,000 websites across 3 locations and 4 devices (both Android and iOS).",
    "descriptor": "\nComments: 25 pages, 11 figures, Passive and Active Measurement Conference 2022 (PAM '22). arXiv admin note: text overlap with arXiv:1910.08951\n",
    "authors": [
      "Matteo Varvello",
      "Kleomenis Katevas",
      "Mihai Plesa",
      "Hamed Haddadi",
      "Fabian Bustamante",
      "Ben Livshits"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.12614"
  },
  {
    "id": "arXiv:2201.12622",
    "title": "Hand Gesture Recognition of Dumb Person Using one Against All Neural  Network",
    "abstract": "We propose a new technique for recognition of dumb person hand gesture in\nreal world environment. In this technique, the hand image containing the\ngesture is preprocessed and then hand region is segmented by convergent the RGB\ncolor image to L.a.b color space. Only few statistical features are used to\nclassify the segmented image to different classes. Artificial Neural Network is\ntrained in sequential manner using one against all. When the system gets\ntrained, it becomes capable of recognition of each class in parallel manner.\nThe result of proposed technique is much better than existing techniques.",
    "descriptor": "",
    "authors": [
      "Muhammad Asim Khan",
      "Lan Hong",
      "Sajjad Ahmed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12622"
  },
  {
    "id": "arXiv:2201.12625",
    "title": "ADC-Net: An Open-Source Deep Learning Network for Automated Dispersion  Compensation in Optical Coherence Tomography",
    "abstract": "Chromatic dispersion is a common problem to degrade the system resolution in\noptical coherence tomography (OCT). This study is to develop a deep learning\nnetwork for automated dispersion compensation (ADC-Net) in OCT. The ADC-Net is\nbased on a redesigned UNet architecture which employs an encoder-decoder\npipeline. The input section encompasses partially compensated OCT B-scans with\nindividual retinal layers optimized. Corresponding output is a fully\ncompensated OCT B-scans with all retinal layers optimized. Two numeric\nparameters, i.e., peak signal to noise ratio (PSNR) and structural similarity\nindex metric computed at multiple scales (MS-SSIM), were used for objective\nassessment of the ADC-Net performance. Comparative analysis of training models,\nincluding single, three, five, seven and nine input channels were implemented.\nThe five-input channels implementation was observed as the optimal mode for\nADC-Net training to achieve robust dispersion compensation in OCT",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Shaiban Ahmed",
      "David Le",
      "Taeyoon Son",
      "Tobiloba Adejumo",
      "Xincheng Yao",
      "Department of Biomedical Engineering",
      "University of Illinois at Chicago",
      "Department of Ophthalmology",
      "Visual Science",
      "University of Illinois at Chicago"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2201.12625"
  },
  {
    "id": "arXiv:2201.12626",
    "title": "Assessing Cross-dataset Generalization of Pedestrian Crossing Predictors",
    "abstract": "Pedestrian crossing prediction has been a topic of active research, resulting\nin many new algorithmic solutions. While measuring the overall progress of\nthose solutions over time tends to be more and more established due to the new\npublicly available benchmark and standardized evaluation procedures, knowing\nhow well existing predictors react to unseen data remains an unanswered\nquestion. This evaluation is imperative as serviceable crossing behavior\npredictors should be set to work in various scenarii without compromising\npedestrian safety due to misprediction. To this end, we conduct a study based\non direct cross-dataset evaluation. Our experiments show that current\nstate-of-the-art pedestrian behavior predictors generalize poorly in\ncross-dataset evaluation scenarii, regardless of their robustness during a\ndirect training-test set evaluation setting. In the light of what we observe,\nwe argue that the future of pedestrian crossing prediction, e.g. reliable and\ngeneralizable implementations, should not be about tailoring models, trained\nwith very little available data, and tested in a classical train-test scenario\nwith the will to infer anything about their behavior in real life. It should be\nabout evaluating models in a cross-dataset setting while considering their\nuncertainty estimates under domain shift.",
    "descriptor": "\nComments: Submitted to the 33rd IEEE Intelligent Vehicles Symposium\n",
    "authors": [
      "Joseph Gesnouin",
      "Steve Pechberti",
      "Bogdan Stanciulescu",
      "Fabien Moutarde"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12626"
  },
  {
    "id": "arXiv:2201.12632",
    "title": "Hyperparameter-free deep active learning for regression problems via  query synthesis",
    "abstract": "In the past decade, deep active learning (DAL) has heavily focused upon\nclassification problems, or problems that have some 'valid' data manifolds,\nsuch as natural languages or images. As a result, existing DAL methods are not\napplicable to a wide variety of important problems -- such as many scientific\ncomputing problems -- that involve regression on relatively unstructured input\nspaces. In this work we propose the first DAL query-synthesis approach for\nregression problems. We frame query synthesis as an inverse problem and use the\nrecently-proposed neural-adjoint (NA) solver to efficiently find points in the\ncontinuous input domain that optimize the query-by-committee (QBC) criterion.\nCrucially, the resulting NA-QBC approach removes the one sensitive\nhyperparameter of the classical QBC active learning approach - the \"pool size\"-\nmaking NA-QBC effectively hyperparameter free. This is significant because DAL\nmethods can be detrimental, even compared to random sampling, if the wrong\nhyperparameters are chosen. We evaluate Random, QBC and NA-QBC sampling\nstrategies on four regression problems, including two contemporary scientific\ncomputing problems. We find that NA-QBC achieves better average performance\nthan random sampling on every benchmark problem, while QBC can be detrimental\nif the wrong hyperparameters are chosen.",
    "descriptor": "",
    "authors": [
      "Simiao Ren",
      "Yang Deng",
      "Willie J. Padilla",
      "Jordan Malof"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12632"
  },
  {
    "id": "arXiv:2201.12633",
    "title": "Image Classification using Graph Neural Network and Multiscale Wavelet  Superpixels",
    "abstract": "Prior studies using graph neural networks (GNNs) for image classification\nhave focused on graphs generated from a regular grid of pixels or similar-sized\nsuperpixels. In the latter, a single target number of superpixels is defined\nfor an entire dataset irrespective of differences across images and their\nintrinsic multiscale structure. On the contrary, this study investigates image\nclassification using graphs generated from an image-specific number of\nmultiscale superpixels. We propose WaveMesh, a new wavelet-based superpixeling\nalgorithm, where the number and sizes of superpixels in an image are\nsystematically computed based on its content. WaveMesh superpixel graphs are\nstructurally different from similar-sized superpixel graphs. We use SplineCNN,\na state-of-the-art network for image graph classification, to compare WaveMesh\nand similar-sized superpixels. Using SplineCNN, we perform extensive\nexperiments on three benchmark datasets under three local-pooling settings: 1)\nno pooling, 2) GraclusPool, and 3) WavePool, a novel spatially heterogeneous\npooling scheme tailored to WaveMesh superpixels. Our experiments demonstrate\nthat SplineCNN learns from multiscale WaveMesh superpixels on-par with\nsimilar-sized superpixels. In all WaveMesh experiments, GraclusPool performs\npoorer than no pooling / WavePool, indicating that poor choice of pooling can\nresult in inferior performance while learning from multiscale superpixels.",
    "descriptor": "\nComments: 17 pages, 6 figures\n",
    "authors": [
      "Varun Vasudevan",
      "Maxime Bassenne",
      "Md Tauhidul Islam",
      "Lei Xing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12633"
  },
  {
    "id": "arXiv:2201.12637",
    "title": "A visualization tool for data analysis on higher education dropout: a  case study at UFES",
    "abstract": "Through the analysis of cultural, socioeconomic and academic performance\naspects it is possible to map the profile of the students and their motivations\nto drop out. This article aims to create a computational tool for data\nvisualization that allows drawing the profile of students to support\neducational institutions managers in the definition of dropout avoidance\npolicies. We present a method to treat data collected by higher education\ninstitutions over the years, analyze them to understand the dropout and provide\nthat information to the university and the general public. Eight questions were\nproposed to clarify the dropout from the Federal University of Esp\\'irito\nSanto, Brazil. The questions were answered through the dashboard that helps to\nunderstand the causes of dropout. It is expected that this tool can be used by\nothers educational institutions to draw student profiles contributing to\npossible resolution of the problem.",
    "descriptor": "",
    "authors": [
      "Pedro P. Ladeira",
      "Leandro M. de Lima",
      "Renato A. Krohling"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.12637"
  },
  {
    "id": "arXiv:2201.12639",
    "title": "On Optimizing Shared-ride Mobility Services with Walking Legs",
    "abstract": "Shared-ride mobility services that incorporate traveler walking legs aim to\nreduce vehicle-kilometers-travelled (VKT), vehicle-hours-travelled (VHT),\nrequest rejections, fleet size, or some combination of these factors, compared\nto door-to-door (D2D) shared-ride services. This paper provides a review of\nshared-ride services with walking legs (SRSWL), particularly the studies in the\nliterature that model the operational problem(s) associated with SRSWL. The\npaper describes the operational and societal benefits of SRSWL as well as\ncompares the SRSWL to circuitous D2D shared-ride services, ride-hailing\nservices, and fixed-route transit services, in terms of VKT and traveler\nwalking distance. The paper then delineates the operational subproblems\nassociated with the SRSWL and discusses their computational complexity.\nAdditionally, the review classifies configurations of SRSWL based on\nflexibility in assigning travelers to pickup and drop-off locations. The paper\nalso discusses four modelling challenge: short-distance person trips, drop-off\nlocation choice for a vehicle's last remaining passenger, allowing vehicles to\nwait for travelers at pickup locations, and simultaneously reducing VHT/VKT and\nimproving customer service quality relative to D2D shared-ride services. The\nreview paper concludes by discussing the most critical areas of future research\nrelated to SRSWL.",
    "descriptor": "\nComments: 25 pages, 4 figures, 2 tables\n",
    "authors": [
      "Zifan Wang",
      "Michael F Hyland",
      "Younghun Bahk",
      "Navjyoth JS Sarma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12639"
  },
  {
    "id": "arXiv:2201.12646",
    "title": "Self Semi Supervised Neural Architecture Search for Semantic  Segmentation",
    "abstract": "In this paper, we propose a Neural Architecture Search strategy based on self\nsupervision and semi-supervised learning for the task of semantic segmentation.\nOur approach builds an optimized neural network (NN) model for this task by\njointly solving a jigsaw pretext task discovered with self-supervised learning\nover unlabeled training data, and, exploiting the structure of the unlabeled\ndata with semi-supervised learning. The search of the architecture of the NN\nmodel is performed by dynamic routing using a gradient descent algorithm.\nExperiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the\ndiscovered neural network is more efficient than a state-of-the-art\nhand-crafted NN model with four times less floating operations.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Lo\u00efc Pauletto",
      "Massih-Reza Amini",
      "Nicolas Winckler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12646"
  },
  {
    "id": "arXiv:2201.12648",
    "title": "Private Boosted Decision Trees via Smooth Re-Weighting",
    "abstract": "Protecting the privacy of people whose data is used by machine learning\nalgorithms is important. Differential Privacy is the appropriate mathematical\nframework for formal guarantees of privacy, and boosted decision trees are a\npopular machine learning technique. So we propose and test a practical\nalgorithm for boosting decision trees that guarantees differential privacy.\nPrivacy is enforced because our booster never puts too much weight on any one\nexample; this ensures that each individual's data never influences a single\ntree \"too much.\" Experiments show that this boosting algorithm can produce\nbetter model sparsity and accuracy than other differentially private ensemble\nclassifiers.",
    "descriptor": "",
    "authors": [
      "Vahid R. Asadi",
      "Marco L. Carmosino",
      "Mohammadmahdi Jahanara",
      "Akbar Rafiey",
      "Bahar Salamatian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12648"
  },
  {
    "id": "arXiv:2201.12649",
    "title": "Transfer Learning for Estimation of Pendubot Angular Position Using Deep  Neural Networks",
    "abstract": "In this paper, a machine learning based approach is introduced to estimate\nPendubot angular position from its captured images. Initially, a baseline\nalgorithm is introduced to estimate the angle using conventional image\nprocessing technique. The baseline algorithm performs well for the cases that\nthe Pendubot is not moving fast. However, when moving quickly due to a free\nfall, the Pendubot appears as a blurred object in the captured image in a way\nthat the baseline algorithm fails to estimate the angle. Consequently, a Deep\nNeural Network (DNN) based algorithm is introduced to cope with this challenge.\nThe approach relies on the concept of transfer learning to allow the training\nof the DNN on a very small fine-tuning dataset. The base algorithm is used to\ncreate the ground truth labels of the fine-tuning dataset. Experimental results\non the held-out evaluation set show that the proposed approach achieves a\nmedian absolute error of 0.02 and 0.06 degrees for the sharp and blurry images\nrespectively.",
    "descriptor": "",
    "authors": [
      "Sina Khanagha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12649"
  },
  {
    "id": "arXiv:2201.12650",
    "title": "New results on the robust coloring problem",
    "abstract": "Many variations of the classical graph coloring model have been intensively\nstudied due to their multiple applications; scheduling problems and aircraft\nassignments, for instance, motivate the \\emph{robust coloring problem}. This\nmodel gets to capture natural constraints of those optimization problems by\ncombining the information provided by two colorings: a vertex coloring of a\ngraph and the induced edge coloring on a subgraph of its complement; the goal\nis to minimize, among all proper colorings of the graph for a fixed number of\ncolors, the number of edges in the subgraph with the endpoints of the same\ncolor. The study of the robust coloring model has been focused on the search\nfor heuristics due to its NP-hard character when using at least three colors,\nbut little progress has been made in other directions. We present a new\napproach on the problem obtaining the first collection of non heuristic results\nfor general graphs; among them, we prove that robust coloring is the model that\nbetter approaches the partition of any system into equal or almost equal\nconflict-free subsystem, relating strongly this model with the well-known\nequitable colorings. We also show the NP-completeness of their decision\nproblems for the unsolved case of two colors, obtain bounds on the associated\nrobust coloring parameter, and solve a conjecture on paths that illustrates the\ncomplexity of studying this coloring model.",
    "descriptor": "",
    "authors": [
      "Delia Garijo",
      "Alberto M\u00e1rquez",
      "Rafael Robles"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2201.12650"
  },
  {
    "id": "arXiv:2201.12657",
    "title": "Prediction of terephthalic acid (TPA) yield in aqueous hydrolysis of  polyethylene terephthalate (PET)",
    "abstract": "Aqueous hydrolysis is used to chemically recycle polyethylene terephthalate\n(PET) due to the production of high-quality terephthalic acid (TPA), the PET\nmonomer. PET hydrolysis depends on various reaction conditions including PET\nsize, catalyst concentration, reaction temperature, etc. So, modeling PET\nhydrolysis by considering the effective factors can provide useful information\nfor material scientists to specify how to design and run these reactions. It\nwill save time, energy, and materials by optimizing the hydrolysis conditions.\nMachine learning algorithms enable to design models to predict output results.\nFor the first time, 381 experimental data were gathered to model the aqueous\nhydrolysis of PET. Effective reaction conditions on PET hydrolysis were\nconnected to TPA yield. The logistic regression was applied to rank the\nreaction conditions. Two algorithms were proposed, artificial neural network\nmultilayer perceptron (ANN-MLP) and adaptive network-based fuzzy inference\nsystem (ANFIS). The dataset was divided into training and testing sets to train\nand test the models, respectively. The models predicted TPA yield sufficiently\nwhere the ANFIS model outperformed. R-squared (R2) and Root Mean Square Error\n(RMSE) loss functions were employed to measure the efficiency of the models and\nevaluate their performance.",
    "descriptor": "\nComments: 36 pages, 11 figures, 3 tables\n",
    "authors": [
      "Hossein Abedsoltan",
      "Zeinab Zoghi",
      "Amir H. Mohammadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12657"
  },
  {
    "id": "arXiv:2201.12658",
    "title": "Learning to Coordinate with Humans using Action Features",
    "abstract": "An unaddressed challenge in human-AI coordination is to enable AI agents to\nexploit the semantic relationships between the features of actions and the\nfeatures of observations. Humans take advantage of these relationships in\nhighly intuitive ways. For instance, in the absence of a shared language, we\nmight point to the object we desire or hold up our fingers to indicate how many\nobjects we want. To address this challenge, we investigate the effect of\nnetwork architecture on the propensity of learning algorithms to exploit these\nsemantic relationships. Across a procedurally generated coordination task, we\nfind that attention-based architectures that jointly process a featurized\nrepresentation of observations and actions have a better inductive bias for\nzero-shot coordination. Through fine-grained evaluation and scenario analysis,\nwe show that the resulting policies are human-interpretable. Moreover, such\nagents coordinate with people without training on any human data.",
    "descriptor": "\nComments: Preprint, under review\n",
    "authors": [
      "Mingwei Ma",
      "Jizhou Liu",
      "Samuel Sokota",
      "Max Kleiman-Weiner",
      "Jakob Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12658"
  },
  {
    "id": "arXiv:2201.12659",
    "title": "Deep Learning based Multi-User Power Allocation and Hybrid Precoding in  Massive MIMO Systems",
    "abstract": "This paper proposes a deep learning based power allocation (DL-PA) and hybrid\nprecoding technique for multiuser massive multiple-input multiple-output\n(MU-mMIMO) systems. We first utilize an angular-based hybrid precoding\ntechnique for reducing the number of RF chains and channel estimation overhead.\nThen, we develop the DL-PA algorithm via a fully-connected deep neural network\n(DNN). DL-PA has two phases: (i) offline supervised learning with the optimal\nallocated powers obtained by particle swarm optimization based PA (PSO-PA)\nalgorithm, (ii) online power prediction by the trained DNN. In comparison to\nthe computationally expensive PSO-PA, it is shown that DL-PA greatly reduces\nthe runtime by 98.6%-99.9%, while closely achieving the optimal sum-rate\ncapacity. It makes DL-PA a promising algorithm for the real-time online\napplications in MU-mMIMO systems.",
    "descriptor": "\nComments: 6 pages, 6 figures\n",
    "authors": [
      "Asil Koc",
      "Mike Wang",
      "Tho Le-Ngoc"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12659"
  },
  {
    "id": "arXiv:2201.12660",
    "title": "Full-Duplex Non-Coherent Communications for Massive MIMO Systems with  Analog Beamforming",
    "abstract": "In this paper, a novel full-duplex non-coherent (FD-NC) transmission scheme\nis developed for massive multiple-input multiple-output (mMIMO) systems using\nanalog beamforming (ABF). We propose to use a structured Grassmannian\nconstellation for the non-coherent communications that does not require channel\nestimation. Then, we design the transmit and receive ABF via the slow\ntime-varying angle-of-departure (AoD) and angle-of-arrival (AoA) information,\nrespectively. The ABF design targets maximizing the intended signal power while\nsuppressing the strong self-interference (SI) occurred in the FD transmission.\nAlso, the proposed ABF technique only needs a single transmit and receive RF\nchain to support large antenna arrays, thus, it reduces hardware\ncost/complexity in the mMIMO systems. It is shown that the proposed FD-NC\noffers a great improvement in bit error rate (BER) in comparison to both\nhalf-duplex non-coherent (HD-NC) and HD coherent schemes. We also observe that\nthe proposed FD-NC both reduces the error floor resulted from the residual SI\nin FD transmission, and provides lower BER compared to the FD coherent\ntransmission.",
    "descriptor": "\nComments: 6 pages, 6 figures\n",
    "authors": [
      "Asil Koc",
      "Ahmed Masmoudi",
      "Tho Le-Ngoc"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12660"
  },
  {
    "id": "arXiv:2201.12662",
    "title": "Fair ranking: a critical review, challenges, and future directions",
    "abstract": "Ranking, recommendation, and retrieval systems are widely used in online\nplatforms and other societal systems, including e-commerce, media-streaming,\nadmissions, gig platforms, and hiring. In the recent past, a large \"fair\nranking\" research literature has been developed around making these systems\nfair to the individuals, providers, or content that are being ranked. Most of\nthis literature defines fairness for a single instance of retrieval, or as a\nsimple additive notion for multiple instances of retrievals over time. This\nwork provides a critical overview of this literature, detailing the often\ncontext-specific concerns that such an approach misses: the gap between high\nranking placements and true provider utility, spillovers and compounding\neffects over time, induced strategic incentives, and the effect of statistical\nuncertainty. We then provide a path forward for a more holistic and\nimpact-oriented fair ranking research agenda, including methodological lessons\nfrom other fields and the role of the broader stakeholder community in\novercoming data bottlenecks and designing effective regulatory environments.",
    "descriptor": "",
    "authors": [
      "Gourab K Patro",
      "Lorenzo Porcaro",
      "Laura Mitchell",
      "Qiuyue Zhang",
      "Meike Zehlike",
      "Nikhil Garg"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.12662"
  },
  {
    "id": "arXiv:2201.12663",
    "title": "Reduced Higher Order SVD: ubiquitous rank-reduction method in  tensor-based scientific computing",
    "abstract": "Tensor numerical methods, based on the rank-structured tensor representation\nof $d$-variate functions and operators, are designed to provide $O(dn)$\ncomplexity of numerical calculations on $n^{\\otimes d }$ grids contrary to\n$O(n^d)$ scaling by conventional grid-based methods. However, multiple tensor\noperations may lead to enormous increase in the tensor ranks (curse of ranks)\nof the target data, making calculation intractable. Therefore one of the most\nimportant steps in tensor calculations is the robust and efficient rank\nreduction procedure which should be performed many times in the course of\nvarious tensor transforms in multidimensional operator and function calculus.\nThe rank reduction scheme based on the Reduced Higher Order SVD (RHOSVD)\nintroduced in [33] played a significant role in the development of tensor\nnumerical methods. Here, we briefly survey the essentials of RHOSVD method and\nthen focus on some new theoretical and computational aspects of the RHOSVD\ndemonstrating that this rank reduction technique constitutes the basic\ningredient in tensor computations for real-life problems. In particular, the\nstability analysis of RHOSVD is presented. We introduce the multilinear algebra\nof tensors represented in the range-separated (RS) tensor format. This allows\nto apply the RHOSVD rank-reduction techniques to non-regular functional data\nwith many singularities, for example, to the rank-structured computation of the\ncollective multi-particle interaction potentials in bio-molecular modeling, as\nwell as to complicated composite radial functions. The new theoretical and\nnumerical results on application of the RHOSVD in scattered data modeling are\npresented. RHOSVD proved to be the efficient rank reduction technique in\nnumerous applications ranging from numerical treatment of multi-particle\nsystems up to a numerical solution of PDE constrained control problems.",
    "descriptor": "\nComments: 32 pages, 15 figures\n",
    "authors": [
      "Venera Khoromskaia",
      "Boris N. Khoromskij"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12663"
  },
  {
    "id": "arXiv:2201.12664",
    "title": "A Deep CNN Architecture with Novel Pooling Layer Applied to Two Sudanese  Arabic Sentiment Datasets",
    "abstract": "Arabic sentiment analysis has become an important research field in recent\nyears. Initially, work focused on Modern Standard Arabic (MSA), which is the\nmost widely-used form. Since then, work has been carried out on several\ndifferent dialects, including Egyptian, Levantine and Moroccan. Moreover, a\nnumber of datasets have been created to support such work. However, up until\nnow, less work has been carried out on Sudanese Arabic, a dialect which has 32\nmillion speakers. In this paper, two new publicly available datasets are\nintroduced, the 2-Class Sudanese Sentiment Dataset (SudSenti2) and the 3-Class\nSudanese Sentiment Dataset (SudSenti3). Furthermore, a CNN architecture, SCM,\nis proposed, comprising five CNN layers together with a novel pooling layer,\nMMA, to extract the best features. This SCM+MMA model is applied to SudSenti2\nand SudSenti3 with accuracies of 92.75% and 84.39%. Next, the model is compared\nto other deep learning classifiers and shown to be superior on these new\ndatasets. Finally, the proposed model is applied to the existing Saudi\nSentiment Dataset and to the MSA Hotel Arabic Review Dataset with accuracies\n85.55% and 90.01%.",
    "descriptor": "\nComments: 19 pages, 11 tables, 11 figures\n",
    "authors": [
      "Mustafa Mhamed",
      "Richard Sutcliffe",
      "Xia Sun",
      "Jun Feng",
      "Eiad Almekhlafi",
      "Ephrem A. Retta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12664"
  },
  {
    "id": "arXiv:2201.12666",
    "title": "Challenges and approaches to privacy preserving post-click conversion  prediction",
    "abstract": "Online advertising has typically been more personalized than offline\nadvertising, through the use of machine learning models and real-time auctions\nfor ad targeting. One specific task, predicting the likelihood of conversion\n(i.e.\\ the probability a user will purchase the advertised product), is crucial\nto the advertising ecosystem for both targeting and pricing ads. Currently,\nthese models are often trained by observing individual user behavior, but,\nincreasingly, regulatory and technical constraints are requiring\nprivacy-preserving approaches. For example, major platforms are moving to\nrestrict tracking individual user events across multiple applications, and\ngovernments around the world have shown steadily more interest in regulating\nthe use of personal data. Instead of receiving data about individual user\nbehavior, advertisers may receive privacy-preserving feedback, such as the\nnumber of installs of an advertised app that resulted from a group of users. In\nthis paper we outline the recent privacy-related changes in the online\nadvertising ecosystem from a machine learning perspective. We provide an\noverview of the challenges and constraints when learning conversion models in\nthis setting. We introduce a novel approach for training these models that\nmakes use of post-ranking signals. We show using offline experiments on real\nworld data that it outperforms a model relying on opt-in data alone, and\nsignificantly reduces model degradation when no individual labels are\navailable. Finally, we discuss future directions for research in this evolving\narea.",
    "descriptor": "",
    "authors": [
      "Conor O'Brien",
      "Arvind Thiagarajan",
      "Sourav Das",
      "Rafael Barreto",
      "Chetan Verma",
      "Tim Hsu",
      "James Neufield",
      "Jonathan J Hunt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.12666"
  },
  {
    "id": "arXiv:2201.12667",
    "title": "Distributed SLIDE: Enabling Training Large Neural Networks on Low  Bandwidth and Simple CPU-Clusters via Model Parallelism and Sparsity",
    "abstract": "More than 70% of cloud computing is paid for but sits idle. A large fraction\nof these idle compute are cheap CPUs with few cores that are not utilized\nduring the less busy hours. This paper aims to enable those CPU cycles to train\nheavyweight AI models. Our goal is against mainstream frameworks, which focus\non leveraging expensive specialized ultra-high bandwidth interconnect to\naddress the communication bottleneck in distributed neural network training.\nThis paper presents a distributed model-parallel training framework that\nenables training large neural networks on small CPU clusters with low Internet\nbandwidth. We build upon the adaptive sparse training framework introduced by\nthe SLIDE algorithm. By carefully deploying sparsity over distributed nodes, we\ndemonstrate several orders of magnitude faster model parallel training than\nHorovod, the main engine behind most commercial software. We show that with\nreduced communication, due to sparsity, we can train close to a billion\nparameter model on simple 4-16 core CPU nodes connected by basic low bandwidth\ninterconnect. Moreover, the training time is at par with some of the best\nhardware accelerators.",
    "descriptor": "",
    "authors": [
      "Minghao Yan",
      "Nicholas Meisburger",
      "Tharun Medini",
      "Anshumali Shrivastava"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12667"
  },
  {
    "id": "arXiv:2201.12670",
    "title": "SMGRL: A Scalable Multi-resolution Graph Representation Learning  Framework",
    "abstract": "Graph convolutional networks (GCNs) allow us to learn topologically-aware\nnode embeddings, which can be useful for classification or link prediction.\nHowever, by construction, they lack positional awareness and are unable to\ncapture long-range dependencies without adding additional layers -- which in\nturn leads to over-smoothing and increased time and space complexity. Further,\nthe complex dependencies between nodes make mini-batching challenging, limiting\ntheir applicability to large graphs.\nThis paper proposes a Scalable Multi-resolution Graph Representation Learning\n(SMGRL) framework that enables us to learn multi-resolution node embeddings\nefficiently. Our framework is model-agnostic and can be applied to any existing\nGCN model. We dramatically reduce training costs by training only on a\nreduced-dimension coarsening of the original graph, then exploit\nself-similarity to apply the resulting algorithm at multiple resolutions.\nInference of these multi-resolution embeddings can be distributed across\nmultiple machines to reduce computational and memory requirements further. The\nresulting multi-resolution embeddings can be aggregated to yield high-quality\nnode embeddings that capture both long- and short-range dependencies between\nnodes. Our experiments show that this leads to improved classification\naccuracy, without incurring high computational costs.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Reza Namazi",
      "Elahe Ghalebi",
      "Sinead Williamson",
      "Hamidreza Mahyar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12670"
  },
  {
    "id": "arXiv:2201.12673",
    "title": "The Challenges Ahead for Bio-inspired Neuromorphic Event Processors: How  Memristors Dynamic Properties Could Revolutionize Machine Learning",
    "abstract": "Neuromorphic engineering has led to the necessary process of rethinking of\nhow we process and integrate information, analyze data, and use the resulting\ninsights to improve computation and avoid the current high power and latency of\nArtificial Intelligence (AI) hardware. Current neuromorphic processors are,\nhowever, limited by digital technologies, which cannot reproduce the abilities\nof biological neural computation in terms of power, latency and area cost. In\nthis paper, we show that the combined use of the dynamic properties of\nmemristors to implement a model of synaptic integration and the determination\nof the correct level of abstraction of biological neural networks has the\npotential to open a new range of capabilities for neuromorphic processors. We\ntest this approach using a novel three-terminal LixWO3 electrochemical\nmemristor, by deriving its conductance model and using it to emulate synaptic\ntemporal kernel computation in the context of a pattern recognition task. We\nshow that these devices allow for robust results with no loss in precision\nwhile opening the path for an energy efficient approach to build novel\nbio-inspired processing units in silicon.",
    "descriptor": "",
    "authors": [
      "Marco Rasetto",
      "Qingzhou Wan",
      "Himanshu Akolkar",
      "Bertram Shi",
      "Feng Xiong",
      "Ryad Benosman"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2201.12673"
  },
  {
    "id": "arXiv:2201.12674",
    "title": "Rewiring with Positional Encodings for Graph Neural Networks",
    "abstract": "Several recent works use positional encodings to extend the receptive fields\nof graph neural network (GNN) layers equipped with attention mechanisms. These\ntechniques, however, extend receptive fields to the complete graph, at\nsubstantial computational cost and risking a change in the inductive biases of\nconventional GNNs, or require complex architecture adjustments. As a\nconservative alternative, we use positional encodings to expand receptive\nfields to any r-ring. Our method augments the input graph with additional\nnodes/edges and uses positional encodings as node and/or edge features. Thus,\nit is compatible with many existing GNN architectures. We also provide examples\nof positional encodings that are non-invasive, i.e., there is a one-to-one map\nbetween the original and the modified graphs. Our experiments demonstrate that\nextending receptive fields via positional encodings and a virtual\nfully-connected node significantly improves GNN performance and alleviates\nover-squashing using small r. We obtain improvements across models, showing\nstate-of-the-art performance even using older architectures than recent\nTransformer models adapted to graphs.",
    "descriptor": "",
    "authors": [
      "Rickard Br\u00fcel-Gabrielsson",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12674"
  },
  {
    "id": "arXiv:2201.12675",
    "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning  for Language Models",
    "abstract": "A central tenet of Federated learning (FL), which trains models without\ncentralizing user data, is privacy. However, previous work has shown that the\ngradient updates used in FL can leak user information. While the most\nindustrial uses of FL are for text applications (e.g. keystroke prediction),\nnearly all attacks on FL privacy have focused on simple image classifiers. We\npropose a novel attack that reveals private user text by deploying malicious\nparameter vectors, and which succeeds even with mini-batches, multiple users,\nand long sequences. Unlike previous attacks on FL, the attack exploits\ncharacteristics of both the Transformer architecture and the token embedding,\nseparately extracting tokens and positional embeddings to retrieve\nhigh-fidelity text. This work suggests that FL on text, which has historically\nbeen resistant to privacy attacks, is far more vulnerable than previously\nthought.",
    "descriptor": "\nComments: First two authors contributed equally. Order chosen by coin flip\n",
    "authors": [
      "Liam Fowl",
      "Jonas Geiping",
      "Steven Reich",
      "Yuxin Wen",
      "Wojtek Czaja",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12675"
  },
  {
    "id": "arXiv:2201.12676",
    "title": "A Deep Learning and Geospatial Data-Based Channel Estimation Technique  for Hybrid Massive MIMO Systems",
    "abstract": "This paper presents a novel channel estimation technique for the multi-user\nmassive multiple-input multiple-output (MU-mMIMO) systems using angular-based\nhybrid precoding (AB-HP). The proposed channel estimation technique generates\ngroup-wise channel state information (CSI) of user terminal (UT) zones in the\nservice area by deep neural networks (DNN) and fuzzy c-Means (FCM) clustering.\nThe slow time-varying CSI between the base station (BS) and feasible UT\nlocations in the service area is calculated from the geospatial data by offline\nray tracing and a DNN-based path estimation model associated with the\n1-dimensional convolutional neural network (1D-CNN) and regression tree\nensembles. Then, the UT-level CSI of all feasible locations is grouped into\nclusters by a proposed FCM clustering. Finally, the service area is divided\ninto a number of non-overlapping UT zones. Each UT zone is characterized by a\ncorresponding set of clusters named as UT-group CSI, which is utilized in the\nanalog RF beamformer design of AB-HP to reduce the required large online CSI\noverhead in the MU-mMIMO systems. Then, the reduced-size online CSI is employed\nin the baseband (BB) precoder of AB-HP. Simulations are conducted in the indoor\nscenario at 28 GHz and tested in an AB-HP MU-mMIMO system with a uniform\nrectangular array (URA) having 16x16=256 antennas and 22 RF chains.\nIllustrative results indicate that 91.4% online CSI can be reduced by using the\nproposed offline channel estimation technique as compared to the conventional\nonline channel sounding. The proposed DNN-based path estimation technique\nproduces same amount of UT-level CSI with runtime reduced by 65.8% as compared\nto the computationally expensive ray tracing.",
    "descriptor": "\nComments: 18 pages, 21 figures\n",
    "authors": [
      "Xiaoyi Zhu",
      "Asil Koc",
      "Robert Morawski",
      "Tho Le-Ngoc"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.12676"
  },
  {
    "id": "arXiv:2201.12677",
    "title": "AIM: An Adaptive and Iterative Mechanism for Differentially Private  Synthetic Data",
    "abstract": "We propose AIM, a novel algorithm for differentially private synthetic data\ngeneration. \\aim is a workload-adaptive algorithm, within the paradigm of\nalgorithms that first selects a set of queries, then privately measures those\nqueries, and finally generates synthetic data from the noisy measurements. It\nuses a set of innovative features to iteratively select the most useful\nmeasurements, reflecting both their relevance to the workload and their value\nin approximating the input data. We also provide analytic expressions to bound\nper-query error with high probability, which can be used to construct\nconfidence intervals and inform users about the accuracy of generated data. We\nshow empirically that AIM consistently outperforms a wide variety of existing\nmechanisms across a variety of experimental settings.",
    "descriptor": "",
    "authors": [
      "Ryan McKenna",
      "Brett Mullins",
      "Daniel Sheldon",
      "Gerome Miklau"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.12677"
  },
  {
    "id": "arXiv:2201.12678",
    "title": "A Stochastic Bundle Method for Interpolating Networks",
    "abstract": "We propose a novel method for training deep neural networks that are capable\nof interpolation, that is, driving the empirical loss to zero. At each\niteration, our method constructs a stochastic approximation of the learning\nobjective. The approximation, known as a bundle, is a pointwise maximum of\nlinear functions. Our bundle contains a constant function that lower bounds the\nempirical loss. This enables us to compute an automatic adaptive learning rate,\nthereby providing an accurate solution. In addition, our bundle includes linear\napproximations computed at the current iterate and other linear estimates of\nthe DNN parameters. The use of these additional approximations makes our method\nsignificantly more robust to its hyperparameters. Based on its desirable\nempirical properties, we term our method Bundle Optimisation for Robust and\nAccurate Training (BORAT). In order to operationalise BORAT, we design a novel\nalgorithm for optimising the bundle approximation efficiently at each\niteration. We establish the theoretical convergence of BORAT in both convex and\nnon-convex settings. Using standard publicly available data sets, we provide a\nthorough comparison of BORAT to other single hyperparameter optimisation\nalgorithms. Our experiments demonstrate BORAT matches the state-of-the-art\ngeneralisation performance for these methods and is the most robust.",
    "descriptor": "",
    "authors": [
      "Alasdair Paren",
      "Leonard Berrada",
      "Rudra P. K. Poudel",
      "M. Pawan Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12678"
  },
  {
    "id": "arXiv:2201.12680",
    "title": "Deep Contrastive Learning is Provably (almost) Principal Component  Analysis",
    "abstract": "We show that Contrastive Learning (CL) under a family of loss functions\n(including InfoNCE) has a game-theoretical formulation, where the \\emph{max\nplayer} finds representation to maximize contrastiveness, and the \\emph{min\nplayer} puts weights on pairs of samples with similar representation. We show\nthat the max player who does \\emph{representation learning} reduces to\nPrincipal Component Analysis for deep linear network, and almost all local\nminima are global, recovering optimal PCA solutions. Experiments show that the\nformulation yields comparable (or better) performance on CIFAR10 and STL-10\nwhen extending beyond InfoNCE, yielding novel contrastive losses. Furthermore,\nwe extend our theoretical analysis to 2-layer ReLU networks, showing its\ndifference from linear ones, and proving that feature composition is preferred\nover picking single dominant feature under strong augmentation.",
    "descriptor": "",
    "authors": [
      "Yuandong Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.12680"
  },
  {
    "id": "arXiv:2201.12681",
    "title": "Revisiting Embodiment for Brain-Computer Interfaces",
    "abstract": "Researchers increasingly explore deploying brain-computer interfaces (BCIs)\nfor able-bodied users, with the motivation of accessing mental states more\ndirectly than allowed by existing body-mediated interaction. This motivation\nseems to contradict the long-standing HCI emphasis on embodiment, namely the\ngeneral claim that the body is crucial for cognition. This paper addresses this\napparent contradiction through a review of insights from embodied cognition and\ninteraction. We first critically examine the recent interest in BCIs and\nidentify the extent cognition in the brain is integrated with the wider body as\na central concern for research. We then define the implications of an\nintegrated view of cognition for interface design and evaluation. A\ncounterintuitive conclusion we draw is that embodiment per se should not imply\na preference for body movements over brain signals. Yet it can nevertheless\nguide research by 1) providing body-grounded explanations for BCI performance,\n2) proposing evaluation considerations that are neglected in modular views of\ncognition, and 3) through the direct transfer of its design insights to BCIs.\nWe finally reflect on HCI's understanding of embodiment and identify the neural\ndimension of embodiment as hitherto overlooked.",
    "descriptor": "",
    "authors": [
      "Bar\u0131\u015f Serim",
      "Michiel Spap\u00e9",
      "Giulio Jacucci"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.12681"
  },
  {
    "id": "arXiv:2201.12684",
    "title": "Fast One-to-Many Multicriteria Shortest Path Search",
    "abstract": "This paper introduces a novel algorithm combination designed for fast\none-to-many multicriteria shortest path search. A preprocessing algorithm\nexcludes irrelevant vertices by building a smaller cover graph. A modified\nversion of multicriteria label-setting algorithm operates on the cover graph\nand employs a dimensionality reduction technique for swifter domination checks.\nWhile the method itself maintains solution optimality, it is able to\nadditionally incorporate existing heuristics for further speedups. The proposed\nalgorithm has been tested on multiple criteria combinations of varying\ncorrelation. The results show the introduced approach provides a speedup of at\nleast 6 times on simple criteria combinations and up to 60 times on hard\ninstances compared to vanilla multicriteria label-setting. Graph preprocessing\nalso decreases memory requirements of queries by up to 13 times.",
    "descriptor": "\nComments: 11 pages, 2 figures\n",
    "authors": [
      "Temirlan Kurbanov",
      "Marek Cuch\u00fd",
      "Ji\u0159\u00ed Vok\u0159\u00ednek"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12684"
  },
  {
    "id": "arXiv:2201.12686",
    "title": "Robustness of Deep Recommendation Systems to Untargeted Interaction  Perturbations",
    "abstract": "While deep learning-based sequential recommender systems are widely used in\npractice, their sensitivity to untargeted training data perturbations is\nunknown. Untargeted perturbations aim to modify ranked recommendation lists for\nall users at test time, by inserting imperceptible input perturbations during\ntraining time. Existing perturbation methods are mostly targeted attacks\noptimized to change ranks of target items, but not suitable for untargeted\nscenarios. In this paper, we develop a novel framework in which user-item\ntraining interactions are perturbed in unintentional and adversarial settings.\nFirst, through comprehensive experiments on four datasets, we show that four\npopular recommender models are unstable against even one random perturbation.\nSecond, we establish a cascading effect in which minor manipulations of early\ntraining interactions can cause extensive changes to the model and the\ngenerated recommendations for all users. Leveraging this effect, we propose an\nadversarial perturbation method CASPER which identifies and perturbs an\ninteraction that induces the maximal cascading effect. Experimentally, we\ndemonstrate that CASPER reduces the stability of recommendation models the\nmost, compared to several baselines and state-of-the-art methods. Finally, we\nshow the runtime and success of CASPER scale near-linearly with the dataset\nsize and the number of perturbations, respectively.",
    "descriptor": "",
    "authors": [
      "Sejoon Oh",
      "Srijan Kumar"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12686"
  },
  {
    "id": "arXiv:2201.12693",
    "title": "Extracting Built Environment Features for Planning Research with  Computer Vision: A Review and Discussion of State-of-the-Art Approaches",
    "abstract": "This is an extended abstract for a presentation at The 17th International\nConference on CUPUM - Computational Urban Planning and Urban Management in June\n2021. This study presents an interdisciplinary synthesis of the\nstate-of-the-art approaches in computer vision technologies to extract built\nenvironment features that could improve the robustness of empirical research in\nplanning. We discussed the findings from the review of studies in both planning\nand computer science.",
    "descriptor": "\nComments: CUPUM 2021 (The 17th International Conference on Computational Urban Planning and Urban Management)\n",
    "authors": [
      "Meiqing Li",
      "Hao Sheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.12693"
  },
  {
    "id": "arXiv:2201.12696",
    "title": "Sharing Behavior in Ride-hailing Trips: A Machine Learning Inference  Approach",
    "abstract": "Ride-hailing is rapidly changing urban and personal transportation. Ride\nsharing or pooling is important to mitigate negative externalities of\nride-hailing such as increased congestion and environmental impacts. However,\nthere lacks empirical evidence on what affect trip-level sharing behavior in\nride-hailing. Using a novel dataset from all ride-hailing trips in Chicago in\n2019, we show that the willingness of riders to request a shared ride has\nmonotonically decreased from 27.0% to 12.8% throughout the year, while the trip\nvolume and mileage have remained statistically unchanged. We find that the\ndecline in sharing preference is due to an increased per-mile costs of shared\ntrips and shifting shorter trips to solo. Using ensemble machine learning\nmodels, we find that the travel impedance variables (trip cost, distance, and\nduration) collectively contribute to 95% and 91% of the predictive power in\ndetermining whether a trip is requested to share and whether it is successfully\nshared, respectively. Spatial and temporal attributes, sociodemographic, built\nenvironment, and transit supply variables do not entail predictive power at the\ntrip level in presence of these travel impedance variables. This implies that\npricing signals are most effective to encourage riders to share their rides.\nOur findings shed light on sharing behavior in ride-hailing trips and can help\ndevise strategies that increase shared ride-hailing, especially as the demand\nrecovers from pandemic.",
    "descriptor": "",
    "authors": [
      "Morteza Taiebat",
      "Elham Amini",
      "Ming Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2201.12696"
  },
  {
    "id": "arXiv:2201.12700",
    "title": "Coordinated Attacks against Contextual Bandits: Fundamental Limits and  Defense Mechanisms",
    "abstract": "Motivated by online recommendation systems, we propose the problem of finding\nthe optimal policy in multitask contextual bandits when a small fraction\n$\\alpha < 1/2$ of tasks (users) are arbitrary and adversarial. The remaining\nfraction of good users share the same instance of contextual bandits with $S$\ncontexts and $A$ actions (items). Naturally, whether a user is good or\nadversarial is not known in advance. The goal is to robustly learn the policy\nthat maximizes rewards for good users with as few user interactions as\npossible. Without adversarial users, established results in collaborative\nfiltering show that $O(1/\\epsilon^2)$ per-user interactions suffice to learn a\ngood policy, precisely because information can be shared across users. This\nparallelization gain is fundamentally altered by the presence of adversarial\nusers: unless there are super-polynomial number of users, we show a lower bound\nof $\\tilde{\\Omega}(\\min(S,A) \\cdot \\alpha^2 / \\epsilon^2)$ {\\it per-user}\ninteractions to learn an $\\epsilon$-optimal policy for the good users. We then\nshow we can achieve an $\\tilde{O}(\\min(S,A)\\cdot \\alpha/\\epsilon^2)$\nupper-bound, by employing efficient robust mean estimators for both uni-variate\nand high-dimensional random variables. We also show that this can be improved\ndepending on the distributions of contexts.",
    "descriptor": "",
    "authors": [
      "Jeongyeol Kwon",
      "Yonathan Efroni",
      "Constantine Caramanis",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12700"
  },
  {
    "id": "arXiv:2201.12701",
    "title": "DearFSAC: An Approach to Optimizing Unreliable Federated Learning via  Deep Reinforcement Learning",
    "abstract": "In federated learning (FL), model aggregation has been widely adopted for\ndata privacy. In recent years, assigning different weights to local models has\nbeen used to alleviate the FL performance degradation caused by differences\nbetween local datasets. However, when various defects make the FL process\nunreliable, most existing FL approaches expose weak robustness. In this paper,\nwe propose the DEfect-AwaRe federated soft actor-critic (DearFSAC) to\ndynamically assign weights to local models to improve the robustness of FL. The\ndeep reinforcement learning algorithm soft actor-critic is adopted for\nnear-optimal performance and stable convergence. Besides, an auto-encoder is\ntrained to output low-dimensional embedding vectors that are further utilized\nto evaluate model quality. In the experiments, DearFSAC outperforms three\nexisting approaches on four datasets for both independent and identically\ndistributed (IID) and non-IID settings under defective scenarios.",
    "descriptor": "",
    "authors": [
      "Chenghao Huang",
      "Weilong Chen",
      "Yuxi Chen",
      "Shunji Yang",
      "Yanru Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12701"
  },
  {
    "id": "arXiv:2201.12702",
    "title": "Robotic Wireless Energy Transfer in Dynamic Environments: System Design  and Experimental Validation",
    "abstract": "Wireless energy transfer (WET) is a ground-breaking technology for cutting\nthe last wire between mobile sensors and power grids in smart cities. Yet, WET\nonly offers effective transmission of energy over a short distance. Robotic WET\nis an emerging paradigm that mounts the energy transmitter on a mobile robot\nand navigates the robot through different regions in a large area to charge\nremote energy harvesters. However, it is challenging to determine the robotic\ncharging strategy in an unknown and dynamic environment due to the uncertainty\nof obstacles. This paper proposes a hardware-in-the-loop joint optimization\nframework that offers three distinctive features: 1) efficient model updates\nand re-optimization based on the last-round experimental data; 2) iterative\nrefinement of the anchor list for adaptation to different environments; 3)\nverification of algorithms in a high-fidelity Gazebo simulator and a\nmulti-robot testbed. Experimental results show that the proposed framework\nsignificantly saves the WET mission completion time while satisfying collision\navoidance and energy harvesting constraints.",
    "descriptor": "\nComments: 7 pages, 5 figures, IEEE Communications Magazine\n",
    "authors": [
      "Shuai Wang",
      "Ruihua Han",
      "Yuncong Hong",
      "Qi Hao",
      "Miaowen Wen",
      "Leila Musavian",
      "Shahid Mumtaz",
      "Derrick Wing Kwan Ng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12702"
  },
  {
    "id": "arXiv:2201.12705",
    "title": "A Robust Framework for Deep Learning Approaches to Facial Emotion  Recognition and Evaluation",
    "abstract": "Facial emotion recognition is a vast and complex problem space within the\ndomain of computer vision and thus requires a universally accepted baseline\nmethod with which to evaluate proposed models. While test datasets have served\nthis purpose in the academic sphere real world application and testing of such\nmodels lacks any real comparison. Therefore we propose a framework in which\nmodels developed for FER can be compared and contrasted against one another in\na constant standardized fashion. A lightweight convolutional neural network is\ntrained on the AffectNet dataset a large variable dataset for facial emotion\nrecognition and a web application is developed and deployed with our proposed\nframework as a proof of concept. The CNN is embedded into our application and\nis capable of instant real time facial emotion recognition. When tested on the\nAffectNet test set this model achieves high accuracy for emotion classification\nof eight different emotions. Using our framework the validity of this model and\nothers can be properly tested by evaluating a model efficacy not only based on\nits accuracy on a sample test dataset, but also on in the wild experiments.\nAdditionally, our application is built with the ability to save and store any\nimage captured or uploaded to it for emotion recognition, allowing for the\ncuration of more quality and diverse facial emotion recognition datasets.",
    "descriptor": "",
    "authors": [
      "Nyle Siddiqui",
      "Rushit Dave",
      "Tyler Bauer",
      "Thomas Reither",
      "Dylan Black",
      "Mitchell Hanson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12705"
  },
  {
    "id": "arXiv:2201.12709",
    "title": "Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty",
    "abstract": "Tensor recovery is an important problem in computer vision and machine\nlearning. It usually uses the convex relaxation of tensor rank and $l_{0}$\nnorm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve the\nproblem. It is well known that convex approximations produce biased estimators.\nIn order to overcome this problem, a corresponding non-convex regularizer has\nbeen proposed to solve it. Inspired by matrix equivalent Minimax-Concave\nPenalty (EMCP), we propose and prove theorems of tensor equivalent\nMinimax-Concave Penalty (TEMCP). The tensor equivalent MCP (TEMCP) as a\nnon-convex regularizer and the equivalent weighted tensor $\\gamma$ norm (EWTGN)\nwhich can represent the low-rank part are obtained. Both of them can realize\nweight adaptive. At the same time, we propose two corresponding adaptive models\nfor two classical tensor recovery problems, low-rank tensor completion (LRTC)\nand tensor robust principal component analysis (TRPCA), and the optimization\nalgorithm is based on alternating direction multiplier (ADMM). This novel\niterative adaptive algorithm can produce more accurate tensor recovery effect.\nFor the tensor completion model, multispectral image (MSI), magnetic resonance\nimaging (MRI) and color video (CV) data sets are considered, while for the\ntensor robust principal component analysis model, hyperspectral image (HSI)\ndenoising under gaussian noise plus salt and pepper noise is considered. The\nproposed algorithm is superior to the state-of-arts method, and the algorithm\nis guaranteed to meet the reduction and convergence through experiments.",
    "descriptor": "",
    "authors": [
      "Hongbing Zhang",
      "Xinyi Liu",
      "Hongtao Fan",
      "Yajing Li",
      "Yinlin Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12709"
  },
  {
    "id": "arXiv:2201.12710",
    "title": "An Asymptotically Optimal Algorithm for Maximum Matching in Dynamic  Streams",
    "abstract": "We present an algorithm for the maximum matching problem in dynamic\n(insertion-deletions) streams with *asymptotically optimal* space complexity:\nfor any $n$-vertex graph, our algorithm with high probability outputs an\n$\\alpha$-approximate matching in a single pass using $O(n^2/\\alpha^3)$ bits of\nspace.\nA long line of work on the dynamic streaming matching problem has reduced the\ngap between space upper and lower bounds first to $n^{o(1)}$ factors\n[Assadi-Khanna-Li-Yaroslavtsev; SODA 2016] and subsequently to\n$\\text{polylog}{(n)}$ factors [Dark-Konrad; CCC 2020]. Our upper bound now\nmatches the Dark-Konrad lower bound up to $O(1)$ factors, thus completing this\nresearch direction.\nOur approach consists of two main steps: we first (provably) identify a\nfamily of graphs, similar to the instances used in prior work to establish the\nlower bounds for this problem, as the only \"hard\" instances to focus on. These\ngraphs include an induced subgraph which is both sparse and contains a large\nmatching. We then design a dynamic streaming algorithm for this family of\ngraphs which is more efficient than prior work. The key to this efficiency is a\nnovel sketching method, which bypasses the typical loss of\n$\\text{polylog}{(n)}$-factors in space compared to standard $L_0$-sampling\nprimitives, and can be of independent interest in designing optimal algorithms\nfor other streaming problems.",
    "descriptor": "\nComments: Full version of the paper accepted to ITCS 2022. 42 pages, 5 Figures\n",
    "authors": [
      "Sepehr Assadi",
      "Vihan Shah"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12710"
  },
  {
    "id": "arXiv:2201.12712",
    "title": "Win the Lottery Ticket via Fourier Analysis: Frequencies Guided Network  Pruning",
    "abstract": "With the remarkable success of deep learning recently, efficient network\ncompression algorithms are urgently demanded for releasing the potential\ncomputational power of edge devices, such as smartphones or tablets. However,\noptimal network pruning is a non-trivial task which mathematically is an\nNP-hard problem. Previous researchers explain training a pruned network as\nbuying a lottery ticket. In this paper, we investigate the Magnitude-Based\nPruning (MBP) scheme and analyze it from a novel perspective through Fourier\nanalysis on the deep learning model to guide model designation. Besides\nexplaining the generalization ability of MBP using Fourier transform, we also\npropose a novel two-stage pruning approach, where one stage is to obtain the\ntopological structure of the pruned network and the other stage is to retrain\nthe pruned network to recover the capacity using knowledge distillation from\nlower to higher on the frequency domain. Extensive experiments on CIFAR-10 and\nCIFAR-100 demonstrate the superiority of our novel Fourier analysis based MBP\ncompared to other traditional MBP algorithms.",
    "descriptor": "\nComments: accepted to ICASSP 2022\n",
    "authors": [
      "Yuzhang Shang",
      "Bin Duan",
      "Ziliang Zong",
      "Liqiang Nie",
      "Yan Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12712"
  },
  {
    "id": "arXiv:2201.12714",
    "title": "The Complete SC-invariant Affine Automorphisms of Polar Codes",
    "abstract": "Automorphism ensemble (AE) decoding for polar codes was proposed by decoding\npermuted codewords with successive cancellation (SC) decoders in parallel and\nhence has lower latency compared to that of successive cancellation list (SCL)\ndecoding. However, some automorphisms are SC-invariant, thus are redundant in\nAE decoding. In this paper, we find a necessary and sufficient condition\nrelated to the block lower-triangular structure of transformation matrices to\nidentify SC-invariant automorphisms. Furthermore, we provide an algorithm to\ndetermine the complete SC-invariant affine automorphisms under a specific polar\ncode construction.",
    "descriptor": "",
    "authors": [
      "Zicheng Ye",
      "Yuan Li",
      "Huazi Zhang",
      "Rong Li",
      "Jun Wang",
      "Guiying Yan",
      "Zhiming Ma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12714"
  },
  {
    "id": "arXiv:2201.12716",
    "title": "You Only Demonstrate Once: Category-Level Manipulation from Single  Visual Demonstration",
    "abstract": "Promising results have been achieved recently in category-level manipulation\nthat generalizes across object instances. Nevertheless, it often requires\nexpensive real-world data collection and manual specification of semantic\nkeypoints for each object category and task. Additionally, coarse keypoint\npredictions and ignoring intermediate action sequences hinder adoption in\ncomplex manipulation tasks beyond pick-and-place. This work proposes a novel,\ncategory-level manipulation framework that leverages an object-centric,\ncategory-level representation and model-free 6 DoF motion tracking. The\ncanonical object representation is learned solely in simulation and then used\nto parse a category-level, task trajectory from a single demonstration video.\nThe demonstration is reprojected to a target trajectory tailored to a novel\nobject via the canonical representation. During execution, the manipulation\nhorizon is decomposed into long-range, collision-free motion and last-inch\nmanipulation. For the latter part, a category-level behavior cloning (CatBC)\nmethod leverages motion tracking to perform closed-loop control. CatBC follows\nthe target trajectory, projected from the demonstration and anchored to a\ndynamically selected category-level coordinate frame. The frame is\nautomatically selected along the manipulation horizon by a local attention\nmechanism. This framework allows to teach different manipulation strategies by\nsolely providing a single demonstration, without complicated manual\nprogramming. Extensive experiments demonstrate its efficacy in a range of\nchallenging industrial tasks in high-precision assembly, which involve learning\ncomplex, long-horizon policies. The process exhibits robustness against\nuncertainty due to dynamics as well as generalization across object instances\nand scene configurations.",
    "descriptor": "",
    "authors": [
      "Bowen Wen",
      "Wenzhao Lian",
      "Kostas Bekris",
      "Stefan Schaal"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12716"
  },
  {
    "id": "arXiv:2201.12718",
    "title": "Communication-Efficient Consensus Mechanism for Federated Reinforcement  Learning",
    "abstract": "The paper considers independent reinforcement learning (IRL) for multi-agent\ndecision-making process in the paradigm of federated learning (FL). We show\nthat FL can clearly improve the policy performance of IRL in terms of training\nefficiency and stability. However, since the policy parameters are trained\nlocally and aggregated iteratively through a central server in FL, frequent\ninformation exchange incurs a large amount of communication overheads. To reach\na good balance between improving the model's convergence performance and\nreducing the required communication and computation overheads, this paper\nproposes a system utility function and develops a consensus-based optimization\nscheme on top of the periodic averaging method, which introduces the consensus\nalgorithm into FL for the exchange of a model's local gradients. This paper\nalso provides novel convergence guarantees for the developed method, and\ndemonstrates its superior effectiveness and efficiency in improving the system\nutility value through theoretical analyses and numerical simulation results.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2103.13026\n",
    "authors": [
      "Xing Xu",
      "Rongpeng Li",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.12718"
  },
  {
    "id": "arXiv:2201.12719",
    "title": "Faster Convergence of Local SGD for Over-Parameterized Models",
    "abstract": "Modern machine learning architectures are often highly expressive. They are\nusually over-parameterized and can interpolate the data by driving the\nempirical loss close to zero. We analyze the convergence of Local SGD (or\nFedAvg) for such over-parameterized models in the heterogeneous data setting\nand improve upon the existing literature by establishing the following\nconvergence rates. We show an error bound of $\\O(\\exp(-T))$ for strongly-convex\nloss functions, where $T$ is the total number of iterations. For general convex\nloss functions, we establish an error bound of $\\O(1/T)$ under a mild data\nsimilarity assumption and an error bound of $\\O(K/T)$ otherwise, where $K$ is\nthe number of local steps. We also extend our results for non-convex loss\nfunctions by proving an error bound of $\\O(K/T)$. Before our work, the\nbest-known convergence rate for strongly-convex loss functions was\n$\\O(\\exp(-T/K))$, and none existed for general convex or non-convex loss\nfunctions under the overparameterized setting. We complete our results by\nproviding problem instances in which such convergence rates are tight to a\nconstant factor under a reasonably small stepsize scheme. Finally, we validate\nour theoretical results using numerical experiments on real and synthetic data.",
    "descriptor": "",
    "authors": [
      "Tiancheng Qin",
      "S. Rasoul Etesami",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12719"
  },
  {
    "id": "arXiv:2201.12723",
    "title": "VC-GPT: Visual Conditioned GPT for End-to-End Generative  Vision-and-Language Pre-training",
    "abstract": "Vision-and-language pre-training models (VLMs) have achieved tremendous\nsuccess in the cross-modal area, but most of them require millions of parallel\nimage-caption data for pre-training. Collating such data is expensive and\nlabor-intensive. In this work, we focus on reducing such need for generative\nvision-and-language pre-training (G-VLP) by taking advantage of the visual\npre-trained model (CLIP-ViT) as encoder and language pre-trained model (GPT2)\nas decoder. Unfortunately, GPT2 lacks a necessary cross-attention module, which\nhinders the direct connection of CLIP-ViT and GPT2. To remedy such defects, we\nconduct extensive experiments to empirically investigate how to design and\npre-train our model. Based on our experimental results, we propose a novel\nG-VLP framework, Visual Conditioned GPT (VC-GPT), and pre-train it with a\nsmall-scale parallel image-caption corpus (Visual Genome, only 110k distinct\nimages). Evaluating on the image captioning downstream tasks (MSCOCO and\nFlickr30k Captioning), VC-GPT achieves either the best or the second-best\nperformance across all evaluation metrics over the previous works which consume\naround 30 times more parallel data during pre-training.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Ziyang Luo",
      "Yadong Xi",
      "Rongsheng Zhang",
      "Jing Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12723"
  },
  {
    "id": "arXiv:2201.12724",
    "title": "Stochastic Neural Networks with Infinite Width are Deterministic",
    "abstract": "This work theoretically studies stochastic neural networks, a main type of\nneural network in use. Specifically, we prove that as the width of an optimized\nstochastic neural network tends to infinity, its predictive variance on the\ntraining set decreases to zero. Two common examples that our theory applies to\nare neural networks with dropout and variational autoencoders. Our result helps\nbetter understand how stochasticity affects the learning of neural networks and\nthus design better architectures for practical problems.",
    "descriptor": "",
    "authors": [
      "Liu Ziyin",
      "Hanlin Zhang",
      "Xiangming Meng",
      "Yuting Lu",
      "Eric Xing",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12724"
  },
  {
    "id": "arXiv:2201.12725",
    "title": "Neural Architecture Ranker",
    "abstract": "Architecture ranking has recently been advocated to design an efficient and\neffective performance predictor for Neural Architecture Search (NAS). The\nprevious contrastive method solves the ranking problem by comparing pairs of\narchitectures and predicting their relative performance, which may suffer\ngeneralization issues due to local pair-wise comparison. Inspired by the\nquality stratification phenomenon in the search space, we propose a predictor,\nnamely Neural Architecture Ranker (NAR), from a new and global perspective by\nexploiting the quality distribution of the whole search space. The NAR learns\nthe similar characteristics of the same quality tier (i.e., level) and\ndistinguishes among different individuals by first matching architectures with\nthe representation of tiers, and then classifying and scoring them. It can\ncapture the features of different quality tiers and thus generalize its ranking\nability to the entire search space. Besides, distributions of different quality\ntiers are also beneficial to guide the sampling procedure, which is free of\ntraining a search algorithm and thus simplifies the NAS pipeline. The proposed\nNAR achieves better performance than the state-of-the-art methods on two widely\naccepted datasets. On NAS-Bench-101, it finds the architectures with top\n0.01$\\unicode{x2030}$ performance among the search space and stably focuses on\nthe top architectures. On NAS-Bench-201, it identifies the optimal\narchitectures on CIFAR-10, CIFAR-100 and, ImageNet-16-120. We expand and\nrelease these two datasets covering detailed cell computational information to\nboost the study of NAS.",
    "descriptor": "",
    "authors": [
      "Bicheng Guo",
      "Shibo He",
      "Tao Chen",
      "Jiming Chen",
      "Peng Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12725"
  },
  {
    "id": "arXiv:2201.12727",
    "title": "Blockchain based AI-enabled Industry 4.0 CPS Protection against Advanced  Persistent Threat",
    "abstract": "Industry 4.0 is all about doing things in a concurrent, secure, and\nfine-grained manner. IoT edge-sensors and their associated data play a\npredominant role in today's industry ecosystem. Breaching data or forging\nsource devices after injecting advanced persistent threats (APT) damages the\nindustry owners' money and loss of operators' lives. The existing challenges\ninclude APT injection attacks targeting vulnerable edge devices, insecure data\ntransportation, trust inconsistencies among stakeholders, incompliant data\nstoring mechanisms, etc. Edge-servers often suffer because of their lightweight\ncomputation capacity to stamp out unauthorized data or instructions, which in\nessence, makes them exposed to attackers. When attackers target edge servers\nwhile transporting data using traditional PKI-rendered trusts, consortium\nblockchain (CBC) offers proven techniques to transfer and maintain those\nsensitive data securely. With the recent improvement of edge machine learning,\nedge devices can filter malicious data at their end which largely motivates us\nto institute a Blockchain and AI aligned APT detection system. The unique\ncontributions of the paper include efficient APT detection at the edge and\ntransparent recording of the detection history in an immutable blockchain\nledger. In line with that, the certificateless data transfer mechanism boost\ntrust among collaborators and ensure an economical and sustainable mechanism\nafter eliminating existing certificate authority. Finally, the edge-compliant\nstorage technique facilitates efficient predictive maintenance. The respective\nexperimental outcomes reveal that the proposed technique outperforms the other\ncompeting systems and models.",
    "descriptor": "\nComments: 10 Pages, 9 Figures, 3 Tables Published in the IEEE Internet of Things Journal\n",
    "authors": [
      "Ziaur Rahman",
      "Xun Yi Ibrahim Khalil"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12727"
  },
  {
    "id": "arXiv:2201.12728",
    "title": "Video-based Facial Micro-Expression Analysis: A Survey of Datasets,  Features and Algorithms",
    "abstract": "Unlike the conventional facial expressions, micro-expressions are involuntary\nand transient facial expressions capable of revealing the genuine emotions that\npeople attempt to hide. Therefore, they can provide important information in a\nbroad range of applications such as lie detection, criminal detection, etc.\nSince micro-expressions are transient and of low intensity, however, their\ndetection and recognition is difficult and relies heavily on expert\nexperiences. Due to its intrinsic particularity and complexity, video-based\nmicro-expression analysis is attractive but challenging, and has recently\nbecome an active area of research. Although there have been numerous\ndevelopments in this area, thus far there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences between macro- and micro-expressions, then use these\ndifferences to guide our research survey of video-based micro-expression\nanalysis in a cascaded structure, encompassing the neuropsychological basis,\ndatasets, features, spotting algorithms, recognition algorithms, applications\nand evaluation of state-of-the-art approaches. For each aspect, the basic\ntechniques, advanced developments and major challenges are addressed and\ndiscussed. Furthermore, after considering the limitations of existing\nmicro-expression datasets, we present and release a new dataset - called\nmicro-and-macro expression warehouse (MMEW) - containing more video samples and\nmore labeled emotion types. We then perform a unified comparison of\nrepresentative methods on CAS(ME)2 for spotting, and on MMEW and SAMM for\nrecognition, respectively. Finally, some potential future research directions\nare explored and outlined.",
    "descriptor": "",
    "authors": [
      "Xianye Ben",
      "Yi Ren",
      "Junping Zhang",
      "Su-Jing Wang",
      "Kidiyo Kpalma",
      "Weixiao Meng",
      "Yong-Jin Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12728"
  },
  {
    "id": "arXiv:2201.12733",
    "title": "TPC: Transformation-Specific Smoothing for Point Cloud Models",
    "abstract": "Point cloud models with neural network architectures have achieved great\nsuccess and have been widely used in safety-critical applications, such as\nLidar-based recognition systems in autonomous vehicles. However, such models\nare shown vulnerable against adversarial attacks which aim to apply stealthy\nsemantic transformations such as rotation and tapering to mislead model\npredictions. In this paper, we propose a transformation-specific smoothing\nframework TPC, which provides tight and scalable robustness guarantees for\npoint cloud models against semantic transformation attacks. We first categorize\ncommon 3D transformations into three categories: additive (e.g., shearing),\ncomposable (e.g., rotation), and indirectly composable (e.g., tapering), and we\npresent generic robustness certification strategies for all categories\nrespectively. We then specify unique certification protocols for a range of\nspecific semantic transformations and their compositions. Extensive experiments\non several common 3D transformations show that TPC significantly outperforms\nthe state of the art. For example, our framework boosts the certified accuracy\nagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$\nto 83.8$\\%$.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Wenda Chu",
      "Linyi Li",
      "Bo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12733"
  },
  {
    "id": "arXiv:2201.12736",
    "title": "No-Regret Learning in Time-Varying Zero-Sum Games",
    "abstract": "Learning from repeated play in a fixed two-player zero-sum game is a classic\nproblem in game theory and online learning. We consider a variant of this\nproblem where the game payoff matrix changes over time, possibly in an\nadversarial manner. We first present three performance measures to guide the\nalgorithmic design for this problem: 1) the well-studied individual regret, 2)\nan extension of duality gap, and 3) a new measure called dynamic Nash\nEquilibrium regret, which quantifies the cumulative difference between the\nplayer's payoff and the minimax game value. Next, we develop a single\nparameter-free algorithm that simultaneously enjoys favorable guarantees under\nall these three performance measures. These guarantees are adaptive to\ndifferent non-stationarity measures of the payoff matrices and, importantly,\nrecover the best known results when the payoff matrix is fixed. Our algorithm\nis based on a two-layer structure with a meta-algorithm learning over a group\nof black-box base-learners satisfying a certain property, along with several\nnovel ingredients specifically designed for the time-varying game setting.\nEmpirical results further validate the effectiveness of our algorithm.",
    "descriptor": "",
    "authors": [
      "Mengxiao Zhang",
      "Peng Zhao",
      "Haipeng Luo",
      "Zhi-Hua Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12736"
  },
  {
    "id": "arXiv:2201.12738",
    "title": "AutoSNN: Towards Energy-Efficient Spiking Neural Networks",
    "abstract": "Spiking neural networks (SNNs) that mimic information transmission in the\nbrain can energy-efficiently process spatio-temporal information through\ndiscrete and sparse spikes, thereby receiving considerable attention. To\nimprove accuracy and energy efficiency of SNNs, most previous studies have\nfocused solely on training methods, and the effect of architecture has rarely\nbeen studied. We investigate the design choices used in the previous studies in\nterms of the accuracy and number of spikes and figure out that they are not\nbest-suited for SNNs. To further improve the accuracy and reduce the spikes\ngenerated by SNNs, we propose a spike-aware neural architecture search\nframework called AutoSNN. We define a search space consisting of architectures\nwithout undesirable design choices. To enable the spike-aware architecture\nsearch, we introduce a fitness that considers both the accuracy and number of\nspikes. AutoSNN successfully searches for SNN architectures that outperform\nhand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate\nthe effectiveness of AutoSNN on various datasets including neuromorphic\ndatasets.",
    "descriptor": "",
    "authors": [
      "Byunggook Na",
      "Jisoo Mok",
      "Seongsik Park",
      "Dongjin Lee",
      "Hyeokjun Choe",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12738"
  },
  {
    "id": "arXiv:2201.12739",
    "title": "Do We Need to Penalize Variance of Losses for Learning with Label Noise?",
    "abstract": "Algorithms which minimize the averaged loss have been widely designed for\ndealing with noisy labels. Intuitively, when there is a finite training sample,\npenalizing the variance of losses will improve the stability and generalization\nof the algorithms. Interestingly, we found that the variance should be\nincreased for the problem of learning with noisy labels. Specifically,\nincreasing the variance will boost the memorization effects and reduce the\nharmfulness of incorrect labels. By exploiting the label noise transition\nmatrix, regularizers can be easily designed to reduce the variance of losses\nand be plugged in many existing algorithms. Empirically, the proposed method by\nincreasing the variance of losses significantly improves the generalization\nability of baselines on both synthetic and real-world datasets.",
    "descriptor": "",
    "authors": [
      "Yexiong Lin",
      "Yu Yao",
      "Yuxuan Du",
      "Jun Yu",
      "Bo Han",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12739"
  },
  {
    "id": "arXiv:2201.12740",
    "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term  Series Forecasting",
    "abstract": "Although Transformer-based methods have significantly improved\nstate-of-the-art results for long-term series forecasting, they are not only\ncomputationally expensive but more importantly, are unable to capture the\nglobal view of time series (e.g. overall trend). To address these problems, we\npropose to combine Transformer with the seasonal-trend decomposition method, in\nwhich the decomposition method captures the global profile of time series while\nTransformers capture more detailed structures. To further enhance the\nperformance of Transformer for long-term prediction, we exploit the fact that\nmost time series tend to have a sparse representation in well-known basis such\nas Fourier transform, and develop a frequency enhanced Transformer. Besides\nbeing more effective, the proposed method, termed as Frequency Enhanced\nDecomposed Transformer ({\\bf FEDformer}), is more efficient than standard\nTransformer with a linear complexity to the sequence length. Our empirical\nstudies with six benchmark datasets show that compared with state-of-the-art\nmethods, FEDformer can reduce prediction error by $14.8\\%$ and $22.6\\%$ for\nmultivariate and univariate time series, respectively. the code will be\nreleased soon.",
    "descriptor": "",
    "authors": [
      "Tian Zhou",
      "Ziqing Ma",
      "Qingsong Wen",
      "Xue Wang",
      "Liang Sun",
      "Rong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12740"
  },
  {
    "id": "arXiv:2201.12741",
    "title": "GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph  Neural Networks",
    "abstract": "Graph neural networks (GNNs) have been increasingly deployed in various\napplications that involve learning on non-Euclidean data. However, recent\nstudies show that GNNs are vulnerable to graph adversarial attacks. Although\nthere are several defense methods to improve GNN robustness by eliminating\nadversarial components, they may also impair the underlying clean graph\nstructure that contributes to GNN training. In addition, few of those defense\nmodels can scale to large graphs due to their high computational complexity and\nmemory usage. In this paper, we propose GARNET, a scalable spectral method to\nboost the adversarial robustness of GNN models. GARNET first leverages weighted\nspectral embedding to construct a base graph, which is not only resistant to\nadversarial attacks but also contains critical (clean) graph structure for GNN\ntraining. Next, GARNET further refines the base graph by pruning additional\nuncritical edges based on probabilistic graphical model. GARNET has been\nevaluated on various datasets, including a large graph with millions of nodes.\nOur extensive experiment results show that GARNET achieves adversarial accuracy\nimprovement and runtime speedup over state-of-the-art GNN (defense) models by\nup to 13.27% and 14.7x, respectively.",
    "descriptor": "",
    "authors": [
      "Chenhui Deng",
      "Xiuyu Li",
      "Zhuo Feng",
      "Zhiru Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12741"
  },
  {
    "id": "arXiv:2201.12746",
    "title": "Efficient Near-Optimal Codes for General Repeat Channels",
    "abstract": "Given a probability distribution $\\mathcal{D}$ over the non-negative\nintegers, a $\\mathcal{D}$-repeat channel acts on an input symbol by repeating\nit a number of times distributed as $\\mathcal{D}$. For example, the binary\ndeletion channel ($\\mathcal{D}=Bernoulli$) and the Poisson repeat channel\n($\\mathcal{D}=Poisson$) are special cases. We say a $\\mathcal{D}$-repeat\nchannel is square-integrable if $\\mathcal{D}$ has finite first and second\nmoments. In this paper, we construct explicit codes for all square-integrable\n$\\mathcal{D}$-repeat channels with rate arbitrarily close to the capacity, that\nare encodable and decodable in linear and quasi-linear time, respectively. We\nalso consider possible extensions to the repeat channel model, and illustrate\nhow our construction can be extended to an even broader class of channels\ncapturing insertions, deletions, and substitutions.\nOur work offers an alternative, simplified, and more general construction to\nthe recent work of Rubinstein (arXiv:2111.00261), who attains similar results\nto ours in the cases of the deletion channel and the Poisson repeat channel. It\nalso improves on the decoding failure probability of the polar codes\nconstructions of Tal et al. for the deletion channel (ISIT 2019) and certain\ninsertion/deletion/substitution channels (arXiv:2102.02155). Our techniques\nfollow closely the approaches of Guruswami and Li (IEEEToIT 2019) and Con and\nShpilka (IEEEToIT 2020); what sets apart our work is that we show that a\ncapacity-achieving code for the channels in question can be assumed to have an\n\"approximate balance\" in the frequency of zeros and ones of all sufficiently\nlong substrings of all codewords. This allows us to attain\nnear-capacity-achieving codes in a general setting. We consider this\n\"approximate balance\" result to be of independent interest, as it can be cast\nin much greater generality than repeat channels.",
    "descriptor": "",
    "authors": [
      "Francisco Pernice",
      "Ray Li",
      "Mary Wootters"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12746"
  },
  {
    "id": "arXiv:2201.12760",
    "title": "Implicit Regularization Towards Rank Minimization in ReLU Networks",
    "abstract": "We study the conjectured relationship between the implicit regularization in\nneural networks, trained with gradient-based methods, and rank minimization of\ntheir weight matrices. Previously, it was proved that for linear networks (of\ndepth 2 and vector-valued outputs), gradient flow (GF) w.r.t. the square loss\nacts as a rank minimization heuristic. However, understanding to what extent\nthis generalizes to nonlinear networks is an open problem. In this paper, we\nfocus on nonlinear ReLU networks, providing several new positive and negative\nresults. On the negative side, we prove (and demonstrate empirically) that,\nunlike the linear case, GF on ReLU networks may no longer tend to minimize\nranks, in a rather strong sense (even approximately, for \"most\" datasets of\nsize 2). On the positive side, we reveal that ReLU networks of sufficient depth\nare provably biased towards low-rank solutions in several reasonable settings.",
    "descriptor": "",
    "authors": [
      "Nadav Timor",
      "Gal Vardi",
      "Ohad Shamir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12760"
  },
  {
    "id": "arXiv:2201.12763",
    "title": "RIM-Net: Recursive Implicit Fields for Unsupervised Learning of  Hierarchical Shape Structures",
    "abstract": "We introduce RIM-Net, a neural network which learns recursive implicit fields\nfor unsupervised inference of hierarchical shape structures. Our network\nrecursively decomposes an input 3D shape into two parts, resulting in a binary\ntree hierarchy. Each level of the tree corresponds to an assembly of shape\nparts, represented as implicit functions, to reconstruct the input shape. At\neach node of the tree, simultaneous feature decoding and shape decomposition\nare carried out by their respective feature and part decoders, with weight\nsharing across the same hierarchy level. As an implicit field decoder, the part\ndecoder is designed to decompose a sub-shape, via a two-way branched\nreconstruction, where each branch predicts a set of parameters defining a\nGaussian to serve as a local point distribution for shape reconstruction. With\nreconstruction losses accounted for at each hierarchy level and a decomposition\nloss at each node, our network training does not require any ground-truth\nsegmentations, let alone hierarchies. Through extensive experiments and\ncomparisons to state-of-the-art alternatives, we demonstrate the quality,\nconsistency, and interpretability of hierarchical structural inference by\nRIM-Net.",
    "descriptor": "",
    "authors": [
      "Chengjie Niu",
      "Manyi Li",
      "Kai Xu",
      "Hao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12763"
  },
  {
    "id": "arXiv:2201.12765",
    "title": "Improving Corruption and Adversarial Robustness by Enhancing Weak  Subnets",
    "abstract": "Deep neural networks have achieved great success in many computer vision\ntasks. However, deep networks have been shown to be very susceptible to\ncorrupted or adversarial images, which often result in significant performance\ndrops. In this paper, we observe that weak subnetwork (subnet) performance is\ncorrelated with a lack of robustness against corruptions and adversarial\nattacks. Based on that observation, we propose a novel robust training method\nwhich explicitly identifies and enhances weak subnets (EWS) during training to\nimprove robustness. Specifically, we develop a search algorithm to find\nparticularly weak subnets and propose to explicitly strengthen them via\nknowledge distillation from the full network. We show that our EWS greatly\nimproves the robustness against corrupted images as well as the accuracy on\nclean data. Being complementary to many state-of-the-art data augmentation\napproaches, EWS consistently improves corruption robustness on top of many of\nthese approaches. Moreover, EWS is also able to boost the adversarial\nrobustness when combined with popular adversarial training methods.",
    "descriptor": "",
    "authors": [
      "Yong Guo",
      "David Stutz",
      "Bernt Schiele"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12765"
  },
  {
    "id": "arXiv:2201.12767",
    "title": "Bayesian Optimization For Multi-Objective Mixed-Variable Problems",
    "abstract": "Optimizing multiple, non-preferential objectives for mixed-variable,\nexpensive black-box problems is important in many areas of engineering and\nscience. The expensive, noisy black-box nature of these problems makes them\nideal candidates for Bayesian optimization (BO). Mixed-variable and\nmulti-objective problems, however, are a challenge due to the BO's underlying\nsmooth Gaussian process surrogate model. Current multi-objective BO algorithms\ncannot deal with mixed-variable problems. We present MixMOBO, the first mixed\nvariable multi-objective Bayesian optimization framework for such problems.\nUsing a genetic algorithm to sample the surrogate surface, optimal\nPareto-fronts for multi-objective, mixed-variable design spaces can be found\nefficiently while ensuring diverse solutions. The method is sufficiently\nflexible to incorporate many different kernels and acquisition functions,\nincluding those that were developed for mixed-variable or multi-objective\nproblems by other authors. We also present HedgeMO, a modified Hedge strategy\nthat uses a portfolio of acquisition functions in multi-objective problems. We\npresent a new acquisition function SMC. We show that MixMOBO performs well\nagainst other mixed-variable algorithms on synthetic problems. We apply MixMOBO\nto the real-world design of an architected material and show that our optimal\ndesign, which was experimentally fabricated and validated, has a normalized\nstrain energy density $10^4$ times greater than existing structures.",
    "descriptor": "\nComments: 16 pages (including 4 index pages), 4 figures, and data sets included\n",
    "authors": [
      "Haris Moazam Sheikh",
      "Philip S. Marcus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12767"
  },
  {
    "id": "arXiv:2201.12769",
    "title": "MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale  Point Clouds",
    "abstract": "Semantic segmentation of 3D point cloud is an essential task for autonomous\ndriving environment perception. The pipeline of most pointwise point cloud\nsemantic segmentation methods includes points sampling, neighbor searching,\nfeature aggregation, and classification. Neighbor searching method like\nK-nearest neighbors algorithm, KNN, has been widely applied. However, the\ncomplexity of KNN is always a bottleneck of efficiency. In this paper, we\npropose an end-to-end neural architecture, Multiple View Pointwise Net,\nMVP-Net, to efficiently and directly infer large-scale outdoor point cloud\nwithout KNN or any complex pre/postprocessing. Instead, assumption-based\nsorting and multi-rotation of point cloud methods are introduced to point\nfeature aggregation and receptive field expanding. Numerical experiments show\nthat the proposed MVP-Net is 11 times faster than the most efficient pointwise\nsemantic segmentation method RandLA-Net and achieves the same accuracy on the\nlarge-scale benchmark SemanticKITTI dataset.",
    "descriptor": "",
    "authors": [
      "Chuanyu Luo",
      "Xiaohan Li",
      "Nuo Cheng",
      "Han Li",
      "Shengguang Lei",
      "Pu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12769"
  },
  {
    "id": "arXiv:2201.12771",
    "title": "Self-Supervised Moving Vehicle Detection from Audio-Visual Cues",
    "abstract": "Robust detection of moving vehicles is a critical task for any autonomously\noperating outdoor robot or self-driving vehicle. Most modern approaches for\nsolving this task rely on training image-based detectors using large-scale\nvehicle detection datasets such as nuScenes or the Waymo Open Dataset.\nProviding manual annotations is an expensive and laborious exercise that does\nnot scale well in practice. To tackle this problem, we propose a\nself-supervised approach that leverages audio-visual cues to detect moving\nvehicles in videos. Our approach employs contrastive learning for localizing\nvehicles in images from corresponding pairs of images and recorded audio. In\nextensive experiments carried out with a real-world dataset, we demonstrate\nthat our approach provides accurate detections of moving vehicles and does not\nrequire manual annotations. We furthermore show that our model can be used as a\nteacher to supervise an audio-only detection model. This student model is\ninvariant to illumination changes and thus effectively bridges the domain gap\ninherent to models leveraging exclusively vision as the predominant modality.",
    "descriptor": "\nComments: 8 pages, 6 figures\n",
    "authors": [
      "Jannik Z\u00fcrn",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12771"
  },
  {
    "id": "arXiv:2201.12772",
    "title": "Polynomial-Time Approximation of Zero-Free Partition Functions",
    "abstract": "Zero-free based algorithm is a major technique for deterministic approximate\ncounting. In Barvinok's original framework[Bar17], by calculating truncated\nTaylor expansions, a quasi-polynomial time algorithm was given for estimating\nzero-free partition functions. Patel and Regts[PR17] later gave a refinement of\nBarvinok's framework, which gave a polynomial-time algorithm for a class of\nzero-free graph polynomials that can be expressed as counting induced subgraphs\nin bounded-degree graphs.\nIn this paper, we give a polynomial-time algorithm for estimating classical\nand quantum partition functions specified by local Hamiltonians with bounded\nmaximum degree, assuming a zero-free property for the temperature.\nConsequently, when the inverse temperature is close enough to zero by a\nconstant gap, we have polynomial-time approximation algorithm for all such\npartition functions. Our result is based on a new abstract framework that\nextends and generalizes the approach of Patel and Regts.",
    "descriptor": "",
    "authors": [
      "Penghui Yao",
      "Yitong Yin",
      "Xinyuan Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.12772"
  },
  {
    "id": "arXiv:2201.12776",
    "title": "Graph Convolution-Based Deep Reinforcement Learning for Multi-Agent  Decision-Making in Mixed Traffic Environments",
    "abstract": "An efficient and reliable multi-agent decision-making system is highly\ndemanded for the safe and efficient operation of connected autonomous vehicles\nin intelligent transportation systems. Current researches mainly focus on the\nDeep Reinforcement Learning (DRL) methods. However, utilizing DRL methods in\ninteractive traffic scenarios is hard to represent the mutual effects between\ndifferent vehicles and model the dynamic traffic environments due to the lack\nof interactive information in the representation of the environments, which\nresults in low accuracy of cooperative decisions generation. To tackle these\ndifficulties, this research proposes a framework to enable different Graph\nReinforcement Learning (GRL) methods for decision-making, and compares their\nperformance in interactive driving scenarios. GRL methods combinate the Graph\nNeural Network (GNN) and DRL to achieve the better decisions generation in\ninteractive scenarios of autonomous vehicles, where the features of interactive\nscenarios are extracted by the GNN, and cooperative behaviors are generated by\nDRL framework. Several GRL approaches are summarized and implemented in the\nproposed framework. To evaluate the performance of the proposed GRL methods, an\ninteractive driving scenarios on highway with two ramps is constructed, and\nsimulated experiment in the SUMO platform is carried out to evaluate the\nperformance of different GRL approaches. Finally, results are analyzed in\nmultiple perspectives and dimensions to compare the characteristic of different\nGRL approaches in intelligent transportation scenarios. Results show that the\nimplementation of GNN can well represents the interaction between vehicles, and\nthe combination of GNN and DRL is able to improve the performance of the\ngeneration of lane-change behaviors. The source code of our work can be found\nat https://github.com/Jacklinkk/TorchGRL.",
    "descriptor": "\nComments: 8 pages, 7 figures, 5 tables, IV2022 conference\n",
    "authors": [
      "Qi Liu",
      "Zirui Li",
      "Xueyuan Li",
      "Jingda Wu",
      "Shihua Yuan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12776"
  },
  {
    "id": "arXiv:2201.12781",
    "title": "Low Rank Approximation of Dual Complex Matrices",
    "abstract": "Dual complex numbers can represent rigid body motion in 2D spaces. Dual\ncomplex matrices are linked with screw theory, and have potential applications\nin various areas. In this paper, we study low rank approximation of dual\ncomplex matrices. We define $2$-norm for dual complex vectors, and Frobenius\nnorm for dual complex matrices. These norms are nonnegative dual numbers. We\nestablish the unitary invariance property of dual complex matrices. We study\neigenvalues of square dual complex matrices, and show that an $n \\times n$ dual\ncomplex Hermitian matrix has exactly $n$ eigenvalues, which are dual numbers.\nWe present a singular value decomposition (SVD) theorem for dual complex\nmatrices, define ranks and appreciable ranks for dual complex matrices, and\nstudy their properties. We establish an Eckart-Young like theorem for dual\ncomplex matrices, and present an algorithm framework for low rank approximation\nof dual complex matrices via truncated SVD. The SVD of dual complex matrices\nalso provides a basic tool for Principal Component Analysis (PCA) via these\nmatrices. Numerical experiments are reported.",
    "descriptor": "",
    "authors": [
      "Liqun Qi",
      "David M. Alexander",
      "Zhongming Chen",
      "Chen Ling",
      "Ziyan Luo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2201.12781"
  },
  {
    "id": "arXiv:2201.12786",
    "title": "Similarity Search on Computational Notebooks",
    "abstract": "Computational notebook software such as Jupyter Notebook is popular for data\nscience tasks. Numerous computational notebooks are available on the Web and\nreusable; however, searching for computational notebooks manually is a tedious\ntask, and so far, there are no tools to search for computational notebooks\neffectively and efficiently. In this paper, we propose a similarity search on\ncomputational notebooks and develop a new framework for the similarity search.\nGiven contents (i.e., source codes, tabular data, libraries, and outputs\nformats) in computational notebooks as a query, the similarity search problem\naims to find top-k computational notebooks with the most similar contents. We\ndefine two similarity measures; set-based and graph-based similarities.\nSet-based similarity handles each content independently, while graph-based\nsimilarity captures the relationships between contents. Our framework can\neffectively prune the candidates of computational notebooks that should not be\nin the top-k results. Furthermore, we develop optimization techniques such as\ncaching and indexing to accelerate the search. Experiments using Kaggle\nnotebooks show that our method, in particular graph-based similarity, can\nachieve high accuracy and high efficiency.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Misato Horiuchi",
      "Yuya Sasaki",
      "Chuan Xiao",
      "Makoto Onizuka"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.12786"
  },
  {
    "id": "arXiv:2201.12787",
    "title": "Graph Self-Attention for learning graph representation with Transformer",
    "abstract": "We propose a novel Graph Self-Attention module to enable Transformer models\nto learn graph representation. We aim to incorporate graph information, on the\nattention map and hidden representations of Transformer. To this end, we\npropose context-aware attention which considers the interactions between query,\nkey and graph information. Moreover, we propose graph-embedded value to encode\nthe graph information on the hidden representation. Our extensive experiments\nand ablation studies validate that our method successfully encodes graph\nrepresentation on Transformer architecture. Finally, our method achieves\nstate-of-the-art performance on multiple benchmarks of graph representation\nlearning, such as graph classification on images and molecules to graph\nregression on quantum chemistry.",
    "descriptor": "",
    "authors": [
      "Wonpyo Park",
      "Woonggi Chang",
      "Donggeon Lee",
      "Juntae Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12787"
  },
  {
    "id": "arXiv:2201.12790",
    "title": "Solving Routing Problems via Important Cuts",
    "abstract": "We introduce a novel approach of using important cuts which allowed us to\ndesign significantly faster fixed-parameter tractable (FPT) algorithms for the\nfollowing routing problems: the Mixed Chinese Postman Problem parameterized by\nthe number of directed edges (Gutin et al., JCSS 2017), the Minimum Shared\nEdges problem (MSE) parameterized by the number p of paths between two\nspecified vertices (Fluschnik et al., JCSS 2019), and the Weighted Min Cut\nPrevention problem (Gruttemeier et al., WG 2021). The Minimum Vulnerability\nproblem (MV) is a generalization of MSE (Assadi et al., Algorithmica 2014). The\nonly known FPT algorithm for MV parameterized by p (the same parameter as for\nMSE) was for chordal graphs (Aoki et al., JCO 2018). We design an FPT algorithm\nfor MV on all undirected graphs.",
    "descriptor": "",
    "authors": [
      "Bin Sheng",
      "Gregory Gutin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12790"
  },
  {
    "id": "arXiv:2201.12792",
    "title": "SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video",
    "abstract": "We propose SelfRecon, a clothed human body reconstruction method that\ncombines implicit and explicit representations to recover space-time coherent\ngeometries from a monocular self-rotating human video. Explicit methods require\na predefined template mesh for a given sequence, while the template is hard to\nacquire for a specific subject. Meanwhile, the fixed topology limits the\nreconstruction accuracy and clothing types. Implicit methods support arbitrary\ntopology and have high quality due to continuous geometric representation.\nHowever, it is difficult to integrate multi-frame information to produce a\nconsistent registration sequence for downstream applications. We propose to\ncombine the advantages of both representations. We utilize differential mask\nloss of the explicit mesh to obtain the coherent overall shape, while the\ndetails on the implicit surface are refined with the differentiable neural\nrendering. Meanwhile, the explicit mesh is updated periodically to adjust its\ntopology changes, and a consistency loss is designed to match both\nrepresentations closely. Compared with existing methods, SelfRecon can produce\nhigh-fidelity surfaces for arbitrary clothed humans with self-supervised\noptimization. Extensive experimental results demonstrate its effectiveness on\nreal captured monocular videos.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Boyi Jiang",
      "Yang Hong",
      "Hujun Bao",
      "Juyong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.12792"
  },
  {
    "id": "arXiv:2201.12793",
    "title": "Part of Speech Tagging (POST) of a Low-resource Language using another  Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged  Persian (Farsi) Corpus)",
    "abstract": "Tagged corpora play a crucial role in a wide range of Natural Language\nProcessing. The Part of Speech Tagging (POST) is essential in developing tagged\ncorpora. It is time-and-effort-consuming and costly, and therefore, it could be\nmore affordable if it is automated. The Kurdish language currently lacks\npublicly available tagged corpora of proper sizes. Tagging the publicly\navailable Kurdish corpora can leverage the capability of those resources to a\nhigher level than what raw or segmented corpora can provide. Developing\nPOS-tagged lexicons can assist the mentioned task. We use a tagged corpus\n(Bijankhan corpus) in Persian (Farsi) as a close language to Kurdish to develop\na POS-tagged lexicon. This paper presents the approach of leveraging the\nresource of a close language to Kurdish to enrich its resources. A partial\ndataset of the results is publicly available for non-commercial use under CC\nBY-NC-SA 4.0 license at https://kurdishblark.github.io/. We plan to make the\nwhole tagged corpus available after further investigation on the outcome. The\ndataset can help in developing POS-tagged lexicons for other Kurdish dialects\nand automated Kurdish corpora tagging.",
    "descriptor": "\nComments: 7pages, 2 tables, 3 figures\n",
    "authors": [
      "Hossein Hassani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12793"
  },
  {
    "id": "arXiv:2201.12795",
    "title": "Training Thinner and Deeper Neural Networks: Jumpstart Regularization",
    "abstract": "Neural networks are more expressive when they have multiple layers. In turn,\nconventional training methods are only successful if the depth does not lead to\nnumerical issues such as exploding or vanishing gradients, which occur less\nfrequently when the layers are sufficiently wide. However, increasing width to\nattain greater depth entails the use of heavier computational resources and\nleads to overparameterized models. These subsequent issues have been partially\naddressed by model compression methods such as quantization and pruning, some\nof which relying on normalization-based regularization of the loss function to\nmake the effect of most parameters negligible. In this work, we propose instead\nto use regularization for preventing neurons from dying or becoming linear, a\ntechnique which we denote as jumpstart regularization. In comparison to\nconventional training, we obtain neural networks that are thinner, deeper, and\n- most importantly - more parameter-efficient.",
    "descriptor": "",
    "authors": [
      "Carles Riera",
      "Camilo Rey",
      "Thiago Serra",
      "Eloi Puertas",
      "Oriol Pujol"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12795"
  },
  {
    "id": "arXiv:2201.12796",
    "title": "Co-Regularized Adversarial Learning for Multi-Domain Text Classification",
    "abstract": "Multi-domain text classification (MDTC) aims to leverage all available\nresources from multiple domains to learn a predictive model that can generalize\nwell on these domains. Recently, many MDTC methods adopt adversarial learning,\nshared-private paradigm, and entropy minimization to yield state-of-the-art\nresults. However, these approaches face three issues: (1) Minimizing domain\ndivergence can not fully guarantee the success of domain alignment; (2)\nAligning marginal feature distributions can not fully guarantee the\ndiscriminability of the learned features; (3) Standard entropy minimization may\nmake the predictions on unlabeled data over-confident, deteriorating the\ndiscriminability of the learned features. In order to address the above issues,\nwe propose a co-regularized adversarial learning (CRAL) mechanism for MDTC.\nThis approach constructs two diverse shared latent spaces, performs domain\nalignment in each of them, and punishes the disagreements of these two\nalignments with respect to the predictions on unlabeled data. Moreover, virtual\nadversarial training (VAT) with entropy minimization is incorporated to impose\nconsistency regularization to the CRAL method. Experiments show that our model\noutperforms state-of-the-art methods on two MDTC benchmarks.",
    "descriptor": "\nComments: The paper will appear in AISTATS 2022\n",
    "authors": [
      "Yuan Wu",
      "Diana Inkpen",
      "Ahmed El-Roby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12796"
  },
  {
    "id": "arXiv:2201.12799",
    "title": "Recognition of Implicit Geographic Movement in Text",
    "abstract": "Analyzing the geographic movement of humans, animals, and other phenomena is\na growing field of research. This research has benefited urban planning,\nlogistics, animal migration understanding, and much more. Typically, the\nmovement is captured as precise geographic coordinates and time stamps with\nGlobal Positioning Systems (GPS). Although some research uses computational\ntechniques to take advantage of implicit movement in descriptions of route\ndirections, hiking paths, and historical exploration routes, innovation would\naccelerate with a large and diverse corpus. We created a corpus of sentences\nlabeled as describing geographic movement or not and including the type of\nentity moving. Creating this corpus proved difficult without any comparable\ncorpora to start with, high human labeling costs, and since movement can at\ntimes be interpreted differently. To overcome these challenges, we developed an\niterative process employing hand labeling, crowd voting for confirmation, and\nmachine learning to predict more labels. By merging advances in word embeddings\nwith traditional machine learning models and model ensembling, prediction\naccuracy is at an acceptable level to produce a large silver-standard corpus\ndespite the small gold-standard corpus training set. Our corpus will likely\nbenefit computational processing of geography in text and spatial cognition, in\naddition to detection of movement.",
    "descriptor": "",
    "authors": [
      "Scott Pezanowski",
      "Prasenjit Mitra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12799"
  },
  {
    "id": "arXiv:2201.12803",
    "title": "Similarity and Generalization: From Noise to Corruption",
    "abstract": "Contrastive learning aims to extract distinctive features from data by\nfinding an embedding representation where similar samples are close to each\nother, and different ones are far apart. We study generalization in contrastive\nlearning, focusing on its simplest representative: Siamese Neural Networks\n(SNNs). We show that Double Descent also appears in SNNs and is exacerbated by\nnoise. We point out that SNNs can be affected by two distinct sources of noise:\nPair Label Noise (PLN) and Single Label Noise (SLN). The effect of SLN is\nasymmetric, but it preserves similarity relations, while PLN is symmetric but\nbreaks transitivity. We show that the dataset topology crucially affects\ngeneralization. While sparse datasets show the same performances under SLN and\nPLN for an equal amount of noise, SLN outperforms PLN in the overparametrized\nregion in dense datasets. Indeed, in this regime, PLN similarity violation\nbecomes macroscopical, corrupting the dataset to the point where complete\noverfitting cannot be achieved. We call this phenomenon Density-Induced Break\nof Similarity (DIBS). We also probe the equivalence between online optimization\nand offline generalization for similarity tasks. We observe that an\nonline/offline correspondence in similarity learning can be affected by both\nthe network architecture and label noise.",
    "descriptor": "",
    "authors": [
      "Nayara Fonseca",
      "Veronica Guidetti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12803"
  },
  {
    "id": "arXiv:2201.12805",
    "title": "Automatic Segmentation of Left Ventricle in Cardiac Magnetic Resonance  Images",
    "abstract": "Segmentation of the left ventricle in cardiac magnetic resonance imaging MRI\nscans enables cardiologists to calculate the volume of the left ventricle and\nsubsequently its ejection fraction. The ejection fraction is a measurement that\nexpresses the percentage of blood leaving the heart with each contraction.\nCardiologists often use ejection fraction to determine one's cardiac function.\nWe propose multiscale template matching technique for detection and an\nelliptical active disc for automated segmentation of the left ventricle in MR\nimages. The elliptical active disc optimizes the local energy function with\nrespect to its five free parameters which define the disc. Gradient descent is\nused to minimize the energy function along with Green's theorem to optimize the\ncomputation expenses. We report validations on 320 scans containing 5,273\nannotated slices which are publicly available through the Multi-Centre,\nMulti-Vendor, and Multi-Disease Cardiac Segmentation (M&Ms) Challenge. We\nachieved successful localization of the left ventricle in 89.63% of the cases\nand a Dice coefficient of 0.873 on diastole slices and 0.770 on systole slices.\nThe proposed technique is based on traditional image processing techniques with\na performance on par with the deep learning techniques.",
    "descriptor": "",
    "authors": [
      "Garvit Chhabra",
      "J. H. Gagan",
      "J. R. Harish Kumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12805"
  },
  {
    "id": "arXiv:2201.12806",
    "title": "Improving End-to-End Contextual Speech Recognition with Fine-grained  Contextual Knowledge Selection",
    "abstract": "Nowadays, most methods in end-to-end contextual speech recognition bias the\nrecognition process towards contextual knowledge. Since all-neural contextual\nbiasing methods rely on phrase-level contextual modeling and attention-based\nrelevance modeling, they may encounter confusion between similar\ncontext-specific phrases, which hurts predictions at the token level. In this\nwork, we focus on mitigating confusion problems with fine-grained contextual\nknowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge\nto reduce the uncertainty of token predictions. Specifically, we first apply\nphrase selection to narrow the range of phrase candidates, and then conduct\ntoken attention on the tokens in the selected phrase candidates. Moreover, we\nre-normalize the attention weights of most relevant phrases in inference to\nobtain more focused phrase-level contextual representations, and inject\nposition information to better discriminate phrases or tokens. On LibriSpeech\nand an in-house 160,000-hour dataset, we explore the proposed methods based on\na controllable all-neural biasing method, collaborative decoding (ColDec). The\nproposed methods provide at most 6.1% relative word error rate reduction on\nLibriSpeech and 16.4% relative character error rate reduction on the in-house\ndataset over ColDec.",
    "descriptor": "\nComments: Accepted by ICASSP 2022\n",
    "authors": [
      "Minglun Han",
      "Linhao Dong",
      "Zhenlin Liang",
      "Meng Cai",
      "Shiyu Zhou",
      "Zejun Ma",
      "Bo Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.12806"
  },
  {
    "id": "arXiv:2201.12809",
    "title": "OverChain: Building a robust overlay with a blockchain",
    "abstract": "Blockchains use peer-to-peer networks for disseminating information among\npeers, but these networks currently do not have any provable guarantees for\ndesirable properties such as Byzantine fault tolerance, good connectivity and\nsmall diameter. This is not just a theoretical problem, as recent works have\nexploited unsafe peer connection policies and weak network synchronization to\nmount partitioning attacks on Bitcoin. Cryptocurrency blockchains are safety\ncritical systems, so we need principled algorithms to maintain their networks.\nOur key insight is that we can leverage the blockchain itself to share\ninformation among the peers, and thus simplify the network maintenance process.\nGiven that the peers have restricted computational resources, and at most a\nconstant fraction of them are Byzantine, we provide communication-efficient\nprotocols to maintain a hypercubic network for blockchains, where peers can\njoin and leave over time. Interestingly, we discover that our design can\n\\emph{recover} from substantial adversarial failures. Moreover, these\nproperties hold despite significant churn.\nA key contribution is a secure mechanism for joining the network that uses\nthe blockchain to help new peers to contact existing peers. Furthermore, by\nexamining how peers join the network, i.e., the \"bootstrapping service,\" we\ngive a lower bound showing that (within log factors) our network tolerates the\nmaximum churn rate possible. In fact, we can give a lower bound on churn for\nany fully distributed service that requires connectivity.",
    "descriptor": "\nComments: 47 pages, 2 figures\n",
    "authors": [
      "Vijeth Aradhya",
      "Seth Gilbert",
      "Aquinas Hobor"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12809"
  },
  {
    "id": "arXiv:2201.12810",
    "title": "A Brief Overview of Physics-inspired Metaheuristic Optimization  Techniques",
    "abstract": "Metaheuristic algorithms are methods devised to efficiently solve\ncomputationally challenging optimization problems. Researchers have taken\ninspiration from various natural and physical processes alike to formulate\nmeta-heuristics that have successfully provided near-optimal or optimal\nsolutions to several engineering tasks. This chapter focuses on meta-heuristic\nalgorithms modelled upon non-linear physical phenomena having a concrete\noptimization paradigm, having shown formidable exploration and exploitation\nabilities for such optimization problems. Specifically, this chapter focuses on\nseveral popular physics-based metaheuristics as well as describing the\nunderlying unique physical processes associated with each algorithm.",
    "descriptor": "",
    "authors": [
      "Soumitri Chattopadhyay",
      "Aritra Marik",
      "Rishav Pramanik"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.12810"
  },
  {
    "id": "arXiv:2201.12811",
    "title": "A DFS Algorithm for Maximum Matchings in General Graphs",
    "abstract": "In this paper, we propose a depth-first search (DFS) algorithm for searching\nmaximum matchings in general graphs. Unlike blossom shrinking algorithms, which\nstore all possible alternative alternating paths in the super-vertices shrunk\nfrom blossoms, the newly proposed algorithm does not involve blossom shrinking.\nThe basic idea is to deflect the alternating path when facing blossoms. The\nalgorithm maintains detour information in an auxiliary stack to minimize the\nredundant data structures. A benefit of our technique is to avoid spending time\non shrinking and expanding blossoms. This DFS algorithm can determine a maximum\nmatching of a general graph with $m$ edges and $n$ vertices in $O(mn)$ time\nwith space complexity $O(n)$.",
    "descriptor": "\nComments: 15 pages, 7 figures, 2 tables\n",
    "authors": [
      "Tony T. Lee",
      "Bojun Lu",
      "Hanli Chu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12811"
  },
  {
    "id": "arXiv:2201.12812",
    "title": "Electrolyte Flow Rate Control for Vanadium Redox Flow Batteries using  the Linear Parameter Varying Framework",
    "abstract": "In this article, an electrolyte flow rate control approach is developed for\nan all-vanadium redox flow battery (VRB) system based on the linear parameter\nvarying (LPV) framework. The electrolyte flow rate is regulated to provide a\ntrade-off between stack voltage efficiency and pumping energy losses, so as to\nachieve optimal battery energy efficiency. The nonlinear process model is\nembedded in a linear parameter varying state-space description and a set of\nstate feedback controllers are designed to handle fluctuations in current\nduring both charging and discharging. Simulation studies have been conducted\nunder different operating conditions to demonstrate the performance of the\nproposed approach. This control approach was further implemented on a\nlaboratory scale VRB system.",
    "descriptor": "\nComments: This work is under review for Journal of Process Control\n",
    "authors": [
      "Ryan McCloy",
      "Yifeng Li",
      "Jie Bao",
      "Maria Skyllas-Kazacos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12812"
  },
  {
    "id": "arXiv:2201.12813",
    "title": "Contrastive Learning from Demonstrations",
    "abstract": "This paper presents a framework for learning visual representations from\nunlabeled video demonstrations captured from multiple viewpoints. We show that\nthese representations are applicable for imitating several robotic tasks,\nincluding pick and place. We optimize a recently proposed self-supervised\nlearning algorithm by applying contrastive learning to enhance task-relevant\ninformation while suppressing irrelevant information in the feature embeddings.\nWe validate the proposed method on the publicly available Multi-View Pouring\nand a custom Pick and Place data sets and compare it with the TCN triplet\nbaseline. We evaluate the learned representations using three metrics:\nviewpoint alignment, stage classification and reinforcement learning, and in\nall cases the results improve when compared to state-of-the-art approaches,\nwith the added benefit of reduced number of training iterations.",
    "descriptor": "",
    "authors": [
      "Andr\u00e9 Correia",
      "Lu\u00eds A. Alexandre"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12813"
  },
  {
    "id": "arXiv:2201.12816",
    "title": "Adaptive Contraction-based Control of Uncertain Nonlinear Processes  using Neural Networks",
    "abstract": "Driven by the flexible manufacturing trend in the process control industry\nand the uncertain nature of chemical process models, this article aims to\nachieve offset-free tracking for a family of uncertain nonlinear systems (e.g.,\nusing process models with parametric uncertainties) with adaptable performance.\nThe proposed adaptive control approach incorporates into the control loop an\nadaptive neural network embedded contraction-based controller (to ensure\nconvergence to time-varying references) and an online parameter identification\nmodule coupled with reference generation (to ensure modelled parameters\nconverge those of the physical system). The integrated learning and control\napproach involves training a state and parameter dependent neural network to\nlearn a contraction metric parameterized by the uncertain parameter and a\ndifferential feedback gain. This neural network is then embedded in an adaptive\ncontraction-based control law which is updated by parameter estimates online.\nAs uncertain parameter estimates converge to the corresponding physical values,\noffset-free tracking, simultaneously with improved convergence rates, can be\nachieved, resulting in a flexible, efficient and less conservative approach to\nthe reference tracking control of uncertain nonlinear processes. An\nillustrative example is included to demonstrate the overall approach. An\nillustrative example is included to demonstrate the overall approach.",
    "descriptor": "\nComments: This paper has been submitted to DYCOPS2022\n",
    "authors": [
      "Lai Wei",
      "Ryan McCloy",
      "Jie Bao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12816"
  },
  {
    "id": "arXiv:2201.12819",
    "title": "A Safety-Critical Decision Making and Control Framework Combining  Machine Learning and Rule-based Algorithms",
    "abstract": "While artificial-intelligence-based methods suffer from lack of transparency,\nrule-based methods dominate in safety-critical systems. Yet, the latter cannot\ncompete with the first ones in robustness to multiple requirements, for\ninstance, simultaneously addressing safety, comfort, and efficiency. Hence, to\nbenefit from both methods they must be joined in a single system. This paper\nproposes a decision making and control framework, which profits from advantages\nof both the rule- and machine-learning-based techniques while compensating for\ntheir disadvantages. The proposed method embodies two controllers operating in\nparallel, called Safety and Learned. A rule-based switching logic selects one\nof the actions transmitted from both controllers. The Safety controller is\nprioritized every time, when the Learned one does not meet the safety\nconstraint, and also directly participates in the safe Learned controller\ntraining. Decision making and control in autonomous driving is chosen as the\nsystem case study, where an autonomous vehicle learns a multi-task policy to\nsafely cross an unprotected intersection. Multiple requirements (i.e., safety,\nefficiency, and comfort) are set for vehicle operation. A numerical simulation\nis performed for the proposed framework validation, where its ability to\nsatisfy the requirements and robustness to changing environment is successfully\ndemonstrated.",
    "descriptor": "\nComments: 11 pages, 9 figures, 2 tables\n",
    "authors": [
      "Andrei Aksjonov",
      "Ville Kyrki"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12819"
  },
  {
    "id": "arXiv:2201.12822",
    "title": "ClassSPLOM -- A Scatterplot Matrix to Visualize Separation of Multiclass  Multidimensional Data",
    "abstract": "In multiclass classification of multidimensional data, the user wants to\nbuild a model of the classes to predict the label of unseen data. The model is\ntrained on the data and tested on unseen data with known labels to evaluate its\nquality. The results are visualized as a confusion matrix which shows how many\ndata labels have been predicted correctly or confused with other classes. The\nmultidimensional nature of the data prevents the direct visualization of the\nclasses so we design ClassSPLOM to give more perceptual insights about the\nclassification results. It uses the Scatterplot Matrix (SPLOM) metaphor to\nvisualize a Linear Discriminant Analysis projection of the data for each pair\nof classes and a set of Receiving Operating Curves to evaluate their\ntrustworthiness. We illustrate ClassSPLOM on a use case in Arabic dialects\nidentification.",
    "descriptor": "\nComments: Work presented as a poster at IEEE VIS 2016 conference in Baltimore, MD, USA\n",
    "authors": [
      "Michael Aupetit",
      "Ahmed Ali"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12822"
  },
  {
    "id": "arXiv:2201.12825",
    "title": "Hyperbolic Neural Networks for Molecular Generation",
    "abstract": "With the recent advance of deep learning, neural networks have been\nextensively used for the task of molecular generation. Many deep generators\nextract atomic relations from molecular graphs and ignore hierarchical\ninformation at both atom and molecule levels. In order to extract such\nhierarchical information, we propose a novel hyperbolic generative model. Our\nmodel contains three parts: first, a fully hyperbolic junction-tree\nencoder-decoder that embeds the hierarchical information of the molecules in\nthe latent hyperbolic space; second, a latent generative adversarial network\nfor generating the latent embeddings; third, a molecular generator that\ninherits the decoders from the first part and the latent generator from the\nsecond part. We evaluate our model on the ZINC dataset using the MOSES\nbenchmarking platform and achieve competitive results, especially in metrics\nabout structural similarity.",
    "descriptor": "",
    "authors": [
      "Eric Qu",
      "Dongmian Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12825"
  },
  {
    "id": "arXiv:2201.12826",
    "title": "Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All  You Need",
    "abstract": "Network sparsity receives popularity mostly due to its capability to reduce\nthe network complexity. Extensive studies excavate gradient-driven sparsity.\nTypically, these methods are constructed upon premise of weight independence,\nwhich however, is contrary to the fact that weights are mutually influenced.\nThus, their performance remains to be improved. In this paper, we propose to\nfurther optimize gradient-driven sparsity (OptG) by solving this independence\nparadox. Our motive comes from the recent advances on supermask training which\nshows that sparse subnetworks can be located in a randomly initialized network\nby simply updating mask values without modifying any weight. We prove that\nsupermask training is to accumulate the weight gradients and can partly solve\nthe independence paradox. Consequently, OptG integrates supermask training into\ngradient-driven sparsity, and a specialized mask optimizer is designed to solve\nthe independence paradox. Experiments show that OptG can well surpass many\nexisting state-of-the-art competitors. Our code is available at\n\\url{https://github.com/zyxxmu/OptG}.",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Yuxin Zhang",
      "Mingbao Lin",
      "Mengzhao Chen",
      "Zihan Xu",
      "Fei Chao",
      "Yunhan Shen",
      "Ke Li",
      "Yongjian Wu",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12826"
  },
  {
    "id": "arXiv:2201.12828",
    "title": "Comprehensive Saliency Fusion for Object Co-segmentation",
    "abstract": "Object co-segmentation has drawn significant attention in recent years,\nthanks to its clarity on the expected foreground, the shared object in a group\nof images. Saliency fusion has been one of the promising ways to carry it out.\nHowever, prior works either fuse saliency maps of the same image or saliency\nmaps of different images to extract the expected foregrounds. Also, they rely\non hand-crafted saliency extraction and correspondence processes in most cases.\nThis paper revisits the problem and proposes fusing saliency maps of both the\nsame image and different images. It also leverages advances in deep learning\nfor the saliency extraction and correspondence processes. Hence, we call it\ncomprehensive saliency fusion. Our experiments reveal that our approach\nachieves much-improved object co-segmentation results compared to prior works\non important benchmark datasets such as iCoseg, MSRC, and Internet Images.",
    "descriptor": "\nComments: Published in IEEE ISM 2021. Please cite this paper in the following manner. H. S. Chhabra and K. Rao Jerripothula, \"Comprehensive Saliency Fusion for Object Co-segmentation,\" 2021 IEEE International Symposium on Multimedia (ISM), 2021, pp. 107-110, doi: 10.1109/ISM52913.2021.00026\n",
    "authors": [
      "Harshit Singh Chhabra",
      "Koteswar Rao Jerripothula"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12828"
  },
  {
    "id": "arXiv:2201.12829",
    "title": "Optimising the reliability that can be claimed for a software-based  system based on failure-free tests of its components",
    "abstract": "This short paper describes a numerical method for optimising the conservative\nconfidence bound on the reliability of a system based on tests of its\nindividual components. This is an alternative to the algorithmic approaches\nidentified in Bishop and Povyakalo (RESS 2020). For a given maximum number of\ncomponent tests, the numerical method can derive an optimal test plan for any\narbitrary system structure.\nThe optimisation method is based on linear programming which is more\nefficient that the alternative integer programming. In addition, the\noptimisation process need only be performed once for any given system structure\nas the solution can be re-used to compute an optimal integer test plan for a\ndifferent maximum number of component tests.\nThis approach might have broader application to other optimisation problems\nthat are normally implemented using integer programming methods.",
    "descriptor": "\nComments: 11 pages, 1 figure,\n",
    "authors": [
      "Peter Bishop",
      "Andrey Povyakalo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.12829"
  },
  {
    "id": "arXiv:2201.12830",
    "title": "Over-smoothing Effect of Graph Convolutional Networks",
    "abstract": "Over-smoothing is a severe problem which limits the depth of Graph\nConvolutional Networks. This article gives a comprehensive analysis of the\nmechanism behind Graph Convolutional Networks and the over-smoothing effect.\nThe article proposes an upper bound for the occurrence of over-smoothing, which\noffers insight into the key factors behind over-smoothing. The results\npresented in this article successfully explain the feasibility of several\nalgorithms that alleviate over-smoothing.",
    "descriptor": "\nComments: 8 pages, 2 figures\n",
    "authors": [
      "Fang Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12830"
  },
  {
    "id": "arXiv:2201.12833",
    "title": "Word Segmentation and Morphological Parsing for Sanskrit",
    "abstract": "We describe our participation in the Word Segmentation and Morphological\nParsing (WSMP) for Sanskrit hackathon. We approach the word segmentation task\nas a sequence labelling task by predicting edit operations from which\nsegmentations are derived. We approach the morphological analysis task by\npredicting morphological tags and rules that transform inflected words into\ntheir corresponding stems. Also, we propose an end-to-end trainable pipeline\nmodel for joint segmentation and morphological analysis. Our model performed\nbest in the joint segmentation and analysis subtask (80.018 F1 score) and\nperformed second best in the individual subtasks (segmentation: 96.189 F1 score\n/ analysis: 69.180 F1 score).\nFinally, we analyse errors made by our models and suggest future work and\npossible improvements regarding data and evaluation.",
    "descriptor": "\nComments: Code can be accessed from this https URL\n",
    "authors": [
      "Jingwen Li",
      "Leander Girrbach"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12833"
  },
  {
    "id": "arXiv:2201.12835",
    "title": "Debiased-CAM to mitigate systematic error with faithful visual  explanations of machine learning",
    "abstract": "Model explanations such as saliency maps can improve user trust in AI by\nhighlighting important features for a prediction. However, these become\ndistorted and misleading when explaining predictions of images that are subject\nto systematic error (bias). Furthermore, the distortions persist despite model\nfine-tuning on images biased by different factors (blur, color temperature,\nday/night). We present Debiased-CAM to recover explanation faithfulness across\nvarious bias types and levels by training a multi-input, multi-task model with\nauxiliary tasks for explanation and bias level predictions. In simulation\nstudies, the approach not only enhanced prediction accuracy, but also generated\nhighly faithful explanations about these predictions as if the images were\nunbiased. In user studies, debiased explanations improved user task\nperformance, perceived truthfulness and perceived helpfulness. Debiased\ntraining can provide a versatile platform for robust performance and\nexplanation faithfulness for a wide range of applications with data biases.",
    "descriptor": "\nComments: 18 pages, 8 figures. arXiv admin note: substantial text overlap with arXiv:2012.05567\n",
    "authors": [
      "Wencan Zhang",
      "Mariella Dimiccoli",
      "Brian Y. Lim"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12835"
  },
  {
    "id": "arXiv:2201.12843",
    "title": "On Recoverability of Graph Neural Network Representations",
    "abstract": "Despite their growing popularity, graph neural networks (GNNs) still have\nmultiple unsolved problems, including finding more expressive aggregation\nmethods, propagation of information to distant nodes, and training on\nlarge-scale graphs. Understanding and solving such problems require developing\nanalytic tools and techniques. In this work, we propose the notion of\nrecoverability, which is tightly related to information aggregation in GNNs,\nand based on this concept, develop the method for GNN embedding analysis. We\ndefine recoverability theoretically and propose a method for its efficient\nempirical estimation. We demonstrate, through extensive experimental results on\nvarious datasets and different GNN architectures, that estimated recoverability\ncorrelates with aggregation method expressivity and graph sparsification\nquality. Therefore, we believe that the proposed method could provide an\nessential tool for understanding the roots of the aforementioned problems, and\npotentially lead to a GNN design that overcomes them. The code to reproduce our\nexperiments is available at https://github.com/Anonymous1252022/Recoverability",
    "descriptor": "",
    "authors": [
      "Maxim Fishman",
      "Chaim Baskin",
      "Evgenii Zheltonozhskii",
      "Ron Banner",
      "Avi Mendelson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12843"
  },
  {
    "id": "arXiv:2201.12845",
    "title": "Potential Destination Prediction Based on Knowledge Graph Under Low  Predictability Data Condition",
    "abstract": "Destination prediction has been a critical topic in transportation research,\nand there are a large number of studies. However, almost all existing studies\nare based on high predictability data conditions while pay less attention to\nthe data condition with low predictability, where the regularity of single\nindividuals is not exposed. Based on a certain period of observation, there is\na fact that individuals may choose destinations beyond observation, which we\ncall \"potential destinations\". The number of potential destinations is very\nlarge and can't be ignored for the data condition with low predictability\nformed by short-term observation.To reveal the choice pattern of potential\ndestination of individuals under the data condition with low predictability, we\npropose a global optimization method based on knowledge graph embedding. First,\nwe joint the trip data of all individuals by constructing Trip Knowledge\nGraph(TKG). Next, we optimize the general algorithm of knowledge graph\nembedding for our data and task in training strategy and objective function,\nthen implement it on TKG. It can achieve global optimization for association\npaths that exist between almost any two entities in TKG. On this basis, a\nmethod for potential destination prediction is proposed, giving the possible\nranking of unobserved destinations for each individual. In addition, we improve\nthe performance by fusing static statistical information that is not passed to\nTKG. Finally, we validate our method in a real-world dataset, and the\nprediction results are highly consistent with individuals' potential\ndestination choice behaviour.",
    "descriptor": "",
    "authors": [
      "Guilong Li",
      "Yixian Chen",
      "Qionghua Liao",
      "Zhaocheng He"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12845"
  },
  {
    "id": "arXiv:2201.12848",
    "title": "Deep Non-Crossing Quantiles through the Partial Derivative",
    "abstract": "Quantile Regression (QR) provides a way to approximate a single conditional\nquantile. To have a more informative description of the conditional\ndistribution, QR can be merged with deep learning techniques to simultaneously\nestimate multiple quantiles. However, the minimisation of the QR-loss function\ndoes not guarantee non-crossing quantiles, which affects the validity of such\npredictions and introduces a critical issue in certain scenarios. In this\narticle, we propose a generic deep learning algorithm for predicting an\narbitrary number of quantiles that ensures the quantile monotonicity constraint\nup to the machine precision and maintains its modelling performance with\nrespect to alternative models. The presented method is evaluated over several\nreal-world datasets obtaining state-of-the-art results as well as showing that\nit scales to large-size data sets.",
    "descriptor": "\nComments: In the Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS)\n",
    "authors": [
      "Axel Brando",
      "Joan Gimeno",
      "Jose A. Rodr\u00edguez-Serrano",
      "Jordi Vitri\u00e0"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12848"
  },
  {
    "id": "arXiv:2201.12854",
    "title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
    "abstract": "We introduce Monte-Carlo Attention (MCA), a randomized approximation method\nfor reducing the computational cost of self-attention mechanisms in Transformer\narchitectures. MCA exploits the fact that the importance of each token in an\ninput sequence varies with respect to their attention scores; thus, some degree\nof error can be tolerable when encoding tokens with low attention. Using\napproximate matrix multiplication, MCA applies different error bounds to encode\ninput tokens such that those with low attention scores are computed with\nrelaxed precision, whereas errors of salient elements are minimized. MCA can\noperate in parallel with other attention optimization schemes and does not\nrequire model modification. We study the theoretical error bounds and\ndemonstrate that MCA reduces attention complexity (in FLOPS) for various\nTransformer models by up to 11$\\times$ in GLUE benchmarks without compromising\nmodel accuracy.",
    "descriptor": "\nComments: AAAI 2022\n",
    "authors": [
      "Hyunjun Kim",
      "JeongGil Ko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12854"
  },
  {
    "id": "arXiv:2201.12855",
    "title": "Augmented Business Process Management Systems: A Research Manifesto",
    "abstract": "Augmented Business Process Management Systems (ABPMSs) are an emerging class\nof process-aware information systems that draws upon trustworthy AI technology.\nAn ABPMS enhances the execution of business processes with the aim of making\nthese processes more adaptable, proactive, explainable, and context-sensitive.\nThis manifesto presents a vision for ABPMSs and discusses research challenges\nthat need to be surmounted to realize this vision. To this end, we define the\nconcept of ABPMS, we outline the lifecycle of processes within an ABPMS, we\ndiscuss core characteristics of an ABPMS, and we derive a set of challenges to\nrealize systems with these characteristics.",
    "descriptor": "\nComments: 19 pages, 1 figure\n",
    "authors": [
      "Marlon Dumas",
      "Fabiana Fournier",
      "Lior Limonad",
      "Andrea Marrella",
      "Marco Montali",
      "Jana-Rebecca Rehse",
      "Rafael Accorsi",
      "Diego Calvanese",
      "Giuseppe De Giacomo",
      "Dirk Fahland",
      "Avigdor Gal",
      "Marcello La Rosa",
      "Hagen V\u00f6lzer",
      "Ingo Weber"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.12855"
  },
  {
    "id": "arXiv:2201.12857",
    "title": "Fast Relative Entropy Coding with A* coding",
    "abstract": "Relative entropy coding (REC) algorithms encode a sample from a target\ndistribution $Q$ using a proposal distribution $P$, such that the expected\ncodelength is $\\mathcal{O}(D_{KL}[Q \\,||\\, P])$. REC can be seamlessly\nintegrated with existing learned compression models since, unlike entropy\ncoding, it does not assume discrete $Q$ or $P$, and does not require\nquantisation. However, general REC algorithms require an intractable\n$\\Omega(e^{D_{KL}[Q \\,||\\, P]})$ runtime. We introduce AS* and AD* coding, two\nREC algorithms based on A* sampling. We prove that, for continuous\ndistributions over $\\mathbb{R}$, if the density ratio is unimodal, AS* has\n$\\mathcal{O}(D_{\\infty}[Q \\,||\\, P])$ expected runtime, where $D_{\\infty}[Q\n\\,||\\, P]$ is the R\\'enyi $\\infty$-divergence. We provide experimental evidence\nthat AD* also has $\\mathcal{O}(D_{\\infty}[Q \\,||\\, P])$ expected runtime. We\nprove that AS* and AD* achieve an expected codelength of $\\mathcal{O}(D_{KL}[Q\n\\,||\\, P])$. Further, we introduce DAD*, an approximate algorithm based on AD*\nwhich retains its favourable runtime and has bias similar to that of\nalternative methods. Focusing on VAEs, we propose the IsoKL VAE (IKVAE), which\ncan be used with DAD* to further improve compression efficiency. We evaluate A*\ncoding with (IK)VAEs on MNIST, showing that it can losslessly compress images\nnear the theoretically optimal limit.",
    "descriptor": "",
    "authors": [
      "Gergely Flamich",
      "Stratis Markou",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12857"
  },
  {
    "id": "arXiv:2201.12859",
    "title": "Deciding Asynchronous Hyperproperties for Recursive Programs",
    "abstract": "We introduce a novel logic for asynchronous hyperproperties with a new\nmechanism to identify relevant positions on traces. While the new logic is more\nexpressive than a related logic presented recently by Bozzelli et. al., we\nobtain the same decidability and complexity of the model checking problem for\nfinite state models. Beyond this, we study the model checking problem of our\nlogic for pushdown models. We argue that this combination of asynchronicity and\na non-regular model class constitutes the first suitable approach for\nhyperproperty model checking against recursive programs.",
    "descriptor": "",
    "authors": [
      "Jens Oliver Gutsfeld",
      "Markus M\u00fcller-Olm",
      "Christoph Ohrem"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.12859"
  },
  {
    "id": "arXiv:2201.12861",
    "title": "Neural-PIM: Efficient Processing-In-Memory with Neural Approximation of  Peripherals",
    "abstract": "Processing-in-memory (PIM) architectures have demonstrated great potential in\naccelerating numerous deep learning tasks. Particularly, resistive\nrandom-access memory (RRAM) devices provide a promising hardware substrate to\nbuild PIM accelerators due to their abilities to realize efficient in-situ\nvector-matrix multiplications (VMMs). However, existing PIM accelerators suffer\nfrom frequent and energy-intensive analog-to-digital (A/D) conversions,\nseverely limiting their performance. This paper presents a new PIM architecture\nto efficiently accelerate deep learning tasks by minimizing the required A/D\nconversions with analog accumulation and neural approximated peripheral\ncircuits. We first characterize the different dataflows employed by existing\nPIM accelerators, based on which a new dataflow is proposed to remarkably\nreduce the required A/D conversions for VMMs by extending shift and add (S+A)\noperations into the analog domain before the final quantizations. We then\nleverage a neural approximation method to design both analog accumulation\ncircuits (S+A) and quantization circuits (ADCs) with RRAM crossbar arrays in a\nhighly-efficient manner. Finally, we apply them to build an RRAM-based PIM\naccelerator (i.e., \\textbf{Neural-PIM}) upon the proposed analog dataflow and\nevaluate its system-level performance. Evaluations on different benchmarks\ndemonstrate that Neural-PIM can improve energy efficiency by 5.36x (1.73x) and\nspeed up throughput by 3.43x (1.59x) without losing accuracy, compared to the\nstate-of-the-art RRAM-based PIM accelerators, i.e., ISAAC (CASCADE).",
    "descriptor": "\nComments: 14 pages, 13 figures, Published in IEEE Transactions on Computers\n",
    "authors": [
      "Weidong Cao",
      "Yilong Zhao",
      "Adith Boloor",
      "Yinhe Han",
      "Xuan Zhang",
      "Li Jiang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12861"
  },
  {
    "id": "arXiv:2201.12862",
    "title": "Lyapunov Conditions for Input-to-State Stability of Hybrid Systems with  Memory",
    "abstract": "This paper studies input-to-state stability for hybrid systems with memory,\nwhich models hybrid dynamics affected by time delays. Using both\nLyapunov-Razumikhin functions and Lyapunov-Krasovskii functionals,\nLyapunov-based sufficient conditions are established for input-to-state\nstability. In addition, further extensions and relaxations are proposed for\nspecial cases, such as the stable flow/jump cases and the cases that Lyapunov\nfunctions do not decrease strictly during flow/jumps. Finally, two examples are\nused to illustrate the developed results.",
    "descriptor": "\nComments: 8 pages, IEEE Transactions on Automatic Control\n",
    "authors": [
      "Wei Ren",
      "Junlin Xiong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12862"
  },
  {
    "id": "arXiv:2201.12868",
    "title": "Anticipation-free Training for Simultaneous Translation",
    "abstract": "Simultaneous translation (SimulMT) speeds up the translation process by\nstarting to translate before the source sentence is completely available. It is\ndifficult due to limited context and word order difference between languages.\nExisting methods increase latency or introduce adaptive read-write policies for\nSimulMT models to handle local reordering and improve translation quality.\nHowever, the long-distance reordering would make the SimulMT models learn\ntranslation mistakenly. Specifically, the model may be forced to predict target\ntokens when the corresponding source tokens have not been read. This leads to\naggressive anticipation during inference, resulting in the hallucination\nphenomenon. To mitigate this problem, we propose a new framework that decompose\nthe translation process into the monotonic translation step and the reordering\nstep, and we model the latter by the auxiliary sorting network (ASN). The ASN\nrearranges the hidden states to match the order in the target language, so that\nthe SimulMT model could learn to translate more reasonably. The entire model is\noptimized end-to-end and does not rely on external aligners or data. During\ninference, ASN is removed to achieve streaming. Experiments show the proposed\nframework could outperform previous methods with less latency.\\footnote{The\nsource code is available.",
    "descriptor": "\nComments: 19 pages, 14 figures\n",
    "authors": [
      "Chih-Chiang Chang",
      "Shun-Po Chuang",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12868"
  },
  {
    "id": "arXiv:2201.12869",
    "title": "A Two-Step Approach to Optimal Dynamic Pricing in Multi-Demand  Combinatorial Markets",
    "abstract": "Online markets are a part of everyday life, and their rules are governed by\nalgorithms. Assuming participants are inherently self-interested, well designed\nrules can help to increase social welfare. Many algorithms for online markets\nare based on prices: the seller is responsible for posting prices while buyers\nmake purchases which are most profitable given the posted prices. To make\nadjustments to the market the seller is allowed to update prices at certain\ntimepoints.\nPosted prices are an intuitive way to design a market. Despite the fact that\neach buyer acts selfishly, the seller's goal is often assumed to be that of\nsocial welfare maximization. Berger, Eden and Feldman recently considered the\ncase of a market with only three buyers where each buyer has a fixed number of\ngoods to buy and the profit of a bought bundle of items is the sum of profits\nof the items in the bundle. For such markets, Berger et. al. showed that the\nseller can maximize social welfare by dynamically updating posted prices before\narrival of each buyer. B\\'{e}rczi, B\\'{e}rczi-Kov\\'{a}cs and Sz\\\"{o}gi showed\nthat the social welfare can be maximized also when each buyer is ready to buy\nat most two items.\nWe study the power of posted prices with dynamical updates in more general\ncases. First, we show that the result of Berger et. al. can be generalized from\nthree to four buyers. Then we show that the result of B\\'{e}rczi,\nB\\'{e}rczi-Kov\\'{a}cs and Sz\\\"{o}gi can be generalized to the case when each\nbuyer is ready to buy up to three items. We also show that a dynamic pricing is\npossible whenever there are at most two allocations maximizing social welfare.",
    "descriptor": "",
    "authors": [
      "Kanstantsin Pashkovich",
      "Xinyue Xie"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.12869"
  },
  {
    "id": "arXiv:2201.12870",
    "title": "General 2-path Problem",
    "abstract": "In this paper, some preliminaries about signal flow graph, linear\ntime-invariant system on F(z) and computational complexity are first introduced\nin detail. In order to synthesize the necessary and sufficient condition on\nF(z) for a general 2-path problem, the sufficient condition on F(z) or R and\nnecessary conditions on F(z) for a general 2-path problem are secondly analyzed\nrespectively. Moreover, an equivalent sufficient and necessary condition on R\nwhether there exists a general 2-path is deduced in detail. Finally, the\ncomputational complexity of the algorithm for this equivalent sufficient and\nnecessary condition is introduced so that it means that the general 2-path\nproblem is a P problem.",
    "descriptor": "\nComments: 14 pages,3 figures\n",
    "authors": [
      "Qianghui Xiao"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2201.12870"
  },
  {
    "id": "arXiv:2201.12872",
    "title": "Discovering Invariant Rationales for Graph Neural Networks",
    "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small\nsubset of the input graph's features -- rationale -- which guides the model\nprediction. Unfortunately, the leading rationalization models often rely on\ndata biases, especially shortcut features, to compose rationales and make\npredictions without probing the critical and causal patterns. Moreover, such\ndata biases easily change outside the training distribution. As a result, these\nmodels suffer from a huge drop in interpretability and predictive performance\non out-of-distribution data. In this work, we propose a new strategy of\ndiscovering invariant rationale (DIR) to construct intrinsically interpretable\nGNNs. It conducts interventions on the training distribution to create multiple\ninterventional distributions. Then it approaches the causal rationales that are\ninvariant across different distributions while filtering out the spurious\npatterns that are unstable. Experiments on both synthetic and real-world\ndatasets validate the superiority of our DIR in terms of interpretability and\ngeneralization ability on graph classification over the leading baselines. Code\nand datasets are available at https://github.com/Wuyxin/DIR-GNN.",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Ying-Xin Wu",
      "Xiang Wang",
      "An Zhang",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12872"
  },
  {
    "id": "arXiv:2201.12874",
    "title": "Comparison of Matrix Norm Sparsification",
    "abstract": "Matrix sparsification is a well-known approach in the design of efficient\nalgorithms, where one approximates a matrix $A$ with a sparse matrix $A'$.\nAchlioptas and McSherry [2007] initiated a long line of work on spectral-norm\nsparsification, which aims to guarantee that $\\|A'-A\\|\\leq \\epsilon \\|A\\|$ for\nerror parameter $\\epsilon>0$. Various forms of matrix approximation motivate\nconsidering this problem with a guarantee according to the Schatten $p$-norm\nfor general $p$, which includes the spectral norm as the special case\n$p=\\infty$. We investigate the relation between fixed but different $p\\neq q$,\nthat is, whether sparsification in Schatten $p$-norm implies (existentially\nand/or algorithmically) sparsification in Schatten $q$-norm with similar\nsparsity. An affirmative answer could be tremendously useful, as it will\nidentify which value of $p$ to focus on. Our main finding is a surprising\ncontrast between this question and the analogous case of $\\ell_p$-norm\nsparsification for vectors: For vectors, the answer is affirmative for $p<q$\nand negative for $p>q$, but for matrices we answer negatively for almost all\n$p\\neq q$.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Robert Krauthgamer",
      "Shay Sapir"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12874"
  },
  {
    "id": "arXiv:2201.12876",
    "title": "DeepCatra: Learning Flow- and Graph-based Behaviors for Android Malware  Detection",
    "abstract": "As Android malware is growing and evolving, deep learning has been introduced\ninto malware detection, resulting in great effectiveness. Recent work is\nconsidering hybrid models and multi-view learning. However, they use only\nsimple features, limiting the accuracy of these approaches in practice. In this\npaper, we propose DeepCatra, a multi-view learning approach for Android malware\ndetection, whose model consists of a bidirectional LSTM (BiLSTM) and a graph\nneural network (GNN) as subnets. The two subnets rely on features extracted\nfrom statically computed call traces leading to critical APIs derived from\npublic vulnerabilities. For each Android app, DeepCatra first constructs its\ncall graph and computes call traces reaching critical APIs. Then, temporal\nopcode features used by the BiLSTM subnet are extracted from the call traces,\nwhile flow graph features used by the GNN subnet are constructed from all the\ncall traces and inter-component communications. We evaluate the effectiveness\nof DeepCatra by comparing it with several state-of-the-art detection\napproaches. Experimental results on over 18,000 real-world apps and prevalent\nmalware show that DeepCatra achieves considerable improvement, e.g., 2.7% to\n14.6% on F1-measure, which demonstrates the feasibility of DeepCatra in\npractice.",
    "descriptor": "",
    "authors": [
      "Yafei Wu",
      "Jian Shi",
      "Peicheng Wang",
      "Dongrui Zeng",
      "Cong Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12876"
  },
  {
    "id": "arXiv:2201.12879",
    "title": "Making Secure Software Insecure without Changing Its Code: The  Possibilities and Impacts of Attacks on the DevOps Pipeline",
    "abstract": "Companies are misled into thinking they solve their security issues by using\na DevSecOps system. This paper aims to answer the question: Could a DevOps\npipeline be misused to transform a securely developed application into an\ninsecure one? To answer the question, we designed a typical DevOps pipeline\nutilizing Kubernetes (K8s} as a case study environment and analyzed the\napplicable threats. Then, we developed four attack scenarios against the case\nstudy environment: maliciously abusing the user's privilege of deploying\ncontainers within the K8s cluster, abusing the Jenkins instance to modify files\nduring the continuous integration, delivery, and deployment systems (CI/CD)\nbuild phase, modifying the K8s DNS layer to expose an internal IP to external\ntraffic, and elevating privileges from an account with create, read, update,\nand delete (CRUD) privileges to root privileges. The attacks answer the\nresearch question positively: companies should design and use a secure DevOps\npipeline and not expect that using a DevSecOps environment alone is sufficient\nto deliver secure software.",
    "descriptor": "",
    "authors": [
      "Nicholas Pecka",
      "Lotfi ben Othmane",
      "Altaz Valani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.12879"
  },
  {
    "id": "arXiv:2201.12880",
    "title": "Self-stabilizing Byzantine-tolerant Broadcast",
    "abstract": "We study a well-known communication abstraction called Byzantine Reliable\nBroadcast (BRB). This abstraction is central in the design and implementation\nof fault-tolerant distributed systems, as many fault-tolerant distributed\napplications require communication with provable guarantees on message\ndeliveries. Our study focuses on fault-tolerant implementations for\nmessage-passing systems that are prone to process-failures, such as crashes and\nmalicious behavior.\nAt PODC 1983, Bracha and Toueg, in short, BT, solved the BRB problem. BT has\noptimal resilience since it can deal with t<n/3 Byzantine processes, where n is\nthe number of processes. This work aims at the design of an even more robust\nsolution than BT by expanding its fault-model with self-stabilization, a\nvigorous notion of fault-tolerance. In addition to tolerating Byzantine and\ncommunication failures, self-stabilizing systems can recover after the\noccurrence of arbitrary transient-faults. These faults represent any violation\nof the assumptions according to which the system was designed to operate\n(provided that the algorithm code remains intact).\nWe propose, to the best of our knowledge, the first self-stabilizing\nByzantine-tolerant BRB solution for signature-free message-passing systems. Our\ncontribution includes a self-stabilizing variation on a BT that solves a\nsingle-round BRB for asynchronous systems. We also consider the problem of\nrecycling instances of single-round BRB. Our self-stabilizing\nByzantine-tolerant recycling for time-free systems facilitates the concurrent\nhandling of a predefined number of BRB invocations. Our proposal can serve as\nthe basis for self-stabilizing Byzantine-tolerant consensus.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2110.08592\n",
    "authors": [
      "Romaric Duvignau",
      "Michel Raynal",
      "Elad Michael Schiller"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12880"
  },
  {
    "id": "arXiv:2201.12884",
    "title": "A Theoretical Comparison of Graph Neural Network Extensions",
    "abstract": "We study and compare different Graph Neural Network extensions that increase\nthe expressive power of GNNs beyond the Weisfeiler-Leman test. We focus on (i)\nGNNs based on higher order WL methods, (ii) GNNs that preprocess small\nsubstructures in the graph, (iii) GNNs that preprocess the graph up to a small\nradius, and (iv) GNNs that slightly perturb the graph to compute an embedding.\nWe begin by presenting a simple improvement for this last extension that\nstrictly increases the expressive power of this GNN variant. Then, as our main\nresult, we compare the expressiveness of these extensions to each other through\na series of example constructions that can be distinguished by one of the\nextensions, but not by another one. We also show negative examples that are\nparticularly challenging for each of the extensions, and we prove several\nclaims about the ability of these extensions to count cliques and cycles in the\ngraph.",
    "descriptor": "",
    "authors": [
      "P\u00e1l Andr\u00e1s Papp",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12884"
  },
  {
    "id": "arXiv:2201.12885",
    "title": "Computational Metacognition",
    "abstract": "Computational metacognition represents a cognitive systems perspective on\nhigh-order reasoning in integrated artificial systems that seeks to leverage\nideas from human metacognition and from metareasoning approaches in artificial\nintelligence. The key characteristic is to declaratively represent and then\nmonitor traces of cognitive activity in an intelligent system in order to\nmanage the performance of cognition itself. Improvements in cognition then lead\nto improvements in behavior and thus performance. We illustrate these concepts\nwith an agent implementation in a cognitive architecture called MIDCA and show\nthe value of metacognition in problem-solving. The results illustrate how\ncomputational metacognition improves performance by changing cognition through\nmeta-level goal operations and learning.",
    "descriptor": "\nComments: 20 pages, 9 figures, 2 tables, Presented at The Ninth Advances in Cognitive Systems (ACS) Conference 2021 (arXiv:2201.06134)\n",
    "authors": [
      "Michael Cox",
      "Zahiduddin Mohammad",
      "Sravya Kondrakunta",
      "Ventaksamapth Raja Gogineni",
      "Dustin Dannenhauer",
      "Othalia Larue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12885"
  },
  {
    "id": "arXiv:2201.12886",
    "title": "N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",
    "abstract": "Recent progress in neural forecasting accelerated improvements in the\nperformance of large-scale forecasting systems. Yet, long-horizon forecasting\nremains a very difficult task. Two common challenges afflicting long-horizon\nforecasting are the volatility of the predictions and their computational\ncomplexity. In this paper, we introduce N-HiTS, a model which addresses both\nchallenges by incorporating novel hierarchical interpolation and multi-rate\ndata sampling techniques. These techniques enable the proposed method to\nassemble its predictions sequentially, selectively emphasizing components with\ndifferent frequencies and scales, while decomposing the input signal and\nsynthesizing the forecast. We conduct an extensive empirical evaluation\ndemonstrating the advantages of N-HiTS over the state-of-the-art long-horizon\nforecasting methods. On an array of multivariate forecasting tasks, the\nproposed method provides an average accuracy improvement of 25% over the latest\nTransformer architectures while reducing the computation time by an order of\nmagnitude. Our code is available at\n\\href{https://github.com/cchallu/n-hits}{this repository}.",
    "descriptor": "",
    "authors": [
      "Cristian Challu",
      "Kin G. Olivares",
      "Boris N. Oreshkin",
      "Federico Garza",
      "Max Mergenthaler",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12886"
  },
  {
    "id": "arXiv:2201.12888",
    "title": "A Dataset for Medical Instructional Video Classification and Question  Answering",
    "abstract": "This paper introduces a new challenge and datasets to foster research toward\ndesigning systems that can understand medical videos and provide visual answers\nto natural language questions. We believe medical videos may provide the best\npossible answers to many first aids, medical emergency, and medical education\nquestions. Toward this, we created the MedVidCL and MedVidQA datasets and\nintroduce the tasks of Medical Video Classification (MVC) and Medical Visual\nAnswer Localization (MVAL), two tasks that focus on cross-modal (medical\nlanguage and medical video) understanding. The proposed tasks and datasets have\nthe potential to support the development of sophisticated downstream\napplications that can benefit the public and medical practitioners. Our\ndatasets consist of 6,117 annotated videos for the MVC task and 3,010 annotated\nquestions and answers timestamps from 899 videos for the MVAL task. These\ndatasets have been verified and corrected by medical informatics experts. We\nhave also benchmarked each task with the created MedVidCL and MedVidQA datasets\nand proposed the multimodal learning methods that set competitive baselines for\nfuture research.",
    "descriptor": "",
    "authors": [
      "Deepak Gupta",
      "Kush Attal",
      "Dina Demner-Fushman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12888"
  },
  {
    "id": "arXiv:2201.12891",
    "title": "Learning Collective Action under Risk Diversity",
    "abstract": "Collective risk dilemmas (CRDs) are a class of n-player games that represent\nsocietal challenges where groups need to coordinate to avoid the risk of a\ndisastrous outcome. Multi-agent systems incurring such dilemmas face\ndifficulties achieving cooperation and often converge to sub-optimal,\nrisk-dominant solutions where everyone defects. In this paper we investigate\nthe consequences of risk diversity in groups of agents learning to play CRDs.\nWe find that risk diversity places new challenges to cooperation that are not\nobserved in homogeneous groups. We show that increasing risk diversity\nsignificantly reduces overall cooperation and hinders collective target\nachievement. It leads to asymmetrical changes in agents' policies -- i.e. the\nincrease in contributions from individuals at high risk is unable to compensate\nfor the decrease in contributions from individuals at low risk -- which overall\nreduces the total contributions in a population. When comparing RL behaviors to\nrational individualistic and social behaviors, we find that RL populations\nconverge to fairer contributions among agents. Our results highlight the need\nfor aligning risk perceptions among agents or develop new learning techniques\nthat explicitly account for risk diversity.",
    "descriptor": "\nComments: 17 pages, 24 figures. Accepted at the Cooperative AI Workshop at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Ramona Merhej",
      "Fernando P. Santos",
      "Francisco S. Melo",
      "Mohamed Chetouani",
      "Francisco C. Santos"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12891"
  },
  {
    "id": "arXiv:2201.12892",
    "title": "Differences in Social Media Usage Exist Between Western and Middle-East  Countries",
    "abstract": "In this paper, we empirically analyze two examples of a Western (DE) versus\nMiddle-East (SA) Online Social Messaging App. By focusing on the system\ninteractions over time in comparison, we identify inherent differences in user\nengagement. We take a deep dive and shed light onto differences in user\nattention shifts and showcase their structural implications to the user\nexperience. Our main findings show that in comparison to the German\ncounterparts, the Saudi communities prefer creating content in longer\nconversations, while voting more conservative.",
    "descriptor": "\nComments: Pre-Print: Passive and Active Measurement Conference 2022; 14 pages, 10 (Sub-)Figures\n",
    "authors": [
      "Jens Helge Reelfs",
      "Oliver Hohlfeld",
      "Niklas Henckell"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.12892"
  },
  {
    "id": "arXiv:2201.12894",
    "title": "Will Metaverse be NextG Internet? Vision, Hype, and Reality",
    "abstract": "Metaverse, with the combination of the prefix \"meta\" (meaning transcending)\nand the word \"universe\", has been deemed as the next-generation (NextG)\nInternet. It aims to create a shared virtual space that connects all virtual\nworlds via the Internet, where users, represented as digital avatars, can\ncommunicate and collaborate as if they are in the physical world. Nevertheless,\nthere is still no unified definition of the Metaverse. This article first\npresents our vision of what the key requirements of Metaverse should be and\nreviews what has been heavily advocated by the industry and the positions of\nvarious high-tech companies. It then briefly introduces existing social virtual\nreality (VR) platforms that can be viewed as early prototypes of Metaverse and\nconducts a reality check by diving into the network operation and performance\nof two representative platforms, Workrooms from Meta and AltspaceVR from\nMicrosoft. Finally, it concludes by discussing several opportunities and future\ndirections for further innovation.",
    "descriptor": "\nComments: 7 pages, 5figures\n",
    "authors": [
      "Ruizhi Cheng",
      "Nan Wu",
      "Songqing Chen",
      "Bo Han"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Multimedia (cs.MM)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12894"
  },
  {
    "id": "arXiv:2201.12895",
    "title": "Road User Position Prediction in Urban Environments via Locally Weighted  Learning",
    "abstract": "Prediction of the future position of a target road user given its current\nposition, velocity and type is formulated as a weighted average. Weights are\ndetermined from data of previously observed road users, specifically from those\nthat are most similar to the target. This formulation results in an\ninterpretable model with few parameters. The model is validated on a dataset of\nvehicles, bicycles, and pedestrians in real-world traffic situations. The model\noutperforms the baseline constant velocity model, wheras a baseline neural\nnetwork model shows comparable performance, but this model lacks the same level\nof interpretability. A comparison is made with state-of-the-arts, where these\nshow superior performance on a sparse dataset, for which it is expected that\nthe weighted average model works less well. With some further refinements a\nweighted average formulation could yield a reliable and interpretable model, in\nconstrast to one which is difficult to analyze and has a huge number of\nuninterpretable parameters.",
    "descriptor": "",
    "authors": [
      "Angelos Toytziaridis",
      "Paolo Falcone",
      "Jonas Sj\u00f6berg"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2201.12895"
  },
  {
    "id": "arXiv:2201.12896",
    "title": "Augmenting Novelty Search with a Surrogate Model to Engineer  Meta-Diversity in Ensembles of Classifiers",
    "abstract": "Using Neuroevolution combined with Novelty Search to promote behavioural\ndiversity is capable of constructing high-performing ensembles for\nclassification. However, using gradient descent to train evolved architectures\nduring the search can be computationally prohibitive. Here we propose a method\nto overcome this limitation by using a surrogate model which estimates the\nbehavioural distance between two neural network architectures required to\ncalculate the sparseness term in Novelty Search. We demonstrate a speedup of 10\ntimes over previous work and significantly improve on previous reported results\non three benchmark datasets from Computer Vision -- CIFAR-10, CIFAR-100, and\nSVHN. This results from the expanded architecture search space facilitated by\nusing a surrogate. Our method represents an improved paradigm for implementing\nhorizontal scaling of learning algorithms by making an explicit search for\ndiversity considerably more tractable for the same bounded resources.",
    "descriptor": "\nComments: 16 pages, 4 figures, 3 tables, EvoStar 2022\n",
    "authors": [
      "Rui P. Cardoso",
      "Emma Hart",
      "David Burth Kurka",
      "Jeremy V. Pitt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.12896"
  },
  {
    "id": "arXiv:2201.12899",
    "title": "Interpretable AI-based Large-scale 3D Pathloss Prediction Model for  enabling Emerging Self-Driving Networks",
    "abstract": "In modern wireless communication systems, radio propagation modeling to\nestimate pathloss has always been a fundamental task in system design and\noptimization. The state-of-the-art empirical propagation models are based on\nmeasurements in specific environments and limited in their ability to capture\nidiosyncrasies of various propagation environments. To cope with this problem,\nray-tracing based solutions are used in commercial planning tools, but they\ntend to be extremely time-consuming and expensive. We propose a Machine\nLearning (ML)-based model that leverages novel key predictors for estimating\npathloss. By quantitatively evaluating the ability of various ML algorithms in\nterms of predictive, generalization and computational performance, our results\nshow that Light Gradient Boosting Machine (LightGBM) algorithm overall\noutperforms others, even with sparse training data, by providing a 65% increase\nin prediction accuracy as compared to empirical models and 13x decrease in\nprediction time as compared to ray-tracing. To address the interpretability\nchallenge that thwarts the adoption of most ML-based models, we perform\nextensive secondary analysis using SHapley Additive exPlanations (SHAP) method,\nyielding many practically useful insights that can be leveraged for\nintelligently tuning the network configuration, selective enrichment of\ntraining data in real networks and for building lighter ML-based propagation\nmodel to enable low-latency use-cases.",
    "descriptor": "\nComments: Accepted at IEEE Transactions on Mobile Computing\n",
    "authors": [
      "Usama Masood",
      "Hasan Farooq",
      "Ali Imran",
      "Adnan Abu-Dayya"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12899"
  },
  {
    "id": "arXiv:2201.12900",
    "title": "OpTopNET: A Learning Optimal Topology Synthesizer for Ad-hoc Robot  Networks",
    "abstract": "In this paper, we synthesize a machine-learning stacked ensemble model a\nvector of which predicts the optimal topology of a robot network. This problem\nis technically a multi-task classification problem. However, we divide it into\na class of multi-class classification problems that can be more efficiently\nsolved. For this purpose, we first compose an algorithm to create ground-truth\ntopologies associated with various configurations of a robot network. This\nalgorithm incorporates a complex collection of nonlinear optimality criteria\nthat our learning model successfully manages to learn. Then, we propose a\nstacked ensemble model whose output is the topology prediction for the\nparticular robot associated with it. Each stacked ensemble instance constitutes\nthree low-level estimators whose outputs will be aggregated by a high-level\nboosting blender. The results of the simulations, applying our model to a\nnetwork of 10 robots, represents over %80 accuracy in the prediction of optimal\ntopologies corresponding to various configurations of this complex optimal\ntopology learning problem.",
    "descriptor": "",
    "authors": [
      "Matin Macktoobian",
      "Zhan Shu",
      "Qing Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12900"
  },
  {
    "id": "arXiv:2201.12901",
    "title": "Training and Evaluating a Jupyter Notebook Data Science Assistant",
    "abstract": "We study the feasibility of a Data Science assistant powered by a\nsequence-to-sequence transformer by training a new model JuPyT5 on all publicly\navailable Jupyter Notebook GitHub repositories and developing a new metric:\nData Science Problems (DSP). DSP is a collection of 1119 problems curated from\n306 pedagogical notebooks with 92 dataset dependencies, natural language and\nMarkdown problem descriptions, and assert-based unit tests. These notebooks\nwere designed to test university students' mastery of various Python\nimplementations of Math and Data Science, and we now leverage them to study the\nability of JuPyT5 to understand and pass the tests. We analyze the content of\nDSP, validate its quality, and we find that given 100 sampling attempts JuPyT5\nis able to solve 77.5\\% of the DSP problems. We further present various\nablation and statistical analyses and compare DSP to other recent natural\nlanguage to code benchmarks.",
    "descriptor": "",
    "authors": [
      "Shubham Chandel",
      "Colin B. Clement",
      "Guillermo Serrato",
      "Neel Sundaresan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.12901"
  },
  {
    "id": "arXiv:2201.12903",
    "title": "Aggregating Global Features into Local Vision Transformer",
    "abstract": "Local Transformer-based classification models have recently achieved\npromising results with relatively low computational costs. However, the effect\nof aggregating spatial global information of local Transformer-based\narchitecture is not clear. This work investigates the outcome of applying a\nglobal attention-based module named multi-resolution overlapped attention (MOA)\nin the local window-based transformer after each stage. The proposed MOA\nemploys slightly larger and overlapped patches in the key to enable\nneighborhood pixel information transmission, which leads to significant\nperformance gain. In addition, we thoroughly investigate the effect of the\ndimension of essential architecture components through extensive experiments\nand discover an optimum architecture design. Extensive experimental results\nCIFAR-10, CIFAR-100, and ImageNet-1K datasets demonstrate that the proposed\napproach outperforms previous vision Transformers with a comparatively fewer\nnumber of parameters.",
    "descriptor": "",
    "authors": [
      "Krushi Patel",
      "Andres M. Bur",
      "Fengjun Li",
      "Guanghui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12903"
  },
  {
    "id": "arXiv:2201.12904",
    "title": "COIN++: Data Agnostic Neural Compression",
    "abstract": "Neural compression algorithms are typically based on autoencoders that\nrequire specialized encoder and decoder architectures for different data\nmodalities. In this paper, we propose COIN++, a neural compression framework\nthat seamlessly handles a wide range of data modalities. Our approach is based\non converting data to implicit neural representations, i.e. neural functions\nthat map coordinates (such as pixel locations) to features (such as RGB\nvalues). Then, instead of storing the weights of the implicit neural\nrepresentation directly, we store modulations applied to a meta-learned base\nnetwork as a compressed code for the data. We further quantize and entropy code\nthese modulations, leading to large compression gains while reducing encoding\ntime by two orders of magnitude compared to baselines. We empirically\ndemonstrate the effectiveness of our method by compressing various data\nmodalities, from images to medical and climate data.",
    "descriptor": "",
    "authors": [
      "Emilien Dupont",
      "Hrushikesh Loya",
      "Milad Alizadeh",
      "Adam Goli\u0144ski",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12904"
  },
  {
    "id": "arXiv:2201.12905",
    "title": "Modularity-based Backbone Extraction in Weighted Complex Networks",
    "abstract": "The constantly growing size of real-world networks is a great challenge.\nTherefore, building a compact version of networks allowing their analyses is a\nmust. Backbone extraction techniques are among the leading solutions to reduce\nnetwork size while preserving its features. Coarse-graining merges similar\nnodes to reduce the network size, while filter-based methods remove nodes or\nedges according to a specific statistical property. Since community structure\nis ubiquitous in real-world networks, preserving it in the backbone extraction\nprocess is of prime interest. To this end, we propose a filter-based method.\nThe so-called \"modularity vitality backbone\" removes nodes with the lower\ncontribution to the network's modularity. Experimental results show that the\nproposed strategy outperforms the \"overlapping nodes ego backbone\" and the\n\"overlapping nodes and hub backbone.\" These two backbone extraction processes\nrecently introduced have proved their efficacy to preserve better the\ninformation of the original network than the popular disparity filter.",
    "descriptor": "\nComments: Accepted in the the International School and Conference on Network Science - Porto, Portugal (NetSciX 2022)\n",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12905"
  },
  {
    "id": "arXiv:2201.12907",
    "title": "A Topological Centrality Measure for Directed Networks",
    "abstract": "Given a directed network $ G $, we are interested in studying the qualitative\nfeatures of $ G $ which govern how perturbations propagate across $ G $.\nVarious classical centrality measures have been already developed and proven\nuseful to capture qualitative features and behaviors for undirected networks.\nIn this paper, we use topological data analysis (TDA) to adapt measures of\ncentrality to capture both directedness and non-local propagating behaviors in\nnetworks. We introduce a new metric for computing centrality in directed\nweighted networks, namely the quasi-centrality measure. We compute these\nmetrics on trade networks to illustrate that our measure successfully captures\npropagating effects in the network and can also be used to identify sources of\nshocks that can disrupt the topology of directed networks. Moreover, we\nintroduce a method that gives a hierarchical representation of the topological\ninfluences of nodes in a directed network.",
    "descriptor": "\nComments: 29 pages, 29 figures\n",
    "authors": [
      "Fenghuan He"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2201.12907"
  },
  {
    "id": "arXiv:2201.12910",
    "title": "Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection",
    "abstract": "We develop a sparse optimization problem for the determination of the total\nset of features that discriminate two or more classes. This is a sparse\nimplementation of the centroid-encoder for nonlinear data reduction and\nvisualization called Sparse Centroid-Encoder (SCE). We also provide a feature\nselection framework that first ranks each feature by its occurrence, and the\noptimal number of features is chosen using a validation set. The algorithm is\napplied to a wide variety of data sets including, single-cell biological data,\nhigh dimensional infectious disease data, hyperspectral data, image data, and\nspeech data. We compared our method to various state-of-the-art feature\nselection techniques, including two neural network-based models (DFS, and\nLassoNet), Sparse SVM, and Random Forest. We empirically showed that SCE\nfeatures produced better classification accuracy on the unseen test data, often\nwith fewer features.",
    "descriptor": "\nComments: 14 pages, 6 figures, 3 tables. Used 9 data sets and 6 state-of-the-art models for comparison\n",
    "authors": [
      "Tomojit Ghosh",
      "Michael Kirby"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12910"
  },
  {
    "id": "arXiv:2201.12911",
    "title": "Grammatical cues are largely, but not completely, redundant with word  meanings in natural language",
    "abstract": "The combinatorial power of language has historically been argued to be\nenabled by syntax: rules that allow words to combine hierarchically to convey\ncomplex meanings. But how important are these rules in practice? We performed a\nbroad-coverage cross-linguistic investigation of the importance of grammatical\ncues for interpretation. First, English and Russian speakers (n=484) were\npresented with subjects, verbs, and objects (in random order and with\nmorphological markings removed) extracted from naturally occurring sentences,\nand were asked to identify which noun is the agent of the action. Accuracy was\nhigh in both languages (~89% in English, ~87% in Russian), suggesting that word\nmeanings strongly constrain who is doing what to whom. Next, we trained a\nneural network machine classifier on a similar task: predicting which nominal\nin a subject-verb-object triad is the subject. Across 30 languages from eight\nlanguage families, performance was consistently high: a median accuracy of 87%,\ncomparable to the accuracy observed in the human experiments. These results\nhave ramifications for any theory of why languages look the way that they do,\nand seemingly pose a challenge for efficiency-based theories: why have\ngrammatical cues for argument role if they only have utility in 10-15% of\nsentences? We suggest that although grammatical cues are not usually necessary,\nthey are useful in the rare cases when the intended meaning cannot be inferred\nfrom the words alone, including descriptions of human interactions, where roles\nare often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing\nnon-canonical meanings (e.g., the man bit the dog). Importantly, for such cues\nto be useful, they have to be reliable, which means being ubiquitously used,\nincluding when they are not needed.",
    "descriptor": "",
    "authors": [
      "Kyle Mahowald",
      "Evgeniia Diachek",
      "Edward Gibson",
      "Evelina Fedorenko",
      "Richard Futrell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12911"
  },
  {
    "id": "arXiv:2201.12914",
    "title": "Investigating Centrality Measures in Social Networks with Community  Structure",
    "abstract": "Centrality measures are crucial in quantifying the influence of the members\nof a social network. Although there has been a great deal of work dealing with\nthis issue, the vast majority of classical centrality measures are agnostic of\nthe community structure characterizing many social networks. Recent works have\ndeveloped community-aware centrality measures that exploit features of the\ncommunity structure information encountered in most real-world complex\nnetworks. In this paper, we investigate the interactions between 5 popular\nclassical centrality measures and 5 community-aware centrality measures using 8\nreal-world online networks. Correlation as well as similarity measures between\nboth type of centrality measures are computed. Results show that\ncommunity-aware centrality measures can be divided into two groups. The first\ngroup, which includes Bridging centrality, Community Hub-Bridge and\nParticipation Coefficient, provides distinctive node information as compared to\nclassical centrality. This behavior is consistent across the networks. The\nsecond group which includes Community-based Mediator and Number of Neighboring\nCommunities is characterized by more mixed results that vary across networks.",
    "descriptor": "\nComments: Accepted in The International Conference on Complex Networks and their Applications (2020)\n",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12914"
  },
  {
    "id": "arXiv:2201.12918",
    "title": "How Correlated are Community-aware and Classical Centrality Measures in  Complex Networks?",
    "abstract": "Unlike classical centrality measures, recently developed community-aware\ncentrality measures use a network's community structure to identify influential\nnodes in complex networks. This paper investigates their relationship on a set\nof fifty real-world networks originating from various domains. Results show\nthat classical and community-aware centrality measures generally exhibit low to\nmedium correlation values. These results are consistent across networks.\nTransitivity and efficiency are the most influential macroscopic network\nfeatures driving the correlation variation between classical and\ncommunity-aware centrality measures. Additionally, the mixing parameter, the\nmodularity, and the Max-ODF are the main mesoscopic topological properties\nexerting the most substantial effect.",
    "descriptor": "\nComments: Accepted in International Conference on Complex Networks: CompleNet-Live 2021\n",
    "authors": [
      "Stephany Rajeh",
      "Marinette Savonnet",
      "Eric Leclercq",
      "Hocine Cherifi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12918"
  },
  {
    "id": "arXiv:2201.12919",
    "title": "Provable Domain Generalization via Invariant-Feature Subspace Recovery",
    "abstract": "Domain generalization asks for models trained on a set of training\nenvironments to perform well on unseen test environments. Recently, a series of\nalgorithms such as Invariant Risk Minimization (IRM) has been proposed for\ndomain generalization. However, Rosenfeld et al. (2021) shows that in a simple\nlinear data model, even if non-convexity issues are ignored, IRM and its\nextensions cannot generalize to unseen environments with less than $d_s+1$\ntraining environments, where $d_s$ is the dimension of the spurious-feature\nsubspace. In this paper, we propose to achieve domain generalization with\nInvariant-feature Subspace Recovery (ISR). Our first algorithm, ISR-Mean, can\nidentify the subspace spanned by invariant features from the first-order\nmoments of the class-conditional distributions, and achieve provable domain\ngeneralization with $d_s+1$ training environments under the data model of\nRosenfeld et al. (2021). Our second algorithm, ISR-Cov, further reduces the\nrequired number of training environments to $O(1)$ using the information of\nsecond-order moments. Notably, unlike IRM, our algorithms bypass non-convexity\nissues and enjoy global convergence guarantees. Empirically, our ISRs can\nobtain superior performance compared with IRM on synthetic benchmarks. In\naddition, on three real-world image and text datasets, we show that ISR-Mean\ncan be used as a simple yet effective post-processing method to increase the\nworst-case accuracy of trained models against spurious correlations and group\nshifts.",
    "descriptor": "\nComments: The code will be released at this https URL\n",
    "authors": [
      "Haoxiang Wang",
      "Haozhe Si",
      "Bo Li",
      "Han Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12919"
  },
  {
    "id": "arXiv:2201.12923",
    "title": "Asynchronous Opinion Dynamics in Social Networks",
    "abstract": "Opinion spreading in a society decides the fate of elections, the success of\nproducts, and the impact of political or social movements. The model by\nHegselmann and Krause is a well-known theoretical model to study such opinion\nformation processes in social networks. In contrast to many other theoretical\nmodels, it does not converge towards a situation where all agents agree on the\nsame opinion. Instead, it assumes that people find an opinion reasonable if and\nonly if it is close to their own. The system converges towards a stable\nsituation where agents sharing the same opinion form a cluster, and agents in\ndifferent clusters do not \\mbox{influence each other.}\nWe focus on the social variant of the Hegselmann-Krause model where agents\nare connected by a social network and their opinions evolve in an iterative\nprocess. When activated, an agent adopts the average of the opinions of its\nneighbors having a similar opinion. By this, the set of influencing neighbors\nof an agent may change over time. To the best of our knowledge, social\nHegselmann-Krause systems with asynchronous opinion updates have only been\nstudied with the complete graph as social network. We show that such opinion\ndynamics with random agent activation are guaranteed to converge for any social\nnetwork. We provide an upper bound of $\\mathcal{O}(n|E|^2\n(\\varepsilon/\\delta)^2)$ on the expected number of opinion updates until\nconvergence, where $|E|$ is the number of edges of the social network. For the\ncomplete social network we show a bound of $\\mathcal{O}(n^3(n^2 +\n(\\varepsilon/\\delta)^2))$ that represents a major improvement over the\npreviously best upper bound of $\\mathcal{O}(n^9 (\\varepsilon/\\delta)^2)$. Our\nbounds are complemented by simulations that indicate asymptotically matching\nlower bounds.",
    "descriptor": "\nComments: Accepted at AAMAS 2022\n",
    "authors": [
      "Petra Berenbrink",
      "Martin Hoefer",
      "Dominik Kaaser",
      "Pascal Lenzner",
      "Malin Rau",
      "Daniel Schmand"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.12923"
  },
  {
    "id": "arXiv:2201.12926",
    "title": "Compositionality as Lexical Symmetry",
    "abstract": "Standard deep network models lack the inductive biases needed to generalize\ncompositionally in tasks like semantic parsing, translation, and question\nanswering. A large body of work in natural language processing seeks to\novercome this limitation with new model architectures that enforce a\ncompositional process of sentence interpretation. In this paper, we present a\ndomain-general framework for compositional modeling that instead formulates\ncompositionality as a constraint on data distributions. We prove that for any\ntask factorizable into a lexicon and a composition function, there exists a\nfamily of data transformation functions that are guaranteed to produce new,\nwell-formed examples when applied to training data. We further show that it is\npossible to identify these data transformations even when the composition\nfunction is unknown (e.g. when we do not know how to write or infer a symbolic\ngrammar). Using these transformation functions to perform data augmentation for\nordinary RNN and transformer sequence models, we obtain state-of-the-art\nresults on the CLEVR-CoGenT visual question answering dataset, and results\ncomparable to specialized model architectures on the COGS semantic parsing\ndataset.",
    "descriptor": "\nComments: 12 pages, 3 Figures\n",
    "authors": [
      "Ekin Aky\u00fcrek",
      "Jacob Andreas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12926"
  },
  {
    "id": "arXiv:2201.12928",
    "title": "PLATINUM: Semi-Supervised Model Agnostic Meta-Learning using Submodular  Mutual Information",
    "abstract": "Few-shot classification (FSC) requires training models using a few (typically\none to five) data points per class. Meta learning has proven to be able to\nlearn a parametrized model for FSC by training on various other classification\ntasks. In this work, we propose PLATINUM (semi-suPervised modeL Agnostic\nmeTa-learnIng usiNg sUbmodular Mutual information), a novel semi-supervised\nmodel agnostic meta-learning framework that uses the submodular mutual\ninformation (SMI) functions to boost the performance of FSC. PLATINUM leverages\nunlabeled data in the inner and outer loop using SMI functions during\nmeta-training and obtains richer meta-learned parameterizations for meta-test.\nWe study the performance of PLATINUM in two scenarios - 1) where the unlabeled\ndata points belong to the same set of classes as the labeled set of a certain\nepisode, and 2) where there exist out-of-distribution classes that do not\nbelong to the labeled set. We evaluate our method on various settings on the\nminiImageNet, tieredImageNet and Fewshot-CIFAR100 datasets. Our experiments\nshow that PLATINUM outperforms MAML and semi-supervised approaches like\npseduo-labeling for semi-supervised FSC, especially for small ratio of labeled\nexamples per class.",
    "descriptor": "\nComments: *Equal Contribution\n",
    "authors": [
      "Changbin Li",
      "Suraj Kothawade",
      "Feng Chen",
      "Rishabh Iyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12928"
  },
  {
    "id": "arXiv:2201.12929",
    "title": "The Geometry of Robust Value Functions",
    "abstract": "The space of value functions is a fundamental concept in reinforcement\nlearning. Characterizing its geometric properties may provide insights for\noptimization and representation. Existing works mainly focus on the value space\nfor Markov Decision Processes (MDPs). In this paper, we study the geometry of\nthe robust value space for the more general Robust MDPs (RMDPs) setting, where\ntransition uncertainties are considered. Specifically, since we find it hard to\ndirectly adapt prior approaches to RMDPs, we start with revisiting the\nnon-robust case, and introduce a new perspective that enables us to\ncharacterize both the non-robust and robust value space in a similar fashion.\nThe key of this perspective is to decompose the value space, in a state-wise\nmanner, into unions of hypersurfaces. Through our analysis, we show that the\nrobust value space is determined by a set of conic hypersurfaces, each of which\ncontains the robust values of all policies that agree on one state.\nFurthermore, we find that taking only extreme points in the uncertainty set is\nsufficient to determine the robust value space. Finally, we discuss some other\naspects about the robust value space, including its non-convexity and policy\nagreement on multiple states.",
    "descriptor": "",
    "authors": [
      "Kaixin Wang",
      "Navdeep Kumar",
      "Kuangqi Zhou",
      "Bryan Hooi",
      "Jiashi Feng",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12929"
  },
  {
    "id": "arXiv:2201.12931",
    "title": "Efficient hybrid topology optimization using GPU and homogenization  based multigrid approach",
    "abstract": "We propose a new hybrid topology optimization algorithm based on multigrid\napproach that combines the parallelization strategy of CPU using OpenMP and\nheavily multithreading capabilities of modern Graphics Processing Units (GPU).\nIn addition to that significant computational efficiency in memory requirement\nhas been achieved using homogenization strategy. The algorithm has been\nintegrated with versitile computing platform of MATLAB for ease of use and\ncustomization. The bottlenecking repetitive solution of the state equation has\nbeen solved using an optimized geometric multigrid approach along with CUDA\nparallelization enabling an order of magnitude faster in computational time\nthan current state of the art implementations. On-the-fly computation of\nauxiliary matrices in the multigrid scheme and modification in interpolation\nschemes using homogenization strategy removes memory limitation of GPUs. Memory\nhierarchy of GPU has also been exploited for further optimized implementations.\nAll these enable solution of structures involving hundred millions of three\ndimensional brick elements to be accomplished in a standard desktop computer or\na workstation. Performance of the proposed algorithm is illustrated using\nseveral examples including design dependent loads and multimaterial.Results\nobtained indicate the excellent performance and scalability of the proposed\napproach.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Arya Prakash Padhi",
      "Souvik Chakraborty",
      "Anupam Chakrabarti",
      "Rajib Chowdhury"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12931"
  },
  {
    "id": "arXiv:2201.12938",
    "title": "Probe-Based Interventions for Modifying Agent Behavior",
    "abstract": "Neural nets are powerful function approximators, but the behavior of a given\nneural net, once trained, cannot be easily modified. We wish, however, for\npeople to be able to influence neural agents' actions despite the agents never\ntraining with humans, which we formalize as a human-assisted decision-making\nproblem. Inspired by prior art initially developed for model explainability, we\ndevelop a method for updating representations in pre-trained neural nets\naccording to externally-specified properties. In experiments, we show how our\nmethod may be used to improve human-agent team performance for a variety of\nneural networks from image classifiers to agents in multi-agent reinforcement\nlearning settings.",
    "descriptor": "",
    "authors": [
      "Mycal Tucker",
      "William Kuhl",
      "Khizer Shahid",
      "Seth Karten",
      "Katia Sycara",
      "Julie Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.12938"
  },
  {
    "id": "arXiv:2201.12939",
    "title": "Race Driver Evaluation at a Driving Simulator using a physical Model and  a Machine Learning Approach",
    "abstract": "Professional race drivers are still superior to automated systems at\ncontrolling a vehicle at its dynamic limit. Gaining insight into race drivers'\nvehicle handling process might lead to further development in the areas of\nautomated driving systems. We present a method to study and evaluate race\ndrivers on a driver-in-the-loop simulator by analysing tire grip potential\nexploitation. Given initial data from a simulator run, two optimiser based on\nphysical models maximise the horizontal vehicle acceleration or the tire\nforces, respectively. An overall performance score, a vehicle-trajectory score\nand a handling score are introduced to evaluate drivers. Our method is thereby\ncompletely track independent and can be used from one single corner up to a\nlarge data set. We apply the proposed method to a motorsport data set\ncontaining over 1200 laps from seven professional race drivers and two amateur\ndrivers whose lap times are 10-20% slower. The difference to the professional\ndrivers comes mainly from their inferior handling skills and not their choice\nof driving line. A downside of the presented method for certain applications is\nan extensive computation time. Therefore, we propose a Long-short-term memory\n(LSTM) neural network to estimate the driver evaluation scores. We show that\nthe neural network is accurate and robust with a root-mean-square error between\n2-5% and can replace the optimisation based method. The time for processing the\ndata set considered in this work is reduced from 68 hours to 12 seconds, making\nthe neural network suitable for real-time application.",
    "descriptor": "",
    "authors": [
      "Julian von Schleinitz",
      "Thomas Schwarzhuber",
      "Lukas W\u00f6rle",
      "Michael Graf",
      "Arno Eichberger",
      "Wolfgang Trutschnig",
      "Andreas Schr\u00f6der"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12939"
  },
  {
    "id": "arXiv:2201.12940",
    "title": "Uplink Transmit Design for Massive MIMO LEO Satellite Communications",
    "abstract": "This paper investigates the uplink (UL) transmit design for massive\nmultiple-input multiple-output (MIMO) low-earth-orbit (LEO) satellite\ncommunication (SATCOM), where the long-term statistical channel state\ninformation is utilized at the user terminals (UTs). We consider that the\nuniform planar arrays are deployed at both the satellite and UTs, and derive\nthe UL massive MIMO LEO satellite channel model. With the aim to achieve the\nergodic sum rate capacity, we show that the rank of each UT's optimal transmit\ncovariance matrix does not exceed that of its channel correlation matrix at the\nUT sides. This reveals the maximum number of independent data streams that can\nbe transmitted from each UT to the satellite. We further show that the design\nof the transmit covariance matrices can be reduced into that of\nlower-dimensional matrices, for which a stochastic programming based algorithm\nis developed by exploiting the optimal lower-dimensional matrices' structure.\nTo reduce the computational complexity, we invoke the asymptotic programming\nand develop a computationally efficient algorithm to compute the transmit\ncovariance matrices. Simulations show that the proposed UL transmit strategies\nare superior to the conventional schemes, and the low-complexity asymptotic\nprogramming based UL transmit design can attain near-optimal performance in\nmassive MIMO LEO SATCOM.",
    "descriptor": "\nComments: 30 pages, submitted to IEEE for possible publication\n",
    "authors": [
      "Ke-Xin Li",
      "Jiaheng Wang",
      "Xiqi Gao",
      "Christos G. Tsinos",
      "Bj\u00f6rn Ottersten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12940"
  },
  {
    "id": "arXiv:2201.12944",
    "title": "Deep Learning Approaches on Image Captioning: A Review",
    "abstract": "Automatic image captioning, which involves describing the contents of an\nimage, is a challenging problem with many applications in various research\nfields. One notable example is designing assistants for the visually impaired.\nRecently, there have been significant advances in image captioning methods\nowing to the breakthroughs in deep learning. This survey paper aims to provide\na structured review of recent image captioning techniques, and their\nperformance, focusing mainly on deep learning methods. We also review\nwidely-used datasets and performance metrics, in addition to the discussions on\nopen problems and unsolved challenges in image captioning.",
    "descriptor": "\nComments: 30 pages, 5 figures\n",
    "authors": [
      "Taraneh Ghandi",
      "Hamidreza Pourreza",
      "Hamidreza Mahyar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12944"
  },
  {
    "id": "arXiv:2201.12950",
    "title": "Network Programming via Computable Products",
    "abstract": "The User Plane Function (UPF) aims to provide network services in the 3GPP 5G\ncore network. These services need to be implemented on demand inexpensively\nwith provable properties. Existing network dataplane programming languages are\nnot up to the task. A new software paradigm is presented for the UPF. It is\ninspired by model checking a concurrent reactive system where conceptually each\ncomponent of the system is modeled as an extended finite-state machine and\ntheir product is verified. We show how such a product can be computed for one\nexample of a UPF and how its state invariants can be inferred, thereby\neliminating the need to formally verify the product separately. Code can be\ngenerated from the product and regenerated on the fly to remain optimal for the\nprobability distribution of network traffic the UPF must process.",
    "descriptor": "\nComments: 6 pages, 7 tables\n",
    "authors": [
      "Dennis Volpano"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.12950"
  },
  {
    "id": "arXiv:2201.12955",
    "title": "Generalized Bayesian Upper Confidence Bound with Approximate Inference  for Bandit Problems",
    "abstract": "Bayesian bandit algorithms with approximate inference have been widely used\nin practice with superior performance. Yet, few studies regarding the\nfundamental understanding of their performances are available. In this paper,\nwe propose a Bayesian bandit algorithm, which we call Generalized Bayesian\nUpper Confidence Bound (GBUCB), for bandit problems in the presence of\napproximate inference. Our theoretical analysis demonstrates that in Bernoulli\nmulti-armed bandit, GBUCB can achieve $O(\\sqrt{T}(\\log T)^c)$ frequentist\nregret if the inference error measured by symmetrized Kullback-Leibler\ndivergence is controllable. This analysis relies on a novel sensitivity\nanalysis for quantile shifts with respect to inference errors. To our best\nknowledge, our work provides the first theoretical regret bound that is better\nthan $o(T)$ in the setting of approximate inference. Our experimental\nevaluations on multiple approximate inference settings corroborate our theory,\nshowing that our GBUCB is consistently superior to BUCB and Thompson sampling.",
    "descriptor": "",
    "authors": [
      "Ziyi Huang",
      "Henry Lam",
      "Amirhossein Meisami",
      "Haofeng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12955"
  },
  {
    "id": "arXiv:2201.12961",
    "title": "Plug-In Inversion: Model-Agnostic Inversion for Vision with Data  Augmentations",
    "abstract": "Existing techniques for model inversion typically rely on hard-to-tune\nregularizers, such as total variation or feature regularization, which must be\nindividually calibrated for each network in order to produce adequate images.\nIn this work, we introduce Plug-In Inversion, which relies on a simple set of\naugmentations and does not require excessive hyper-parameter tuning. Under our\nproposed augmentation-based scheme, the same set of augmentation\nhyper-parameters can be used for inverting a wide range of image classification\nmodels, regardless of input dimensions or the architecture. We illustrate the\npracticality of our approach by inverting Vision Transformers (ViTs) and\nMulti-Layer Perceptrons (MLPs) trained on the ImageNet dataset, tasks which to\nthe best of our knowledge have not been successfully accomplished by any\nprevious works.",
    "descriptor": "",
    "authors": [
      "Amin Ghiasi",
      "Hamid Kazemi",
      "Steven Reich",
      "Chen Zhu",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12961"
  },
  {
    "id": "arXiv:2201.12965",
    "title": "Inverse design of photonic devices with strict foundry fabrication  constraints",
    "abstract": "We introduce a new method for inverse design of nanophotonic devices which\nguarantees that designs satisfy strict length scale constraints -- including\nminimum width and spacing constraints required by commercial semiconductor\nfoundries. The method adopts several concepts from machine learning to\ntransform the problem of topology optimization with strict length scale\nconstraints to an unconstrained stochastic gradient optimization problem.\nSpecifically, we introduce a conditional generator for feasible designs and\nadopt a straight-through estimator for backpropagation of gradients to a latent\ndesign. We demonstrate the performance and reliability of our method by\ndesigning several common integrated photonic components.",
    "descriptor": "\nComments: 13 pages, 14 figures\n",
    "authors": [
      "Martin F. Schubert",
      "Alfred K. C. Cheung",
      "Ian A. D. Williamson",
      "Aleksandra Spyra",
      "David H. Alexander"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2201.12965"
  },
  {
    "id": "arXiv:2201.12972",
    "title": "Implementation of an Elastic Reconfigurable Optical Add/Drop Multiplexer  based on Subcarriers for Application in Optical Multichannel Networks",
    "abstract": "We designed a Reconfigurable Optical Add/Drop Multiplexer (ROADM) based on a\nsubcarrier add/drop node in an optical communication system that is suitable\nfor all kinds of optical multiplexing signals. To achieve this goal, at first,\nwe designed an optical comb generator based on a dual-drive Mach Zehnder. The\nnew ROADM setup is validated by a 100 Gb/s 4-subcarrier. In the final step, we\nchecked the performance of the system in terms of the bit error rate (BER)\nversus optical signal-to-noise ratio (OSNR) to verify the add/drop operation\nhad been successfully performed at 10-9 and is suitable to apply in an\nall-optical multiplexing technique.",
    "descriptor": "\nComments: 4 pages, 3 figures, conference\n",
    "authors": [
      "Faranak Khosravi",
      "Mehdi Tarhani",
      "Shivani Kurle",
      "Mehdi Shadaram"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12972"
  },
  {
    "id": "arXiv:2201.12975",
    "title": "Rotting infinitely many-armed bandits",
    "abstract": "We consider the infinitely many-armed bandit problem with rotting rewards,\nwhere the mean reward of an arm decreases at each pull of the arm according to\nan arbitrary trend with maximum rotting rate $\\varrho=o(1)$. We show that this\nlearning problem has an $\\Omega(\\max\\{\\varrho^{1/3}T,\\sqrt{T}\\})$ worst-case\nregret lower bound where $T$ is the horizon time. We show that a matching upper\nbound $\\tilde{O}(\\max\\{\\varrho^{1/3}T,\\sqrt{T}\\})$, up to a poly-logarithmic\nfactor, can be achieved by an algorithm that uses a UCB index for each arm and\na threshold value to decide whether to continue pulling an arm or remove the\narm from further consideration, when the algorithm knows the value of the\nmaximum rotting rate $\\varrho$. We also show that an\n$\\tilde{O}(\\max\\{\\varrho^{1/3}T,T^{3/4}\\})$ regret upper bound can be achieved\nby an algorithm that does not know the value of $\\varrho$, by using an adaptive\nUCB index along with an adaptive threshold value.",
    "descriptor": "",
    "authors": [
      "Jung-hun Kim",
      "Milan Vojnovic",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12975"
  },
  {
    "id": "arXiv:2201.12976",
    "title": "Heterogeneous Federated Learning via Grouped Sequential-to-Parallel  Training",
    "abstract": "Federated learning (FL) is a rapidly growing privacy-preserving collaborative\nmachine learning paradigm. In practical FL applications, local data from each\ndata silo reflect local usage patterns. Therefore, there exists heterogeneity\nof data distributions among data owners (a.k.a. FL clients). If not handled\nproperly, this can lead to model performance degradation. This challenge has\ninspired the research field of heterogeneous federated learning, which\ncurrently remains open. In this paper, we propose a data heterogeneity-robust\nFL approach, FedGSP, to address this challenge by leveraging on a novel concept\nof dynamic Sequential-to-Parallel (STP) collaborative training. FedGSP assigns\nFL clients to homogeneous groups to minimize the overall distribution\ndivergence among groups, and increases the degree of parallelism by reassigning\nmore groups in each round. It is also incorporated with a novel Inter-Cluster\nGrouping (ICG) algorithm to assist in group assignment, which uses the centroid\nequivalence theorem to simplify the NP-hard grouping problem to make it\nsolvable. Extensive experiments have been conducted on the non-i.i.d. FEMNIST\ndataset. The results show that FedGSP improves the accuracy by 3.7% on average\ncompared with seven state-of-the-art approaches, and reduces the training time\nand communication overhead by more than 90%.",
    "descriptor": "\nComments: Accepted by the 27th International Conference on Database Systems for Advanced Applications (DASFAA-2022), Hyderabad, India\n",
    "authors": [
      "Shenglai Zeng",
      "Zonghang Li",
      "Hongfang Yu",
      "Yihong He",
      "Zenglin Xu",
      "Dusit Niyato",
      "Han Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.12976"
  },
  {
    "id": "arXiv:2201.12981",
    "title": "G$ \\mathbf{^2} $VD Planner: An Efficient Motion Planning Approach With  Grid-based Generalized Voronoi Diagrams",
    "abstract": "In this letter, an efficient motion planning approach with grid-based\ngeneralized Voronoi diagrams is newly proposed for mobile robots. Different\nfrom existing approaches, the novelty of this work is twofold: 1) a new state\nlattice-based path searching approach is proposed, in which the search space is\nreduced to a Voronoi corridor to further improve the search efficiency, along\nwith a Voronoi potential field constructed to make the searched path keep a\nreasonable distance from obstacles to provide sufficient optimization margin\nfor the subsequent path smoothing, and 2) an efficient quadratic\nprogramming-based path smoothing approach is presented, wherein the clearance\nto obstacles is considered in the form of the penalty of the deviation from the\nsafe reference path to improve the path clearance of hard-constrained path\nsmoothing approaches. We validate the efficiency and smoothness of our approach\nin various challenging simulation scenarios and large-scale outdoor\nenvironments. It is shown that the computational efficiency is improved by\n17.1% in the path searching stage, and smoothing the path with our approach is\n11.86 times faster than a recent gradient-based path smoothing approach. We\nwill release the source code to the robotics community.",
    "descriptor": "\nComments: Submitted to IEEE RA-L with 2022 IEEE/RSJ IROS, under review\n",
    "authors": [
      "Jian Wen",
      "Xuebo Zhang",
      "Hui Liu",
      "Haoyue Liu",
      "Jing Yuan",
      "Yongchun Fang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12981"
  },
  {
    "id": "arXiv:2201.12987",
    "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention  Mechanism",
    "abstract": "Interpretable graph learning is in need as many scientific applications\ndepend on learning models to collect insights from graph-structured data.\nPrevious works mostly focused on using post-hoc approaches to interpret a\npre-trained model (graph neural network models in particular). They argue\nagainst inherently interpretable models because good interpretation of these\nmodels is often at the cost of their prediction accuracy. And, the widely used\nattention mechanism for inherent interpretation often fails to provide faithful\ninterpretation in graph learning tasks. In this work, we address both issues by\nproposing Graph Stochastic Attention (GSAT), an attention mechanism derived\nfrom the information bottleneck principle. GSAT leverages stochastic attention\nto block the information from the task-irrelevant graph components while\nlearning stochasticity-reduced attention to select the task-relevant subgraphs\nfor interpretation. GSAT can also apply to fine-tuning and interpreting\npre-trained models via stochastic attention mechanism. Extensive experiments on\neight datasets show that GSAT outperforms the state-of-the-art methods by up to\n20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy.",
    "descriptor": "",
    "authors": [
      "Siqi Miao",
      "Miaoyuan Liu",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12987"
  },
  {
    "id": "arXiv:2201.12990",
    "title": "Lightweight Projective Derivative Codes for Compressed Asynchronous  Gradient Descent",
    "abstract": "Coded distributed computation has become common practice for performing\ngradient descent on large datasets to mitigate stragglers and other faults.\nThis paper proposes a novel algorithm that encodes the partial derivatives\nthemselves and furthermore optimizes the codes by performing lossy compression\non the derivative codewords by maximizing the information contained in the\ncodewords while minimizing the information between the codewords. The utility\nof this application of coding theory is a geometrical consequence of the\nobserved fact in optimization research that noise is tolerable, sometimes even\nhelpful, in gradient descent based learning algorithms since it helps avoid\noverfitting and local minima. This stands in contrast with much current\nconventional work on distributed coded computation which focuses on recovering\nall of the data from the workers. A second further contribution is that the\nlow-weight nature of the coding scheme allows for asynchronous gradient updates\nsince the code can be iteratively decoded; i.e., a worker's task can\nimmediately be updated into the larger gradient. The directional derivative is\nalways a linear function of the direction vectors; thus, our framework is\nrobust since it can apply linear coding techniques to general machine learning\nframeworks such as deep neural networks.",
    "descriptor": "\nComments: 10 pages, 3 figures, preprint\n",
    "authors": [
      "Pedro Soto",
      "Ilia Ilmer",
      "Haibin Guan",
      "Jun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12990"
  },
  {
    "id": "arXiv:2201.12991",
    "title": "Federated Learning with Erroneous Communication Links",
    "abstract": "In this paper, we consider the federated learning (FL) problem in the\npresence of communication errors. We model the link between the devices and the\ncentral node (CN) by a packet erasure channel, where the local parameters from\ndevices are either erased or received correctly by CN with probability $e$ and\n$1-e$, respectively. We provide mathematical proof for the convergence of the\nFL algorithm in the presence of communication errors, where the CN uses past\nlocal updates when the fresh updates are not received from some devices. We\nshow via simulations that by using the past local updates, the FL algorithm can\nconverge in the presence of communication errors. We also show that when the\ndataset is uniformly distributed among devices, the FL algorithm that only uses\nfresh updates and discards missing updates might converge faster than the FL\nalgorithm that uses past local updates.",
    "descriptor": "\nComments: The paper is submitted to IEEE Communications Letters\n",
    "authors": [
      "Mahyar Shirvanimoghaddam",
      "Yifeng Gao",
      "Aradhika Guha",
      "Ayoob Salari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12991"
  },
  {
    "id": "arXiv:2201.12993",
    "title": "Three types of quasi-Trefftz functions for the 3D convected Helmholtz  equation: construction and theoretical approximation properties",
    "abstract": "Trefftz methods are numerical methods for the approximation of solutions to\nboundary and/or initial value problems. They are Galerkin methods with\nparticular test and trial functions, which solve locally the governing partial\ndifferential equation (PDE). This property is called the Trefftz property.\nQuasi-Trefftz methods were introduced to leverage the advantages of Trefftz\nmethods for problems governed by variable coefficient PDEs, by relaxing the\nTrefftz property into a so-called quasi-Trefftz property: test and trial\nfunctions are not exact solutions but rather local approximate solutions to the\ngoverning PDE. In order to develop quassi-Trefftz methods for aero-acoustics\nproblems governed by the convected Helmholtz equation, the present work tackles\nthe question of the definition, construction and approximation properties of\nthree families of quasi-Trefftz functions: two based on generalizations on\nplane wave solutions, and one polynomial.",
    "descriptor": "",
    "authors": [
      "Lise-Marie Imbert-Gerard",
      "Guillaume Sylvand"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12993"
  },
  {
    "id": "arXiv:2201.12994",
    "title": "GSN: A Universal Graph Neural Network Inspired by Spring Network",
    "abstract": "The design of universal Graph Neural Networks (GNNs) that operate on both\nhomophilous and heterophilous graphs has received increased research attention\nin recent years. Existing heterophilous GNNs, particularly those designed in\nthe spatial domain, lack a convincing theoretical or physical motivation. In\nthis paper, we propose the Graph Spring Network (GSN), a universal GNN model\nthat works for both homophilous and heterophilous graphs, inspired by spring\nnetworks and metric learning. We show that the GSN framework interprets many\nexisting GNN models from the perspective of spring potential energy\nminimization with various metrics, which gives these models strong physical\nmotivations. We also conduct extensive experiments to demonstrate our GSN\nframework's superior performance on real-world homophilous and heterophilous\ndata sets.",
    "descriptor": "\nComments: 14 pages. Preprint\n",
    "authors": [
      "Guanyu Cui",
      "Zhewei Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.12994"
  },
  {
    "id": "arXiv:2201.12995",
    "title": "Deep Petrov-Galerkin Method for Solving Partial Differential Equations",
    "abstract": "Deep neural networks are powerful tools for approximating functions, and they\nare applied to successfully solve various problems in many fields. In this\npaper, we propose a neural network-based numerical method to solve partial\ndifferential equations. In this new framework, the method is designed on weak\nformulations, and the unknown functions are approximated by deep neural\nnetworks and test functions can be chosen by different approaches, for\ninstance, basis functions of finite element methods, neural networks, and so\non. Because the spaces of trial function and test function are different, we\nname this new approach by Deep Petrov-Galerkin Method (DPGM). The resulted\nlinear system is not necessarily to be symmetric and square, so the discretized\nproblem is solved by a least-square method. Take the Poisson problem as an\nexample, mixed DPGMs based on several mixed formulations are proposed and\nstudied as well. In addition, we apply the DPGM to solve two classical\ntime-dependent problems based on the space-time approach, that is, the unknown\nfunction is approximated by a neural network, in which temporal variable and\nspatial variables are treated equally, and the initial conditions are regarded\nas boundary conditions for the space-time domain. Finally, several numerical\nexamples are presented to show the performance of the DPGMs, and we observe\nthat this new method outperforms traditional numerical methods in several\naspects.",
    "descriptor": "",
    "authors": [
      "Yong Shang",
      "Fei Wang",
      "Jingbo Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12995"
  },
  {
    "id": "arXiv:2201.13001",
    "title": "Out-of-distribution Detection Using Kernel Density Polytopes",
    "abstract": "Any reasonable machine learning (ML) model should not only interpolate\nefficiently in between the training samples provided (in-distribution region),\nbut also approach the extrapolative or out-of-distribution (OOD) region without\nbeing overconfident. Our experiment on human subjects justifies the\naforementioned properties for human intelligence as well. Many state-of-the-art\nalgorithms have tried to fix the overconfidence problem of ML models in the OOD\nregion. However, in doing so, they have often impaired the in-distribution\nperformance of the model. Our key insight is that ML models partition the\nfeature space into polytopes and learn constant (random forests) or affine\n(ReLU networks) functions over those polytopes. This leads to the OOD\noverconfidence problem for the polytopes which lie in the training data\nboundary and extend to infinity. To resolve this issue, we propose kernel\ndensity methods that fit Gaussian kernel over the polytopes, which are learned\nusing ML models. Specifically, we introduce two variants of kernel density\npolytopes: Kernel Density Forest (KDF) and Kernel Density Network (KDN) based\non random forests and deep networks, respectively. Studies on various\nsimulation settings show that both KDF and KDN achieve uniform confidence over\nthe classes in the OOD region while maintaining good in-distribution accuracy\ncompared to that of their respective parent models.",
    "descriptor": "",
    "authors": [
      "Jayanta Dey",
      "Ashwin De Silva",
      "Will LeVine",
      "Jong Shin",
      "Haoyin Xu",
      "Ali Geisa",
      "Tiffany Chu",
      "Leyla Isik",
      "Joshua T. Vogelstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13001"
  },
  {
    "id": "arXiv:2201.13005",
    "title": "On Sub-optimality of Random Binning for Distributed Hypothesis Testing",
    "abstract": "We investigate the quantize and binning scheme, known as the\nShimokawa-Han-Amari (SHA) scheme, for the distributed hypothesis testing. We\ndevelop tools to evaluate the critical rate attainable by the SHA scheme. For a\nproduct of binary symmetric double sources, we present a sequential scheme that\nimproves upon the SHA scheme.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Shun Watanabe"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.13005"
  },
  {
    "id": "arXiv:2201.13007",
    "title": "Speed-ANN: Low-Latency and High-Accuracy Nearest Neighbor Search via  Intra-Query Parallelism",
    "abstract": "Nearest Neighbor Search (NNS) has recently drawn a rapid increase of interest\ndue to its core role in managing high-dimensional vector data in data science\nand AI applications. The interest is fueled by the success of neural embedding,\nwhere deep learning models transform unstructured data into semantically\ncorrelated feature vectors for data analysis, e.g., recommend popular items.\nAmong several categories of methods for fast NNS, similarity graph is one of\nthe most successful algorithmic trends. Several of the most popular and\ntop-performing similarity graphs, such as NSG and HNSW, at their core employ\nbest-first traversal along the underlying graph indices to search near\nneighbors. Maximizing the performance of the search is essential for many\ntasks, especially at the large-scale and high-recall regime. In this work, we\nprovide an in-depth examination of the challenges of the state-of-the-art\nsimilarity search algorithms, revealing its challenges in leveraging multi-core\nprocessors to speed up the search efficiency. We also exploit whether\nsimilarity graph search is robust to deviation from maintaining strict order by\nallowing multiple walkers to simultaneously advance the search frontier. Based\non our insights, we propose Speed-ANN, a parallel similarity search algorithm\nthat exploits hidden intra-query parallelism and memory hierarchy that allows\nsimilarity search to take advantage of multiple CPU cores to significantly\naccelerate search speed while achieving high accuracy.\nWe evaluate Speed-ANN on a wide range of datasets, ranging from million to\nbillion data points, and show its shorter query latency than NSG and HNSW,\nrespectively. Besides, with multicore support, we show that our approach offers\nfaster search latency than highly-optimized GPU implementation and provides\ngood scalability as the increase of the number of hardware resources (e.g., CPU\ncores) and graph sizes.",
    "descriptor": "",
    "authors": [
      "Zhen Peng",
      "Minjia Zhang",
      "Kai Li",
      "Ruoming Jin",
      "Bin Ren"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13007"
  },
  {
    "id": "arXiv:2201.13010",
    "title": "Architectures for Protecting Cloud Data Planes",
    "abstract": "This paper explores three approaches for protecting cloud application data\nplanes to prevent unauthorized access to the application and its data and to\nprevent unwanted data exfiltration. Through an exploration of various concrete\nsecurity architectures, we focus on (1) Cloud Security Perimeters to provide a\nboundary around data and infrastructure in the cloud that provides a line of\ndefense both to improper access to sensitive information and the exfiltration\nof that information, (2) Cloud Landing Points to provide a safe integration\npoint between parts of your cloud applications and on-premises applications to\ncommunicate through, and (3) Zero Trust security architectures that are built\non the principles of defense in depth and least-privilege access. Using these\napproaches together provides critical protection for services and applications\nas they transition from traditional on-premises network security to the Cloud\nsecurity architectures, and then to potentially Zero Trust security\narchitectures.",
    "descriptor": "",
    "authors": [
      "Grant Dasher",
      "Ines Envid",
      "Brad Calder"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.13010"
  },
  {
    "id": "arXiv:2201.13011",
    "title": "On the Power-Law Spectrum in Deep Learning: A Bridge to Protein Science",
    "abstract": "It is well-known that the Hessian matters to optimization, generalization,\nand even robustness of deep learning. Recent works empirically discovered that\nthe Hessian spectrum in deep learning has a two-component structure that\nconsists of a small number of large eigenvalues and a large number of\nnearly-zero eigenvalues. However, the theoretical mechanism behind the Hessian\nspectrum is still absent or under-explored. We are the first to theoretically\nand empirically demonstrate that the Hessian spectrums of well-trained deep\nneural networks exhibit simple power-law distributions. Our work further\nreveals how the power-law spectrum essentially matters to deep learning: (1) it\nleads to low-dimensional and robust learning space, and (2) it implicitly\npenalizes the variational free energy, which results in low-complexity\nsolutions. We further used the power-law spectral framework as a powerful tool\nto demonstrate multiple novel behaviors of deep learning. Interestingly, the\npower-law spectrum is also known to be important in protein, which indicates a\nnovel bridge between deep learning and protein science.",
    "descriptor": "\nComments: 23 Pages, 19 Figures\n",
    "authors": [
      "Zeke Xie",
      "Qian-Yuan Tang",
      "Yunfeng Cai",
      "Mingming Sun",
      "Ping Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2201.13011"
  },
  {
    "id": "arXiv:2201.13012",
    "title": "Topology-Preserving Dimensionality Reduction via Interleaving  Optimization",
    "abstract": "Dimensionality reduction techniques are powerful tools for data preprocessing\nand visualization which typically come with few guarantees concerning the\ntopological correctness of an embedding. The interleaving distance between the\npersistent homology of Vietoris-Rips filtrations can be used to identify a\nscale at which topological features such as clusters or holes in an embedding\nand original data set are in correspondence. We show how optimization seeking\nto minimize the interleaving distance can be incorporated into dimensionality\nreduction algorithms, and explicitly demonstrate its use in finding an optimal\nlinear projection. We demonstrate the utility of this framework to data\nvisualization.",
    "descriptor": "",
    "authors": [
      "Bradley J. Nelson",
      "Yuan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.13012"
  },
  {
    "id": "arXiv:2201.13013",
    "title": "Filtering In Implicit Neural Networks",
    "abstract": "Implicit neural networks (INNs) are very effective for learning data\nrepresentation. However, most INNs inevitably generate over-smoothed patches or\nobvious noisy artifacts in the results when the data has many scales of details\nor a wide range of frequencies, leading to significant performance reduction.\nAdapting the result containing both noise and over-smoothed regions usually\nsuffers from either over smoothing or noisy issues. To overcome this challenge,\nwe propose a new framework, coined FINN, that integrated a \\emph{filtering}\nmodule to the \\emph{implicit neural network} to perform data fitting while\nfiltering artifacts. The filtering module has a smoothing operator that acts on\nthe intermediate results of the network and a recovering operator that brings\ndistinct details from the input back to the regions overly smoothed. The\nproposed method significantly alleviates over smoothing or noisy issues. We\ndemonstrate the advantage of the FINN on the image regression task, considering\nboth real and synthetic images, and showcases significant improvement on both\nquantitative and qualitative results compared to state-of-the-art methods.\nMoreover, FINN yields better performance in both convergence speed and network\nstability. Source code is available at https://github.com/yixin26/FINN.",
    "descriptor": "\nComments: 8 pages, 33.4Mb\n",
    "authors": [
      "Yixin Zhuang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13013"
  },
  {
    "id": "arXiv:2201.13018",
    "title": "Monitoring Jitter in Software Defined Networks",
    "abstract": "End-to-end jitter of a flow is an important metric that indicates the Quality\nof Service a user is experiencing, particularly for real-time applications such\nas video streaming, cloud gaming, and so on. Monitoring the jitter can help\ncontrollers make routing decisions for certain flows. This paper discusses\nmethods in which we can estimate/follow important patterns in the end-to-end\njitter of a flow. Two main approaches are taken, one using a Software Defined\nNetwork controller and the other using P4 programmable data-planes. Results\nfrom the simulations of each method are discussed.",
    "descriptor": "",
    "authors": [
      "Jithin Kallukalam Sojan",
      "K. Haribabu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.13018"
  },
  {
    "id": "arXiv:2201.13019",
    "title": "On the Robustness of Quality Measures for GANs",
    "abstract": "This work evaluates the robustness of quality measures of generative models\nsuch as Inception Score (IS) and Fr\\'echet Inception Distance (FID). Analogous\nto the vulnerability of deep models against a variety of adversarial attacks,\nwe show that such metrics can also be manipulated by additive pixel\nperturbations. Our experiments indicate that one can generate a distribution of\nimages with very high scores but low perceptual quality. Conversely, one can\noptimize for small imperceptible perturbations that, when added to real world\nimages, deteriorate their scores. Furthermore, we extend our evaluation to\ngenerative models themselves, including the state of the art network\nStyleGANv2. We show the vulnerability of both the generative model and the FID\nagainst additive perturbations in the latent space. Finally, we show that the\nFID can be robustified by directly replacing the Inception model by a robustly\ntrained Inception. We validate the effectiveness of the robustified metric\nthrough extensive experiments, which show that it is more robust against\nmanipulation.",
    "descriptor": "\nComments: 20 pages, 16 figures, 5 tables\n",
    "authors": [
      "Motasem Alfarra",
      "Juan C. P\u00e9rez",
      "Anna Fr\u00fchst\u00fcck",
      "Philip H. S. Torr",
      "Peter Wonka",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13019"
  },
  {
    "id": "arXiv:2201.13020",
    "title": "SZx: an Ultra-fast Error-bounded Lossy Compressor for Scientific  Datasets",
    "abstract": "Today's scientific high performance computing (HPC) applications or advanced\ninstruments are producing vast volumes of data across a wide range of domains,\nwhich introduces a serious burden on data transfer and storage. Error-bounded\nlossy compression has been developed and widely used in scientific community,\nbecause not only can it significantly reduce the data volumes but it can also\nstrictly control the data distortion based on the use-specified error bound.\nExisting lossy compressors, however, cannot offer ultra-fast compression speed,\nwhich is highly demanded by quite a few applications or use-cases (such as\nin-memory compression and online instrument data compression). In this paper,\nwe propose a novel ultra-fast error-bounded lossy compressor, which can obtain\nfairly high compression performance on both CPU and GPU, also with reasonably\nhigh compression ratios. The key contributions are three-fold: (1) We propose a\nnovel, generic ultra-fast error-bounded lossy compression framework -- called\nUFZ, by confining our design to be composed of only super-lightweight\noperations such as bitwise and addition/subtraction operation, still keeping a\ncertain high compression ratio. (2) We implement UFZ on both CPU and GPU and\noptimize the performance according to their architectures carefully. (3) We\nperform a comprehensive evaluation with 6 real-world production-level\nscientific datasets on both CPU and GPU. Experiments show that UFZ is 2~16X as\nfast as the second-fastest existing error-bounded lossy compressor (either SZ\nor ZFP) on CPU and GPU, with respect to both compression and decompression.",
    "descriptor": "",
    "authors": [
      "Xiaodong Yu",
      "Sheng Di",
      "Kai Zhao",
      "jiannan Tian",
      "Dingwen Tao",
      "Xin Liang",
      "Franck Cappello"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13020"
  },
  {
    "id": "arXiv:2201.13025",
    "title": "Learning Robust Representation through Graph Adversarial Contrastive  Learning",
    "abstract": "Existing studies show that node representations generated by graph neural\nnetworks (GNNs) are vulnerable to adversarial attacks, such as unnoticeable\nperturbations of adjacent matrix and node features. Thus, it is requisite to\nlearn robust representations in graph neural networks. To improve the\nrobustness of graph representation learning, we propose a novel Graph\nAdversarial Contrastive Learning framework (GraphACL) by introducing\nadversarial augmentations into graph self-supervised learning. In this\nframework, we maximize the mutual information between local and global\nrepresentations of a perturbed graph and its adversarial augmentations, where\nthe adversarial graphs can be generated in either supervised or unsupervised\napproaches. Based on the Information Bottleneck Principle, we theoretically\nprove that our method could obtain a much tighter bound, thus improving the\nrobustness of graph representation learning. Empirically, we evaluate several\nmethods on a range of node classification benchmarks and the results\ndemonstrate GraphACL could achieve comparable accuracy over previous supervised\nmethods.",
    "descriptor": "",
    "authors": [
      "Jiayan Guo",
      "Shangyang Li",
      "Yue Zhao",
      "Yan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13025"
  },
  {
    "id": "arXiv:2201.13027",
    "title": "BOAT: Bilateral Local Attention Vision Transformer",
    "abstract": "Vision Transformers achieved outstanding performance in many computer vision\ntasks. Early Vision Transformers such as ViT and DeiT adopt global\nself-attention, which is computationally expensive when the number of patches\nis large. To improve efficiency, recent Vision Transformers adopt local\nself-attention mechanisms, where self-attention is computed within local\nwindows. Despite the fact that window-based local self-attention significantly\nboosts efficiency, it fails to capture the relationships between distant but\nsimilar patches in the image plane. To overcome this limitation of image-space\nlocal attention, in this paper, we further exploit the locality of patches in\nthe feature space. We group the patches into multiple clusters using their\nfeatures, and self-attention is computed within every cluster. Such\nfeature-space local attention effectively captures the connections between\npatches across different local windows but still relevant. We propose a\nBilateral lOcal Attention vision Transformer (BOAT), which integrates\nfeature-space local attention with image-space local attention. We further\nintegrate BOAT with both Swin and CSWin models, and extensive experiments on\nseveral benchmark datasets demonstrate that our BOAT-CSWin model clearly and\nconsistently outperforms existing state-of-the-art CNN models and vision\nTransformers.",
    "descriptor": "",
    "authors": [
      "Tan Yu",
      "Gangming Zhao",
      "Ping Li",
      "Yizhou Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13027"
  },
  {
    "id": "arXiv:2201.13029",
    "title": "A Flexible IAB Architecture for Beyond 5G Network",
    "abstract": "IAB is an innovative wireless backhaul solution to provide cost-efficient\ndeployment of small cells for successful 5G adoption. Besides, IAB can utilize\nthe same spectrum for access and backhaul purposes. The 3GPP standardized IAB\nin Release 16 and would incorporate a few enhancements in the upcoming\nreleases. The 3GPP IAB architecture, however, suffers from some limitations,\nsuch as it does not support mobile relays or dual-connectivity. This article\npresents a novel IAB architecture that addresses these limitations and is\ntransparent to legacy operations of the 5G system. The architecture also\nsupports multi-RAT coexistence where access and backhaul may belong to\ndifferent RATs. These factors (and many others) enable operators to capitalize\non the architecture for deploying IAB anywhere in a plug-and-play manner. We\nalso show the merits of the architecture by evaluating its capacity and\nmobility robustness compared to the 3GPP architecture. Simulation results\ncorroborate our design approach. Owing its robust design, the architecture can\ncontend for standardization in B5G system.",
    "descriptor": "\nComments: 7 pages, 5 figures, journal\n",
    "authors": [
      "Shashi Ranjan",
      "Pranav Jha",
      "Abhay Karandikar",
      "Prasanna Chaporkar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.13029"
  },
  {
    "id": "arXiv:2201.13033",
    "title": "Integrated Decision Control Approach for Cooperative Safety-Critical  Payload Transport in a Cluttered Environment",
    "abstract": "In this paper, the problem of coordinated transportation of heavy payload by\na team of UAVs in a cluttered environment is addressed. The payload is modeled\nas a rigid body and is assumed to track a pre-computed global flight trajectory\nfrom a start point to a goal point. Due to the presence of local dynamic\nobstacles in the environment, the UAVs must ensure that there is no collision\nbetween the payload and these obstacles while ensuring that the payload\noscillations are kept minimum. An Integrated Decision Controller (IDC) is\nproposed, that integrates the optimal tracking control law given by a\ncentralized Model Predictive Controller with safety-critical constraints\nprovided by the Exponential Control Barrier Functions. The entire payload-UAV\nsystem is enclosed by a safe convex hull boundary, and the IDC ensures that no\nobstacle enters this boundary. To evaluate the performance of the IDC, the\nresults for a numerical simulation as well as a high-fidelity Gazebo simulation\nare presented. An ablation study is conducted to analyze the robustness of the\nproposed IDC against practical dubieties like noisy state values, relative\nobstacle safety margin, and payload mass uncertainty. The results clearly show\nthat the IDC achieves both trajectory tracking and obstacle avoidance\nsuccessfully while restricting the payload oscillations within a safe limit.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Intelligent Transporation Systems (IEEE T - ITS)\n",
    "authors": [
      "Nishanth Rao",
      "Suresh Sundaram"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.13033"
  },
  {
    "id": "arXiv:2201.13036",
    "title": "Predicting Cancer Treatments Induced Cardiotoxicity of Breast Cancer  Patients",
    "abstract": "Cardiotoxicity induced by the breast cancer treatments (i.e., chemotherapy,\ntargeted therapy and radiation therapy) is a significant problem for breast\ncancer patients. The cardiotoxicity risk for breast cancer patients receiving\ndifferent treatments remains unclear. We developed and evaluated risk\npredictive models for cardiotoxicity in breast cancer patients using EHR data.\nThe AUC scores to predict the CHF, CAD, CM and MI are 0.846, 0.857, 0.858 and\n0.804 respectively. After adjusting for baseline differences in cardiovascular\nhealth, patients who received chemotherapy or targeted therapy appeared to have\nhigher risk of cardiotoxicity than patients who received radiation therapy. Due\nto differences in baseline cardiac health across the different breast cancer\ntreatment groups, caution is recommended in interpreting the cardiotoxic effect\nof these treatments.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Sicheng Zhou",
      "Rui Zhang",
      "Anne Blaes",
      "Chetan Shenoy",
      "Gyorgy Simon"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13036"
  },
  {
    "id": "arXiv:2201.13040",
    "title": "A high-order velocity-based discontinuous Galerkin scheme for the  shallow water equations: local conservation, entropy stability, well-balanced  property, and positivity preservation",
    "abstract": "We present a novel class of locally conservative, entropy stable and\nwell-balanced discontinuous Galerkin (DG) methods for the nonlinear shallow\nwater equation with\na non-flat bottom topography. The major novelty of our work is the use of\nvelocity field as an independent solution unknown in the DG scheme, which is\nclosely related to the entropy variable approach to entropy stable schemes for\nsystem of conservation laws proposed by Tadmor [22] back in 1986, where recall\nthat velocity is part of the entropy variable for the shallow water equations.\nDue to the use of velocity as an independent solution unknown, no specific\nnumerical quadrature rules are needed to achieve entropy stability of our\nscheme on general unstructured meshes in two dimensions.\nThe proposed DG semi-discretization is then carefully combined with the\nclassical explicit strong stability preserving Runge-Kutta (SSP-RK) time\nintegrators [13] to yield a locally conservative, well-balanced, and positivity\npreserving fully discrete scheme. Here the positivity preservation property is\nenforced with the help of a simple scaling limiter. In the fully discrete\nscheme, we re-introduce discharge as an auxiliary unknown variable. In doing\nso, standard slope limiting procedures can be applied on the conservative\nvariables (water height and discharge) without violating the local conservation\nproperty. Here we apply a characteristic-wise TVB limiter [5] on the\nconservative variables using the Fu-Shu troubled cell indicator [10] in each\ninner stage of the Runge-Kutta time stepping to suppress numerical\noscillations.",
    "descriptor": "\nComments: 25 pages, 8 figures\n",
    "authors": [
      "Guosheng Fu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13040"
  },
  {
    "id": "arXiv:2201.13047",
    "title": "Evaluating Persistent Memory Range Indexes: Part Two",
    "abstract": "Scalable persistent memory (PM) has opened up new opportunities for building\nindexes that operate and persist data directly on the memory bus, potentially\nenabling instant recovery, low latency and high throughput. When real PM\nhardware (Intel Optane DCPMM) first became available, previous work evaluated\nPM indexes proposed in the pre-Optane era. Since then, newer indexes based on\nreal PM have appeared, but it is unclear how they compare to each other and to\nprevious proposals, and what further challenges remain. This paper addresses\nthese issues by analyzing and experimentally evaluating state-of-the-art PM\nrange indexes built for real PM. We find newer designs inherited past\ntechniques with new improvements, but they do not necessarily outperform\npre-Optane era proposals. Moreover, PM indexes are often also very competitive\nor even outperform indexes tailored for DRAM, highlighting the potential of\nusing a unified design for both PM and DRAM. Functionalitywise, these indexes\nstill lack good support for variable-length keys and handling NUMA effect.\nBased on our findings, we distill new design principles and highlight future\ndirections.",
    "descriptor": "",
    "authors": [
      "Yuliang He",
      "Duo Lu",
      "Kaisong Huang",
      "Tianzheng Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.13047"
  },
  {
    "id": "arXiv:2201.13052",
    "title": "Inductive Matrix Completion: No Bad Local Minima and a Fast Algorithm",
    "abstract": "The inductive matrix completion (IMC) problem is to recover a low rank matrix\nfrom few observed entries while incorporating prior knowledge about its row and\ncolumn subspaces. In this work, we make three contributions to the IMC problem:\n(i) we prove that under suitable conditions, the IMC optimization landscape has\nno bad local minima; (ii) we derive a simple scheme with theoretical guarantees\nto estimate the rank of the unknown matrix; and (iii) we propose GNIMC, a\nsimple Gauss-Newton based method to solve the IMC problem, analyze its runtime\nand derive recovery guarantees for it. The guarantees for GNIMC are sharper in\nseveral aspects than those available for other methods, including a quadratic\nconvergence rate, fewer required observed entries and stability to errors or\ndeviations from low-rank. Empirically, given entries observed uniformly at\nrandom, GNIMC recovers the underlying matrix substantially faster than several\ncompeting methods.",
    "descriptor": "",
    "authors": [
      "Pini Zilber",
      "Boaz Nadler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.13052"
  },
  {
    "id": "arXiv:2201.13056",
    "title": "The complexity gap in the static analysis of cache accesses grows if  procedure calls are added",
    "abstract": "The static analysis of cache accesses consists in correctly predicting which\naccesses are hits or misses. While there exist good exact and approximate\nanalyses for caches implementing the least recently used (LRU) replacement\npolicy, such analyses were harder to find for other replacement policies. A\ntheoretical explanation was found: for an appropriate setting of analysis over\ncontrol-flow graphs, cache analysis is PSPACE-complete for all common\nreplacement policies (FIFO, PLRU, NMRU) except for LRU, for which it is only\nNP-complete. In this paper, we show that if procedure calls are added to the\ncontrol flow, then the gap widens: analysis remains NP-complete for LRU, but\nbecomes EXPTIME-complete for the three other policies. For this, we improve on\nearlier results on the complexity of reachability problems on Boolean programs\nwith procedure calls. In addition, for the LRU policy we derive a backtracking\nalgorithm as well as an approach for using it as a last resort after other\nanalyses have failed to conclude.",
    "descriptor": "",
    "authors": [
      "David Monniaux"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.13056"
  },
  {
    "id": "arXiv:2201.13063",
    "title": "NeuralTailor: Reconstructing Sewing Pattern Structures from 3D Point  Clouds of Garments",
    "abstract": "The fields of SocialVR, performance capture, and virtual try-on are often\nfaced with a need to faithfully reproduce real garments in the virtual world.\nOne critical task is the disentanglement of the intrinsic garment shape from\ndeformations due to fabric properties, physical forces, and contact with the\nbody. We propose to use a garment sewing pattern, a realistic and compact\ngarment descriptor, to facilitate the intrinsic garment shape estimation.\nAnother major challenge is a high diversity of shapes and designs in the\ndomain. The most common approach for Deep Learning on 3D garments is to build\nspecialized models for individual garments or garment types. We argue that\nbuilding a unified model for various garment designs has the benefit of\ngeneralization to novel garment types, hence covering a larger design domain\nthan individual models would. We introduce NeuralTailor, a novel architecture\nbased on point-level attention for set regression with variable cardinality,\nand apply it to the task of reconstructing 2D garment sewing patterns from the\n3D point could garment models. Our experiments show that NeuralTailor\nsuccessfully reconstructs sewing patterns and generalizes to garment types with\npattern topologies unseen during training.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Maria Korosteleva",
      "Sung-Hee Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.13063"
  },
  {
    "id": "arXiv:2201.13064",
    "title": "Glowing Experience or Bad Trip? A Quantitative Analysis of User Reported  Drug Experiences on Erowid.org",
    "abstract": "Erowid.org is a website dedicated to documenting information about\npsychoactive substances, with over 36,000 user-submitted drug Experience\nReports. We study the potential of these reports to provide information about\ncharacteristic experiences with drugs. First, we assess different kinds of drug\nexperiences, such as 'addiction' or 'bad trips'. We quantitatively analyze how\nsuch experiences are related to substances and user variables. Furthermore, we\nclassify positive and negative experiences as well as reported addiction using\ninformation about the consumer, substance, context and location of the drug\nexperience. While variables based only on objective characteristics yield poor\npredictive performance for subjective experiences, we find subjective user\nreports can help to identify new patterns and impact factors on drug\nexperiences. In particular, we found a positive association between addiction\nexperiences and dextromethorphan, a substance with largely unknown withdrawal\neffects. Our research can help to gain a deeper sociological understanding of\ndrug consumption and to identify relationships which may have clinical\nrelevance. Moreover, it can show how non-mainstream social media platforms can\nbe utilized to study characteristics of human behavior and how this can be done\nin an ethical way in collaboration with the platform providers.",
    "descriptor": "",
    "authors": [
      "Angelina Mooseder",
      "Momin M. Malik",
      "Hemank Lamba",
      "Earth Erowid",
      "Sylvia Thyssen",
      "Juergen Pfeffer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.13064"
  },
  {
    "id": "arXiv:2201.13065",
    "title": "Rigidity Preserving Image Transformations and Equivariance in  Perspective",
    "abstract": "We characterize the class of image plane transformations which realize rigid\ncamera motions and call these transformations `rigidity preserving'. In\nparticular, 2D translations of pinhole images are not rigidity preserving.\nHence, when using CNNs for 3D inference tasks, it can be beneficial to modify\nthe inductive bias from equivariance towards translations to equivariance\ntowards rigidity preserving transformations. We investigate how equivariance\nwith respect to rigidity preserving transformations can be approximated in\nCNNs, and test our ideas on both 6D object pose estimation and visual\nlocalization. Experimentally, we improve on several competitive baselines.",
    "descriptor": "",
    "authors": [
      "Lucas Brynte",
      "Georg B\u00f6kman",
      "Axel Flinth",
      "Fredrik Kahl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13065"
  },
  {
    "id": "arXiv:2201.13066",
    "title": "Single Object Tracking: A Survey of Methods, Datasets, and Evaluation  Metrics",
    "abstract": "Object tracking is one of the foremost assignments in computer vision that\nhas numerous commonsense applications such as traffic monitoring, robotics,\nautonomous vehicle tracking, and so on. Different researches have been tried\nlater a long time, but since of diverse challenges such as occlusion,\nillumination variations, fast motion, etc. researches in this area continues.\nIn this paper, different strategies of the following objects are inspected and\na comprehensive classification is displayed that classified the following\nstrategies into four fundamental categories of feature-based,\nsegmentation-based, estimation-based, and learning-based methods that each of\nwhich has its claim sub-categories. The most center of this paper is on\nlearning-based strategies, which are classified into three categories of\ngenerative strategies, discriminative strategies, and reinforcement learning.\nOne of the sub-categories of the discriminative show is deep learning. Since of\nhigh-performance, deep learning has as of late been exceptionally much\nconsider. Finally, the different datasets and the evaluation methods that are\nmost commonly used will be introduced.",
    "descriptor": "\nComments: 15 pages. This paper is about object tracking and review of methods in this task. The paper first published in the ICCKE2019 conference and then extended in this new paper\n",
    "authors": [
      "Zahra Soleimanitaleb",
      "Mohammad Ali Keyvanrad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13066"
  },
  {
    "id": "arXiv:2201.13072",
    "title": "Are Mutually Intelligible Languages Easier to Translate?",
    "abstract": "Two languages are considered mutually intelligible if their native speakers\ncan communicate with each other, while using their own mother tongue. How does\nthe fact that humans perceive a language pair as mutually intelligible affect\nthe ability to learn a translation model between them? We hypothesize that the\namount of data needed to train a neural ma-chine translation model is\nanti-proportional to the languages' mutual intelligibility. Experiments on the\nRomance language group reveal that there is indeed strong correlation between\nthe area under a model's learning curve and mutual intelligibility scores\nobtained by studying human speakers.",
    "descriptor": "",
    "authors": [
      "Avital Friedland",
      "Jonathan Zeltser",
      "Omer Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13072"
  },
  {
    "id": "arXiv:2201.13073",
    "title": "Learning Representations of Entities and Relations",
    "abstract": "Encoding facts as representations of entities and binary relationships\nbetween them, as learned by knowledge graph representation models, is useful\nfor various tasks, including predicting new facts, question answering, fact\nchecking and information retrieval. The focus of this thesis is on (i)\nimproving knowledge graph representation with the aim of tackling the link\nprediction task; and (ii) devising a theory on how semantics can be captured in\nthe geometry of relation representations. Most knowledge graphs are very\nincomplete and manually adding new information is costly, which drives the\ndevelopment of methods which can automatically infer missing facts. The first\ncontribution of this thesis is HypER, a convolutional model which simplifies\nand improves upon the link prediction performance of the existing convolutional\nstate-of-the-art model ConvE and can be mathematically explained in terms of\nconstrained tensor factorisation. The second contribution is TuckER, a\nrelatively straightforward linear model, which, at the time of its\nintroduction, obtained state-of-the-art link prediction performance across\nstandard datasets. The third contribution is MuRP, first multi-relational graph\nrepresentation model embedded in hyperbolic space. MuRP outperforms all\nexisting models and its Euclidean counterpart MuRE in link prediction on\nhierarchical knowledge graph relations whilst requiring far fewer dimensions.\nDespite the development of a large number of knowledge graph representation\nmodels with gradually increasing predictive performance, relatively little is\nknown of the latent structure they learn. We generalise recent theoretical\nunderstanding of how semantic relations of similarity, paraphrase and analogy\nare encoded in the geometric interactions of word embeddings to how more\ngeneral relations, as found in knowledge graphs, can be encoded in their\nrepresentations.",
    "descriptor": "",
    "authors": [
      "Ivana Bala\u017eevi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13073"
  },
  {
    "id": "arXiv:2201.13077",
    "title": "An Overview of Various Biometric Approaches: ECG One of its Trait",
    "abstract": "A Bio-metrics system is actually a pattern recognition system that utilizes\nvarious patterns like iris, retina and biological traits like fingerprint,\nvoice recognition, facial geometry and hand geometry. What makes Bio-metrics\nreally attractive is that the various security codes like passwords and ID\ncards can be interchanged, stolen or duplicated. To enhance the security and\nreliability of the system, physiological traits can be used. This paper gives\nthe overview of key bio-metric technologies and basic techniques involved and\ntheir drawbacks. Then the paper illustrates the working of ECG and the various\nopportunities for ECG are also mentioned.",
    "descriptor": "\nComments: Accepted and Published in the International Journal of Emerging Trends & Technology in Computer Science (IJETTCS). Volume 4, Issue 1, January-February 2015\n",
    "authors": [
      "Kavyashree U",
      "K N Deeksha",
      "Suma Ballal",
      "Vitina Mary Dsouza",
      "Rama Moorthy H"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13077"
  },
  {
    "id": "arXiv:2201.13078",
    "title": "Lymphoma segmentation from 3D PET-CT images using a deep evidential  network",
    "abstract": "An automatic evidential segmentation method based on Dempster-Shafer theory\nand deep learning is proposed to segment lymphomas from three-dimensional\nPositron Emission Tomography (PET) and Computed Tomography (CT) images. The\narchitecture is composed of a deep feature-extraction module and an evidential\nlayer. The feature extraction module uses an encoder-decoder framework to\nextract semantic feature vectors from 3D inputs. The evidential layer then uses\nprototypes in the feature space to compute a belief function at each voxel\nquantifying the uncertainty about the presence or absence of a lymphoma at this\nlocation. Two evidential layers are compared, based on different ways of using\ndistances to prototypes for computing mass functions. The whole model is\ntrained end-to-end by minimizing the Dice loss function. The proposed\ncombination of deep feature extraction and evidential segmentation is shown to\noutperform the baseline UNet model as well as three other state-of-the-art\nmodels on a dataset of 173 patients.",
    "descriptor": "\nComments: Preprint submitted to International Journal of Approximate Reasoning\n",
    "authors": [
      "Ling Huang",
      "Su Ruan",
      "Pierre Decazes",
      "Thierry Denoeux"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13078"
  },
  {
    "id": "arXiv:2201.13079",
    "title": "Jet noise characterization for advanced pipeline leak detection",
    "abstract": "The detection of leaks in pipeline transportation systems is a matter of\nserious concern for operators, who pursue the integrity of their assets, the\nreduction of losses and the prevention of environmental hazards. Whenever a\nhole occurs in a pressurized pipeline, the corresponding fluid leakage is\ncharacterized by a turbulent flow and a peculiar acoustic noise, whose\ncharacteristics depend also on the size of the hole itself. This study shows\nthat both the presence and the size of such a leaking hole can be successfully\ndetected, by exploiting the acoustic noise (pressure transients) generated by\nthe fluid exiting the pipe and recorded internally by hydrophones, or by\nconsidering the corresponding vibrations (e.g., acceleration signals)\npropagating along the external shell of the conduit. To this purpose, several\nexperimental campaigns of acoustic noise generation have been performed using\nmultiple calibrated nozzles on a 16 ID connection pipeline in a fuel tanks\narea. Detection and classification procedures are proposed to control the\npresence of leakages and to estimate the size of the hole, using pressure and\nvibration signals.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Riccardo Angelo Giro",
      "Giancarlo Bernasconi",
      "Giuseppe Giunta",
      "Simone Cesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.13079"
  },
  {
    "id": "arXiv:2201.13081",
    "title": "Unsupervised Anomaly Detection in 3D Brain MRI using Deep Learning with  Multi-Task Brain Age Prediction",
    "abstract": "Lesion detection in brain Magnetic Resonance Images (MRIs) remains a\nchallenging task. MRIs are typically read and interpreted by domain experts,\nwhich is a tedious and time-consuming process. Recently, unsupervised anomaly\ndetection (UAD) in brain MRI with deep learning has shown promising results to\nprovide a quick, initial assessment. So far, these methods only rely on the\nvisual appearance of healthy brain anatomy for anomaly detection. Another\nbiomarker for abnormal brain development is the deviation between the brain age\nand the chronological age, which is unexplored in combination with UAD. We\npropose deep learning for UAD in 3D brain MRI considering additional age\ninformation. We analyze the value of age information during training, as an\nadditional anomaly score, and systematically study several architecture\nconcepts. Based on our analysis, we propose a novel deep learning approach for\nUAD with multi-task age prediction. We use clinical T1-weighted MRIs of 1735\nhealthy subjects and the publicly available BraTs 2019 data set for our study.\nOur novel approach significantly improves UAD performance with an AUC of 92.60%\ncompared to an AUC-score of 84.37% using previous approaches without age\ninformation.",
    "descriptor": "\nComments: Accepted at SPIE Medical Imaging 2022\n",
    "authors": [
      "Marcel Bengs",
      "Finn Behrendt",
      "Max-Heinrich Laves",
      "Julia Kr\u00fcger",
      "Roland Opfer",
      "Alexander Schlaefer"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13081"
  },
  {
    "id": "arXiv:2201.13084",
    "title": "Crowd-powered Face Manipulation Detection: Fusing Human Examiner  Decisions",
    "abstract": "We investigate the potential of fusing human examiner decisions for the task\nof digital face manipulation detection. To this end, various decision fusion\nmethods are proposed incorporating the examiners' decision confidence,\nexperience level, and their time to take a decision. Conducted experiments are\nbased on a psychophysical evaluation of digital face image manipulation\ndetection capabilities of humans in which different manipulation techniques\nwere applied, i.e. face morphing, face swapping and retouching. The decisions\nof 223 participants were fused to simulate crowds of up to seven human\nexaminers. Experimental results reveal that (1) despite the moderate detection\nperformance achieved by single human examiners, a high accuracy can be obtained\nthrough decision fusion and (2) a weighted fusion which takes the examiners'\ndecision confidence into account yields the most competitive detection\nperformance.",
    "descriptor": "",
    "authors": [
      "Christian Rathgeb",
      "Robert Nichols",
      "Mathias Ibsen",
      "Pawel Drozdowski",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13084"
  },
  {
    "id": "arXiv:2201.13086",
    "title": "Securing Federated Sensitive Topic Classification against Poisoning  Attacks",
    "abstract": "We present a Federated Learning (FL) based solution for building a\ndistributed classifier capable of detecting URLs containing GDPR-sensitive\ncontent related to categories such as health, sexual preference, political\nbeliefs, etc. Although such a classifier addresses the limitations of previous\noffline/centralised classifiers,it is still vulnerable to poisoning attacks\nfrom malicious users that may attempt to reduce the accuracy for benign users\nby disseminating faulty model updates. To guard against this, we develop a\nrobust aggregation scheme based on subjective logic and residual-based attack\ndetection. Employing a combination of theoretical analysis, trace-driven\nsimulation, as well as experimental validation with a prototype and real users,\nwe show that our classifier can detect sensitive content with high accuracy,\nlearn new labels fast, and remain robust in view of poisoning attacks from\nmalicious users, as well as imperfect input from non-malicious ones.",
    "descriptor": "",
    "authors": [
      "Tianyue Chu",
      "Alvaro Garcia-Recuero",
      "Costas Iordanou",
      "Georgios Smaragdakis",
      "Nikolaos Laoutaris"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13086"
  },
  {
    "id": "arXiv:2201.13094",
    "title": "Metric Hypertransformers are Universal Adapted Maps",
    "abstract": "We introduce a universal class of geometric deep learning models, called\nmetric hypertransformers (MHTs), capable of approximating any adapted map\n$F:\\mathscr{X}^{\\mathbb{Z}}\\rightarrow \\mathscr{Y}^{\\mathbb{Z}}$ with\napproximable complexity, where $\\mathscr{X}\\subseteq \\mathbb{R}^d$ and\n$\\mathscr{Y}$ is any suitable metric space, and $\\mathscr{X}^{\\mathbb{Z}}$\n(resp. $\\mathscr{Y}^{\\mathbb{Z}}$) capture all discrete-time paths on\n$\\mathscr{X}$ (resp. $\\mathscr{Y}$). Suitable spaces $\\mathscr{Y}$ include\nvarious (adapted) Wasserstein spaces, all Fr\\'{e}chet spaces admitting a\nSchauder basis, and a variety of Riemannian manifolds arising from information\ngeometry. Even in the static case, where $f:\\mathscr{X}\\rightarrow \\mathscr{Y}$\nis a H\\\"{o}lder map, our results provide the first (quantitative) universal\napproximation theorem compatible with any such $\\mathscr{X}$ and $\\mathscr{Y}$.\nOur universal approximation theorems are quantitative, and they depend on the\nregularity of $F$, the choice of activation function, the metric entropy and\ndiameter of $\\mathscr{X}$, and on the regularity of the compact set of paths\nwhereon the approximation is performed. Our guiding examples originate from\nmathematical finance. Notably, the MHT models introduced here are able to\napproximate a broad range of stochastic processes' kernels, including solutions\nto SDEs, many processes with arbitrarily long memory, and functions mapping\nsequential data to sequences of forward rate curves.",
    "descriptor": "\nComments: 39 Pages + References, 8 Figures, 3 Tables\n",
    "authors": [
      "Beatrice Acciaio",
      "Anastasis Kratsios",
      "Gudmund Pammer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Metric Geometry (math.MG)",
      "Probability (math.PR)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2201.13094"
  },
  {
    "id": "arXiv:2201.13096",
    "title": "SPDY: Accurate Pruning with Speedup Guarantees",
    "abstract": "The recent focus on the efficiency of deep neural networks (DNNs) has led to\nsignificant work on model compression approaches, of which weight pruning is\none of the most popular. At the same time, there is rapidly-growing\ncomputational support for efficiently executing the unstructured-sparse models\nobtained via pruning. Yet, most existing pruning methods minimize just the\nnumber of remaining weights, i.e. the size of the model, rather than optimizing\nfor inference time. We address this gap by introducing SPDY, a new compression\nmethod which automatically determines layer-wise sparsity targets achieving a\ndesired inference speedup on a given system, while minimizing accuracy loss.\nSPDY is composed of two new techniques: the first is an efficient dynamic\nprogramming algorithm for solving the speedup-constrained layer-wise\ncompression problem assuming a set of given layer-wise sensitivity scores; the\nsecond is a local search procedure for determining accurate layer-wise\nsensitivity scores. Experiments across popular vision and language models show\nthat SPDY guarantees speedups while recovering higher accuracy relative to\nexisting strategies, both for one-shot and gradual pruning scenarios, and is\ncompatible with most existing pruning approaches. We also extend our approach\nto the recently-proposed task of pruning with very little data, where we\nachieve the best known accuracy recovery when pruning to the GPU-supported 2:4\nsparsity pattern.",
    "descriptor": "",
    "authors": [
      "Elias Frantar",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13096"
  },
  {
    "id": "arXiv:2201.13099",
    "title": "Max Flow Vitality of Edges and Vertices in Undirected Planar Graphs",
    "abstract": "We study the problem of computing the vitality with respect to max flow of\nedges and vertices in undirected planar graphs, where the vitality of an\nedge/vertex in a graph with respect to max flow between two fixed vertices\n$s,t$ is defined as the max flow decrease when the edge/vertex is removed from\nthe graph. We show that the vitality of any $k$ selected edges can be computed\nin $O(kn + n\\log\\log n)$ worst-case time, and that a $\\delta$ additive\napproximation of the vitality of all edges with capacity at most $c$ can be\ncomputed in $O(\\frac{c}{\\delta}n +n \\log \\log n)$ worst-case time, where $n$ is\nthe size of the graph. Similar results are given for the vitality of vertices.\nAll our algorithms work in $O(n)$ space.",
    "descriptor": "\nComments: 22 pages, 15 figures\n",
    "authors": [
      "Lorenzo Balzotti",
      "Paolo G. Franciosa"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.13099"
  },
  {
    "id": "arXiv:2201.13100",
    "title": "Adversarial Masking for Self-Supervised Learning",
    "abstract": "We propose ADIOS, a masked image model (MIM) framework for self-supervised\nlearning, which simultaneously learns a masking function and an image encoder\nusing an adversarial objective. The image encoder is trained to minimise the\ndistance between representations of the original and that of a masked image.\nThe masking function, conversely, aims at maximising this distance. ADIOS\nconsistently improves on state-of-the-art self-supervised learning (SSL)\nmethods on a variety of tasks and datasets -- including classification on\nImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and\niNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao\net al., 2021) -- while generating semantically meaningful masks. Unlike modern\nMIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch\ntokenisation construction of Vision Transformers, and can be implemented with\nconvolutional backbones. We further demonstrate that the masks learned by ADIOS\nare more effective in improving representation learning of SSL methods than\nmasking schemes used in popular MIM models.",
    "descriptor": "",
    "authors": [
      "Yuge Shi",
      "N. Siddharth",
      "Philip H.S. Torr",
      "Adam R. Kosiorek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13100"
  },
  {
    "id": "arXiv:2201.13102",
    "title": "GADoT: GAN-based Adversarial Training for Robust DDoS Attack Detection",
    "abstract": "Machine Learning (ML) has proven to be effective in many application domains.\nHowever, ML methods can be vulnerable to adversarial attacks, in which an\nattacker tries to fool the classification/prediction mechanism by crafting the\ninput data. In the case of ML-based Network Intrusion Detection Systems\n(NIDSs), the attacker might use their knowledge of the intrusion detection\nlogic to generate malicious traffic that remains undetected. One way to solve\nthis issue is to adopt adversarial training, in which the training set is\naugmented with adversarial traffic samples. This paper presents an adversarial\ntraining approach called GADoT, which leverages a Generative Adversarial\nNetwork (GAN) to generate adversarial DDoS samples for training. We show that a\nstate-of-the-art NIDS with high accuracy on popular datasets can experience\nmore than 60% undetected malicious flows under adversarial attacks. We then\ndemonstrate how this score drops to 1.8% or less after adversarial training\nusing GADoT.",
    "descriptor": "",
    "authors": [
      "Maged Abdelaty",
      "Sandra Scott-Hayward",
      "Roberto Doriguzzi-Corin",
      "Domenico Siracusa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13102"
  },
  {
    "id": "arXiv:2201.13103",
    "title": "Detecting False Rumors from Retweet Dynamics on Social Media",
    "abstract": "False rumors are known to have detrimental effects on society. To prevent the\nspread of false rumors, social media platforms such as Twitter must detect them\nearly. In this work, we develop a novel probabilistic mixture model that\nclassifies true vs. false rumors based on the underlying spreading process.\nSpecifically, our model is the first to formalize the self-exciting nature of\ntrue vs. false retweeting processes. This results in a novel mixture marked\nHawkes model (MMHM). Owing to this, our model obviates the need for feature\nengineering; instead, it directly models the spreading process in order to make\ninferences of whether online rumors are incorrect. Our evaluation is based on\n13,650 retweet cascades of both true. vs. false rumors from Twitter. Our model\nrecognizes false rumors with a balanced accuracy of 64.97% and an AUC of\n69.46%. It outperforms state-of-the-art baselines (both neural and feature\nengineering) by a considerable margin but while being fully interpretable. Our\nwork has direct implications for practitioners: it leverages the spreading\nprocess as an implicit quality signal and, based on it, detects false content.",
    "descriptor": "\nComments: Accepted at The Web Conference (WWW 2022)\n",
    "authors": [
      "Christof Naumzik",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2201.13103"
  },
  {
    "id": "arXiv:2201.13106",
    "title": "Computational Complexity of Segmentation",
    "abstract": "Computational feasibility is a widespread concern that guides the framing and\nmodeling of biological and artificial intelligence. The specification of\ncognitive system capacities is often shaped by unexamined intuitive assumptions\nabout the search space and complexity of a subcomputation. However, a mistaken\nintuition might make such initial conceptualizations misleading for what\nempirical questions appear relevant later on. We undertake here\ncomputational-level modeling and complexity analyses of segmentation - a widely\nhypothesized subcomputation that plays a requisite role in explanations of\ncapacities across domains - as a case study to show how crucial it is to\nformally assess these assumptions. We mathematically prove two sets of results\nregarding hardness and search space size that may run counter to intuition, and\nposition their implications with respect to existing views on the subcapacity.",
    "descriptor": "",
    "authors": [
      "Federico Adolfi",
      "Todd Wareham",
      "Iris van Rooij"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computational Complexity (cs.CC)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2201.13106"
  },
  {
    "id": "arXiv:2201.13108",
    "title": "Multi-twisted Reed-Solomon codes with small dimensional hull",
    "abstract": "In this paper, we find a necessary and sufficient condition for multi-twisted\nReed-Solomon codes to be MDS. Further, we obtain necessary conditions for the\nexistence of multi-twisted RS codes with zero and one-dimensional hulls.",
    "descriptor": "",
    "authors": [
      "Harshdeep Singh",
      "Kapish Chand Meena"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Commutative Algebra (math.AC)"
    ],
    "url": "https://arxiv.org/abs/2201.13108"
  },
  {
    "id": "arXiv:2201.13119",
    "title": "XNLP-completeness for Parameterized Problems on Graphs with a Linear  Structure",
    "abstract": "In this paper, we show several parameterized problems to be complete for the\nclass XNLP: parameterized problems that can be solved with a non-deterministic\nalgorithm that uses $f(k)\\log n$ space and $f(k)n^c$ time, with $f$ a\ncomputable function, $n$ the input size, $k$ the parameter and $c$ a constant.\nThe problems include Maximum Regular Induced Subgraph and Max Cut parameterized\nby linear clique-width, Capacitated (Red-Blue) Dominating Set parameterized by\npathwidth, Odd Cycle Transversal parameterized by a new parameter we call\nlogarithmic linear clique-width (defined as $k/\\log n$ for an $n$-vertex graph\nof linear clique-width $k$), and Bipartite Bandwidth.",
    "descriptor": "",
    "authors": [
      "Hans L. Bodlaender",
      "Carla Groenland",
      "Hugo Jacob"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2201.13119"
  },
  {
    "id": "arXiv:2201.13123",
    "title": "Lessons from the AdKDD'21 Privacy-Preserving ML Challenge",
    "abstract": "Designing data sharing mechanisms providing performance and strong privacy\nguarantees is a hot topic for the Online Advertising industry. Namely, a\nprominent proposal discussed under the Improving Web Advertising Business Group\nat W3C only allows sharing advertising signals through aggregated,\ndifferentially private reports of past displays. To study this proposal\nextensively, an open Privacy-Preserving Machine Learning Challenge took place\nat AdKDD'21, a premier workshop on Advertising Science with data provided by\nadvertising company Criteo. In this paper, we describe the challenge tasks, the\nstructure of the available datasets, report the challenge results, and enable\nits full reproducibility. A key finding is that learning models on large,\naggregated data in the presence of a small set of unaggregated data points can\nbe surprisingly efficient and cheap. We also run additional experiments to\nobserve the sensitivity of winning methods to different parameters such as\nprivacy budget or quantity of available privileged side information. We\nconclude that the industry needs either alternate designs for private data\nsharing or a breakthrough in learning with aggregated data only to keep ad\nrelevance at a reasonable level.",
    "descriptor": "",
    "authors": [
      "Eustache Diemert",
      "Romain Fabre",
      "Alexandre Gilotte",
      "Fei Jia",
      "Basile Leparmentier",
      "J\u00e9r\u00e9mie Mary",
      "Zhonghua Qu",
      "Ugo Tanielian",
      "Hui Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13123"
  },
  {
    "id": "arXiv:2201.13125",
    "title": "Corpus for Automatic Structuring of Legal Documents",
    "abstract": "In populous countries, pending legal cases have been growing exponentially.\nThere is a need for developing techniques for processing and organizing legal\ndocuments. In this paper, we introduce a new corpus for structuring legal\ndocuments. In particular, we introduce a corpus of legal judgment documents in\nEnglish that are segmented into topical and coherent parts. Each of these parts\nis annotated with a label coming from a list of pre-defined Rhetorical Roles.\nWe develop baseline models for automatically predicting rhetorical roles in a\nlegal document based on the annotated corpus. Further, we show the application\nof rhetorical roles to improve performance on the tasks of summarization and\nlegal judgment prediction. We release the corpus and baseline model code along\nwith the paper.",
    "descriptor": "\nComments: 10 Pages (8 page main paper + 2 page references)\n",
    "authors": [
      "Prathamesh Kalamkar",
      "Aman Tiwari",
      "Astha Agarwal",
      "Saurabh Karn",
      "Smita Gupta",
      "Vivek Raghavan",
      "Ashutosh Modi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13125"
  },
  {
    "id": "arXiv:2201.13127",
    "title": "Unified Perspective on Probability Divergence via Maximum Likelihood  Density Ratio Estimation: Bridging KL-Divergence and Integral Probability  Metrics",
    "abstract": "This paper provides a unified perspective for the Kullback-Leibler\n(KL)-divergence and the integral probability metrics (IPMs) from the\nperspective of maximum likelihood density-ratio estimation (DRE). Both the\nKL-divergence and the IPMs are widely used in various fields in applications\nsuch as generative modeling. However, a unified understanding of these concepts\nhas still been unexplored. In this paper, we show that the KL-divergence and\nthe IPMs can be represented as maximal likelihoods differing only by sampling\nschemes, and use this result to derive a unified form of the IPMs and a relaxed\nestimation method. To develop the estimation problem, we construct an\nunconstrained maximum likelihood estimator to perform DRE with a stratified\nsampling scheme. We further propose a novel class of probability divergences,\ncalled the Density Ratio Metrics (DRMs), that interpolates the KL-divergence\nand the IPMs. In addition to these findings, we also introduce some\napplications of the DRMs, such as DRE and generative adversarial networks. In\nexperiments, we validate the effectiveness of our proposed methods.",
    "descriptor": "",
    "authors": [
      "Masahiro Kato",
      "Masaaki Imaizumi",
      "Kentaro Minami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13127"
  },
  {
    "id": "arXiv:2201.13128",
    "title": "Deletion Robust Submodular Maximization over Matroids",
    "abstract": "Maximizing a monotone submodular function is a fundamental task in machine\nlearning. In this paper, we study the deletion robust version of the problem\nunder the classic matroids constraint. Here the goal is to extract a small size\nsummary of the dataset that contains a high value independent set even after an\nadversary deleted some elements. We present constant-factor approximation\nalgorithms, whose space complexity depends on the rank $k$ of the matroid and\nthe number $d$ of deleted elements. In the centralized setting we present a\n$(3.582+O(\\varepsilon))$-approximation algorithm with summary size $O(k +\n\\frac{d \\log k}{\\varepsilon^2})$. In the streaming setting we provide a\n$(5.582+O(\\varepsilon))$-approximation algorithm with summary size and memory\n$O(k + \\frac{d \\log k}{\\varepsilon^2})$. We complement our theoretical results\nwith an in-depth experimental analysis showing the effectiveness of our\nalgorithms on real-world datasets.",
    "descriptor": "",
    "authors": [
      "Paul D\u00fctting",
      "Federico Fusco",
      "Silvio Lattanzi",
      "Ashkan Norouzi-Fard",
      "Morteza Zadimoghaddam"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13128"
  },
  {
    "id": "arXiv:2201.13132",
    "title": "On the identifiability of mixtures of ranking models",
    "abstract": "Mixtures of ranking models are standard tools for ranking problems. However,\neven the fundamental question of parameter identifiability is not fully\nunderstood: the identifiability of a mixture model with two Bradley-Terry-Luce\n(BTL) components has remained open. In this work, we show that popular mixtures\nof ranking models with two components (Plackett-Luce, multinomial logistic\nmodel with slates of size 3, or BTL) are generically identifiable, i.e., the\nground-truth parameters can be identified except when they are from a\npathological subset of measure zero. We provide a framework for verifying the\nnumber of solutions in a general family of polynomial systems using algebraic\ngeometry, and apply it to these mixtures of ranking models. The framework can\nbe applied more broadly to other learning models and may be of independent\ninterest.",
    "descriptor": "\nComments: 43 pages, 2 tables. Comments are very welcome\n",
    "authors": [
      "Xiaomin Zhang",
      "Xucheng Zhang",
      "Po-Ling Loh",
      "Yingyu Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Geometry (math.AG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13132"
  },
  {
    "id": "arXiv:2201.13140",
    "title": "Polynomial kernels for edge modification problems towards block and  strictly chordal graphs",
    "abstract": "We consider edge modification problems towards block and strictly chordal\ngraphs, where one is given an undirected graph $G = (V,E)$ and an integer $k\n\\in \\mathbb{N}$ and seeks to edit (add or delete) at most $k$ edges from $G$ to\nobtain a block graph or a strictly chordal graph. The completion and deletion\nvariants of these problems are defined similarly by only allowing edge\nadditions for the former and only edge deletions for the latter. Block graphs\nare a well-studied class of graphs and admit several characterizations, e.g.\nthey are diamond-free chordal graphs. Strictly chordal graphs, also referred to\nas block duplicate graphs, are a natural generalization of block graphs where\none can add true twins of cut-vertices. Strictly chordal graphs are exactly\ndart and gem-free chordal graphs. We prove the NP-completeness for most\nvariants of these problems and provide $O(k^2)$ vertex-kernels for Block Graph\nEdition and Block Graph Deletion, $O(k^3)$ vertex-kernels for Strictly Chordal\nCompletion and Strictly Chordal Deletion and a $O(k^4)$ vertex-kernel for\nStrictly Chordal Edition.",
    "descriptor": "",
    "authors": [
      "Ma\u00ebl Dumas",
      "Anthony Perez",
      "Mathis Rocton",
      "Ioan Todinca"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2201.13140"
  },
  {
    "id": "arXiv:2201.13143",
    "title": "CoTV: Cooperative Control for Traffic Light Signals and Connected  Autonomous Vehicles using Deep Reinforcement Learning",
    "abstract": "The target of reducing travel time only is insufficient to support the\ndevelopment of future smart transportation systems. To align with the United\nNations Sustainable Development Goals (UN-SDG), a further reduction of fuel and\nemissions, improvements of traffic safety, and the ease of infrastructure\ndeployment and maintenance should also be considered. Different from existing\nwork focusing on the optimization of the control in either traffic light signal\n(to improve the intersection throughput), or vehicle speed (to stabilize the\ntraffic), this paper presents a multi-agent deep reinforcement learning (DRL)\nsystem called CoTV, which Cooperatively controls both Traffic light signals and\nconnected autonomous Vehicles (CAV). Therefore, our CoTV can well balance the\nachievement of the reduction of travel time, fuel, and emission. In the\nmeantime, CoTV can also be easy to deploy by cooperating with only one CAV that\nis the nearest to the traffic light controller on each incoming road. This\nenables more efficient coordination between traffic light controllers and CAV,\nthus leading to the convergence of training CoTV under the large-scale\nmulti-agent scenario that is traditionally difficult to converge. We give the\ndetailed system design of CoTV, and demonstrate its effectiveness in a\nsimulation study using SUMO under various grid maps and realistic urban\nscenarios with mixed-autonomy traffic.",
    "descriptor": "",
    "authors": [
      "Jiaying Guo",
      "Long Cheng",
      "Shen Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13143"
  },
  {
    "id": "arXiv:2201.13144",
    "title": "partitura: A Python Package for Handling Symbolic Musical Data",
    "abstract": "This demo paper introduces partitura, a Python package for handling symbolic\nmusical information. The principal aim of this package is to handle richly\nstructured musical information as conveyed by modern staff music notation. It\nprovides a much wider range of possibilities to deal with music than the more\nreductive (but very common) piano roll-oriented approach inspired by the MIDI\nstandard. The package is an open source project and is available on GitHub.",
    "descriptor": "\nComments: This preprint is a slightly updated and reformatted version of the work presented at the Late Breaking/Demo Session of the 20th International Society for Music Information Retrieval Conference (ISMIR 2019), Delft, The Netherlands\n",
    "authors": [
      "Maarten Grachten",
      "Carlos Cancino-Chac\u00f3n",
      "Thassilo Gadermaier"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.13144"
  },
  {
    "id": "arXiv:2201.13153",
    "title": "A new idea for RSA backdoors",
    "abstract": "This article proposes a new method to inject backdoors in RSA and other\ncryptographic primitives based on the Integer Factorization problem for\nbalanced semi-primes. The method relies on mathematical congruences among the\nfactors of the semi-primes modulo a large prime number, which acts as a\n\"designer key\" or \"escrow key\". In particular, two different backdoors are\nproposed, one targeting a single semi-prime and the other one a pair of\nsemi-primes. The article also describes the results of tests performed on a\nSageMath implementation of the backdoors.",
    "descriptor": "\nComments: 19 pages, 5 figures\n",
    "authors": [
      "Marco Cesati"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13153"
  },
  {
    "id": "arXiv:2201.13156",
    "title": "Low-Rank Updates of Matrix Square Roots",
    "abstract": "Models in which the covariance matrix has the structure of a sparse matrix\nplus a low rank perturbation are ubiquitous in machine learning applications.\nIt is often desirable for learning algorithms to take advantage of such\nstructures, avoiding costly matrix computations that often require cubic time\nand quadratic storage. This is often accomplished by performing operations that\nmaintain such structures, e.g. matrix inversion via the\nSherman-Morrison-Woodbury formula. In this paper we consider the matrix square\nroot and inverse square root operations. Given a low rank perturbation to a\nmatrix, we argue that a low-rank approximate correction to the (inverse) square\nroot exists. We do so by establishing a geometric decay bound on the true\ncorrection's eigenvalues. We then proceed to frame the correction has the\nsolution of an algebraic Ricatti equation, and discuss how a low-rank solution\nto that equation can be computed. We analyze the approximation error incurred\nwhen approximately solving the algebraic Ricatti equation, providing spectral\nand Frobenius norm forward and backward error bounds. Finally, we describe\nseveral applications of our algorithms, and demonstrate their utility in\nnumerical experiments.",
    "descriptor": "",
    "authors": [
      "Shany Shumeli",
      "Petros Drineas",
      "Haim Avron"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13156"
  },
  {
    "id": "arXiv:2201.13157",
    "title": "Equivariant neural networks for recovery of Hadamard matrices",
    "abstract": "We propose a message passing neural network architecture designed to be\nequivariant to column and row permutations of a matrix. We illustrate its\nadvantages over traditional architectures like multi-layer perceptrons (MLPs),\nconvolutional neural networks (CNNs) and even Transformers, on the\ncombinatorial optimization task of recovering a set of deleted entries of a\nHadamard matrix. We argue that this is a powerful application of the principles\nof Geometric Deep Learning to fundamental mathematics, and a potential stepping\nstone toward more insights on the Hadamard conjecture using Machine Learning\ntechniques.",
    "descriptor": "",
    "authors": [
      "Augusto Peres",
      "Eduardo Dias",
      "Lu\u00eds Sarmento",
      "Hugo Penedones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2201.13157"
  },
  {
    "id": "arXiv:2201.13158",
    "title": "FEN-Hedonic Games with Distance-Based Preferences",
    "abstract": "Hedonic games formalize coalition formation scenarios where players evaluate\nan outcome based on the coalition they are contained in. Due to a large number\nof possible coalitions, compact representations of these games are crucial. We\ncomplement known compact representation models by a distance-based approach:\nPlayers' preferences are encoded in a bipolar manner by ordinal preferences\nover a small set of known neighbouring players, coalitions are represented by\nadequate preference orders from a player's perspective, and preferences over\ncoalitions are extended based on a directed form of Hausdorff-Kendall-tau\ndistance between individual preferences and coalitions. We show that this model\nsatisfies desirable axiomatic properties and has reasonable computational\ncomplexity in terms of selected individual-based stability notions.",
    "descriptor": "",
    "authors": [
      "Anja Rey",
      "Lisa Rey"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.13158"
  },
  {
    "id": "arXiv:2201.13160",
    "title": "AnyCall: Fast and Flexible System-Call Aggregation",
    "abstract": "Operating systems rely on system calls to allow the controlled communication\nof isolated processes with the kernel and other processes. Every system call\nincludes a processor mode switch from the unprivileged user mode to the\nprivileged kernel mode. Although processor mode switches are the essential\nisolation mechanism to guarantee the system's integrity, they induce direct and\nindirect performance costs as they invalidate parts of the processor state. In\nrecent years, high-performance networks and storage hardware has made the\nuser/kernel transition overhead the bottleneck for IO-heavy applications. To\nmake matters worse, security vulnerabilities in modern processors (e.g.,\nMeltdown) have prompted kernel mitigations that further increase the transition\noverhead. To decouple system calls from user/kernel transitions we propose\nAnyCall, which uses an in-kernel compiler to execute safety-checked user\nbytecode in kernel mode. This allows for very fast system calls interleaved\nwith error checking and processing logic using only a single user/kernel\ntransition. We have implemented AnyCall based on the Linux kernel's eBPF\nsubsystem. Our evaluation demonstrates that system call bursts are up to 55\ntimes faster using AnyCall and that real-world applications can be sped up by\n24% even if only a minimal part of their code is run by AnyCall.",
    "descriptor": "",
    "authors": [
      "Luis Gerhorst",
      "Benedict Herzog",
      "Stefan Reif",
      "Wolfgang Schr\u00f6der-Preikschat",
      "Timo H\u00f6nig"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Operating Systems (cs.OS)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.13160"
  },
  {
    "id": "arXiv:2201.13162",
    "title": "Nonholonomic Newmark method",
    "abstract": "Using the nonholonomic exponential map, we generalize the well-known family\nof Newmark methods for nonholonomic systems. We give numerical examples\nincluding a test problem where the structure of reversible integrability\nresponsible for good energy behaviour as described in [16] is lost. We observe\nthat the composition of two Newmark methods is able to produce good energy\nbehaviour on this test problem.",
    "descriptor": "\nComments: 20 pages, 5 figures\n",
    "authors": [
      "Alexandre Anahory Sim\u00f5es",
      "Sebasti\u00e1n J. Ferraro",
      "Juan Carlos Marrero",
      "David Mart\u00edn de Diego"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2201.13162"
  },
  {
    "id": "arXiv:2201.13164",
    "title": "Imperceptible and Multi-channel Backdoor Attack against Deep Neural  Networks",
    "abstract": "Recent researches demonstrate that Deep Neural Networks (DNN) models are\nvulnerable to backdoor attacks. The backdoored DNN model will behave\nmaliciously when images containing backdoor triggers arrive. To date, existing\nbackdoor attacks are single-trigger and single-target attacks, and the triggers\nof most existing backdoor attacks are obvious thus are easy to be detected or\nnoticed. In this paper, we propose a novel imperceptible and multi-channel\nbackdoor attack against Deep Neural Networks by exploiting Discrete Cosine\nTransform (DCT) steganography. Based on the proposed backdoor attack method, we\nimplement two variants of backdoor attacks, i.e., N-to-N backdoor attack and\nN-to-One backdoor attack. Specifically, for a colored image, we utilize DCT\nsteganography to construct the trigger on different channels of the image. As a\nresult, the trigger is stealthy and natural. Based on the proposed method, we\nimplement multi-target and multi-trigger backdoor attacks. Experimental results\ndemonstrate that the average attack success rate of the N-to-N backdoor attack\nis 93.95% on CIFAR-10 dataset and 91.55% on TinyImageNet dataset, respectively.\nThe average attack success rate of N-to-One attack is 90.22% and 89.53% on\nCIFAR-10 and TinyImageNet datasets, respectively. Meanwhile, the proposed\nbackdoor attack does not affect the classification accuracy of the DNN model.\nMoreover, the proposed attack is demonstrated to be robust to the\nstate-of-the-art backdoor defense (Neural Cleanse).",
    "descriptor": "",
    "authors": [
      "Mingfu Xue",
      "Shifeng Ni",
      "Yinghao Wu",
      "Yushu Zhang",
      "Jian Wang",
      "Weiqiang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13164"
  },
  {
    "id": "arXiv:2201.13167",
    "title": "Decoupled, linear, unconditionally energy stable and charge-conservative  finite element method for a inductionless magnetohydrodynamic phase-field  model",
    "abstract": "In this paper, we consider the numerical approximation for a diffuse\ninterface model of the two-phase incompressible inductionless\nmagnetohydrodynamics problem. This model consists of Cahn-Hilliard equations,\nNavier-Stokes equations and Poisson equation. We propose a linear and decoupled\nfinite element method to solve this highly nonlinear and multi-physics system.\nFor the time variable, the discretization is a combination of first-order Euler\nsemi-implicit scheme, several first-order stabilization terms and\nimplicit-explicit treatments for coupling terms. For the space variables, we\nadopt the finite element discretization, especially, we approximate the current\ndensity and electric potential by inf-sup stable face-volume mixed finite\nelement pairs. With these techniques, the scheme only involves a sequence of\ndecoupled linear equations to solve at each time step. We show that the scheme\nis provably mass-conservative, charge-conservative and unconditionally energy\nstable. Numerical experiments are performed to illustrate the features,\naccuracy and efficiency of the proposed scheme.",
    "descriptor": "",
    "authors": [
      "Xiaorong Wang",
      "Xiaodi Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13167"
  },
  {
    "id": "arXiv:2201.13168",
    "title": "SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation",
    "abstract": "Neural implicit fields are quickly emerging as an attractive representation\nfor learning based techniques. However, adopting them for 3D shape modeling and\nediting is challenging. We introduce a method for $\\mathbf{E}$diting\n$\\mathbf{I}$mplicit $\\mathbf{S}$hapes $\\mathbf{T}$hrough $\\mathbf{P}$art\n$\\mathbf{A}$ware $\\mathbf{G}$enera$\\mathbf{T}$ion, permuted in short as\nSPAGHETTI. Our architecture allows for manipulation of implicit shapes by means\nof transforming, interpolating and combining shape segments together, without\nrequiring explicit part supervision. SPAGHETTI disentangles shape part\nrepresentation into extrinsic and intrinsic geometric information. This\ncharacteristic enables a generative framework with part-level control. The\nmodeling capabilities of SPAGHETTI are demonstrated using an interactive\ngraphical interface, where users can directly edit neural implicit shapes.",
    "descriptor": "",
    "authors": [
      "Amir Hertz",
      "Or Perel",
      "Raja Giryes",
      "Olga Sorkine-Hornung",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13168"
  },
  {
    "id": "arXiv:2201.13169",
    "title": "Causal Explanations and XAI",
    "abstract": "Although standard Machine Learning models are optimized for making\npredictions about observations, more and more they are used for making\npredictions about the results of actions. An important goal of Explainable\nArtificial Intelligence (XAI) is to compensate for this mismatch by offering\nexplanations about the predictions of an ML-model which ensure that they are\nreliably action-guiding. As action-guiding explanations are causal\nexplanations, the literature on this topic is starting to embrace insights from\nthe literature on causal models. Here I take a step further down this path by\nformally defining the causal notions of sufficient explanations and\ncounterfactual explanations. I show how these notions relate to (and improve\nupon) existing work, and motivate their adequacy by illustrating how different\nexplanations are action-guiding under different circumstances. Moreover, this\nwork is the first to offer a formal definition of actual causation that is\nfounded entirely in action-guiding explanations. Although the definitions are\nmotivated by a focus on XAI, the analysis of causal explanation and actual\ncausation applies in general. I also touch upon the significance of this work\nfor fairness in AI by showing how actual causation can be used to improve the\nidea of path-specific counterfactual fairness.",
    "descriptor": "\nComments: To appear in Causal Learning and Reasoning 2022\n",
    "authors": [
      "Sander Beckers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13169"
  },
  {
    "id": "arXiv:2201.13170",
    "title": "Cooperative Online Learning in Stochastic and Adversarial MDPs",
    "abstract": "We study cooperative online learning in stochastic and adversarial Markov\ndecision process (MDP). That is, in each episode, $m$ agents interact with an\nMDP simultaneously and share information in order to minimize their individual\nregret. We consider environments with two types of randomness: \\emph{fresh} --\nwhere each agent's trajectory is sampled i.i.d, and \\emph{non-fresh} -- where\nthe realization is shared by all agents (but each agent's trajectory is also\naffected by its own actions). More precisely, with non-fresh randomness the\nrealization of every cost and transition is fixed at the start of each episode,\nand agents that take the same action in the same state at the same time observe\nthe same cost and next state. We thoroughly analyze all relevant settings,\nhighlight the challenges and differences between the models, and prove\nnearly-matching regret lower and upper bounds. To our knowledge, we are the\nfirst to consider cooperative reinforcement learning (RL) with either non-fresh\nrandomness or in adversarial MDPs.",
    "descriptor": "",
    "authors": [
      "Tal Lancewicki",
      "Aviv Rosenberg",
      "Yishay Mansour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13170"
  },
  {
    "id": "arXiv:2201.13172",
    "title": "Near-Optimal Regret for Adversarial MDP with Delayed Bandit Feedback",
    "abstract": "The standard assumption in reinforcement learning (RL) is that agents observe\nfeedback for their actions immediately. However, in practice feedback is often\nobserved in delay. This paper studies online learning in episodic Markov\ndecision process (MDP) with unknown transitions, adversarially changing costs,\nand unrestricted delayed bandit feedback. More precisely, the feedback for the\nagent in episode $k$ is revealed only in the end of episode $k + d^k$, where\nthe delay $d^k$ can be changing over episodes and chosen by an oblivious\nadversary. We present the first algorithms that achieve near-optimal $\\sqrt{K +\nD}$ regret, where $K$ is the number of episodes and $D = \\sum_{k=1}^K d^k$ is\nthe total delay, significantly improving upon the best known regret bound of\n$(K + D)^{2/3}$.",
    "descriptor": "",
    "authors": [
      "Tiancheng Jin",
      "Tal Lancewicki",
      "Haipeng Luo",
      "Yishay Mansour",
      "Aviv Rosenberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13172"
  },
  {
    "id": "arXiv:2201.13176",
    "title": "Leela Zero Score: a Study of a Score-based AlphaGo Zero",
    "abstract": "AlphaGo, AlphaGo Zero, and all of their derivatives can play with superhuman\nstrength because they are able to predict the win-lose outcome with great\naccuracy. However, Go as a game is decided by a final score difference, and in\nfinal positions AlphaGo plays suboptimal moves: this is not surprising, since\nAlphaGo is completely unaware of the final score difference, all winning final\npositions being equivalent from the winrate perspective. This can be an issue,\nfor instance when trying to learn the \"best\" move or to play with an initial\nhandicap. Moreover, there is the theoretical quest of the \"perfect game\", that\nis, the minimax solution. Thus, a natural question arises: is it possible to\ntrain a successful Reinforcement Learning agent to predict score differences\ninstead of winrates? No empirical or theoretical evidence can be found in the\nliterature to support the folklore statement that \"this does not work\". In this\npaper we present Leela Zero Score, a software designed to support or disprove\nthe \"does not work\" statement. Leela Zero Score is designed on the open-source\nsolution known as Leela Zero, and is trained on a 9x9 board to predict score\ndifferences instead of winrates. We find that the training produces a rational\nplayer, and we analyze its style against a strong amateur human player, to find\nthat it is prone to some mistakes when the outcome is close. We compare its\nstrength against SAI, an AlphaGo Zero-like software working on the 9x9 board,\nand find that the training of Leela Zero Score has reached a premature\nconvergence to a player weaker than SAI.",
    "descriptor": "",
    "authors": [
      "Luca Pasqualini",
      "Maurizio Parton",
      "Francesco Morandin",
      "Gianluca Amato",
      "Rosa Gini",
      "Carlo Metta"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13176"
  },
  {
    "id": "arXiv:2201.13178",
    "title": "Few-Shot Backdoor Attacks on Visual Object Tracking",
    "abstract": "Visual object tracking (VOT) has been widely adopted in mission-critical\napplications, such as autonomous driving and intelligent surveillance systems.\nIn current practice, third-party resources such as datasets, backbone networks,\nand training platforms are frequently used to train high-performance VOT\nmodels. Whilst these resources bring certain convenience, they also introduce\nnew security threats into VOT models. In this paper, we reveal such a threat\nwhere an adversary can easily implant hidden backdoors into VOT models by\ntempering with the training process. Specifically, we propose a simple yet\neffective few-shot backdoor attack (FSBA) that optimizes two losses\nalternately: 1) a \\emph{feature loss} defined in the hidden feature space, and\n2) the standard \\emph{tracking loss}. We show that, once the backdoor is\nembedded into the target model by our FSBA, it can trick the model to lose\ntrack of specific objects even when the \\emph{trigger} only appears in one or a\nfew frames. We examine our attack in both digital and physical-world settings\nand show that it can significantly degrade the performance of state-of-the-art\nVOT trackers. We also show that our attack is resistant to potential defenses,\nhighlighting the vulnerability of VOT models to potential backdoor attacks.",
    "descriptor": "\nComments: This work is accepted by the ICLR 2022. The first two authors contributed equally to this work. 21 pages\n",
    "authors": [
      "Yiming Li",
      "Haoxiang Zhong",
      "Xingjun Ma",
      "Yong Jiang",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13178"
  },
  {
    "id": "arXiv:2201.13180",
    "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
    "abstract": "Training with backpropagation (BP) in standard deep learning consists of two\nmain steps: a forward pass that maps a data point to its prediction, and a\nbackward pass that propagates the error of this prediction back through the\nnetwork. This process is highly effective when the goal is to minimize a\nspecific objective function. However, it does not allow training on networks\nwith cyclic or backward connections. This is an obstacle to reaching brain-like\ncapabilities, as the highly complex heterarchical structure of the neural\nconnections in the neocortex are potentially fundamental for its effectiveness.\nIn this paper, we show how predictive coding (PC), a theory of information\nprocessing in the cortex, can be used to perform inference and learning on\narbitrary graph topologies. We experimentally show how this formulation, called\nPC graphs, can be used to flexibly perform different tasks with the same\nnetwork by simply stimulating specific neurons, and investigate how the\ntopology of the graph influences the final performance. We conclude by\ncomparing against simple baselines trained~with~BP.",
    "descriptor": "\nComments: 15 pages, 11 figures\n",
    "authors": [
      "Tommaso Salvatori",
      "Luca Pinchetti",
      "Beren Millidge",
      "Yuhang Song",
      "Rafal Bogacz",
      "Thomas Lukasiewicz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13180"
  },
  {
    "id": "arXiv:2201.13182",
    "title": "Learning Super-Features for Image Retrieval",
    "abstract": "Methods that combine local and global features have recently shown excellent\nperformance on multiple challenging deep image retrieval benchmarks, but their\nuse of local features raises at least two issues. First, these local features\nsimply boil down to the localized map activations of a neural network, and\nhence can be extremely redundant. Second, they are typically trained with a\nglobal loss that only acts on top of an aggregation of local features; by\ncontrast, testing is based on local feature matching, which creates a\ndiscrepancy between training and testing. In this paper, we propose a novel\narchitecture for deep image retrieval, based solely on mid-level features that\nwe call Super-features. These Super-features are constructed by an iterative\nattention module and constitute an ordered set in which each element focuses on\na localized and discriminant image pattern. For training, they require only\nimage labels. A contrastive loss operates directly at the level of\nSuper-features and focuses on those that match across images. A second\ncomplementary loss encourages diversity. Experiments on common landmark\nretrieval benchmarks validate that Super-features substantially outperform\nstate-of-the-art methods when using the same number of features, and only\nrequire a significantly smaller memory footprint to match their performance.\nCode and models are available at: https://github.com/naver/FIRe.",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Diane Larlus",
      "Yannis Kalantidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13182"
  },
  {
    "id": "arXiv:2201.13185",
    "title": "A note on numerical singular values of compositions with non-compact  operators",
    "abstract": "Linear non-compact operators are difficult to study because they do not exist\nin the finite dimensional world. Recently, Math\\'{e} and Hofmann studied the\nsingular values of the compact composition of the non-compact Hausdorff moment\noperator and the compact integral operator and found credible arguments, but no\nstrict proof, that those singular values fall only slightly faster than those\nof the integral operator alone. However, the fact that numerically the singular\nvalues of the combined operator fall exponentially fast was not mentioned. In\nthis note, we provide the missing numerical results and provide an explanation\nwhy the two seemingly contradicting results may both be true.",
    "descriptor": "",
    "authors": [
      "Daniel Gerth"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13185"
  },
  {
    "id": "arXiv:2201.13189",
    "title": "Resultant Tools for Parametric Polynomial Systems with Application to  Population Models",
    "abstract": "We are concerned with the problem of decomposing the parameter space of a\nparametric system of polynomial equations, and possibly some polynomial\ninequality constraints, with respect to the number of real solutions that the\nsystem attains. Previous studies apply a two step approach to this problem,\nwhere first the discriminant variety of the system is computed via a Groebner\nBasis (GB), and then a Cylindrical Algebraic Decomposition (CAD) of this is\nproduced to give the desired computation.\nHowever, even on some reasonably small applied examples this process is too\nexpensive, with computation of the discriminant variety alone infeasible. In\nthis paper we develop new approaches to build the discriminant variety using\nresultant methods (the Dixon resultant and a new method using iterated\nunivariate resultants). This reduces the complexity compared to GB and allows\nfor a previous infeasible example to be tackled.\nWe demonstrate the benefit by giving a symbolic solution to a problem from\npopulation dynamics - the analysis of the steady states of three connected\npopulations which exhibit Allee effects - which previously could only be\ntackled numerically.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "AmirHosein Sadeghimanesh",
      "Matthew England"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2201.13189"
  },
  {
    "id": "arXiv:2201.13190",
    "title": "Differentiable Neural Radiosity",
    "abstract": "We introduce Differentiable Neural Radiosity, a novel method of representing\nthe solution of the differential rendering equation using a neural network.\nInspired by neural radiosity techniques, we minimize the norm of the residual\nof the differential rendering equation to directly optimize our network. The\nnetwork is capable of outputting continuous, view-independent gradients of the\nradiance field with respect to scene parameters, taking into account\ndifferential global illumination effects while keeping memory and time\ncomplexity constant in path length. To solve inverse rendering problems, we use\na pre-trained instance of our network that represents the differential radiance\nfield with respect to a limited number of scene parameters. In our experiments,\nwe leverage this to achieve faster and more accurate convergence compared to\nother techniques such as Automatic Differentiation, Radiative Backpropagation,\nand Path Replay Backpropagation.",
    "descriptor": "",
    "authors": [
      "Saeed Hadadan",
      "Matthias Zwicker"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13190"
  },
  {
    "id": "arXiv:2201.13194",
    "title": "Compactness Score: A Fast Filter Method for Unsupervised Feature  Selection",
    "abstract": "For feature engineering, feature selection seems to be an important research\ncontent in which is anticipated to select \"excellent\" features from candidate\nones. Different functions can be realized through feature selection, such as\ndimensionality reduction, model effect improvement, and model performance\nimprovement. Along with the flourish of the information age, huge amounts of\nhigh-dimensional data are generated day by day, while we need to spare great\nefforts and time to label such data. Therefore, various algorithms are proposed\nto address such data, among which unsupervised feature selection has attracted\ntremendous interests. In many classification tasks, researchers found that data\nseem to be usually close to each other if they are from the same class; thus,\nlocal compactness is of great importance for the evaluation of a feature. In\nthis manuscript, we propose a fast unsupervised feature selection method, named\nas, Compactness Score (CSUFS), to select desired features. To demonstrate the\nefficiency and accuracy, several data sets are chosen with intensive\nexperiments being performed. Later, the effectiveness and superiority of our\nmethod are revealed through addressing clustering tasks. Here, the performance\nis indicated by several well-known evaluation metrics, while the efficiency is\nreflected by the corresponding running time. As revealed by the simulation\nresults, our proposed algorithm seems to be more accurate and efficient\ncompared with existing algorithms.",
    "descriptor": "",
    "authors": [
      "Peican Zhu",
      "Xin Hou",
      "Zhen Wang",
      "Feiping Nie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13194"
  },
  {
    "id": "arXiv:2201.13195",
    "title": "Memory-Efficient Backpropagation through Large Linear Layers",
    "abstract": "In modern neural networks like Transformers, linear layers require\nsignificant memory to store activations during backward pass. This study\nproposes a memory reduction approach to perform backpropagation through linear\nlayers. Since the gradients of linear layers are computed by matrix\nmultiplications, we consider methods for randomized matrix multiplications and\ndemonstrate that they require less memory with a moderate decrease of the test\naccuracy. Also, we investigate the variance of the gradient estimate induced by\nthe randomized matrix multiplication. We compare this variance with the\nvariance coming from gradient estimation based on the batch of samples. We\ndemonstrate the benefits of the proposed method on the fine-tuning of the\npre-trained RoBERTa model on GLUE tasks.",
    "descriptor": "\nComments: Submitted\n",
    "authors": [
      "Daniel Bershatsky",
      "Aleksandr Mikhalev",
      "Alexandr Katrutsa",
      "Julia Gusak",
      "Daniil Merkulov",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13195"
  },
  {
    "id": "arXiv:2201.13202",
    "title": "ODSearch: A Fast and Resource Efficient On-device Information Retrieval  for Mobile and Wearable Devices",
    "abstract": "Mobile and wearable technologies have promised significant changes to the\nhealthcare industry. Although cutting-edge communication and cloud-based\ntechnologies have allowed for these upgrades, their implementation and\npopularization in low-income countries have been challenging. We propose\nODSearch, an On-device Search framework equipped with a natural language\ninterface for mobile and wearable devices. To implement search, ODSearch\nemploys compression and Bloom filter, it provides near real-time search query\nresponses without network dependency. Our experiments were conducted on a\nmobile phone and smartwatch. We compared ODSearch with current state-of-the-art\nsearch mechanisms, and it outperformed them on average by 55 times in execution\ntime, 26 times in energy usage, and 2.3% in memory utilization.",
    "descriptor": "\nComments: 23 pages, 11 figures, 10 tables\n",
    "authors": [
      "Yi Rong",
      "Reza Rawassizadeh"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.13202"
  },
  {
    "id": "arXiv:2201.13215",
    "title": "The Curvature Effect in Gaussian Random Fields",
    "abstract": "Random field models are mathematical structures used in the study of\nstochastic complex systems. In this paper, we compute the shape operator of\nGaussian random field manifolds using the first and second fundamental forms\n(Fisher information matrices). Using Markov Chain Monte Carlo techniques, we\nsimulate the dynamics of these random fields and compute the Gaussian curvature\nof the parametric space, analyzing how this quantity changes along phase\ntransitions. During the simulation, we have observed an unexpected phenomenon\nthat we called the \\emph{curvature effect}, which indicates that a highly\nasymmetric geometric deformation happens in the underlying parametric space\nwhen there are significant increase/decrease in the system's entropy. This\nasymmetric pattern relates to the emergence of hysteresis, leading to an\nintrinsic arrow of time along the dynamics.",
    "descriptor": "\nComments: 5 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2109.09204\n",
    "authors": [
      "Alexandre L. M. Levada"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.13215"
  },
  {
    "id": "arXiv:2201.13217",
    "title": "Fast Distributed k-Means with a Small Number of Rounds",
    "abstract": "We propose a new algorithm for k-means clustering in a distributed setting,\nwhere the data is distributed across many machines, and a coordinator\ncommunicates with these machines to calculate the output clustering. Our\nalgorithm guarantees a cost approximation factor and a number of communication\nrounds that depend only on the computational capacity of the coordinator.\nMoreover, the algorithm includes a built-in stopping mechanism, which allows it\nto use fewer communication rounds whenever possible. We show both theoretically\nand empirically that in many natural cases, indeed 1-4 rounds suffice. In\ncomparison with the popular k-means|| algorithm, our approach allows exploiting\na larger coordinator capacity to obtain a smaller number of rounds. Our\nexperiments show that the k-means cost obtained by the proposed algorithm is\nusually better than the cost obtained by k-means||, even when the latter is\nallowed a larger number of rounds. Moreover, the machine running time in our\napproach is considerably smaller than that of k-means||. Code for running the\nalgorithm and experiments is available at\nhttps://github.com/selotape/distributed_k_means.",
    "descriptor": "",
    "authors": [
      "Tom Hess",
      "Ron Visbord",
      "Sivan Sabato"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13217"
  },
  {
    "id": "arXiv:2201.13221",
    "title": "Risk-based Design of Regular Plane Frames Subject to Damage by Abnormal  Events: a Conceptual Study",
    "abstract": "Constructed facilities should be robust with respect to the loss of\nload-bearing elements due to abnormal events. Yet, strengthening structures to\nwithstand such damage has a significant impact on construction costs.\nStrengthening costs should be justified by the threat and should result in\nsmaller expected costs of progressive collapse. In regular frame structures,\nbeams and columns compete for the strengthening budget. In this paper, we\npresent a risk-based formulation to address the optimal design of regular plane\nframes under element loss conditions. We address the threat probabilities for\nwhich strengthening has better cost-benefit than usual design, for different\nframe configurations, and study the impacts of strengthening extent and cost.\nThe risk-based optimization reveals optimum points of compromise between\ncompeting failure modes: local bending of beams, local crushing of columns, and\nglobal pancake collapse, for frames of different aspect ratios. The conceptual\nstudy is based on a simple analytical model for progressive collapse, but it\nprovides relevant insight for the design and strengthening of real structures.",
    "descriptor": "\nComments: 36 pages 12 figures\n",
    "authors": [
      "Andre T. Beck",
      "Lucas da Rosa Ribeiro",
      "Marcos Valdebenito",
      "Hector Jensen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.13221"
  },
  {
    "id": "arXiv:2201.13222",
    "title": "Perspective on Code Submission and Automated Evaluation Platforms for  University Teaching",
    "abstract": "We present a perspective on platforms for code submission and automated\nevaluation in the context of university teaching. Due to the COVID-19 pandemic,\nsuch platforms have become an essential asset for remote courses and a\nreasonable standard for structured code submission concerning increasing\nnumbers of students in computer sciences. Utilizing automated code evaluation\ntechniques exhibits notable positive impacts for both students and teachers in\nterms of quality and scalability. We identified relevant technical and\nnon-technical requirements for such platforms in terms of practical\napplicability and secure code submission environments. Furthermore, a survey\namong students was conducted to obtain empirical data on general perception. We\nconclude that submission and automated evaluation involves continuous\nmaintenance yet lowers the required workload for teachers and provides better\nevaluation transparency for students.",
    "descriptor": "\nComments: Source code: this https URL Manuscript accepted for publication in the following Conference - Jorunal: MedInfo 2021, Virtual Conference, October 2-4, 2021 - IOS Press\n",
    "authors": [
      "Florian Auer",
      "Johann Frei",
      "Dominik M\u00fcller",
      "Frank Kramer"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.13222"
  },
  {
    "id": "arXiv:2201.13223",
    "title": "A Complete Helmholtz Decomposition on Multiply Connected Subdivision  Surfaces and Its Application to Integral Equations",
    "abstract": "The analysis of electromagnetic scattering in the isogeometric analysis (IGA)\nframework based on Loop subdivision has long been restricted to\nsimply-connected geometries. The inability to analyze multiply-connected\nobjects is a glaring omission. In this paper, we address this challenge. IGA\nprovides seamless integration between the geometry and analysis by using the\nsame basis set to represent both. In particular, IGA methods using subdivision\nbasis sets exploit the fact that the basis functions used for surface\ndescription are smooth (with continuous second derivatives) almost everywhere.\nOn simply-connected surfaces, this permits the definition of basis sets that\nare divergence-free and curl-free. What is missing from this suite is a basis\nset that is both divergence-free and curl-free, a necessary ingredient for a\ncomplete Helmholtz decomposition of currents on multiply-connected structures.\nIn this paper, we effect this missing ingredient numerically using random\npolynomial vector fields. We show that this basis set is analytically\ndivergence-free and curl-free. Furthermore, we show that these basis recovers\ncurl-free, divergence-free, and curl-free and divergence-free fields. Finally,\nwe use this basis set to discretize a well-conditioned integral equation for\nanalyzing perfectly conducting objects and demonstrate excellent agreement with\nother methods.",
    "descriptor": "",
    "authors": [
      "A. M. A. Alsnayyan",
      "L. Kempel",
      "B. Shanker"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.13223"
  },
  {
    "id": "arXiv:2201.13224",
    "title": "Evaluating a Methodology for Increasing AI Transparency: A Case Study",
    "abstract": "In reaction to growing concerns about the potential harms of artificial\nintelligence (AI), societies have begun to demand more transparency about how\nAI models and systems are created and used. To address these concerns, several\nefforts have proposed documentation templates containing questions to be\nanswered by model developers. These templates provide a useful starting point,\nbut no single template can cover the needs of diverse documentation consumers.\nIt is possible in principle, however, to create a repeatable methodology to\ngenerate truly useful documentation. Richards et al. [25] proposed such a\nmethodology for identifying specific documentation needs and creating templates\nto address those needs. Although this is a promising proposal, it has not been\nevaluated.\nThis paper presents the first evaluation of this user-centered methodology in\npractice, reporting on the experiences of a team in the domain of AI for\nhealthcare that adopted it to increase transparency for several AI models. The\nmethodology was found to be usable by developers not trained in user-centered\ntechniques, guiding them to creating a documentation template that addressed\nthe specific needs of their consumers while still being reusable across\ndifferent models and use cases. Analysis of the benefits and costs of this\nmethodology are reviewed and suggestions for further improvement in both the\nmethodology and supporting tools are summarized.",
    "descriptor": "",
    "authors": [
      "David Piorkowski",
      "John Richards",
      "Michael Hind"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13224"
  },
  {
    "id": "arXiv:2201.13226",
    "title": "Online Assessment Misconduct Detection using Internet Protocol and  Behavioural Classification",
    "abstract": "With the recent prevalence of remote education, academic assessments are\noften conducted online, leading to further concerns surrounding assessment\nmisconducts. This paper investigates the potentials of online assessment\nmisconduct (e-cheating) and proposes practical countermeasures against them.\nThe mechanism for detecting the practices of online cheating is presented in\nthe form of an e-cheating intelligent agent, comprising of an internet protocol\n(IP) detector and a behavioural monitor. The IP detector is an auxiliary\ndetector which assigns randomised and unique assessment sets as an early\nprocedure to reduce potential misconducts. The behavioural monitor scans for\nirregularities in assessment responses from the candidates, further reducing\nany misconduct attempts. This is highlighted through the proposal of the\nDenseLSTM using a deep learning approach. Additionally, a new PT Behavioural\nDatabase is presented and made publicly available. Experiments conducted on\nthis dataset confirm the effectiveness of the DenseLSTM, resulting in\nclassification accuracies of up to 90.7%.",
    "descriptor": "",
    "authors": [
      "Leslie Ching Ow Tiong",
      "HeeJeong Jasmine Lee",
      "Kai Li Lim"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13226"
  },
  {
    "id": "arXiv:2201.13227",
    "title": "A Proof Procedure For Separation Logic With Inductive Definitions and  Theory Reasoning",
    "abstract": "A proof procedure, in the spirit of the sequent calculus, is proposed to\ncheck the validity of entailments between Separation Logic formulas combining\ninductively defined predicates denoted structures of bounded tree width and\ntheory reasoning. The calculus is sound and complete, in the sense that a\nsequent is valid iff it admits a (possibly infinite) proof tree. We show that\nthe procedure terminates in the two following cases: (i) When the inductive\nrules that define the predicates occurring on the left-hand side of the\nentailment terminate, in which case the proof tree is always finite. (ii) When\nthe theory is empty, in which case every valid sequent admits a rational proof\ntree, where the total number of pairwise distinct sequents occurring in the\nproof tree is doubly exponential w.r.t.\\ the size of the end-sequent. We also\nshow that the validity problem is undecidable for a wide class of theories,\neven with a very low expressive power.",
    "descriptor": "",
    "authors": [
      "Mnacho Echenim",
      "Nicolas Peltier"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2201.13227"
  },
  {
    "id": "arXiv:2201.13229",
    "title": "Network-level Safety Metrics for Overall Traffic Safety Assessment: A  Case Study",
    "abstract": "Driving safety analysis has recently witnessed unprecedented results due to\nadvances in computation frameworks, connected vehicle technology, new\ngeneration sensors, and artificial intelligence (AI). Particularly, the recent\nadvances performance of deep learning (DL) methods realized higher levels of\nsafety for autonomous vehicles and empowered volume imagery processing for\ndriving safety analysis. An important application of DL methods is extracting\ndriving safety metrics from traffic imagery. However, the majority of current\nmethods use safety metrics for micro-scale analysis of individual crash\nincidents or near-crash events, which does not provide insightful guidelines\nfor the overall network-level traffic management. On the other hand,\nlarge-scale safety assessment efforts mainly emphasize spatial and temporal\ndistributions of crashes, while not always revealing the safety violations that\ncause crashes. To bridge these two perspectives, we define a new set of\nnetwork-level safety metrics for the overall safety assessment of traffic flow\nby processing imagery taken by roadside infrastructure sensors. An integrative\nanalysis of the safety metrics and crash data reveals the insightful temporal\nand spatial correlation between the representative network-level safety metrics\nand the crash frequency. The analysis is performed using two video cameras in\nthe state of Arizona along with a 5-year crash report obtained from the Arizona\nDepartment of Transportation. The results confirm that network-level safety\nmetrics can be used by the traffic management teams to equip traffic monitoring\nsystems with advanced AI-based risk analysis, and timely traffic flow control\ndecisions.",
    "descriptor": "",
    "authors": [
      "Xiwen Chen",
      "Hao Wang",
      "Abolfazl Razi",
      "Brendan Russo",
      "Jason Pacheco",
      "John Roberts",
      "Jeffrey Wishart",
      "Larry Head"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.13229"
  },
  {
    "id": "arXiv:2201.13230",
    "title": "POTATO: exPlainable infOrmation exTrAcTion framewOrk",
    "abstract": "We present POTATO, a task- and languageindependent framework for\nhuman-in-the-loop (HITL) learning of rule-based text classifiers using\ngraph-based features. POTATO handles any type of directed graph and supports\nparsing text into Abstract Meaning Representations (AMR), Universal\nDependencies (UD), and 4lang semantic graphs. A streamlit-based user interface\nallows users to build rule systems from graph patterns, provides real-time\nevaluation based on ground truth data, and suggests rules by ranking graph\nfeatures using interpretable machine learning models. Users can also provide\npatterns over graphs using regular expressions, and POTATO can recommend\nrefinements of such rules. POTATO is applied in projects across domains and\nlanguages, including classification tasks on German legal text and English\nsocial media data. All components of our system are written in Python, can be\ninstalled via pip, and are released under an MIT License on GitHub.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "\u00c1d\u00e1m Kov\u00e1cs",
      "Kinga G\u00e9mes",
      "Eszter Ikl\u00f3di",
      "G\u00e1bor Recski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.13230"
  },
  {
    "id": "arXiv:2201.13232",
    "title": "EXSeQETIC: Expert System to Support the Implementation of eQETIC Model",
    "abstract": "The digital educational solutions are increasingly used demanding high\nquality functionalities. In this sense, standards and models are made available\nby governments, associations, and researchers being most used in quality\ncontrol and assessment sessions. The eQETIC model was built according to the\napproach of continuous process improvement favoring the quality management for\ndevelopment and maintenance of digital educational solutions. This article\npresents two expert systems to support the implementation of eQETIC model and\ndemonstrates that such systems are able to support users during the model\nimplementation. Developed according to two types of shells (SINTA/UFC and\ne2gLite/eXpertise2go), the systems were used by a professional who develops\nthese type of solutions and showed positive results regarding the support\noffered by them in implementing the rules proposed by eQETIC model.",
    "descriptor": "\nComments: in Portuguese\n",
    "authors": [
      "Rogerio Rossi",
      "Pollyana Notagiarcomo"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13232"
  },
  {
    "id": "arXiv:2201.13233",
    "title": "\"It's A Blessing and A Curse\": Unpacking Creators' Practices with  Non-Fungible Tokens (NFTs) and Their Communities",
    "abstract": "NFTs (Non-Fungible Tokens) are blockchain-based cryptographic tokens to\nrepresent ownership of unique content such as images, videos, or 3D objects.\nDespite NFTs' increasing popularity and skyrocketing trading prices, little is\nknown about people's perceptions of and experiences with NFTs. In this work, we\nfocus on NFT creators and present results of an exploratory qualitative study\nin which we interviewed 15 NFT creators from nine different countries. Our\nparticipants had nuanced feelings about NFTs and their communities. We found\nthat most of our participants were enthusiastic about the underlying\ntechnologies and how they empower individuals to express their creativity and\npursue new business models of content creation. Our participants also gave\nkudos to the NFT communities that have supported them to learn, collaborate,\nand grow in their NFT endeavors. However, these positivities were juxtaposed by\ntheir accounts of the many challenges that they encountered and thorny issues\nthat the NFT ecosystem is grappling with around ownership of digital content,\nlow-quality NFTs, scams, possible money laundering, and regulations. We discuss\nhow the built-in properties (e.g., decentralization) of blockchains and NFTs\nmight have contributed to some of these issues. We present design implications\non how to improve the NFT ecosystem (e.g., making NFTs even more accessible to\nnewcomers and the broader population).",
    "descriptor": "",
    "authors": [
      "Tanusree Sharma",
      "Zhixuan Zhou",
      "Yun Huang",
      "Yang Wang"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.13233"
  },
  {
    "id": "arXiv:2201.13234",
    "title": "A simple and fast algorithm for computing discrete Voronoi, Johnson-Mehl  or Laguerre diagrams of points",
    "abstract": "This article presents an algorithm to compute digital images of Voronoi,\nJohnson-Mehl or Laguerre diagrams of a set of punctual sites, in a domain of a\nEuclidean space of any dimension. The principle of the algorithm is, in a first\nstep, to investigate the voxels in balls centred around the sites, and, in a\nsecond step, to process the voxels remaining outside the balls. The optimal\nchoice of ball radii can be determined analytically or numerically, which\nallows a performance of the algorithm in $O(N_v \\ln N_s$), where $N_v$ is the\ntotal number of voxels of the domain and $N_s$ the number of sites of the\ntessellation. Periodic and non-periodic boundary conditions are considered. A\nmajor advantage of the algorithm is its simplicity which makes it very easy to\nimplement. This makes the algorithm suitable for creating high resolution\nimages of microstructures containing a large number of cells, in particular\nwhen calculations using FFT-based homogenisation methods are then to be applied\nto the simulated materials.",
    "descriptor": "",
    "authors": [
      "H Moulinec"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2201.13234"
  },
  {
    "id": "arXiv:2201.13237",
    "title": "CDNNs: The coupled deep neural networks for coupling of the Stokes and  Darcy-Forchheimer problems",
    "abstract": "In this article, we present an efficient deep learning method called coupled\ndeep neural networks (CDNNs) for coupled physical problems. Our method compiles\nthe interface conditions of the coupled PDEs into the networks properly and can\nbe served as an efficient alternative to the complex coupled problems. To\nimpose energy conservation constraints, the CDNNs utilize simple fully\nconnected layers and a custom loss function to perform the model training\nprocess as well as the physical property of the exact solution. The approach\ncan be beneficial for the following reasons: Firstly, we sampled randomly and\nonly input spatial coordinates without being restricted by the nature of\nsamples. Secondly, our method is meshfree which makes it more efficient than\nthe traditional methods. Finally, our method is parallel and can solve multiple\nvariables independently at the same time. We give the theory to guarantee the\nconvergence of the loss function and the convergence of the neural networks to\nthe exact solution. Some numerical experiments are performed and discussed to\ndemonstrate the performance of the proposed method.",
    "descriptor": "\nComments: 31 pages, 14 figures, 4 tables. arXiv admin note: text overlap with arXiv:1906.06652 by other authors\n",
    "authors": [
      "Jing Yue",
      "Jian Li",
      "Wen Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13237"
  },
  {
    "id": "arXiv:2201.13240",
    "title": "Grid-Free Monte Carlo for PDEs with Spatially Varying Coefficients",
    "abstract": "Partial differential equations (PDEs) with spatially-varying coefficients\narise throughout science and engineering, modeling rich heterogeneous material\nbehavior. Yet conventional PDE solvers struggle with the immense complexity\nfound in nature, since they must first discretize the problem -- leading to\nspatial aliasing, and global meshing/sampling that is costly and error-prone.\nWe describe a method that approximates neither the domain geometry, the problem\ndata, nor the solution space, providing the exact solution (in expectation)\neven for problems with extremely detailed geometry and intricate coefficients.\nOur main contribution is to extend the walk on spheres (WoS) algorithm from\nconstant- to variable-coefficient problems, by drawing on techniques from\nvolumetric rendering. In particular, an approach inspired by null-scattering\nyields unbiased Monte Carlo estimators for a large class of 2nd-order elliptic\nPDEs, which share many attractive features with Monte Carlo rendering: no\nmeshing, trivial parallelism, and the ability to evaluate the solution at any\npoint without solving a global system of equations.",
    "descriptor": "",
    "authors": [
      "Rohan Sawhney",
      "Dario Seyb",
      "Wojciech Jarosz",
      "Keenan Crane"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13240"
  },
  {
    "id": "arXiv:2201.13242",
    "title": "Correcting diacritics and typos with ByT5 transformer model",
    "abstract": "Due to the fast pace of life and online communications, the prevalence of\nEnglish and the QWERTY keyboard, people tend to forgo using diacritics, make\ntypographical errors (typos) when typing. Restoring diacritics and correcting\nspelling is important for proper language use and disambiguation of texts for\nboth humans and downstream algorithms. However, both of these problems are\ntypically addressed separately, i.e., state-of-the-art diacritics restoration\nmethods do not tolerate other typos. In this work, we tackle both problems at\nonce by employing newly-developed ByT5 byte-level transformer models. Our\nsimultaneous diacritics restoration and typos correction approach demonstrates\nnear state-of-the-art performance in 13 languages, reaching >96% of the\nalpha-word accuracy. We also perform diacritics restoration alone on 12\nbenchmark datasets with the additional one for the Lithuanian language. The\nexperimental investigation proves that our approach is able to achieve\ncomparable results (>98%) to previously reported despite being trained on fewer\ndata. Our approach is also able to restore diacritics in words not seen during\ntraining with >76% accuracy. We also show the accuracies to further improve\nwith longer training. All this shows a great real-world application potential\nof our suggested methods to more data, languages, and error classes.",
    "descriptor": "",
    "authors": [
      "Lukas Stankevi\u010dius",
      "Mantas Luko\u0161evi\u010dius",
      "Jurgita Kapo\u010di\u016bt\u0117-Dzikien\u0117",
      "Monika Briedien\u0117",
      "Tomas Krilavi\u010dius"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13242"
  },
  {
    "id": "arXiv:2201.13243",
    "title": "A holistic approach for rapid development of IIoT systems",
    "abstract": "While lots of research has been conducted on the architecture of Industrial\nInternet of Things (IIoT) systems, concepts of structuring their development\nprocesses are missing. Therefore, we propose a holistic approach supporting\norganizations in rapid development of IIoT systems. It includes the structuring\nof the development process into multiple projects sharing project conventions.\nUtilizing a single configurable build script for all projects, our goal is to\nmake the integration of code from various projects into broader IIoT systems\neasy and the systems decomposable with minimal effort.",
    "descriptor": "",
    "authors": [
      "Konstantin Merker",
      "Tilman Klaeger",
      "Lukas Oehm"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13243"
  },
  {
    "id": "arXiv:2201.13248",
    "title": "SafeAPT: Safe Simulation-to-Real Robot Learning using Diverse Policies  Learned in Simulation",
    "abstract": "The framework of Simulation-to-real learning, i.e, learning policies in\nsimulation and transferring those policies to the real world is one of the most\npromising approaches towards data-efficient learning in robotics. However, due\nto the inevitable reality gap between the simulation and the real world, a\npolicy learned in the simulation may not always generate a safe behaviour on\nthe real robot. As a result, during adaptation of the policy in the real world,\nthe robot may damage itself or cause harm to its surroundings. In this work, we\nintroduce a novel learning algorithm called SafeAPT that leverages a diverse\nrepertoire of policies evolved in the simulation and transfers the most\npromising safe policy to the real robot through episodic interaction. To\nachieve this, SafeAPT iteratively learns a probabilistic reward model as well\nas a safety model using real-world observations combined with simulated\nexperiences as priors. Then, it performs Bayesian optimization on the\nrepertoire with the reward model while maintaining the specified safety\nconstraint using the safety model. SafeAPT allows a robot to adapt to a wide\nrange of goals safely with the same repertoire of policies evolved in the\nsimulation. We compare SafeAPT with several baselines, both in simulated and\nreal robotic experiments and show that SafeAPT finds high-performance policies\nwithin a few minutes in the real world while minimizing safety violations\nduring the interactions.",
    "descriptor": "\nComments: Under review. For video of the paper this http URL\n",
    "authors": [
      "Rituraj Kaushik",
      "Karol Arndt",
      "Ville Kyrki"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.13248"
  },
  {
    "id": "arXiv:2201.13254",
    "title": "Learning Hamiltonians of constrained mechanical systems",
    "abstract": "Recently, there has been an increasing interest in modelling and computation\nof physical systems with neural networks. Hamiltonian systems are an elegant\nand compact formalism in classical mechanics, where the dynamics is fully\ndetermined by one scalar function, the Hamiltonian. The solution trajectories\nare often constrained to evolve on a submanifold of a linear vector space. In\nthis work, we propose new approaches for the accurate approximation of the\nHamiltonian function of constrained mechanical systems given sample data\ninformation of their solutions. We focus on the importance of the preservation\nof the constraints in the learning strategy by using both explicit Lie group\nintegrators and other classical schemes.",
    "descriptor": "\nComments: 18 pages, 6 figures, Conference proceeding for NUMDIFF-16\n",
    "authors": [
      "Elena Celledoni",
      "Andrea Leone",
      "Davide Murari",
      "Brynjulf Owren"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.13254"
  },
  {
    "id": "arXiv:2201.13259",
    "title": "Trajectory Balance: Improved Credit Assignment in GFlowNets",
    "abstract": "Generative Flow Networks (GFlowNets) are a method for learning a stochastic\npolicy for generating compositional objects, such as graphs or strings, from a\ngiven unnormalized density by sequences of actions, where many possible action\nsequences may lead to the same object. Prior temporal difference-like learning\nobjectives for training GFlowNets, such as flow matching and detailed balance,\nare prone to inefficient credit propagation across action sequences,\nparticularly in the case of long sequences. We propose a new learning objective\nfor GFlowNets, trajectory balance, as a more efficient alternative to\npreviously used objectives. We prove that any global minimizer of the\ntrajectory balance objective can define a policy that samples exactly from the\ntarget distribution. In experiments on four distinct domains, we empirically\ndemonstrate the benefits of the trajectory balance objective for GFlowNet\nconvergence, diversity of generated samples, and robustness to long action\nsequences and large action spaces.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Nikolay Malkin",
      "Moksh Jain",
      "Emmanuel Bengio",
      "Chen Sun",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13259"
  },
  {
    "id": "arXiv:2201.13261",
    "title": "A note on the use of equidistant contours for presenting scientific data",
    "abstract": "The passionate plea for the use of scientific colour maps misses some aspects\nin the visual presentation of scientific data. While a linear colour map based\non scientific human colour perception is useful for the presentation of some\nimages, like the three examples given of the topography of the earth, an apple\nand a passport photograph, scientific data are not presented. In this note it\nwill be shown that there is more in scientific oceanographic data as they are\npresented in forms varying from historic equidistant contours, via a linear\nblack-(grey)-white b&w map, a linear colour map and a nonlinear colour map.\nFrom an objective perspective, equidistant contouring is the best means for\npresenting scientific information in a relatively unbiased way. Nonlinear\ncolour maps may add information to that by highlighting certain aspects also by\nvarying the colour range if needed. Such information is not available from\nlinear colour maps. Finally, the aesthetic aspect of visual data presentation\nis discussed.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Hans van Haren"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2201.13261"
  },
  {
    "id": "arXiv:2201.13266",
    "title": "Aggregation and Transformation of Vector-Valued Messages in the Shuffle  Model of Differential Privacy",
    "abstract": "Advances in communications, storage and computational technology allow\nsignificant quantities of data to be collected and processed by distributed\ndevices. Combining the information from these endpoints can realize significant\nsocietal benefit but presents challenges in protecting the privacy of\nindividuals, especially important in an increasingly regulated world.\nDifferential privacy (DP) is a technique that provides a rigorous and provable\nprivacy guarantee for aggregation and release. The Shuffle Model for DP has\nbeen introduced to overcome challenges regarding the accuracy of local-DP\nalgorithms and the privacy risks of central-DP. In this work we introduce a new\nprotocol for vector aggregation in the context of the Shuffle Model. The aim of\nthis paper is twofold; first, we provide a single message protocol for the\nsummation of real vectors in the Shuffle Model, using advanced composition\nresults. Secondly, we provide an improvement on the bound on the error achieved\nthrough using this protocol through the implementation of a Discrete Fourier\nTransform, thereby minimizing the initial error at the expense of the loss in\naccuracy through the transformation itself. This work will further the\nexploration of more sophisticated structures such as matrices and\nhigher-dimensional tensors in this context, both of which are reliant on the\nfunctionality of the vector case.",
    "descriptor": "\nComments: 16 pages, 5 figures, in: IEEE Transactions on Information Forensics and Security (TIFS), 2022. arXiv admin note: substantial text overlap with arXiv:2112.05464\n",
    "authors": [
      "Mary Scott",
      "Graham Cormode",
      "Carsten Maple"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13266"
  },
  {
    "id": "arXiv:2201.13267",
    "title": "Micro-level Reserving for General Insurance Claims using a Long  Short-Term Memory Network",
    "abstract": "Detailed information about individual claims are completely ignored when\ninsurance claims data are aggregated and structured in development triangles\nfor loss reserving. In the hope of extracting predictive power from the\nindividual claims characteristics, researchers have recently proposed to move\naway from these macro-level methods in favor of micro-level loss reserving\napproaches. We introduce a discrete-time individual reserving framework\nincorporating granular information in a deep learning approach named Long\nShort-Term Memory (LSTM) neural network. At each time period, the network has\ntwo tasks: first, classifying whether there is a payment or a recovery, and\nsecond, predicting the corresponding non-zero amount, if any. We illustrate the\nestimation procedure on a simulated and a real general insurance dataset. We\ncompare our approach with the chain-ladder aggregate method using the\npredictive outstanding loss estimates and their actual values. Based on a\ngeneralized Pareto model for excess payments over a threshold, we adjust the\nLSTM reserve prediction to account for extreme payments.",
    "descriptor": "",
    "authors": [
      "Ihsan Chaoubi",
      "Camille Besse",
      "H\u00e9l\u00e8ne Cossette",
      "Marie-Pier C\u00f4t\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2201.13267"
  },
  {
    "id": "arXiv:2201.13268",
    "title": "PreDefense: Defending Underserved AI Students and Researchers from  Predatory Conferences",
    "abstract": "Mentorship in the AI community is crucial to maintaining and increasing\ndiversity, especially with respect to fostering the academic growth of\nunderserved students. While the research process itself is important, there is\nnot sufficient emphasis on the submission, presentation, and publication\nprocess, which is a cause for concern given the meteoric rise of predatory\nscientific conferences, which are based on profit only and have little to no\npeer review. These conferences are a direct threat to integrity in science by\npromoting work with little to no scientific merit. However, they also threaten\ndiversity in the AI community by marginalizing underrepresented groups away\nfrom legitimate conferences due to convenience and targeting mechanisms like\ne-mail invitations. Due to the importance of conference presentation in AI\nresearch, this very specific problem must be addressed through direct\nmentorship. In this work, we propose PreDefense, a mentorship program that\nseeks to guide underrepresented students through the scientific conference and\nworkshop process, with an emphasis on choosing legitimate venues that align\nwith the specific work that the students are focused in and preparing students\nof all backgrounds for future successful, integrous AI research careers.",
    "descriptor": "\nComments: 6 pages, published in Proceedings of Machine Learning Research (PMLR)\n",
    "authors": [
      "Thomas Y. Chen"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.13268"
  },
  {
    "id": "arXiv:2201.13271",
    "title": "StRegA: Unsupervised Anomaly Detection in Brain MRIs using a Compact  Context-encoding Variational Autoencoder",
    "abstract": "Expert interpretation of anatomical images of the human brain is the central\npart of neuro-radiology. Several machine learning-based techniques have been\nproposed to assist in the analysis process. However, the ML models typically\nneed to be trained to perform a specific task, e.g., brain tumour segmentation\nor classification. Not only do the corresponding training data require\nlaborious manual annotations, but a wide variety of abnormalities can be\npresent in a human brain MRI - even more than one simultaneously, which renders\nrepresentation of all possible anomalies very challenging. Hence, a possible\nsolution is an unsupervised anomaly detection (UAD) system that can learn a\ndata distribution from an unlabelled dataset of healthy subjects and then be\napplied to detect out of distribution samples. Such a technique can then be\nused to detect anomalies - lesions or abnormalities, for example, brain\ntumours, without explicitly training the model for that specific pathology.\nSeveral Variational Autoencoder (VAE) based techniques have been proposed in\nthe past for this task. Even though they perform very well on controlled\nartificially simulated anomalies, many of them perform poorly while detecting\nanomalies in clinical data. This research proposes a compact version of the\n\"context-encoding\" VAE (ceVAE) model, combined with pre and post-processing\nsteps, creating a UAD pipeline (StRegA), which is more robust on clinical data,\nand shows its applicability in detecting anomalies such as tumours in brain\nMRIs. The proposed pipeline achieved a Dice score of 0.642$\\pm$0.101 while\ndetecting tumours in T2w images of the BraTS dataset and 0.859$\\pm$0.112 while\ndetecting artificially induced anomalies, while the best performing baseline\nachieved 0.522$\\pm$0.135 and 0.783$\\pm$0.111, respectively.",
    "descriptor": "",
    "authors": [
      "Soumick Chatterjee",
      "Alessandro Sciarra",
      "Max D\u00fcnnwald",
      "Pavan Tummala",
      "Shubham Kumar Agrawal",
      "Aishwarya Jauhari",
      "Aman Kalra",
      "Steffen Oeltze-Jafra",
      "Oliver Speck",
      "Andreas N\u00fcrnberger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.13271"
  },
  {
    "id": "arXiv:2201.13278",
    "title": "Combining Local and Global Pose Estimation for Precise Tracking of  Similar Objects",
    "abstract": "In this paper, we present a multi-object 6D detection and tracking pipeline\nfor potentially similar and non-textured objects. The combination of a\nconvolutional neural network for object classification and rough pose\nestimation with a local pose refinement and an automatic mismatch detection\nenables direct application in real-time AR scenarios. A new network\narchitecture, trained solely with synthetic images, allows simultaneous pose\nestimation of multiple objects with reduced GPU memory consumption and enhanced\nperformance. In addition, the pose estimates are further improved by a local\nedge-based refinement step that explicitly exploits known object geometry\ninformation. For continuous movements, the sole use of local refinement reduces\npose mismatches due to geometric ambiguities or occlusions. We showcase the\nentire tracking pipeline and demonstrate the benefits of the combined approach.\nExperiments on a challenging set of non-textured similar objects demonstrate\nthe enhanced quality compared to the baseline method. Finally, we illustrate\nhow the system can be used in a real AR assistance application within the field\nof construction.",
    "descriptor": "\nComments: Accepted at VISAPP 2022\n",
    "authors": [
      "Niklas Gard",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13278"
  },
  {
    "id": "arXiv:2201.13279",
    "title": "UQGAN: A Unified Model for Uncertainty Quantification of Deep  Classifiers trained via Conditional GANs",
    "abstract": "We present an approach to quantifying both aleatoric and epistemic\nuncertainty for deep neural networks in image classification, based on\ngenerative adversarial networks (GANs). While most works in the literature that\nuse GANs to generate out-of-distribution (OoD) examples only focus on the\nevaluation of OoD detection, we present a GAN based approach to learn a\nclassifier that exhibits proper uncertainties for OoD examples as well as for\nfalse positives (FPs). Instead of shielding the entire in-distribution data\nwith GAN generated OoD examples which is state-of-the-art, we shield each class\nseparately with out-of-class examples generated by a conditional GAN and\ncomplement this with a one-vs-all image classifier. In our experiments, in\nparticular on CIFAR10, we improve over the OoD detection and FP detection\nperformance of state-of-the-art GAN-training based classifiers. Furthermore, we\nalso find that the generated GAN examples do not significantly affect the\ncalibration error of our classifier and result in a significant gain in model\naccuracy.",
    "descriptor": "",
    "authors": [
      "Philipp Oberdiek",
      "Gernot A. Fink",
      "Matthias Rottmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13279"
  },
  {
    "id": "arXiv:2201.13284",
    "title": "Exploring Preferences for Transportation Modes in the City of Munich  after the Recent Incorporation of Ride-Hailing Companies",
    "abstract": "The growth of ridehailing (RH) companies over the past few years has affected\nurban mobility in numerous ways. Despite widespread claims about the benefits\nof such services, limited research has been conducted on the topic. This paper\nassesses the willingness of Munich transportation users to pay for RH services.\nRealizing the difficulty of obtaining data directly from RH companies, a stated\npreference survey was designed. The dataset includes responses from 500\ncommuters. Sociodemographic attributes, current travel behavior and\ntransportation mode preference in an 8 km trip scenario using RH service and\nits similar modes (auto and transit), were collected. A multinomial logit model\nwas used to estimate the time and cost coefficients for using RH services\nacross income groups, which was then used to estimate the value of time (VOT)\nfor RH. The model results indicate RH services popularity among those aged 18\nto 39, larger households and households with fewer autos. Higher income groups\nare also willing to pay more for using RH services. To examine the impact of RH\nservices on modal split in the city of Munich, we incorporated RH as a new mode\ninto an existing nested logit mode choice model using an incremental logit.\nTravel time, travel cost and VOT were used as measures for the choice commuters\nmake when choosing between RH and its closest mode, metro. A total of 20\nscenarios were evaluated at four different congestion levels and four price\nlevels to reflect the demand in response to acceptable costs and time\ntradeoffs.",
    "descriptor": "",
    "authors": [
      "Maged Shoman",
      "Ana Tsui Moreno"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.13284"
  },
  {
    "id": "arXiv:2201.13287",
    "title": "Top-K Ranking Deep Contextual Bandits for Information Selection Systems",
    "abstract": "In today's technology environment, information is abundant, dynamic, and\nheterogeneous in nature. Automated filtering and prioritization of information\nis based on the distinction between whether the information adds substantial\nvalue toward one's goal or not. Contextual multi-armed bandit has been widely\nused for learning to filter contents and prioritize according to user interest\nor relevance. Learn-to-Rank technique optimizes the relevance ranking on items,\nallowing the contents to be selected accordingly. We propose a novel approach\nto top-K rankings under the contextual multi-armed bandit framework. We model\nthe stochastic reward function with a neural network to allow non-linear\napproximation to learn the relationship between rewards and contexts. We\ndemonstrate the approach and evaluate the the performance of learning from the\nexperiments using real world data sets in simulated scenarios. Empirical\nresults show that this approach performs well under the complexity of a reward\nstructure and high dimensional contextual features.",
    "descriptor": "",
    "authors": [
      "Jade Freeman",
      "Michael Rawson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.13287"
  },
  {
    "id": "arXiv:2201.13290",
    "title": "Model-Based Engineering of CPPS Functions and Code Generation for Skills",
    "abstract": "Today's production systems are complex networks of cyber-physical systems\nwhich combine mechanical and electronic parts with software and networking\ncapabilities. To the inherent complexity of such systems additional complexity\narises from the context in which these systems operate. Manufacturing companies\nneed to be able to adapt their production to ever changing customer demands as\nwell as decreasing lot sizes. Engineering such systems, which need to be\ncombined and reconfigured into different networks under changing conditions,\nrequires engineering methods to carefully design them for possible future uses.\nSuch engineering methods need to preserve the flexibility of functions into\nruntime, so that reconfiguring machines can be done with as little effort as\npossible. In this paper we present a model-based approach that is focused on\nmachine functions and allows to methodically develop system functionalities for\nchanging system networks. These functions are implemented as so-called skills\nusing automated code-generation.",
    "descriptor": "",
    "authors": [
      "Aljosha Koecher",
      "Alexander Hayward",
      "Alexander Fay"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13290"
  },
  {
    "id": "arXiv:2201.13291",
    "title": "Metrics for saliency map evaluation of deep learning explanation methods",
    "abstract": "Due to the black-box nature of deep learning models, there is a recent\ndevelopment of solutions for visual explanations of CNNs. Given the high cost\nof user studies, metrics are necessary to compare and evaluate these different\nmethods. In this paper, we critically analyze the Deletion Area Under Curve\n(DAUC) and Insertion Area Under Curve (IAUC) metrics proposed by Petsiuk et al.\n(2018). These metrics were designed to evaluate the faithfulness of saliency\nmaps generated by generic methods such as Grad-CAM or RISE. First, we show that\nthe actual saliency score values given by the saliency map are ignored as only\nthe ranking of the scores is taken into account. This shows that these metrics\nare insufficient by themselves, as the visual appearance of a saliency map can\nchange significantly without the ranking of the scores being modified.\nSecondly, we argue that during the computation of DAUC and IAUC, the model is\npresented with images that are out of the training distribution which might\nlead to an unreliable behavior of the model being explained. %First, we show\nthat one can drastically change the visual appearance of an explanation map\nwithout changing the pixel ranking, i.e. without changing the DAUC and IAUC\nvalues. %We argue that DAUC and IAUC only takes into account the scores ranking\nand ignore the score values. To complement DAUC/IAUC, we propose new metrics\nthat quantify the sparsity and the calibration of explanation methods, two\npreviously unstudied properties. Finally, we give general remarks about the\nmetrics studied in this paper and discuss how to evaluate them in a user study.",
    "descriptor": "",
    "authors": [
      "Tristan Gomez",
      "Thomas Fr\u00e9our",
      "Harold Mouch\u00e8re"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13291"
  },
  {
    "id": "arXiv:2201.13292",
    "title": "Fragmented ARES: Dynamic Storage for Large Objects",
    "abstract": "Data availability is one of the most important features in distributed\nstorage systems, made possible by data replication. Nowadays data are generated\nrapidly and the goal to develop efficient, scalable and reliable storage\nsystems has become one of the major challenges for high performance computing.\nIn this work, we develop a dynamic, robust and strongly consistent distributed\nstorage implementation suitable for handling large objects (such as files). We\ndo so by integrating an Adaptive, Reconfigurable, Atomic Storage framework,\ncalled ARES, with a distributed file system, called COBFS, which relies on a\nblock fragmentation technique to handle large objects. With the addition of\nARES, we also enable the use of an erasure-coded algorithm to further split our\ndata and to potentially improve storage efficiency at the replica servers and\noperation latency. To put the practicality of our outcomes at test, we conduct\nan in-depth experimental evaluation on the Emulab and AWS EC2 testbeds,\nillustrating the benefits of our approaches, as well as other interesting\ntradeoffs.",
    "descriptor": "\nComments: 18 pages (in two-column IEEE format), 12 figures, 5 algorithm codes, Technical Report\n",
    "authors": [
      "Chryssis Georgiou",
      "Nicolas Nicolaou",
      "Andria Trigeorgi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13292"
  },
  {
    "id": "arXiv:2201.13297",
    "title": "Beyond synchronization: Body gestures and gaze direction in duo  performance",
    "abstract": "In this chapter, we focus on two main categories of visual interaction: body\ngestures and gaze direction. Our focus on body gestures is motivated by\nresearch showing that gesture patterns often change during joint action tasks\nto become more predictable (van der Wel et al., 2016). Moreover, coordination\nsometimes emerges between musicians at the level of body sway (Chang et al.,\n2017). Our focus on gaze direction was motivated by the fact that gaze can\nserve simultaneously as a means of obtaining information about the world and as\na means of communicating one's own attention and intent.",
    "descriptor": "\nComments: Please cite as: Bishop, L., Cancino-Chac\\'on, C., & Goebl, W. (2021). Beyond synchronization: Body gestures and gaze direction in duo performance. In Timmers, R., Bailes, F., and Daffern, H. (Eds.), Together in Music: Participation, Co-Ordination, and Creativity in Ensembles. Oxford: Oxford University. This version is a preprint of the chapter prepared by the authors\n",
    "authors": [
      "Laura Bishop",
      "Carlos Cancino-Chac\u00f3n",
      "Werner Goebl"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.13297"
  },
  {
    "id": "arXiv:2201.13298",
    "title": "A Safe Control Architecture Based on a Model Predictive Control  Supervisor for Autonomous Driving",
    "abstract": "This paper presents a novel, safe control architecture (SCA) for controlling\nan important class of systems: safety-critical systems. Ensuring the safety of\ncontrol decisions has always been a challenge in automatic control. The\nproposed SCA aims to address this challenge by using a Model Predictive\nController (MPC) that acts as a supervisor for the operating controller, in the\nsense that the MPC constantly checks the safety of the control inputs generated\nby the operating controller and intervenes if the control input is predicted to\nlead to a hazardous situation in the foreseeable future invariably. Then an\nappropriate backup scheme can be activated, e.g., a degraded control mechanism,\nthe transfer of the system to a safe state, or a warning signal issued to a\nhuman supervisor. For a proof of concept, the proposed SCA is applied to an\nautonomous driving scenario, where it is illustrated and compared in different\nobstacle avoidance scenarios. A major challenge of the SCA lies in the mismatch\nbetween the MPC prediction model and the real system, for which possible\nremedies are explored.",
    "descriptor": "",
    "authors": [
      "Maryam Nezami",
      "Georg Maennel",
      "Hossam Seddik Abbas",
      "Georg Schildbach"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13298"
  },
  {
    "id": "arXiv:2201.13300",
    "title": "End-to-End Quality-of-Service Assurance with Autonomous Systems: 5G/6G  Case Study",
    "abstract": "Providing differentiated services to meet the unique requirements of\ndifferent use cases is a major goal of the fifth generation (5G)\ntelecommunication networks and will be even more critical for future 6G\nsystems. Fulfilling this goal requires the ability to assure quality of service\n(QoS) end to end (E2E), which remains a challenge. A key factor that makes E2E\nQoS assurance difficult in a telecommunication system is that access networks\n(ANs) and core networks (CNs) manage their resources autonomously. So far, few\nresults have been available that can ensure E2E QoS over autonomously managed\nANs and CNs. Existing techniques rely predominately on each subsystem to meet\nstatic local QoS budgets with no recourse in case any subsystem fails to meet\nits local budgets and, hence will have difficulty delivering E2E assurance.\nMoreover, most existing distributed optimization techniques that can be applied\nto assure E2E QoS over autonomous subsystems require the subsystems to exchange\nsensitive information such as their local decision variables. This paper\npresents a novel framework and a distributed algorithm that can enable ANs and\nCNs to autonomously \"cooperate\" with each other to dynamically negotiate their\nlocal QoS budgets and to collectively meet E2E QoS goals by sharing only their\nestimates of the global constraint functions, without disclosing their local\ndecision variables. We prove that this new distributed algorithm converges to\nan optimal solution almost surely, and also present numerical results to\ndemonstrate that the convergence occurs quickly even with measurement noise.",
    "descriptor": "\nComments: 8 pages, 4 figures, IEEE CCNC 2022\n",
    "authors": [
      "Van Sy Mai",
      "Richard J. La",
      "Tao Zhang",
      "Abdella Battou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13300"
  },
  {
    "id": "arXiv:2201.13301",
    "title": "Won't you see my neighbor?: User predictions, mental models, and  similarity-based explanations of AI classifiers",
    "abstract": "Humans should be able work more effectively with artificial\nintelligence-based systems when they can predict likely failures and form\nuseful mental models of how the systems work. We conducted a study of human's\nmental models of artificial intelligence systems using a high-performing image\nclassifier, focusing on participants' ability to predict the classification\nresult for a particular image. Participants viewed individual labeled images in\none of two classes and then tried to predict whether the classifier would label\nthem correctly. In this experiment we explored the effect of giving\nparticipants additional information about an image's nearest neighbors in a\nspace representing the otherwise uninterpretable features extracted by the\nlower layers of the classifier's neural network. We found that providing this\ninformation did increase participants' prediction performance, and that the\nperformance improvement could be related to the neighbor images' similarity to\nthe target image. We also found indications that the presentation of this\ninformation may influence people's own classification of the target image --\nthat is, rather than just anthropomorphizing the system, in some cases the\nhumans become \"mechanomorphized\" in their judgements.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Kimberly Glasgow",
      "Jonathan Kopecky",
      "John Gersh",
      "Adam Crego"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2201.13301"
  },
  {
    "id": "arXiv:2201.13302",
    "title": "Eris: Measuring discord among multidimensional data sources",
    "abstract": "Data integration is a classical problem in databases, typically decomposed\ninto schema matching, entity matching and record merging. To solve the latter,\nit is mostly assumed that ground truth can be determined, either as master data\nor from user feedback. However, in many cases, this is not the case because\nfirstly the merging processes cannot be accurate enough, and also the data\ngathering processes in the different sources are simply imperfect and cannot\nprovide high quality data. Instead of enforcing consistency, we propose to\nevaluate how concordant or discordant sources are as a measure of\ntrustworthiness (the more discordant are the sources, the less we can trust\ntheir data).\nThus, we define the discord measurement problem in which given a set of\nuncertain raw observations or aggregate results (such as\ncase/hospitalization/death data relevant to COVID-19) and information on the\nalignment of different data (for example, cases and deaths), we wish to assess\nwhether the different sources are concordant, or if not, measure how discordant\nthey are. We also define a set of algebraic operators to describe the\nalignments, together with two alternative relational implementations that\nreduce the problem to linear or quadratic programming. These are evaluated\nagainst both COVID-19 and synthetic data, and our experimental results show\nthat discordancy measurement can be performed efficiently in realistic\nsituations.",
    "descriptor": "\nComments: 16 pages, 11 figures\n",
    "authors": [
      "Alberto Abello",
      "James Cheney"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.13302"
  },
  {
    "id": "arXiv:2201.13305",
    "title": "Optimizing LLVM Pass Sequences with Shackleton: A Linear Genetic  Programming Framework",
    "abstract": "In this paper we introduce Shackleton as a generalized framework enabling the\napplication of linear genetic programming -- a technique under the umbrella of\nevolutionary algorithms -- to a variety of use cases. We also explore here a\nnovel application for this class of methods: optimizing sequences of LLVM\noptimization passes. The algorithm underpinning Shackleton is discussed, with\nan emphasis on the effects of different features unique to the framework when\napplied to LLVM pass sequences. Combined with analysis of different\nhyperparameter settings, we report the results on automatically optimizing pass\nsequences using Shackleton for two software applications at differing\ncomplexity levels. Finally, we reflect on the advantages and limitations of our\ncurrent implementation and lay out a path for further improvements. These\nimprovements aim to surpass hand-crafted solutions with an automatic discovery\nmethod for an optimal pass sequence.",
    "descriptor": "\nComments: 11 pages (with references), 14 figures, 8 tables\n",
    "authors": [
      "Hannah Peeler",
      "Shuyue Stella Li",
      "Andrew N. Sloss",
      "Kenneth N. Reid",
      "Yuan Yuan",
      "Wolfgang Banzhaf"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.13305"
  },
  {
    "id": "arXiv:2201.13311",
    "title": "Masked Transformer for Neighhourhood-aware Click-Through Rate Prediction",
    "abstract": "Click-Through Rate (CTR) prediction, is an essential component of online\nadvertising. The mainstream techniques mostly focus on feature interaction or\nuser interest modeling, which rely on users' directly interacted items. The\nperformance of these methods are usally impeded by inactive behaviours and\nsystem's exposure, incurring that the features extracted do not contain enough\ninformation to represent all potential interests. For this sake, we propose\nNeighbor-Interaction based CTR prediction, which put this task into a\nHeterogeneous Information Network (HIN) setting, then involves local\nneighborhood of the target user-item pair in the HIN to predict their linkage.\nIn order to enhance the representation of the local neighbourhood, we consider\nfour types of topological interaction among the nodes, and propose a novel\nGraph-masked Transformer architecture to effectively incorporates both feature\nand topological information.\nWe conduct comprehensive experiments on two real world datasets and the\nexperimental results show that our proposed method outperforms state-of-the-art\nCTR models significantly.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Erxue Min",
      "Yu Rong",
      "Tingyang Xu",
      "Yatao Bian",
      "Peilin Zhao",
      "Junzhou Huang",
      "Da Luo",
      "Kangyi Lin",
      "Sophia Ananiadou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13311"
  },
  {
    "id": "arXiv:2201.13312",
    "title": "On scale-invariant properties in natural images and their simulations",
    "abstract": "We study samples of natural images for which a set of statistical\ncharacteristics is computed and scale-invariant properties of samples are\ndemonstrated computationally. Computations of the power spectrum are carried\nout and a power-law decaying power spectrum is observed on samples taken from\nvan Hateren images of natural scenes. We propose a dynamic model to reproduce\nthe observed slope in the power spectrum qualitatively. For two types of\nsources for this model the behaviour of power spectrum is investigated and\nscale-invariance confirmed numerically. We then discuss potential applications\nof scale-invariant properties of natural images.",
    "descriptor": "\nComments: 7 pages, 13 figures\n",
    "authors": [
      "Maxim Koroteev",
      "Kirill Aistov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13312"
  },
  {
    "id": "arXiv:2201.13313",
    "title": "Efficiently Maintaining Next Basket Recommendations under Additions and  Deletions of Baskets and Items",
    "abstract": "Recommender systems play an important role in helping people find information\nand make decisions in today's increasingly digitalized societies. However, the\nwide adoption of such machine learning applications also causes concerns in\nterms of data privacy. These concerns are addressed by the recent \"General Data\nProtection Regulation\" (GDPR) in Europe, which requires companies to delete\npersonal user data upon request when users enforce their \"right to be\nforgotten\". Many researchers argue that this deletion obligation does not only\napply to the data stored in primary data stores such as relational databases\nbut also requires an update of machine learning models whose training set\nincluded the personal data to delete. We explore this direction in the context\nof a sequential recommendation task called Next Basket Recommendation (NBR),\nwhere the goal is to recommend a set of items based on a user's purchase\nhistory. We design efficient algorithms for incrementally and decrementally\nupdating a state-of-the-art next basket recommendation model in response to\nadditions and deletions of user baskets and items. Furthermore, we discuss an\nefficient, data-parallel implementation of our method in the Spark Structured\nStreaming system. We evaluate our implementation on a variety of real-world\ndatasets, where we investigate the impact of our update techniques on several\nranking metrics and measure the time to perform model updates. Our results show\nthat our method provides constant update time efficiency with respect to an\nadditional user basket in the incremental case, and linear efficiency in the\ndecremental case where we delete existing baskets. With modest computational\nresources, we are able to update models with a latency of around\n0.2~milliseconds regardless of the history size in the incremental case, and\nless than one millisecond in the decremental case.",
    "descriptor": "\nComments: ORSUM Workshop at Recommender Systems Conference 2021, Amsterdam, the Netherlands\n",
    "authors": [
      "Benjamin Longxiang Wang",
      "Sebastian Schelter"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13313"
  },
  {
    "id": "arXiv:2201.13314",
    "title": "Error analysis of a class of semi-discrete schemes for solving the  Gross-Pitaevskii equation at low regularity",
    "abstract": "We analyse a class of time discretizations for solving the Gross-Pitaevskii\nequation at low-regularity on an arbitrary Lipschitz domain $\\Omega \\subset\n\\mathbb{R}^d$, $d \\le 3$, with a non-smooth potential. We show that these\nschemes, together with their optimal local error structure, allow for\nconvergence under lower regularity assumptions on both the solution and the\npotential than is required by classical methods, such as splitting or\nexponential integrator methods. Moreover, we show convergence in the case of\nperiodic boundary conditions, in any fractional positive Sobolev space $H^{r}$,\n$r \\ge 0$ beyond the more typical $L^2$-error analysis.",
    "descriptor": "",
    "authors": [
      "Yvonne Alama Bronsard"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2201.13314"
  },
  {
    "id": "arXiv:2201.13317",
    "title": "Hyper-Class Representation of Data",
    "abstract": "Data representation is often of the natural form with their attribute values.\nTo utilize the data efficiently, one needs to well understand the observed\nattribute values and identify the potential useful information in the\ndata/smaples, or training data. In this paper, a new data representation, named\nas hyper-classes representation, is proposed for improving recommendation. At\nfirst, the cross entropy, KL divergence and JS divergence of features in data\nare defined. And then, the hyper-classes in data can be discovered with these\nthree parameters. Finally, a kind of recommendation algorithm is used to\nevaluate the proposed hyper-class representation of data, and shows that the\nhyper-class representation is able to provide truly useful reference\ninformation for recommendation systems and makes recommendations much better\nthan existing algorithms, i.e., this approach is efficient and promising.",
    "descriptor": "",
    "authors": [
      "Shichao Zhang",
      "Jiaye Li",
      "Wenzhen Zhang",
      "Yongsong Qin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2201.13317"
  },
  {
    "id": "arXiv:2201.13320",
    "title": "BEER: Fast $O(1/T)$ Rate for Decentralized Nonconvex Optimization with  Communication Compression",
    "abstract": "Communication efficiency has been widely recognized as the bottleneck for\nlarge-scale decentralized machine learning applications in multi-agent or\nfederated environments. To tackle the communication bottleneck, there have been\nmany efforts to design communication-compressed algorithms for decentralized\nnonconvex optimization, where the clients are only allowed to communicate a\nsmall amount of quantized information (aka bits) with their neighbors over a\npredefined graph topology. Despite significant efforts, the state-of-the-art\nalgorithm in the nonconvex setting still suffers from a slower rate of\nconvergence $O((G/T)^{2/3})$ compared with their uncompressed counterpart,\nwhere $G$ measures the data heterogeneity across different clients, and $T$ is\nthe number of communication rounds. This paper proposes BEER, which adopts\ncommunication compression with gradient tracking, and shows it converges at a\nfaster rate of $O(1/T)$. This significantly improves over the state-of-the-art\nrate, by matching the rate without compression even under arbitrary data\nheterogeneity. Numerical experiments are also provided to corroborate our\ntheory and confirm the practical superiority of BEER in the data heterogeneous\nregime.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Haoyu Zhao",
      "Boyue Li",
      "Zhize Li",
      "Peter Richt\u00e1rik",
      "Yuejie Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13320"
  },
  {
    "id": "arXiv:2201.13322",
    "title": "Learning to Hash Naturally Sorts",
    "abstract": "Locality sensitive hashing pictures a list-wise sorting problem. Its testing\nmetrics, e.g., mean-average precision, count on a sorted candidate list ordered\nby pair-wise code similarity. However, scarcely does one train a deep hashing\nmodel with the sorted results end-to-end because of the non-differentiable\nnature of the sorting operation. This inconsistency in the objectives of\ntraining and test may lead to sub-optimal performance since the training loss\noften fails to reflect the actual retrieval metric. In this paper, we tackle\nthis problem by introducing Naturally-Sorted Hashing (NSH). We sort the Hamming\ndistances of samples' hash codes and accordingly gather their latent\nrepresentations for self-supervised training. Thanks to the recent advances in\ndifferentiable sorting approximations, the hash head receives gradients from\nthe sorter so that the hash encoder can be optimized along with the training\nprocedure. Additionally, we describe a novel Sorted Noise-Contrastive\nEstimation (SortedNCE) loss that selectively picks positive and negative\nsamples for contrastive learning, which allows NSH to mine data semantic\nrelations during training in an unsupervised manner. Our extensive experiments\nshow the proposed NSH model significantly outperforms the existing unsupervised\nhashing methods on three benchmarked datasets.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Yuming Shen",
      "Jiaguo Yu",
      "Haofeng Zhang",
      "Philip H.S. Torr",
      "Menghan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13322"
  },
  {
    "id": "arXiv:2201.13324",
    "title": "Guided Semi-Supervised Non-negative Matrix Factorization on Legal  Documents",
    "abstract": "Classification and topic modeling are popular techniques in machine learning\nthat extract information from large-scale datasets. By incorporating a priori\ninformation such as labels or important features, methods have been developed\nto perform classification and topic modeling tasks; however, most methods that\ncan perform both do not allow for guidance of the topics or features. In this\npaper, we propose a method, namely Guided Semi-Supervised Non-negative Matrix\nFactorization (GSSNMF), that performs both classification and topic modeling by\nincorporating supervision from both pre-assigned document class labels and\nuser-designed seed words. We test the performance of this method through its\napplication to legal documents provided by the California Innocence Project, a\nnonprofit that works to free innocent convicted persons and reform the justice\nsystem. The results show that our proposed method improves both classification\naccuracy and topic coherence in comparison to past methods like Semi-Supervised\nNon-negative Matrix Factorization (SSNMF) and Guided Non-negative Matrix\nFactorization (Guided NMF).",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Pengyu Li",
      "Christine Tseng",
      "Yaxuan Zheng",
      "Joyce A. Chew",
      "Longxiu Huang",
      "Benjamin Jarman",
      "Deanna Needell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13324"
  },
  {
    "id": "arXiv:2201.13329",
    "title": "Can Adversarial Training Be Manipulated By Non-Robust Features?",
    "abstract": "Adversarial training, originally designed to resist test-time adversarial\nexamples, has shown to be promising in mitigating training-time availability\nattacks. This defense ability, however, is challenged in this paper. We\nidentify a novel threat model named stability attacks, which aims to hinder\nrobust availability by slightly perturbing the training data. Under this\nthreat, we find that adversarial training using a conventional defense budget\n$\\epsilon$ provably fails to provide test robustness in a simple statistical\nsetting when the non-robust features of the training data are reinforced by\n$\\epsilon$-bounded perturbation. Further, we analyze the necessity of enlarging\nthe defense budget to counter stability attacks. Finally, comprehensive\nexperiments demonstrate that stability attacks are harmful on benchmark\ndatasets, and thus the adaptive defense is necessary to maintain robustness.",
    "descriptor": "",
    "authors": [
      "Lue Tao",
      "Lei Feng",
      "Hongxin Wei",
      "Jinfeng Yi",
      "Sheng-Jun Huang",
      "Songcan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13329"
  },
  {
    "id": "arXiv:2201.13331",
    "title": "Steady-State Error Compensation in Reference Tracking and Disturbance  Rejection Problems for Reinforcement Learning-Based Control",
    "abstract": "Reinforcement learning (RL) is a promising, upcoming topic in automatic\ncontrol applications. Where classical control approaches require a priori\nsystem knowledge, data-driven control approaches like RL allow a model-free\ncontroller design procedure, rendering them emergent techniques for systems\nwith changing plant structures and varying parameters. While it was already\nshown in various applications that the transient control behavior for complex\nsystems can be sufficiently handled by RL, the challenge of non-vanishing\nsteady-state control errors remains, which arises from the usage of control\npolicy approximations and finite training times. To overcome this issue, an\nintegral action state augmentation (IASA) for actor-critic-based RL controllers\nis introduced that mimics an integrating feedback, which is inspired by the\ndelta-input formulation within model predictive control. This augmentation does\nnot require any expert knowledge, leaving the approach model free. As a result,\nthe RL controller learns how to suppress steady-state control deviations much\nmore effectively. Two exemplary applications from the domain of electrical\nenergy engineering validate the benefit of the developed method both for\nreference tracking and disturbance rejection. In comparison to a standard deep\ndeterministic policy gradient (DDPG) setup, the suggested IASA extension allows\nto reduce the steady-state error by up to 52 $\\%$ within the considered\nvalidation scenarios.",
    "descriptor": "",
    "authors": [
      "Daniel Weber",
      "Maximilian Schenke",
      "Oliver Wallscheid"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13331"
  },
  {
    "id": "arXiv:2201.13332",
    "title": "The Metric Distortion of Multiwinner Voting",
    "abstract": "We extend the recently introduced framework of metric distortion to\nmultiwinner voting. In this framework, $n$ agents and $m$ alternatives are\nlocated in an underlying metric space. The exact distances between agents and\nalternatives are unknown. Instead, each agent provides a ranking of the\nalternatives, ordered from the closest to the farthest. Typically, the goal is\nto select a single alternative that approximately minimizes the total distance\nfrom the agents, and the worst-case approximation ratio is termed distortion.\nIn the case of multiwinner voting, the goal is to select a committee of $k$\nalternatives that (approximately) minimizes the total cost to all agents. We\nconsider the scenario where the cost of an agent for a committee is her\ndistance from the $q$-th closest alternative in the committee. We reveal a\nsurprising trichotomy on the distortion of multiwinner voting rules in terms of\n$k$ and $q$: The distortion is unbounded when $q \\leq k/3$, asymptotically\nlinear in the number of agents when $k/3 < q \\leq k/2$, and constant when $q >\nk/2$.",
    "descriptor": "\nComments: A preliminary version of this paper appears in AAAI 2022\n",
    "authors": [
      "Ioannis Caragiannis",
      "Nisarg Shah",
      "Alexandros A. Voudouris"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.13332"
  },
  {
    "id": "arXiv:2201.13338",
    "title": "Modeling the Background for Incremental and Weakly-Supervised Semantic  Segmentation",
    "abstract": "Deep neural networks have enabled major progresses in semantic segmentation.\nHowever, even the most advanced neural architectures suffer from important\nlimitations. First, they are vulnerable to catastrophic forgetting, i.e. they\nperform poorly when they are required to incrementally update their model as\nnew classes are available. Second, they rely on large amount of pixel-level\nannotations to produce accurate segmentation maps. To tackle these issues, we\nintroduce a novel incremental class learning approach for semantic segmentation\ntaking into account a peculiar aspect of this task: since each training step\nprovides annotation only for a subset of all possible classes, pixels of the\nbackground class exhibit a semantic shift. Therefore, we revisit the\ntraditional distillation paradigm by designing novel loss terms which\nexplicitly account for the background shift. Additionally, we introduce a novel\nstrategy to initialize classifier's parameters at each step in order to prevent\nbiased predictions toward the background class. Finally, we demonstrate that\nour approach can be extended to point- and scribble-based weakly supervised\nsegmentation, modeling the partial annotations to create priors for unlabeled\npixels. We demonstrate the effectiveness of our approach with an extensive\nevaluation on the Pascal-VOC, ADE20K, and Cityscapes datasets, significantly\noutperforming state-of-the-art methods.",
    "descriptor": "\nComments: Accepted by T-PAMI (this https URL). arXiv admin note: substantial text overlap with arXiv:2002.00718\n",
    "authors": [
      "Fabio Cermelli",
      "Massimiliano Mancini",
      "Samuel Rota Bul\u00f3",
      "Elisa Ricci",
      "Barbara Caputo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13338"
  },
  {
    "id": "arXiv:2201.13348",
    "title": "Advantages and Disadvantages of (Dedicated) Model Transformation  Languages A Qualitative Interview Study",
    "abstract": "In a recent study we have shown, that a large number of claims about model\ntransformation languages have not yet been substantiated and are made without\nmuch context to be able to critically asses their merit or built meaningful\nempirical studies around them. The objective of our work was to elicit the\nreasoning, influences and background knowledge of researchers and practitioners\nthat lead them to assuming benefits or drawbacks of model transformation\nlanguages compared to general purpose languages for the task of developing\nmodel transformations. For this we put our focus on the following 6 properties\nthat have strong relevance for wider adoption: Ease of writing,\nComprehensibility, Tool Support, Practical Expressiveness, Productivity, Reuse\nand Maintainability. We conducted a large-scale interview study involving 56\nparticipants from research and industry. Interviewees were presented with\nclaims about model transformation languages and were asked to provide reasons\nas to why they believe or dispute these claims. Our interviews show, that the\ngeneral purpose expressiveness of GPLs, the domain specific capabilities of\nMTLs and the tooling of MTLs all have strong influences on how people view\nproperties of model transformation languages. Their specific influences differ\ndepending on different concrete characteristics, such as, for example,\nBidirectionality or Debugging Tooling. Moreover, the choice of MTL, the use\ncase for which a transformation should be developed as well as the skills of\ninvolved stakeholders have an indirect effect on MTL properties by changing the\ncontextual circumstances under examination. We conclude that there is a broad\nbody of experience of interviews that suggests positive and negative influences\nfor properties of MTLs. However, our qualitative data suggests that much needs\nto be done in order to convey the viability of model transformation languages.",
    "descriptor": "",
    "authors": [
      "Stefan H\u00f6ppner",
      "Yves Haas",
      "Matthias Tichy",
      "Katharina Juhnke"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.13348"
  },
  {
    "id": "arXiv:2201.13351",
    "title": "LinSyn: Synthesizing Tight Linear Bounds for Arbitrary Neural Network  Activation Functions",
    "abstract": "The most scalable approaches to certifying neural network robustness depend\non computing sound linear lower and upper bounds for the network's activation\nfunctions. Current approaches are limited in that the linear bounds must be\nhandcrafted by an expert, and can be sub-optimal, especially when the network's\narchitecture composes operations using, for example, multiplication such as in\nLSTMs and the recently popular Swish activation. The dependence on an expert\nprevents the application of robustness certification to developments in the\nstate-of-the-art of activation functions, and furthermore the lack of tightness\nguarantees may give a false sense of insecurity about a particular model. To\nthe best of our knowledge, we are the first to consider the problem of\nautomatically computing tight linear bounds for arbitrary n-dimensional\nactivation functions. We propose LinSyn, the first approach that achieves tight\nbounds for any arbitrary activation function, while only leveraging the\nmathematical definition of the activation function itself. Our approach\nleverages an efficient heuristic approach to synthesize bounds that are tight\nand usually sound, and then verifies the soundness (and adjusts the bounds if\nnecessary) using the highly optimized branch-and-bound SMT solver, dReal. Even\nthough our approach depends on an SMT solver, we show that the runtime is\nreasonable in practice, and, compared with state of the art, our approach often\nachieves 2-5X tighter final output bounds and more than quadruple certified\nrobustness.",
    "descriptor": "\nComments: Published as a conference paper at TACAS 2022\n",
    "authors": [
      "Brandon Paulsen",
      "Chao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13351"
  },
  {
    "id": "arXiv:2201.13354",
    "title": "Graph Set-colorings And Hypergraphs In Topological Coding",
    "abstract": "In order to make more complex number-based strings from topological coding\nfor defending against the intelligent attacks equipped with quantum computing\nand providing effective protection technology for the age of quantum computing,\nwe will introduce set-colored graphs admitting set-colorings that has been\nconsiderable cryptanalytic significance, and especially related with\nhypergraphs. We use the set-coloring of graphs to reflect the intersection of\nelements, and add other constraint requirements to express more connections\nbetween sets (as hyperedges). Since we try to find some easy and effective\ntechniques based on graph theory for practical application, we use\nintersected-graphs admitting set-colorings defined on hyperedge sets to observe\ntopological structures of hypergraphs, string-type Topcode-matrix, set-type\nTopcode-matrix, graph-type Topcode-matrix, hypergraph-type Topcode-matrix,\nmatrix-type Topcode-matrix \\emph{etc}. We will show that each connected graph\nis the intersected-graph of some hypergraph and investigate hypergraph's\nconnectivity, colorings of hypergraphs, hypergraph homomorphism, hypernetworks,\nscale-free network generator, compound hypergraphs having their\nintersected-graphs with vertices to be hypergraphs (for high-dimensional\nextension diagram). Naturally, we get various graphic lattices, such as\nedge-coincided intersected-graph lattice, vertex-coincided intersected-graph\nlattice, edge-hamiltonian graphic lattice, hypergraph lattice and\nintersected-network lattice. Many techniques in this article can be translated\ninto polynomial algorithms, since we are aiming to apply hypergraphs and graph\nset-colorings to homomorphic encryption and asymmetric cryptograph.",
    "descriptor": "",
    "authors": [
      "Bing Yao",
      "Fei Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.13354"
  },
  {
    "id": "arXiv:2201.13357",
    "title": "DNS: Determinantal Point Process Based Neural Network Sampler for  Ensemble Reinforcement Learning",
    "abstract": "Application of ensemble of neural networks is becoming an imminent tool for\nadvancing the state-of-the-art in deep reinforcement learning algorithms.\nHowever, training these large numbers of neural networks in the ensemble has an\nexceedingly high computation cost which may become a hindrance in training\nlarge-scale systems. In this paper, we propose DNS: a Determinantal Point\nProcess based Neural Network Sampler that specifically uses k-dpp to sample a\nsubset of neural networks for backpropagation at every training step thus\nsignificantly reducing the training time and computation cost. We integrated\nDNS in REDQ for continuous control tasks and evaluated on MuJoCo environments.\nOur experiments show that DNS augmented REDQ outperforms baseline REDQ in terms\nof average cumulative reward and achieves this using less than 50% computation\nwhen measured in FLOPS.",
    "descriptor": "",
    "authors": [
      "Hassam Sheikh",
      "Kizza Frisbee",
      "Mariano Phielipp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13357"
  },
  {
    "id": "arXiv:2201.13360",
    "title": "Hydra: A Real-time Spatial Perception Engine for 3D Scene Graph  Construction and Optimization",
    "abstract": "3D scene graphs have recently emerged as a powerful high-level representation\nof 3D environments. A 3D scene graph describes the environment as a layered\ngraph where nodes represent spatial concepts at multiple levels of abstraction\nand edges represent relations between concepts. While 3D scene graphs can serve\nas an advanced \"mental model\" for robots, how to build such a rich\nrepresentation in real-time is still uncharted territory. This paper describes\nthe first real-time Spatial Perception engINe (SPIN), a suite of algorithms to\nbuild a 3D scene graph from sensor data in real-time. Our first contribution is\nto develop real-time algorithms to incrementally construct the layers of a\nscene graph as the robot explores the environment; these algorithms build a\nlocal Euclidean Signed Distance Function (ESDF) around the current robot\nlocation, extract a topological map of places from the ESDF, and then segment\nthe places into rooms using an approach inspired by community-detection\ntechniques. Our second contribution is to investigate loop closure detection\nand optimization in 3D scene graphs. We show that 3D scene graphs allow\ndefining hierarchical descriptors for loop closure detection; our descriptors\ncapture statistics across layers in the scene graph, ranging from low-level\nvisual appearance, to summary statistics about objects and places. We then\npropose the first algorithm to optimize a 3D scene graph in response to loop\nclosures; our approach relies on embedded deformation graphs to simultaneously\ncorrect all layers of the scene graph. We implement the proposed SPIN into a\nhighly parallelized architecture, named Hydra, that combines fast early and\nmid-level perception processes with slower high-level perception. We evaluate\nHydra on simulated and real data and show it is able to reconstruct 3D scene\ngraphs with an accuracy comparable with batch offline methods, while running\nonline.",
    "descriptor": "\nComments: 11 pages, 10 figures\n",
    "authors": [
      "Nathan Hughes",
      "Yun Chang",
      "Luca Carlone"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.13360"
  },
  {
    "id": "arXiv:2201.13361",
    "title": "Signing the Supermask: Keep, Hide, Invert",
    "abstract": "The exponential growth in numbers of parameters of neural networks over the\npast years has been accompanied by an increase in performance across several\nfields. However, due to their sheer size, the networks not only became\ndifficult to interpret but also problematic to train and use in real-world\napplications, since hardware requirements increased accordingly. Tackling both\nissues, we present a novel approach that either drops a neural network's\ninitial weights or inverts their respective sign. Put simply, a network is\ntrained by weight selection and inversion without changing their absolute\nvalues. Our contribution extends previous work on masking by additionally\nsign-inverting the initial weights and follows the findings of the Lottery\nTicket Hypothesis. Through this extension and adaptations of initialization\nmethods, we achieve a pruning rate of up to 99%, while still matching or\nexceeding the performance of various baseline and previous models. Our approach\nhas two main advantages. First, and most notable, signed Supermask models\ndrastically simplify a model's structure, while still performing well on given\ntasks. Second, by reducing the neural network to its very foundation, we gain\ninsights into which weights matter for performance.",
    "descriptor": "\nComments: ICLR 2022 camera ready\n",
    "authors": [
      "Nils Koster",
      "Oliver Grothe",
      "Achim Rettinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13361"
  },
  {
    "id": "arXiv:2201.13367",
    "title": "Don't let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution  to the Hiring Paradox",
    "abstract": "Companies that try to address inequality in employment face a hiring paradox.\nFailing to address workforce imbalance can result in legal sanctions and\nscrutiny, but proactive measures to address these issues might result in the\nsame legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor\nDepartment's Office of Federal Contract Compliance Programs (OFCCP) are not\nisolated and are likely to persist. To add to the confusion, existing\nscholarship on Ricci v. DeStefano often deems solutions to this paradox\nimpossible. Circumventive practices such as the 4/5ths rule further illustrate\ntensions between too little action and too much action.\nIn this work, we give a powerful way to solve this hiring paradox that tracks\nboth legal and algorithmic challenges. We unpack the nuances of Ricci v.\nDeStefano and extend the legal literature arguing that certain algorithmic\napproaches to employment are allowed by introducing the legal practice of\nbanding to evaluate candidates. We thus show that a bias-aware technique can be\nused to diagnose and mitigate \"built-in\" headwinds in the employment pipeline.\nWe use the machinery of partially ordered sets to handle the presence of\nuncertainty in evaluations data. This approach allows us to move away from\ntreating \"people as numbers\" to treating people as individuals -- a property\nthat is sought after by Title VII in the context of employment.",
    "descriptor": "\nComments: 29 pages, 9 figures\n",
    "authors": [
      "Jad Salem",
      "Deven R. Desai",
      "Swati Gupta"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.13367"
  },
  {
    "id": "arXiv:2201.13376",
    "title": "Differentially Private Top-k Selection via Canonical Lipschitz Mechanism",
    "abstract": "Selecting the top-$k$ highest scoring items under differential privacy (DP)\nis a fundamental task with many applications. This work presents three new\nresults. First, the exponential mechanism, permute-and-flip and\nreport-noisy-max, as well as their oneshot variants, are unified into the\nLipschitz mechanism, an additive noise mechanism with a single DP-proof via a\nmandated Lipschitz property for the noise distribution. Second, this new\ngeneralized mechanism is paired with a canonical loss function to obtain the\ncanonical Lipschitz mechanism, which can directly select k-subsets out of $d$\nitems in $O(dk+d \\log d)$ time. The canonical loss function assesses subsets by\nhow many users must change for the subset to become top-$k$. Third, this\ncomposition-free approach to subset selection improves utility guarantees by an\n$\\Omega(\\log k)$ factor compared to one-by-one selection via sequential\ncomposition, and our experiments on synthetic and real-world data indicate\nsubstantial utility improvements.",
    "descriptor": "\nComments: 22 pages, 10 figures, 1 table\n",
    "authors": [
      "Michael Shekelyan",
      "Grigorios Loukides"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13376"
  },
  {
    "id": "arXiv:2201.13377",
    "title": "Neural Network Training with Asymmetric Crosspoint Elements",
    "abstract": "Analog crossbar arrays comprising programmable nonvolatile resistors are\nunder intense investigation for acceleration of deep neural network training.\nHowever, the ubiquitous asymmetric conductance modulation of practical\nresistive devices critically degrades the classification performance of\nnetworks trained with conventional algorithms. Here, we describe and\nexperimentally demonstrate an alternative fully-parallel training algorithm:\nStochastic Hamiltonian Descent. Instead of conventionally tuning weights in the\ndirection of the error function gradient, this method programs the network\nparameters to successfully minimize the total energy (Hamiltonian) of the\nsystem that incorporates the effects of device asymmetry. We provide critical\nintuition on why device asymmetry is fundamentally incompatible with\nconventional training algorithms and how the new approach exploits it as a\nuseful feature instead. Our technique enables immediate realization of analog\ndeep learning accelerators based on readily available device technologies.",
    "descriptor": "",
    "authors": [
      "Murat Onen",
      "Tayfun Gokmen",
      "Teodor K. Todorov",
      "Tomasz Nowicki",
      "Jesus A. del Alamo",
      "John Rozen",
      "Wilfried Haensch",
      "Seyoung Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.13377"
  },
  {
    "id": "arXiv:2201.13386",
    "title": "On a linearization of quadratic Wasserstein distance",
    "abstract": "This paper studies the problem of computing a linear approximation of\nquadratic Wasserstein distance $W_2$. In particular, we compute an\napproximation of the negative homogeneous weighted Sobolev norm whose\nconnection to Wasserstein distance follows from a classic linearization of a\ngeneral Monge-Amp\\'ere equation. Our contribution is threefold. First, we\nprovide expository material on this classic linearization of Wasserstein\ndistance including a quantitative error estimate. econd, we reduce the\ncomputational problem to solving a elliptic boundary value problem involving\nthe Witten Laplacian, which is a Schr\\\"odinger operator of the form $H =\n-\\Delta + V$, and describe an associated embedding. Third, for the case of\nprobability distributions on the unit square $[0,1]^2$ represented by $n \\times\nn$ arrays we present a fast code demonstrating our approach. Several numerical\nexamples are presented.",
    "descriptor": "\nComments: 24 pages, 6 figures\n",
    "authors": [
      "Philip Greengard",
      "Jeremy G. Hoskins",
      "Nicholas F. Marshall",
      "Amit Singer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Analysis and ODEs (math.CA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2201.13386"
  },
  {
    "id": "arXiv:2201.13387",
    "title": "L-SVRG and L-Katyusha with Adaptive Sampling",
    "abstract": "Stochastic gradient-based optimization methods, such as L-SVRG and its\naccelerated variant L-Katyusha [12], are widely used to train machine learning\nmodels. Theoretical and empirical performance of L-SVRG and L-Katyusha can be\nimproved by sampling the observations from a non-uniform distribution [17].\nHowever, to design a desired sampling distribution, Qian et al.[17] rely on\nprior knowledge of smoothness constants that can be computationally intractable\nto obtain in practice when the dimension of the model parameter is high. We\npropose an adaptive sampling strategy for L-SVRG and L-Katyusha that learns the\nsampling distribution with little computational overhead, while allowing it to\nchange with iterates, and at the same time does not require any prior knowledge\non the problem parameters. We prove convergence guarantees for L-SVRG and\nL-Katyusha for convex objectives when the sampling distribution changes with\niterates. These results show that even without prior information, the proposed\nadaptive sampling strategy matches, and in some cases even surpasses, the\nperformance of the sampling scheme in Qian et al.[17]. Extensive simulations\nsupport our theory and the practical utility of the proposed sampling scheme on\nreal data.",
    "descriptor": "",
    "authors": [
      "Boxin Zhao",
      "Boxiang Lyu",
      "Mladen Kolar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.13387"
  },
  {
    "id": "arXiv:2201.13388",
    "title": "Compositional Multi-Object Reinforcement Learning with Linear Relation  Networks",
    "abstract": "Although reinforcement learning has seen remarkable progress over the last\nyears, solving robust dexterous object-manipulation tasks in multi-object\nsettings remains a challenge. In this paper, we focus on models that can learn\nmanipulation tasks in fixed multi-object settings and extrapolate this skill\nzero-shot without any drop in performance when the number of objects changes.\nWe consider the generic task of bringing a specific cube out of a set to a goal\nposition. We find that previous approaches, which primarily leverage attention\nand graph neural network-based architectures, do not generalize their skills\nwhen the number of input objects changes while scaling as $K^2$. We propose an\nalternative plug-and-play module based on relational inductive biases to\novercome these limitations. Besides exceeding performances in their training\nenvironment, we show that our approach, which scales linearly in $K$, allows\nagents to extrapolate and generalize zero-shot to any new object number.",
    "descriptor": "",
    "authors": [
      "Davide Mambelli",
      "Frederik Tr\u00e4uble",
      "Stefan Bauer",
      "Bernhard Sch\u00f6lkopf",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13388"
  },
  {
    "id": "arXiv:2201.13389",
    "title": "Physics-informed neural networks for non-Newtonian fluid  thermo-mechanical problems: an application to rubber calendering process",
    "abstract": "Physics-Informed Neural Networks (PINNs) have gained much attention in\nvarious fields of engineering thanks to their capability of incorporating\nphysical laws into the models. However, the assessment of PINNs in industrial\napplications involving coupling between mechanical and thermal fields is still\nan active research topic. In this work, we present an application of PINNs to a\nnon-Newtonian fluid thermo-mechanical problem which is often considered in the\nrubber calendering process. We demonstrate the effectiveness of PINNs when\ndealing with inverse and ill-posed problems, which are impractical to be solved\nby classical numerical discretization methods. We study the impact of the\nplacement of the sensors and the distribution of unsupervised points on the\nperformance of PINNs in a problem of inferring hidden physical fields from some\npartial data. We also investigate the capability of PINNs to identify unknown\nphysical parameters from the measurements captured by sensors. The effect of\nnoisy measurements is also considered throughout this work. The results of this\npaper demonstrate that in the problem of identification, PINNs can successfully\nestimate the unknown parameters using only the measurements on the sensors. In\nill-posed problems where boundary conditions are not completely defined, even\nthough the placement of the sensors and the distribution of unsupervised points\nhave a great impact on PINNs performance, we show that the algorithm is able to\ninfer the hidden physics from local measurements.",
    "descriptor": "\nComments: 16 pages, 30 figures, 4 tables\n",
    "authors": [
      "Thi Nguyen Khoa Nguyen",
      "Thibault Dairay",
      "Rapha\u00ebl Meunier",
      "Mathilde Mougeot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13389"
  },
  {
    "id": "arXiv:2201.13391",
    "title": "Data-driven structure-preserving model reduction for stochastic  Hamiltonian systems",
    "abstract": "In this work we demonstrate that SVD-based model reduction techniques known\nfor ordinary differential equations, such as the proper orthogonal\ndecomposition, can be extended to stochastic differential equations in order to\nreduce the computational cost arising from both the high dimension of the\nconsidered stochastic system and the large number of independent Monte Carlo\nruns. We also extend the proper symplectic decomposition method to stochastic\nHamiltonian systems, both with and without external forcing, and argue that\npreserving the underlying symplectic or variational structures results in more\naccurate and stable solutions that conserve energy better than when the\nnon-geometric approach is used. We validate our proposed techniques with\nnumerical experiments for a semi-discretization of the stochastic nonlinear\nSchr\\\"odinger equation and the Kubo oscillator.",
    "descriptor": "\nComments: 36 pages, 11 figures\n",
    "authors": [
      "Tomasz M. Tyranowski"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Differential Geometry (math.DG)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2201.13391"
  },
  {
    "id": "arXiv:2201.13392",
    "title": "MHSnet: Multi-head and Spatial Attention Network with False-Positive  Reduction for Pulmonary Nodules Detection",
    "abstract": "The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.",
    "descriptor": "",
    "authors": [
      "Juanyun Mai",
      "Minghao Wang",
      "Jiayin Zheng",
      "Yanbo Shao",
      "Zhaoqi Diao",
      "Xinliang Fu",
      "Yulong Chen",
      "Jianyu Xiao",
      "Jian You",
      "Airu Yin",
      "Yang Yang",
      "Xiangcheng Qiu",
      "Jingsheng Tao",
      "Bo Wang",
      "Hua Ji"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13392"
  },
  {
    "id": "arXiv:2201.13394",
    "title": "A Formal Model of Checked C",
    "abstract": "We present a formal model of Checked C, a dialect of C that aims to enforce\nspatial memory safety. Our model pays particular attention to the semantics of\ndynamically sized, potentially null-terminated arrays. We formalize this model\nin Coq, and prove that any spatial memory safety errors can be blamed on\nportions of the program labeled unchecked; this is a Checked C feature that\nsupports incremental porting and backward compatibility. While our model's\noperational semantics uses annotated (\"fat\") pointers to enforce spatial\nsafety, we show that such annotations can be safely erased: Using PLT Redex we\nformalize an executable version of our model and a compilation procedure from\nit to an untyped C-like language, and use randomized testing to validate that\ngenerated code faithfully simulates the original. Finally, we develop a custom\nrandom generator for well-typed and almost-well-typed terms in our Redex model,\nand use it to search for inconsistencies between our model and the Clang\nChecked C implementation. We find these steps to be a useful way to co-develop\na language (Checked C is still in development) and a core model of it.",
    "descriptor": "\nComments: This is an extended version of a paper that appears at the 2022 Computer Security Foundations Symposium\n",
    "authors": [
      "Liyi Li",
      "Yiyun Liu",
      "Deena L. Postol",
      "Leonidas Lampropoulos",
      "David Van Horn",
      "Michael Hicks"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.13394"
  },
  {
    "id": "arXiv:2201.13395",
    "title": "Neural Collaborative Filtering Bandits via Meta Learning",
    "abstract": "Contextual multi-armed bandits provide powerful tools to solve the\nexploitation-exploration dilemma in decision making, with direct applications\nin the personalized recommendation. In fact, collaborative effects among users\ncarry the significant potential to improve the recommendation. In this paper,\nwe introduce and study the problem by exploring `Neural Collaborative Filtering\nBandits', where the rewards can be non-linear functions and groups are formed\ndynamically given different specific contents. To solve this problem, inspired\nby meta-learning, we propose Meta-Ban (meta-bandits), where a meta-learner is\ndesigned to represent and rapidly adapt to dynamic groups, along with a\nUCB-based exploration strategy. Furthermore, we analyze that Meta-Ban can\nachieve the regret bound of $\\mathcal{O}(\\sqrt{T \\log T})$, improving a\nmultiplicative factor $\\sqrt{\\log T}$ over state-of-the-art related works. In\nthe end, we conduct extensive experiments showing that Meta-Ban significantly\noutperforms six strong baselines.",
    "descriptor": "\nComments: In submission\n",
    "authors": [
      "{Yikun Ban",
      "Yunzhe Qi",
      "Tianxin Wei",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13395"
  },
  {
    "id": "arXiv:2201.13396",
    "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy",
    "abstract": "The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201,\nhas significantly lowered the computational overhead for conducting scientific\nresearch in neural architecture search (NAS). Although they have been widely\nadopted and used to tune real-world NAS algorithms, these benchmarks are\nlimited to small search spaces and focus solely on image classification.\nRecently, several new NAS benchmarks have been introduced that cover\nsignificantly larger search spaces over a wide range of tasks, including object\ndetection, speech recognition, and natural language processing. However,\nsubstantial differences among these NAS benchmarks have so far prevented their\nwidespread adoption, limiting researchers to using just a few benchmarks. In\nthis work, we present an in-depth analysis of popular NAS algorithms and\nperformance prediction methods across 25 different combinations of search\nspaces and datasets, finding that many conclusions drawn from a few NAS\nbenchmarks do not generalize to other benchmarks. To help remedy this problem,\nwe introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS\nbenchmarks, accessible through a unified interface, created with the aim to\nfacilitate reproducible, generalizable, and rapid NAS research. Our code is\navailable at https://github.com/automl/naslib.",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Yash Mehta",
      "Colin White",
      "Arber Zela",
      "Arjun Krishnakumar",
      "Guri Zabergja",
      "Shakiba Moradian",
      "Mahmoud Safari",
      "Kaicheng Yu",
      "Frank Hutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13396"
  },
  {
    "id": "arXiv:2201.13402",
    "title": "Privacy Limitations Of Interest-based Advertising On The Web: A  Post-mortem Empirical Analysis Of Google's FLoC",
    "abstract": "In 2021, Google announced they would disable third-party cookies in the\nChrome browser in order to improve user privacy. They proposed FLoC as an\nalternative, meant to enable interest-based advertising while mitigating risks\nof individualized user tracking. The FLoC algorithm assigns users to 'cohorts'\nthat represent groups of users with similar browsing behaviors so that\nthird-parties can serve users ads based on their group. After testing FLoC in a\nreal world trial, Google canceled the proposal, with little explanation, in\nfavor of new alternatives to third-party cookies. In this work, we offer a\npost-mortem analysis of how FLoC handled balancing utility and privacy.\nIn particular, we analyze two potential problems raised by privacy advocates:\nFLoC (1) allows individualized user tracking rather than prevents it and (2)\nrisks revealing sensitive user demographic information, presenting a new\nprivacy risk. We test these problems by implementing FLoC and compute cohorts\nfor users in a dataset of browsing histories collected from more than 90,000\nU.S. devices over a one-year period.\nFor (1) we investigate the uniqueness of users' cohort ID sequences over\ntime. We find that more than 95% are uniquely identifiable after 4 weeks. We\nshow how these risks increase when cohort IDs are combined with fingerprinting\ndata. While these risks may be mitigated by frequently clearing first-party\ncookies and increasing cohort sizes, such changes would degrade utility for\nusers and advertisers, respectively. For (2), although we find a statistically\nsignificant relationship between domain visits and racial background, we do not\nfind that FLoC risks correlating cohort IDs with race. However, alternative\nclustering techniques could elevate this risk.\nOur contributions provide example analyses for those seeking to develop novel\napproaches to monetizing the web in the future.",
    "descriptor": "",
    "authors": [
      "Alex Berke",
      "Dan Calacci"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.13402"
  },
  {
    "id": "arXiv:2201.13403",
    "title": "Vibration Fault Diagnosis in Wind Turbines based on Automated Feature  Learning",
    "abstract": "A growing number of wind turbines are equipped with vibration measurement\nsystems to enable a close monitoring and early detection of developing fault\nconditions. The vibration measurements are analyzed to continuously assess the\ncomponent health and prevent failures that can result in downtimes. This study\nfocuses on gearbox monitoring but is applicable also to other subsystems. The\ncurrent state-of-the-art gearbox fault diagnosis algorithms rely on statistical\nor machine learning methods based on fault signatures that have been defined by\nhuman analysts. This has multiple disadvantages. Defining the fault signatures\nby human analysts is a time-intensive process that requires highly detailed\nknowledge of the gearbox composition. This effort needs to be repeated for\nevery new turbine, so it does not scale well with the increasing number of\nmonitored turbines, especially in fast growing portfolios. Moreover, fault\nsignatures defined by human analysts can result in biased and imprecise\ndecision boundaries that lead to imprecise and uncertain fault diagnosis\ndecisions. We present a novel accurate fault diagnosis method for\nvibration-monitored wind turbine components that overcomes these disadvantages.\nOur approach combines autonomous data-driven learning of fault signatures and\nhealth state classification based on convolutional neural networks and\nisolation forests. We demonstrate its performance with vibration measurements\nfrom two wind turbine gearboxes. Unlike the state-of-the-art methods, our\napproach does not require gearbox-type specific diagnosis expertise and is not\nrestricted to predefined frequencies or spectral ranges but can monitor the\nfull spectrum at once.",
    "descriptor": "",
    "authors": [
      "Angela Meyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13403"
  },
  {
    "id": "arXiv:2201.13405",
    "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
    "abstract": "Multilingual task-oriented dialogue (ToD) facilitates access to services and\ninformation for many (communities of) speakers. Nevertheless, the potential of\nthis technology is not fully realised, as current datasets for multilingual ToD\n- both for modular and end-to-end modelling - suffer from severe limitations.\n1) When created from scratch, they are usually small in scale and fail to cover\nmany possible dialogue flows. 2) Translation-based ToD datasets might lack\nnaturalness and cultural specificity in the target language. In this work, to\ntackle these limitations we propose a novel outline-based annotation process\nfor multilingual ToD datasets, where domain-specific abstract schemata of\ndialogue are mapped into natural language outlines. These in turn guide the\ntarget language annotators in writing a dialogue by providing instructions\nabout each turn's intents and slots. Through this process we annotate a new\nlarge-scale dataset for training and evaluation of multilingual and\ncross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset\n(termed COD) enables natural language understanding, dialogue state tracking,\nand end-to-end dialogue modelling and evaluation in 4 diverse languages:\nArabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative\nanalyses of COD versus an equivalent translation-based dataset demonstrate\nimprovements in data quality, unlocked by the outline-based approach. Finally,\nwe benchmark a series of state-of-the-art systems for cross-lingual ToD,\nsetting reference scores for future work and demonstrating that COD prevents\nover-inflated performance, typically met with prior translation-based ToD\ndatasets.",
    "descriptor": "",
    "authors": [
      "Olga Majewska",
      "Evgeniia Razumovskaia",
      "Edoardo Maria Ponti",
      "Ivan Vuli\u0107",
      "Anna Korhonen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.13405"
  },
  {
    "id": "arXiv:2201.13408",
    "title": "Extreme precipitation forecasting using attention augmented convolutions",
    "abstract": "Extreme precipitation wreaks havoc throughout the world, causing billions of\ndollars in damage and uprooting communities, ecosystems, and economies.\nAccurate extreme precipitation prediction allows more time for preparation and\ndisaster risk management for such extreme events. In this paper, we focus on\nshort-term extreme precipitation forecasting (up to a 12-hour ahead-of-time\nprediction) from a sequence of sea level pressure and zonal wind anomalies.\nAlthough existing machine learning approaches have shown promising results, the\nassociated model and climate uncertainties may reduce their reliability. To\naddress this issue, we propose a self-attention augmented convolution mechanism\nfor extreme precipitation forecasting, systematically combining attention\nscores with traditional convolutions to enrich feature data and reduce the\nexpected errors of the results. The proposed network architecture is further\nfused with a highway neural network layer to gain the benefits of unimpeded\ninformation flow across several layers. Our experimental results show that the\nframework outperforms classical convolutional models by 12%. The proposed\nmethod increases machine learning as a tool for gaining insights into the\nphysical causes of changing extremes, lowering uncertainty in future forecasts.",
    "descriptor": "",
    "authors": [
      "Weichen Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.13408"
  },
  {
    "id": "arXiv:2201.13410",
    "title": "Weisfeiler and Leman Go Infinite: Spectral and Combinatorial  Pre-Colorings",
    "abstract": "Graph isomorphism testing is usually approached via the comparison of graph\ninvariants. Two popular alternatives that offer a good trade-off between\nexpressive power and computational efficiency are combinatorial (i.e., obtained\nvia the Weisfeiler-Leman (WL) test) and spectral invariants. While the exact\npower of the latter is still an open question, the former is regularly\ncriticized for its limited power, when a standard configuration of uniform\npre-coloring is used. This drawback hinders the applicability of Message\nPassing Graph Neural Networks (MPGNNs), whose expressive power is upper bounded\nby the WL test. Relaxing the assumption of uniform pre-coloring, we show that\none can increase the expressive power of the WL test ad infinitum. Following\nthat, we propose an efficient pre-coloring based on spectral features that\nprovably increase the expressive power of the vanilla WL test. The above claims\nare accompanied by extensive synthetic and real data experiments. The code to\nreproduce our experiments is available at\nhttps://github.com/TPFI22/Spectral-and-Combinatorial",
    "descriptor": "",
    "authors": [
      "Or Feldman",
      "Amit Boyarski",
      "Shai Feldman",
      "Dani Kogan",
      "Avi Mendelson",
      "Chaim Baskin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.13410"
  },
  {
    "id": "arXiv:2201.13415",
    "title": "Towards Scaling Difference Target Propagation by Learning Backprop  Targets",
    "abstract": "The development of biologically-plausible learning algorithms is important\nfor understanding learning in the brain, but most of them fail to scale-up to\nreal-world tasks, limiting their potential as explanations for learning by real\nbrains. As such, it is important to explore learning algorithms that come with\nstrong theoretical guarantees and can match the performance of backpropagation\n(BP) on complex tasks. One such algorithm is Difference Target Propagation\n(DTP), a biologically-plausible learning algorithm whose close relation with\nGauss-Newton (GN) optimization has been recently established. However, the\nconditions under which this connection rigorously holds preclude layer-wise\ntraining of the feedback pathway synaptic weights (which is more biologically\nplausible). Moreover, good alignment between DTP weight updates and loss\ngradients is only loosely guaranteed and under very specific conditions for the\narchitecture being trained. In this paper, we propose a novel feedback weight\ntraining scheme that ensures both that DTP approximates BP and that layer-wise\nfeedback weight training can be restored without sacrificing any theoretical\nguarantees. Our theory is corroborated by experimental results and we report\nthe best performance ever achieved by DTP on CIFAR-10 and ImageNet 32$\\times$32",
    "descriptor": "",
    "authors": [
      "Maxence Ernoult",
      "Fabrice Normandin",
      "Abhinav Moudgil",
      "Sean Spinney",
      "Eugene Belilovsky",
      "Irina Rish",
      "Blake Richards",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.13415"
  },
  {
    "id": "arXiv:2201.13418",
    "title": "GParareal: A time-parallel ODE solver using Gaussian process emulation",
    "abstract": "Sequential numerical methods for integrating initial value problems (IVPs)\ncan be prohibitively expensive when high numerical accuracy is required over\nthe entire interval of integration. One remedy is to integrate in a parallel\nfashion, \"predicting\" the solution serially using a cheap (coarse) solver and\n\"correcting\" these values using an expensive (fine) solver that runs in\nparallel on a number of temporal subintervals. In this work, we propose a\ntime-parallel algorithm (GParareal) that solves IVPs by modelling the\ncorrection term, i.e. the difference between fine and coarse solutions, using a\nGaussian process emulator. This approach compares favourably with the classic\nparareal algorithm and we demonstrate, on a number of IVPs, that GParareal can\nconverge in fewer iterations than parareal, leading to an increase in parallel\nspeed-up. GParareal also manages to locate solutions to certain IVPs where\nparareal fails and has the additional advantage of being able to use archives\nof legacy solutions, e.g. solutions from prior runs of the IVP for different\ninitial conditions, to further accelerate convergence of the method - something\nthat existing time-parallel methods do not do.",
    "descriptor": "",
    "authors": [
      "Kamran Pentland",
      "Massimiliano Tamborrino",
      "T. J. Sullivan",
      "James Buchanan",
      "L. C. Appel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2201.13418"
  },
  {
    "id": "arXiv:2201.13419",
    "title": "Agnostic Learnability of Halfspaces via Logistic Loss",
    "abstract": "We investigate approximation guarantees provided by logistic regression for\nthe fundamental problem of agnostic learning of homogeneous halfspaces.\nPreviously, for a certain broad class of \"well-behaved\" distributions on the\nexamples, Diakonikolas et al. (2020) proved an $\\tilde{\\Omega}(\\textrm{OPT})$\nlower bound, while Frei et al. (2021) proved an\n$\\tilde{O}(\\sqrt{\\textrm{OPT}})$ upper bound, where $\\textrm{OPT}$ denotes the\nbest zero-one/misclassification risk of a homogeneous halfspace. In this paper,\nwe close this gap by constructing a well-behaved distribution such that the\nglobal minimizer of the logistic risk over this distribution only achieves\n$\\Omega(\\sqrt{\\textrm{OPT}})$ misclassification risk, matching the upper bound\nin (Frei et al., 2021). On the other hand, we also show that if we impose a\nradial-Lipschitzness condition in addition to well-behaved-ness on the\ndistribution, logistic regression on a ball of bounded radius reaches\n$\\tilde{O}(\\textrm{OPT})$ misclassification risk. Our techniques also show for\nany well-behaved distribution, regardless of radial Lipschitzness, we can\novercome the $\\Omega(\\sqrt{\\textrm{OPT}})$ lower bound for logistic loss simply\nat the cost of one additional convex optimization step involving the hinge loss\nand attain $\\tilde{O}(\\textrm{OPT})$ misclassification risk. This two-step\nconvex optimization algorithm is simpler than previous methods obtaining this\nguarantee, all of which require solving $O(\\log(1/\\textrm{OPT}))$ minimization\nproblems.",
    "descriptor": "",
    "authors": [
      "Ziwei Ji",
      "Kwangjun Ahn",
      "Pranjal Awasthi",
      "Satyen Kale",
      "Stefani Karp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13419"
  },
  {
    "id": "arXiv:2201.13425",
    "title": "Don't Change the Algorithm, Change the Data: Exploratory Data for  Offline Reinforcement Learning",
    "abstract": "Recent progress in deep learning has relied on access to large and diverse\ndatasets. Such data-driven progress has been less evident in offline\nreinforcement learning (RL), because offline RL data is usually collected to\noptimize specific target tasks limiting the data's diversity. In this work, we\npropose Exploratory data for Offline RL (ExORL), a data-centric approach to\noffline RL. ExORL first generates data with unsupervised reward-free\nexploration, then relabels this data with a downstream reward before training a\npolicy with offline RL. We find that exploratory data allows vanilla off-policy\nRL algorithms, without any offline-specific modifications, to outperform or\nmatch state-of-the-art offline RL algorithms on downstream tasks. Our findings\nsuggest that data generation is as important as algorithmic advances for\noffline RL and hence requires careful consideration from the community.",
    "descriptor": "",
    "authors": [
      "Denis Yarats",
      "David Brandfonbrener",
      "Hao Liu",
      "Michael Laskin",
      "Pieter Abbeel",
      "Alessandro Lazaric",
      "Lerrel Pinto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13425"
  },
  {
    "id": "arXiv:2201.13427",
    "title": "Fuzzy Segmentations of a String",
    "abstract": "This article discusses a particular case of the data clustering problem,\nwhere it is necessary to find groups of adjacent text segments of the\nappropriate length that match a fuzzy pattern represented as a sequence of\nfuzzy properties. To solve this problem, a heuristic algorithm for finding a\nsufficiently large number of solutions is proposed. The key idea of the\nproposed algorithm is the use of the prefix structure to track the process of\nmapping text segments to fuzzy properties.\nAn important special case of the text segmentation problem is the fuzzy\nstring matching problem, when adjacent text segments have unit length and,\naccordingly, the fuzzy pattern is a sequence of fuzzy properties of text\ncharacters. It is proven that the heuristic segmentation algorithm in this case\nfinds all text segments that match the fuzzy pattern.\nFinally, we consider the problem of a best segmentation of the entire text\nbased on a fuzzy pattern, which is solved using the dynamic programming method.\nKeywords: fuzzy clustering, fuzzy string matching, approximate string\nmatching",
    "descriptor": "",
    "authors": [
      "Armen Kostanyan",
      "Arevik Harmandayan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.13427"
  },
  {
    "id": "arXiv:2201.13428",
    "title": "PokeRRT: Poking as a Skill and Failure Recovery Tactic for Planar  Non-Prehensile Manipulation",
    "abstract": "In this work, we introduce PokeRRT, a novel motion planning algorithm that\ndemonstrates poking as an effective non-prehensile manipulation skill to enable\nfast manipulation of objects and increase the size of a robot's reachable\nworkspace. We showcase poking as a failure recovery tactic used synergistically\nwith pick-and-place for resiliency in cases where pick-and-place initially\nfails or is unachievable. Our experiments demonstrate the efficiency of the\nproposed framework in planning object trajectories using poking manipulation in\nuncluttered and cluttered environments. In addition to quantitatively and\nqualitatively demonstrating the adaptability of PokeRRT to different scenarios\nin both simulation and real-world settings, our results show the advantages of\npoking over pushing and grasping in terms of success rate and task time.",
    "descriptor": "",
    "authors": [
      "Anuj Pasricha",
      "Yi-Shiuan Tung",
      "Bradley Hayes",
      "Alessandro Roncone"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.13428"
  },
  {
    "id": "arXiv:2201.13429",
    "title": "Constrained Density Matching and Modeling for Cross-lingual Alignment of  Contextualized Representations",
    "abstract": "Multilingual representations pre-trained with monolingual data exhibit\nconsiderably unequal task performances across languages. Previous studies\naddress this challenge with resource-intensive contextualized alignment, which\nassumes the availability of large parallel data, thereby leaving\nunder-represented language communities behind. In this work, we attribute the\ndata hungriness of previous alignment techniques to two limitations: (i) the\ninability to sufficiently leverage data and (ii) these techniques are not\ntrained properly. To address these issues, we introduce supervised and\nunsupervised density-based approaches named Real-NVP and GAN-Real-NVP, driven\nby Normalizing Flow, to perform alignment, both dissecting the alignment of\nmultilingual subspaces into density matching and density modeling. We\ncomplement these approaches with our validation criteria in order to guide the\ntraining process. Our experiments encompass 16 alignments, including our\napproaches, evaluated across 6 language pairs, synthetic data and 4 NLP tasks.\nWe demonstrate the effectiveness of our approaches in the scenarios of limited\nand no parallel data. First, our supervised approach trained on 20k parallel\ndata mostly surpasses Joint-Align and InfoXLM trained on much larger parallel\ndata. Second, parallel data can be removed without sacrificing performance when\nintegrating our unsupervised approach in our bootstrapping procedure, which is\ntheoretically motivated to enforce equality of multilingual subspaces.\nMoreover, we demonstrate the advantages of validation criteria over validation\ndata for guiding supervised training. Our code is available at\n\\url{https://github.com/AIPHES/Real-NVP}.",
    "descriptor": "",
    "authors": [
      "Wei Zhao",
      "Steffen Eger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.13429"
  },
  {
    "id": "arXiv:2201.13433",
    "title": "Third Time's the Charm? Image and Video Editing with StyleGAN3",
    "abstract": "StyleGAN is arguably one of the most intriguing and well-studied generative\nmodels, demonstrating impressive performance in image generation, inversion,\nand manipulation. In this work, we explore the recent StyleGAN3 architecture,\ncompare it to its predecessor, and investigate its unique advantages, as well\nas drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained\non unaligned data, one can still use aligned data for training, without\nhindering the ability to generate unaligned imagery. Next, our analysis of the\ndisentanglement of the different latent spaces of StyleGAN3 indicates that the\ncommonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,\nunderscoring the benefits of using the StyleSpace for fine-grained editing.\nConsidering image inversion, we observe that existing encoder-based techniques\nstruggle when trained on unaligned data. We therefore propose an encoding\nscheme trained solely on aligned data, yet can still invert unaligned images.\nFinally, we introduce a novel video inversion and editing workflow that\nleverages the capabilities of a fine-tuned StyleGAN3 generator to reduce\ntexture sticking and expand the field of view of the edited video.",
    "descriptor": "\nComments: Project page available at this https URL\n",
    "authors": [
      "Yuval Alaluf",
      "Or Patashnik",
      "Zongze Wu",
      "Asif Zamir",
      "Eli Shechtman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13433"
  },
  {
    "id": "arXiv:2201.13444",
    "title": "Boundary Defense Against Black-box Adversarial Attacks",
    "abstract": "Black-box adversarial attacks generate adversarial samples via iterative\noptimizations using repeated queries. Defending deep neural networks against\nsuch attacks has been challenging. In this paper, we propose an efficient\nBoundary Defense (BD) method which mitigates black-box attacks by exploiting\nthe fact that the adversarial optimizations often need samples on the\nclassification boundary. Our method detects the boundary samples as those with\nlow classification confidence and adds white Gaussian noise to their logits.\nThe method's impact on the deep network's classification accuracy is analyzed\ntheoretically. Extensive experiments are conducted and the results show that\nthe BD method can reliably defend against both soft and hard label black-box\nattacks. It outperforms a list of existing defense methods. For IMAGENET\nmodels, by adding zero-mean white Gaussian noise with standard deviation 0.1 to\nlogits when the classification confidence is less than 0.3, the defense reduces\nthe attack success rate to almost 0 while limiting the classification accuracy\ndegradation to around 1 percent.",
    "descriptor": "",
    "authors": [
      "Manjushree B. Aithal",
      "Xiaohua Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13444"
  },
  {
    "id": "arXiv:2201.13448",
    "title": "Warmth and competence in human-agent cooperation",
    "abstract": "Interaction and cooperation with humans are overarching aspirations of\nartificial intelligence (AI) research. Recent studies demonstrate that AI\nagents trained with deep reinforcement learning are capable of collaborating\nwith humans. These studies primarily evaluate human compatibility through\n\"objective\" metrics such as task performance, obscuring potential variation in\nthe levels of trust and subjective preference that different agents garner. To\nbetter understand the factors shaping subjective preferences in human-agent\ncooperation, we train deep reinforcement learning agents in Coins, a two-player\nsocial dilemma. We recruit participants for a human-agent cooperation study and\nmeasure their impressions of the agents they encounter. Participants'\nperceptions of warmth and competence predict their stated preferences for\ndifferent agents, above and beyond objective performance metrics. Drawing\ninspiration from social science and biology research, we subsequently implement\na new \"partner choice\" framework to elicit revealed preferences: after playing\nan episode with an agent, participants are asked whether they would like to\nplay the next round with the same agent or to play alone. As with stated\npreferences, social perception better predicts participants' revealed\npreferences than does objective performance. Given these results, we recommend\nhuman-agent interaction researchers routinely incorporate the measurement of\nsocial perception and subjective preferences into their studies.",
    "descriptor": "\nComments: Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022)\n",
    "authors": [
      "Kevin R. McKee",
      "Xuechunzi Bai",
      "Susan T. Fiske"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13448"
  },
  {
    "id": "arXiv:2201.12362",
    "title": "Physics-informed neural networks to learn cardiac fiber orientation from  multiple electroanatomical maps",
    "abstract": "We propose FiberNet, a method to estimate in-vivo the cardiac fiber\narchitecture of the human atria from multiple catheter recordings of the\nelectrical activation. Cardiac fibers play a central rolein the\nelectro-mechanical function of the heart, yet they aredifficult to determine\nin-vivo, and hence rarely truly patient-specificin existing cardiac\nmodels.FiberNet learns the fibers arrangement by solvingan inverse problem with\nphysics-informed neural networks. The inverse problem amounts to identifyingthe\nconduction velocity tensor of a cardiac propagation modelfrom a set of sparse\nactivation maps. The use of multiple mapsenables the simultaneous\nidentification of all the componentsof the conduction velocity tensor,\nincluding the local fiber angle.We extensively test FiberNet on synthetic 2-D\nand 3-D examples, diffusion tensor fibers, and a patient-specific case. We show\nthat 3 maps are sufficient to accurately capture the fibers, also in\nthepresence of noise. With fewer maps, the role of regularization\nbecomesprominent. Moreover, we show that the fitted model can robustlyreproduce\nunseen activation maps. We envision that FiberNet will help the creation of\npatient-specific models for personalized medicine.The full code is available at\nthis http URL",
    "descriptor": "\nComments: 28 pages, 11 figures\n",
    "authors": [
      "Carlos Ruiz Herrera",
      "Thomas Grandits",
      "Gernot Plank",
      "Paris Perdikaris",
      "Francisco Sahli Costabal",
      "Simone Pezzuto"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2201.12362"
  },
  {
    "id": "arXiv:2201.12389",
    "title": "DoubleU-Net++: Architecture with Exploit Multiscale Features for  Vertebrae Segmentation",
    "abstract": "Accurate segmentation of the vertebra is an important prerequisite in various\nmedical applications (E.g. tele surgery) to assist surgeons. Following the\nsuccessful development of deep neural networks, recent studies have focused on\nthe essential rule of vertebral segmentation. Prior works contain a large\nnumber of parameters, and their segmentation is restricted to only one view.\nInspired by DoubleU-Net, we propose a novel model named DoubleU-Net++ in which\nDensNet as feature extractor, special attention module from Convolutional Block\nAttention on Module (CBAM) and, Pyramid Squeeze Attention (PSA) module are\nemployed to improve extracted features. We evaluate our proposed model on three\ndifferent views (sagittal, coronal, and axial) of VerSe2020 and xVertSeg\ndatasets. Compared with state-of-the-art studies, our architecture is trained\nfaster and achieves higher precision, recall, and F1-score as evaluation\n(imporoved by 4-6%) and the result of above 94% for sagittal view and above 94%\nfor both coronal view and above 93% axial view were gained for VerSe2020\ndataset, respectively. Also, for xVertSeg dataset, we achieved precision,\nrecall,and F1-score of above 97% for sagittal view, above 93% for coronal view\n,and above 96% for axial view.",
    "descriptor": "",
    "authors": [
      "Simindokht Jahangard",
      "Mahdi Bonyani",
      "Abbas Khosravi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12389"
  },
  {
    "id": "arXiv:2201.12419",
    "title": "FastFlows: Flow-Based Models for Molecular Graph Generation",
    "abstract": "We propose a framework using normalizing-flow based models, SELF-Referencing\nEmbedded Strings, and multi-objective optimization that efficiently generates\nsmall molecules. With an initial training set of only 100 small molecules,\nFastFlows generates thousands of chemically valid molecules in seconds. Because\nof the efficient sampling, substructure filters can be applied as desired to\neliminate compounds with unreasonable moieties. Using easily computable and\nlearned metrics for druglikeness, synthetic accessibility, and synthetic\ncomplexity, we perform a multi-objective optimization to demonstrate how\nFastFlows functions in a high-throughput virtual screening context. Our model\nis significantly simpler and easier to train than autoregressive molecular\ngenerative models, and enables fast generation and identification of druglike,\nsynthesizable molecules.",
    "descriptor": "\nComments: 7 pages, 4 figures, ELLIS Machine Learning for Molecule Discovery Workshop 2021\n",
    "authors": [
      "Nathan C. Frey",
      "Vijay Gadepally",
      "Bharath Ramsundar"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12419"
  },
  {
    "id": "arXiv:2201.12450",
    "title": "Finding fault-tolerant Clifford circuits using SMT solvers",
    "abstract": "Universal fault-tolerant quantum computers will require the use of efficient\nprotocols to implement encoded operations necessary in the execution of\nalgorithms. In this work, we show how SMT solvers can be used to automate the\nconstruction of Clifford circuits with certain fault-tolerance properties and\napply our techniques to a fault-tolerant magic state preparation protocol. Part\nof the protocol requires converting magic states encoded in the color code to\nmagic states encoded in the surface code. Since the teleportation step involves\ndecoding a color code merged with a surface code, we develop a new decoding\nalgorithm applicable to such codes.",
    "descriptor": "\nComments: 22 pages, 8 figures\n",
    "authors": [
      "Noah Shutty",
      "Christopher Chamberland"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2201.12450"
  },
  {
    "id": "arXiv:2201.12475",
    "title": "Retroformer: Pushing the Limits of Interpretable End-to-end  Retrosynthesis Transformer",
    "abstract": "Retrosynthesis prediction is one of the fundamental challenges in organic\nsynthesis. The task is to predict the reactants given a core product. With the\nadvancement of machine learning, computer-aided synthesis planning has gained\nincreasing interest. Numerous methods were proposed to solve this problem with\ndifferent levels of dependency on additional chemical knowledge. In this paper,\nwe propose Retroformer, a novel Transformer-based architecture for\nretrosynthesis prediction without relying on any cheminformatics tools for\nmolecule editing. Via the proposed local attention head, the model can jointly\nencode the molecular sequence and graph, and efficiently exchange information\nbetween the local reactive region and the global reaction context. Retroformer\nreaches the new state-of-the-art accuracy for the end-to-end template-free\nretrosynthesis, and improves over many strong baselines on better molecule and\nreaction validity. In addition, its generative procedure is highly\ninterpretable and controllable. Overall, Retroformer pushes the limits of the\nreaction reasoning ability of deep generative models.",
    "descriptor": "",
    "authors": [
      "Yue Wan",
      "Benben Liao",
      "Chang-Yu Hsieh",
      "Shengyu Zhang"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12475"
  },
  {
    "id": "arXiv:2201.12535",
    "title": "Validation and Generalizability of Self-Supervised Image Reconstruction  Methods for Undersampled MRI",
    "abstract": "Purpose: To investigate aspects of the validation of self-supervised\nalgorithms for reconstruction of undersampled MR images: quantitative\nevaluation of prospective reconstructions, potential differences between\nprospective and retrospective reconstructions, suitability of commonly used\nquantitative metrics, and generalizability.\nTheory and Methods: Two self-supervised algorithms based on self-supervised\ndenoising and neural network image priors were investigated. These methods are\ncompared to a least squares fitting and a compressed sensing reconstruction\nusing in-vivo and phantom data. Their generalizability was tested with\nprospectively under-sampled data from experimental conditions different to the\ntraining.\nResults: Prospective reconstructions can exhibit significant distortion\nrelative to retrospective reconstructions/ground truth. Pixel-wise quantitative\nmetrics may not capture differences in perceptual quality accurately, in\ncontrast to a perceptual metric. All methods showed potential for\ngeneralization; generalizability is more affected by changes in\nanatomy/contrast than other changes. No-reference image metrics correspond well\nwith human rating of image quality for studying generalizability. Compressed\nSensing and learned denoising perform similarly well on all data.\nConclusion: Self-supervised methods show promising results for accelerating\nimage reconstruction in clinical routines. Nonetheless, more work is required\nto investigate standardized methods to validate reconstruction algorithms for\nfuture clinical use.",
    "descriptor": "\nComments: Submitted to Magnetic Resonance in Medicine\n",
    "authors": [
      "Thomas Yu",
      "Tom Hilbert",
      "Gian Franco Piredda",
      "Arun Joseph",
      "Gabriele Bonanno",
      "Salim Zenkhri",
      "Patrick Omoumi",
      "Meritxell Bach Cuadra",
      "Erick Jorge Canales-Rodr\u00edguez",
      "Tobias Kober",
      "Jean-Philippe Thiran"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.12535"
  },
  {
    "id": "arXiv:2201.12550",
    "title": "Recommender System Expedited Quantum Control Optimization",
    "abstract": "Quantum control optimization algorithms are routinely used to generate\noptimal quantum gates or efficient quantum state transfers. However, there are\ntwo main challenges in designing efficient optimization algorithms, namely\novercoming the sensitivity to local optima and improving the computational\nspeed. The former challenge can be dealt with by designing hybrid algorithms,\nsuch as a combination of gradient and simulated annealing methods. Here, we\npropose and demonstrate the use of a machine learning method, specifically the\nrecommender system (RS), to deal with the latter challenge of enhancing\ncomputational efficiency. We first describe ways to set up a rating matrix\ninvolving gradients or gate fidelities. We then establish that RS can rapidly\nand accurately predict elements of a sparse rating matrix. Using this approach,\nwe expedite a gradient ascent based quantum control optimization, namely GRAPE\nand demonstrate the faster performance for up to 8 qubits. Finally, we describe\nand implement the enhancement of the computational speed of a hybrid algorithm,\nnamely SAGRAPE.",
    "descriptor": "\nComments: 7 pages, 4 figures, 2 tables\n",
    "authors": [
      "Priya Batra",
      "M. Harshant Ram",
      "T. S. Mahesh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12550"
  },
  {
    "id": "arXiv:2201.12557",
    "title": "Polyphonic audio event detection: multi-label or multi-class multi-task  classification problem?",
    "abstract": "Polyphonic events are the main error source of audio event detection (AED)\nsystems. In deep-learning context, the most common approach to deal with event\noverlaps is to treat the AED task as a multi-label classification problem. By\ndoing this, we inherently consider multiple one-vs.-rest classification\nproblems, which are jointly solved by a single (i.e. shared) network. In this\nwork, to better handle polyphonic mixtures, we propose to frame the task as a\nmulti-class classification problem by considering each possible label\ncombination as one class. To circumvent the large number of arising classes due\nto combinatorial explosion, we divide the event categories into multiple groups\nand construct a multi-task problem in a divide-and-conquer fashion, where each\nof the tasks is a multi-class classification problem. A network architecture is\nthen devised for multi-class multi-task modelling. The network is composed of a\nbackbone subnet and multiple task-specific subnets. The task-specific subnets\nare designed to learn time-frequency and channel attention masks to extract\nfeatures for the task at hand from the common feature maps learned by the\nbackbone. Experiments on the TUT-SED-Synthetic-2016 with high degree of event\noverlap show that the proposed approach results in more favorable performance\nthan the common multi-label approach.",
    "descriptor": "\nComments: This paper has been accepted to IEEE ICASSP 2022\n",
    "authors": [
      "Huy Phan",
      "Thi Ngoc Tho Nguyen",
      "Philipp Koch",
      "Alfred Mertins"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.12557"
  },
  {
    "id": "arXiv:2201.12570",
    "title": "AntBO: Towards Real-World Automated Antibody Design with Combinatorial  Bayesian Optimisation",
    "abstract": "Antibodies are canonically Y-shaped multimeric proteins capable of highly\nspecific molecular recognition. The CDRH3 region located at the tip of variable\nchains of an antibody dominates antigen-binding specificity. Therefore, it is a\npriority to design optimal antigen-specific CDRH3 regions to develop\ntherapeutic antibodies to combat harmful pathogens. However, the combinatorial\nnature of CDRH3 sequence space makes it impossible to search for an optimal\nbinding sequence exhaustively and efficiently, especially not experimentally.\nHere, we present AntBO: a Combinatorial Bayesian Optimisation framework\nenabling efficient in silico design of the CDRH3 region. Ideally, antibodies\nshould bind to their target antigen and be free from any harmful outcomes.\nTherefore, we introduce the CDRH3 trust region that restricts the search to\nsequences with feasible developability scores. To benchmark AntBO, we use the\nAbsolut! software suite as a black-box oracle because it can score the target\nspecificity and affinity of designed antibodies in silico in an unconstrained\nfashion. The results across 188 antigens demonstrate the benefit of AntBO in\ndesigning CDRH3 regions with diverse biophysical properties. In under 200\nprotein designs, AntBO can suggest antibody sequences that outperform the best\nbinding sequence drawn from 6.9 million experimentally obtained CDRH3s and a\ncommonly used genetic algorithm baseline. Additionally, AntBO finds very-high\naffinity CDRH3 sequences in only 38 protein designs whilst requiring no domain\nknowledge. We conclude AntBO brings automated antibody design methods closer to\nwhat is practically viable for in vitro experimentation.",
    "descriptor": "",
    "authors": [
      "Asif Khan",
      "Alexander I. Cowen-Rivers",
      "Derrick-Goh-Xin Deik",
      "Antoine Grosnit",
      "Kamil Dreczkowski",
      "Philippe A. Robert",
      "Victor Greiff",
      "Rasul Tutunov",
      "Dany Bou-Ammar",
      "Jun Wang",
      "Haitham Bou-Ammar"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12570"
  },
  {
    "id": "arXiv:2201.12582",
    "title": "Radio labelling of two-branch trees",
    "abstract": "A radio labelling of a graph $G$ is a mapping $f : V(G) \\rightarrow \\{0, 1,\n2,\\ldots\\}$ such that $|f(u)-f(v)| \\geq diam(G) + 1 - d(u,v)$ for every pair of\ndistinct vertices $u,v$ of $G$, where $diam(G)$ is the diameter of $G$ and\n$d(u,v)$ is the distance between $u$ and $v$ in $G$. The radio number $rn(G)$\nof $G$ is the smallest integer $k$ such that $G$ admits a radio labelling $f$\nwith $\\max\\{f(v):v \\in V(G)\\} = k$. The weight of a tree $T$ from a vertex $v\n\\in V(T)$ is the sum of the distances in $T$ from $v$ to all other vertices,\nand a vertex of $T$ achieving the minimum weight is called a weight center of\n$T$. It is known that any tree has one or two weight centers. A tree is called\na two-branch tree if the removal of all its weight centers results in a forest\nwith exactly two components. In this paper we obtain a sharp lower bound for\nthe radio number of two-branch trees which improves a known lower bound for\ngeneral trees. We also give a necessary and sufficient condition for this\nimproved lower bound to be achieved. Using these results, we determine the\nradio number of two families of level-wise regular two-branch trees.",
    "descriptor": "",
    "authors": [
      "Devsi Bantva",
      "Samir Vaidya",
      "Sanming Zhou"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2201.12582"
  },
  {
    "id": "arXiv:2201.12584",
    "title": "Convolutional Filtering in Simplicial Complexes",
    "abstract": "This paper proposes convolutional filtering for data whose structure can be\nmodeled by a simplicial complex (SC). SCs are mathematical tools that not only\ncapture pairwise relationships as graphs but account also for higher-order\nnetwork structures. These filters are built by following the shift-and-sum\nprinciple of the convolution operation and rely on the Hodge-Laplacians to\nshift the signal within the simplex. But since in SCs we have also\ninter-simplex coupling, we use the incidence matrices to transfer the signal in\nadjacent simplices and build a filter bank to jointly filter signals from\ndifferent levels. We prove some interesting properties for the proposed filter\nbank, including permutation and orientation equivariance, a computational\ncomplexity that is linear in the SC dimension, and a spectral interpretation\nusing the simplicial Fourier transform. We illustrate the proposed approach\nwith numerical experiments.",
    "descriptor": "\nComments: 5 pages, 2 figures, accepted in ICASSP 2022\n",
    "authors": [
      "Elvin Isufi",
      "Maosheng Yang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12584"
  },
  {
    "id": "arXiv:2201.12589",
    "title": "FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine  Transform Loss",
    "abstract": "The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in the diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is impractical or even\nluxurious, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. Previously, the misaligned\nunpaired neuroimaging data (termed as MUD) are generally treated as noisy\nlabel. However, such a noisy label-based method could not work very well when\nmisaligned data occurs distortions severely, for example, different angles of\nrotation. In this paper, we propose a novel federated self-supervised learning\n(FedMed) for brain image synthesis. An affine transform loss (ATL) was\nformulated to make use of severely distorted images without violating privacy\nlegislation for the hospital. We then introduce a new data augmentation\nprocedure for self-supervised training and fed it into three auxiliary heads,\nnamely auxiliary rotation, auxiliary translation, and auxiliary scaling heads.\nThe proposed method demonstrates advanced performance in both the quality of\nsynthesized results under a severely misaligned and unpaired data setting, and\nbetter stability than other GAN-based algorithms. The proposed method also\nreduces the demand for deformable registration while encouraging to realize the\nusage of those misaligned and unpaired data. Experimental results verify the\noutstanding ability of our learning paradigm compared to other state-of-the-art\napproaches. Our code is available on the website:\nhttps://github.com/FedMed-Meta/FedMed-ATL",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2201.08953\n",
    "authors": [
      "Guoyang Xie",
      "Jinbao Wang",
      "Yawen Huang",
      "Yefeng Zheng",
      "Feng Zheng",
      "Yaochu Jin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12589"
  },
  {
    "id": "arXiv:2201.12610",
    "title": "On the generalized Helly property of hypergraphs, cliques, and bicliques",
    "abstract": "A family of sets is $(p,q)$-intersecting if every nonempty subfamily of $p$\nor fewer sets has at least $q$ elements in its total intersection. A family of\nsets has the $(p,q)$-Helly property if every nonempty $(p,q)$-intersecting\nsubfamily has total intersection of cardinality at least $q$. The $(2,1)$-Helly\nproperty is the usual Helly property. A hypergraph is $(p,q)$-Helly if its edge\nfamily has the $(p,q)$-Helly property and hereditary $(p,q)$-Helly if each of\nits subhypergraphs has the $(p,q)$-Helly property. A graph is\n$(p,q)$-clique-Helly if the family of its maximal cliques has the $(p,q)$-the\nHelly property and hereditary $(p,q)$-clique-Helly if each of its induced\nsubgraphs is $(p,q)$-clique-Helly. The classes of $(p,q)$-biclique-Helly and\nhereditary $(p,q)$-biclique-Helly graphs are defined analogously. We prove\nseveral characterizations of hereditary $(p,q)$-Helly hypergraphs, including\none by minimal forbidden partial subhypergraphs. We give an improved time bound\nfor the recognition of $(p,q)$-Helly hypergraphs for each fixed $q$ and show\nthat the recognition of hereditary $(p,q)$-Helly hypergraphs can be solved in\npolynomial time if $p$ and $q$ are fixed but co-NP-complete if $p$ is part of\nthe input. In addition, we generalize to $(p,q)$-clique-Helly graphs the\ncharacterization of $p$-clique-Helly graphs in terms of expansions and give\ndifferent characterizations of hereditary $(p,q)$-clique-Helly graphs,\nincluding one by forbidden induced subgraphs. We give an improvement on the\ntime bound for the recognition of $(p,q)$-clique-Helly graphs and prove that\nthe recognition problem of hereditary $(p,q)$-clique-Helly graphs is\npolynomial-time solvable for $p$ and $q$ fixed but NP-hard if $p$ or $q$ is\npart of the input. Finally, we provide different characterizations, give\nrecognition algorithms, and prove hardness results for (hereditary)\n$(p,q)$-biclique-Helly graphs.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Mitre C. Dourado",
      "Luciano N. Grippo",
      "Mart\u00edn D. Safe"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2201.12610"
  },
  {
    "id": "arXiv:2201.12611",
    "title": "Learning Stochastic Graph Neural Networks with Constrained Variance",
    "abstract": "Stochastic graph neural networks (SGNNs) are information processing\narchitectures that learn representations from data over random graphs. SGNNs\nare trained with respect to the expected performance, which comes with no\nguarantee about deviations of particular output realizations around the optimal\nexpectation. To overcome this issue, we propose a variance-constrained\noptimization problem for SGNNs, balancing the expected performance and the\nstochastic deviation. An alternating primal-dual learning procedure is\nundertaken that solves the problem by updating the SGNN parameters with\ngradient descent and the dual variable with gradient ascent. To characterize\nthe explicit effect of the variance-constrained learning, we conduct a\ntheoretical analysis on the variance of the SGNN output and identify a\ntrade-off between the stochastic robustness and the discrimination power. We\nfurther analyze the duality gap of the variance-constrained optimization\nproblem and the converging behavior of the primal-dual learning procedure. The\nformer indicates the optimality loss induced by the dual transformation and the\nlatter characterizes the limiting error of the iterative algorithm, both of\nwhich guarantee the performance of the variance-constrained learning. Through\nnumerical simulations, we corroborate our theoretical findings and observe a\nstrong expected performance with a controllable standard deviation.",
    "descriptor": "",
    "authors": [
      "Zhan Gao",
      "Elvin Isufi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12611"
  },
  {
    "id": "arXiv:2201.12634",
    "title": "Deep Task-Based Analog-to-Digital Conversion",
    "abstract": "Analog-to-digital converters (ADCs) allow physical signals to be processed\nusing digital hardware. Their conversion consists of two stages: Sampling,\nwhich maps a continuous-time signal into discrete-time, and quantization, i.e.,\nrepresenting the continuous-amplitude quantities using a finite number of bits.\nADCs typically implement generic uniform conversion mappings that are ignorant\nof the task for which the signal is acquired, and can be costly when operating\nin high rates and fine resolutions. In this work we design task-oriented ADCs\nwhich learn from data how to map an analog signal into a digital representation\nsuch that the system task can be efficiently carried out. We propose a model\nfor sampling and quantization that facilitates the learning of non-uniform\nmappings from data. Based on this learnable ADC mapping, we present a mechanism\nfor optimizing a hybrid acquisition system comprised of analog combining,\ntunable ADCs with fixed rates, and digital processing, by jointly learning its\ncomponents end-to-end. Then, we show how one can exploit the representation of\nhybrid acquisition systems as deep network to optimize the sampling rate and\nquantization rate given the task by utilizing Bayesian meta-learning\ntechniques. We evaluate the proposed deep task-based ADC in two case studies:\nthe first considers symbol detection in multi-antenna digital receivers, where\nmultiple analog signals are simultaneously acquired in order to recover a set\nof discrete information symbols. The second application is the beamforming of\nanalog channel data acquired in ultrasound imaging. Our numerical results\ndemonstrate that the proposed approach achieves performance which is comparable\nto operating with high sampling rates and fine resolution quantization, while\noperating with reduced overall bit rate.",
    "descriptor": "",
    "authors": [
      "Nir Shlezinger",
      "Ariel Amar",
      "Ben Luijten",
      "Ruud J. G. van Sloun",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12634"
  },
  {
    "id": "arXiv:2201.12655",
    "title": "Error Rates for Kernel Classification under Source and Capacity  Conditions",
    "abstract": "In this manuscript, we consider the problem of kernel classification under\nthe Gaussian data design, and under source and capacity assumptions on the\ndataset. While the decay rates of the prediction error have been extensively\nstudied under much more generic assumptions for kernel ridge regression,\nderiving decay rates for the classification problem has been hitherto\nconsidered a much more challenging task. In this work we leverage recent\nanalytical results for learning curves of linear classification with generic\nloss function to derive the rates of decay of the misclassification\n(prediction) error with the sample complexity for two standard classification\nsettings, namely margin-maximizing Support Vector Machines (SVM) and ridge\nclassification. Using numerical and analytical arguments, we derive the error\nrates as a function of the source and capacity coefficients, and contrast the\ntwo methods.",
    "descriptor": "",
    "authors": [
      "Hugo Cui",
      "Bruno Loureiro",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12655"
  },
  {
    "id": "arXiv:2201.12656",
    "title": "Few-Shot Transfer Learning for Device-Free Fingerprinting Indoor  Localization",
    "abstract": "Device-free wireless indoor localization is an essential technology for the\nInternet of Things (IoT), and fingerprint-based methods are widely used. A\ncommon challenge to fingerprint-based methods is data collection and labeling.\nThis paper proposes a few-shot transfer learning system that uses only a small\namount of labeled data from the current environment and reuses a large amount\nof existing labeled data previously collected in other environments, thereby\nsignificantly reducing the data collection and labeling cost for localization\nin each new environment. The core method lies in graph neural network (GNN)\nbased few-shot transfer learning and its modifications. Experimental results\nconducted on real-world environments show that the proposed system achieves\ncomparable performance to a convolutional neural network (CNN) model, with 40\ntimes fewer labeled data.",
    "descriptor": "\nComments: IEEE International Conference on Communications (ICC) 2022\n",
    "authors": [
      "Bing-Jia Chen",
      "Ronald Y. Chang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12656"
  },
  {
    "id": "arXiv:2201.12669",
    "title": "Identification of MIMO Wiener-type Koopman Models for Data-Driven Model  Reduction using Deep Learning",
    "abstract": "We use Koopman theory to develop a data-driven nonlinear model reduction and\nidentification strategy for multiple-input multiple-output (MIMO) input-affine\ndynamical systems. While the present literature has focused on linear and\nbilinear Koopman models, we derive and use a Wiener-type Koopman formulation.\nWe discuss that the Wiener structure is particularly suitable for model\nreduction, and can be naturally derived from Koopman theory. Moreover, the\nWiener block-structure unifies the mathematical simplicity of linear dynamical\nblocks and the accuracy of bilinear dynamics. We present a Koopman\ndeep-learning strategy combining autoencoders and linear dynamics that\ngenerates low-order surrogate models of MIMO Wiener type. In three case\nstudies, we apply our framework for identification and reduction of a system\nwith input multiplicity, a chemical reactor and a high-purity distillation\ncolumn. We compare the prediction performance of the identified Wiener models\nto linear and bilinear Koopman models. We observe the highest accuracy and\nstrongest model reduction capabilities of low-order Wiener-type Koopman models,\nmaking them promising for control.",
    "descriptor": "",
    "authors": [
      "Jan C. Schulze",
      "Danimir T. Doncevic",
      "Alexander Mitsos"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12669"
  },
  {
    "id": "arXiv:2201.12682",
    "title": "Geometry- and Accuracy-Preserving Random Forest Proximities",
    "abstract": "Random forests are considered one of the best out-of-the-box classification\nand regression algorithms due to their high level of predictive performance\nwith relatively little tuning. Pairwise proximities can be computed from a\ntrained random forest which measure the similarity between data points relative\nto the supervised task. Random forest proximities have been used in many\napplications including the identification of variable importance, data\nimputation, outlier detection, and data visualization. However, existing\ndefinitions of random forest proximities do not accurately reflect the data\ngeometry learned by the random forest. In this paper, we introduce a novel\ndefinition of random forest proximities called Random Forest-Geometry- and\nAccuracy-Preserving proximities (RF-GAP). We prove that the proximity-weighted\nsum (regression) or majority vote (classification) using RF-GAP exactly match\nthe out-of-bag random forest prediction, thus capturing the data geometry\nlearned by the random forest. We empirically show that this improved geometric\nrepresentation outperforms traditional random forest proximities in tasks such\nas data imputation and provides outlier detection and visualization results\nconsistent with the learned data geometry.",
    "descriptor": "",
    "authors": [
      "Jake S. Rhodes",
      "Adele Cutler",
      "Kevin R. Moon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2201.12682"
  },
  {
    "id": "arXiv:2201.12683",
    "title": "A Priori Denoising Strategies for Sparse Identification of Nonlinear  Dynamical Systems: A Comparative Study",
    "abstract": "In recent years, identification of nonlinear dynamical systems from data has\nbecome increasingly popular. Sparse regression approaches, such as Sparse\nIdentification of Nonlinear Dynamics (SINDy), fostered the development of novel\ngoverning equation identification algorithms assuming the state variables are\nknown a priori and the governing equations lend themselves to sparse, linear\nexpansions in a (nonlinear) basis of the state variables. In the context of the\nidentification of governing equations of nonlinear dynamical systems, one faces\nthe problem of identifiability of model parameters when state measurements are\ncorrupted by noise. Measurement noise affects the stability of the recovery\nprocess yielding incorrect sparsity patterns and inaccurate estimation of\ncoefficients of the governing equations. In this work, we investigate and\ncompare the performance of several local and global smoothing techniques to a\npriori denoise the state measurements and numerically estimate the state\ntime-derivatives to improve the accuracy and robustness of two sparse\nregression methods to recover governing equations: Sequentially Thresholded\nLeast Squares (STLS) and Weighted Basis Pursuit Denoising (WBPDN) algorithms.\nWe empirically show that, in general, global methods, which use the entire\nmeasurement data set, outperform local methods, which employ a neighboring data\nsubset around a local point. We additionally compare Generalized Cross\nValidation (GCV) and Pareto curve criteria as model selection techniques to\nautomatically estimate near optimal tuning parameters, and conclude that Pareto\ncurves yield better results. The performance of the denoising strategies and\nsparse regression methods is empirically evaluated through well-known benchmark\nproblems of nonlinear dynamical systems.",
    "descriptor": "\nComments: 37 pages, 22 figures, 7 tables\n",
    "authors": [
      "Alexandre Cortiella",
      "Kwang-Chun Park",
      "Alireza Doostan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.12683"
  },
  {
    "id": "arXiv:2201.12691",
    "title": "Coordinate Descent Methods for Fractional Minimization",
    "abstract": "We consider a class of structured fractional minimization problems, in which\nthe numerator part of the objective is the sum of a differentiable convex\nfunction and a convex nonsmooth function, while the denominator part is a\nconcave or convex function. This problem is difficult to solve since it is\nnonconvex. By exploiting the structure of the problem, we propose two\nCoordinate Descent (CD) methods for solving this problem. One is applied to the\noriginal fractional function, the other is based on the associated parametric\nproblem. The proposed methods iteratively solve a one-dimensional subproblem\n\\textit{globally}, and they are guaranteed to converge to coordinate-wise\nstationary points. In the case of a convex denominator, we prove that the\nproposed CD methods using sequential nonconvex approximation find stronger\nstationary points than existing methods. Under suitable conditions, CD methods\nwith an appropriate initialization converge linearly to the optimal point (also\nthe coordinate-wise stationary point). In the case of a concave denominator, we\nshow that the resulting problem is quasi-convex, and any critical point is a\nglobal minimum. We prove that the algorithms converge to the global optimal\nsolution with a sublinear convergence rate. We demonstrate the applicability of\nthe proposed methods to some machine learning and signal processing models. Our\nexperiments on real-world data have shown that our method significantly and\nconsistently outperforms existing methods in terms of accuracy.",
    "descriptor": "",
    "authors": [
      "Ganzhao Yuan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12691"
  },
  {
    "id": "arXiv:2201.12697",
    "title": "Why the Rich Get Richer? On the Balancedness of Random Partition Models",
    "abstract": "Random partition models are widely used in Bayesian methods for various\nclustering tasks, such as mixture models, topic models, and community detection\nproblems. While the number of clusters induced by random partition models has\nbeen studied extensively, another important model property regarding the\nbalancedness of cluster sizes has been largely neglected. We formulate a\nframework to define and theoretically study the balancedness of exchangeable\nrandom partition models, by analyzing how a model assigns probabilities to\npartitions with different levels of balancedness. We demonstrate that the\n\"rich-get-richer\" characteristic of many existing popular random partition\nmodels is an inevitable consequence of two common assumptions: product-form\nexchangeability and projectivity. We propose a principled way to compare the\nbalancedness of random partition models, which gives a better understanding of\nwhat model works better and what doesn't for different applications. We also\nintroduce the \"rich-get-poorer\" random partition models and illustrate their\napplication to entity resolution tasks.",
    "descriptor": "\nComments: 26 pages, 8 figures\n",
    "authors": [
      "Changwoo J. Lee",
      "Huiyan Sang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2201.12697"
  },
  {
    "id": "arXiv:2201.12745",
    "title": "Approximate Bayesian Computation Based on Maxima Weighted Isolation  Kernel Mapping",
    "abstract": "Motivation: The branching processes model yields unevenly stochastically\ndistributed data that consists of sparse and dense regions. The work tries to\nsolve the problem of a precise evaluation of a parameter for this type of\nmodel. The application of the branching processes model to cancer cell\nevolution has many difficulties like high dimensionality and the rare\nappearance of a result of interest. Moreover, we would like to solve the\nambitious task of obtaining the coefficients of the model reflecting the\nrelationship of driver genes mutations and cancer hallmarks on the basis of\npersonal data of variant allele frequencies. Results: The Approximate Bayesian\ncomputation method based on the Isolation kernel is designed. The method\nincludes a transformation row data to a Hilbert space (mapping) and measures\nthe similarity between simulation points and maxima weighted Isolation kernel\nmapping related to the observation point. Also, we designed a heuristic\nalgorithm to find parameter estimation without gradient calculation and\ndimension-independent. The advantage of the proposed machine learning method is\nshown for multidimensional test data as well as for an example of cancer cell\nevolution.",
    "descriptor": "",
    "authors": [
      "Iurii S. Nagornov"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2201.12745"
  },
  {
    "id": "arXiv:2201.12755",
    "title": "HGCN: harmonic gated compensation network for speech enhancement",
    "abstract": "Mask processing in the time-frequency (T-F) domain through the neural network\nhas been one of the mainstreams for single-channel speech enhancement. However,\nit is hard for most models to handle the situation when harmonics are partially\nmasked by noise. To tackle this challenge, we propose a harmonic gated\ncompensation network (HGCN). We design a high-resolution harmonic integral\nspectrum to improve the accuracy of harmonic locations prediction. Then we add\nvoice activity detection (VAD) and voiced region detection (VRD) to the\nconvolutional recurrent network (CRN) to filter harmonic locations. Finally,\nthe harmonic gating mechanism is used to guide the compensation model to adjust\nthe coarse results from CRN to obtain the refinedly enhanced results. Our\nexperiments show HGCN achieves substantial gain over a number of advanced\napproaches in the community.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Tianrui Wang",
      "Weibin Zhu",
      "Yingying Gao",
      "Junlan Feng",
      "Shilei Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.12755"
  },
  {
    "id": "arXiv:2201.12773",
    "title": "Practical Noise Simulation for RGB Images",
    "abstract": "This document describes a noise generator that simulates realistic noise\nfound in smartphone cameras. The generator simulates Poissonian-Gaussian noise\nwhose parameters have been estimated on the Smartphone Image Denoising Dataset\n(SIDD). The generator is available online, and is currently being used in\ncompressed-domain denoising exploration experiments in JPEG AI.",
    "descriptor": "\nComments: Reference paper for the code\n",
    "authors": [
      "Saeed Ranjbar Alvar",
      "Ivan V. Baji\u0107"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12773"
  },
  {
    "id": "arXiv:2201.12782",
    "title": "SRKCD: a stabilized Runge-Kutta method for stochastic optimization",
    "abstract": "We introduce a family of stochastic optimization methods based on the\nRunge-Kutta-Chebyshev (RKC) schemes. The RKC methods are explicit methods\noriginally designed for solving stiff ordinary differential equations by\nensuring that their stability regions are of maximal size.In the optimization\ncontext, this allows for larger step sizes (learning rates) and better\nrobustness compared to e.g. the popular stochastic gradient descent method. Our\nmain contribution is a convergence proof for essentially all stochastic\nRunge-Kutta optimization methods. This shows convergence in expectation with an\noptimal sublinear rate under standard assumptions of strong convexity and\nLipschitz-continuous gradients. For non-convex objectives, we get convergence\nto zero in expectation of the gradients. The proof requires certain natural\nconditions on the Runge-Kutta coefficients, and we further demonstrate that the\nRKC schemes satisfy these. Finally, we illustrate the improved stability\nproperties of the methods in practice by performing numerical experiments on\nboth a small-scale test example and on a problem arising from an image\nclassification application in machine learning.",
    "descriptor": "",
    "authors": [
      "Tony Stillfjord",
      "M\u00e5ns Williamson"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.12782"
  },
  {
    "id": "arXiv:2201.12785",
    "title": "TransBTSV2: Wider Instead of Deeper Transformer for Medical Image  Segmentation",
    "abstract": "Transformer, benefiting from global (long-range) information modeling using\nself-attention mechanism, has been successful in natural language processing\nand computer vision recently. Convolutional Neural Networks, capable of\ncapturing local features, are unable to model explicit long-distance\ndependencies from global feature space. However, both local and global features\nare crucial for dense prediction tasks, especially for 3D medical image\nsegmentation. In this paper, we exploit Transformer in 3D CNN for 3D medical\nimage volumetric segmentation and propose a novel network named TransBTSV2\nbased on the encoder-decoder structure. Different from our original TransBTS,\nthe proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but\nfocuses on general medical image segmentation, providing a strong and efficient\n3D baseline for volumetric segmentation of medical images. As a hybrid\nCNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of\nmedical images without any pre-training. With the proposed insight to redesign\nthe internal structure of Transformer and the introduced Deformable Bottleneck\nModule, a highly efficient architecture is achieved with superior performance.\nExtensive experimental results on four medical image datasets (BraTS 2019,\nBraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves\ncomparable or better results as compared to the state-of-the-art methods for\nthe segmentation of brain tumor, liver tumor as well as kidney tumor. Code is\navailable at https://github.com/Wenxuan-1119/TransBTS.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Jiangyun Li",
      "Wenxuan Wang",
      "Chen Chen",
      "Tianxiang Zhang",
      "Sen Zha",
      "Hong Yu",
      "Jing Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.12785"
  },
  {
    "id": "arXiv:2201.12878",
    "title": "Polynomial functors and Shannon entropy",
    "abstract": "Past work shows that one can associate a notion of Shannon entropy to a\nDirichlet polynomial, regarded as an empirical distribution. Indeed, entropy\ncan be extracted from any $d\\in\\mathsf{Dir}$ by a two-step process, where the\nfirst step is a rig homomorphism out of $\\mathsf{Dir}$, the \\emph{set} of\nDirichlet polynomials, with rig structure given by standard addition and\nmultiplication. In this short note, we show that this rig homomorphism can be\nupgraded to a rig \\emph{functor}, when we replace the set of Dirichlet\npolynomials by the \\emph{category} of ordinary (Cartesian) polynomials.\nIn the Cartesian case, the process has three steps. The first step is a rig\nfunctor $\\mathbf{Poly}^{\\mathbf{Cart}}\\to\\mathbf{Poly}$ sending a polynomial\n$p$ to $\\dot{p}\\mathcal{y}$, where $\\dot{p}$ is the derivative of $p$. The\nsecond is a rig functor\n$\\mathbf{Poly}\\to\\mathbf{Set}\\times\\mathbf{Set}^{\\text{op}}$, sending a\npolynomial $q$ to the pair $(q(1),\\Gamma(q))$, where\n$\\Gamma(q)=\\mathbf{Poly}(q,\\mathcal{y})$ can be interpreted as the global\nsections of $q$ viewed as a bundle, and $q(1)$ as its base. To make this\nprecise we define what appears to be a new distributive monoidal structure on\n$\\mathbf{Set}\\times\\mathbf{Set}^{\\text{op}}$, which can be understood\ngeometrically in terms of rectangles. The last step, as for Dirichlet\npolynomials, is simply to extract the entropy as a real number from a pair of\nsets $(A,B)$; it is given by $\\log A-\\log \\sqrt[A]{B}$ and can be thought of as\nthe log aspect ratio of the rectangle.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "David I. Spivak"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.12878"
  },
  {
    "id": "arXiv:2201.12893",
    "title": "Cryptocurrency Valuation: An Explainable AI Approach",
    "abstract": "Currently, there are no convincing proxies for the fundamentals of\ncryptocurrency assets. We propose a new market-to-fundamental ratio, the\nprice-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We\nthen proxy various fundamental-to-market ratios by Bitcoin historical data and\nfind they have little predictive power for short-term bitcoin returns. However,\nPU ratio effectively predicts long-term bitcoin returns. We verify PU ratio\nvaluation by unsupervised and supervised machine learning. The valuation method\ninforms investment returns and predicts bull markets effectively. Finally, we\npresent an automated trading strategy advised by the PU ratio that outperforms\nthe conventional buy-and-hold and market-timing strategies. We distribute the\ntrading algorithms as open-source software via Python Package Index for future\nresearch.",
    "descriptor": "",
    "authors": [
      "Yulin Liu",
      "Luyao Zhang"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computational Finance (q-fin.CP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12893"
  },
  {
    "id": "arXiv:2201.12898",
    "title": "ClearingPayments in Dynamic Financial Networks",
    "abstract": "In this paper, we propose a novel dynamical model of clearing in a financial\nnetwork, which stems from the classical Eisenberg- Noe model of financial\ncontagion. The Eisenberg-Noe model assumes that at one point in time (say, at\nthe end of a day), all liabilities are claimed and due simultaneously, and that\nthe entire network of banks becomes aware of the claims and possible defaults\nand instantaneously agrees on the clearing payments. The motivation for the\ndynamic model we propose in this paper is that one may expect that if financial\noperations are allowed for a given number of time periods after the initial\ntheoretical defaults, some nodes may actually recover and eventually manage to\nfulfill their obligations. We prove that the proposed model obeys the standard\nrequirement known as the priority of debt claims, that is, each node either\npays its liabilities in full, or it pays out all its balance. We also show that\nthe requirements of ro-rata payments determines the solution uniquely.",
    "descriptor": "",
    "authors": [
      "Giuseppe C. Calafiore",
      "Giulia Fracastoro",
      "Anton V. Proskurnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Systems and Control (eess.SY)",
      "Mathematical Finance (q-fin.MF)",
      "Risk Management (q-fin.RM)"
    ],
    "url": "https://arxiv.org/abs/2201.12898"
  },
  {
    "id": "arXiv:2201.12909",
    "title": "Scaling Gaussian Process Optimization by Evaluating a Few Unique  Candidates Multiple Times",
    "abstract": "Computing a Gaussian process (GP) posterior has a computational cost cubical\nin the number of historical points. A reformulation of the same GP posterior\nhighlights that this complexity mainly depends on how many \\emph{unique}\nhistorical points are considered. This can have important implication in active\nlearning settings, where the set of historical points is constructed\nsequentially by the learner. We show that sequential black-box optimization\nbased on GPs (GP-Opt) can be made efficient by sticking to a candidate solution\nfor multiple evaluation steps and switch only when necessary. Limiting the\nnumber of switches also limits the number of unique points in the history of\nthe GP. Thus, the efficient GP reformulation can be used to exactly and cheaply\ncompute the posteriors required to run the GP-Opt algorithms. This approach is\nespecially useful in real-world applications of GP-Opt with high switch costs\n(e.g. switching chemicals in wet labs, data/model loading in hyperparameter\noptimization). As examples of this meta-approach, we modify two\nwell-established GP-Opt algorithms, GP-UCB and GP-EI, to switch candidates as\ninfrequently as possible adapting rules from batched GP-Opt. These versions\npreserve all the theoretical no-regret guarantees while improving practical\naspects of the algorithms such as runtime, memory complexity, and the ability\nof batching candidates and evaluating them in parallel.",
    "descriptor": "",
    "authors": [
      "Daniele Calandriello",
      "Luigi Carratino",
      "Alessandro Lazaric",
      "Michal Valko",
      "Lorenzo Rosasco"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12909"
  },
  {
    "id": "arXiv:2201.12925",
    "title": "Multimodal Maximum Entropy Dynamic Games",
    "abstract": "Environments with multi-agent interactions often result a rich set of\nmodalities of behavior between agents due to the inherent suboptimality of\ndecision making processes when agents settle for satisfactory decisions.\nHowever, existing algorithms for solving these dynamic games are strictly\nunimodal and fail to capture the intricate multimodal behaviors of the agents.\nIn this paper, we propose MMELQGames (Multimodal Maximum-Entropy Linear\nQuadratic Games), a novel constrained multimodal maximum entropy formulation of\nthe Differential Dynamic Programming algorithm for solving generalized Nash\nequilibria. By formulating the problem as a certain dynamic game with\nincomplete and asymmetric information where agents are uncertain about the cost\nand dynamics of the game itself, the proposed method is able to reason about\nmultiple local generalized Nash equilibria, enforce constraints with the\nAugmented Lagrangian framework and also perform Bayesian inference on the\nlatent mode from past observations. We assess the efficacy of the proposed\nalgorithm on two illustrative examples: multi-agent collision avoidance and\nautonomous racing. In particular, we show that only MMELQGames is able to\neffectively block a rear vehicle when given a speed disadvantage and the rear\nvehicle can overtake from multiple positions.",
    "descriptor": "\nComments: Under review for RSS 2022. Supplementary Video: this https URL\n",
    "authors": [
      "Oswin So",
      "Kyle Stachowicz",
      "Evangelos A. Theodorou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12925"
  },
  {
    "id": "arXiv:2201.12937",
    "title": "Spatial search with multiple marked vertices is optimal for almost all  queries and its quantum advantage is not always guaranteed",
    "abstract": "We contribute to fulfil the long-lasting gap in the understanding of the\nspatial search with multiple marked vertices. The theoretical framework is that\nof discrete-time quantum walks (QW), i.e. local unitary matrices that drive the\nevolution of a single particle on the lattice. QW based search algorithms are\nwell understood when they have to tackle the fundamental problem of finding\nonly one marked element in a $d-$dimensional grid and it has been proven they\nprovide a quadratic advantage over classical searching protocols. However, once\nwe consider to search more than one element, the behaviour of the algorithm may\nbe affected by the spatial configuration of the marked elements, due to the\nquantum interference among themselves and even the quantum advantage is no\nlonger granted. Here our main contribution is twofold : (i) we provide strong\nnumerical evidence that spatial configurations are almost all optimal; and (ii)\nwe analytically prove that the quantum advantage with respect to the classical\ncounterpart is not always granted and it does depend on the proportion of\nsearched elements over the total number of grid points $\\tau$. We finally\nproviding a clear phase diagram for the QW search advantage with respect to the\nclassical random algorithm.",
    "descriptor": "",
    "authors": [
      "Mathieu Roget",
      "Hachem Kadri",
      "Giuseppe Di Molfetta"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12937"
  },
  {
    "id": "arXiv:2201.12942",
    "title": "The road problem and homomorphisms of directed graphs",
    "abstract": "We make progress on a generalization of the road (colouring) problem. The\nroad problem was posed by Adler-Goodwyn-Weiss and solved by Trahtman. The\ngeneralization was posed, and solved in certain special cases, by\nAshley-Marcus-Tuncel. We resolve two new families of cases, of which one\ngeneralizes the road problem and follows Trahtman's solution, and the other\ngeneralizes a result of Ashley-Marcus-Tuncel with a proof quite different from\ntheirs. Along the way, we prove a universal property for the fiber product of\ncertain graph homomorphisms, which may be of independent interest. We provide\npolynomial-time algorithms for relevant constructions and decision problems.",
    "descriptor": "\nComments: 25 pages, no figures\n",
    "authors": [
      "Sophie MacDonald"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12942"
  },
  {
    "id": "arXiv:2201.12946",
    "title": "Pauli Error Propagation-Based Gate Reschedulingfor Quantum Circuit Error  Mitigation",
    "abstract": "Noisy Intermediate-Scale Quantum (NISQ) algorithms, which run on noisy\nquantum computers should be carefully designed to boost the output state\nfidelity. While several compilation approaches have been proposed to minimize\ncircuit errors, they often omit the detailed circuit structure information that\ndoes not affect the circuit depth or the gate count. In the presence of spatial\nvariation in the error rate of the quantum gates, adjusting the circuit\nstructure can play a major role in mitigating errors. In this paper, we exploit\nthe freedom of gate reordering based on the commutation rules to show the\nimpact of gate error propagation paths on the output state fidelity of the\nquantum circuit, propose advanced predictive techniques to project the success\nrate of the circuit, and develop a new compilation phase post-quantum circuit\nmapping to improve its reliability. Our proposed approaches have been validated\nusing a variety of quantum circuits with different success metrics, which are\nexecuted on IBM quantum computers. Our results show that rescheduling quantum\ngates based on their error propagation paths can significantly improve the\nfidelity of the quantum circuit in the presence of variable gate error rates.",
    "descriptor": "",
    "authors": [
      "Vedika Saravanan",
      "Samah Mohamed Saeed"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2201.12946"
  },
  {
    "id": "arXiv:2201.12947",
    "title": "Fair Wrapping for Black-box Predictions",
    "abstract": "We introduce a new family of techniques to post-process (\"wrap\") a black-box\nclassifier in order to reduce its bias. Our technique builds on the recent\nanalysis of improper loss functions whose optimisation can correct any twist in\nprediction, unfairness being treated as a twist. In the post-processing, we\nlearn a wrapper function which we define as an {\\alpha}-tree, which modifies\nthe prediction. We provide two generic boosting algorithms to learn\n{\\alpha}-trees. We show that our modification has appealing properties in terms\nof composition of{\\alpha}-trees, generalization, interpretability, and KL\ndivergence between modified and original predictions. We exemplify the use of\nour technique in three fairness notions: conditional value at risk, equality of\nopportunity, and statistical parity; and provide experiments on several readily\navailable datasets.",
    "descriptor": "",
    "authors": [
      "Alexander Soen",
      "Ibrahim Alabdulmohsin",
      "Sanmi Koyejo",
      "Yishay Mansour",
      "Nyalleng Moorosi",
      "Richard Nock",
      "Ke Sun",
      "Lexing Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12947"
  },
  {
    "id": "arXiv:2201.12973",
    "title": "GenMod: A generative modeling approach for spectral representation of  PDEs with random inputs",
    "abstract": "We propose a method for quantifying uncertainty in high-dimensional PDE\nsystems with random parameters, where the number of solution evaluations is\nsmall. Parametric PDE solutions are often approximated using a spectral\ndecomposition based on polynomial chaos expansions. For the class of systems we\nconsider (i.e., high dimensional with limited solution evaluations) the\ncoefficients are given by an underdetermined linear system in a regression\nformulation. This implies additional assumptions, such as sparsity of the\ncoefficient vector, are needed to approximate the solution. Here, we present an\napproach where we assume the coefficients are close to the range of a\ngenerative model that maps from a low to a high dimensional space of\ncoefficients. Our approach is inspired be recent work examining how generative\nmodels can be used for compressed sensing in systems with random Gaussian\nmeasurement matrices. Using results from PDE theory on coefficient decay rates,\nwe construct an explicit generative model that predicts the polynomial chaos\ncoefficient magnitudes. The algorithm we developed to find the coefficients,\nwhich we call GenMod, is composed of two main steps. First, we predict the\ncoefficient signs using Orthogonal Matching Pursuit. Then, we assume the\ncoefficients are within a sparse deviation from the range of a sign-adjusted\ngenerative model. This allows us to find the coefficients by solving a\nnonconvex optimization problem, over the input space of the generative model\nand the space of sparse vectors. We obtain theoretical recovery results for a\nLipschitz continuous generative model and for a more specific generative model,\nbased on coefficient decay rate bounds. We examine three high-dimensional\nproblems and show that, for all three examples, the generative model approach\noutperforms sparsity promoting methods at small sample sizes.",
    "descriptor": "",
    "authors": [
      "Jacqueline Wentz",
      "Alireza Doostan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12973"
  },
  {
    "id": "arXiv:2201.13008",
    "title": "Communication-Efficient Distributed Multiple Testing for Large-Scale  Inference",
    "abstract": "The Benjamini-Hochberg (BH) procedure is a celebrated method for multiple\ntesting with false discovery rate (FDR) control. In this paper, we consider\nlarge-scale distributed networks where each node possesses a large number of\np-values and the goal is to achieve the global BH performance in a\ncommunication-efficient manner. We propose that every node performs a local\ntest with an adjusted test size according to the (estimated) global proportion\nof true null hypotheses. With suitable assumptions, our method is\nasymptotically equivalent to the global BH procedure. Motivated by this, we\ndevelop an algorithm for star networks where each node only needs to transmit\nan estimate of the (local) proportion of nulls and the (local) number of\np-values to the center node; the center node then broadcasts a parameter\n(computed based on the global estimate and test size) to the local nodes. In\nthe experiment section, we utilize existing estimators of the proportion of\ntrue nulls and consider various settings to evaluate the performance and\nrobustness of our method.",
    "descriptor": "\nComments: Submitted to the 2022 IEEE International Symposium on Information Theory (ISIT)\n",
    "authors": [
      "Mehrdad Pournaderi",
      "Yu Xiang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13008"
  },
  {
    "id": "arXiv:2201.13055",
    "title": "Nystr\u00f6m Kernel Mean Embeddings",
    "abstract": "Kernel mean embeddings are a powerful tool to represent probability\ndistributions over arbitrary spaces as single points in a Hilbert space. Yet,\nthe cost of computing and storing such embeddings prohibits their direct use in\nlarge-scale settings. We propose an efficient approximation procedure based on\nthe Nystr\\\"om method, which exploits a small random subset of the dataset. Our\nmain result is an upper bound on the approximation error of this procedure. It\nyields sufficient conditions on the subsample size to obtain the standard\n$n^{-1/2}$ rate while reducing computational costs. We discuss applications of\nthis result for the approximation of the maximum mean discrepancy and\nquadrature rules, and illustrate our theoretical findings with numerical\nexperiments.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Antoine Chatalic",
      "Nicolas Schreuder",
      "Alessandro Rudi",
      "Lorenzo Rosasco"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13055"
  },
  {
    "id": "arXiv:2201.13093",
    "title": "PostGAN: A GAN-Based Post-Processor to Enhance the Quality of Coded  Speech",
    "abstract": "The quality of speech coded by transform coding is affected by various\nartefacts especially when bitrates to quantize the frequency components become\ntoo low. In order to mitigate these coding artefacts and enhance the quality of\ncoded speech, a post-processor that relies on a-priori information transmitted\nfrom the encoder is traditionally employed at the decoder side. In recent\nyears, several data-driven post-postprocessors have been proposed which were\nshown to outperform traditional approaches. In this paper, we propose PostGAN,\na GAN-based neural post-processor that operates in the sub-band domain and\nrelies on the U-Net architecture and a learned affine transform. It has been\ntested on the recently standardized low-complexity, low-delay bluetooth codec\n(LC3) for wideband speech at the lowest bitrate (16 kbit/s). Subjective\nevaluations and objective scores show that the newly introduced post-processor\nsurpasses previously published methods and can improve the quality of coded\nspeech by around 20 MUSHRA points.",
    "descriptor": "\nComments: Accepted to ICASSP 2022\n",
    "authors": [
      "Srikanth Korse",
      "Nicola Pia",
      "Kishan Gupta",
      "Guillaume Fuchs"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.13093"
  },
  {
    "id": "arXiv:2201.13112",
    "title": "Bayesian Optimization for Distributionally Robust Chance-constrained  Problem",
    "abstract": "In black-box function optimization, we need to consider not only controllable\ndesign variables but also uncontrollable stochastic environment variables. In\nsuch cases, it is necessary to solve the optimization problem by taking into\naccount the uncertainty of the environmental variables. Chance-constrained (CC)\nproblem, the problem of maximizing the expected value under a certain level of\nconstraint satisfaction probability, is one of the practically important\nproblems in the presence of environmental variables. In this study, we consider\ndistributionally robust CC (DRCC) problem and propose a novel DRCC Bayesian\noptimization method for the case where the distribution of the environmental\nvariables cannot be precisely specified. We show that the proposed method can\nfind an arbitrary accurate solution with high probability in a finite number of\ntrials, and confirm the usefulness of the proposed method through numerical\nexperiments.",
    "descriptor": "\nComments: 18 pages, 2 figures\n",
    "authors": [
      "Yu Inatsu",
      "Shion Takeno",
      "Masayuki Karasuyama",
      "Ichiro Takeuchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13112"
  },
  {
    "id": "arXiv:2201.13114",
    "title": "An end-to-end deep learning approach for extracting stochastic dynamical  systems with $\u03b1$-stable L\u00e9vy noise",
    "abstract": "Recently, extracting data-driven governing laws of dynamical systems through\ndeep learning frameworks has gained a lot of attention in various fields.\nMoreover, a growing amount of research work tends to transfer deterministic\ndynamical systems to stochastic dynamical systems, especially those driven by\nnon-Gaussian multiplicative noise. However, lots of log-likelihood based\nalgorithms that work well for Gaussian cases cannot be directly extended to\nnon-Gaussian scenarios which could have high error and low convergence issues.\nIn this work, we overcome some of these challenges and identify stochastic\ndynamical systems driven by $\\alpha$-stable L\\'evy noise from only random\npairwise data. Our innovations include: (1) designing a deep learning approach\nto learn both drift and diffusion terms for L\\'evy induced noise with $\\alpha$\nacross all values, (2) learning complex multiplicative noise without\nrestrictions on small noise intensity, (3) proposing an end-to-end complete\nframework for stochastic systems identification under a general input data\nassumption, that is, $\\alpha$-stable random variable. Finally, numerical\nexperiments and comparisons with the non-local Kramers-Moyal formulas with\nmoment generating function confirm the effectiveness of our method.",
    "descriptor": "\nComments: 19 pages,12 figures\n",
    "authors": [
      "Cheng Fang",
      "Yubin Lu",
      "Ting Gao",
      "Jinqiao Duan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13114"
  },
  {
    "id": "arXiv:2201.13117",
    "title": "Continual Repeated Annealed Flow Transport Monte Carlo",
    "abstract": "We propose Continual Repeated Annealed Flow Transport Monte Carlo (CRAFT), a\nmethod that combines a sequential Monte Carlo (SMC) sampler (itself a\ngeneralization of Annealed Importance Sampling) with variational inference\nusing normalizing flows. The normalizing flows are directly trained to\ntransport between annealing temperatures using a KL divergence for each\ntransition. This optimization objective is itself estimated using the\nnormalizing flow/SMC approximation. We show conceptually and using multiple\nempirical examples that CRAFT improves on Annealed Flow Transport Monte Carlo\n(Arbel et al., 2021), on which it builds and also on Markov chain Monte Carlo\n(MCMC) based Stochastic Normalizing Flows (Wu et al., 2020). By incorporating\nCRAFT within particle MCMC, we show that such learnt samplers can achieve\nimpressively accurate results on a challenging lattice field theory example.",
    "descriptor": "\nComments: 21 pages, 6 figures\n",
    "authors": [
      "Alexander G. D. G. Matthews",
      "Michael Arbel",
      "Danilo J. Rezende",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Lattice (hep-lat)"
    ],
    "url": "https://arxiv.org/abs/2201.13117"
  },
  {
    "id": "arXiv:2201.13145",
    "title": "Assessment of DeepONet for reliability analysis of stochastic nonlinear  dynamical systems",
    "abstract": "Time dependent reliability analysis and uncertainty quantification of\nstructural system subjected to stochastic forcing function is a challenging\nendeavour as it necessitates considerable computational time. We investigate\nthe efficacy of recently proposed DeepONet in solving time dependent\nreliability analysis and uncertainty quantification of systems subjected to\nstochastic loading. Unlike conventional machine learning and deep learning\nalgorithms, DeepONet learns is a operator network and learns a function to\nfunction mapping and hence, is ideally suited to propagate the uncertainty from\nthe stochastic forcing function to the output responses. We use DeepONet to\nbuild a surrogate model for the dynamical system under consideration. Multiple\ncase studies, involving both toy and benchmark problems, have been conducted to\nexamine the efficacy of DeepONet in time dependent reliability analysis and\nuncertainty quantification of linear and nonlinear dynamical systems. Results\nobtained indicate that the DeepONet architecture is accurate as well as\nefficient. Moreover, DeepONet posses zero shot learning capabilities and hence,\na trained model easily generalizes to unseen and new environment with no\nfurther training.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Shailesh Garg",
      "Harshit Gupta",
      "Souvik Chakraborty"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13145"
  },
  {
    "id": "arXiv:2201.13148",
    "title": "Threshold Independent Evaluation of Sound Event Detection Scores",
    "abstract": "Performing an adequate evaluation of sound event detection (SED) systems is\nfar from trivial and is still subject to ongoing research. The recently\nproposed polyphonic sound detection (PSD)-receiver operating characteristic\n(ROC) and PSD score (PSDS) make an important step into the direction of an\nevaluation of SED systems which is independent from a certain decision\nthreshold. This allows to obtain a more complete picture of the overall system\nbehavior which is less biased by threshold tuning. Yet, the PSD-ROC is\ncurrently only approximated using a finite set of thresholds. The choice of the\nthresholds used in approximation, however, can have a severe impact on the\nresulting PSDS. In this paper we propose a method which allows for computing\nsystem performance on an evaluation set for all possible thresholds jointly,\nenabling accurate computation not only of the PSD-ROC and PSDS but also of\nother collar-based and intersection-based performance curves. It further allows\nto select the threshold which best fulfills the requirements of a given\napplication. Source code is publicly available in our SED evaluation package\nsed_scores_eval.",
    "descriptor": "\nComments: accepted for ICASSP 2022\n",
    "authors": [
      "Janek Ebbers",
      "Romain Serizel",
      "Reinhold Haeb-Umbach"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.13148"
  },
  {
    "id": "arXiv:2201.13192",
    "title": "Positive-Unlabeled Learning with Uncertainty-aware Pseudo-label  Selection",
    "abstract": "Pseudo-labeling solutions for positive-unlabeled (PU) learning have the\npotential to result in higher performance compared to cost-sensitive learning\nbut are vulnerable to incorrectly estimated pseudo-labels. In this paper, we\nprovide a theoretical analysis of a risk estimator that combines risk on PU and\npseudo-labeled data. Furthermore, we show analytically as well as\nexperimentally that such an estimator results in lower excess risk compared to\nusing PU data alone, provided that enough samples are pseudo-labeled with\nacceptable error rates. We then propose PUUPL, a novel training procedure for\nPU learning that leverages the epistemic uncertainty of an ensemble of deep\nneural networks to minimize errors in pseudo-label selection. We conclude with\nextensive experiments showing the effectiveness of our proposed algorithm over\ndifferent datasets, modalities, and learning tasks. These show that PUUPL\nenables a reduction of up to 20% in test error rates even when prior and\nnegative samples are not provided for validation, setting a new\nstate-of-the-art for PU learning.",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Emilio Dorigatti",
      "Jann Goschenhofer",
      "Benjamin Schubert",
      "Mina Rezaei",
      "Bernd Bischl"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13192"
  },
  {
    "id": "arXiv:2201.13235",
    "title": "A hybrid deep learning approach for purchasing strategy of carbon  emission rights -- Based on Shanghai pilot market",
    "abstract": "The price of carbon emission rights play a crucial role in carbon trading\nmarkets. Therefore, accurate prediction of the price is critical. Taking the\nShanghai pilot market as an example, this paper attempted to design a carbon\nemission purchasing strategy for enterprises, and establish a carbon emission\nprice prediction model to help them reduce the purchasing cost. To make\npredictions more precise, we built a hybrid deep learning model by embedding\nGeneralized Autoregressive Conditional Heteroskedastic (GARCH) into the Gate\nRecurrent Unit (GRU) model, and compared the performance with those of other\nmodels. Then, based on the Iceberg Order Theory and the predicted price, we\nproposed the purchasing strategy of carbon emission rights. As a result, the\nprediction errors of the GARCH-GRU model with a 5-day sliding time window were\nthe minimum values of all six models. And in the simulation, the purchasing\nstrategy based on the GARCH-GRU model was executed with the least cost as well.\nThe carbon emission purchasing strategy constructed by the hybrid deep learning\nmethod can accurately send out timing signals, and help enterprises reduce the\npurchasing cost of carbon emission permits.",
    "descriptor": "",
    "authors": [
      "Jiayue Xu"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13235"
  },
  {
    "id": "arXiv:2201.13246",
    "title": "Impact of Naturalistic Field Acoustic Environments on Forensic  Text-independent Speaker Verification System",
    "abstract": "Audio analysis for forensic speaker verification offers unique challenges in\nsystem performance due in part to data collected in naturalistic field acoustic\nenvironments where location/scenario uncertainty is common in the forensic data\ncollection process. Forensic speech data as potential evidence can be obtained\nin random naturalistic environments resulting in variable data quality. Speech\nsamples may include variability due to vocal efforts such as yelling over 911\nemergency calls, whereas others might be whisper or situational stressed voice\nin a field location or interview room. Such speech variability consists of\nintrinsic and extrinsic characteristics and makes forensic speaker verification\na complicated and daunting task. Extrinsic properties include recording\nequipment such as microphone type and placement, ambient noise, room\nconfiguration including reverberation, and other environmental scenario-based\nissues. Some factors, such as noise and non-target speech, will impact the\nverification system performance by their mere presence. To investigate the\nimpact of field acoustic environments, we performed a speaker verification\nstudy based on the CRSS-Forensic corpus with audio collected from 8 field\nlocations including police interviews. This investigation includes an analysis\nof the impact of seven unseen acoustic environments on speaker verification\nsystem performance using an x-Vector system.",
    "descriptor": "\nComments: IAFPA-2021-International Association for Forensic Phonetics and Acoustics\n",
    "authors": [
      "Zhenyu Wang",
      "John H.L. Hansen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.13246"
  },
  {
    "id": "arXiv:2201.13250",
    "title": "Differentiating and Integrating ZX Diagrams",
    "abstract": "ZX-calculus has proved to be a useful tool for quantum technology with a wide\nrange of successful applications. Most of these applications are of an\nalgebraic nature. However, other tasks that involve differentiation and\nintegration remain unreachable with current ZX techniques. Here we elevate ZX\nto an analytical perspective by realising differentiation and integration\nentirely within the framework of ZX-calculus. We explicitly illustrate the new\nanalytic framework of ZX-calculus by applying it in context of quantum machine\nlearning.",
    "descriptor": "\nComments: 32 pages, lots of diagrams\n",
    "authors": [
      "Quanlong Wang",
      "Richie Yeung"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13250"
  },
  {
    "id": "arXiv:2201.13256",
    "title": "Proximal denoiser for convergent plug-and-play optimization with  nonconvex regularization",
    "abstract": "Plug-and-Play (PnP) methods solve ill-posed inverse problems through\niterative proximal algorithms by replacing a proximal operator by a denoising\noperation. When applied with deep neural network denoisers, these methods have\nshown state-of-the-art visual performance for image restoration problems.\nHowever, their theoretical convergence analysis is still incomplete. Most of\nthe existing convergence results consider nonexpansive denoisers, which is\nnon-realistic, or limit their analysis to strongly convex data-fidelity terms\nin the inverse problem to solve. Recently, it was proposed to train the\ndenoiser as a gradient descent step on a functional parameterized by a deep\nneural network. Using such a denoiser guarantees the convergence of the PnP\nversion of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this\npaper, we show that this gradient denoiser can actually correspond to the\nproximal operator of another scalar function. Given this new result, we exploit\nthe convergence theory of proximal algorithms in the nonconvex setting to\nobtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM\n(Alternating Direction Method of Multipliers). When built on top of a smooth\ngradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target\nstationary points of an explicit functional. These convergence results are\nconfirmed with numerical experiments on deblurring, super-resolution and\ninpainting.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Samuel Hurault",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13256"
  },
  {
    "id": "arXiv:2201.13263",
    "title": "Bootstrap percolation on the stochastic block model",
    "abstract": "We analyze the bootstrap percolation process on the stochastic block model\n(SBM), a natural extension of the Erd\\H{o}s--R\\'{e}nyi random graph that\nincorporates the community structure observed in many real systems. In the SBM,\nnodes are partitioned into two subsets, which represent different communities,\nand pairs of nodes are independently connected with a probability that depends\non the communities they belong to. Under mild assumptions on the system\nparameters, we prove the existence of a sharp phase transition for the final\nnumber of active nodes and characterize the sub-critical and the super-critical\nregimes in terms of the number of initially active nodes, which are selected\nuniformly at random in each community.",
    "descriptor": "\nComments: 36 pages, 7 figures. arXiv admin note: text overlap with arXiv:1812.09107\n",
    "authors": [
      "Giovanni Luca Torrisi",
      "Michele Garetto",
      "Emilio Leonardi"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2201.13263"
  },
  {
    "id": "arXiv:2201.13272",
    "title": "Output-Feedback Control of Viscous Liquid-Tank System and its Numerical  Approximation",
    "abstract": "We solve the output-feedback stabilization problem for a tank with a liquid\nmodeled by the viscous Saint-Venant PDE system. The control input is the\nacceleration of the tank and a Control Lyapunov Functional methodology is used.\nThe measurements are the tank position and the liquid level at the tank walls.\nThe control scheme is a combination of a state feedback law with functional\nobservers for the tank velocity and the liquid momentum. Four different types\nof output feedback stabilizers are proposed. A full-order observer and a\nreduced-order observer are used in order to estimate the tank velocity while\nthe unmeasured liquid momentum is either estimated by using an appropriate\nscalar filter or is ignored. The reduced order observer differs from the full\norder observer because it omits the estimation of the measured tank position.\nExponential convergence of the closed-loop system to the desired equilibrium\npoint is achieved in each case. An algorithm is provided that guarantees that a\nrobotic arm can move a glass of water to a pre-specified position no matter how\nfull the glass is, without spilling water out of the glass, without residual\nend point sloshing and without measuring the water momentum and the glass\nvelocity. Finally, the efficiency of the proposed output feedback laws is\nvalidated by numerical examples, obtained by using a simple finite-difference\nnumerical scheme. The properties of the proposed, explicit, finite-difference\nscheme are determined.",
    "descriptor": "\nComments: 35 pages, 4 figures, submitted to Automatica for possible publication. arXiv admin note: text overlap with arXiv:2108.11052\n",
    "authors": [
      "Iasson Karafyllis",
      "Filippos Vokos",
      "Miroslav Krstic"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2201.13272"
  },
  {
    "id": "arXiv:2201.13283",
    "title": "On asynchronous non-uniform cellular automata and sofic groups",
    "abstract": "In this paper, we study the class of asynchronous non-uniform cellular\nautomata (ANUCA) over an arbitrary group universe with multiple local\ntransition rules. We define the notion of stable injectivity and stable\npost-surjectivity and investigate several dynamical properties of such\nautomata. In particular, we establish generalizations of the celebrated\nsurjunctivity theorem of Gromov-Weiss as well as the dual-surjunctivity theorem\nof Capobianco-Kari-Taati for cellular automata to the larger class of ANUCA\nover sofic universes.",
    "descriptor": "",
    "authors": [
      "Xuan Kien Phung"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.13283"
  },
  {
    "id": "arXiv:2201.13288",
    "title": "A Regret Minimization Approach to Multi-Agent Contro",
    "abstract": "We study the problem of multi-agent control of a dynamical system with known\ndynamics and adversarial disturbances. Our study focuses on optimal control\nwithout centralized precomputed policies, but rather with adaptive control\npolicies for the different agents that are only equipped with a stabilizing\ncontroller. We give a reduction from any (standard) regret minimizing control\nmethod to a distributed algorithm. The reduction guarantees that the resulting\ndistributed algorithm has low regret relative to the optimal precomputed joint\npolicy. Our methodology involves generalizing online convex optimization to a\nmulti-agent setting and applying recent tools from nonstochastic control\nderived for a single agent. We empirically evaluate our method on a model of an\noveractuated aircraft. We show that the distributed method is robust to failure\nand to adversarial perturbations in the dynamics.",
    "descriptor": "",
    "authors": [
      "Udaya Ghai",
      "Udari Madhushani",
      "Naomi Leonard",
      "Elad Hazan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13288"
  },
  {
    "id": "arXiv:2201.13299",
    "title": "Directed Weight Neural Networks for Protein Structure Representation  Learning",
    "abstract": "A protein performs biological functions by folding to a particular 3D\nstructure. To accurately model the protein structures, both the overall\ngeometric topology and local fine-grained relations between amino acids (e.g.\nside-chain torsion angles and inter-amino-acid orientations) should be\ncarefully considered. In this work, we propose the Directed Weight Neural\nNetwork for better capturing geometric relations among different amino acids.\nExtending a single weight from a scalar to a 3D directed vector, our new\nframework supports a rich set of geometric operations on both classical and\nSO(3)--representation features, on top of which we construct a perceptron unit\nfor processing amino-acid information. In addition, we introduce an equivariant\nmessage passing paradigm on proteins for plugging the directed weight\nperceptrons into existing Graph Neural Networks, showing superior versatility\nin maintaining SO(3)-equivariance at the global scale. Experiments show that\nour network has remarkably better expressiveness in representing geometric\nrelations in comparison to classical neural networks and the (globally)\nequivariant networks. It also achieves state-of-the-art performance on various\ncomputational biology applications related to protein 3D structures.",
    "descriptor": "",
    "authors": [
      "Jiahan Li",
      "Shitong Luo",
      "Congyue Deng",
      "Chaoran Cheng",
      "Jiaqi Guan",
      "Leonidas Guibas",
      "Jian Peng",
      "Jianzhu Ma"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13299"
  },
  {
    "id": "arXiv:2201.13308",
    "title": "Solving the Cauchy problem for a three-dimensional difference equation  in a parallelepiped",
    "abstract": "The aim of this article is further development of the theory of linear\ndifference equations with constant coefficients. We present a new algorithm for\ncalculating the solution to the Cauchy problem for a three-dimensional\ndifference equation with constant coefficients in a parallelepiped at the point\nusing the coefficients of the difference equation and Cauchy data. The\nimplemented algorithm is the next significant achievement in a series of\narticles justifying the Apanovich and Leinartas' theorems about the solvability\nand well-posedness of the Cauchy problem. We also use methods of computer\nalgebra since the three-dimensional case usually demands extended calculations.",
    "descriptor": "",
    "authors": [
      "Marina S. Apanovich",
      "Alexander P. Lyapin",
      "Konstantin V. Shadrin"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13308"
  },
  {
    "id": "arXiv:2201.13309",
    "title": "Accelerating Laue Depth Reconstruction Algorithm with CUDA",
    "abstract": "The Laue diffraction microscopy experiment uses the polychromatic Laue\nmicro-diffraction technique to examine the structure of materials with\nsub-micron spatial resolution in all three dimensions. During this experiment,\nlocal crystallographic orientations, orientation gradients and strains are\nmeasured as properties which will be recorded in HDF5 image format. The\nrecorded images will be processed with a depth reconstruction algorithm for\nfuture data analysis. But the current depth reconstruction algorithm consumes\nconsiderable processing time and might take up to 2 weeks for reconstructing\ndata collected from one single experiment. To improve the depth reconstruction\ncomputation speed, we propose a scalable GPU program solution on the depth\nreconstruction problem in this paper. The test result shows that the running\ntime would be 10 to 20 times faster than the prior CPU design for various size\nof input data.",
    "descriptor": "\nComments: 2015 IEEE International Conference on Cluster Computing\n",
    "authors": [
      "Ke Yue",
      "Schwarz Nicholas",
      "Tischler Jonathan Z"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2201.13309"
  },
  {
    "id": "arXiv:2201.13323",
    "title": "Constructing coarse-scale bifurcation diagrams from spatio-temporal  observations of microscopic simulations: A parsimonious machine learning  approach",
    "abstract": "We address a three-tier computational approach for the construction of\ncoarse-grained bifurcation diagrams from spatio-temporal data produced by\nmicroscopic simulators using machine learning. In the first step, we exploit\nmanifold learning and in particular parsimonious Diffusion Maps to identify the\nintrinsic dimension of the manifolds where the emergent dynamics evolve and\nfeature selection for the parametrization of these manifolds. In the second\nstep, based on the selected features we learn the right-hand-side of the\neffective partial differential equations (PDEs) using two machine learning\nschemes, namely Feed-forward Neural Networks (FNNs) and Random Projection\nNetworks (RPNNs). Finally, based on the learned black-box PDE model, we\nconstruct the corresponding bifurcation diagram, thus exploiting numerical\nbifurcation theory algorithms. For our illustrations, we implemented the\nproposed method to construct the one-parameter bifurcation diagram of the 1D\nFitzHugh-Nagumo PDEs from data generated by Lattice-Boltzman (LBM) numerical\nsimulations.",
    "descriptor": "",
    "authors": [
      "Evangelos Galaris",
      "Gianluca Fabiani",
      "Ioannis Gallos",
      "Ioannis Kevrekidis",
      "Constantinos Siettos"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.13323"
  },
  {
    "id": "arXiv:2201.13344",
    "title": "The collective vs individual nature of mountaineering: a network and  simplicial approach",
    "abstract": "Mountaineering is a sport of contrary forces: teamwork plays a large role in\nmental fortitude and skills, but the actual act of climbing, and indeed\nsurvival, is largely individualistic. This work studies the effects of the\nstructure and topology of relationships within climbers on the level of\ncooperation and success. It does so using simplicial complexes, where\nrelationships between climbers are captured through simplexes that correspond\nto joint previous expeditions with dimension given by the number of climbers\nminus one and weight given by the number of occurrences of the simplex. First,\nthis analysis establishes the importance of relationships and shows that\nchances of failure to summit reduce drastically when climbing with repeat\npartners. From a climber-centric perspective, climbers that belong to simplexes\nwith large dimension were more likely to be successful, across experience\nlevels. From an expedition-centric perspective, the distribution of\nrelationships within a group is explored to identify collective human behavior:\nfrom polarized to cooperative. Expeditions containing simplices with large\ndimension, and usually low weight, i.e., a large number of people had a small\nnumber of previous joint expeditions, tended to be more cooperative, with more\nhomogeneity in success amongst climbers. On the other hand, the existence of\nsmall, usually strong, subgroups lead to a polarized style where climbers that\nwere not a part of the subgroup were less likely to succeed. Lastly, this work\nexamines the effects of individual features and expedition-wide factors that\nmay play different roles in individualistic and cooperative expeditions.\nCentrality indicates that individual traits of youth and oxygen use while\nascending are strong drivers of success. Of expedition-wide factors, the\nexpedition size and number of expedition days are found to be strongly\ncorrelated with success rate.",
    "descriptor": "\nComments: 14 pages, 5 figures. arXiv admin note: text overlap with arXiv:2109.13340\n",
    "authors": [
      "Sanjukta Krishnagopal"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "General Topology (math.GN)"
    ],
    "url": "https://arxiv.org/abs/2201.13344"
  },
  {
    "id": "arXiv:2201.13372",
    "title": "Robust supervised learning with coordinate gradient descent",
    "abstract": "This paper considers the problem of supervised learning with linear methods\nwhen both features and labels can be corrupted, either in the form of heavy\ntailed data and/or corrupted rows. We introduce a combination of coordinate\ngradient descent as a learning algorithm together with robust estimators of the\npartial derivatives. This leads to robust statistical learning methods that\nhave a numerical complexity nearly identical to non-robust ones based on\nempirical risk minimization. The main idea is simple: while robust learning\nwith gradient descent requires the computational cost of robustly estimating\nthe whole gradient to update all parameters, a parameter can be updated\nimmediately using a robust estimator of a single partial derivative in\ncoordinate gradient descent. We prove upper bounds on the generalization error\nof the algorithms derived from this idea, that control both the optimization\nand statistical errors with and without a strong convexity assumption of the\nrisk. Finally, we propose an efficient implementation of this approach in a new\npython library called linlearn, and demonstrate through extensive numerical\nexperiments that our approach introduces a new interesting compromise between\nrobustness, statistical performance and numerical efficiency for this problem.",
    "descriptor": "\nComments: 57 pages, 6 figures\n",
    "authors": [
      "St\u00e9phane Ga\u00efffas",
      "Ibrahim Merad"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2201.13372"
  },
  {
    "id": "arXiv:2201.13373",
    "title": "Exact linear reduction for rational dynamical systems",
    "abstract": "Detailed dynamical systems models used in life sciences may include dozens or\neven hundreds of state variables. Models of large dimension are not only harder\nfrom the numerical perspective (e.g., for parameter estimation or simulation),\nbut it is also becoming challenging to derive mechanistic insights from such\nmodels. Exact model reduction is a way to address this issue by finding a\nself-consistent lower-dimensional projection of the corresponding dynamical\nsystem. A recent algorithm CLUE allows one to construct an exact linear\nreduction of the smallest possible dimension such that the fixed variables of\ninterest are preserved. However, CLUE is restricted to systems with polynomial\ndynamics. Since rational dynamics occurs frequently in the life sciences (e.g.,\nMichaelis-Menten or Hill kinetics), it is desirable to extend CLUE to the\nmodels with rational dynamics.\nIn this paper, we present an extension of CLUE to the case of rational\ndynamics and demonstrate its applicability on examples from literature. Our\nimplementation is available in version 1.5 of CLUE at\nhttps://github.com/pogudingleb/CLUE.",
    "descriptor": "\nComments: 21 pages, 4 tables, 1 figure\n",
    "authors": [
      "Antonio Jim\u00e9nez-Pastor",
      "Joshua Paul Jacob",
      "Gleb Pogudin"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Symbolic Computation (cs.SC)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.13373"
  },
  {
    "id": "arXiv:2201.13375",
    "title": "On the Innocuousness of Deterministic p-type Antithetic Integral  Controllers Arising in Integral Rein Control",
    "abstract": "The innocuousness property of a controller is that property that makes the\nclosed-loop system stable regardless the values of the controller parameters.\nIn other words, the closed-loop system exhibits some structural stability\nproperty with respect to the parameters of the controller. The innocuousness\nproperty was first emphasized in [Briat, Gupta, and Khammash, Cell Systems,\n2016] where it was shown that for stochastic unimolecular networks, the\nAntithetic Integral Controller (AIC) is innocuous under very mild conditions on\nthe controlled network; namely the ergodicity and the output-controllability of\nthe open-loop network, and the admissibility of the set-point value. We show\nhere that the class of p-type AIC controllers arising in the use of Antithetic\nIntegral Rein Controllers (AIRC) also exhibit such a property. It is shown in\nthe unimolecular reaction network case that the closed-loop network is\nstructurally stable with respect to the controller parameters provided that the\nopen-loop dynamics is stable and that the set-point is admissible. Those\nresults are then extended to the case of so-called output unstable linear\nsystems and to stable nonlinear networks. Some examples are given for\nillustration.",
    "descriptor": "\nComments: 20 pages, 1 figure\n",
    "authors": [
      "Corentin Briat",
      "Mustafa Khammash"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2201.13375"
  },
  {
    "id": "arXiv:2201.13380",
    "title": "Deep Learning Macroeconomics",
    "abstract": "Limited datasets and complex nonlinear relationships are among the challenges\nthat may emerge when applying econometrics to macroeconomic problems. This\nresearch proposes deep learning as an approach to transfer learning in the\nformer case and to map relationships between variables in the latter case.\nAlthough macroeconomists already apply transfer learning when assuming a given\na priori distribution in a Bayesian context, estimating a structural VAR with\nsignal restriction and calibrating parameters based on results observed in\nother models, to name a few examples, advance in a more systematic transfer\nlearning strategy in applied macroeconomics is the innovation we are\nintroducing. We explore the proposed strategy empirically, showing that data\nfrom different but related domains, a type of transfer learning, helps identify\nthe business cycle phases when there is no business cycle dating committee and\nto quick estimate a economic-based output gap. Next, since deep learning\nmethods are a way of learning representations, those that are formed by the\ncomposition of multiple non-linear transformations, to yield more abstract\nrepresentations, we apply deep learning for mapping low-frequency from\nhigh-frequency variables. The results obtained show the suitability of deep\nlearning models applied to macroeconomic problems. First, models learned to\nclassify United States business cycles correctly. Then, applying transfer\nlearning, they were able to identify the business cycles of out-of-sample\nBrazilian and European data. Along the same lines, the models learned to\nestimate the output gap based on the U.S. data and obtained good performance\nwhen faced with Brazilian data. Additionally, deep learning proved adequate for\nmapping low-frequency variables from high-frequency data to interpolate,\ndistribute, and extrapolate time series by related series.",
    "descriptor": "\nComments: Doctoral thesis\n",
    "authors": [
      "Rafael R. S. Guimaraes"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13380"
  },
  {
    "id": "arXiv:2201.13383",
    "title": "Fluctuations, Bias, Variance & Ensemble of Learners: Exact Asymptotics  for Convex Losses in High-Dimension",
    "abstract": "From the sampling of data to the initialisation of parameters, randomness is\nubiquitous in modern Machine Learning practice. Understanding the statistical\nfluctuations engendered by the different sources of randomness in prediction is\ntherefore key to understanding robust generalisation. In this manuscript we\ndevelop a quantitative and rigorous theory for the study of fluctuations in an\nensemble of generalised linear models trained on different, but correlated,\nfeatures in high-dimensions. In particular, we provide a complete description\nof the asymptotic joint distribution of the empirical risk minimiser for\ngeneric convex loss and regularisation in the high-dimensional limit. Our\nresult encompasses a rich set of classification and regression tasks, such as\nthe lazy regime of overparametrised neural networks, or equivalently the random\nfeatures approximation of kernels. While allowing to study directly the\nmitigating effect of ensembling (or bagging) on the bias-variance decomposition\nof the test error, our analysis also helps disentangle the contribution of\nstatistical fluctuations, and the singular role played by the interpolation\nthreshold that are at the roots of the \"double-descent\" phenomenon.",
    "descriptor": "\nComments: 17 pages + Appendix\n",
    "authors": [
      "Bruno Loureiro",
      "C\u00e9dric Gerbelot",
      "Maria Refinetti",
      "Gabriele Sicuro",
      "Florent Krzakala"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13383"
  },
  {
    "id": "arXiv:2201.13398",
    "title": "Spectral image clustering on dual-energy CT scans using functional  regression mixtures",
    "abstract": "Dual-energy computed tomography (DECT) is an advanced CT scanning technique\nenabling material characterization not possible with conventional CT scans. It\nallows the reconstruction of energy decay curves at each 3D image voxel,\nrepresenting varying image attenuation at different effective scanning energy\nlevels. In this paper, we develop novel functional data analysis (FDA)\ntechniques and adapt them to the analysis of DECT decay curves. More\nspecifically, we construct functional mixture models that integrate spatial\ncontext in mixture weights, with mixture component densities being constructed\nupon the energy decay curves as functional observations. We design unsupervised\nclustering algorithms by developing dedicated expectation maximization (EM)\nalgorithms for the maximum likelihood estimation of the model parameters. To\nour knowledge, this is the first article to adapt statistical FDA tools and\nmodel-based clustering to take advantage of the full spectral information\nprovided by DECT. We evaluate our methods on 91 head and neck cancer DECT\nscans. We compare our unsupervised clustering results to tumor contours traced\nmanually by radiologists, as well as to several baseline algorithms. Given the\ninter-rater variability even among experts at delineating head and neck tumors,\nand given the potential importance of tissue reactions surrounding the tumor\nitself, our proposed methodology has the potential to add value in downstream\nmachine learning applications for clinical outcome prediction based on DECT\ndata in head and neck cancer.",
    "descriptor": "\nComments: submitted to IEEE Transactions on Medical Imaging\n",
    "authors": [
      "Segolene Brivet",
      "Faicel Chamroukhi",
      "Mark Coates",
      "Reza Forghani",
      "Peter Savadjiev"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2201.13398"
  },
  {
    "id": "arXiv:2201.13409",
    "title": "A framework for bilevel optimization that enables stochastic and global  variance reduction algorithms",
    "abstract": "Bilevel optimization, the problem of minimizing a value function which\ninvolves the arg-minimum of another function, appears in many areas of machine\nlearning. In a large scale setting where the number of samples is huge, it is\ncrucial to develop stochastic methods, which only use a few samples at a time\nto progress. However, computing the gradient of the value function involves\nsolving a linear system, which makes it difficult to derive unbiased stochastic\nestimates. To overcome this problem we introduce a novel framework, in which\nthe solution of the inner problem, the solution of the linear system, and the\nmain variable evolve at the same time. These directions are written as a sum,\nmaking it straightforward to derive unbiased estimates. The simplicity of our\napproach allows us to develop global variance reduction algorithms, where the\ndynamics of all variables is subject to variance reduction. We demonstrate that\nSABA, an adaptation of the celebrated SAGA algorithm in our framework, has\n$O(\\frac1T)$ convergence rate, and that it achieves linear convergence under\nPolyak-Lojasciewicz assumption. This is the first stochastic algorithm for\nbilevel optimization that verifies either of these properties. Numerical\nexperiments validate the usefulness of our method.",
    "descriptor": "",
    "authors": [
      "Mathieu Dagr\u00e9ou",
      "Pierre Ablin",
      "Samuel Vaiter",
      "Thomas Moreau"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.13409"
  },
  {
    "id": "arXiv:2201.13446",
    "title": "A Note on the Relation between Recognisable Series and Regular  Sequences, and their Minimal Linear Representations",
    "abstract": "In this note, we precisely elaborate the connection between recognisable\nseries (in the sense of Berstel and Reutenauer) and $q$-regular sequences (in\nthe sense of Allouche and Shallit) via their linear representations. In\nparticular, we show that the minimisation algorithm for recognisable series can\nalso be used to minimise linear representations of $q$-regular sequences.",
    "descriptor": "",
    "authors": [
      "Clemens Heuberger",
      "Daniel Krenn",
      "Gabriel F. Lipnik"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2201.13446"
  },
  {
    "id": "arXiv:2201.13450",
    "title": "An efficient quantum algorithm for lattice problems achieving  subexponential approximation factor",
    "abstract": "We give a quantum algorithm for solving the Bounded Distance Decoding (BDD)\nproblem with a subexponential approximation factor on a class of integer\nlattices. The quantum algorithm uses a well-known but challenging-to-use\nquantum state on lattices as a type of approximate quantum eigenvector to\nrandomly self-reduce the BDD instance to a random BDD instance which is\nsolvable classically. The running time of the quantum algorithm is polynomial\nfor one range of approximation factors and subexponential time for a second\nrange of approximation factors.\nThe subclass of lattices we study has a natural description in terms of the\nlattice's periodicity and finite abelian group rank. This view makes for a\nclean quantum algorithm in terms of finite abelian groups, uses very relatively\nlittle from lattice theory, and suggests exploring approximation algorithms for\nlattice problems in parameters other than dimension alone.\nA talk on this paper sparked many lively discussions and resulted in a new\nclassical algorithm matching part of our result. We leave it as a challenge to\ngive a classcial algorithm matching the general case.",
    "descriptor": "",
    "authors": [
      "Lior Eldar",
      "Sean Hallgren"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.13450"
  },
  {
    "id": "arXiv:1701.06937",
    "title": "Optimizing tree decompositions in MSO",
    "abstract": "Optimizing tree decompositions in MSO",
    "descriptor": "",
    "authors": [
      "Miko\u0142aj Boja\u0144czyk",
      "Micha\u0142 Pilipczuk"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1701.06937"
  },
  {
    "id": "arXiv:1804.08992",
    "title": "Infrared and visible image fusion using Latent Low-Rank Representation",
    "abstract": "Comments: 6 pages, 8 figures, 1 tables",
    "descriptor": "\nComments: 6 pages, 8 figures, 1 tables\n",
    "authors": [
      "Hui Li",
      "Xiao-Jun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1804.08992"
  },
  {
    "id": "arXiv:1904.11592",
    "title": "Optical Flow Techniques for Facial Expression Analysis -- a Practical  Evaluation Study",
    "abstract": "Optical Flow Techniques for Facial Expression Analysis -- a Practical  Evaluation Study",
    "descriptor": "",
    "authors": [
      "Benjamin Allaert",
      "Isaac Ronald Ward",
      "Ioan Marius Bilasco",
      "Chaabane Djeraba",
      "Mohammed Bennamoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1904.11592"
  },
  {
    "id": "arXiv:1905.12461",
    "title": "On the Clique-Width of Unigraphs",
    "abstract": "On the Clique-Width of Unigraphs",
    "descriptor": "",
    "authors": [
      "Yu Nakahata"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1905.12461"
  },
  {
    "id": "arXiv:1908.10388",
    "title": "Singletons for Simpletons: Revisiting Windowed Backoff using Chernoff  Bounds",
    "abstract": "Comments: Corrections to first version",
    "descriptor": "\nComments: Corrections to first version\n",
    "authors": [
      "Qian M. Zhou",
      "Alice Calvert",
      "Maxwell Young"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1908.10388"
  },
  {
    "id": "arXiv:1909.07972",
    "title": "A Joint Learning and Communications Framework for Federated Learning  over Wireless Networks",
    "abstract": "Comments: This paper has been accepted by IEEE Transactions on Wireless Communications. Our code is available at: this https URL",
    "descriptor": "\nComments: This paper has been accepted by IEEE Transactions on Wireless Communications. Our code is available at: this https URL\n",
    "authors": [
      "Mingzhe Chen",
      "Zhaohui Yang",
      "Walid Saad",
      "Changchuan Yin",
      "H. Vincent Poor",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1909.07972"
  },
  {
    "id": "arXiv:1910.01615",
    "title": "Fair Division of Goods in the Shadow of Market Values",
    "abstract": "Comments: 42 pages, 4 figure, 15 tables",
    "descriptor": "\nComments: 42 pages, 4 figure, 15 tables\n",
    "authors": [
      "Marco Dall'Aglio"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/1910.01615"
  },
  {
    "id": "arXiv:1910.09359",
    "title": "Building Efficient CNNs Using Depthwise Convolutional Eigen-Filters  (DeCEF)",
    "abstract": "Comments: key words: subspace analysis, convolutional neural networks, FLOPs, number of parameters, depthwise separable convolutions",
    "descriptor": "\nComments: key words: subspace analysis, convolutional neural networks, FLOPs, number of parameters, depthwise separable convolutions\n",
    "authors": [
      "Yinan Yu",
      "Samuel Scheidegger",
      "Tomas McKelvey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.09359"
  },
  {
    "id": "arXiv:1912.07959",
    "title": "Multi-focus Image Fusion Based on Similarity Characteristics",
    "abstract": "Comments: 7 pages, 10 figures, 4 tables",
    "descriptor": "\nComments: 7 pages, 10 figures, 4 tables\n",
    "authors": [
      "Ya-Qiong Zhang",
      "Xiao-Jun Wu",
      "Hui Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/1912.07959"
  },
  {
    "id": "arXiv:1912.08805",
    "title": "Pseudospectral Shattering, the Sign Function, and Diagonalization in  Nearly Matrix Multiplication Time",
    "abstract": "Comments: 83 pages, 3 figures, comments welcome. Slightly edited intro from first version + explicit statement of forward error Theorem (Corolary 1.7). Minor corrections, new references and clarifications. Appendix with some new proofs added",
    "descriptor": "\nComments: 83 pages, 3 figures, comments welcome. Slightly edited intro from first version + explicit statement of forward error Theorem (Corolary 1.7). Minor corrections, new references and clarifications. Appendix with some new proofs added\n",
    "authors": [
      "Jess Banks",
      "Jorge Garza-Vargas",
      "Archit Kulkarni",
      "Nikhil Srivastava"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Functional Analysis (math.FA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/1912.08805"
  },
  {
    "id": "arXiv:2002.01102",
    "title": "Improved dual channel pulse coupled neural network and its application  to multi-focus image fusion",
    "abstract": "Comments: 16 pages, 7 figures, 9 tables",
    "descriptor": "\nComments: 16 pages, 7 figures, 9 tables\n",
    "authors": [
      "Huai-Shui Tong",
      "Xiao-Jun Wu",
      "Hui Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2002.01102"
  },
  {
    "id": "arXiv:2002.04727",
    "title": "A simple certifying algorithm for 3-edge-connectivity",
    "abstract": "Comments: Section 3.2 has been rewritten to improve clarity. In Section 3.3, the correctness proof is elaborated. Figures 5 and 8 are shifted to the end as an appendix. Their captions are more detailed",
    "descriptor": "\nComments: Section 3.2 has been rewritten to improve clarity. In Section 3.3, the correctness proof is elaborated. Figures 5 and 8 are shifted to the end as an appendix. Their captions are more detailed\n",
    "authors": [
      "Yung H. Tsin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2002.04727"
  },
  {
    "id": "arXiv:2002.07153",
    "title": "Cover Combinatorial Filters and their Minimization Problem (Extended  Version)",
    "abstract": "Comments: 20 pages, 9 figures, WAFR 2020",
    "descriptor": "\nComments: 20 pages, 9 figures, WAFR 2020\n",
    "authors": [
      "Yulin Zhang",
      "Dylan A. Shell"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2002.07153"
  },
  {
    "id": "arXiv:2003.00937",
    "title": "Buffered Asynchronous SGD for Byzantine Learning",
    "abstract": "Buffered Asynchronous SGD for Byzantine Learning",
    "descriptor": "",
    "authors": [
      "Yi-Rui Yang",
      "Wu-Jun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.00937"
  },
  {
    "id": "arXiv:2003.06777",
    "title": "DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning",
    "abstract": "Comments: Extended version of DeepEMD in CVPR2020 (oral)",
    "descriptor": "\nComments: Extended version of DeepEMD in CVPR2020 (oral)\n",
    "authors": [
      "Chi Zhang",
      "Yujun Cai",
      "Guosheng Lin",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.06777"
  },
  {
    "id": "arXiv:2003.10622",
    "title": "Adaptive Cooperative Tracking and Parameter Estimation of an Uncertain  Leader over General Directed Graphs",
    "abstract": "Adaptive Cooperative Tracking and Parameter Estimation of an Uncertain  Leader over General Directed Graphs",
    "descriptor": "",
    "authors": [
      "Shimin Wang",
      "Hongwei Zhang",
      "Zhiyong Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2003.10622"
  },
  {
    "id": "arXiv:2004.08883",
    "title": "Variational Policy Propagation for Multi-agent Reinforcement Learning",
    "abstract": "Comments: The title of previous version was \"Intention Propagation for Multi-agent Reinforcement Learning\"",
    "descriptor": "\nComments: The title of previous version was \"Intention Propagation for Multi-agent Reinforcement Learning\"\n",
    "authors": [
      "Chao Qu",
      "Hui Li",
      "Chang Liu",
      "Junwu Xiong",
      "James Zhang",
      "Wei Chu",
      "Weiqiang Wang",
      "Yuan Qi",
      "Le Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.08883"
  },
  {
    "id": "arXiv:2005.05144",
    "title": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian  Portuguese",
    "abstract": "TTS-Portuguese Corpus: a corpus for speech synthesis in Brazilian  Portuguese",
    "descriptor": "",
    "authors": [
      "Edresson Casanova",
      "Arnaldo Candido Junior",
      "Christopher Shulby",
      "Frederico Santos de Oliveira",
      "Jo\u00e3o Paulo Teixeira",
      "Moacir Antonelli Ponti",
      "Sandra Maria Aluisio"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2005.05144"
  },
  {
    "id": "arXiv:2005.07275",
    "title": "Variational Inference as Iterative Projection in a Bayesian Hilbert  Space",
    "abstract": "Comments: 38 pages, 10 figures, submitted to Robotica",
    "descriptor": "\nComments: 38 pages, 10 figures, submitted to Robotica\n",
    "authors": [
      "Timothy D. Barfoot",
      "Gabriele M. T. D'Eleuterio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.07275"
  },
  {
    "id": "arXiv:2005.14220",
    "title": "Task-Based Information Compression for Multi-Agent Communication  Problems with Channel Rate Constraints",
    "abstract": "Comments: 15 pages, 8 figures",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Arsham Mostaani",
      "Thang X. Vu",
      "Symeon Chatzinotas",
      "Bj\u00f6rn Ottersten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2005.14220"
  },
  {
    "id": "arXiv:2006.04652",
    "title": "Satisfiability and Model Checking for the Logic of Sub-Intervals under  the Homogeneity Assumption",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1901.03880",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1901.03880\n",
    "authors": [
      "Laura Bozzelli",
      "Alberto Molinari",
      "Angelo Montanari",
      "Adriano Peron",
      "Pietro Sala"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2006.04652"
  },
  {
    "id": "arXiv:2006.05161",
    "title": "Provable tradeoffs in adversarially robust classification",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. 47 pages, 5 figures",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. 47 pages, 5 figures\n",
    "authors": [
      "Edgar Dobriban",
      "Hamed Hassani",
      "David Hong",
      "Alexander Robey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.05161"
  },
  {
    "id": "arXiv:2006.15221",
    "title": "Semi-discrete optimization through semi-discrete optimal transport: a  framework for neural architecture search",
    "abstract": "Semi-discrete optimization through semi-discrete optimal transport: a  framework for neural architecture search",
    "descriptor": "",
    "authors": [
      "Nicolas Garcia Trillos",
      "Javier Morales"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.15221"
  },
  {
    "id": "arXiv:2008.07381",
    "title": "Dissecting liabilities in adversarial surgical robot failures: A  national (Danish) and European law perspective",
    "abstract": "Comments: 43 pages, 3 figures, forthcoming in CLSR",
    "descriptor": "\nComments: 43 pages, 3 figures, forthcoming in CLSR\n",
    "authors": [
      "Kaspar Rosager Ludvigsen",
      "Shishir Nagaraja"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2008.07381"
  },
  {
    "id": "arXiv:2008.12233",
    "title": "Exploring British Accents: Modelling the Trap-Bath Split with Functional  Data Analysis",
    "abstract": "Comments: 45 pages, 27 figures",
    "descriptor": "\nComments: 45 pages, 27 figures\n",
    "authors": [
      "Aranya Koshy",
      "Shahin Tavakoli"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2008.12233"
  },
  {
    "id": "arXiv:2008.12515",
    "title": "On modularity in reactive control architectures, with an application to  formal verification",
    "abstract": "Comments: Accepted to ACM Transactions on Cyber-Physical Systems. 26 pages, 9 figures. Version 3 changes: accepted version, minor changes",
    "descriptor": "\nComments: Accepted to ACM Transactions on Cyber-Physical Systems. 26 pages, 9 figures. Version 3 changes: accepted version, minor changes\n",
    "authors": [
      "Oliver Biggar",
      "Mohammad Zamani",
      "Iman Shames"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2008.12515"
  },
  {
    "id": "arXiv:2009.01492",
    "title": "Explainable Empirical Risk Minimization",
    "abstract": "Explainable Empirical Risk Minimization",
    "descriptor": "",
    "authors": [
      "L. Zhang",
      "G. Karakasidis",
      "A. Odnoblyudova",
      "L. Dogruel",
      "A. Jung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.01492"
  },
  {
    "id": "arXiv:2009.03968",
    "title": "Equations in virtually abelian groups: languages and growth",
    "abstract": "Comments: Final version, to appear in Internat. J. Algebra Comput",
    "descriptor": "\nComments: Final version, to appear in Internat. J. Algebra Comput\n",
    "authors": [
      "Alex Evetts",
      "Alex Levine"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2009.03968"
  },
  {
    "id": "arXiv:2010.01792",
    "title": "Can we Generalize and Distribute Private Representation Learning?",
    "abstract": "Comments: In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022",
    "descriptor": "\nComments: In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022\n",
    "authors": [
      "Sheikh Shams Azam",
      "Taejin Kim",
      "Seyyedali Hosseinalipour",
      "Carlee Joe-Wong",
      "Saurabh Bagchi",
      "Christopher Brinton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.01792"
  },
  {
    "id": "arXiv:2010.08936",
    "title": "Arboricity games: the core and the nucleolus",
    "abstract": "Arboricity games: the core and the nucleolus",
    "descriptor": "",
    "authors": [
      "Han Xiao",
      "Qizhi Fang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2010.08936"
  },
  {
    "id": "arXiv:2010.11136",
    "title": "Adaptive Frequency-Regulation Demand Response Using Substation Solar  Irradiance Measurement in High-PV Power Systems",
    "abstract": "Comments: 3 pages, 2 figures",
    "descriptor": "\nComments: 3 pages, 2 figures\n",
    "authors": [
      "Shutang You"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.11136"
  },
  {
    "id": "arXiv:2010.11150",
    "title": "Large-Scale High PV Power Grid Dynamic Model Development -- A Case Study  on the U.S. Eastern Interconnection",
    "abstract": "Comments: 9 pages, 12 figures",
    "descriptor": "\nComments: 9 pages, 12 figures\n",
    "authors": [
      "Shutang You"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.11150"
  },
  {
    "id": "arXiv:2010.11218",
    "title": "Compressive Sensing Based Situational Awareness and Sensor Placement for  DC Microgrids with Relatively Fixed Operation Patterns",
    "abstract": "Comments: 6 pages, 9 figures",
    "descriptor": "\nComments: 6 pages, 9 figures\n",
    "authors": [
      "Shutang You",
      "Yilu Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.11218"
  },
  {
    "id": "arXiv:2010.11340",
    "title": "Photovoltaic (PV) Virtual Inertia and Fast Frequency Regulation in High  PV Power Grids",
    "abstract": "Comments: 7 pages, 14 figures. arXiv admin note: text overlap with arXiv:2010.13316",
    "descriptor": "\nComments: 7 pages, 14 figures. arXiv admin note: text overlap with arXiv:2010.13316\n",
    "authors": [
      "Shutang You"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.11340"
  },
  {
    "id": "arXiv:2010.11994",
    "title": "Thresholded Lasso Bandit",
    "abstract": "Thresholded Lasso Bandit",
    "descriptor": "",
    "authors": [
      "Kaito Ariu",
      "Kenshi Abe",
      "Alexandre Prouti\u00e8re"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.11994"
  },
  {
    "id": "arXiv:2010.12379",
    "title": "A Simulation-based Education Approach for the Electromagnetic and  Electromechanical Transient Waves in Power Systems",
    "abstract": "Comments: 8 pages, 16 figures",
    "descriptor": "\nComments: 8 pages, 16 figures\n",
    "authors": [
      "Abdulelah Alharbi",
      "Shutang You"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.12379"
  },
  {
    "id": "arXiv:2010.14548",
    "title": "Relatively Complete Verification of Probabilistic Programs",
    "abstract": "Relatively Complete Verification of Probabilistic Programs",
    "descriptor": "",
    "authors": [
      "Kevin Batz",
      "Benjamin Lucien Kaminski",
      "Joost-Pieter Katoen",
      "Christoph Matheja"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2010.14548"
  },
  {
    "id": "arXiv:2010.15490",
    "title": "Linearizing Combinators",
    "abstract": "Linearizing Combinators",
    "descriptor": "",
    "authors": [
      "Robin Cockett",
      "Jean-Simon Pacaud Lemay"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2010.15490"
  },
  {
    "id": "arXiv:2010.15922",
    "title": "A configurable computer simulation model for reducing patient waiting  time in oncology departments",
    "abstract": "Comments: 40 pages, 3 figures, 7 tables",
    "descriptor": "\nComments: 40 pages, 3 figures, 7 tables\n",
    "authors": [
      "R. R. Corsini",
      "A. Costa",
      "S. Fichera",
      "A.Pluchino"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2010.15922"
  },
  {
    "id": "arXiv:2011.00629",
    "title": "Distances between probability distributions of different dimensions",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Yuhang Cai",
      "Lek-Heng Lim"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2011.00629"
  },
  {
    "id": "arXiv:2011.05001",
    "title": "Integral Probability Metric based Regularization for Optimal Transport",
    "abstract": "Integral Probability Metric based Regularization for Optimal Transport",
    "descriptor": "",
    "authors": [
      "Piyushi Manupriya",
      "J. Saketha Nath",
      "Pratik Jawanpuria"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2011.05001"
  },
  {
    "id": "arXiv:2011.14165",
    "title": "Verifying liquidity of recursive Bitcoin contracts",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2003.00296",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2003.00296\n",
    "authors": [
      "Massimo Bartoletti",
      "Stefano Lande",
      "Maurizio Murgia",
      "Roberto Zunino"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2011.14165"
  },
  {
    "id": "arXiv:2011.14330",
    "title": "A Boundary Regression Model for Nested Named Entity Recognition",
    "abstract": "A Boundary Regression Model for Nested Named Entity Recognition",
    "descriptor": "",
    "authors": [
      "Yanping Chen",
      "Lefei Wu",
      "Qinghua Zheng",
      "Ruizhang Huang",
      "Jun Liu",
      "Liyuan Deng",
      "Junhui Yu",
      "Yongbin Qing",
      "Bo Dong",
      "Ping Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.14330"
  },
  {
    "id": "arXiv:2012.04185",
    "title": "Formalism-Driven Development of Decentralized Systems",
    "abstract": "Comments: To appear in ICECCS 2022",
    "descriptor": "\nComments: To appear in ICECCS 2022\n",
    "authors": [
      "Yepeng Ding",
      "Hiroyuki Sato"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2012.04185"
  },
  {
    "id": "arXiv:2012.06695",
    "title": "Generating Adversarial Disturbances for Controller Verification",
    "abstract": "Generating Adversarial Disturbances for Controller Verification",
    "descriptor": "",
    "authors": [
      "Udaya Ghai",
      "David Snyder",
      "Anirudha Majumdar",
      "Elad Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.06695"
  },
  {
    "id": "arXiv:2012.07139",
    "title": "FSOCO: The Formula Student Objects in Context Dataset",
    "abstract": "FSOCO: The Formula Student Objects in Context Dataset",
    "descriptor": "",
    "authors": [
      "Niclas V\u00f6disch",
      "David Dodel",
      "Michael Sch\u00f6tz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2012.07139"
  },
  {
    "id": "arXiv:2101.04041",
    "title": "Evaluating Disentanglement of Structured Representations",
    "abstract": "Evaluating Disentanglement of Structured Representations",
    "descriptor": "",
    "authors": [
      "Rapha\u00ebl Dang-Nhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.04041"
  },
  {
    "id": "arXiv:2101.05940",
    "title": "Implicit Surface Reconstruction with a Curl-free Radial Basis Function  Partition of Unity Method",
    "abstract": "Implicit Surface Reconstruction with a Curl-free Radial Basis Function  Partition of Unity Method",
    "descriptor": "",
    "authors": [
      "Kathryn P. Drake",
      "Edward J. Fuselier",
      "Grady B. Wright"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.05940"
  },
  {
    "id": "arXiv:2101.06757",
    "title": "Higher Order Automatic Differentiation of Higher Order Functions",
    "abstract": "Comments: 34 pages, 5 figures, submitted at LMCS 2020. arXiv admin note: substantial text overlap with arXiv:2001.02209",
    "descriptor": "\nComments: 34 pages, 5 figures, submitted at LMCS 2020. arXiv admin note: substantial text overlap with arXiv:2001.02209\n",
    "authors": [
      "Mathieu Huot",
      "Sam Staton",
      "Matthijs V\u00e1k\u00e1r"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.06757"
  },
  {
    "id": "arXiv:2101.12653",
    "title": "Non-adaptive Combinatorial Quantitative Group Testing with Adversarially  Perturbed Measurements",
    "abstract": "Non-adaptive Combinatorial Quantitative Group Testing with Adversarially  Perturbed Measurements",
    "descriptor": "",
    "authors": [
      "Yun-Han Li",
      "I-Hsiang Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2101.12653"
  },
  {
    "id": "arXiv:2102.03926",
    "title": "Lower Bounds and Accelerated Algorithms for Bilevel Optimization",
    "abstract": "Comments: 53 pages, 3 Table",
    "descriptor": "\nComments: 53 pages, 3 Table\n",
    "authors": [
      "Kaiyi Ji",
      "Yingbin Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.03926"
  },
  {
    "id": "arXiv:2102.05174",
    "title": "On the Hardness of PAC-learning Stabilizer States with Noise",
    "abstract": "On the Hardness of PAC-learning Stabilizer States with Noise",
    "descriptor": "",
    "authors": [
      "Aravind Gollakota",
      "Daniel Liang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05174"
  },
  {
    "id": "arXiv:2102.06559",
    "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations",
    "abstract": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential  Equations",
    "descriptor": "",
    "authors": [
      "Winnie Xu",
      "Ricky T.Q. Chen",
      "Xuechen Li",
      "David Duvenaud"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06559"
  },
  {
    "id": "arXiv:2102.06737",
    "title": "Kronecker-factored Quasi-Newton Methods for Deep Learning",
    "abstract": "Kronecker-factored Quasi-Newton Methods for Deep Learning",
    "descriptor": "",
    "authors": [
      "Yi Ren",
      "Achraf Bahamou",
      "Donald Goldfarb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06737"
  },
  {
    "id": "arXiv:2102.07432",
    "title": "Fast and accurate optimization on the orthogonal manifold without  retraction",
    "abstract": "Fast and accurate optimization on the orthogonal manifold without  retraction",
    "descriptor": "",
    "authors": [
      "Pierre Ablin",
      "Gabriel Peyr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2102.07432"
  },
  {
    "id": "arXiv:2102.09407",
    "title": "Adaptive Rational Activations to Boost Deep Reinforcement Learning",
    "abstract": "Comments: Main paper: 8 pages, References: 2 pages, Appendix: 8 pages. Main paper: 4 figures, Appendix: 3 figures. Rational Activation Functions repository: this https URL Rational Supervised Learning: this https URL Rational Reinforcement Learning: this https URL",
    "descriptor": "\nComments: Main paper: 8 pages, References: 2 pages, Appendix: 8 pages. Main paper: 4 figures, Appendix: 3 figures. Rational Activation Functions repository: this https URL Rational Supervised Learning: this https URL Rational Reinforcement Learning: this https URL\n",
    "authors": [
      "Quentin Delfosse",
      "Patrick Schramowski",
      "Martin Mundt",
      "Alejandro Molina",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09407"
  },
  {
    "id": "arXiv:2102.09700",
    "title": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
    "abstract": "AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods",
    "descriptor": "",
    "authors": [
      "Zheng Shi",
      "Abdurakhmon Sadiev",
      "Nicolas Loizou",
      "Peter Richt\u00e1rik",
      "Martin Tak\u00e1\u010d"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.09700"
  },
  {
    "id": "arXiv:2102.10769",
    "title": "MobILE: Model-Based Imitation Learning From Observation Alone",
    "abstract": "Comments: 29 pages, 7 figures",
    "descriptor": "\nComments: 29 pages, 7 figures\n",
    "authors": [
      "Rahul Kidambi",
      "Jonathan Chang",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.10769"
  },
  {
    "id": "arXiv:2102.10869",
    "title": "Introducing a Novel Data over Voice Technique for Secure Voice  Communication",
    "abstract": "Comments: 22 pages, 43 figures; submitted to Wireless Personal Communications, Springer on 17 Jul. 2020; initially accepted on 13 Apr. 2021; revised on 27 Apr. 2021; finally accepted on 6 Jan. 2022; published on 25 Jun. 2022; this work is supported by grant DGA Cifre-Defense program No 01D17022178 DGA/DS/MRIS and AID program No SED0456JE75",
    "descriptor": "\nComments: 22 pages, 43 figures; submitted to Wireless Personal Communications, Springer on 17 Jul. 2020; initially accepted on 13 Apr. 2021; revised on 27 Apr. 2021; finally accepted on 6 Jan. 2022; published on 25 Jun. 2022; this work is supported by grant DGA Cifre-Defense program No 01D17022178 DGA/DS/MRIS and AID program No SED0456JE75\n",
    "authors": [
      "Piotr Krasnowski",
      "Jerome Lebrun",
      "Bruno Martin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2102.10869"
  },
  {
    "id": "arXiv:2102.12466",
    "title": "Information Directed Reward Learning for Reinforcement Learning",
    "abstract": "Comments: Presented at Conference on Neural Information Processing Systems (NeurIPS), 2021",
    "descriptor": "\nComments: Presented at Conference on Neural Information Processing Systems (NeurIPS), 2021\n",
    "authors": [
      "David Lindner",
      "Matteo Turchetta",
      "Sebastian Tschiatschek",
      "Kamil Ciosek",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12466"
  },
  {
    "id": "arXiv:2103.04046",
    "title": "Simplicial Complex Representation Learning",
    "abstract": "Comments: MACHINE LEARNING ON GRAPHS, MLoG Workshop at WSDM'22",
    "descriptor": "\nComments: MACHINE LEARNING ON GRAPHS, MLoG Workshop at WSDM'22\n",
    "authors": [
      "Mustafa Hajij",
      "Ghada Zamzmi",
      "Theodore Papamarkou",
      "Vasileios Maroulas",
      "Xuanting Cai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Algebraic Topology (math.AT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.04046"
  },
  {
    "id": "arXiv:2103.06426",
    "title": "XDO: A Double Oracle Algorithm for Extensive-Form Games",
    "abstract": "Comments: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)",
    "descriptor": "\nComments: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Stephen McAleer",
      "John Lanier",
      "Kevin Wang",
      "Pierre Baldi",
      "Roy Fox"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2103.06426"
  },
  {
    "id": "arXiv:2103.07972",
    "title": "Open-independent, open-locating-dominating sets: structural aspects of  some classes of graphs",
    "abstract": "Comments: 18 pages, 5 figures",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "M\u00e1rcia R. Cappelle",
      "Erika Coelho",
      "Les R. Foulds",
      "Humberto J. Longo"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2103.07972"
  },
  {
    "id": "arXiv:2103.10185",
    "title": "About subordinated generalizations of 3 classical models of option  pricing",
    "abstract": "About subordinated generalizations of 3 classical models of option  pricing",
    "descriptor": "",
    "authors": [
      "Micha\u0142 Balcerek",
      "Grzegorz Krzy\u017canowski",
      "Marcin Magdziarz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.10185"
  },
  {
    "id": "arXiv:2103.11824",
    "title": "Big Data for Traffic Estimation and Prediction: A Survey of Data and  Tools",
    "abstract": "Big Data for Traffic Estimation and Prediction: A Survey of Data and  Tools",
    "descriptor": "",
    "authors": [
      "Weiwei Jiang",
      "Jiayun Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2103.11824"
  },
  {
    "id": "arXiv:2103.16827",
    "title": "Integer-only Zero-shot Quantization for Efficient Speech Recognition",
    "abstract": "Integer-only Zero-shot Quantization for Efficient Speech Recognition",
    "descriptor": "",
    "authors": [
      "Sehoon Kim",
      "Amir Gholami",
      "Zhewei Yao",
      "Nicholas Lee",
      "Patrick Wang",
      "Aniruddha Nrusimha",
      "Bohan Zhai",
      "Tianren Gao",
      "Michael W. Mahoney",
      "Kurt Keutzer"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2103.16827"
  },
  {
    "id": "arXiv:2103.17235",
    "title": "FANet: A Feedback Attention Network for Improved Biomedical Image  Segmentation",
    "abstract": "FANet: A Feedback Attention Network for Improved Biomedical Image  Segmentation",
    "descriptor": "",
    "authors": [
      "Nikhil Kumar Tomar",
      "Debesh Jha",
      "Michael A. Riegler",
      "H\u00e5vard D. Johansen",
      "Dag Johansen",
      "Jens Rittscher",
      "P\u00e5l Halvorsen",
      "Sharib Ali"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2103.17235"
  },
  {
    "id": "arXiv:2104.03169",
    "title": "Empowering Prosumer Communities in Smart Grid with Wireless  Communications and Federated Edge Learning",
    "abstract": "Empowering Prosumer Communities in Smart Grid with Wireless  Communications and Federated Edge Learning",
    "descriptor": "",
    "authors": [
      "Afaf Taik",
      "Boubakr Nour",
      "Soumaya Cherkaoui"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2104.03169"
  },
  {
    "id": "arXiv:2104.03712",
    "title": "The human side of Software Engineering Teams: an investigation of  contemporary challenges",
    "abstract": "Comments: 18 pages, 6 Figures, Accepted by IEEE TSE",
    "descriptor": "\nComments: 18 pages, 6 Figures, Accepted by IEEE TSE\n",
    "authors": [
      "Marco Hoffmann",
      "Daniel Mendez",
      "Fabian Fagerholm",
      "Anton Luckhardt"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2104.03712"
  },
  {
    "id": "arXiv:2104.04465",
    "title": "Bootstrapping Semantic Segmentation with Regional Contrast",
    "abstract": "Comments: Published at ICLR 2022. Project Page: this https URL Code: this https URL",
    "descriptor": "\nComments: Published at ICLR 2022. Project Page: this https URL Code: this https URL\n",
    "authors": [
      "Shikun Liu",
      "Shuaifeng Zhi",
      "Edward Johns",
      "Andrew J. Davison"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.04465"
  },
  {
    "id": "arXiv:2104.04892",
    "title": "Improving the Feasibility of Moment-Based Safety Analysis for Stochastic  Dynamics",
    "abstract": "Improving the Feasibility of Moment-Based Safety Analysis for Stochastic  Dynamics",
    "descriptor": "",
    "authors": [
      "Peter Du",
      "Katherine Driggs-Campbell",
      "Roy Dong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.04892"
  },
  {
    "id": "arXiv:2104.05348",
    "title": "Quotients of Bounded Natural Functors",
    "abstract": "Quotients of Bounded Natural Functors",
    "descriptor": "",
    "authors": [
      "Basil F\u00fcrer",
      "Andreas Lochbihler",
      "Joshua Schneider",
      "Dmitriy Traytel"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2104.05348"
  },
  {
    "id": "arXiv:2104.06280",
    "title": "Fair Allocation of Conflicting Items",
    "abstract": "Comments: 33 pages; Revised manuscript; Experiment code included; Published in JAAMAS",
    "descriptor": "\nComments: 33 pages; Revised manuscript; Experiment code included; Published in JAAMAS\n",
    "authors": [
      "Halvard Hummel",
      "Magnus Lie Hetland"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2104.06280"
  },
  {
    "id": "arXiv:2104.06283",
    "title": "Energy Efficiency Optimization of Reconfigurable Intelligent Surfaces  with Electromagnetic Field Exposure Constraints",
    "abstract": "Energy Efficiency Optimization of Reconfigurable Intelligent Surfaces  with Electromagnetic Field Exposure Constraints",
    "descriptor": "",
    "authors": [
      "Alessio Zappone",
      "Marco Di Renzo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2104.06283"
  },
  {
    "id": "arXiv:2104.07631",
    "title": "Fair and Reliable Reconnections for Temporary Disruptions in Electric  Distribution Networks using Submodularity",
    "abstract": "Comments: 44 pages, 9 figures",
    "descriptor": "\nComments: 44 pages, 9 figures\n",
    "authors": [
      "Cyrus Hettle",
      "Swati Gupta",
      "Daniel Molzahn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2104.07631"
  },
  {
    "id": "arXiv:2104.10480",
    "title": "Print Your Own Money: A Cash-Like Experience for Digital Payment Systems",
    "abstract": "Print Your Own Money: A Cash-Like Experience for Digital Payment Systems",
    "descriptor": "",
    "authors": [
      "Ye Wang",
      "Chenhang Zhou",
      "Yu Chen",
      "Shenyi Wang",
      "Xiaochen Zheng",
      "Roger Wattenhofer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2104.10480"
  },
  {
    "id": "arXiv:2104.11906",
    "title": "A Review on C3I Systems' Security: Vulnerabilities, Attacks, and  Countermeasures",
    "abstract": "A Review on C3I Systems' Security: Vulnerabilities, Attacks, and  Countermeasures",
    "descriptor": "",
    "authors": [
      "Hussain Ahmad",
      "Isuru Dharmadasa",
      "Faheem Ullah",
      "M. Ali Babar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.11906"
  },
  {
    "id": "arXiv:2104.14669",
    "title": "Extracting total Amb programs from proofs",
    "abstract": "Extracting total Amb programs from proofs",
    "descriptor": "",
    "authors": [
      "Ulrich Berger",
      "Hideki Tsuiki"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.14669"
  },
  {
    "id": "arXiv:2105.01869",
    "title": "Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model  Compression",
    "abstract": "Comments: ICLR 2022 Accepted",
    "descriptor": "\nComments: ICLR 2022 Accepted\n",
    "authors": [
      "Baeseong Park",
      "Se Jung Kwon",
      "Daehwan Oh",
      "Byeongwook Kim",
      "Dongsoo Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.01869"
  },
  {
    "id": "arXiv:2105.03322",
    "title": "Are Pre-trained Convolutions Better than Pre-trained Transformers?",
    "abstract": "Comments: ACL'21 + updated code/ckpt pointers",
    "descriptor": "\nComments: ACL'21 + updated code/ckpt pointers\n",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Jai Gupta",
      "Dara Bahri",
      "Vamsi Aribandi",
      "Zhen Qin",
      "Donald Metzler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.03322"
  },
  {
    "id": "arXiv:2105.07451",
    "title": "MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image  Segmentation",
    "abstract": "MSRF-Net: A Multi-Scale Residual Fusion Network for Biomedical Image  Segmentation",
    "descriptor": "",
    "authors": [
      "Abhishek Srivastava",
      "Debesh Jha",
      "Sukalpa Chanda",
      "Umapada Pal",
      "H\u00e5vard D. Johansen",
      "Dag Johansen",
      "Michael A. Riegler",
      "Sharib Ali",
      "P\u00e5l Halvorsen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.07451"
  },
  {
    "id": "arXiv:2105.07583",
    "title": "It\u00f4TTS and It\u00f4Wave: Linear Stochastic Differential Equation Is All  You Need For Audio Generation",
    "abstract": "Comments: The generated audio samples are available at this https URL",
    "descriptor": "\nComments: The generated audio samples are available at this https URL\n",
    "authors": [
      "Shoule Wu",
      "Ziqiang Shi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2105.07583"
  },
  {
    "id": "arXiv:2105.07623",
    "title": "Sentence Similarity Based on Contexts",
    "abstract": "Comments: Accepted by TACL; pre-MIT Press publication version",
    "descriptor": "\nComments: Accepted by TACL; pre-MIT Press publication version\n",
    "authors": [
      "Xiaofei Sun",
      "Yuxian Meng",
      "Xiang Ao",
      "Fei Wu",
      "Tianwei Zhang",
      "Jiwei Li",
      "Chun Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.07623"
  },
  {
    "id": "arXiv:2105.08590",
    "title": "UncertaintyFuseNet: Robust Uncertainty-aware Hierarchical Feature Fusion  Model with Ensemble Monte Carlo Dropout for COVID-19 Detection",
    "abstract": "Comments: 16 pages, 18 figures",
    "descriptor": "\nComments: 16 pages, 18 figures\n",
    "authors": [
      "Moloud Abdar",
      "Soorena Salari",
      "Sina Qahremani",
      "Hak-Keung Lam",
      "Fakhri Karray",
      "Sadiq Hussain",
      "Abbas Khosravi",
      "U. Rajendra Acharya",
      "Vladimir Makarenkov",
      "Saeid Nahavandi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.08590"
  },
  {
    "id": "arXiv:2105.09557",
    "title": "Power-law escape rate of SGD",
    "abstract": "Comments: 17+8 pages",
    "descriptor": "\nComments: 17+8 pages\n",
    "authors": [
      "Takashi Mori",
      "Liu Ziyin",
      "Kangqiao Liu",
      "Masahito Ueda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.09557"
  },
  {
    "id": "arXiv:2105.10176",
    "title": "Efficient Temporal Piecewise-Linear Numeric Planning with Lazy  Consistency Checking",
    "abstract": "Comments: Accepted version to be published in IEEE Transactions on Artificial Intelligence",
    "descriptor": "\nComments: Accepted version to be published in IEEE Transactions on Artificial Intelligence\n",
    "authors": [
      "Josef Bajada",
      "Maria Fox",
      "Derek Long"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.10176"
  },
  {
    "id": "arXiv:2105.11640",
    "title": "Safe Model-based Off-policy Reinforcement Learning for Eco-Driving in  Connected and Automated Hybrid Electric Vehicles",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication and is under review. Paper summary: 13 pages, 11 figures",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication and is under review. Paper summary: 13 pages, 11 figures\n",
    "authors": [
      "Zhaoxuan Zhu",
      "Nicola Pivaro",
      "Shobhit Gupta",
      "Abhishek Gupta",
      "Marcello Canova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2105.11640"
  },
  {
    "id": "arXiv:2105.11674",
    "title": "Unbiased Asymmetric Reinforcement Learning under Partial Observability",
    "abstract": "Unbiased Asymmetric Reinforcement Learning under Partial Observability",
    "descriptor": "",
    "authors": [
      "Andrea Baisero",
      "Christopher Amato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.11674"
  },
  {
    "id": "arXiv:2105.12398",
    "title": "Three-dimensional analytical discrete-ordinates method for the radiative  transport equation",
    "abstract": "Three-dimensional analytical discrete-ordinates method for the radiative  transport equation",
    "descriptor": "",
    "authors": [
      "Manabu Machida",
      "Kaustav Das"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.12398"
  },
  {
    "id": "arXiv:2105.13827",
    "title": "Extended Cyclic Codes Sandwiched Between Reed-Muller Codes",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Yan Xu",
      "Changjiang Ji",
      "Ran Tao",
      "Sihuang Hu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.13827"
  },
  {
    "id": "arXiv:2105.14491",
    "title": "How Attentive are Graph Attention Networks?",
    "abstract": "Comments: Published in ICLR 2022",
    "descriptor": "\nComments: Published in ICLR 2022\n",
    "authors": [
      "Shaked Brody",
      "Uri Alon",
      "Eran Yahav"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.14491"
  },
  {
    "id": "arXiv:2105.14673",
    "title": "A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression",
    "abstract": "Comments: 8 pages; published in Proc. 55th Asilomar Conf. Signals, Systems, and Computers, Pacific Grove, CA, Oct. 31-Nov. 3, 2021",
    "descriptor": "\nComments: 8 pages; published in Proc. 55th Asilomar Conf. Signals, Systems, and Computers, Pacific Grove, CA, Oct. 31-Nov. 3, 2021\n",
    "authors": [
      "Batoul Taki",
      "Mohsen Ghassemi",
      "Anand D. Sarwate",
      "Waheed U. Bajwa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.14673"
  },
  {
    "id": "arXiv:2105.15013",
    "title": "SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning",
    "abstract": "SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning",
    "descriptor": "",
    "authors": [
      "Jianhong Wang",
      "Yuan Zhang",
      "Yunjie Gu",
      "Tae-Kyun Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2105.15013"
  },
  {
    "id": "arXiv:2106.00188",
    "title": "PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D  World",
    "abstract": "Comments: ACL 2021 camera ready, project page at this https URL",
    "descriptor": "\nComments: ACL 2021 camera ready, project page at this https URL\n",
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Matthew Peters",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi",
      "Ali Farhadi",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.00188"
  },
  {
    "id": "arXiv:2106.00553",
    "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level  optimization and implicit models",
    "abstract": "Comments: Accepted as a spotlight to ICLR 2022",
    "descriptor": "\nComments: Accepted as a spotlight to ICLR 2022\n",
    "authors": [
      "Zaccharie Ramzi",
      "Florian Mannel",
      "Shaojie Bai",
      "Jean-Luc Starck",
      "Philippe Ciuciu",
      "Thomas Moreau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00553"
  },
  {
    "id": "arXiv:2106.01425",
    "title": "Gradient Assisted Learning",
    "abstract": "Gradient Assisted Learning",
    "descriptor": "",
    "authors": [
      "Enmao Diao",
      "Jie Ding",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01425"
  },
  {
    "id": "arXiv:2106.01432",
    "title": "SemiFL: Communication Efficient Semi-Supervised Federated Learning with  Unlabeled Clients",
    "abstract": "SemiFL: Communication Efficient Semi-Supervised Federated Learning with  Unlabeled Clients",
    "descriptor": "",
    "authors": [
      "Enmao Diao",
      "Jie Ding",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01432"
  },
  {
    "id": "arXiv:2106.01761",
    "title": "Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced  Convex-Concave Minimax Optimization",
    "abstract": "Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced  Convex-Concave Minimax Optimization",
    "descriptor": "",
    "authors": [
      "Luo Luo",
      "Guangzeng Xie",
      "Tong Zhang",
      "Zhihua Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01761"
  },
  {
    "id": "arXiv:2106.01981",
    "title": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse  Kinematics",
    "abstract": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse  Kinematics",
    "descriptor": "",
    "authors": [
      "Boris N. Oreshkin",
      "Florent Bocquelet",
      "F\u00e9lix G. Harvey",
      "Bay Raitt",
      "Dominic Laflamme"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01981"
  },
  {
    "id": "arXiv:2106.02104",
    "title": "Semi-Empirical Objective Functions for MCMC Proposal Optimization",
    "abstract": "Comments: 39 pages, 19 tables, 22 figures",
    "descriptor": "\nComments: 39 pages, 19 tables, 22 figures\n",
    "authors": [
      "Chris Cannella",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02104"
  },
  {
    "id": "arXiv:2106.02483",
    "title": "You can't always get what you want: towards user-controlled privacy on  Android",
    "abstract": "You can't always get what you want: towards user-controlled privacy on  Android",
    "descriptor": "",
    "authors": [
      "Davide Caputo",
      "Francesco Pagano",
      "Giovanni Bottino",
      "Luca Verderame",
      "Alessio Merlo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02483"
  },
  {
    "id": "arXiv:2106.02566",
    "title": "BR-NPA: A Non-Parametric High-Resolution Attention Model to improve the  Interpretability of Attention",
    "abstract": "BR-NPA: A Non-Parametric High-Resolution Attention Model to improve the  Interpretability of Attention",
    "descriptor": "",
    "authors": [
      "Tristan Gomez",
      "Suiyi Ling",
      "Thomas Fr\u00e9our",
      "Harold Mouch\u00e8re"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02566"
  },
  {
    "id": "arXiv:2106.03087",
    "title": "Neural Implicit 3D Shapes from Single Images with Spatial Patterns",
    "abstract": "Comments: 8 pages, 7Mb",
    "descriptor": "\nComments: 8 pages, 7Mb\n",
    "authors": [
      "Yixin Zhuang",
      "Yunzhe Liu",
      "Yujie Wang",
      "Baoquan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03087"
  },
  {
    "id": "arXiv:2106.03149",
    "title": "Large-scale Unsupervised Semantic Segmentation",
    "abstract": "Comments: benchmark: this https URL",
    "descriptor": "\nComments: benchmark: this https URL\n",
    "authors": [
      "Shanghua Gao",
      "Zhong-Yu Li",
      "Ming-Hsuan Yang",
      "Ming-Ming Cheng",
      "Junwei Han",
      "Philip Torr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03149"
  },
  {
    "id": "arXiv:2106.03207",
    "title": "Mitigating Covariate Shift in Imitation Learning via Offline Data  Without Great Coverage",
    "abstract": "Comments: 42 pages, 5 figures, 7 tables",
    "descriptor": "\nComments: 42 pages, 5 figures, 7 tables\n",
    "authors": [
      "Jonathan D. Chang",
      "Masatoshi Uehara",
      "Dhruv Sreenivas",
      "Rahul Kidambi",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03207"
  },
  {
    "id": "arXiv:2106.03223",
    "title": "Meta-learning with implicit gradients in a few-shot setting for medical  image segmentation",
    "abstract": "Meta-learning with implicit gradients in a few-shot setting for medical  image segmentation",
    "descriptor": "",
    "authors": [
      "Rabindra Khadga",
      "Debesh Jha",
      "Steven Hicks",
      "Vajira Thambawita",
      "Michael A. Riegler",
      "Sharib Ali",
      "P\u00e5l Halvorsen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03223"
  },
  {
    "id": "arXiv:2106.04114",
    "title": "Theoretically Motivated Data Augmentation and Regularization for  Portfolio Construction",
    "abstract": "Comments: Preprint. Title and contents updated",
    "descriptor": "\nComments: Preprint. Title and contents updated\n",
    "authors": [
      "Liu Ziyin",
      "Kentaro Minami",
      "Kentaro Imajo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "General Finance (q-fin.GN)",
      "Portfolio Management (q-fin.PM)"
    ],
    "url": "https://arxiv.org/abs/2106.04114"
  },
  {
    "id": "arXiv:2106.05042",
    "title": "Hermite Polynomial Features for Private Data Generation",
    "abstract": "Hermite Polynomial Features for Private Data Generation",
    "descriptor": "",
    "authors": [
      "Mijung Park",
      "Margarita Vinaroz",
      "Mohammad-Amin Charusaie",
      "Frederik Harder",
      "Kamil Adamczewski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05042"
  },
  {
    "id": "arXiv:2106.05194",
    "title": "DIGRAC: Digraph Clustering Based on Flow Imbalance",
    "abstract": "Comments: 36 pages (8 pages for main text)",
    "descriptor": "\nComments: 36 pages (8 pages for main text)\n",
    "authors": [
      "Yixuan He",
      "Gesine Reinert",
      "Mihai Cucuringu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05194"
  },
  {
    "id": "arXiv:2106.06338",
    "title": "Understanding approximate and unrolled dictionary learning for pattern  recovery",
    "abstract": "Understanding approximate and unrolled dictionary learning for pattern  recovery",
    "descriptor": "",
    "authors": [
      "Beno\u00eet Mal\u00e9zieux",
      "Thomas Moreau",
      "Matthieu Kowalski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06338"
  },
  {
    "id": "arXiv:2106.06804",
    "title": "Entropy-based Logic Explanations of Neural Networks",
    "abstract": "Entropy-based Logic Explanations of Neural Networks",
    "descriptor": "",
    "authors": [
      "Pietro Barbiero",
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Pietro Li\u00f3",
      "Marco Gori",
      "Stefano Melacci"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2106.06804"
  },
  {
    "id": "arXiv:2106.07830",
    "title": "On the Convergence and Calibration of Deep Learning with Differential  Privacy",
    "abstract": "On the Convergence and Calibration of Deep Learning with Differential  Privacy",
    "descriptor": "",
    "authors": [
      "Zhiqi Bu",
      "Hua Wang",
      "Qi Long",
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07830"
  },
  {
    "id": "arXiv:2106.08325",
    "title": "Fuel-Economical Distributed Model Predictive Control for Heavy-Duty  Truck Platoon",
    "abstract": "Comments: accepted for publication at the 24th IEEE Intelligent Transportation Systems Conference (ITSC 2021)",
    "descriptor": "\nComments: accepted for publication at the 24th IEEE Intelligent Transportation Systems Conference (ITSC 2021)\n",
    "authors": [
      "Mehmet Fatih Ozkan",
      "Yao Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.08325"
  },
  {
    "id": "arXiv:2106.08609",
    "title": "Reinforcement learning for pursuit and evasion of microswimmers at low  Reynolds number",
    "abstract": "Comments: 20 pages, 5 figures (Supplementary Material in ancillary directory)",
    "descriptor": "\nComments: 20 pages, 5 figures (Supplementary Material in ancillary directory)\n",
    "authors": [
      "Francesco Borra",
      "Luca Biferale",
      "Massimo Cencini",
      "Antonio Celani"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2106.08609"
  },
  {
    "id": "arXiv:2106.08641",
    "title": "Best of both worlds: local and global explanations with  human-understandable concepts",
    "abstract": "Best of both worlds: local and global explanations with  human-understandable concepts",
    "descriptor": "",
    "authors": [
      "Jessica Schrouff",
      "Sebastien Baur",
      "Shaobo Hou",
      "Diana Mincu",
      "Eric Loreaux",
      "Ralph Blanes",
      "James Wexler",
      "Alan Karthikesalingam",
      "Been Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.08641"
  },
  {
    "id": "arXiv:2106.09672",
    "title": "The 2021 Image Similarity Dataset and Challenge",
    "abstract": "The 2021 Image Similarity Dataset and Challenge",
    "descriptor": "",
    "authors": [
      "Matthijs Douze",
      "Giorgos Tolias",
      "Ed Pizzi",
      "Zo\u00eb Papakipos",
      "Lowik Chanussot",
      "Filip Radenovic",
      "Tomas Jenicek",
      "Maxim Maximov",
      "Laura Leal-Taix\u00e9",
      "Ismail Elezi",
      "Ond\u0159ej Chum",
      "Cristian Canton Ferrer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.09672"
  },
  {
    "id": "arXiv:2106.09762",
    "title": "Causal Bias Quantification for Continuous Treatments",
    "abstract": "Causal Bias Quantification for Continuous Treatments",
    "descriptor": "",
    "authors": [
      "Gianluca Detommaso",
      "Michael Br\u00fcckner",
      "Philip Schulz",
      "Victor Chernozhukov"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.09762"
  },
  {
    "id": "arXiv:2106.10605",
    "title": "Global and Local Contrastive Self-Supervised Learning for Semantic  Segmentation of HR Remote Sensing Images",
    "abstract": "Comments: 14 pages, 13 figures, 4 tables",
    "descriptor": "\nComments: 14 pages, 13 figures, 4 tables\n",
    "authors": [
      "Haifeng Li",
      "Yi Li",
      "Guo Zhang",
      "Ruoyun Liu",
      "Haozhe Huang",
      "Qing Zhu",
      "Chao Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.10605"
  },
  {
    "id": "arXiv:2106.11214",
    "title": "Private and Secure Distributed Matrix Multiplication Schemes for  Replicated or MDS-Coded Servers",
    "abstract": "Comments: Accepted for publication in the IEEE Transactions on Information Forensics and Security",
    "descriptor": "\nComments: Accepted for publication in the IEEE Transactions on Information Forensics and Security\n",
    "authors": [
      "Jie Li",
      "Camilla Hollanti"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.11214"
  },
  {
    "id": "arXiv:2106.11814",
    "title": "Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A  Repeated Game Perspective",
    "abstract": "Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A  Repeated Game Perspective",
    "descriptor": "",
    "authors": [
      "Ning Zhang",
      "Qian Ma",
      "Xu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.11814"
  },
  {
    "id": "arXiv:2106.12433",
    "title": "On Symmetric Positive Definite Preconditioners for Multiple Saddle-Point  Systems",
    "abstract": "Comments: 16 pages",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "John W. Pearson",
      "Andreas Potschka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.12433"
  },
  {
    "id": "arXiv:2106.12529",
    "title": "Who Leads and Who Follows in Strategic Classification?",
    "abstract": "Who Leads and Who Follows in Strategic Classification?",
    "descriptor": "",
    "authors": [
      "Tijana Zrnic",
      "Eric Mazumdar",
      "S. Shankar Sastry",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.12529"
  },
  {
    "id": "arXiv:2106.12575",
    "title": "Weisfeiler and Lehman Go Cellular: CW Networks",
    "abstract": "Comments: NeurIPS 2021. Contains 28 pages, 9 figures",
    "descriptor": "\nComments: NeurIPS 2021. Contains 28 pages, 9 figures\n",
    "authors": [
      "Cristian Bodnar",
      "Fabrizio Frasca",
      "Nina Otter",
      "Yu Guang Wang",
      "Pietro Li\u00f2",
      "Guido Mont\u00fafar",
      "Michael Bronstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.12575"
  },
  {
    "id": "arXiv:2106.13863",
    "title": "Steerable 3D Spherical Neurons",
    "abstract": "Steerable 3D Spherical Neurons",
    "descriptor": "",
    "authors": [
      "Pavlo Melnyk",
      "Michael Felsberg",
      "M\u00e5rten Wadenb\u00e4ck"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.13863"
  },
  {
    "id": "arXiv:2106.15933",
    "title": "Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization  Training, Symmetry, and Sparsity",
    "abstract": "Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization  Training, Symmetry, and Sparsity",
    "descriptor": "",
    "authors": [
      "Arthur Jacot",
      "Fran\u00e7ois Ged",
      "Berfin \u015eim\u015fek",
      "Cl\u00e9ment Hongler",
      "Franck Gabriel"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.15933"
  },
  {
    "id": "arXiv:2107.00243",
    "title": "Preconditioning for Scalable Gaussian Process Hyperparameter  Optimization",
    "abstract": "Preconditioning for Scalable Gaussian Process Hyperparameter  Optimization",
    "descriptor": "",
    "authors": [
      "Jonathan Wenger",
      "Geoff Pleiss",
      "Philipp Hennig",
      "John P. Cunningham",
      "Jacob R. Gardner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.00243"
  },
  {
    "id": "arXiv:2107.00853",
    "title": "Adaptive Regularized Zero-Forcing Precoding for Massive MIMO Systems  with Multi-Antenna Users",
    "abstract": "Comments: 25 pages, 6 figures, 4 tables, comments are welcome",
    "descriptor": "\nComments: 25 pages, 6 figures, 4 tables, comments are welcome\n",
    "authors": [
      "Evgeny Bobrov",
      "Boris Chinyaev",
      "Viktor Kuznetsov",
      "Hao Lu",
      "Dmitrii Minenkov",
      "Sergey Troshin",
      "Daniil Yudakov",
      "Danila Zaev"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2107.00853"
  },
  {
    "id": "arXiv:2107.03313",
    "title": "A Leap among Quantum Computing and Quantum Neural Networks: A Survey",
    "abstract": "A Leap among Quantum Computing and Quantum Neural Networks: A Survey",
    "descriptor": "",
    "authors": [
      "Fabio Valerio Massoli",
      "Lucia Vadicamo",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.03313"
  },
  {
    "id": "arXiv:2107.03974",
    "title": "Offline Meta-Reinforcement Learning with Online Self-Supervision",
    "abstract": "Comments: 8.5 pages, 6 figures",
    "descriptor": "\nComments: 8.5 pages, 6 figures\n",
    "authors": [
      "Vitchyr H. Pong",
      "Ashvin Nair",
      "Laura Smith",
      "Catherine Huang",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2107.03974"
  },
  {
    "id": "arXiv:2107.04276",
    "title": "Secure Consensus via Objective Coding: Robustness Analysis to Channel  Tampering",
    "abstract": "Comments: 12 pages, 5 figures, submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems",
    "descriptor": "\nComments: 12 pages, 5 figures, submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems\n",
    "authors": [
      "Marco Fabris",
      "Daniel Zelazo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2107.04276"
  },
  {
    "id": "arXiv:2107.05214",
    "title": "Split, embed and merge: An accurate table structure recognizer",
    "abstract": "Split, embed and merge: An accurate table structure recognizer",
    "descriptor": "",
    "authors": [
      "Zhenrong Zhang",
      "Jianshu Zhang",
      "Jun Du"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.05214"
  },
  {
    "id": "arXiv:2107.07511",
    "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free  Uncertainty Quantification",
    "abstract": "Comments: Blog and tutorial video this http URL",
    "descriptor": "\nComments: Blog and tutorial video this http URL\n",
    "authors": [
      "Anastasios N. Angelopoulos",
      "Stephen Bates"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.07511"
  },
  {
    "id": "arXiv:2107.07999",
    "title": "From block-Toeplitz matrices to differential equations on graphs:  towards a general theory for scalable masked Transformers",
    "abstract": "Comments: 20 pages, 11 figures",
    "descriptor": "\nComments: 20 pages, 11 figures\n",
    "authors": [
      "Krzysztof Choromanski",
      "Han Lin",
      "Haoxian Chen",
      "Tianyi Zhang",
      "Arijit Sehanobish",
      "Valerii Likhosherstov",
      "Jack Parker-Holder",
      "Tamas Sarlos",
      "Adrian Weller",
      "Thomas Weingarten"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.07999"
  },
  {
    "id": "arXiv:2107.09028",
    "title": "Structured Stochastic Gradient MCMC",
    "abstract": "Structured Stochastic Gradient MCMC",
    "descriptor": "",
    "authors": [
      "Antonios Alexos",
      "Alex Boyd",
      "Stephan Mandt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.09028"
  },
  {
    "id": "arXiv:2107.09370",
    "title": "An Embedding of ReLU Networks and an Analysis of their Identifiability",
    "abstract": "Comments: Constructive Approximation camera-ready",
    "descriptor": "\nComments: Constructive Approximation camera-ready\n",
    "authors": [
      "Pierre Stock",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.09370"
  },
  {
    "id": "arXiv:2107.10446",
    "title": "Online Service Caching and Routing at the Edge with Unknown Arrivals",
    "abstract": "Comments: This paper is accepted for publication in IEEE ICC 2022",
    "descriptor": "\nComments: This paper is accepted for publication in IEEE ICC 2022\n",
    "authors": [
      "Siqi Fan",
      "I-Hong Hou",
      "Van Sy Mai",
      "Lotfi Benmohamed"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2107.10446"
  },
  {
    "id": "arXiv:2107.11102",
    "title": "Automatically generating models of IT systems",
    "abstract": "Comments: 20 pages, 16 figures",
    "descriptor": "\nComments: 20 pages, 16 figures\n",
    "authors": [
      "Ivan Kova\u010devi\u0107",
      "Stjepan Gro\u0161",
      "Ante \u0110erek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2107.11102"
  },
  {
    "id": "arXiv:2107.11852",
    "title": "Reconfigurable Intelligent Surface Phase Hopping for Ultra-Reliable  Communications",
    "abstract": "Comments: 32 pages, 12 figures",
    "descriptor": "\nComments: 32 pages, 12 figures\n",
    "authors": [
      "Karl-Ludwig Besser",
      "Eduard A. Jorswieck"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2107.11852"
  },
  {
    "id": "arXiv:2107.12711",
    "title": "Reason Against the Machine: Future Directions for Mass Online  Deliberation",
    "abstract": "Comments: Adjusting title and abstract to arxiv metadata",
    "descriptor": "\nComments: Adjusting title and abstract to arxiv metadata\n",
    "authors": [
      "Ruth Shortall",
      "Anatol Itten",
      "Michiel van der Meer",
      "Pradeep K. Murukannaiah",
      "Catholijn M. Jonker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2107.12711"
  },
  {
    "id": "arXiv:2107.12780",
    "title": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling",
    "abstract": "Physics-constrained Deep Learning for Robust Inverse ECG Modeling",
    "descriptor": "",
    "authors": [
      "Jianxin Xie",
      "Bing Yao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.12780"
  },
  {
    "id": "arXiv:2107.13404",
    "title": "XFL: eXtreme Function Labeling",
    "abstract": "XFL: eXtreme Function Labeling",
    "descriptor": "",
    "authors": [
      "James Patrick-Evans",
      "Moritz Dannehl",
      "Johannes Kinder"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2107.13404"
  },
  {
    "id": "arXiv:2107.13668",
    "title": "Discovering User-Interpretable Capabilities of Black-Box Planning Agents",
    "abstract": "Comments: AAAI 2022 Workshop on Explainable Agency in Artificial Intelligence",
    "descriptor": "\nComments: AAAI 2022 Workshop on Explainable Agency in Artificial Intelligence\n",
    "authors": [
      "Pulkit Verma",
      "Shashank Rao Marpally",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.13668"
  },
  {
    "id": "arXiv:2107.13945",
    "title": "Effects of homophily and heterophily on preferred-degree networks:  mean-field analysis and overwhelming transition",
    "abstract": "Comments: 24 pages, 10 figures",
    "descriptor": "\nComments: 24 pages, 10 figures\n",
    "authors": [
      "Xiang Li",
      "Mauro Mobilia",
      "Alastair M. Rucklidge",
      "R.K.P. Zia"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2107.13945"
  },
  {
    "id": "arXiv:2107.14226",
    "title": "Learning more skills through optimistic exploration",
    "abstract": "Comments: Accepted at ICLR 2022 (spotlight)",
    "descriptor": "\nComments: Accepted at ICLR 2022 (spotlight)\n",
    "authors": [
      "DJ Strouse",
      "Kate Baumli",
      "David Warde-Farley",
      "Vlad Mnih",
      "Steven Hansen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.14226"
  },
  {
    "id": "arXiv:2108.00138",
    "title": "Learning to Control Direct Current Motor for Steering in Real Time via  Reinforcement Learning",
    "abstract": "Comments: 13 pages, 4 figures",
    "descriptor": "\nComments: 13 pages, 4 figures\n",
    "authors": [
      "Bibek Poudel",
      "Thomas Watson",
      "Weizi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2108.00138"
  },
  {
    "id": "arXiv:2108.00351",
    "title": "LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic  Occlusion-Aware Data and Neural Mesh Rendering",
    "abstract": "LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic  Occlusion-Aware Data and Neural Mesh Rendering",
    "descriptor": "",
    "authors": [
      "Kaibing Yang",
      "Renshu Gu",
      "Maoyu Wang",
      "Masahiro Toyoura",
      "Gang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.00351"
  },
  {
    "id": "arXiv:2108.00602",
    "title": "Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails",
    "abstract": "Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails",
    "descriptor": "",
    "authors": [
      "Yang Zhang",
      "Xin Yu",
      "Xiaobo Lu",
      "Ping Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.00602"
  },
  {
    "id": "arXiv:2108.02642",
    "title": "Robust interior penalty discontinuous Galerkin methods",
    "abstract": "Robust interior penalty discontinuous Galerkin methods",
    "descriptor": "",
    "authors": [
      "Zhaonan Dong",
      "Emmanuil H. Georgoulis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.02642"
  },
  {
    "id": "arXiv:2108.02770",
    "title": "Scheduling with Communication Delay in Near-Linear Time",
    "abstract": "Comments: To appear in STACS 2022",
    "descriptor": "\nComments: To appear in STACS 2022\n",
    "authors": [
      "Quanquan C. Liu",
      "Manish Purohit",
      "Zoya Svitkina",
      "Erik Vee",
      "Joshua R. Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.02770"
  },
  {
    "id": "arXiv:2108.03039",
    "title": "Identifiable Energy-based Representations: An Application to Estimating  Heterogeneous Causal Effects",
    "abstract": "Comments: 20 pages, 2 figures, 9 tables",
    "descriptor": "\nComments: 20 pages, 2 figures, 9 tables\n",
    "authors": [
      "Yao Zhang",
      "Jeroen Berrevoets",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.03039"
  },
  {
    "id": "arXiv:2108.03702",
    "title": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "abstract": "BIGRoC: Boosting Image Generation via a Robust Classifier",
    "descriptor": "",
    "authors": [
      "Roy Ganz",
      "Michael Elad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.03702"
  },
  {
    "id": "arXiv:2108.04424",
    "title": "FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for  Blind Face Inpainting",
    "abstract": "FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for  Blind Face Inpainting",
    "descriptor": "",
    "authors": [
      "Junke Wang",
      "Shaoxiang Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.04424"
  },
  {
    "id": "arXiv:2108.05669",
    "title": "Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author  Discovery",
    "abstract": "Comments: CHI 2022",
    "descriptor": "\nComments: CHI 2022\n",
    "authors": [
      "Jason Portenoy",
      "Marissa Radensky",
      "Jevin West",
      "Eric Horvitz",
      "Daniel Weld",
      "Tom Hope"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2108.05669"
  },
  {
    "id": "arXiv:2108.06547",
    "title": "Spectral Detection of Simplicial Communities via Hodge Laplacians",
    "abstract": "Comments: 18 pages, 8 figures",
    "descriptor": "\nComments: 18 pages, 8 figures\n",
    "authors": [
      "Sanjukta Krishnagopal",
      "Ginestra Bianconi"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2108.06547"
  },
  {
    "id": "arXiv:2108.07045",
    "title": "A scaleable projection-based branch-and-cut algorithm for the $p$-center  problem",
    "abstract": "A scaleable projection-based branch-and-cut algorithm for the $p$-center  problem",
    "descriptor": "",
    "authors": [
      "Elisabeth Gaar",
      "Markus Sinnl"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2108.07045"
  },
  {
    "id": "arXiv:2108.07472",
    "title": "Towards the PAC Learnability of Nash Equilibrium",
    "abstract": "Towards the PAC Learnability of Nash Equilibrium",
    "descriptor": "",
    "authors": [
      "Zhijian Duan",
      "Dinghuai Zhang",
      "Wenhan Huang",
      "Yali Du",
      "Jun Wang",
      "Yaodong Yang",
      "Xiaotie Deng"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2108.07472"
  },
  {
    "id": "arXiv:2108.09490",
    "title": "Incrementally Stochastic and Accelerated Gradient Information mixed  Optimization for Manipulator Motion Planning",
    "abstract": "Incrementally Stochastic and Accelerated Gradient Information mixed  Optimization for Manipulator Motion Planning",
    "descriptor": "",
    "authors": [
      "Yichang Feng",
      "Jin Wang",
      "Haiyun Zhang",
      "Guodong Lu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2108.09490"
  },
  {
    "id": "arXiv:2108.11481",
    "title": "Learning to discover: expressive Gaussian mixture models for  multi-dimensional simulation and parameter inference in the physical sciences",
    "abstract": "Comments: 42 pages, 20 figures, 6 tables. Simulated data, model files and code available at: this https URL",
    "descriptor": "\nComments: 42 pages, 20 figures, 6 tables. Simulated data, model files and code available at: this https URL\n",
    "authors": [
      "Stephen B. Menary",
      "Darren D. Price"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2108.11481"
  },
  {
    "id": "arXiv:2108.13376",
    "title": "City-Scale Holographic Traffic Flow Data based on Vehicular Trajectory  Resampling",
    "abstract": "City-Scale Holographic Traffic Flow Data based on Vehicular Trajectory  Resampling",
    "descriptor": "",
    "authors": [
      "Yimin Wang",
      "Yixian Chen",
      "Guilong Li",
      "Yuhuan Lu",
      "Zhi Yu",
      "Zhaocheng He"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2108.13376"
  },
  {
    "id": "arXiv:2109.02122",
    "title": "Decoding Reed-Muller Codes with Successive Codeword Permutations",
    "abstract": "Comments: Submitted to an IEEE journal for possible publication",
    "descriptor": "\nComments: Submitted to an IEEE journal for possible publication\n",
    "authors": [
      "Nghia Doan",
      "Seyyed Ali Hashemi",
      "Marco Mondelli",
      "Warren J. Gross"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2109.02122"
  },
  {
    "id": "arXiv:2109.03867",
    "title": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "abstract": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "descriptor": "",
    "authors": [
      "Emanuele Sansone"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.03867"
  },
  {
    "id": "arXiv:2109.04150",
    "title": "Self-supervised Reinforcement Learning with Independently Controllable  Subgoals",
    "abstract": "Self-supervised Reinforcement Learning with Independently Controllable  Subgoals",
    "descriptor": "",
    "authors": [
      "Andrii Zadaianchuk",
      "Georg Martius",
      "Fanny Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.04150"
  },
  {
    "id": "arXiv:2109.05402",
    "title": "Differentially Private Variable Selection via the Knockoff Filter",
    "abstract": "Comments: Accepted to the 2021 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)",
    "descriptor": "\nComments: Accepted to the 2021 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)\n",
    "authors": [
      "Mehrdad Pournaderi",
      "Yu Xiang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Databases (cs.DB)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.05402"
  },
  {
    "id": "arXiv:2109.05424",
    "title": "Pairwise Supervised Contrastive Learning of Sentence Representations",
    "abstract": "Comments: 9 pages, EMNLP 2021",
    "descriptor": "\nComments: 9 pages, EMNLP 2021\n",
    "authors": [
      "Dejiao Zhang",
      "Shang-Wen Li",
      "Wei Xiao",
      "Henghui Zhu",
      "Ramesh Nallapati",
      "Andrew O. Arnold",
      "Bing Xiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.05424"
  },
  {
    "id": "arXiv:2109.06404",
    "title": "Detecting Multi-Sensor Fusion Errors in Advanced Driver-Assistance  Systems",
    "abstract": "Detecting Multi-Sensor Fusion Errors in Advanced Driver-Assistance  Systems",
    "descriptor": "",
    "authors": [
      "Ziyuan Zhong",
      "Zhisheng Hu",
      "Shengjian Guo",
      "Xinyang Zhang",
      "Zhenyu Zhong",
      "Baishakhi Ray"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2109.06404"
  },
  {
    "id": "arXiv:2109.07117",
    "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for  Streaming Data",
    "abstract": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for  Streaming Data",
    "descriptor": "",
    "authors": [
      "Antoine Godichon-Baggioni",
      "Nicklas Werge",
      "Olivier Wintenberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.07117"
  },
  {
    "id": "arXiv:2109.07557",
    "title": "CounterNet: End-to-End Training of Counterfactual Aware Predictions",
    "abstract": "CounterNet: End-to-End Training of Counterfactual Aware Predictions",
    "descriptor": "",
    "authors": [
      "Hangzhi Guo",
      "Thanh Hong Nguyen",
      "Amulya Yadav"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.07557"
  },
  {
    "id": "arXiv:2109.09001",
    "title": "Machine Learning-Based COVID-19 Patients Triage Algorithm using  Patient-Generated Health Data from Nationwide Multicenter Database",
    "abstract": "Comments: To appear in Infectious Diseases and Therapy",
    "descriptor": "\nComments: To appear in Infectious Diseases and Therapy\n",
    "authors": [
      "Min Sue Park",
      "Hyeontae Jo",
      "Haeun Lee",
      "Se Young Jung",
      "Hyung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.09001"
  },
  {
    "id": "arXiv:2109.10686",
    "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning  Transformers",
    "abstract": "Comments: ICLR 2022 + Updated Checkpoint Release",
    "descriptor": "\nComments: ICLR 2022 + Updated Checkpoint Release\n",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Jinfeng Rao",
      "William Fedus",
      "Samira Abnar",
      "Hyung Won Chung",
      "Sharan Narang",
      "Dani Yogatama",
      "Ashish Vaswani",
      "Donald Metzler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.10686"
  },
  {
    "id": "arXiv:2109.10864",
    "title": "Reliable Linearized Phase Retrieval for Near-Field Antenna Measurements  with Truncated Measurement Surfaces",
    "abstract": "Comments: 6 pages, 5 figures, accepted by IEEE Transaction and Antennas and Propagation",
    "descriptor": "\nComments: 6 pages, 5 figures, accepted by IEEE Transaction and Antennas and Propagation\n",
    "authors": [
      "Alexander Paulus",
      "Josef Knapp",
      "Jonas Kornprobst",
      "Thomas F. Eibert"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.10864"
  },
  {
    "id": "arXiv:2109.11308",
    "title": "Breaking BERT: Understanding its Vulnerabilities for Named Entity  Recognition through Adversarial Attack",
    "abstract": "Breaking BERT: Understanding its Vulnerabilities for Named Entity  Recognition through Adversarial Attack",
    "descriptor": "",
    "authors": [
      "Anne Dirkson",
      "Suzan Verberne",
      "Wessel Kraaij"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2109.11308"
  },
  {
    "id": "arXiv:2109.11375",
    "title": "Stochastic Normalizing Flows for Inverse Problems: a Markov Chains  Viewpoint",
    "abstract": "Stochastic Normalizing Flows for Inverse Problems: a Markov Chains  Viewpoint",
    "descriptor": "",
    "authors": [
      "Paul Hagemann",
      "Johannes Hertrich",
      "Gabriele Steidl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2109.11375"
  },
  {
    "id": "arXiv:2109.12290",
    "title": "Distributed Computation of Stochastic GNE with Partial Information: An  Augmented Best-Response Approach",
    "abstract": "Distributed Computation of Stochastic GNE with Partial Information: An  Augmented Best-Response Approach",
    "descriptor": "",
    "authors": [
      "Yuanhanqing Huang",
      "Jianghai Hu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.12290"
  },
  {
    "id": "arXiv:2109.12555",
    "title": "Self-loop Compensation in Signed Networks",
    "abstract": "Self-loop Compensation in Signed Networks",
    "descriptor": "",
    "authors": [
      "Haibin Shao",
      "Lulu Pan",
      "Dewei Li",
      "Yugeng Xi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2109.12555"
  },
  {
    "id": "arXiv:2109.12556",
    "title": "Frequency Disentangled Residual Network",
    "abstract": "Frequency Disentangled Residual Network",
    "descriptor": "",
    "authors": [
      "Satya Rajendra Singh",
      "Roshan Reddy Yedla",
      "Shiv Ram Dubey",
      "Rakesh Sanodiya",
      "Wei-Ta Chu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.12556"
  },
  {
    "id": "arXiv:2109.12888",
    "title": "Mixed Integer Neural Inverse Design",
    "abstract": "Mixed Integer Neural Inverse Design",
    "descriptor": "",
    "authors": [
      "Navid Ansari",
      "Hans-Peter Seidel",
      "Vahid Babaei"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.12888"
  },
  {
    "id": "arXiv:2109.15166",
    "title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech",
    "abstract": "Comments: Accepted by NeurIPS 2021",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Yi Ren",
      "Jinglin Liu",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2109.15166"
  },
  {
    "id": "arXiv:2110.00035",
    "title": "Energy-Efficient and Delay-Guaranteed Joint Resource Allocation and DU  Selection in O-RAN",
    "abstract": "Comments: Accepted by IEEE. Footnote in Page 1 provides the copyright information",
    "descriptor": "\nComments: Accepted by IEEE. Footnote in Page 1 provides the copyright information\n",
    "authors": [
      "Turgay Pamuklu",
      "Shahram Mollahasani",
      "Melike Erol-Kantarci"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.00035"
  },
  {
    "id": "arXiv:2110.02865",
    "title": "Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural  Networks",
    "abstract": "Comments: Spotlight paper at ICLR 2022",
    "descriptor": "\nComments: Spotlight paper at ICLR 2022\n",
    "authors": [
      "Alan Jeffares",
      "Qinghai Guo",
      "Pontus Stenetorp",
      "Timoleon Moraitis"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.02865"
  },
  {
    "id": "arXiv:2110.03183",
    "title": "Attention is All You Need? Good Embeddings with Statistics are  enough:Large Scale Audio Understanding without Transformers/ Convolutions/  BERTs/ Mixers/ Attention/ RNNs or ....",
    "abstract": "Comments: IEEE Copyright: written as told",
    "descriptor": "\nComments: IEEE Copyright: written as told\n",
    "authors": [
      "Prateek Verma"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03183"
  },
  {
    "id": "arXiv:2110.03210",
    "title": "Universality of Winning Tickets: A Renormalization Group Perspective",
    "abstract": "Comments: 18 pages, 3 figures, 7 tables",
    "descriptor": "\nComments: 18 pages, 3 figures, 7 tables\n",
    "authors": [
      "William T. Redman",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Akshunna S. Dogra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ],
    "url": "https://arxiv.org/abs/2110.03210"
  },
  {
    "id": "arXiv:2110.03484",
    "title": "Creating Training Sets via Weak Indirect Supervision",
    "abstract": "Comments: ICLR 2022",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Jieyu Zhang",
      "Bohan Wang",
      "Xiangchen Song",
      "Yujing Wang",
      "Yaming Yang",
      "Jing Bai",
      "Alexander Ratner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03484"
  },
  {
    "id": "arXiv:2110.03588",
    "title": "A transformer-based deep learning approach for classifying brain  metastases into primary organ sites using clinical whole brain MRI",
    "abstract": "A transformer-based deep learning approach for classifying brain  metastases into primary organ sites using clinical whole brain MRI",
    "descriptor": "",
    "authors": [
      "Qing Lyu",
      "Sanjeev V. Namjoshi",
      "Emory McTyre",
      "Umit Topaloglu",
      "Richard Barcus",
      "Michael D. Chan",
      "Christina K. Cramer",
      "Waldemar Debinski",
      "Metin N. Gurcan",
      "Glenn J. Lesser",
      "Hui-Kuan Lin",
      "Reginald F. Munden",
      "Boris C. Pasche",
      "Kiran Kumar Solingapuram Sai",
      "Roy E. Strowd",
      "Stephen B. Tatter",
      "Kounosuke Watabe",
      "Wei Zhang",
      "Ge Wang",
      "Christopher T. Whitlow"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03588"
  },
  {
    "id": "arXiv:2110.03608",
    "title": "How to Sense the World: Leveraging Hierarchy in Multimodal Perception  for Robust Reinforcement Learning Agents",
    "abstract": "Comments: Accepted at the International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) 2022",
    "descriptor": "\nComments: Accepted at the International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) 2022\n",
    "authors": [
      "Miguel Vasco",
      "Hang Yin",
      "Francisco S. Melo",
      "Ana Paiva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03608"
  },
  {
    "id": "arXiv:2110.03743",
    "title": "Reinforcement Learning in Reward-Mixing MDPs",
    "abstract": "Comments: NeurIPS 2021; fixed typo",
    "descriptor": "\nComments: NeurIPS 2021; fixed typo\n",
    "authors": [
      "Jeongyeol Kwon",
      "Yonathan Efroni",
      "Constantine Caramanis",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03743"
  },
  {
    "id": "arXiv:2110.04175",
    "title": "RelaySum for Decentralized Deep Learning on Heterogeneous Data",
    "abstract": "Comments: Presented at NeurIPS 2021",
    "descriptor": "\nComments: Presented at NeurIPS 2021\n",
    "authors": [
      "Thijs Vogels",
      "Lie He",
      "Anastasia Koloskova",
      "Tao Lin",
      "Sai Praneeth Karimireddy",
      "Sebastian U. Stich",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04175"
  },
  {
    "id": "arXiv:2110.04367",
    "title": "Hybrid Random Features",
    "abstract": "Comments: Published as a conference paper at ICLR 2022",
    "descriptor": "\nComments: Published as a conference paper at ICLR 2022\n",
    "authors": [
      "Krzysztof Choromanski",
      "Haoxian Chen",
      "Han Lin",
      "Yuanzhe Ma",
      "Arijit Sehanobish",
      "Deepali Jain",
      "Michael S Ryoo",
      "Jake Varley",
      "Andy Zeng",
      "Valerii Likhosherstov",
      "Dmitry Kalashnikov",
      "Vikas Sindhwani",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04367"
  },
  {
    "id": "arXiv:2110.04743",
    "title": "ZARTS: On Zero-order Optimization for Neural Architecture Search",
    "abstract": "ZARTS: On Zero-order Optimization for Neural Architecture Search",
    "descriptor": "",
    "authors": [
      "Xiaoxing Wang",
      "Wenxuan Guo",
      "Junchi Yan",
      "Jianlin Su",
      "Xiaokang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.04743"
  },
  {
    "id": "arXiv:2110.04791",
    "title": "Stepwise-Refining Speech Separation Network via Fine-Grained Encoding in  High-order Latent Domain",
    "abstract": "Comments: Accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)",
    "descriptor": "\nComments: Accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)\n",
    "authors": [
      "Zengwei Yao",
      "Wenjie Pei",
      "Fanglin Chen",
      "Guangming Lu",
      "David Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.04791"
  },
  {
    "id": "arXiv:2110.04903",
    "title": "NormVAE: Normative Modeling on Neuroimaging Data using Variational  Autoencoders",
    "abstract": "Comments: IEEE International Conference on Healthcare Informatics",
    "descriptor": "\nComments: IEEE International Conference on Healthcare Informatics\n",
    "authors": [
      "Sayantan Kumar"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04903"
  },
  {
    "id": "arXiv:2110.05682",
    "title": "Provably Efficient Reinforcement Learning in Decentralized General-Sum  Markov Games",
    "abstract": "Provably Efficient Reinforcement Learning in Decentralized General-Sum  Markov Games",
    "descriptor": "",
    "authors": [
      "Weichao Mao",
      "Tamer Ba\u015far"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.05682"
  },
  {
    "id": "arXiv:2110.05707",
    "title": "On Improving Model-Free Algorithms for Decentralized Multi-Agent  Reinforcement Learning",
    "abstract": "On Improving Model-Free Algorithms for Decentralized Multi-Agent  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Weichao Mao",
      "Lin F. Yang",
      "Kaiqing Zhang",
      "Tamer Ba\u015far"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.05707"
  },
  {
    "id": "arXiv:2110.05904",
    "title": "Video Is Graph: Structured Graph Module for Video Action Recognition",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Rongchang Li",
      "Xiao-Jun Wu",
      "Tianyang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.05904"
  },
  {
    "id": "arXiv:2110.06283",
    "title": "Detecting Corrupted Labels Without Training a Model to Predict",
    "abstract": "Detecting Corrupted Labels Without Training a Model to Predict",
    "descriptor": "",
    "authors": [
      "Zhaowei Zhu",
      "Zihao Dong",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06283"
  },
  {
    "id": "arXiv:2110.08105",
    "title": "Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps  and Relevance Orderings",
    "abstract": "Comments: 18 pages, 23 figures, 1 table",
    "descriptor": "\nComments: 18 pages, 23 figures, 1 table\n",
    "authors": [
      "Jan Macdonald",
      "Mathieu Besan\u00e7on",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.08105"
  },
  {
    "id": "arXiv:2110.08258",
    "title": "A Framework for Learning to Request Rich and Contextually Useful  Information from Humans",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Khanh Nguyen",
      "Yonatan Bisk",
      "Hal Daum\u00e9 III"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.08258"
  },
  {
    "id": "arXiv:2110.08263",
    "title": "FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo  Labeling",
    "abstract": "Comments: NeurIPS 2021; camera-ready version; 16 pages with appendix; code: this https URL",
    "descriptor": "\nComments: NeurIPS 2021; camera-ready version; 16 pages with appendix; code: this https URL\n",
    "authors": [
      "Bowen Zhang",
      "Yidong Wang",
      "Wenxin Hou",
      "Hao Wu",
      "Jindong Wang",
      "Manabu Okumura",
      "Takahiro Shinozaki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08263"
  },
  {
    "id": "arXiv:2110.08577",
    "title": "Nys-Newton: Nystr\u00f6m-Approximated Curvature for Stochastic Optimization",
    "abstract": "Nys-Newton: Nystr\u00f6m-Approximated Curvature for Stochastic Optimization",
    "descriptor": "",
    "authors": [
      "Dinesh Singh",
      "Hardik Tankaria",
      "Makoto Yamada"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.08577"
  },
  {
    "id": "arXiv:2110.09154",
    "title": "Measuring the influence of beliefs in belief networks",
    "abstract": "Comments: 14 pages, 4 figures. Earlier version of this work was presented at Networks 2021 conference. Second, shorter version of the paper",
    "descriptor": "\nComments: 14 pages, 4 figures. Earlier version of this work was presented at Networks 2021 conference. Second, shorter version of the paper\n",
    "authors": [
      "Aleksandar Toma\u0161evi\u0107"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.09154"
  },
  {
    "id": "arXiv:2110.09284",
    "title": "Towards a Systematic Survey for Carbon Neutral Data Centers",
    "abstract": "Towards a Systematic Survey for Carbon Neutral Data Centers",
    "descriptor": "",
    "authors": [
      "Zhiwei Cao",
      "Xin Zhou",
      "Han Hu",
      "Zhi Wang",
      "Yonggang Wen"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.09284"
  },
  {
    "id": "arXiv:2110.10103",
    "title": "Continual self-training with bootstrapped remixing for speech  enhancement",
    "abstract": "Comments: To appear in Proc. ICASSP 2022, May 22-27, 2022, Singapore",
    "descriptor": "\nComments: To appear in Proc. ICASSP 2022, May 22-27, 2022, Singapore\n",
    "authors": [
      "Efthymios Tzinis",
      "Yossi Adi",
      "Vamsi K. Ithapu",
      "Buye Xu",
      "Anurag Kumar"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.10103"
  },
  {
    "id": "arXiv:2110.10863",
    "title": "Deep Generative Models in Engineering Design: A Review",
    "abstract": "Deep Generative Models in Engineering Design: A Review",
    "descriptor": "",
    "authors": [
      "Lyle Regenwetter",
      "Amin Heyrani Nobari",
      "Faez Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.10863"
  },
  {
    "id": "arXiv:2110.11219",
    "title": "PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for  Piece-Wise Plane Detection and Reconstruction from a Single RGB Image",
    "abstract": "Comments: accepted to BMVC 2021, code opensource: this https URL",
    "descriptor": "\nComments: accepted to BMVC 2021, code opensource: this https URL\n",
    "authors": [
      "Yaxu Xie",
      "Fangwen Shu",
      "Jason Rambach",
      "Alain Pagani",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.11219"
  },
  {
    "id": "arXiv:2110.11442",
    "title": "Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic  Gradient Descent",
    "abstract": "Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic  Gradient Descent",
    "descriptor": "",
    "authors": [
      "Sharan Vaswani",
      "Benjamin Dubois-Taine",
      "Reza Babanezhad"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.11442"
  },
  {
    "id": "arXiv:2110.11794",
    "title": "Federated Unlearning via Class-Discriminative Pruning",
    "abstract": "Comments: WWW2022",
    "descriptor": "\nComments: WWW2022\n",
    "authors": [
      "Junxiao Wang",
      "Song Guo",
      "Xin Xie",
      "Heng Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.11794"
  },
  {
    "id": "arXiv:2110.12065",
    "title": "Multiplication-Avoiding Variant of Power Iteration with Applications",
    "abstract": "Comments: This is the technique report for the paper \"MULTIPLICATION-AVOIDING VARIANT OF POWER ITERATION WITH APPLICATIONS\", which has been accepted by ICASSP 2022",
    "descriptor": "\nComments: This is the technique report for the paper \"MULTIPLICATION-AVOIDING VARIANT OF POWER ITERATION WITH APPLICATIONS\", which has been accepted by ICASSP 2022\n",
    "authors": [
      "Hongyi Pan",
      "Diaa Badawi",
      "Runxuan Miao",
      "Erdem Koyuncu",
      "Ahmet Enis Cetin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.12065"
  },
  {
    "id": "arXiv:2110.12122",
    "title": "Quantifying Epistemic Uncertainty in Deep Learning",
    "abstract": "Quantifying Epistemic Uncertainty in Deep Learning",
    "descriptor": "",
    "authors": [
      "Ziyi Huang",
      "Henry Lam",
      "Haofeng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.12122"
  },
  {
    "id": "arXiv:2110.12279",
    "title": "Hierarchical Few-Shot Generative Models",
    "abstract": "Hierarchical Few-Shot Generative Models",
    "descriptor": "",
    "authors": [
      "Giorgio Giannone",
      "Ole Winther"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.12279"
  },
  {
    "id": "arXiv:2110.12307",
    "title": "Characterizing The Limits of Linear Modeling of Non-Linear Swarm  Behaviors",
    "abstract": "Characterizing The Limits of Linear Modeling of Non-Linear Swarm  Behaviors",
    "descriptor": "",
    "authors": [
      "John Harwell",
      "Angel Sylvester",
      "Maria Gini"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.12307"
  },
  {
    "id": "arXiv:2110.12311",
    "title": "Vector Optimization with Stochastic Bandit Feedback",
    "abstract": "Comments: 18 pages, 3 tables, 2 figures",
    "descriptor": "\nComments: 18 pages, 3 tables, 2 figures\n",
    "authors": [
      "\u00c7a\u011f\u0131n Ararat",
      "Cem Tekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.12311"
  },
  {
    "id": "arXiv:2110.13523",
    "title": "Automating Control of Overestimation Bias for Reinforcement Learning",
    "abstract": "Automating Control of Overestimation Bias for Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Arsenii Kuznetsov",
      "Alexander Grishin",
      "Artem Tsypin",
      "Arsenii Ashukha",
      "Artur Kadurin",
      "Dmitry Vetrov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.13523"
  },
  {
    "id": "arXiv:2110.13814",
    "title": "Bidders' Responses to Auction Format Change in Internet Display  Advertising Auctions",
    "abstract": "Comments: 35 pages, 37 figures",
    "descriptor": "\nComments: 35 pages, 37 figures\n",
    "authors": [
      "Shumpei Goke",
      "Gabriel Y. Weintraub",
      "Ralph Mastromonaco",
      "Sam Seljan"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.13814"
  },
  {
    "id": "arXiv:2110.14022",
    "title": "Intelligent Meta-Imagers: From Compressed to Learned Sensing",
    "abstract": "Comments: 63 pages including 9 figures",
    "descriptor": "\nComments: 63 pages including 9 figures\n",
    "authors": [
      "Chlo\u00e9 Saigre-Tardif",
      "Rashid Faqiri",
      "Hanting Zhao",
      "Lianlin Li",
      "Philipp del Hougne"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2110.14022"
  },
  {
    "id": "arXiv:2110.14532",
    "title": "FacTeR-Check: Semi-automated fact-checking through Semantic Similarity  and Natural Language Inference",
    "abstract": "FacTeR-Check: Semi-automated fact-checking through Semantic Similarity  and Natural Language Inference",
    "descriptor": "",
    "authors": [
      "Alejandro Mart\u00edn",
      "Javier Huertas-Tato",
      "\u00c1lvaro Huertas-Garc\u00eda",
      "Guillermo Villar-Rodr\u00edguez",
      "David Camacho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14532"
  },
  {
    "id": "arXiv:2110.14583",
    "title": "Deep learning via message passing algorithms based on belief propagation",
    "abstract": "Deep learning via message passing algorithms based on belief propagation",
    "descriptor": "",
    "authors": [
      "Carlo Lucibello",
      "Fabrizio Pittorino",
      "Gabriele Perugini",
      "Riccardo Zecchina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14583"
  },
  {
    "id": "arXiv:2110.15318",
    "title": "Communication-Efficient ADMM-based Federated Learning",
    "abstract": "Communication-Efficient ADMM-based Federated Learning",
    "descriptor": "",
    "authors": [
      "Shenglong Zhou",
      "Geoffrey Ye Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.15318"
  },
  {
    "id": "arXiv:2110.15403",
    "title": "Selective Regression Under Fairness Criteria",
    "abstract": "Selective Regression Under Fairness Criteria",
    "descriptor": "",
    "authors": [
      "Abhin Shah",
      "Yuheng Bu",
      "Joshua Ka-Wing Lee",
      "Subhro Das",
      "Rameswar Panda",
      "Prasanna Sattigeri",
      "Gregory W. Wornell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.15403"
  },
  {
    "id": "arXiv:2111.00289",
    "title": "Intrusion Prevention through Optimal Stopping",
    "abstract": "Comments: Preprint; Submitted to IEEE for review. Minor text updates 17/1 2022. arXiv admin note: substantial text overlap with arXiv:2106.07160",
    "descriptor": "\nComments: Preprint; Submitted to IEEE for review. Minor text updates 17/1 2022. arXiv admin note: substantial text overlap with arXiv:2106.07160\n",
    "authors": [
      "Kim Hammar",
      "Rolf Stadler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2111.00289"
  },
  {
    "id": "arXiv:2111.00585",
    "title": "JEDAI: A System for Skill-Aligned Explainable Robot Planning",
    "abstract": "JEDAI: A System for Skill-Aligned Explainable Robot Planning",
    "descriptor": "",
    "authors": [
      "Naman Shah",
      "Pulkit Verma",
      "Trevor Angle",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2111.00585"
  },
  {
    "id": "arXiv:2111.00743",
    "title": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "abstract": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Weiran Huang",
      "Mingyang Yi",
      "Xuyang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.00743"
  },
  {
    "id": "arXiv:2111.01713",
    "title": "Realistic galaxy image simulation via score-based generative models",
    "abstract": "Comments: 11 pages, 8 figures. Code: this https URL . Follow the Twitter bot @ThisIsNotAnApod for DDPM-generated APODs",
    "descriptor": "\nComments: 11 pages, 8 figures. Code: this https URL . Follow the Twitter bot @ThisIsNotAnApod for DDPM-generated APODs\n",
    "authors": [
      "Michael J. Smith",
      "James E. Geach",
      "Ryan A. Jackson",
      "Nikhil Arora",
      "Connor Stone",
      "St\u00e9phane Courteau"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.01713"
  },
  {
    "id": "arXiv:2111.03289",
    "title": "Improved Regret Analysis for Variance-Adaptive Linear Bandits and  Horizon-Free Linear Mixture MDPs",
    "abstract": "Comments: fixed error in the proof; improved the bound for MDPs",
    "descriptor": "\nComments: fixed error in the proof; improved the bound for MDPs\n",
    "authors": [
      "Yeoneung Kim",
      "Insoon Yang",
      "Kwang-Sung Jun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2111.03289"
  },
  {
    "id": "arXiv:2111.03711",
    "title": "Spatiotemporal Impact of Hurricanes on a Power Grid",
    "abstract": "Comments: 6 pages, 7 figures, accepted in 2022 Power and Energy Society General Meeting",
    "descriptor": "\nComments: 6 pages, 7 figures, accepted in 2022 Power and Energy Society General Meeting\n",
    "authors": [
      "Abodh Poudyal",
      "Vishnu Iyengar",
      "Diego Garcia-Camargo",
      "Anamika Dubey"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.03711"
  },
  {
    "id": "arXiv:2111.05070",
    "title": "Almost Optimal Universal Lower Bound for Learning Causal DAGs with  Atomic Interventions",
    "abstract": "Comments: To appear in the proceedings of AISTATS 2022",
    "descriptor": "\nComments: To appear in the proceedings of AISTATS 2022\n",
    "authors": [
      "Vibhor Porwal",
      "Piyush Srivastava",
      "Gaurav Sinha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.05070"
  },
  {
    "id": "arXiv:2111.05388",
    "title": "Complexity of the Ackermann fragment with one leading existential  quantifier",
    "abstract": "Comments: Major revision. The alternating procedure that was presented in the previous version was not sound and hence accepted sentences which were not satisfiable. The problem present in the previous version is now fixed, but the resulting proof is more involved. The presentation has been improved significantly",
    "descriptor": "\nComments: Major revision. The alternating procedure that was presented in the previous version was not sound and hence accepted sentences which were not satisfiable. The problem present in the previous version is now fixed, but the resulting proof is more involved. The presentation has been improved significantly\n",
    "authors": [
      "Reijo Jaakkola"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2111.05388"
  },
  {
    "id": "arXiv:2111.06236",
    "title": "Discovering and Explaining the Representation Bottleneck of DNNs",
    "abstract": "Discovering and Explaining the Representation Bottleneck of DNNs",
    "descriptor": "",
    "authors": [
      "Huiqi Deng",
      "Qihan Ren",
      "Hao Zhang",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.06236"
  },
  {
    "id": "arXiv:2111.06346",
    "title": "Robust Moving Target Defence Against False Data Injection Attacks in  Power Grids",
    "abstract": "Robust Moving Target Defence Against False Data Injection Attacks in  Power Grids",
    "descriptor": "",
    "authors": [
      "Wangkun Xu",
      "Imad M. Jaimoukha",
      "Fei Teng"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.06346"
  },
  {
    "id": "arXiv:2111.08175",
    "title": "Inverse-Weighted Survival Games",
    "abstract": "Comments: Neurips 2021",
    "descriptor": "\nComments: Neurips 2021\n",
    "authors": [
      "Xintian Han",
      "Mark Goldstein",
      "Aahlad Puli",
      "Thomas Wies",
      "Adler J Perotte",
      "Rajesh Ranganath"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.08175"
  },
  {
    "id": "arXiv:2111.09794",
    "title": "A Survey of Generalisation in Deep Reinforcement Learning",
    "abstract": "Comments: Updated versions: fixed typos, added benchmarks and methods, including adjusted Adapting Online section",
    "descriptor": "\nComments: Updated versions: fixed typos, added benchmarks and methods, including adjusted Adapting Online section\n",
    "authors": [
      "Robert Kirk",
      "Amy Zhang",
      "Edward Grefenstette",
      "Tim Rockt\u00e4schel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.09794"
  },
  {
    "id": "arXiv:2111.10095",
    "title": "An Index for Single Source All Destinations Distance Queries in Temporal  Graphs",
    "abstract": "An Index for Single Source All Destinations Distance Queries in Temporal  Graphs",
    "descriptor": "",
    "authors": [
      "Lutz Oettershagen",
      "Petra Mutzel"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.10095"
  },
  {
    "id": "arXiv:2111.10952",
    "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning",
    "abstract": "Comments: ICLR 2022; see this https URL for a video overview of the paper",
    "descriptor": "\nComments: ICLR 2022; see this https URL for a video overview of the paper\n",
    "authors": [
      "Vamsi Aribandi",
      "Yi Tay",
      "Tal Schuster",
      "Jinfeng Rao",
      "Huaixiu Steven Zheng",
      "Sanket Vaibhav Mehta",
      "Honglei Zhuang",
      "Vinh Q. Tran",
      "Dara Bahri",
      "Jianmo Ni",
      "Jai Gupta",
      "Kai Hui",
      "Sebastian Ruder",
      "Donald Metzler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.10952"
  },
  {
    "id": "arXiv:2111.10968",
    "title": "Functorial aggregation",
    "abstract": "Comments: 57 pages, updated bibliography",
    "descriptor": "\nComments: 57 pages, updated bibliography\n",
    "authors": [
      "David I. Spivak"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2111.10968"
  },
  {
    "id": "arXiv:2111.11755",
    "title": "Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance",
    "abstract": "Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance",
    "descriptor": "",
    "authors": [
      "Heeseung Kim",
      "Sungwon Kim",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2111.11755"
  },
  {
    "id": "arXiv:2111.12215",
    "title": "Explainable multiple abnormality classification of chest CT volumes with  deep learning",
    "abstract": "Comments: 25 pages, 7 figures, 6 tables",
    "descriptor": "\nComments: 25 pages, 7 figures, 6 tables\n",
    "authors": [
      "Rachel Lea Draelos",
      "Lawrence Carin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.12215"
  },
  {
    "id": "arXiv:2111.13219",
    "title": "Differentially private stochastic expectation propagation (DP-SEP)",
    "abstract": "Differentially private stochastic expectation propagation (DP-SEP)",
    "descriptor": "",
    "authors": [
      "Margarita Vinaroz",
      "Mijung Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.13219"
  },
  {
    "id": "arXiv:2111.14603",
    "title": "Quantifying the Computational Capability of a Nanomagnetic Reservoir  Computing Platform with Emergent Magnetization Dynamics",
    "abstract": "Quantifying the Computational Capability of a Nanomagnetic Reservoir  Computing Platform with Emergent Magnetization Dynamics",
    "descriptor": "",
    "authors": [
      "Ian T Vidamour",
      "Matthew O A Ellis",
      "David Griffin",
      "Guru Venkat",
      "Charles Swindells",
      "Richard W S Dawidek",
      "Thomas J Broomhall",
      "Nina-Juliane Steinke",
      "Joshaniel F K Cooper",
      "Francisco Maccherozzi",
      "Sarnjeet S Dhesi",
      "Susan Stepney",
      "Eleni Vasilaki",
      "Dan A Allwood",
      "Thomas J Hayward"
    ],
    "subjectives": [
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.14603"
  },
  {
    "id": "arXiv:2111.14625",
    "title": "Cyclic Graph Attentive Match Encoder (CGAME): A Novel Neural Network For  OD Estimation",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Guanzhou Li",
      "Yujing He",
      "Jianping Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.14625"
  },
  {
    "id": "arXiv:2111.15160",
    "title": "Mitigating Adversarial Attacks by Distributing Different Copies to  Different Users",
    "abstract": "Mitigating Adversarial Attacks by Distributing Different Copies to  Different Users",
    "descriptor": "",
    "authors": [
      "Jiyi Zhang",
      "Wesley Joon-Wie Tann",
      "Ee-Chien Chang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.15160"
  },
  {
    "id": "arXiv:2112.00656",
    "title": "Object-aware Video-language Pre-training for Retrieval",
    "abstract": "Comments: 10 pages, 5 figures; Code: this https URL",
    "descriptor": "\nComments: 10 pages, 5 figures; Code: this https URL\n",
    "authors": [
      "Alex Jinpeng Wang",
      "Yixiao Ge",
      "Guanyu Cai",
      "Rui Yan",
      "Xudong Lin",
      "Ying Shan",
      "Xiaohu Qie",
      "Mike Zheng Shou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.00656"
  },
  {
    "id": "arXiv:2112.01197",
    "title": "Sample Prior Guided Robust Model Learning to Suppress Noisy Labels",
    "abstract": "Sample Prior Guided Robust Model Learning to Suppress Noisy Labels",
    "descriptor": "",
    "authors": [
      "Wenkai Chen",
      "Chuang Zhu",
      "Yi Chen",
      "Mengting Li",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.01197"
  },
  {
    "id": "arXiv:2112.02418",
    "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice  Conversion for everyone",
    "abstract": "Comments: This paper is under consideration at 39th International Conference on Machine Learning (ICML 2022)",
    "descriptor": "\nComments: This paper is under consideration at 39th International Conference on Machine Learning (ICML 2022)\n",
    "authors": [
      "Edresson Casanova",
      "Julian Weber",
      "Christopher Shulby",
      "Arnaldo Candido Junior",
      "Eren G\u00f6lge",
      "Moacir Antonelli Ponti"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2112.02418"
  },
  {
    "id": "arXiv:2112.03347",
    "title": "Structured learning of safety guarantees for the control of uncertain  dynamical systems",
    "abstract": "Structured learning of safety guarantees for the control of uncertain  dynamical systems",
    "descriptor": "",
    "authors": [
      "Marc-Antoine Beaudoin",
      "Benoit Boulet"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.03347"
  },
  {
    "id": "arXiv:2112.04468",
    "title": "Revisiting Contrastive Learning through the Lens of Neighborhood  Component Analysis: an Integrated Framework",
    "abstract": "Revisiting Contrastive Learning through the Lens of Neighborhood  Component Analysis: an Integrated Framework",
    "descriptor": "",
    "authors": [
      "Ching-Yun Ko",
      "Jeet Mohapatra",
      "Sijia Liu",
      "Pin-Yu Chen",
      "Luca Daniel",
      "Lily Weng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.04468"
  },
  {
    "id": "arXiv:2112.04812",
    "title": "Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input",
    "abstract": "Deep Visual Constraints: Neural Implicit Models for Manipulation  Planning from Visual Input",
    "descriptor": "",
    "authors": [
      "Jung-Su Ha",
      "Danny Driess",
      "Marc Toussaint"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2112.04812"
  },
  {
    "id": "arXiv:2112.05464",
    "title": "Applying the Shuffle Model of Differential Privacy to Vector Aggregation",
    "abstract": "Comments: 17 pages, 3 figures, in: British International Conference on Databases (BICOD21), London, UK, 28 Mar 2022",
    "descriptor": "\nComments: 17 pages, 3 figures, in: British International Conference on Databases (BICOD21), London, UK, 28 Mar 2022\n",
    "authors": [
      "Mary Scott",
      "Graham Cormode",
      "Carsten Maple"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.05464"
  },
  {
    "id": "arXiv:2112.05999",
    "title": "Curvature-guided dynamic scale networks for Multi-view Stereo",
    "abstract": "Curvature-guided dynamic scale networks for Multi-view Stereo",
    "descriptor": "",
    "authors": [
      "Khang Truong Giang",
      "Soohwan Song",
      "Sungho Jo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2112.05999"
  },
  {
    "id": "arXiv:2112.06209",
    "title": "Measuring Complexity of Learning Schemes Using Hessian-Schatten Total  Variation",
    "abstract": "Measuring Complexity of Learning Schemes Using Hessian-Schatten Total  Variation",
    "descriptor": "",
    "authors": [
      "Shayan Aziznejad",
      "Joaquim Campos",
      "Michael Unser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.06209"
  },
  {
    "id": "arXiv:2112.06211",
    "title": "Quantum kernels for real-world predictions based on electronic health  records",
    "abstract": "Quantum kernels for real-world predictions based on electronic health  records",
    "descriptor": "",
    "authors": [
      "Zoran Krunic",
      "Frederik F. Fl\u00f6ther",
      "George Seegan",
      "Nathan Earnest-Noble",
      "Omar Shehab"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.06211"
  },
  {
    "id": "arXiv:2112.07054",
    "title": "Graph network for learning bi-directional physics",
    "abstract": "Graph network for learning bi-directional physics",
    "descriptor": "",
    "authors": [
      "Sakthi Kumar Arul Prakash",
      "Conrad Tucker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.07054"
  },
  {
    "id": "arXiv:2112.07222",
    "title": "Meta-CPR: Generalize to Unseen Large Number of Agents with Communication  Pattern Recognition Module",
    "abstract": "Meta-CPR: Generalize to Unseen Large Number of Agents with Communication  Pattern Recognition Module",
    "descriptor": "",
    "authors": [
      "Wei-Cheng Tseng",
      "Wei Wei",
      "Da-Cheng Juan",
      "Min Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.07222"
  },
  {
    "id": "arXiv:2112.07323",
    "title": "Experimental Data-Driven Model Predictive Control of a Hospital HVAC  System During Regular Use",
    "abstract": "Comments: 11 pages, 8 figures",
    "descriptor": "\nComments: 11 pages, 8 figures\n",
    "authors": [
      "Emilio T. Maddalena",
      "Silvio A. Muller",
      "Rafael M. dos Santos",
      "Christophe Salzmann",
      "Colin N. Jones"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.07323"
  },
  {
    "id": "arXiv:2112.08369",
    "title": "Feature-Attending Recurrent Modules for Generalizing Object-Centric  Behavior",
    "abstract": "Comments: Updated",
    "descriptor": "\nComments: Updated\n",
    "authors": [
      "Wilka Carvalho",
      "Andrew Lampinen",
      "Kyriacos Nikiforou",
      "Felix Hill",
      "Murray Shanahan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.08369"
  },
  {
    "id": "arXiv:2112.08926",
    "title": "On the accuracy and performance of the lattice Boltzmann method with  64-bit, 32-bit and novel 16-bit number formats",
    "abstract": "Comments: 30 pages, 20 figures, 4 tables, 2 code listings",
    "descriptor": "\nComments: 30 pages, 20 figures, 4 tables, 2 code listings\n",
    "authors": [
      "Moritz Lehmann",
      "Mathias J. Krause",
      "Giorgio Amati",
      "Marcello Sega",
      "Jens Harting",
      "Stephan Gekle"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Biological Physics (physics.bio-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2112.08926"
  },
  {
    "id": "arXiv:2112.09036",
    "title": "The Dual PC Algorithm for Structure Learning",
    "abstract": "The Dual PC Algorithm for Structure Learning",
    "descriptor": "",
    "authors": [
      "Enrico Giudice",
      "Jack Kuipers",
      "Giusi Moffa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09036"
  },
  {
    "id": "arXiv:2112.10699",
    "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with  GreaseTerminator",
    "abstract": "Comments: Accepted in ACM IUI 2022",
    "descriptor": "\nComments: Accepted in ACM IUI 2022\n",
    "authors": [
      "Siddhartha Datta",
      "Konrad Kollnig",
      "Nigel Shadbolt"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.10699"
  },
  {
    "id": "arXiv:2112.13078",
    "title": "Learning Bi-typed Multi-relational Heterogeneous Graph via Dual  Hierarchical Attention Networks",
    "abstract": "Comments: 11 pages, 8 figures and 4 tables",
    "descriptor": "\nComments: 11 pages, 8 figures and 4 tables\n",
    "authors": [
      "Yu Zhao",
      "Shaopeng Wei",
      "Huaming Du",
      "Xingyan Chen",
      "Qing Li",
      "Fuzhen Zhuang",
      "Ji Liu",
      "Gang Kou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.13078"
  },
  {
    "id": "arXiv:2112.13414",
    "title": "Reinforcement Learning with Dynamic Convex Risk Measures",
    "abstract": "Comments: 25 pages, 9 figures",
    "descriptor": "\nComments: 25 pages, 9 figures\n",
    "authors": [
      "Anthony Coache",
      "Sebastian Jaimungal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)",
      "Mathematical Finance (q-fin.MF)",
      "Risk Management (q-fin.RM)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2112.13414"
  },
  {
    "id": "arXiv:2112.14331",
    "title": "360\u00b0 Optical Flow using Tangent Images",
    "abstract": "Comments: The 32nd British Machine Vision Conference (BMVC 2021)",
    "descriptor": "\nComments: The 32nd British Machine Vision Conference (BMVC 2021)\n",
    "authors": [
      "Mingze Yuan",
      "Christian Richardt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.14331"
  },
  {
    "id": "arXiv:2112.14540",
    "title": "Res2NetFuse: A Fusion Method for Infrared and Visible Images",
    "abstract": "Res2NetFuse: A Fusion Method for Infrared and Visible Images",
    "descriptor": "",
    "authors": [
      "Xu Song",
      "Xiao-Jun Wu",
      "Hui Li",
      "Jun Sun",
      "Vasile Palade"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2112.14540"
  },
  {
    "id": "arXiv:2112.14834",
    "title": "Training Quantized Deep Neural Networks via Cooperative Coevolution",
    "abstract": "Training Quantized Deep Neural Networks via Cooperative Coevolution",
    "descriptor": "",
    "authors": [
      "Fu Peng",
      "Shengcai Liu",
      "Ning Lu",
      "Ke Tang"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.14834"
  },
  {
    "id": "arXiv:2201.00299",
    "title": "Improving Out-of-Distribution Robustness via Selective Augmentation",
    "abstract": "Improving Out-of-Distribution Robustness via Selective Augmentation",
    "descriptor": "",
    "authors": [
      "Huaxiu Yao",
      "Yu Wang",
      "Sai Li",
      "Linjun Zhang",
      "Weixin Liang",
      "James Zou",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.00299"
  },
  {
    "id": "arXiv:2201.00813",
    "title": "Lock-Free Locks Revisited",
    "abstract": "Comments: This is the full version of the paper appearing in PPoPP 2022",
    "descriptor": "\nComments: This is the full version of the paper appearing in PPoPP 2022\n",
    "authors": [
      "Naama Ben-David",
      "Guy E. Blelloch",
      "Yuanhao Wei"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.00813"
  },
  {
    "id": "arXiv:2201.01902",
    "title": "Gaussian Imagination in Bandit Learning",
    "abstract": "Gaussian Imagination in Bandit Learning",
    "descriptor": "",
    "authors": [
      "Yueyang Liu",
      "Adithya M. Devraj",
      "Benjamin Van Roy",
      "Kuang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.01902"
  },
  {
    "id": "arXiv:2201.02373",
    "title": "Mirror Learning: A Unifying Framework of Policy Optimisation",
    "abstract": "Mirror Learning: A Unifying Framework of Policy Optimisation",
    "descriptor": "",
    "authors": [
      "Jakub Grudzien Kuba",
      "Christian Schroeder de Witt",
      "Jakob Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.02373"
  },
  {
    "id": "arXiv:2201.02707",
    "title": "ALPHA: Audit that Learns from Previously Hand-Audited Ballots",
    "abstract": "ALPHA: Audit that Learns from Previously Hand-Audited Ballots",
    "descriptor": "",
    "authors": [
      "Philip B. Stark"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.02707"
  },
  {
    "id": "arXiv:2201.02908",
    "title": "Solution to Morgan Problem",
    "abstract": "Comments: 19 pages,5 figures",
    "descriptor": "\nComments: 19 pages,5 figures\n",
    "authors": [
      "Qianghui Xiao"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.02908"
  },
  {
    "id": "arXiv:2201.02944",
    "title": "Adaptive Performance Anomaly Detection for Online Service Systems via  Pattern Sketching",
    "abstract": "Comments: Accepted by The 44th International Conference on Software Engineering (ICSE 2022)",
    "descriptor": "\nComments: Accepted by The 44th International Conference on Software Engineering (ICSE 2022)\n",
    "authors": [
      "Zhuangbin Chen",
      "Jinyang Liu",
      "Yuxin Su",
      "Hongyu Zhang",
      "Xiao Ling",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.02944"
  },
  {
    "id": "arXiv:2201.04066",
    "title": "VGAER: graph neural network reconstruction based community detection",
    "abstract": "Comments: Accepted by AAAI-22: DLG-AAAI'22 (this https URL)",
    "descriptor": "\nComments: Accepted by AAAI-22: DLG-AAAI'22 (this https URL)\n",
    "authors": [
      "Chenyang Qiu",
      "Zhaoci Huang",
      "Wenzhe Xu",
      "Huijia Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.04066"
  },
  {
    "id": "arXiv:2201.04699",
    "title": "The Recurrent Reinforcement Learning Crypto Agent",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2110.04745",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.04745\n",
    "authors": [
      "Gabriel Borrageiro",
      "Nick Firoozye",
      "Paolo Barucca"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2201.04699"
  },
  {
    "id": "arXiv:2201.05610",
    "title": "When less is more: Simplifying inputs aids neural network understanding",
    "abstract": "When less is more: Simplifying inputs aids neural network understanding",
    "descriptor": "",
    "authors": [
      "Robin Tibor Schirrmeister",
      "Rosanne Liu",
      "Sara Hooker",
      "Tonio Ball"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.05610"
  },
  {
    "id": "arXiv:2201.05912",
    "title": "Common Phone: A Multilingual Dataset for Robust Acoustic Modelling",
    "abstract": "Comments: Pre-print submitted to LREC 2022 Link to Common Phone: this https URL",
    "descriptor": "\nComments: Pre-print submitted to LREC 2022 Link to Common Phone: this https URL\n",
    "authors": [
      "Philipp Klumpp",
      "Tom\u00e1s Arias-Vergara",
      "Paula Andrea P\u00e9rez-Toro",
      "Elmar N\u00f6th",
      "Juan Rafael Orozco-Arroyave"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.05912"
  },
  {
    "id": "arXiv:2201.07098",
    "title": "Compatibility and accessibility: lattice representations for semantics  of non-classical and modal logics",
    "abstract": "Comments: Added references to Ploscica and Craig, Havier, and Priestley. Updated Section 3.1.4 with new evidence relevant to Question 3.12 and Conjecture 3.13",
    "descriptor": "\nComments: Added references to Ploscica and Craig, Havier, and Priestley. Updated Section 3.1.4 with new evidence relevant to Question 3.12 and Conjecture 3.13\n",
    "authors": [
      "Wesley H. Holliday"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2201.07098"
  },
  {
    "id": "arXiv:2201.07322",
    "title": "Interpretable Single-Cell Set Classification with Kernel Mean Embeddings",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Siyuan Shan",
      "Vishal Baskaran",
      "Haidong Yi",
      "Jolene Ranek",
      "Natalie Stanley",
      "Junier Oliva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2201.07322"
  },
  {
    "id": "arXiv:2201.07487",
    "title": "A Concise Tutorial on Approximate Message Passing",
    "abstract": "Comments: 16 pages, 9 figures",
    "descriptor": "\nComments: 16 pages, 9 figures\n",
    "authors": [
      "Qiuyun Zou",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.07487"
  },
  {
    "id": "arXiv:2201.07700",
    "title": "Anytime PSRO for Two-Player Zero-Sum Games",
    "abstract": "Comments: Published in AAAI Reinforcement Learning in Games Workshop",
    "descriptor": "\nComments: Published in AAAI Reinforcement Learning in Games Workshop\n",
    "authors": [
      "Stephen McAleer",
      "Kevin Wang",
      "John Lanier",
      "Marc Lanctot",
      "Pierre Baldi",
      "Tuomas Sandholm",
      "Roy Fox"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.07700"
  },
  {
    "id": "arXiv:2201.08368",
    "title": "An Alternative Issue Tracking Dataset of Public Jira Repositories",
    "abstract": "Comments: 5 pages",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Lloyd Montgomery",
      "Clara L\u00fcders",
      "Walid Maalej"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.08368"
  },
  {
    "id": "arXiv:2201.08385",
    "title": "Improving Specificity in Mammography Using Cross-correlation between  Wavelet and Fourier Transform",
    "abstract": "Improving Specificity in Mammography Using Cross-correlation between  Wavelet and Fourier Transform",
    "descriptor": "",
    "authors": [
      "Liuhua Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.08385"
  },
  {
    "id": "arXiv:2201.08504",
    "title": "Deep reinforcement learning under signal temporal logic constraints  using Lagrangian relaxation",
    "abstract": "Comments: 9 pages, 10 figures. arXiv admin note: text overlap with arXiv:2108.01317",
    "descriptor": "\nComments: 9 pages, 10 figures. arXiv admin note: text overlap with arXiv:2108.01317\n",
    "authors": [
      "Junya Ikemoto",
      "Toshimitsu Ushio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.08504"
  },
  {
    "id": "arXiv:2201.08638",
    "title": "Approximation approach to the fractional BVP with the Dirichlet type  boundary conditions",
    "abstract": "Approximation approach to the fractional BVP with the Dirichlet type  boundary conditions",
    "descriptor": "",
    "authors": [
      "Kateryna Marynets",
      "Dona Pantova"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.08638"
  },
  {
    "id": "arXiv:2201.08802",
    "title": "Deconfounding to Explanation Evaluation in Graph Neural Networks",
    "abstract": "Comments: 18 pages",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Ying-Xin",
      "Xiang Wang",
      "An Zhang",
      "Xia Hu",
      "Fuli Feng",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.08802"
  },
  {
    "id": "arXiv:2201.08949",
    "title": "Temporal Aggregation for Adaptive RGBT Tracking",
    "abstract": "Comments: 12 pages, 10 figures",
    "descriptor": "\nComments: 12 pages, 10 figures\n",
    "authors": [
      "Zhangyong Tang",
      "Tianyang Xu",
      "Xiao-Jun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.08949"
  },
  {
    "id": "arXiv:2201.08951",
    "title": "Visual Representation Learning with Self-Supervised Attention for  Low-Label High-data Regime",
    "abstract": "Comments: Accepted to ICASSP-2022",
    "descriptor": "\nComments: Accepted to ICASSP-2022\n",
    "authors": [
      "Prarthana Bhattacharyya",
      "Chenge Li",
      "Xiaonan Zhao",
      "Istv\u00e1n Feh\u00e9rv\u00e1ri",
      "Jason Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.08951"
  },
  {
    "id": "arXiv:2201.08984",
    "title": "PiCO: Contrastive Label Disambiguation for Partial Label Learning",
    "abstract": "Comments: Accepted to ICLR 2022 (Oral Presentation, Acceptance Ratio: 1.6%)",
    "descriptor": "\nComments: Accepted to ICLR 2022 (Oral Presentation, Acceptance Ratio: 1.6%)\n",
    "authors": [
      "Haobo Wang",
      "Ruixuan Xiao",
      "Yixuan Li",
      "Lei Feng",
      "Gang Niu",
      "Gang Chen",
      "Junbo Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.08984"
  },
  {
    "id": "arXiv:2201.09050",
    "title": "Scheduling Policies for Stability and Optimal Server Running Cost in  Cloud Computing Platforms",
    "abstract": "Scheduling Policies for Stability and Optimal Server Running Cost in  Cloud Computing Platforms",
    "descriptor": "",
    "authors": [
      "Haritha K",
      "Chandramani Singh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.09050"
  },
  {
    "id": "arXiv:2201.09170",
    "title": "Online Self-Calibration for Visual-Inertial Navigation Systems: Models,  Analysis and Degeneracy",
    "abstract": "Comments: We study the observability analysis and degeneracy for VINS with full-parameter calibration, including IMU/camera intrinsic and IMU-camera spatial-temporal calibration (with rolling shutter readout time). Extensive simulations and real-world experiments are performed to fully verify the analysis and the proposed system",
    "descriptor": "\nComments: We study the observability analysis and degeneracy for VINS with full-parameter calibration, including IMU/camera intrinsic and IMU-camera spatial-temporal calibration (with rolling shutter readout time). Extensive simulations and real-world experiments are performed to fully verify the analysis and the proposed system\n",
    "authors": [
      "Yulin Yang",
      "Patrick Geneva",
      "Xingxing Zuo",
      "Guoquan Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.09170"
  },
  {
    "id": "arXiv:2201.09207",
    "title": "Visual Object Tracking on Multi-modal RGB-D Videos: A Review",
    "abstract": "Visual Object Tracking on Multi-modal RGB-D Videos: A Review",
    "descriptor": "",
    "authors": [
      "Xue-Feng Zhu",
      "Tianyang Xu",
      "Xiao-Jun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.09207"
  },
  {
    "id": "arXiv:2201.09246",
    "title": "Face recognition via compact second order image gradient orientations",
    "abstract": "Comments: 26 pages, 6 figures",
    "descriptor": "\nComments: 26 pages, 6 figures\n",
    "authors": [
      "He-Feng Yin",
      "Xiao-Jun Wu",
      "Xiaoning Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.09246"
  },
  {
    "id": "arXiv:2201.09280",
    "title": "SpiroMask: Measuring Lung Function Using Consumer-Grade Masks",
    "abstract": "Comments: 33 pages",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Rishiraj Adhikary",
      "Dhruvi Lodhavia",
      "Chris Francis",
      "Rohit Patil",
      "Tanmay Srivastava",
      "Prerna Khanna",
      "Nipun Batra",
      "Joe Breda",
      "Jacob Peplinski",
      "Shwetak Patel"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09280"
  },
  {
    "id": "arXiv:2201.09296",
    "title": "A Survey for Deep RGBT Tracking",
    "abstract": "Comments: 7 pages, 3 figures",
    "descriptor": "\nComments: 7 pages, 3 figures\n",
    "authors": [
      "Zhangyong Tang",
      "Tianyang Xu",
      "Xiao-Jun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.09296"
  },
  {
    "id": "arXiv:2201.09457",
    "title": "Homotopic Policy Mirror Descent: Policy Convergence, Implicit  Regularization, and Improved Sample Complexity",
    "abstract": "Homotopic Policy Mirror Descent: Policy Convergence, Implicit  Regularization, and Improved Sample Complexity",
    "descriptor": "",
    "authors": [
      "Yan Li",
      "Tuo Zhao",
      "Guanghui Lan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.09457"
  },
  {
    "id": "arXiv:2201.09493",
    "title": "STRIDE-based Cyber Security Threat Modeling for IoT-enabled Precision  Agriculture Systems",
    "abstract": "STRIDE-based Cyber Security Threat Modeling for IoT-enabled Precision  Agriculture Systems",
    "descriptor": "",
    "authors": [
      "Md. Rashid Al Asif",
      "Khondokar Fida Hasan",
      "Md Zahidul Islam",
      "Rahamatullah Khondoker"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.09493"
  },
  {
    "id": "arXiv:2201.09556",
    "title": "Prefix palindromic length of the Sierpinski word",
    "abstract": "Comments: Submitted to DLT 2022",
    "descriptor": "\nComments: Submitted to DLT 2022\n",
    "authors": [
      "Dora V. Bulgakova",
      "Anna E. Frid",
      "J\u00e9r\u00e9my Scanvic"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2201.09556"
  },
  {
    "id": "arXiv:2201.09598",
    "title": "On the Optimization Landscape of Dynamic Output Feedback Linear  Quadratic Control",
    "abstract": "On the Optimization Landscape of Dynamic Output Feedback Linear  Quadratic Control",
    "descriptor": "",
    "authors": [
      "Jingliang Duan",
      "Wenhan Cao",
      "Yang Zheng",
      "Lin Zhao"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.09598"
  },
  {
    "id": "arXiv:2201.09635",
    "title": "Hierarchical Reinforcement Learning with Adversarially Guided Subgoals",
    "abstract": "Hierarchical Reinforcement Learning with Adversarially Guided Subgoals",
    "descriptor": "",
    "authors": [
      "Vivienne Huiling Wang",
      "Joni Pajarinen",
      "Tinghuai Wang",
      "Joni-Kristian K\u00e4m\u00e4r\u00e4inen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09635"
  },
  {
    "id": "arXiv:2201.09802",
    "title": "Constrained Policy Optimization via Bayesian World Models",
    "abstract": "Constrained Policy Optimization via Bayesian World Models",
    "descriptor": "",
    "authors": [
      "Yarden As",
      "Ilnura Usmanova",
      "Sebastian Curi",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.09802"
  },
  {
    "id": "arXiv:2201.10082",
    "title": "Polar Coded Computing: The Role of the Scaling Exponent",
    "abstract": "Polar Coded Computing: The Role of the Scaling Exponent",
    "descriptor": "",
    "authors": [
      "Dorsa Fathollahi",
      "Marco Mondelli"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.10082"
  },
  {
    "id": "arXiv:2201.10110",
    "title": "A Hybrid Quantum-Classical Algorithm for Robust Fitting",
    "abstract": "A Hybrid Quantum-Classical Algorithm for Robust Fitting",
    "descriptor": "",
    "authors": [
      "Anh-Dzung Doan",
      "Michele Sasdelli",
      "David Suter",
      "Tat-Jun Chin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10110"
  },
  {
    "id": "arXiv:2201.10145",
    "title": "MSNet: A Deep Multi-scale Submanifold Network for Visual Classification",
    "abstract": "MSNet: A Deep Multi-scale Submanifold Network for Visual Classification",
    "descriptor": "",
    "authors": [
      "Ziheng Chen",
      "Xiao-Jun Wu",
      "Tianyang Xu",
      "Rui Wang",
      "Zhiwu Huang",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10145"
  },
  {
    "id": "arXiv:2201.10152",
    "title": "Unsupervised Image Fusion Method based on Feature Mutual Mapping",
    "abstract": "Unsupervised Image Fusion Method based on Feature Mutual Mapping",
    "descriptor": "",
    "authors": [
      "Dongyu Rao",
      "Xiao-Jun Wu",
      "Tianyang Xu",
      "Guoyang Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10152"
  },
  {
    "id": "arXiv:2201.10179",
    "title": "Flexible skylines, regret minimization and skyline ranking: a comparison  to know how to select the right approach",
    "abstract": "Flexible skylines, regret minimization and skyline ranking: a comparison  to know how to select the right approach",
    "descriptor": "",
    "authors": [
      "Vittorio Fabris"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.10179"
  },
  {
    "id": "arXiv:2201.10207",
    "title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning  for Speech Pre-Training",
    "abstract": "Comments: ICLR 2022",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Wenyong Huang",
      "Zhenhe Zhang",
      "Yu Ting Yeung",
      "Xin Jiang",
      "Qun Liu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2201.10207"
  },
  {
    "id": "arXiv:2201.10217",
    "title": "Poisson's CDF applied to Flexible Skylines",
    "abstract": "Comments: 10 pages, 3 figures",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Jaime Pons Garrido"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.10217"
  },
  {
    "id": "arXiv:2201.10361",
    "title": "Reinforcement Learning-Based Deadline and Battery-Aware Offloading in  Smart Farm IoT-UAV Networks",
    "abstract": "Comments: Accepted Paper. Please check footnote in Page 1 for copyright",
    "descriptor": "\nComments: Accepted Paper. Please check footnote in Page 1 for copyright\n",
    "authors": [
      "Anne Catherine Nguyen",
      "Turgay Pamuklu",
      "Aisha Syed",
      "W. Sean Kennedy",
      "Melike Erol-Kantarci"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.10361"
  },
  {
    "id": "arXiv:2201.10485",
    "title": "Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks",
    "abstract": "Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks",
    "descriptor": "",
    "authors": [
      "Jana Wagemaker",
      "Nate Foster",
      "Tobias Kapp\u00e9",
      "Dexter Kozen",
      "Jurriaan Rot",
      "Alexandra Silva"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2201.10485"
  },
  {
    "id": "arXiv:2201.10751",
    "title": "Graph Neural Networks with Dynamic and Static Representations for Social  Recommendation",
    "abstract": "Comments: 17 pages, 4 figures. Extended version of paper accepted by DASFAA 2022",
    "descriptor": "\nComments: 17 pages, 4 figures. Extended version of paper accepted by DASFAA 2022\n",
    "authors": [
      "Junfa Lin",
      "Siyuan Chen",
      "Jiahai Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.10751"
  },
  {
    "id": "arXiv:2201.10758",
    "title": "An Efficient Approximation Algorithm for the Colonel Blotto Game",
    "abstract": "An Efficient Approximation Algorithm for the Colonel Blotto Game",
    "descriptor": "",
    "authors": [
      "Daniel Beaglehole"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.10758"
  },
  {
    "id": "arXiv:2201.10821",
    "title": "Localization in Ensemble Kalman inversion",
    "abstract": "Comments: 37 pages, 7 figures",
    "descriptor": "\nComments: 37 pages, 7 figures\n",
    "authors": [
      "Xin T. Tong",
      "Matthias Morzfeld"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2201.10821"
  },
  {
    "id": "arXiv:2201.10905",
    "title": "Boomerang Spectra of Two Classes of Power Functions via Their  Differential Spectra",
    "abstract": "Comments: Part of the results of our article are covered by sihem BFA's article",
    "descriptor": "\nComments: Part of the results of our article are covered by sihem BFA's article\n",
    "authors": [
      "Ziying Zhang",
      "Haode Yan",
      "Zhen Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.10905"
  },
  {
    "id": "arXiv:2201.11085",
    "title": "Understanding and Compressing Music with Maximal Transformable Patterns",
    "abstract": "Understanding and Compressing Music with Maximal Transformable Patterns",
    "descriptor": "",
    "authors": [
      "David Meredith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.11085"
  },
  {
    "id": "arXiv:2201.11146",
    "title": "Machine-learning of nonlocal kernels for anomalous subsurface transport  from breakthrough curves",
    "abstract": "Machine-learning of nonlocal kernels for anomalous subsurface transport  from breakthrough curves",
    "descriptor": "",
    "authors": [
      "Xiao Xu",
      "Marta D'Elia",
      "Christian Glusa",
      "John T. Foster"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.11146"
  },
  {
    "id": "arXiv:2201.11331",
    "title": "Epistemic AI platform accelerates innovation by connecting biomedical  knowledge",
    "abstract": "Comments: 12 pages, 2 main figures",
    "descriptor": "\nComments: 12 pages, 2 main figures\n",
    "authors": [
      "Emily Koo",
      "Heather Bowling",
      "Kenneth Ashworth",
      "David J. Heeger",
      "Stefano Pacifico"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.11331"
  },
  {
    "id": "arXiv:2201.11528",
    "title": "Beyond ImageNet Attack: Towards Crafting Adversarial Examples for  Black-box Domains",
    "abstract": "Comments: Accepted by ICLR 2022",
    "descriptor": "\nComments: Accepted by ICLR 2022\n",
    "authors": [
      "Qilong Zhang",
      "Xiaodan Li",
      "Yuefeng Chen",
      "Jingkuan Song",
      "Lianli Gao",
      "Yuan He",
      "Hui Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.11528"
  },
  {
    "id": "arXiv:2201.11674",
    "title": "Vision Checklist: Towards Testable Error Analysis of Image Models to  Help System Designers Interrogate Model Capabilities",
    "abstract": "Comments: 17 pages, 18 figures",
    "descriptor": "\nComments: 17 pages, 18 figures\n",
    "authors": [
      "Xin Du",
      "Benedicte Legastelois",
      "Bhargavi Ganesh",
      "Ajitha Rajan",
      "Hana Chockler",
      "Vaishak Belle",
      "Stuart Anderson",
      "Subramanian Ramamoorthy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11674"
  },
  {
    "id": "arXiv:2201.11727",
    "title": "Multi-Agent Reinforcement Learning for Network Load Balancing in Data  Center",
    "abstract": "Multi-Agent Reinforcement Learning for Network Load Balancing in Data  Center",
    "descriptor": "",
    "authors": [
      "Zhiyuan Yao",
      "Zihan Ding",
      "Thomas Clausen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.11727"
  },
  {
    "id": "arXiv:2201.11742",
    "title": "A neural net architecture based on principles of neural plasticity and  development evolves to effectively catch prey in a simulated environment",
    "abstract": "Comments: 7 pages, 4 figures, 1 appendix page",
    "descriptor": "\nComments: 7 pages, 4 figures, 1 appendix page\n",
    "authors": [
      "Addison Wood",
      "Jory Schossau",
      "Nick Sabaj",
      "Richard Liu",
      "Mark Reimers"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.11742"
  },
  {
    "id": "arXiv:2201.11769",
    "title": "Design Optimization of a Three-Phase Transformer Using Finite Element  Analysis",
    "abstract": "Comments: 6 pages, 7 figures",
    "descriptor": "\nComments: 6 pages, 7 figures\n",
    "authors": [
      "Ahmet Furkan Hacan",
      "Bilal Kabas",
      "Samet Oguten"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2201.11769"
  },
  {
    "id": "arXiv:2201.11795",
    "title": "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG  Encoder-Decoder",
    "abstract": "Comments: Accepted in DCC 2022, 11 pages",
    "descriptor": "\nComments: Accepted in DCC 2022, 11 pages\n",
    "authors": [
      "Ankur Mali",
      "Alexander Ororbia",
      "Daniel Kifer",
      "Lee Giles"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.11795"
  },
  {
    "id": "arXiv:2201.11867",
    "title": "Neural-FST Class Language Model for End-to-End Speech Recognition",
    "abstract": "Comments: Accepted for publication at ICASSP 2022",
    "descriptor": "\nComments: Accepted for publication at ICASSP 2022\n",
    "authors": [
      "Antoine Bruguier",
      "Duc Le",
      "Rohit Prabhavalkar",
      "Dangna Li",
      "Zhe Liu",
      "Bo Wang",
      "Eun Chang",
      "Fuchun Peng",
      "Ozlem Kalinli",
      "Michael L. Seltzer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2201.11867"
  },
  {
    "id": "arXiv:2201.11987",
    "title": "Computer-aided Recognition and Assessment of a Porous Bioelastomer on  Ultrasound Images for Regenerative Medicine Applications",
    "abstract": "Computer-aided Recognition and Assessment of a Porous Bioelastomer on  Ultrasound Images for Regenerative Medicine Applications",
    "descriptor": "",
    "authors": [
      "Dun Wang",
      "Kaixuan Guo",
      "Yanying Zhu",
      "Jia Sun",
      "Aliona Dreglea",
      "Jiao Yu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.11987"
  },
  {
    "id": "arXiv:2201.11990",
    "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A  Large-Scale Generative Language Model",
    "abstract": "Comments: Shaden Smith and Mostofa Patwary contributed equally",
    "descriptor": "\nComments: Shaden Smith and Mostofa Patwary contributed equally\n",
    "authors": [
      "Shaden Smith",
      "Mostofa Patwary",
      "Brandon Norick",
      "Patrick LeGresley",
      "Samyam Rajbhandari",
      "Jared Casper",
      "Zhun Liu",
      "Shrimai Prabhumoye",
      "George Zerveas",
      "Vijay Korthikanti",
      "Elton Zhang",
      "Rewon Child",
      "Reza Yazdani Aminabadi",
      "Julie Bernauer",
      "Xia Song",
      "Mohammad Shoeybi",
      "Yuxiong He",
      "Michael Houston",
      "Saurabh Tiwary",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.11990"
  },
  {
    "id": "arXiv:2201.11994",
    "title": "FCMNet: Full Communication Memory Net for Team-Level Cooperation in  Multi-Agent Systems",
    "abstract": "Comments: To appear in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022)",
    "descriptor": "\nComments: To appear in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2022)\n",
    "authors": [
      "Yutong Wang",
      "Guillaume Sartoretti"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2201.11994"
  },
  {
    "id": "arXiv:2201.12006",
    "title": "Provably Improving Expert Predictions with Conformal Prediction",
    "abstract": "Provably Improving Expert Predictions with Conformal Prediction",
    "descriptor": "",
    "authors": [
      "Eleni Straitouri",
      "Lequn Wang",
      "Nastaran Okati",
      "Manuel Gomez Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12006"
  },
  {
    "id": "arXiv:2201.12151",
    "title": "Sampling Theorems for Learning from Incomplete Measurements",
    "abstract": "Sampling Theorems for Learning from Incomplete Measurements",
    "descriptor": "",
    "authors": [
      "Juli\u00e1n Tachella",
      "Dongdong Chen",
      "Mike Davies"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12151"
  },
  {
    "id": "arXiv:2201.12170",
    "title": "Unsupervised Single-shot Depth Estimation using Perceptual  Reconstruction",
    "abstract": "Comments: submitted to the International Conference on Machine Learning (ICML) 2022. arXiv admin note: text overlap with arXiv:2103.16938",
    "descriptor": "\nComments: submitted to the International Conference on Machine Learning (ICML) 2022. arXiv admin note: text overlap with arXiv:2103.16938\n",
    "authors": [
      "Christoph Angermann",
      "Matthias Schwab",
      "Markus Haltmeier",
      "Christian Laubichler",
      "Steinbj\u00f6rn J\u00f3nsson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12170"
  },
  {
    "id": "arXiv:2201.12332",
    "title": "On the Hidden Biases of Policy Mirror Ascent in Continuous Action Spaces",
    "abstract": "On the Hidden Biases of Policy Mirror Ascent in Continuous Action Spaces",
    "descriptor": "",
    "authors": [
      "Amrit Singh Bedi",
      "Souradip Chakraborty",
      "Anjaly Parayil",
      "Brian Sadler",
      "Pratap Tokekar",
      "Alec Koppel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12332"
  }
]