[
  {
    "id": "arXiv:2209.14299",
    "title": "Software Defect Prediction Using Support Vector Machine",
    "abstract": "Software defect prediction is an essential task during the software\ndevelopment Lifecycle as it can help managers to identify the most\ndefect-proneness modules. Thus, it can reduce the test cost and assign testing\nresources efficiently. Many classification methods can be used to determine if\nthe software is defective or not. Support Vector Machine (SVM) has not been\nused extensively for such problems because of its instability when applied on\ndifferent datasets and parameter settings. The main parameter that influences\nthe accuracy is the choice of the kernel function. The use of kernel functions\nhas not been studied thoroughly in previous papers. Therefore, this research\nexamines the performance and accuracy of SVM with six different kernel\nfunctions. Various public datasets from the PROMISE project empirically\nvalidate our hypothesis. The results demonstrate that no kernel function can\ngive stable performance across different experimental settings. In addition,\nthe use of PCA as a feature reduction algorithm shows slight accuracy\nimprovement over some datasets.",
    "descriptor": "",
    "authors": [
      "Haneen Abu Alhija",
      "Mohammad Azzeh",
      "Fadi Almasalha"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14299"
  },
  {
    "id": "arXiv:2209.14300",
    "title": "Locally Weighted Regression with different Kernel Smoothers for Software  Effort Estimation",
    "abstract": "Estimating software effort has been a largely unsolved problem for decades.\nOne of the main reasons that hinders building accurate estimation models is the\noften heterogeneous nature of software data with a complex structure.\nTypically, building effort estimation models from local data tends to be more\naccurate than using the entire data. Previous studies have focused on the use\nof clustering techniques and decision trees to generate local and coherent data\nthat can help in building local prediction models. However, these approaches\nmay fall short in some aspect due to limitations in finding optimal clusters\nand processing noisy data. In this paper we used a more sophisticated locality\napproach that can mitigate these shortcomings that is Locally Weighted\nRegression (LWR). This method provides an efficient solution to learn from\nlocal data by building an estimation model that combines multiple local\nregression models in k-nearest-neighbor based model. The main factor affecting\nthe accuracy of this method is the choice of the kernel function used to derive\nthe weights for local regression models. This paper investigates the effects of\nchoosing different kernels on the performance of Locally Weighted Regression of\na software effort estimation problem. After comprehensive experiments with 7\ndatasets, 10 kernels, 3 polynomial degrees and 4 bandwidth values with a total\nof 840 Locally Weighted Regression variants, we found that: 1) Uniform kernel\nfunctions cannot outperform non-uniform kernel functions, and 2) kernel type,\npolynomial degrees and bandwidth parameters have no specific effect on the\nestimation accuracy.",
    "descriptor": "",
    "authors": [
      "Yousef Alqasrawi",
      "Mohammad Azzeh",
      "Yousef Elsheikh"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14300"
  },
  {
    "id": "arXiv:2209.14338",
    "title": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
    "abstract": "Language models such as GPT-3 have caused a furore in the research community.\nSome studies found that GPT-3 has some creative abilities and makes mistakes\nthat are on par with human behaviour. This paper answers a related question:\nwho is GPT-3? We administered two validated measurement tools to GPT-3 to\nassess its personality, the values it holds and its self-reported demographics.\nOur results show that GPT-3 scores similarly to human samples in terms of\npersonality and - when provided with a model response memory - in terms of the\nvalues it holds. We provide the first evidence of psychological assessment of\nthe GPT-3 model and thereby add to our understanding of the GPT-3 model. We\nclose with suggestions for future research that moves social science closer to\nlanguage models and vice versa.",
    "descriptor": "",
    "authors": [
      "Maril\u00f9 Miotto",
      "Nicola Rossberg",
      "Bennett Kleinberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14338"
  },
  {
    "id": "arXiv:2209.14340",
    "title": "Using Processing Fluency as a Metric of Trust in Scatterplot  Visualizations",
    "abstract": "Establishing trust with readers is an important first step in visual data\ncommunication. But what makes a visualization trustworthy? Psychology and\nbehavioral economics research has found processing fluency (i.e., speed and\naccuracy of perceiving and processing a stimulus) is central to perceived\ntrust. We examine the association between processing fluency and trust in\nvisualizations through two empirical studies. In Experiment 1, we tested the\neffect of camouflaging a visualization on processing fluency. Participants\nestimated the proportion of data values within a specified range for six\ncamouflaged visualizations and one non-camouflaged control; they also reported\ntheir perceived difficulty for each of the visualizations. Camouflaged\nvisualizations produced less accurate estimations compared to the control. In\nExperiment 2, we created a decision task based on trust games adapted from\nbehavioral economics. We asked participants to invest money in two hypothetical\ncompanies and report how much they trust each company. One company communicates\nits strategy with a camouflaged visualization, the other with a controlled\nvisualization. Participants tended to invest less money in the company\npresenting a camouflaged visualization. Hence, we found support for the\nhypothesis that processing fluency is key to the perception of trust in visual\ndata communication.",
    "descriptor": "\nComments: IEEE VIS TREX Workshop, 6 pages, 2 page reference, 6 figures\n",
    "authors": [
      "Hamza Elhamdadi",
      "Lace Padilla",
      "Cindy Xiong"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14340"
  },
  {
    "id": "arXiv:2209.14341",
    "title": "The Change You Want to See",
    "abstract": "We live in a dynamic world where things change all the time. Given two images\nof the same scene, being able to automatically detect the changes in them has\npractical applications in a variety of domains. In this paper, we tackle the\nchange detection problem with the goal of detecting \"object-level\" changes in\nan image pair despite differences in their viewpoint and illumination. To this\nend, we make the following four contributions: (i) we propose a scalable\nmethodology for obtaining a large-scale change detection training dataset by\nleveraging existing object segmentation benchmarks; (ii) we introduce a\nco-attention based novel architecture that is able to implicitly determine\ncorrespondences between an image pair and find changes in the form of bounding\nbox predictions; (iii) we contribute four evaluation datasets that cover a\nvariety of domains and transformations, including synthetic image changes, real\nsurveillance images of a 3D scene, and synthetic 3D scenes with camera motion;\n(iv) we evaluate our model on these four datasets and demonstrate zero-shot and\nbeyond training transformation generalization.",
    "descriptor": "\nComments: Paper accepted at WACV 2023\n",
    "authors": [
      "Ragav Sachdeva",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14341"
  },
  {
    "id": "arXiv:2209.14344",
    "title": "Pareto Actor-Critic for Equilibrium Selection in Multi-Agent  Reinforcement Learning",
    "abstract": "Equilibrium selection in multi-agent games refers to the problem of selecting\na Pareto-optimal equilibrium. It has been shown that many state-of-the-art\nmulti-agent reinforcement learning (MARL) algorithms are prone to converging to\nPareto-dominated equilibria due to the uncertainty each agent has about the\npolicy of the other agents during training. To address suboptimal equilibrium\nselection, we propose Pareto-AC (PAC), an actor-critic algorithm that utilises\na simple principle of no-conflict games (a superset of cooperative games with\nidentical rewards): each agent can assume the others will choose actions that\nwill lead to a Pareto-optimal equilibrium. We evaluate PAC in a diverse set of\nmulti-agent games and show that it converges to higher episodic returns\ncompared to alternative MARL algorithms, as well as successfully converging to\na Pareto-optimal equilibrium in a range of matrix games. Finally, we propose a\ngraph neural network extension which is shown to efficiently scale in games\nwith up to 15 agents.",
    "descriptor": "\nComments: 10 pages, 14 figures\n",
    "authors": [
      "Filippos Christianos",
      "Georgios Papoudakis",
      "Stefano V. Albrecht"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2209.14344"
  },
  {
    "id": "arXiv:2209.14345",
    "title": "Audio Barlow Twins: Self-Supervised Audio Representation Learning",
    "abstract": "The Barlow Twins self-supervised learning objective requires neither negative\nsamples or asymmetric learning updates, achieving results on a par with the\ncurrent state-of-the-art within Computer Vision. As such, we present Audio\nBarlow Twins, a novel self-supervised audio representation learning approach,\nadapting Barlow Twins to the audio domain. We pre-train on the large-scale\naudio dataset AudioSet, and evaluate the quality of the learnt representations\non 18 tasks from the HEAR 2021 Challenge, achieving results which outperform,\nor otherwise are on a par with, the current state-of-the-art for instance\ndiscrimination self-supervised learning approaches to audio representation\nlearning. Code at https://github.com/jonahanton/SSL_audio.",
    "descriptor": "\nComments: 15 pages (4 main text, rest references + appendices)\n",
    "authors": [
      "Jonah Anton",
      "Harry Coppock",
      "Pancham Shukla",
      "Bjorn W.Schuller"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14345"
  },
  {
    "id": "arXiv:2209.14350",
    "title": "Callipepla: Stream Centric Instruction Set and Mixed Precision for  Accelerating Conjugate Gradient Solver",
    "abstract": "The continued growth in the processing power of FPGAs coupled with high\nbandwidth memories (HBM), makes systems like the Xilinx U280 credible platforms\nfor the linear solvers which often dominate the run time of scientific and\nengineering applications. In this paper we present Callipepla, an accelerator\nfor a preconditioned conjugate gradient linear solver (CG). FPGA acceleration\nof CG faces three challenges: (1) how to support an arbitrary problem and\nterminate acceleration processing on the fly, (2) how to coordinate long-vector\ndata flow among processing modules, and (3) how to save off-chip memory\nbandwidth and maintain double (FP64) precision accuracy. To tackle the three\nchallenges, we present (1) a stream-centric instruction set for efficient\nstreaming processing and control, (2) decentralized vector flow scheduling to\ncoordinate vector data flow among modules and further reduce off-chip memory\naccesses with a double memory channel design, and (3) a mixed precision scheme\nto save bandwidth yet still achieve effective double precision quality\nsolutions. We prototype the accelerator on a Xilinx U280 HBM FPGA. Our\nevaluation shows that compared to the Xilinx HPC product, the XcgSolver,\nCallipepla archives a speedup of 3.94x, 3.36x higher throughput, and 2.94x\nbetter energy efficiency. Compared to an NVIDIA A100 GPU which has 4x the\nmemory bandwidth of Callipepla, we still achieve 77% of its throughput with\n3.34x higher the energy efficiency.",
    "descriptor": "",
    "authors": [
      "Linghao Song",
      "Licheng Guo",
      "Suhail Basalama",
      "Yuze Chi",
      "Robert F. Lucas",
      "Jason Cong"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.14350"
  },
  {
    "id": "arXiv:2209.14359",
    "title": "Robust Incremental Smoothing and Mapping (riSAM)",
    "abstract": "This paper presents a method for robust optimization for online incremental\nSimultaneous Localization and Mapping (SLAM). Due to the NP-Hardness of data\nassociation in the presence of perceptual aliasing, tractable (approximate)\napproaches to data association will produce erroneous measurements. We require\nSLAM back-ends that can converge to accurate solutions in the presence of\noutlier measurements while meeting online efficiency constraints. Existing\nrobust SLAM methods either remain sensitive to outliers, become increasingly\nsensitive to initialization, or fail to provide online efficiency. We present\nthe robust incremental Smoothing and Mapping (riSAM) algorithm, a robust\nback-end optimizer for incremental SLAM based on Graduated Non-Convexity. We\ndemonstrate on benchmarking datasets that our algorithm achieves online\nefficiency, outperforms existing online approaches, and matches or improves the\nperformance of existing offline methods.",
    "descriptor": "\nComments: Under review for ICRA 2023\n",
    "authors": [
      "Daniel McGann",
      "John G. Rogers III",
      "Michael Kaess"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14359"
  },
  {
    "id": "arXiv:2209.14360",
    "title": "Robust Lattice-based Motion Planning",
    "abstract": "This paper proposes a robust lattice-based motion-planning algorithm for\nnonlinear systems affected by a bounded disturbance. The proposed motion\nplanner utilizes the nominal disturbance-free system model to generate motion\nprimitives, which are associated with fixed-size tubes. These tubes are\ncharacterized through designing a feedback controller, that guarantees\nboundedness of the errors occurring due to mismatch between the disturbed\nnonlinear system and the nominal system. The motion planner then sequentially\nimplements the tube-based motion primitives while solving an online\ngraph-search problem. The objective of the graph-search problem is to connect\nthe initial state to the final state, through sampled states in a suitably\ndiscretized state space, such that the tubes do not pass through any unsafe\nstates (representing obstacles) appearing during runtime. The proposed strategy\nis implemented on an Euler-Lagrange based ship model which is affected by\nsignificant wind disturbance. It is shown that the uncertain system\ntrajectories always stay within a suitably constructed tube around the nominal\ntrajectory and terminate within a region around the final state, whose size is\ndictated by the size of the tube.",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Abhishek Dhar",
      "Carl Hyn\u00e9n",
      "Johan L\u00f6fberg",
      "Daniel Axehill"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14360"
  },
  {
    "id": "arXiv:2209.14363",
    "title": "Happy or grumpy? A Machine Learning Approach to Analyze the Sentiment of  Airline Passengers' Tweets",
    "abstract": "As one of the most extensive social networking services, Twitter has more\nthan 300 million active users as of 2022. Among its many functions, Twitter is\nnow one of the go-to platforms for consumers to share their opinions about\nproducts or experiences, including flight services provided by commercial\nairlines. This study aims to measure customer satisfaction by analyzing\nsentiments of Tweets that mention airlines using a machine learning approach.\nRelevant Tweets are retrieved from Twitter's API and processed through\ntokenization and vectorization. After that, these processed vectors are passed\ninto a pre-trained machine learning classifier to predict the sentiments. In\naddition to sentiment analysis, we also perform lexical analysis on the\ncollected Tweets to model keywords' frequencies, which provide meaningful\ncontexts to facilitate the interpretation of sentiments. We then apply time\nseries methods such as Bollinger Bands to detect abnormalities in sentiment\ndata. Using historical records from January to July 2022, our approach is\nproven to be capable of capturing sudden and significant changes in passengers'\nsentiment. This study has the potential to be developed into an application\nthat can help airlines, along with several other customer-facing businesses,\nefficiently detect abrupt changes in customers' sentiments and take adequate\nmeasures to counteract them.",
    "descriptor": "\nComments: 11 pages, 7 figures. Accepted by 2023 TRBAM, under review for Transportation Research Record\n",
    "authors": [
      "Shengyang Wu",
      "Yi Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14363"
  },
  {
    "id": "arXiv:2209.14364",
    "title": "Semantic Segmentation of Vegetation in Remote Sensing Imagery Using Deep  Learning",
    "abstract": "In recent years, the geospatial industry has been developing at a steady\npace. This growth implies the addition of satellite constellations that produce\na copious supply of satellite imagery and other Remote Sensing data on a daily\nbasis. Sometimes, this information, even if in some cases we are referring to\npublicly available data, it sits unaccounted for due to the sheer size of it.\nProcessing such large amounts of data with the help of human labour or by using\ntraditional automation methods is not always a viable solution from the\nstandpoint of both time and other resources.\nWithin the present work, we propose an approach for creating a multi-modal\nand spatio-temporal dataset comprised of publicly available Remote Sensing data\nand testing for feasibility using state of the art Machine Learning (ML)\ntechniques. Precisely, the usage of Convolutional Neural Networks (CNN) models\nthat are capable of separating different classes of vegetation that are present\nin the proposed dataset. Popularity and success of similar methods in the\ncontext of Geographical Information Systems (GIS) and Computer Vision (CV) more\ngenerally indicate that methods alike should be taken in consideration and\nfurther analysed and developed.",
    "descriptor": "\nComments: Masters thesis presented in 2021 at the West University of Timisoara, faculty of Mathematics and Computer Science\n",
    "authors": [
      "Alexandru Munteanu",
      "Marian Neagul"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14364"
  },
  {
    "id": "arXiv:2209.14369",
    "title": "Social Search: retrieving information in Online Social Platforms -- A  Survey",
    "abstract": "Social Search research deals with studying methodologies exploiting social\ninformation to better satisfy user information needs in Online Social Media\nwhile simplifying the search effort and consequently reducing the time spent\nand the computational resources utilized. Starting from previous studies, in\nthis work, we analyze the current state of the art of the Social Search area,\nproposing a new taxonomy and highlighting current limitations and open research\ndirections. We divide the Social Search area into three subcategories, where\nthe social aspect plays a pivotal role: Social Question&Answering, Social\nContent Search, and Social Collaborative Search. For each subcategory, we\npresent the key concepts and selected representative approaches in the\nliterature in greater detail. We found that, up to now, a large body of studies\nmodel users' preferences and their relations by simply combining social\nfeatures made available by social platforms. It paves the way for significant\nresearch to exploit more structured information about users' social profiles\nand behaviors (as they can be inferred from data available on social platforms)\nto optimize their information needs further.",
    "descriptor": "",
    "authors": [
      "Maddalena Amendola",
      "Andrea Passarella",
      "Raffaele Perego"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.14369"
  },
  {
    "id": "arXiv:2209.14375",
    "title": "Improving alignment of dialogue agents via targeted human judgements",
    "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.",
    "descriptor": "",
    "authors": [
      "Amelia Glaese",
      "Nat McAleese",
      "Maja Tr\u0119bacz",
      "John Aslanides",
      "Vlad Firoiu",
      "Timo Ewalds",
      "Maribeth Rauh",
      "Laura Weidinger",
      "Martin Chadwick",
      "Phoebe Thacker",
      "Lucy Campbell-Gillingham",
      "Jonathan Uesato",
      "Po-Sen Huang",
      "Ramona Comanescu",
      "Fan Yang",
      "Abigail See",
      "Sumanth Dathathri",
      "Rory Greig",
      "Charlie Chen",
      "Doug Fritz",
      "Jaume Sanchez Elias",
      "Richard Green",
      "So\u0148a Mokr\u00e1",
      "Nicholas Fernando",
      "Boxi Wu",
      "Rachel Foley",
      "Susannah Young",
      "Iason Gabriel",
      "William Isaac",
      "John Mellor",
      "Demis Hassabis",
      "Koray Kavukcuoglu",
      "Lisa Anne Hendricks",
      "Geoffrey Irving"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14375"
  },
  {
    "id": "arXiv:2209.14377",
    "title": "Clinical Language Understanding Evaluation (CLUE)",
    "abstract": "Clinical language processing has received a lot of attention in recent years,\nresulting in new models or methods for disease phenotyping, mortality\nprediction, and other tasks. Unfortunately, many of these approaches are tested\nunder different experimental settings (e.g., data sources, training and testing\nsplits, metrics, evaluation criteria, etc.) making it difficult to compare\napproaches and determine state-of-the-art. To address these issues and\nfacilitate reproducibility and comparison, we present the Clinical Language\nUnderstanding Evaluation (CLUE) benchmark with a set of four clinical language\nunderstanding tasks, standard training, development, validation and testing\nsets derived from MIMIC data, as well as a software toolkit. It is our hope\nthat these data will enable direct comparison between approaches, improve\nreproducibility, and reduce the barrier-to-entry for developing novel models or\nmethods for these clinical language understanding tasks.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Travis R. Goodwin",
      "Dina Demner-Fushman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14377"
  },
  {
    "id": "arXiv:2209.14378",
    "title": "UNesT: Local Spatial Representation Learning with Hierarchical  Transformer for Efficient Medical Segmentation",
    "abstract": "Transformer-based models, capable of learning better global dependencies,\nhave recently demonstrated exceptional representation learning capabilities in\ncomputer vision and medical image analysis. Transformer reformats the image\ninto separate patches and realize global communication via the self-attention\nmechanism. However, positional information between patches is hard to preserve\nin such 1D sequences, and loss of it can lead to sub-optimal performance when\ndealing with large amounts of heterogeneous tissues of various sizes in 3D\nmedical image segmentation. Additionally, current methods are not robust and\nefficient for heavy-duty medical segmentation tasks such as predicting a large\nnumber of tissue classes or modeling globally inter-connected tissues\nstructures. Inspired by the nested hierarchical structures in vision\ntransformer, we proposed a novel 3D medical image segmentation method (UNesT),\nemploying a simplified and faster-converging transformer encoder design that\nachieves local communication among spatially adjacent patch sequences by\naggregating them hierarchically. We extensively validate our method on multiple\nchallenging datasets, consisting anatomies of 133 structures in brain, 14\norgans in abdomen, 4 hierarchical components in kidney, and inter-connected\nkidney tumors). We show that UNesT consistently achieves state-of-the-art\nperformance and evaluate its generalizability and data efficiency.\nParticularly, the model achieves whole brain segmentation task complete ROI\nwith 133 tissue classes in single network, outperforms prior state-of-the-art\nmethod SLANT27 ensembled with 27 network tiles, our model performance increases\nthe mean DSC score of the publicly available Colin and CANDI dataset from\n0.7264 to 0.7444 and from 0.6968 to 0.7025, respectively.",
    "descriptor": "\nComments: 19 pages, 17 figures. arXiv admin note: text overlap with arXiv:2203.02430\n",
    "authors": [
      "Xin Yu",
      "Qi Yang",
      "Yinchi Zhou",
      "Leon Y. Cai",
      "Riqiang Gao",
      "Ho Hin Lee",
      "Thomas Li",
      "Shunxing Bao",
      "Zhoubing Xu",
      "Thomas A. Lasko",
      "Richard G. Abramson",
      "Zizhao Zhang",
      "Yuankai Huo",
      "Bennett A. Landman",
      "Yucheng Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14378"
  },
  {
    "id": "arXiv:2209.14383",
    "title": "Assessing Coarse-to-Fine Deep Learning Models for Optic Disc and Cup  Segmentation in Fundus Images",
    "abstract": "Automated optic disc (OD) and optic cup (OC) segmentation in fundus images is\nrelevant to efficiently measure the vertical cup-to-disc ratio (vCDR), a\nbiomarker commonly used in ophthalmology to determine the degree of\nglaucomatous optic neuropathy. In general this is solved using coarse-to-fine\ndeep learning algorithms in which a first stage approximates the OD and a\nsecond one uses a crop of this area to predict OD/OC masks. While this approach\nis widely applied in the literature, there are no studies analyzing its real\ncontribution to the results. In this paper we present a comprehensive analysis\nof different coarse-to-fine designs for OD/OC segmentation using 5 public\ndatabases, both from a standard segmentation perspective and for estimating the\nvCDR for glaucoma assessment. Our analysis shows that these algorithms not\nnecessarily outperfom standard multi-class single-stage models, especially when\nthese are learned from sufficiently large and diverse training sets.\nFurthermore, we noticed that the coarse stage achieves better OD segmentation\nresults than the fine one, and that providing OD supervision to the second\nstage is essential to ensure accurate OC masks. Moreover, both the single-stage\nand two-stage models trained on a multi-dataset setting showed results in pair\nor even better than other state-of-the-art alternatives, while ranking first in\nREFUGE for OD/OC segmentation. Finally, we evaluated the models for vCDR\nprediction in comparison with six ophthalmologists on a subset of AIROGS\nimages, to understand them in the context of inter-observer variability. We\nnoticed that vCDR estimates recovered both from single-stage and coarse-to-fine\nmodels can obtain good glaucoma detection results even when they are not highly\ncorrelated with manual measurements from experts.",
    "descriptor": "",
    "authors": [
      "Eugenia Moris",
      "Nicol\u00e1s Dazeo",
      "Maria Paula Albina de Rueda",
      "Francisco Filizzola",
      "Nicol\u00e1s Iannuzzo",
      "Danila Nejamkin",
      "Kevin Wignall",
      "Mercedes Legu\u00eda",
      "Ignacio Larrabide",
      "Jos\u00e9 Ignacio Orlando"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14383"
  },
  {
    "id": "arXiv:2209.14385",
    "title": "Feature Decoupling in Self-supervised Representation Learning for Open  Set Recognition",
    "abstract": "Assuming unknown classes could be present during classification, the open set\nrecognition (OSR) task aims to classify an instance into a known class or\nreject it as unknown. In this paper, we use a two-stage training strategy for\nthe OSR problems. In the first stage, we introduce a self-supervised feature\ndecoupling method that finds the content features of the input samples from the\nknown classes. Specifically, our feature decoupling approach learns a\nrepresentation that can be split into content features and transformation\nfeatures. In the second stage, we fine-tune the content features with the class\nlabels. The fine-tuned content features are then used for the OSR problems.\nMoreover, we consider an unsupervised OSR scenario, where we cluster the\ncontent features learned from the first stage. To measure representation\nquality, we introduce intra-inter ratio (IIR). Our experimental results\nindicate that our proposed self-supervised approach outperforms others in image\nand malware OSR problems. Also, our analyses indicate that IIR is correlated\nwith OSR performance.",
    "descriptor": "",
    "authors": [
      "Jingyun Jia",
      "Philip K. Chan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14385"
  },
  {
    "id": "arXiv:2209.14389",
    "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
    "abstract": "For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream datasets, we observe that self-pretraining rivals standard\npretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Our results\nsuggest that in many scenarios, performance gains attributable to pretraining\nare driven primarily by the pretraining objective itself and are not always\nattributable to the incorporation of massive datasets. These findings are\nespecially relevant in light of concerns about intellectual property and\noffensive content in web-scale pretraining data.",
    "descriptor": "",
    "authors": [
      "Kundan Krishna",
      "Saurabh Garg",
      "Jeffrey P. Bigham",
      "Zachary C. Lipton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14389"
  },
  {
    "id": "arXiv:2209.14390",
    "title": "Neighborhood Gradient Clustering: An Efficient Decentralized Learning  Method for Non-IID Data Distributions",
    "abstract": "Decentralized learning algorithms enable the training of deep learning models\nover large distributed datasets generated at different devices and locations,\nwithout the need for a central server. In practical scenarios, the distributed\ndatasets can have significantly different data distributions across the agents.\nThe current state-of-the-art decentralized algorithms mostly assume the data\ndistributions to be Independent and Identically Distributed (IID). This paper\nfocuses on improving decentralized learning over non-IID data distributions\nwith minimal compute and memory overheads. We propose Neighborhood Gradient\nClustering (NGC), a novel decentralized learning algorithm that modifies the\nlocal gradients of each agent using self- and cross-gradient information. In\nparticular, the proposed method replaces the local gradients of the model with\nthe weighted mean of the self-gradients, model-variant cross-gradients\n(derivatives of the received neighbors' model parameters with respect to the\nlocal dataset), and data-variant cross-gradients (derivatives of the local\nmodel with respect to its neighbors' datasets). Further, we present CompNGC, a\ncompressed version of NGC that reduces the communication overhead by $32\n\\times$ by compressing the cross-gradients. We demonstrate the empirical\nconvergence and efficiency of the proposed technique over non-IID data\ndistributions sampled from the CIFAR-10 dataset on various model architectures\nand graph topologies. Our experiments demonstrate that NGC and CompNGC\noutperform the existing state-of-the-art (SoTA) decentralized learning\nalgorithm over non-IID data by $1-5\\%$ with significantly less compute and\nmemory requirements. Further, we also show that the proposed NGC method\noutperforms the baseline by $5-40\\%$ with no additional communication.",
    "descriptor": "\nComments: 15 pages, 5 figures, 7 tables\n",
    "authors": [
      "Sai Aparna Aketi",
      "Sangamesh Kodge",
      "Kaushik Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2209.14390"
  },
  {
    "id": "arXiv:2209.14395",
    "title": "The Role of Metadata in Non-Fungible Tokens: Marketplace Analysis and  Collection Organization",
    "abstract": "An explosion of interest in Non-Fungible Tokens (NFTs) has led to the\nemergence of vibrant online marketplaces that enable users to buy, sell and\ncreate digital assets. Largely considered contractual representations of\ndigital artworks, NFTs allow ownership and authenticity to be proven through\nstoring an asset and its associated metadata on a Blockchain. Yet, variation\nexists between chains, token protocols (such as the ERC-721 NFT standard) and\nmarketplaces, leading to inconsistencies in the definitions and roles of token\nmetadata. This research thus aims to define metadata in the context of NFTs,\nexplore the boundary of metadata and asset data within tokens, and understand\nthe variances and impacts these structures have on the curation of NFTs within\nonline marketplaces and collections.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Sarah Barrington"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14395"
  },
  {
    "id": "arXiv:2209.14399",
    "title": "FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge  Computing Migrations",
    "abstract": "In edge computing, users' service profiles must be migrated in response to\nuser mobility. Reinforcement learning (RL) frameworks have been proposed to do\nso. Nevertheless, these frameworks do not consider occasional server failures,\nwhich although rare, can prevent the smooth and safe functioning of edge\ncomputing users' latency sensitive applications such as autonomous driving and\nreal-time obstacle detection, because users' computing jobs can no longer be\ncompleted. As these failures occur at a low probability, it is difficult for RL\nalgorithms, which are inherently data-driven, to learn an optimal service\nmigration solution for both the typical and rare event scenarios. Therefore, we\nintroduce a rare events adaptive resilience framework FIRE, which integrates\nimportance sampling into reinforcement learning to place backup services. We\nsample rare events at a rate proportional to their contribution to the value\nfunction, to learn an optimal policy. Our framework balances service migration\ntrade-offs between delay and migration costs, with the costs of failure and the\ncosts of backup placement and migration. We propose an importance sampling\nbased Q-learning algorithm, and prove its boundedness and convergence to\noptimality. Following which we propose novel eligibility traces, linear\nfunction approximation and deep Q-learning versions of our algorithm to ensure\nit scales to real-world scenarios. We extend our framework to cater to users\nwith different risk tolerances towards failure. Finally, we use trace driven\nexperiments to show that our algorithm gives cost reductions in the event of\nfailures.",
    "descriptor": "",
    "authors": [
      "Marie Siew",
      "Shikhar Sharma",
      "Kun Guo",
      "Chao Xu",
      "Tony Q.S. Quek",
      "Carlee Joe-Wong"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14399"
  },
  {
    "id": "arXiv:2209.14401",
    "title": "Shortest Beer Path Queries in Interval Graphs",
    "abstract": "Our interest is in paths between pairs of vertices that go through at least\none of a subset of the vertices known as beer vertices. Such a path is called a\nbeer path, and the beer distance between two vertices is the length of the\nshortest beer path.\nWe show that we can represent unweighted interval graphs using $2n \\log n +\nO(n) + O(|B|\\log n)$ bits where $|B|$ is the number of beer vertices. This data\nstructure answers beer distance queries in $O(\\log^\\varepsilon n)$ time for any\nconstant $\\varepsilon > 0$ and shortest beer path queries in\n$O(\\log^\\varepsilon n + d)$ time, where $d$ is the beer distance between the\ntwo nodes. We also show that proper interval graphs may be represented using\n$3n + o(n)$ bits to support beer distance queries in $O(f(n)\\log n)$ time for\nany $f(n) \\in \\omega(1)$ and shortest beer path queries in $O(d)$ time. All of\nthese results also have time-space trade-offs.\nLastly we show that the information theoretic lower bound for beer proper\ninterval graphs is very close to the space of our structure, namely\n$\\log(4+2\\sqrt{3})n - o(n)$ (or about $ 2.9 n$) bits.",
    "descriptor": "\nComments: To appear in ISAAC 2022\n",
    "authors": [
      "Rathish Das",
      "Meng He",
      "Eitan Kondratovsky",
      "J. Ian Munro",
      "Anurag Murty Naredla",
      "Kaiyu Wu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14401"
  },
  {
    "id": "arXiv:2209.14402",
    "title": "Learning to Explain Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) are a popular class of machine learning models.\nInspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a\nframework for explainable GNNs which provides faithful explanations by design.\nL2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which\nare exclusively used in the GNNs message-passing operations. L2XGNN is able to\nselect, for each input graph, a subgraph with specific properties such as being\nsparse and connected. Imposing such constraints on the motifs often leads to\nmore interpretable and effective explanations. Experiments on several datasets\nsuggest that L2XGNN achieves the same classification accuracy as baseline\nmethods using the entire input graph while ensuring that only the provided\nexplanations are used to make predictions. Moreover, we show that L2XGNN is\nable to identify motifs responsible for the graph's properties it is intended\nto predict.",
    "descriptor": "",
    "authors": [
      "Giuseppe Serra",
      "Mathias Niepert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14402"
  },
  {
    "id": "arXiv:2209.14406",
    "title": "Biological connectomes as a representation for the architecture of  artificial neural networks",
    "abstract": "Grand efforts in neuroscience are working toward mapping the connectomes of\nmany new species, including the near completion of the Drosophila melanogaster.\nIt is important to ask whether these models could benefit artificial\nintelligence. In this work we ask two fundamental questions: (1) where and when\nbiological connectomes can provide use in machine learning, (2) which design\nprinciples are necessary for extracting a good representation of the\nconnectome. Toward this end, we translate the motor circuit of the C. Elegans\nnematode into artificial neural networks at varying levels of biophysical\nrealism and evaluate the outcome of training these networks on motor and\nnon-motor behavioral tasks. We demonstrate that biophysical realism need not be\nupheld to attain the advantages of using biological circuits. We also establish\nthat, even if the exact wiring diagram is not retained, the architectural\nstatistics provide a valuable prior. Finally, we show that while the C. Elegans\nlocomotion circuit provides a powerful inductive bias on locomotion problems,\nits structure may hinder performance on tasks unrelated to locomotion such as\nvisual classification problems.",
    "descriptor": "",
    "authors": [
      "Samuel Schmidgall",
      "Catherine Schuman",
      "Maryam Parsa"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14406"
  },
  {
    "id": "arXiv:2209.14408",
    "title": "RADACS: Towards Higher-Order Reasoning using Action Recognition in  Autonomous Vehicles",
    "abstract": "When applied to autonomous vehicle settings, action recognition can help\nenrich an environment model's understanding of the world and improve plans for\nfuture action. Towards these improvements in autonomous vehicle\ndecision-making, we propose in this work a novel two-stage online action\nrecognition system, termed RADACS. RADACS formulates the problem of active\nagent detection and adapts ideas about actor-context relations from human\nactivity recognition in a straightforward two-stage pipeline for action\ndetection and classification. We show that our proposed scheme can outperform\nthe baseline on the ICCV2021 Road Challenge dataset and by deploying it on a\nreal vehicle platform, we demonstrate how a higher-order understanding of agent\nactions in an environment can improve decisions on a real autonomous vehicle.",
    "descriptor": "",
    "authors": [
      "Alex Zhuang",
      "Eddy Zhou",
      "Quanquan Li",
      "Rowan Dempster",
      "Alikasim Budhwani",
      "Mohammad Al-Sharman",
      "Derek Rayside",
      "William Melek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14408"
  },
  {
    "id": "arXiv:2209.14409",
    "title": "Applying Machine Learning for Duplicate Detection, Throttling and  Prioritization of Equipment Commissioning Audits at Fulfillment Network",
    "abstract": "VQ (Vendor Qualification) and IOQ (Installation and Operation Qualification)\naudits are implemented in warehouses to ensure all equipment being turned over\nin the fulfillment network meets the quality standards. Audit checks are likely\nto be skipped if there are many checks to be performed in a short time. In\naddition, exploratory data analysis reveals several instances of similar checks\nbeing performed on the same assets and thus, duplicating the effort. In this\nwork, Natural Language Processing and Machine Learning are applied to trim a\nlarge checklist dataset for a network of warehouses by identifying similarities\nand duplicates, and predict the non-critical ones with a high passing rate. The\nstudy proposes ML classifiers to identify checks which have a high passing\nprobability of IOQ and VQ and assign priorities to checks to be prioritized\nwhen the time is not available to perform all checks. This research proposes\nusing NLP-based BlazingText classifier to throttle the checklists with a high\npassing rate, which can reduce 10%-37% of the checks and achieve significant\ncost reduction. The applied algorithm over performs Random Forest and Neural\nNetwork classifiers and achieves an area under the curve of 90%. Because of\nimbalanced data, down-sampling and upweighting have shown a positive impact on\nthe models' accuracy using F1 score, which improve from 8% to 75%. In addition,\nthe proposed duplicate detection process identifies 17% possible redundant\nchecks to be trimmed.",
    "descriptor": "",
    "authors": [
      "Farouq Halawa",
      "Majid Abdul",
      "Raashid Mohammed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14409"
  },
  {
    "id": "arXiv:2209.14413",
    "title": "Masked Multi-Step Multivariate Time Series Forecasting with Future  Information",
    "abstract": "In this paper, we introduce Masked Multi-Step Multivariate Forecasting\n(MMMF), a novel and general self-supervised learning framework for time series\nforecasting with known future information. In many real-world forecasting\nscenarios, some future information is known, e.g., the weather information when\nmaking a short-to-mid-term electricity demand forecast, or the oil price\nforecasts when making an airplane departure forecast. Existing machine learning\nforecasting frameworks can be categorized into (1) sample-based approaches\nwhere each forecast is made independently, and (2) time series regression\napproaches where the future information is not fully incorporated. To overcome\nthe limitations of existing approaches, we propose MMMF, a framework to train\nany neural network model capable of generating a sequence of outputs, that\ncombines both the temporal information from the past and the known information\nabout the future to make better predictions. Experiments are performed on two\nreal-world datasets for (1) mid-term electricity demand forecasting, and (2)\ntwo-month ahead flight departures forecasting. They show that the proposed MMMF\nframework outperforms not only sample-based methods but also existing time\nseries forecasting models with the exact same base models. Furthermore, once a\nneural network model is trained with MMMF, its inference speed is similar to\nthat of the same model trained with traditional regression formulations, thus\nmaking MMMF a better alternative to existing regression-trained time series\nforecasting models if there is some available future information.",
    "descriptor": "",
    "authors": [
      "Yiwei Fu",
      "Honggang Wang",
      "Nurali Virani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14413"
  },
  {
    "id": "arXiv:2209.14415",
    "title": "Improving Text-to-SQL Semantic Parsing with Fine-grained Query  Understanding",
    "abstract": "Most recent research on Text-to-SQL semantic parsing relies on either parser\nitself or simple heuristic based approach to understand natural language query\n(NLQ). When synthesizing a SQL query, there is no explicit semantic information\nof NLQ available to the parser which leads to undesirable generalization\nperformance. In addition, without lexical-level fine-grained query\nunderstanding, linking between query and database can only rely on fuzzy string\nmatch which leads to suboptimal performance in real applications. In view of\nthis, in this paper we present a general-purpose, modular neural semantic\nparsing framework that is based on token-level fine-grained query\nunderstanding. Our framework consists of three modules: named entity recognizer\n(NER), neural entity linker (NEL) and neural semantic parser (NSP). By jointly\nmodeling query and database, NER model analyzes user intents and identifies\nentities in the query. NEL model links typed entities to schema and cell values\nin database. Parser model leverages available semantic information and linking\nresults and synthesizes tree-structured SQL queries based on dynamically\ngenerated grammar. Experiments on SQUALL, a newly released semantic parsing\ndataset, show that we can achieve 56.8% execution accuracy on\nWikiTableQuestions (WTQ) test set, which outperforms the state-of-the-art model\nby 2.7%.",
    "descriptor": "\nComments: EMNLP Industry Track 2022\n",
    "authors": [
      "Jun Wang",
      "Patrick Ng",
      "Alexander Hanbo Li",
      "Jiarong Jiang",
      "Zhiguo Wang",
      "Ramesh Nallapati",
      "Bing Xiang",
      "Sudipta Sengupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14415"
  },
  {
    "id": "arXiv:2209.14417",
    "title": "Multi-Robot Coordination and Cooperation with Task Precedence  Relationships",
    "abstract": "We propose a new formulation for the multi-robot task planning and allocation\nproblem that incorporates (a) precedence relationships between tasks; (b)\ncoordination for tasks allowing multiple robots to achieve increased\nefficiency; and (c) cooperation through the formation of robot coalitions for\ntasks that cannot be performed by individual robots alone. In our formulation,\nthe tasks and the relationships between the tasks are specified by a task\ngraph. We define a set of reward functions over the task graph's nodes and\nedges. These functions model the effect of robot coalition size on the task\nperformance, and incorporate the influence of one task's performance on a\ndependent task. Solving this problem optimally is NP-hard. However, using the\ntask graph formulation allows us to leverage min-cost network flow approaches\nto obtain approximate solutions efficiently. Additionally, we explore a mixed\ninteger programming approach, which gives optimal solutions for small instances\nof the problem but is computationally expensive. We also develop a greedy\nheuristic algorithm as a baseline. Our modeling and solution approaches result\nin task plans that leverage task precedence relationships and robot\ncoordination and cooperation to achieve high mission performance, even in large\nmissions with many agents.",
    "descriptor": "\nComments: 6 pages, 7 figures. Submitted to IEEE ICRA 2023\n",
    "authors": [
      "Walker Gosrich",
      "Siddharth Mayya",
      "Saaketh Narayan",
      "Matthew Malencia",
      "Saurav Agarwal",
      "Vijay Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14417"
  },
  {
    "id": "arXiv:2209.14419",
    "title": "Category-Level Global Camera Pose Estimation with Multi-Hypothesis Point  Cloud Correspondences",
    "abstract": "Correspondence search is an essential step in rigid point cloud registration\nalgorithms. Most methods maintain a single correspondence at each step and\ngradually remove wrong correspondances. However, building one-to-one\ncorrespondence with hard assignments is extremely difficult, especially when\nmatching two point clouds with many locally similar features. This paper\nproposes an optimization method that retains all possible correspondences for\neach keypoint when matching a partial point cloud to a complete point cloud.\nThese uncertain correspondences are then gradually updated with the estimated\nrigid transformation by considering the matching cost. Moreover, we propose a\nnew point feature descriptor that measures the similarity between local point\ncloud regions. Extensive experiments show that our method outperforms the\nstate-of-the-art (SoTA) methods even when matching different objects within the\nsame category. Notably, our method outperforms the SoTA methods when\nregistering real-world noisy depth images to a template shape by up to 20%\nperformance.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Jun-Jee Chao",
      "Selim Engin",
      "Nicolai H\u00e4ni",
      "Volkan Isler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14419"
  },
  {
    "id": "arXiv:2209.14422",
    "title": "StacerBot: A Stacktrace Search Engine for Stack Overflow",
    "abstract": "We as software developers or researchers very often get stacktrace error\nmessages while we are trying to write some code or install some packages. Many\ntimes these error messages are very obscure and verbose; do not make much sense\nto us. There is a good chance that someone else has also faced similar issues\nprobably shared similar stacktrace in various online developers' forums.\nHowever traditional google searches or other search engines are not very\nhelpful to find web pages with similar stacktraces. In order to address this\nproblem, we have developed a web interface; a better search engine: as an\noutcome of this research project where users can find appropriate stack\noverflow posts by submitting the whole stacktrace error message. The current\ndeveloped solution can serve real-time parallel user queries with top-matched\nstack overflow posts within 50 seconds using a server with 300GB RAM. This\nstudy provides a comprehensive overview of the NLP techniques used in this\nstudy and an extensive overview of the research pipeline. This comprehensive\nresult, limitations, and computational overhead mentioned in this study can be\nused by future researchers and software developers to build a better solution\nfor this same problem or similar large-scale text matching-related tasks.",
    "descriptor": "",
    "authors": [
      "Md Abdullah Al Alamin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14422"
  },
  {
    "id": "arXiv:2209.14426",
    "title": "View-Invariant Localization using Semantic Objects in Changing  Environments",
    "abstract": "This paper proposes a novel framework for real-time localization and\negomotion tracking of a vehicle in a reference map. The core idea is to map the\nsemantic objects observed by the vehicle and register them to their\ncorresponding objects in the reference map. While several recent works have\nleveraged semantic information for cross-view localization, the main\ncontribution of this work is a view-invariant formulation that makes the\napproach directly applicable to any viewpoint configuration for which objects\nare detectable. Another distinctive feature is robustness to changes in the\nenvironment/objects due to a data association scheme suited for extreme outlier\nregimes (e.g., 90% association outliers). To demonstrate our framework, we\nconsider an example of localizing a ground vehicle in a reference object map\nusing only cars as objects. While only a stereo camera is used for the ground\nvehicle, we consider reference maps constructed a priori from ground viewpoints\nusing stereo cameras and Lidar scans, and georeferenced aerial images captured\nat a different date to demonstrate the framework's robustness to different\nmodalities, viewpoints, and environment changes. Evaluations on the KITTI\ndataset show that over a 3.7 km trajectory, localization occurs in 36 sec and\nis followed by real-time egomotion tracking with an average position error of\n8.5 m in a Lidar reference map, and on an aerial object map where 77% of\nobjects are outliers, localization is achieved in 71 sec with an average\nposition error of 7.9 m.",
    "descriptor": "",
    "authors": [
      "Jacqueline Ankenbauer",
      "Kaveh Fathian",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14426"
  },
  {
    "id": "arXiv:2209.14427",
    "title": "Dynamic Beam-Based Random Access Scheme for M2M Communications in  Massive MIMO Systems",
    "abstract": "Internet of things, supported by machine-to-machine (M2M) communications, is\none of the most important applications for future 6th generation (6G) systems.\nA major challenge facing by 6G is enabling a massive number of M2M devices to\naccess networks in a timely manner. Therefore, this paper exploits the spatial\nselectivity of massive multi-input multi-output (MIMO) to reduce the collision\nissue when massive M2M devices initiate random access simultaneously. In\nparticular, a beam-based random access protocol is first proposed to make\nefficient use of the limited uplink resources for massive M2M devices. To\naddress the non-uniform distribution of M2M devices in the space and time\ndimensions, an Markov decision process (MDP) problem with the objective of\nminimizing the average access delay is then formulated. Next, we present a\ndynamic beam-based access scheme based on the double deep Q network (DDQN)\nalgorithm to solve the optimal policy. Finally, simulations are conducted to\ndemonstrate the effectiveness of the proposed scheme including the model\ntraining and random access performance.",
    "descriptor": "",
    "authors": [
      "Kan Zheng",
      "Haojun Yang",
      "Xiong Xiong",
      "Jie Mei",
      "Kuan Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14427"
  },
  {
    "id": "arXiv:2209.14429",
    "title": "Efficient parameterized algorithms on graphs with heterogeneous  structure: Combining tree-depth and modular-width",
    "abstract": "Many computational problems admit fast algorithms on special inputs, however,\nthe required properties might be quite restrictive. E.g., many graph problems\ncan be solved much faster on interval or cographs, or on graphs of small\nmodular-width or small tree-width, than on general graphs. One challenge is to\nattain the greatest generality of such results, i.e., being applicable to less\nrestrictive input classes, without losing much in terms of running time.\nBuilding on the use of algebraic expressions we present a clean and robust\nway of combining such homogeneous structure into more complex heterogeneous\nstructure, and we show-case this for the combination of modular-width,\ntree-depth, and a natural notion of modular tree-depth. We give a generic\nframework for designing efficient parameterized algorithms on the created graph\nclasses, aimed at getting competitive running times that match the homogeneous\ncases. To show the applicability we give efficient parameterized algorithms for\nNegative Cycle Detection, Vertex-Weighted All-Pairs Shortest Paths, and\nTriangle Counting.",
    "descriptor": "",
    "authors": [
      "Stefan Kratsch",
      "Florian Nelles"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14429"
  },
  {
    "id": "arXiv:2209.14430",
    "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training",
    "abstract": "Learning mappings between infinite-dimensional function spaces has achieved\nempirical success in many disciplines of machine learning, including generative\nmodeling, functional data analysis, causal inference, and multi-agent\nreinforcement learning. In this paper, we study the statistical limit of\nlearning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev\nreproducing kernel Hilbert spaces. We establish the information-theoretic lower\nbound in terms of the Sobolev Hilbert-Schmidt norm and show that a\nregularization that learns the spectral components below the bias contour and\nignores the ones that are above the variance contour can achieve the optimal\nlearning rate. At the same time, the spectral components between the bias and\nvariance contours give us flexibility in designing computationally feasible\nmachine learning algorithms. Based on this observation, we develop a multilevel\nkernel operator learning algorithm that is optimal when learning linear\noperators between infinite-dimensional function spaces.",
    "descriptor": "",
    "authors": [
      "Jikai Jin",
      "Yiping Lu",
      "Jose Blanchet",
      "Lexing Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Numerical Analysis (math.NA)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14430"
  },
  {
    "id": "arXiv:2209.14431",
    "title": "Increasing the Accuracy of a Neural Network Using Frequency Selective  Mesh-to-Grid Resampling",
    "abstract": "Neural networks are widely used for almost any task of recognizing image\ncontent. Even though much effort has been put into investigating efficient\nnetwork architectures, optimizers, and training strategies, the influence of\nimage interpolation on the performance of neural networks is not well studied.\nFurthermore, research has shown that neural networks are often sensitive to\nminor changes in the input image leading to drastic drops of their performance.\nTherefore, we propose the use of keypoint agnostic frequency selective\nmesh-to-grid resampling (FSMR) for the processing of input data for neural\nnetworks in this paper. This model-based interpolation method already showed\nthat it is capable of outperforming common interpolation methods in terms of\nPSNR. Using an extensive experimental evaluation we show that depending on the\nnetwork architecture and classification task the application of FSMR during\ntraining aids the learning process. Furthermore, we show that the usage of FSMR\nin the application phase is beneficial. The classification accuracy can be\nincreased by up to 4.31 percentage points for ResNet50 and the Oxflower17\ndataset.",
    "descriptor": "\nComments: accepted for IEEE International Symposium on Circuits and Systems (ISCAS). May 2022\n",
    "authors": [
      "Andreas Spruck",
      "Viktoria Heimann",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14431"
  },
  {
    "id": "arXiv:2209.14434",
    "title": "Efficient Medical Image Assessment via Self-supervised Learning",
    "abstract": "High-performance deep learning methods typically rely on large annotated\ntraining datasets, which are difficult to obtain in many clinical applications\ndue to the high cost of medical image labeling. Existing data assessment\nmethods commonly require knowing the labels in advance, which are not feasible\nto achieve our goal of 'knowing which data to label.' To this end, we formulate\nand propose a novel and efficient data assessment strategy, EXponentiAl\nMarginal sINgular valuE (EXAMINE) score, to rank the quality of unlabeled\nmedical image data based on their useful latent representations extracted via\nSelf-supervised Learning (SSL) networks. Motivated by theoretical implication\nof SSL embedding space, we leverage a Masked Autoencoder for feature\nextraction. Furthermore, we evaluate data quality based on the marginal change\nof the largest singular value after excluding the data point in the dataset. We\nconduct extensive experiments on a pathology dataset. Our results indicate the\neffectiveness and efficiency of our proposed methods for selecting the most\nvaluable data to label.",
    "descriptor": "",
    "authors": [
      "Chun-Yin Huang",
      "Qi Lei",
      "Xiaoxiao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14434"
  },
  {
    "id": "arXiv:2209.14435",
    "title": "Out-of-Distribution Detection for LiDAR-based 3D Object Detection",
    "abstract": "3D object detection is an essential part of automated driving, and deep\nneural networks (DNNs) have achieved state-of-the-art performance for this\ntask. However, deep models are notorious for assigning high confidence scores\nto out-of-distribution (OOD) inputs, that is, inputs that are not drawn from\nthe training distribution. Detecting OOD inputs is challenging and essential\nfor the safe deployment of models. OOD detection has been studied extensively\nfor the classification task, but it has not received enough attention for the\nobject detection task, specifically LiDAR-based 3D object detection. In this\npaper, we focus on the detection of OOD inputs for LiDAR-based 3D object\ndetection. We formulate what OOD inputs mean for object detection and propose\nto adapt several OOD detection methods for object detection. We accomplish this\nby our proposed feature extraction method. To evaluate OOD detection methods,\nwe develop a simple but effective technique of generating OOD objects for a\ngiven object detection model. Our evaluation based on the KITTI dataset shows\nthat different OOD detection methods have biases toward detecting specific OOD\nobjects. It emphasizes the importance of combined OOD detection methods and\nmore research in this direction.",
    "descriptor": "\nComments: Accepted at ITSC 2022\n",
    "authors": [
      "Chengjie Huang",
      "Van Duong Nguyen",
      "Vahdat Abdelzad",
      "Christopher Gus Mannes",
      "Luke Rowe",
      "Benjamin Therien",
      "Rick Salay",
      "Krzysztof Czarnecki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14435"
  },
  {
    "id": "arXiv:2209.14436",
    "title": "Precision Landing of a UAV on a Moving Platform for Outdoor Applications",
    "abstract": "As UAV technology improves, more uses have been found for these versatile\nautonomous vehicles, from surveillance to aerial photography, to package\ndelivery, and each of these applications poses unique challenges. This paper\nimplements a solution for one such challenge: To land on a moving target. This\nproblem has been addressed before with varying degrees of success, however,\nmost implementations focus on indoor applications. Outdoor poses greater\nchallenges in the form of variables such as wind and lighting, and outdoor\ndrones are heavier and more susceptible to inertial effects. Our approach is\npurely vision based, using a monocular camera and fiducial markers to localize\nthe drone and a PID control to follow and land on the platform.",
    "descriptor": "",
    "authors": [
      "Adarsh Salagame",
      "Sushant Govindraj",
      "S. N. Omkar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14436"
  },
  {
    "id": "arXiv:2209.14439",
    "title": "Breaking Time Invariance: Assorted-Time Normalization for RNNs",
    "abstract": "Methods such as Layer Normalization (LN) and Batch Normalization (BN) have\nproven to be effective in improving the training of Recurrent Neural Networks\n(RNNs). However, existing methods normalize using only the instantaneous\ninformation at one particular time step, and the result of the normalization is\na preactivation state with a time-independent distribution. This implementation\nfails to account for certain temporal differences inherent in the inputs and\nthe architecture of RNNs. Since these networks share weights across time steps,\nit may also be desirable to account for the connections between time steps in\nthe normalization scheme. In this paper, we propose a normalization method\ncalled Assorted-Time Normalization (ATN), which preserves information from\nmultiple consecutive time steps and normalizes using them. This setup allows us\nto introduce longer time dependencies into the traditional normalization\nmethods without introducing any new trainable parameters. We present\ntheoretical derivations for the gradient propagation and prove the weight\nscaling invariance property. Our experiments applying ATN to LN demonstrate\nconsistent improvement on various tasks, such as Adding, Copying, and Denoise\nProblems and Language Modeling Problems.",
    "descriptor": "\nComments: 25 pages, 12 figures, 6 tables\n",
    "authors": [
      "Cole Pospisil",
      "Vasily Zadorozhnyy",
      "Qiang Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14439"
  },
  {
    "id": "arXiv:2209.14440",
    "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
    "abstract": "Optimal transport (OT) offers a versatile framework to compare complex data\ndistributions in a geometrically meaningful way. Traditional methods for\ncomputing the Wasserstein distance and geodesic between probability measures\nrequire mesh-dependent domain discretization and suffer from the\ncurse-of-dimensionality. We present GeONet, a mesh-invariant deep neural\noperator network that learns the non-linear mapping from the input pair of\ninitial and terminal distributions to the Wasserstein geodesic connecting the\ntwo endpoint distributions. In the offline training stage, GeONet learns the\nsaddle point optimality conditions for the dynamic formulation of the OT\nproblem in the primal and dual spaces that are characterized by a coupled PDE\nsystem. The subsequent inference stage is instantaneous and can be deployed for\nreal-time predictions in the online learning setting. We demonstrate that\nGeONet achieves comparable testing accuracy to the standard OT solvers on a\nsimulation example and the CIFAR-10 dataset with considerably reduced\ninference-stage computational cost by orders of magnitude.",
    "descriptor": "",
    "authors": [
      "Andrew Gracyk",
      "Xiaohui Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14440"
  },
  {
    "id": "arXiv:2209.14444",
    "title": "Hierarchical Integration of Model Predictive and Fuzzy Logic Control for  Combined Coverage and Target-Oriented Search-and-Rescue via Robots with  Imperfect Sensors",
    "abstract": "Search-and-rescue (SaR) in unknown environments requires precise, optimal,\nand fast decisions. Robots are promising candidates for autonomously performing\nSaR tasks in unknown environments. While humans use their heuristics to\neffectively deal with uncertainties, optimisation of multiple objectives in the\npresence of physical and control constraints is a mathematical challenge that\nrequires machine computations. Thus having both human-inspired and mathematical\ncontrol capabilities is desired for SaR robots. Moreover, coordinating the\ndecisions of robots with little computation cost in large-scale SaR missions is\nan open challenge. Finally, in real-life data perceived by SaR robots may be\nprone to uncertainties. We introduce a hierarchical multi-agent control\narchitecture that exploits non-homogeneous and imperfect perception\ncapabilities of SaR robots, as well as the computational efficiency and\nrobustness to failure of decentralised control methods and global performance\nimprovement of centralised control methods. The integrated structure of the\nproposed control framework allows to combine human-inspired and mathematical\ndecision making methods in a coordinated and computationally efficient way. The\nresults of various computer-based simulations show that while the area coverage\nof the proposed approach is comparable to existing heuristic methods that are\nparticularly developed for coverage-oriented SaR, the efficiency of the\nintroduced approach in locating the trapped victims is significantly higher.\nFurthermore, with comparable computation times, the proposed control approach\nsuccessfully avoids potential conflicts that exist in non-cooperative methods.\nThese results confirm that the proposed multi-agent control system is capable\nof combining coverage-oriented and target-oriented SaR in a balanced and\ncoordinated way.",
    "descriptor": "\nComments: 23 pages, 22 figures\n",
    "authors": [
      "Christopher de Koning",
      "Anahita Jamshidnejad"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14444"
  },
  {
    "id": "arXiv:2209.14447",
    "title": "Visual Detection of Diver Attentiveness for Underwater Human-Robot  Interaction",
    "abstract": "Many underwater tasks, such as cable-and-wreckage inspection,\nsearch-and-rescue, benefit from robust human-robot interaction (HRI)\ncapabilities. With the recent advancements in vision-based underwater HRI\nmethods, autonomous underwater vehicles (AUVs) can communicate with their human\npartners even during a mission. However, these interactions usually require\nactive participation especially from humans (e.g., one must keep looking at the\nrobot during an interaction). Therefore, an AUV must know when to start\ninteracting with a human partner, i.e., if the human is paying attention to the\nAUV or not. In this paper, we present a diver attention estimation framework\nfor AUVs to autonomously detect the attentiveness of a diver and then navigate\nand reorient itself, if required, with respect to the diver to initiate an\ninteraction. The core element of the framework is a deep neural network (called\nDATT-Net) which exploits the geometric relation among 10 facial keypoints of\nthe divers to determine their head orientation. Our on-the-bench experimental\nevaluations (using unseen data) demonstrate that the proposed DATT-Net\narchitecture can determine the attentiveness of human divers with promising\naccuracy. Our real-world experiments also confirm the efficacy of DATT-Net\nwhich enables real-time inference and allows the AUV to position itself for an\nAUV-diver interaction.",
    "descriptor": "\nComments: 7 pages, 6 figures, 2 tables\n",
    "authors": [
      "Sadman Sakib Enan",
      "Junaed Sattar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14447"
  },
  {
    "id": "arXiv:2209.14448",
    "title": "Synthesizing Annotated Image and Video Data Using a Rendering-Based  Pipeline for Improved License Plate Recognition",
    "abstract": "An insufficient number of training samples is a common problem in neural\nnetwork applications. While data augmentation methods require at least a\nminimum number of samples, we propose a novel, rendering-based pipeline for\nsynthesizing annotated data sets. Our method does not modify existing samples\nbut synthesizes entirely new samples. The proposed rendering-based pipeline is\ncapable of generating and annotating synthetic and partly-real image and video\ndata in a fully automatic procedure. Moreover, the pipeline can aid the\nacquisition of real data. The proposed pipeline is based on a rendering\nprocess. This process generates synthetic data. Partly-real data bring the\nsynthetic sequences closer to reality by incorporating real cameras during the\nacquisition process. The benefits of the proposed data generation pipeline,\nespecially for machine learning scenarios with limited available training data,\nare demonstrated by an extensive experimental validation in the context of\nautomatic license plate recognition. The experiments demonstrate a significant\nreduction of the character error rate and miss rate from 73.74% and 100% to\n14.11% and 41.27% respectively, compared to an OCR algorithm trained on a real\ndata set solely. These improvements are achieved by training the algorithm on\nsynthesized data solely. When additionally incorporating real data, the error\nrates can be decreased further. Thereby, the character error rate and miss rate\ncan be reduced to 11.90% and 39.88% respectively. All data used during the\nexperiments as well as the proposed rendering-based pipeline for the automated\ndata generation is made publicly available under (URL will be revealed upon\npublication).",
    "descriptor": "\nComments: submitted to IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Andreas Spruck",
      "Maximilane Gruber",
      "Anatol Maier",
      "Denise Moussa",
      "J\u00fcrgen Seiler",
      "Christian Riess",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14448"
  },
  {
    "id": "arXiv:2209.14450",
    "title": "Mathematical Models of Theory of Mind",
    "abstract": "Socially assistive robots provide physical and mental assistance for humans\nvia cognitive human-machine interactions. These robots should sustain long-term\nengaging interactions with humans in a similar way humans interact with each\nother. According to the theory of mind, in their interactions humans develop\ncognitive models of each other in order to estimate their unobservable\nstate-of-mind, predict their behavior, and act accordingly. Based on the theory\nof mind, we propose mathematical cognitive models of humans, which enable\nmachines to understand cognitive procedures of humans in general and as\ndistinct individuals. In particular, a network representation that is\nformulated based on a proposed extended version of fuzzy cognitive maps is\nintroduced. The resulting models are identified and validated using (1)\ncomputer-based simulations designed according to a general data set of human's\nintuitive reasoning and literature and (2) real-life personalised experiments\nwith 15 human participants. The results of the experiments show that the\nproposed cognitive models can excellently be personalised to different\nparticipants and precisely estimate and predict their current and future\nstate-of-mind and expected behaviors.",
    "descriptor": "",
    "authors": [
      "Maria Mor\u00e3o Patr\u00edcio",
      "Anahita Jamshidnejad"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14450"
  },
  {
    "id": "arXiv:2209.14454",
    "title": "CompNet: A Designated Model to Handle Combinations of Images and  Designed features",
    "abstract": "Convolutional neural networks (CNNs) are one of the most popular models of\nArtificial Neural Networks (ANN)s in Computer Vision (CV). A variety of\nCNN-based structures were developed by researchers to solve problems like image\nclassification, object detection, and image similarity measurement. Although\nCNNs have shown their value in most cases, they still have a downside: they\neasily overfit when there are not enough samples in the dataset. Most medical\nimage datasets are examples of such a dataset. Additionally, many datasets also\ncontain both designed features and images, but CNNs can only deal with images\ndirectly. This represents a missed opportunity to leverage additional\ninformation. For this reason, we propose a new structure of CNN-based model:\nCompNet, a composite convolutional neural network. This is a specially designed\nneural network that accepts combinations of images and designed features as\ninput in order to leverage all available information. The novelty of this\nstructure is that it uses learned features from images to weight designed\nfeatures in order to gain all information from both images and designed\nfeatures. With the use of this structure on classification tasks, the results\nindicate that our approach has the capability to significantly reduce\noverfitting. Furthermore, we also found several similar approaches proposed by\nother researchers that can combine images and designed features. To make\ncomparison, we first applied those similar approaches on LIDC and compared the\nresults with the CompNet results, then we applied our CompNet on the datasets\nthat those similar approaches originally used in their works and compared the\nresults with the results they proposed in their papers. All these comparison\nresults showed that our model outperformed those similar approaches on\nclassification tasks either on LIDC dataset or on their proposed datasets.",
    "descriptor": "",
    "authors": [
      "Bowen Qiu",
      "Daniela Raicu",
      "Jacob Furst",
      "Roselyne Tchoua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14454"
  },
  {
    "id": "arXiv:2209.14456",
    "title": "Machine Learning for Optical Motion Capture-driven Musculoskeletal  Modeling from Inertial Motion Capture Data",
    "abstract": "Marker-based Optical Motion Capture (OMC) systems and the associated\nmusculoskeletal (MSK) modeling predictions have offered the ability to gain\ninsights into in vivo joint and muscle loading non-invasively as well as aid\nclinical decision-making. However, an OMC system is lab-based, expensive, and\nrequires a line of sight. A widely used alternative is the Inertial Motion\nCapture (IMC) system, which is portable, user-friendly, and relatively low\ncost, although it is not as accurate as an OMC system. Irrespective of the\nchoice of motion capture technique, one needs to use an MSK model to obtain the\nkinematic and kinetic outputs, which is a computationally expensive tool\nincreasingly well approximated by machine learning (ML) methods. Here, we\npresent an ML approach to map IMC data to the human upper-extremity MSK outputs\ncomputed from OMC input data. Essentially, we attempt to predict high-quality\nMSK outputs from the relatively easier-to-obtain IMC data. We use OMC and IMC\ndata simultaneously collected for the same subjects to train an ML\n(feed-forward multi-layer perceptron) model that predicts OMC-based MSK outputs\nfrom IMC measurements. We demonstrate that our ML predictions have a high\ndegree of agreement with the desired OMC-based MSK estimates. Thus, this\napproach will be instrumental in getting the technology from 'lab to field'\nwhere OMC-based systems are infeasible.",
    "descriptor": "\nComments: 21 pages, 8 figures, 4 tables\n",
    "authors": [
      "Abhishek Dasgupta",
      "Rahul Sharma",
      "Challenger Mishra",
      "Vikranth H. Nagaraja"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14456"
  },
  {
    "id": "arXiv:2209.14457",
    "title": "Consensus-Free Spreadsheet Integration",
    "abstract": "We describe a method for merging multiple spreadsheets into one sheet, and/or\nexchanging data among the sheets, by expressing each sheet's formulae as an\nalgebraic (equational) theory and each sheet's values as a model of its theory,\nexpressing the overlap between the sheets as theory and model morphisms, and\nthen performing colimit, lifting, and Kan-extension constructions from category\ntheory to compute a canonically universal integrated theory and model, which\ncan then be expressed as a spreadsheet. Our motivation is to find methods of\nmerging engineering models that do not require consensus (agreement) among the\nauthors of the models being merged, a condition fulfilled by our method because\ntheory and model morphisms are semantics-preserving. We describe a case study\nof this methodology on a real-world oil and gas calculation at a major energy\ncompany, describing the theories and models that arise when integrating two\ndifferent casing pressure test (MASP) calculation spreadsheets constructed by\ntwo non-interacting engineers. We also describe the automated theorem proving\nburden associated with both verifying the semantics preservation of the overlap\nmappings as well as verifying the conservativity/consistency of the resulting\nintegrated sheet. We conclude with thoughts on how to apply the methodology to\nscale engineering efforts across the enterprise.",
    "descriptor": "",
    "authors": [
      "Brandon Baylor",
      "Eric Daimler",
      "James Hansen",
      "Esteban Montero",
      "Ryan Wisnesky"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14457"
  },
  {
    "id": "arXiv:2209.14458",
    "title": "The Chamber Ensemble Generator: Limitless High-Quality MIR Data via  Generative Modeling",
    "abstract": "Data is the lifeblood of modern machine learning systems, including for those\nin Music Information Retrieval (MIR). However, MIR has long been mired by small\ndatasets and unreliable labels. In this work, we propose to break this\nbottleneck using generative modeling. By pipelining a generative model of notes\n(Coconet trained on Bach Chorales) with a structured synthesis model of chamber\nensembles (MIDI-DDSP trained on URMP), we demonstrate a system capable of\nproducing unlimited amounts of realistic chorale music with rich annotations\nincluding mixes, stems, MIDI, note-level performance attributes (staccato,\nvibrato, etc.), and even fine-grained synthesis parameters (pitch, amplitude,\netc.). We call this system the Chamber Ensemble Generator (CEG), and use it to\ngenerate a large dataset of chorales from four different chamber ensembles\n(CocoChorales). We demonstrate that data generated using our approach improves\nstate-of-the-art models for music transcription and source separation, and we\nrelease both the system and the dataset as an open-source foundation for future\nwork in the MIR community.",
    "descriptor": "",
    "authors": [
      "Yusong Wu",
      "Josh Gardner",
      "Ethan Manilow",
      "Ian Simon",
      "Curtis Hawthorne",
      "Jesse Engel"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.14458"
  },
  {
    "id": "arXiv:2209.14461",
    "title": "Constrained Dynamic Movement Primitives for Safe Learning of Motor  Skills",
    "abstract": "Dynamic movement primitives are widely used for learning skills which can be\ndemonstrated to a robot by a skilled human or controller. While their\ngeneralization capabilities and simple formulation make them very appealing to\nuse, they possess no strong guarantees to satisfy operational safety\nconstraints for a task. In this paper, we present constrained dynamic movement\nprimitives (CDMP) which can allow for constraint satisfaction in the robot\nworkspace. We present a formulation of a non-linear optimization to perturb the\nDMP forcing weights regressed by locally-weighted regression to admit a Zeroing\nBarrier Function (ZBF), which certifies workspace constraint satisfaction. We\ndemonstrate the proposed CDMP under different constraints on the end-effector\nmovement such as obstacle avoidance and workspace constraints on a physical\nrobot. A video showing the implementation of the proposed algorithm using\ndifferent manipulators in different environments could be found here\nhttps://youtu.be/hJegJJkJfys.",
    "descriptor": "",
    "authors": [
      "Seiji Shaw",
      "Devesh K. Jha",
      "Arvind Raghunathan",
      "Radu Corcodel",
      "Diego Romeres",
      "George Konidaris",
      "Daniel Nikovski"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14461"
  },
  {
    "id": "arXiv:2209.14462",
    "title": "What Can Cryptography Do For Decentralized Mechanism Design",
    "abstract": "Recent works of Roughgarden (EC'21) and Chung and Shi (Highlights Beyond\nEC'22) initiate the study of a new decentralized mechanism design problem\ncalled transaction fee mechanism design (TFM). Unlike the classical mechanism\ndesign literature, in the decentralized environment, even the auctioneer (i.e.,\nthe miner) can be a strategic player, and it can even collude with a subset of\nthe users facilitated by binding side contracts. Chung and Shi showed two main\nimpossibility results that rule out the existence of a {\\it dream} TFM. First,\nany TFM that provides incentive compatibility for individual users and\nminer-user coalitions must always have zero miner revenue, no matter whether\nthe block size is finite or infinite. Second, assuming finite block size, no\nnon-trivial TFM can simultaenously provide incentive compatibility for any\nindividual user, and for any miner-user coalition.\nIn this work, we explore what new models and meaningful relaxations can allow\nus to circumvent the impossibility results of Chung and Shi. Besides today's\nmodel that does not employ cryptography, we introduce a new MPC-assisted model\nwhere the TFM is implemented by a joint multi-party computation (MPC) protocol\namong the miners. We prove several feasibility and infeasibility results for\nachieving {\\it strict} and {\\it approximate} incentive compatibility,\nrespectively, in the plain model as well as the MPC-assisted model. We show\nthat while cryptography is not a panacea, it indeed allows us to overcome some\nimpossibility results pertaining to the plain model, leading to non-trivial\nmechanisms with useful guarantees that are otherwise impossible in the plain\nmodel. Our work is also the first to characterize the mathematical landscape of\ntransaction fee mechanism design under approximate incentive compatibility, as\nwell as in a cryptography-assisted model.",
    "descriptor": "",
    "authors": [
      "Elaine Shi",
      "Hao Chung",
      "Ke Wu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14462"
  },
  {
    "id": "arXiv:2209.14464",
    "title": "Neural Methods for Logical Reasoning Over Knowledge Graphs",
    "abstract": "Reasoning is a fundamental problem for computers and deeply studied in\nArtificial Intelligence. In this paper, we specifically focus on answering\nmulti-hop logical queries on Knowledge Graphs (KGs). This is a complicated task\nbecause, in real-world scenarios, the graphs tend to be large and incomplete.\nMost previous works have been unable to create models that accept full\nFirst-Order Logical (FOL) queries, which include negative queries, and have\nonly been able to process a limited set of query structures. Additionally, most\nmethods present logic operators that can only perform the logical operation\nthey are made for. We introduce a set of models that use Neural Networks to\ncreate one-point vector embeddings to answer the queries. The versatility of\nneural networks allows the framework to handle FOL queries with Conjunction\n($\\wedge$), Disjunction ($\\vee$) and Negation ($\\neg$) operators. We\ndemonstrate experimentally the performance of our model through extensive\nexperimentation on well-known benchmarking datasets. Besides having more\nversatile operators, the models achieve a 10\\% relative increase over the best\nperforming state of the art and more than 30\\% over the original method based\non single-point vector embeddings.",
    "descriptor": "\nComments: 14 pages, 5 figures, 11 tables\n",
    "authors": [
      "Alfonso Amayuelas",
      "Shuai Zhang",
      "Susie Xi Rao",
      "Ce Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14464"
  },
  {
    "id": "arXiv:2209.14465",
    "title": "Practical Challenges in Landing a UAV on a Dynamic Target",
    "abstract": "Unmanned Aerial Vehicles grow more popular by the day and applications for\nthem are crossing boundaries of science and industry, with everything from\naerial photography to package delivery to disaster management benefiting from\nthe technology. But before they become commonplace, there are challenges to be\nsolved to make them reliable and safe.\nThe following paper discusses the challenges associated with the precision\nlanding of an Unmanned Aerial Vehicle, including methods for sensing and\ncontrol and their merits and shortcomings for various applications.",
    "descriptor": "",
    "authors": [
      "Adarsh Salagame",
      "Sushant Govindraj",
      "S. N. Omkar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14465"
  },
  {
    "id": "arXiv:2209.14468",
    "title": "Auditing for Core Stability in Participatory Budgeting",
    "abstract": "We consider the participatory budgeting problem where each of $n$ voters\nspecifies additive utilities over $m$ candidate projects with given sizes, and\nthe goal is to choose a subset of projects (i.e., a committee) with total size\nat most $k$. Participatory budgeting mathematically generalizes multiwinner\nelections, and both have received great attention in computational social\nchoice recently. A well-studied notion of group fairness in this setting is\ncore stability: Each voter is assigned an \"entitlement\" of $\\frac{k}{n}$, so\nthat a subset $S$ of voters can pay for a committee of size at most $|S| \\cdot\n\\frac{k}{n}$. A given committee is in the core if no subset of voters can pay\nfor another committee that provides each of them strictly larger utility. This\nprovides proportional representation to all voters in a strong sense.\nIn this paper, we study the following auditing question: Given a committee\ncomputed by some preference aggregation method, how close is it to the core?\nConcretely, how much does the entitlement of each voter need to be scaled down\nby, so that the core property subsequently holds? As our main contribution, we\npresent computational hardness results for this problem, as well as a\nlogarithmic approximation algorithm via linear program rounding. We show that\nour analysis is tight against the linear programming bound. Additionally, we\nconsider two related notions of group fairness that have similar audit\nproperties. The first is Lindahl priceability, which audits the closeness of a\ncommittee to a market clearing solution. We show that this is related to the\nlinear programming relaxation of auditing the core, leading to efficient exact\nand approximation algorithms for auditing. The second is a novel weakening of\nthe core that we term the sub-core, and we present computational results for\nauditing this notion as well.",
    "descriptor": "\nComments: accepted by the 18th Conference on Web and Internet Economics (WINE 2022)\n",
    "authors": [
      "Kamesh Munagala",
      "Yiheng Shen",
      "Kangning Wang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.14468"
  },
  {
    "id": "arXiv:2209.14471",
    "title": "Lazy Probabilistic Roadmaps Revisited",
    "abstract": "This paper describes a revision of the classic Lazy Probabilistic Roadmaps\nalgorithm (Lazy PRM), that results from pairing PRM and a novel Branch-and-Cut\n(BC) algorithm. Cuts are dynamically generated constraints that are imposed on\nminimum cost paths over the geometric graphs selected by PRM. Cuts eliminate\npaths that cannot be mapped into smooth plans that satisfy suitably defined\nkinematic constraints. We generate candidate smooth plans by fitting splines to\nvertices in minimum-cost path. Plans are validated with a recently proposed\nalgorithm that maps them into finite traces, without need to choose a fixed\ndiscretization step. Trace elements exactly describe when plans cross\nconstraint boundaries modulo arithmetic precision. We evaluate several planners\nusing our methods over the recently proposed BARN benchmark, and we report\nevidence of the scalability of our approach.",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Miquel Ramirez",
      "Daniel Selvaratnam",
      "Chris Manzie"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14471"
  },
  {
    "id": "arXiv:2209.14474",
    "title": "A slight generalization of Steffensen Method for Solving Non Linear  Equations",
    "abstract": "In this article, we present an iterative method to find simple roots of\nnonlinear equations, that is, to solving an equation of the form $f(x) = 0$.\nDifferent from Newton's method, the method we purpose do not require evaluation\nof derivatives. The method is based on the classical Steffensen's method and it\nis a slight modification of it. The proofs of theoretical results are stated\nusing Landau's Little o notation and simples concepts of Real Analysis. We\nprove that the method converges and its rate of convergence is quadratic. The\nmethod present some advantages when compared with Newton's and Steffesen's\nmethods as ilustrated by numerical tests given.",
    "descriptor": "",
    "authors": [
      "Eder Marinho Martins",
      "Geraldo Cesar Gon\u00e7alves Ferreira",
      "Thais Ester Gon\u00e7alves"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14474"
  },
  {
    "id": "arXiv:2209.14475",
    "title": "Intrinsic Dimensionality Estimation within Tight Localities: A  Theoretical and Experimental Analysis",
    "abstract": "Accurate estimation of Intrinsic Dimensionality (ID) is of crucial importance\nin many data mining and machine learning tasks, including dimensionality\nreduction, outlier detection, similarity search and subspace clustering.\nHowever, since their convergence generally requires sample sizes (that is,\nneighborhood sizes) on the order of hundreds of points, existing ID estimation\nmethods may have only limited usefulness for applications in which the data\nconsists of many natural groups of small size. In this paper, we propose a\nlocal ID estimation strategy stable even for `tight' localities consisting of\nas few as 20 sample points. The estimator applies MLE techniques over all\navailable pairwise distances among the members of the sample, based on a recent\nextreme-value-theoretic model of intrinsic dimensionality, the Local Intrinsic\nDimension (LID). Our experimental results show that our proposed estimation\ntechnique can achieve notably smaller variance, while maintaining comparable\nlevels of bias, at much smaller sample sizes than state-of-the-art estimators.",
    "descriptor": "\nComments: 21 pages, 16 figures, 3 tables\n",
    "authors": [
      "Laurent Amsaleg",
      "Oussama Chelly",
      "Michael E. Houle",
      "Ken-ichi Kawarabayashi",
      "Milo\u0161 Radovanovi\u0107",
      "Weeris Treeratanajaru"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.14475"
  },
  {
    "id": "arXiv:2209.14479",
    "title": "Semantics-Guided Object Removal for Facial Images: with Broad  Applicability and Robust Style Preservation",
    "abstract": "Object removal and image inpainting in facial images is a task in which\nobjects that occlude a facial image are specifically targeted, removed, and\nreplaced by a properly reconstructed facial image. Two different approaches\nutilizing U-net and modulated generator respectively have been widely endorsed\nfor this task for their unique advantages but notwithstanding each method's\ninnate disadvantages. U-net, a conventional approach for conditional GANs,\nretains fine details of unmasked regions but the style of the reconstructed\nimage is inconsistent with the rest of the original image and only works\nrobustly when the size of the occluding object is small enough. In contrast,\nthe modulated generative approach can deal with a larger occluded area in an\nimage and provides {a} more consistent style, yet it usually misses out on most\nof the detailed features. This trade-off between these two models necessitates\nan invention of a model that can be applied to any size of mask while\nmaintaining a consistent style and preserving minute details of facial\nfeatures. Here, we propose Semantics-Guided Inpainting Network (SGIN) which\nitself is a modification of the modulated generator, aiming to take advantage\nof its advanced generative capability and preserve the high-fidelity details of\nthe original image. By using the guidance of a semantic map, our model is\ncapable of manipulating facial features which grants direction to the\none-to-many problem for further practicability.",
    "descriptor": "\nComments: 9 pages, 9 figures\n",
    "authors": [
      "Jookyung Song",
      "Yeonjin Chang",
      "Seonguk Park",
      "Nojun Kwak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14479"
  },
  {
    "id": "arXiv:2209.14488",
    "title": "Hierarchical Training of Deep Ensemble Policies for Reinforcement  Learning in Continuous Spaces",
    "abstract": "Many actor-critic deep reinforcement learning (DRL) algorithms have achieved\ncutting-edge performance in tackling various challenging reinforcement learning\n(RL) problems, including complex control tasks with high-dimensional continuous\nstate and action spaces. Despite of widely reported success, existing DRL\nalgorithms often suffer from the ineffective exploration issue, resulting in\nlimited learning stability and performance. To address this limitation, several\nensemble DRL algorithms have been proposed recently to boost exploration and\nstabilize the learning process. However, many existing ensemble algorithms are\ndesigned to train each base learner individually without controlling explicitly\nthe collaboration among the trained base learners. In this paper, we propose a\nnew technique to train an ensemble of base learners based on the multi-step\nintegration methods. The new multi-step training technique enables us to\ndevelop a new hierarchical training algorithm for ensemble DRL that promotes\ninter-learner collaboration through explicit inter-learner parameter sharing.\nThe design of our new algorithm is verified theoretically. The algorithm is\nalso shown empirically to outperform several cutting-edge DRL algorithms on\nmultiple benchmark RL problems.",
    "descriptor": "",
    "authors": [
      "Gang Chen",
      "Victoria Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14488"
  },
  {
    "id": "arXiv:2209.14491",
    "title": "Re-Imagen: Retrieval-Augmented Text-to-Image Generator",
    "abstract": "Research on text-to-image generation has witnessed significant progress in\ngenerating diverse and photo-realistic images, driven by diffusion and\nauto-regressive models trained on large-scale image-text data. Though\nstate-of-the-art models can generate high-quality images of common entities,\nthey often have difficulty generating images of uncommon entities, such as\n`Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the\nRetrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model\nthat uses retrieved information to produce high-fidelity and faithful images,\neven for rare or unseen entities. Given a text prompt, Re-Imagen accesses an\nexternal multi-modal knowledge base to retrieve relevant (image, text) pairs,\nand uses them as references to generate the image. With this retrieval step,\nRe-Imagen is augmented with the knowledge of high-level semantics and low-level\nvisual details of the mentioned entities, and thus improves its accuracy in\ngenerating the entities' visual appearances. We train Re-Imagen on a\nconstructed dataset containing (image, text, retrieval) triples to teach the\nmodel to ground on both text prompt and retrieval. Furthermore, we develop a\nnew sampling strategy to interleave the classifier-free guidance for text and\nretrieval condition to balance the text and retrieval alignment. Re-Imagen\nachieves new SoTA FID results on two image generation benchmarks, such as COCO\n(ie, FID = 5.25) and WikiImage (ie, FID = 5.82) without fine-tuning. To further\nevaluate the capabilities of the model, we introduce EntityDrawBench, a new\nbenchmark that evaluates image generation for diverse entities, from frequent\nto rare, across multiple visual domains. Human evaluation on EntityDrawBench\nshows that Re-Imagen performs on par with the best prior models in\nphoto-realism, but with significantly better faithfulness, especially on less\nfrequent entities.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Wenhu Chen",
      "Hexiang Hu",
      "Chitwan Saharia",
      "William W. Cohen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14491"
  },
  {
    "id": "arXiv:2209.14494",
    "title": "Multi-stage Information Retrieval for Vietnamese Legal Texts",
    "abstract": "This study deals with the problem of information retrieval (IR) for\nVietnamese legal texts. Despite being well researched in many languages,\ninformation retrieval has still not received much attention from the Vietnamese\nresearch community. This is especially true for the case of legal documents,\nwhich are hard to process. This study proposes a new approach for information\nretrieval for Vietnamese legal documents using sentence-transformer. Besides,\nvarious experiments are conducted to make comparisons between different\ntransformer models, ranking scores, syllable-level, and word-level training.\nThe experiment results show that the proposed model outperforms models used in\ncurrent research on information retrieval for Vietnamese documents.",
    "descriptor": "\nComments: PKAW 2022\n",
    "authors": [
      "Nhat-Minh Pham",
      "Ha-Thanh Nguyen",
      "Trong-Hop Do"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14494"
  },
  {
    "id": "arXiv:2209.14498",
    "title": "Teaching Where to Look: Attention Similarity Knowledge Distillation for  Low Resolution Face Recognition",
    "abstract": "Deep learning has achieved outstanding performance for face recognition\nbenchmarks, but performance reduces significantly for low resolution (LR)\nimages. We propose an attention similarity knowledge distillation approach,\nwhich transfers attention maps obtained from a high resolution (HR) network as\na teacher into an LR network as a student to boost LR recognition performance.\nInspired by humans being able to approximate an object's region from an LR\nimage based on prior knowledge obtained from HR images, we designed the\nknowledge distillation loss using the cosine similarity to make the student\nnetwork's attention resemble the teacher network's attention. Experiments on\nvarious LR face related benchmarks confirmed the proposed method generally\nimproved recognition performances on LR settings, outperforming\nstate-of-the-art results by simply transferring well-constructed attention\nmaps. The code and pretrained models are publicly available in the\nhttps://github.com/gist-ailab/teaching-where-to-look.",
    "descriptor": "\nComments: ECCV 2022 accepted\n",
    "authors": [
      "Sungho Shin",
      "Joosoon Lee",
      "Junseok Lee",
      "Yeonguk Yu",
      "Kyoobin Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14498"
  },
  {
    "id": "arXiv:2209.14499",
    "title": "NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for  Autonomous Driving",
    "abstract": "Detecting obstacles is crucial for safe and efficient autonomous driving. To\nthis end, we present NVRadarNet, a deep neural network (DNN) that detects\ndynamic obstacles and drivable free space using automotive RADAR sensors. The\nnetwork utilizes temporally accumulated data from multiple RADAR sensors to\ndetect dynamic obstacles and compute their orientation in a top-down bird's-eye\nview (BEV). The network also regresses drivable free space to detect\nunclassified obstacles. Our DNN is the first of its kind to utilize sparse\nRADAR signals in order to perform obstacle and free space detection in real\ntime from RADAR data only. The network has been successfully used for\nperception on our autonomous vehicles in real self-driving scenarios. The\nnetwork runs faster than real time on an embedded GPU and shows good\ngeneralization across geographic regions.",
    "descriptor": "\nComments: 7 pages, 6 figures, submitted to ICRA 2023 conference, for associated mpeg file, see this https URL\n",
    "authors": [
      "Alexander Popov",
      "Patrik Gebhardt",
      "Ke Chen",
      "Ryan Oldja",
      "Heeseok Lee",
      "Shane Murray",
      "Ruchi Bhargava",
      "Nikolai Smolyanskiy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14499"
  },
  {
    "id": "arXiv:2209.14500",
    "title": "Bidirectional Language Models Are Also Few-shot Learners",
    "abstract": "Large language models such as GPT-3 (Brown et al., 2020) can perform\narbitrary tasks without undergoing fine-tuning after being prompted with only a\nfew labeled examples. An arbitrary task can be reformulated as a natural\nlanguage prompt, and a language model can be asked to generate the completion,\nindirectly performing the task in a paradigm known as prompt-based learning. To\ndate, emergent prompt-based learning capabilities have mainly been demonstrated\nfor unidirectional language models. However, bidirectional language models\npre-trained on denoising objectives such as masked language modeling produce\nstronger learned representations for transfer learning. This motivates the\npossibility of prompting bidirectional models, but their pre-training\nobjectives have made them largely incompatible with the existing prompting\nparadigm. We present SAP (Sequential Autoregressive Prompting), a technique\nthat enables the prompting of bidirectional models. Utilizing the machine\ntranslation task as a case study, we prompt the bidirectional mT5 model (Xue et\nal., 2021) with SAP and demonstrate its few-shot and zero-shot translations\noutperform the few-shot translations of unidirectional models like GPT-3 and\nXGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We\nfurther show SAP is effective on question answering and summarization. For the\nfirst time, our results demonstrate prompt-based learning is an emergent\nproperty of a broader class of language models, rather than only unidirectional\nmodels.",
    "descriptor": "",
    "authors": [
      "Ajay Patel",
      "Bryan Li",
      "Mohammad Sadegh Rasooli",
      "Noah Constant",
      "Colin Raffel",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14500"
  },
  {
    "id": "arXiv:2209.14503",
    "title": "Tourism's trend Ranking on Social Media Data Using Fuzzy-AHP vs. AHP",
    "abstract": "Tourism is an exciting thing to be visited by people in the world. Search for\nattractive and popular places can be done through social media. Data from\nsocial media or websites can be used as a reference to find current travel\ntrends and get information about reviews, stories, likes, forums, blogs, and\nfeedback from a place. However, if the search is done manually one by one, it\ntakes a long time, and it becomes interesting to do research. So, searching\nbased on current trends will be easier and faster. For this reason, this study\nuses a computer base to search by ranking tourist facilities from social media\ndata or websites using the multi-criteria decision-making (MCDM) method. The\nimplementation of the method used in finding the trend is the Fuzzy-AHP method\nin comparison with the AHP. The data used is data reviews, stories, likes,\nforums, blogs, and feedback from the web or social media. Because with these\ncomponents, tourism can be developed according to visitors\\'' wishes. The\nresearch aims to rank facilities\\'' tourism attractions (trends) and\ndevelopment priorities. The priority and ranking used the fuzzy-AHP and AHP\nmethod to determine weight criteria and the ranking process. The highest\nranking is on the Parks/Picnic Spots attraction, and make it a priority to\ndevelop. The methods have an average value MSE of all data is \\approx 0.0002,\nwhich can be used for this ranking.",
    "descriptor": "\nComments: 7 pages, 5 Tables, 3 figures\n",
    "authors": [
      "Shoffan Saifullah"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.14503"
  },
  {
    "id": "arXiv:2209.14504",
    "title": "Decentralized Coordinated Precoding Design in Cell-Free Massive MIMO  Systems for URLLC",
    "abstract": "Cell-free massive multiple-input multiple-output (MIMO) is a promising\nnetwork to offer huge improvement of the achievable rate compared with\nconventional cellular massive MIMO systems. However, the commonly adopted\nShannon-type achievable rate is only valid in the long block length regime that\nis not applicable to the emerging short-packet communication. To realize\nultra-reliable and low-latency communication (URLLC) in cell-free massive MIMO\nsystems, we optimize the precoding vector at the access points (APs) to\nmaximize the minimum user rate in both the centralized and decentralized\nfashion. The design takes into account the impact of URLLC and we propose\npath-following algorithms (PFA) to address the considered problem which\ngenerates a sequence of advanced feasible points and converges to at least a\nlocally optimal solution of the design problem. Moreover, we investigate the\nrequirement of the precoding schemes, the length of the transmission duration,\nthe number of antennas equipped at each AP, and the size of each AP cluster on\nthe URLLC rate. Numerical results show that the decentralized PFA precoding can\nachieve 80\\% of the 95\\%-likely URLLC rate of the centralized precoding and\n89\\% of the average URLLC rate with only 12\\% computational complexity of the\ncentralized precoding.",
    "descriptor": "\nComments: 6 pages,3 figures\n",
    "authors": [
      "Enyu Shi",
      "Jing Zhang",
      "Jiayi Zhang",
      "Derrick Wing Kwan Ng",
      "Bo Ai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14504"
  },
  {
    "id": "arXiv:2209.14505",
    "title": "Optimal Retail Tariff Design with Prosumers: Pursuing Equity at the  Expenses of Economic Efficiencies?",
    "abstract": "Distributed renewable resources owned by prosumers can be an effective way of\nfortifying grid resilience and enhancing sustainability. However, prosumers\nserve their own interests and their objectives are unlikely to align with that\nof society. This paper develops a bilevel model to study the optimal design of\nretail electricity tariffs considering the balance between economic efficiency\nand energy equity. The retail tariff entails a fixed charge and a volumetric\ncharge tied to electricity usage to recover utilities' fixed costs. We analyze\nsolution properties of the bilevel problem and prove an optimal rate design,\nwhich is to use fixed charges to recover fixed costs and to balance energy\nequity among different income groups. This suggests that programs similar to\nCARE (California Alternative Rate of Energy), which offer lower retail rates to\nlow-income households, are unlikely to be efficient, even if they are\npolitically appealing.",
    "descriptor": "",
    "authors": [
      "Yihsu Chen",
      "Andrew L. Liu",
      "Makoto Tanaka",
      "Ryuta Takashima"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "General Economics (econ.GN)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.14505"
  },
  {
    "id": "arXiv:2209.14512",
    "title": "A Two-Stage Method for Chinese AMR Parsing",
    "abstract": "In this paper, we provide a detailed description of our system at CAMRP-2022\nevaluation. We firstly propose a two-stage method to conduct Chinese AMR\nParsing with alignment generation, which includes Concept-Prediction and\nRelation-Prediction stages. Our model achieves 0.7756 and 0.7074 Align-Smatch\nF1 scores on the CAMR 2.0 test set and the blind-test set of CAMRP-2022\nindividually. We also analyze the result and the limitation such as the error\npropagation and class imbalance problem we conclude in the current method. Code\nand the trained models are released at\nhttps://github.com/PKUnlp-icler/Two-Stage-CAMRP for reproduction.",
    "descriptor": "\nComments: 10 pages, CAMRP-2022\n",
    "authors": [
      "Liang Chen",
      "Bofei Gao",
      "Baobao Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14512"
  },
  {
    "id": "arXiv:2209.14513",
    "title": "How Does Value Distribution in Distributional Reinforcement Learning  Help Optimization?",
    "abstract": "We consider the problem of learning a set of probability distributions from\nthe Bellman dynamics in distributional reinforcement learning~(RL) that learns\nthe whole return distribution compared with only its expectation in classical\nRL. Despite its success to obtain superior performance, we still have a poor\nunderstanding of how the value distribution in distributional RL works. In this\nstudy, we analyze the optimization benefits of distributional RL by leverage of\nadditional value distribution information over classical RL in the Neural\nFitted Z-Iteration~(Neural FZI) framework. To begin with, we demonstrate that\nthe distribution loss of distributional RL has desirable smoothness\ncharacteristics and hence enjoys stable gradients, which is in line with its\ntendency to promote optimization stability. Furthermore, the acceleration\neffect of distributional RL is revealed by decomposing the return distribution.\nIt turns out that distributional RL can perform favorably if the value\ndistribution approximation is appropriate, measured by the variance of gradient\nestimates in each environment for any specific distributional RL algorithm.\nRigorous experiments validate the stable optimization behaviors of\ndistributional RL, contributing to its acceleration effects compared to\nclassical RL. The findings of our research illuminate how the value\ndistribution in distributional RL algorithms helps the optimization.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.03155\n",
    "authors": [
      "Ke Sun",
      "Bei Jiang",
      "Linglong Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14513"
  },
  {
    "id": "arXiv:2209.14514",
    "title": "How Powerful is Implicit Denoising in Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs), which aggregate features from neighbors, are\nwidely used for graph-structured data processing due to their powerful\nrepresentation learning capabilities. It is generally believed that GNNs can\nimplicitly remove the non-predictive noises. However, the analysis of implicit\ndenoising effect in graph neural networks remains open. In this work, we\nconduct a comprehensive theoretical study and analyze when and why the implicit\ndenoising happens in GNNs. Specifically, we study the convergence properties of\nnoise matrix. Our theoretical analysis suggests that the implicit denoising\nlargely depends on the connectivity, the graph size, and GNN architectures.\nMoreover, we formally define and propose the adversarial graph signal denoising\n(AGSD) problem by extending graph signal denoising problem. By solving such a\nproblem, we derive a robust graph convolution, where the smoothness of the node\nrepresentations and the implicit denoising effect can be enhanced. Extensive\nempirical evaluations verify our theoretical analyses and the effectiveness of\nour proposed model.",
    "descriptor": "",
    "authors": [
      "Songtao Liu",
      "Rex Ying",
      "Hanze Dong",
      "Lu Lin",
      "Jinghui Chen",
      "Dinghao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14514"
  },
  {
    "id": "arXiv:2209.14515",
    "title": "Effect of the Dynamics of a Horizontally Wobbling Mass on Biped Walking  Performance",
    "abstract": "We have developed biped robots with a passive dynamic walking mechanism. This\nstudy proposes a compass model with a wobbling mass connected to the upper body\nand oscillating in the horizontal direction to clarify the influence of the\nhorizontal dynamics of the upper body on bipedal walking. The limit cycles of\nthe model were numerically searched, and their stability and energy efficiency\nwas investigated. Several qualitatively different limit cycles were obtained\ndepending mainly on the spring constant that supports the wobbling mass.\nSpecific types of solutions decreased the stability while reducing the risk of\naccidental falling and improving the energy efficiency. The obtained results\nwere attributed to the wobbling mass moving in the opposite direction to the\nupper body, thereby preventing large changes in acceleration and deceleration\nwhile walking. The relationship between the locomotion of the proposed model\nand the actual biped robot and human gaits was investigated.",
    "descriptor": "\nComments: 6 pages, 8 figures, submitted to IEEE International Conference on Robotics and Automation (ICRA 2023)\n",
    "authors": [
      "Tomoya Kamimura",
      "Akihito Sano"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14515"
  },
  {
    "id": "arXiv:2209.14516",
    "title": "Matroid Intersection under Restricted Oracles",
    "abstract": "Matroid intersection is one of the most powerful frameworks of matroid theory\nthat generalizes various problems in combinatorial optimization. Edmonds'\nfundamental theorem provides a min-max characterization for the unweighted\nsetting, while Frank's weight-splitting theorem provides one for the weighted\ncase. Several efficient algorithms were developed for these problems, all\nrelying on the usage of one of the conventional oracles for both matroids.\nIn the present paper, we consider the tractability of the matroid\nintersection problem under restricted oracles. In particular, we focus on the\nrank sum, common independence, and maximum rank oracles. We give a strongly\npolynomial-time algorithm for weighted matroid intersection under the rank sum\noracle. In the common independence oracle model, we prove that the unweighted\nmatroid intersection problem is tractable when one of the matroids is a\npartition matroid, and that even the weighted case is solvable when one of the\nmatroids is an elementary split matroid. Finally, we show that the common\nindependence and maximum rank oracles together are strong enough to realize the\nsteps of our algorithm under the rank sum oracle.",
    "descriptor": "\nComments: 20 pages, 2 figures\n",
    "authors": [
      "Krist\u00f3f B\u00e9rczi",
      "Tam\u00e1s Kir\u00e1ly",
      "Yutaro Yamaguchi",
      "Yu Yokoi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2209.14516"
  },
  {
    "id": "arXiv:2209.14517",
    "title": "The Fungibility of Non-Fungible Tokens: A Quantitative Analysis of  ERC-721 Metadata",
    "abstract": "Non-Fungible Tokens (NFTs), digital certificates of ownership for virtual\nart, have until recently been traded on a highly lucrative and speculative\nmarket. Yet, an emergence of misconceptions, along with a sustained market\ndowntime, are calling the value of NFTs into question. This project (1)\ndescribes three properties that any valuable NFT should possess (permanence,\nimmutability and uniqueness), (2) creates a quantitative summary of permanence\nas an initial criteria, and (3) tests our measures on 6 months of NFTs on the\nEthereum blockchain, finding 45% of ERC721 tokens in our corpus do not satisfy\nthis initial criteria. Our work could help buyers and marketplaces identify and\nwarn users against purchasing NFTs that may be overvalued.",
    "descriptor": "\nComments: 8 pages inc. appendix, 3 figures\n",
    "authors": [
      "Sarah Barrington",
      "Nick Merrill"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.14517"
  },
  {
    "id": "arXiv:2209.14520",
    "title": "Label driven Knowledge Distillation for Federated Learning with non-IID  Data",
    "abstract": "In real-world applications, Federated Learning (FL) meets two challenges: (1)\nscalability, especially when applied to massive IoT networks; and (2) how to be\nrobust against an environment with heterogeneous data. Realizing the first\nproblem, we aim to design a novel FL framework named Full-stack FL (F2L). More\nspecifically, F2L utilizes a hierarchical network architecture, making\nextending the FL network accessible without reconstructing the whole network\nsystem. Moreover, leveraging the advantages of hierarchical network design, we\npropose a new label-driven knowledge distillation (LKD) technique at the global\nserver to address the second problem. As opposed to current knowledge\ndistillation techniques, LKD is capable of training a student model, which\nconsists of good knowledge from all teachers' models. Therefore, our proposed\nalgorithm can effectively extract the knowledge of the regions' data\ndistribution (i.e., the regional aggregated models) to reduce the divergence\nbetween clients' models when operating under the FL system with non-independent\nidentically distributed data. Extensive experiment results reveal that: (i) our\nF2L method can significantly improve the overall FL efficiency in all global\ndistillations, and (ii) F2L rapidly achieves convergence as global distillation\nstages occur instead of increasing on each communication cycle.",
    "descriptor": "\nComments: 28 pages, 5 figures, 10 tables\n",
    "authors": [
      "Minh-Duong Nguyen",
      "Quoc-Viet Pham",
      "Dinh Thai Hoang",
      "Long Tran-Thanh",
      "Diep N. Nguyen",
      "Won-Joo Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14520"
  },
  {
    "id": "arXiv:2209.14525",
    "title": "Self-Configurable Stabilized Real-Time Detection Learning for Autonomous  Driving Applications",
    "abstract": "Guaranteeing real-time and accurate object detection simultaneously is\nparamount in autonomous driving environments. However, the existing object\ndetection neural network systems are characterized by a tradeoff between\ncomputation time and accuracy, making it essential to optimize such a tradeoff.\nFortunately, in many autonomous driving environments, images come in a\ncontinuous form, providing an opportunity to use optical flow. In this paper,\nwe improve the performance of an object detection neural network utilizing\noptical flow estimation. In addition, we propose a Lyapunov optimization\nframework for time-average performance maximization subject to stability. It\nadaptively determines whether to use optical flow to suit the dynamic vehicle\nenvironment, thereby ensuring the vehicle's queue stability and the\ntime-average maximum performance simultaneously. To verify the key ideas, we\nconduct numerical experiments with various object detection neural networks and\noptical flow estimation networks. In addition, we demonstrate the\nself-configurable stabilized detection with YOLOv3-tiny and FlowNet2-S, which\nare the real-time object detection network and an optical flow estimation\nnetwork, respectively. In the demonstration, our proposed framework improves\nthe accuracy by 3.02%, the number of detected objects by 59.6%, and the queue\nstability for computing capabilities.",
    "descriptor": "",
    "authors": [
      "Won Joon Yun",
      "Soohyun Park",
      "Joongheon Kim",
      "David Mohaisen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14525"
  },
  {
    "id": "arXiv:2209.14529",
    "title": "Motion and Appearance Adaptation for Cross-Domain Motion Transfer",
    "abstract": "Motion transfer aims to transfer the motion of a driving video to a source\nimage. When there are considerable differences between object in the driving\nvideo and that in the source image, traditional single domain motion transfer\napproaches often produce notable artifacts; for example, the synthesized image\nmay fail to preserve the human shape of the source image (cf . Fig. 1 (a)). To\naddress this issue, in this work, we propose a Motion and Appearance Adaptation\n(MAA) approach for cross-domain motion transfer, in which we regularize the\nobject in the synthesized image to capture the motion of the object in the\ndriving frame, while still preserving the shape and appearance of the object in\nthe source image. On one hand, considering the object shapes of the synthesized\nimage and the driving frame might be different, we design a shape-invariant\nmotion adaptation module that enforces the consistency of the angles of object\nparts in two images to capture the motion information. On the other hand, we\nintroduce a structure-guided appearance consistency module designed to\nregularize the similarity between the corresponding patches of the synthesized\nimage and the source image without affecting the learned motion in the\nsynthesized image. Our proposed MAA model can be trained in an end-to-end\nmanner with a cyclic reconstruction loss, and ultimately produces a\nsatisfactory motion transfer result (cf . Fig. 1 (b)). We conduct extensive\nexperiments on human dancing dataset Mixamo-Video to Fashion-Video and human\nface dataset Vox-Celeb to Cufs; on both of these, our MAA model outperforms\nexisting methods both quantitatively and qualitatively.",
    "descriptor": "",
    "authors": [
      "Borun Xu",
      "Biao Wang",
      "Jinhong Deng",
      "Jiale Tao",
      "Tiezheng Ge",
      "Yuning Jiang",
      "Wen Li",
      "Lixin Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14529"
  },
  {
    "id": "arXiv:2209.14532",
    "title": "Feature Selection via the Intervened Interpolative Decomposition and its  Application in Diversifying Quantitative Strategies",
    "abstract": "In this paper, we propose a probabilistic model for computing an\ninterpolative decomposition (ID) in which each column of the observed matrix\nhas its own priority or importance, so that the end result of the decomposition\nfinds a set of features that are representative of the entire set of features,\nand the selected features also have higher priority than others. This approach\nis commonly used for low-rank approximation, feature selection, and extracting\nhidden patterns in data, where the matrix factors are latent variables\nassociated with each data dimension. Gibbs sampling for Bayesian inference is\napplied to carry out the optimization. We evaluate the proposed models on\nreal-world datasets, including ten Chinese A-share stocks, and demonstrate that\nthe proposed Bayesian ID algorithm with intervention (IID) produces comparable\nreconstructive errors to existing Bayesian ID algorithms while selecting\nfeatures with higher scores or priority.",
    "descriptor": "",
    "authors": [
      "Jun Lu",
      "Joerg Osterrieder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Finance (q-fin.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.14532"
  },
  {
    "id": "arXiv:2209.14536",
    "title": "Convergence of the mini-batch SIHT algorithm",
    "abstract": "The Iterative Hard Thresholding (IHT) algorithm has been considered\nextensively as an effective deterministic algorithm for solving sparse\noptimizations. The IHT algorithm benefits from the information of the batch\n(full) gradient at each point and this information is a crucial key for the\nconvergence analysis of the generated sequence. However, this strength becomes\na weakness when it comes to machine learning and high dimensional statistical\napplications because calculating the batch gradient at each iteration is\ncomputationally expensive or impractical. Fortunately, in these applications\nthe objective function has a summation structure that can be taken advantage of\nto approximate the batch gradient by the stochastic mini-batch gradient. In\nthis paper, we study the mini-batch Stochastic IHT (SIHT) algorithm for solving\nthe sparse optimizations. As opposed to previous works where increasing and\nvariable mini-batch size is necessary for derivation, we fix the mini-batch\nsize according to a lower bound that we derive and show our work. To prove\nstochastic convergence of the objective value function we first establish a\ncritical sparse stochastic gradient descent property. Using this stochastic\ngradient descent property we show that the sequence generated by the stochastic\nmini-batch SIHT is a supermartingale sequence and converges with probability\none. Unlike previous work we do not assume the function to be a restricted\nstrongly convex. To the best of our knowledge, in the regime of sparse\noptimization, this is the first time in the literature that it is shown that\nthe sequence of the stochastic function values converges with probability one\nby fixing the mini-batch size for all steps.",
    "descriptor": "",
    "authors": [
      "Saeed Damadi",
      "Jinglai Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.14536"
  },
  {
    "id": "arXiv:2209.14537",
    "title": "GPU-based Data-parallel Rendering of Large, Unstructured, and  Non-convexly Partitioned Data",
    "abstract": "Computational fluid dynamic simulations often produce large clusters of\nfinite elements with non-trivial, non-convex boundaries and uneven\ndistributions among compute nodes, posing challenges to compositing during\ninteractive volume rendering. Correct, in-place visualization of such clusters\nbecomes difficult because viewing rays straddle domain boundaries across\nmultiple compute nodes. We propose a GPU-based, scalable, memory-efficient\ndirect volume visualization framework suitable for in~situ and post~hoc usage.\nOur approach reduces memory usage of the unstructured volume elements by\nleveraging an exclusive or-based index reduction scheme and provides fast\nray-marching-based traversal without requiring large external data structures\nbuilt over the elements themselves. Moreover, we present a GPU-optimized deep\ncompositing scheme that allows correct order compositing of intermediate color\nvalues accumulated across different ranks that works even for non-convex\nclusters. Our method scales well on large data-parallel systems and achieves\ninteractive frame rates during visualization. We can interactively render both\nFun3D Small Mars Lander (14 GB / 798.4 million finite elements) and Huge Mars\nLander (111.57 GB / 6.4 billion finite elements) data sets at 14 and 10 frames\nper second using 72 and 80 GPUs, respectively, on TACC's Frontera\nsupercomputer.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Alper Sahistan",
      "Serkan Demirci",
      "Ingo Wald",
      "Stefan Zellmann",
      "Jo\u00e3o Barbosa",
      "Nathan Morrical",
      "U\u011fur G\u00fcd\u00fckbay"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2209.14537"
  },
  {
    "id": "arXiv:2209.14539",
    "title": "Transmission Model for Resonant Beam SWIPT with Telescope Internal  Modulator",
    "abstract": "To satisfy the long-range and energy self-sustaining communication needs of\nelectronic devices in the Internet of Things (IoT), we introduce a simultaneous\nwireless information and power transfer (SWIPT) system using the resonant beam\nthat incorporates a telescope modulator inside a cavity for suppressing\ndiffraction losses. We theoretically analyze power transfer in the resonant\nbeam system with telescope internal modulator (TIM-RBS) considering the\nelectromagnetic field propagation, the end-to-end (E2E) power transfer, and\npower and information reception. The numerical evaluation demonstrates that the\nTIM can effectively compress the beam spot, which allows the TIM-RBS to\ntransmit energy twice as far as the RBS without TIM at higher power.\nAdditionally, the largest transmission distance and maximum output power are\nproportional to the input power, and about 34m transmission distance, 4W\nelectric power, and 12bps/Hz spectral efficiency can be achieved in the TIM-RBS\nwith 200W input power. Hence, TIM-RBS can be considered as a promising option\nfor realizing long-range, high-power, and high-rate SWIPT.",
    "descriptor": "",
    "authors": [
      "Wen Fang",
      "Yunfeng Bai",
      "Qingwen Liu",
      "Shengli Zhou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Applied Physics (physics.app-ph)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2209.14539"
  },
  {
    "id": "arXiv:2209.14547",
    "title": "A Secure Federated Learning Framework for Residential Short Term Load  Forecasting",
    "abstract": "Smart meter measurements, though critical for accurate demand forecasting,\nface several drawbacks including consumers' privacy, data breach issues, to\nname a few. Recent literature has explored Federated Learning (FL) as a\npromising privacy-preserving machine learning alternative which enables\ncollaborative learning of a model without exposing private raw data for short\nterm load forecasting. Despite its virtue, standard FL is still vulnerable to\nan intractable cyber threat known as Byzantine attack carried out by faulty\nand/or malicious clients. Therefore, to improve the robustness of federated\nshort-term load forecasting against Byzantine threats, we develop a\nstate-of-the-art differentially private secured FL-based framework that ensures\nthe privacy of the individual smart meter's data while protect the security of\nFL models and architecture. Our proposed framework leverages the idea of\ngradient quantization through the Sign Stochastic Gradient Descent (SignSGD)\nalgorithm, where the clients only transmit the `sign' of the gradient to the\ncontrol centre after local model training. As we highlight through our\nexperiments involving benchmark neural networks with a set of Byzantine attack\nmodels, our proposed approach mitigates such threats quite effectively and thus\noutperforms conventional Fed-SGD models.",
    "descriptor": "",
    "authors": [
      "Muhammad Akbar Husnoo",
      "Adnan Anwar",
      "Nasser Hosseinzadeh",
      "Shama Naz Islam",
      "Abdun Naser Mahmood",
      "Robin Doss"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14547"
  },
  {
    "id": "arXiv:2209.14548",
    "title": "Offline Reinforcement Learning via High-Fidelity Generative Behavior  Modeling",
    "abstract": "In offline reinforcement learning, weighted regression is a common method to\nensure the learned policy stays close to the behavior policy and to prevent\nselecting out-of-sample actions. In this work, we show that due to the limited\ndistributional expressivity of policy models, previous methods might still\nselect unseen actions during training, which deviates from their initial\nmotivation. To address this problem, we adopt a generative approach by\ndecoupling the learned policy into two parts: an expressive generative behavior\nmodel and an action evaluation model. The key insight is that such decoupling\navoids learning an explicitly parameterized policy model with a closed-form\nexpression. Directly learning the behavior policy allows us to leverage\nexisting advances in generative modeling, such as diffusion-based methods, to\nmodel diverse behaviors. As for action evaluation, we combine our method with\nan in-sample planning technique to further avoid selecting out-of-sample\nactions and increase computational efficiency. Experimental results on D4RL\ndatasets show that our proposed method achieves competitive or superior\nperformance compared with state-of-the-art offline RL methods, especially in\ncomplex tasks such as AntMaze. We also empirically demonstrate that our method\ncan successfully learn from a heterogeneous dataset containing multiple\ndistinctive but similarly successful strategies, whereas previous unimodal\npolicies fail.",
    "descriptor": "",
    "authors": [
      "Huayu Chen",
      "Cheng Lu",
      "Chengyang Ying",
      "Hang Su",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14548"
  },
  {
    "id": "arXiv:2209.14552",
    "title": "Centralized and Decentralized Techniques for Analysis and Synthesis of  Non-Linear Networked Systems",
    "abstract": "Networked systems comprised of interconnected sets of non-linear subsystems\nappear in many real-world scenarios such as in robotics, communication networks\nand biochemical processes. In this paper, we develop centralized and\ndecentralized techniques for analyzing and synthesizing such networked systems\nonly using dissipativity properties of individual constituent subsystems. In\nparticular, this work can be seen as an extension of a recent and celebrated\nwork that proposed a centralized analysis approach for networked systems as a\nlinear matrix inequality (LMI) problem. First, we consider four networked\nsystem configurations (NSCs) of interest and provide centralized analysis\n(stability/dissipativity analysis) techniques for them as LMI problems. Second,\nwe show that the centralized synthesis (interconnection topology design)\ntechniques for these NSCs can also be formulated as LMI problems. Note that\nthis enables designing the interconnection topology among the given set of\nsubsystems such that specific stability/dissipativity measures of the resulting\ninterconnected system are optimized. Next, we provide decentralized and\ncompositional (i.e., resilient to subsystem additions and removals)\ncounterparts for the earlier proposed centralized analysis and synthesis\ntechniques. Finally, we include several numerical results to demonstrate our\ncontributions.",
    "descriptor": "\nComments: To be submitted to ACC 2023\n",
    "authors": [
      "Shirantha Welikala",
      "Hai Lin",
      "Panos J. Antsaklis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14552"
  },
  {
    "id": "arXiv:2209.14553",
    "title": "Regularizing Neural Network Training via Identity-wise Discriminative  Feature Suppression",
    "abstract": "It is well-known that a deep neural network has a strong fitting capability\nand can easily achieve a low training error even with randomly assigned class\nlabels. When the number of training samples is small, or the class labels are\nnoisy, networks tend to memorize patterns specific to individual instances to\nminimize the training error. This leads to the issue of overfitting and poor\ngeneralisation performance. This paper explores a remedy by suppressing the\nnetwork's tendency to rely on instance-specific patterns for empirical error\nminimisation. The proposed method is based on an adversarial training\nframework. It suppresses features that can be utilized to identify individual\ninstances among samples within each class. This leads to classifiers only using\nfeatures that are both discriminative across classes and common within each\nclass. We call our method Adversarial Suppression of Identity Features (ASIF),\nand demonstrate the usefulness of this technique in boosting generalisation\naccuracy when faced with small datasets or noisy labels. Our source code is\navailable.",
    "descriptor": "\nComments: DICTA 2022\n",
    "authors": [
      "Avraham Chapman",
      "Lingqiao Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14553"
  },
  {
    "id": "arXiv:2209.14557",
    "title": "Neural Media Bias Detection Using Distant Supervision With BABE -- Bias  Annotations By Experts",
    "abstract": "Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of a gold standard data set\nand high context dependencies. This paper presents BABE, a robust and diverse\ndata set created by trained experts, for media bias research. We also analyze\nwhy expert labeling is essential within this domain. Our data set offers better\nannotation quality and higher inter-annotator agreement than existing work. It\nconsists of 3,700 sentences balanced among topics and outlets, containing media\nbias labels on the word and sentence level. Based on our data, we also\nintroduce a way to detect bias-inducing sentences in news articles\nautomatically. Our best performing BERT-based model is pre-trained on a larger\ncorpus consisting of distant labels. Fine-tuning and evaluating the model on\nour proposed supervised data set, we achieve a macro F1-score of 0.804,\noutperforming existing methods.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2112.13352\n",
    "authors": [
      "Timo Spinde",
      "Manuel Plank",
      "Jan-David Krieger",
      "Terry Ruas",
      "Bela Gipp",
      "Akiko Aizawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14557"
  },
  {
    "id": "arXiv:2209.14558",
    "title": "Computational Complexity of Sub-linear Convergent Algorithms",
    "abstract": "Optimizing machine learning algorithms that are used to solve the objective\nfunction has been of great interest. Several approaches to optimize common\nalgorithms, such as gradient descent and stochastic gradient descent, were\nexplored. One of these approaches is reducing the gradient variance through\nadaptive sampling to solve large-scale optimization's empirical risk\nminimization (ERM) problems. In this paper, we will explore how starting with a\nsmall sample and then geometrically increasing it and using the solution of the\nprevious sample ERM to compute the new ERM. This will solve ERM problems with\nfirst-order optimization algorithms of sublinear convergence but with lower\ncomputational complexity. This paper starts with theoretical proof of the\napproach, followed by two experiments comparing the gradient descent with the\nadaptive sampling of the gradient descent and ADAM with adaptive sampling ADAM\non different datasets.",
    "descriptor": "\nComments: 8 Pages\n",
    "authors": [
      "Hilal AlQuabeh",
      "Farha AlBreiki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14558"
  },
  {
    "id": "arXiv:2209.14561",
    "title": "Phase function methods for second order linear ordinary differential  equations with turning points",
    "abstract": "It is well known that second order linear ordinary differential equations\nwith slowly varying coefficients admit slowly varying phase functions. This\nobservation is the basis of the Liouville-Green method and many other\ntechniques for the asymptotic approximation of the solutions of such equations.\nMore recently, it was exploited by the author to develop a highly efficient\nsolver for second order linear ordinary differential equations whose solutions\nare oscillatory. In many cases of interest, that algorithm achieves near\noptimal accuracy in time independent of the frequency of oscillation of the\nsolutions. Here we show that, after minor modifications, it also allows for the\nefficient solution of second order differential equation equations which have\nturning points. That is, it is effective in the case of equations whose\nsolutions are oscillatory in some regions and behave like linear combinations\nof increasing and decreasing exponentials in others. We present the results of\nnumerical experiments demonstrating the properties of our method, including\nsome which show that it can used to evaluate many classical special functions\nin time independent of the parameters on which they depend.",
    "descriptor": "",
    "authors": [
      "James Bremer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14561"
  },
  {
    "id": "arXiv:2209.14569",
    "title": "COLO: A Contrastive Learning based Re-ranking Framework for One-Stage  Summarization",
    "abstract": "Traditional training paradigms for extractive and abstractive summarization\nsystems always only use token-level or sentence-level training objectives.\nHowever, the output summary is always evaluated from summary-level which leads\nto the inconsistency in training and evaluation. In this paper, we propose a\nContrastive Learning based re-ranking framework for one-stage summarization\ncalled COLO. By modeling a contrastive objective, we show that the\nsummarization model is able to directly generate summaries according to the\nsummary-level score without additional modules and parameters. Extensive\nexperiments demonstrate that COLO boosts the extractive and abstractive results\nof one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1\nscore while preserving the parameter efficiency and inference efficiency.\nCompared with state-of-the-art multi-stage systems, we save more than 100 GPU\ntraining hours and obtaining 3~8 speed-up ratio during inference while\nmaintaining comparable results.",
    "descriptor": "\nComments: Accepted by COLING 2022\n",
    "authors": [
      "Chenxin An",
      "Ming Zhong",
      "Zhiyong Wu",
      "Qin Zhu",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14569"
  },
  {
    "id": "arXiv:2209.14575",
    "title": "Correcting the Sub-optimal Bit Allocation",
    "abstract": "In this paper, we investigate the problem of bit allocation in Neural Video\nCompression (NVC). First, we reveal that a recent bit allocation approach\nclaimed to be optimal is, in fact, sub-optimal due to its implementation.\nSpecifically, we find that its sub-optimality lies in the improper application\nof semi-amortized variational inference (SAVI) on latent with non-factorized\nvariational posterior. Then, we show that the corrected version of SAVI on\nnon-factorized latent requires recursively applying back-propagating through\ngradient ascent, based on which we derive the corrected optimal bit allocation\nalgorithm. Due to the computational in-feasibility of the corrected bit\nallocation, we design an efficient approximation to make it practical.\nEmpirical results show that our proposed correction significantly improves the\nincorrect bit allocation in terms of R-D performance and bitrate error, and\noutperforms all other bit allocation methods by a large margin. The source code\nis provided in the supplementary material.",
    "descriptor": "",
    "authors": [
      "Tongda Xu",
      "Han Gao",
      "Yuanyuan Wang",
      "Hongwei Qin",
      "Yan Wang",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14575"
  },
  {
    "id": "arXiv:2209.14579",
    "title": "On the Forsythe conjecture",
    "abstract": "Forsythe formulated a conjecture about the asymptotic behavior of the\nrestarted conjugate gradient method in 1968. We translate several of his\nresults into modern terms, and generalize the conjecture (originally formulated\nonly for symmetric positive definite matrices) to symmetric and nonsymmetric\nmatrices. Our generalization is based on a two-sided or cross iteration with\nthe given matrix and its transpose, which is based on the projection process\nused in the Arnoldi (or for symmetric matrices the Lanczos) algorithm. We prove\nseveral new results about the limiting behavior of this iteration, but the\nconjecture still remains largely open.",
    "descriptor": "",
    "authors": [
      "Vance Faber",
      "J\u00f6rg Liesen",
      "Petr Tich\u00fd"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14579"
  },
  {
    "id": "arXiv:2209.14583",
    "title": "Spatial Moment Pooling Improves Neural Image Assessment",
    "abstract": "In recent years, there has been widespread attention drawn to convolutional\nneural network (CNN) based blind image quality assessment (IQA). A large number\nof works start by extracting deep features from CNN. Then, those features are\nprocessed through spatial average pooling (SAP) and fully connected layers to\npredict quality. Inspired by full reference IQA and texture features, in this\npaper, we extend SAP ($1^{st}$ moment) into spatial moment pooling (SMP) by\nincorporating higher order moments (such as variance, skewness). Moreover, we\nprovide learning friendly normalization to circumvent numerical issue when\ncomputing gradients of higher moments. Experimental results suggest that simply\nupgrading SAP to SMP significantly enhances CNN-based blind IQA methods and\nachieves state of the art performance.",
    "descriptor": "\nComments: ICIP 2022\n",
    "authors": [
      "Tongda Xu",
      "Yifan Shao",
      "Yan Wang",
      "Hongwei Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14583"
  },
  {
    "id": "arXiv:2209.14586",
    "title": "DIY Graphics Tab: A Cost-Effective Alternative to Graphics Tablet for  Educators",
    "abstract": "Recording lectures is a normal task for online educators, and a graphics\ntablet is a great tool for that. However, it is very expensive for many\ninstructors. In this paper, we propose an alternative called \"DIY Graphics Tab\"\nthat functions largely in the same way as a graphic tab, but requires only a\npen, paper, and laptop's webcam. Our system takes images of writings on a paper\nwith a webcam and outputs the contents. The task is not straightforward since\nthere are obstacles, such as hand occlusion, paper movements, lighting, and\nperspective distortion due to the viewing angle. A pipeline is used that\napplies segmentation and post-processing for generating appropriate output from\ninput frames. We also conducted user experience evaluations from the teachers.",
    "descriptor": "\nComments: Accepted at ACM UIST 2022 (poster)\n",
    "authors": [
      "Mohammad Imrul Jubair",
      "Arafat Ibne Yousuf",
      "Tashfiq Ahmed",
      "Hasanath Jamy",
      "Foisal Reza",
      "Mohsena Ashraf"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14586"
  },
  {
    "id": "arXiv:2209.14591",
    "title": "PerSign: Personalized Bangladeshi Sign Letters Synthesis",
    "abstract": "Bangladeshi Sign Language (BdSL) - like other sign languages - is tough to\nlearn for general people, especially when it comes to expressing letters. In\nthis poster, we propose PerSign, a system that can reproduce a person's image\nby introducing sign gestures in it. We make this operation personalized, which\nmeans the generated image keeps the person's initial image profile - face, skin\ntone, attire, background - unchanged while altering the hand, palm, and finger\npositions appropriately. We use an image-to-image translation technique and\nbuild a corresponding unique dataset to accomplish the task. We believe the\ntranslated image can reduce the communication gap between signers (person who\nuses sign language) and non-signers without having prior knowledge of BdSL.",
    "descriptor": "\nComments: Accepted at ACM UIST 2022 (poster)\n",
    "authors": [
      "Mohammad Imrul Jubair",
      "Ali Ahnaf",
      "Tashfiq Nahiyan Khan",
      "Ullash Bhattacharjee",
      "Tanjila Joti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14591"
  },
  {
    "id": "arXiv:2209.14593",
    "title": "Denoising MCMC for Accelerating Diffusion-Based Generative Models",
    "abstract": "Diffusion models are powerful generative models that simulate the reverse of\ndiffusion processes using score functions to synthesize data from noise. The\nsampling process of diffusion models can be interpreted as solving the reverse\nstochastic differential equation (SDE) or the ordinary differential equation\n(ODE) of the diffusion process, which often requires up to thousands of\ndiscretization steps to generate a single image. This has sparked a great\ninterest in developing efficient integration techniques for reverse-S/ODEs.\nHere, we propose an orthogonal approach to accelerating score-based sampling:\nDenoising MCMC (DMCMC). DMCMC first uses MCMC to produce samples in the product\nspace of data and variance (or diffusion time). Then, a reverse-S/ODE\nintegrator is used to denoise the MCMC samples. Since MCMC traverses close to\nthe data manifold, the computation cost of producing a clean sample for DMCMC\nis much less than that of producing a clean sample from noise. To verify the\nproposed concept, we show that Denoising Langevin Gibbs (DLG), an instance of\nDMCMC, successfully accelerates all six reverse-S/ODE integrators considered in\nthis work on the tasks of CIFAR10 and CelebA-HQ-256 image generation. Notably,\ncombined with integrators of Karras et al. (2022) and pre-trained score models\nof Song et al. (2021b), DLG achieves SOTA results. In the limited number of\nscore function evaluation (NFE) settings on CIFAR10, we have $3.86$ FID with\n$\\approx 10$ NFE and $2.63$ FID with $\\approx 20$ NFE. On CelebA-HQ-256, we\nhave $6.99$ FID with $\\approx 160$ NFE, which beats the current best record of\nKim et al. (2022) among score-based models, $7.16$ FID with $4000$ NFE. Code:\nhttps://github.com/1202kbs/DMCMC",
    "descriptor": "",
    "authors": [
      "Beomsu Kim",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14593"
  },
  {
    "id": "arXiv:2209.14594",
    "title": "Bayesian Neural Network Versus Ex-Post Calibration For Prediction  Uncertainty",
    "abstract": "Probabilistic predictions from neural networks which account for predictive\nuncertainty during classification is crucial in many real-world and high-impact\ndecision making settings. However, in practice most datasets are trained on\nnon-probabilistic neural networks which by default do not capture this inherent\nuncertainty. This well-known problem has led to the development of post-hoc\ncalibration procedures, such as Platt scaling (logistic), isotonic and beta\ncalibration, which transforms the scores into well calibrated empirical\nprobabilities. A plausible alternative to the calibration approach is to use\nBayesian neural networks, which directly models a predictive distribution.\nAlthough they have been applied to images and text datasets, they have seen\nlimited adoption in the tabular and small data regime. In this paper, we\ndemonstrate that Bayesian neural networks yields competitive performance when\ncompared to calibrated neural networks and conduct experiments across a wide\narray of datasets.",
    "descriptor": "",
    "authors": [
      "Satya Borgohain",
      "Klaus Ackermann",
      "Ruben Loaiza-Maya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14594"
  },
  {
    "id": "arXiv:2209.14598",
    "title": "Dynamic Surrogate Switching: Sample-Efficient Search for Factorization  Machine Configurations in Online Recommendations",
    "abstract": "Hyperparameter optimization is the process of identifying the appropriate\nhyperparameter configuration of a given machine learning model with regard to a\ngiven learning task. For smaller data sets, an exhaustive search is possible;\nHowever, when the data size and model complexity increase, the number of\nconfiguration evaluations becomes the main computational bottleneck. A\npromising paradigm for tackling this type of problem is surrogate-based\noptimization. The main idea underlying this paradigm considers an incrementally\nupdated model of the relation between the hyperparameter space and the output\n(target) space; the data for this model are obtained by evaluating the main\nlearning engine, which is, for example, a factorization machine-based model. By\nlearning to approximate the hyperparameter-target relation, the surrogate\n(machine learning) model can be used to score large amounts of hyperparameter\nconfigurations, exploring parts of the configuration space beyond the reach of\ndirect machine learning engine evaluation. Commonly, a surrogate is selected\nprior to optimization initialization and remains the same during the search. We\ninvestigated whether dynamic switching of surrogates during the optimization\nitself is a sensible idea of practical relevance for selecting the most\nappropriate factorization machine-based models for large-scale online\nrecommendation. We conducted benchmarks on data sets containing hundreds of\nmillions of instances against established baselines such as Random Forest- and\nGaussian process-based surrogates. The results indicate that surrogate\nswitching can offer good performance while considering fewer learning engine\nevaluations.",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Bla\u017e \u0160krlj",
      "Adi Schwartz",
      "Jure Ferle\u017e",
      "Davorin Kopi\u010d",
      "Naama Ziporin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14598"
  },
  {
    "id": "arXiv:2209.14599",
    "title": "Online pseudo labeling for polyp segmentation with momentum networks",
    "abstract": "Semantic segmentation is an essential task in developing medical image\ndiagnosis systems. However, building an annotated medical dataset is expensive.\nThus, semi-supervised methods are significant in this circumstance. In\nsemi-supervised learning, the quality of labels plays a crucial role in model\nperformance. In this work, we present a new pseudo labeling strategy that\nenhances the quality of pseudo labels used for training student networks. We\nfollow the multi-stage semi-supervised training approach, which trains a\nteacher model on a labeled dataset and then uses the trained teacher to render\npseudo labels for student training. By doing so, the pseudo labels will be\nupdated and more precise as training progress. The key difference between\nprevious and our methods is that we update the teacher model during the student\ntraining process. So the quality of pseudo labels is improved during the\nstudent training process. We also propose a simple but effective strategy to\nenhance the quality of pseudo labels using a momentum model -- a slow copy\nversion of the original model during training. By applying the momentum model\ncombined with re-rendering pseudo labels during student training, we achieved\nan average of 84.1% Dice Score on five datasets (i.e., Kvarsir, CVC-ClinicDB,\nETIS-LaribPolypDB, CVC-ColonDB, and CVC-300) with only 20% of the dataset used\nas labeled data. Our results surpass common practice by 3% and even approach\nfully-supervised results on some datasets. Our source code and pre-trained\nmodels are available at https://github.com/sun-asterisk-research/online\nlearning ssl",
    "descriptor": "\nComments: Accepted in KSE 2022\n",
    "authors": [
      "Toan Pham Van",
      "Linh Bao Doan",
      "Thanh Tung Nguyen",
      "Duc Trung Tran",
      "Quan Van Nguyen",
      "Dinh Viet Sang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14599"
  },
  {
    "id": "arXiv:2209.14601",
    "title": "The behaviour of the Gauss-Radau upper bound of the error norm in CG",
    "abstract": "Consider the problem of solving systems of linear algebraic equations $Ax=b$\nwith a real symmetric positive definite matrix $A$ using the conjugate gradient\n(CG) method. To stop the algorithm at the appropriate moment, it is important\nto monitor the quality of the approximate solution $x_k$. One of the most\nrelevant quantities for measuring the quality of $x_k$ is the $A$-norm of the\nerror. This quantity cannot be easily evaluated, however, it can be estimated.\nIn this paper we discuss and analyze the behaviour of the Gauss-Radau upper\nbound on the $A$-norm of the error, based on viewing CG as a procedure for\napproximating a certain Riemann-Stieltjes integral. This upper bound depends on\na prescribed underestimate $\\mu$ to the smallest eigenvalue of $A$. We\nconcentrate on explaining a phenomenon observed during computations showing\nthat, in later CG iterations, the upper bound loses its accuracy, and it is\nalmost independent of~$\\mu$. We construct a model problem that is used to\ndemonstrate and study the behaviour of the upper bound in dependence of~$\\mu$,\nand developed formulas that are helpful in understanding this behavior. We show\nthat the above mentioned phenomenon is closely related to the convergence of\nthe smallest Ritz value to the smallest eigenvalue of $A$. It occurs when the\nsmallest Ritz value is a better approximation to the smallest eigenvalue than\nthe prescribed underestimate $\\mu$. We also suggest an adaptive strategy for\nimproving the accuracy of the Gauss-Radau upper bound such that the resulting\nestimate approximates the quantity of interest with a prescribed relative\naccuracy.",
    "descriptor": "\nComments: 24 pages, 12 figures\n",
    "authors": [
      "G\u00e9rard Meurant",
      "Petr Tich\u00fd"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14601"
  },
  {
    "id": "arXiv:2209.14602",
    "title": "Exploring Cross-Point Embeddings for 3D Dense Uncertainty Estimation",
    "abstract": "Dense prediction tasks are common for 3D point clouds, but the inherent\nuncertainties in massive points and their embeddings have long been ignored. In\nthis work, we present CUE, a novel uncertainty estimation method for dense\nprediction tasks of 3D point clouds. Inspired by metric learning, the key idea\nof CUE is to explore cross-point embeddings upon a conventional dense\nprediction pipeline. Specifically, CUE involves building a probabilistic\nembedding model and then enforcing metric alignments of massive points in the\nembedding space. We demonstrate that CUE is a generic and effective tool for\ndense uncertainty estimation of 3D point clouds in two different tasks: (1) in\n3D geometric feature learning we for the first time obtain well-calibrated\ndense uncertainty, and (2) in semantic segmentation we reduce uncertainty`s\nExpected Calibration Error of the state-of-the-arts by 43.8%. All uncertainties\nare estimated without compromising predictive performance.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Kaiwen Cai",
      "Chris Xiaoxuan Lu",
      "Xiaowei Huang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14602"
  },
  {
    "id": "arXiv:2209.14603",
    "title": "Dataset Distillation for Medical Dataset Sharing",
    "abstract": "Sharing medical datasets between hospitals is challenging because of the\nprivacy-protection problem and the massive cost of transmitting and storing\nmany high-resolution medical images. However, dataset distillation can\nsynthesize a small dataset such that models trained on it achieve comparable\nperformance with the original large dataset, which shows potential for solving\nthe existing medical sharing problems. Hence, this paper proposes a novel\ndataset distillation-based method for medical dataset sharing. Experimental\nresults on a COVID-19 chest X-ray image dataset show that our method can\nachieve high detection performance even using scarce anonymized chest X-ray\nimages.",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14603"
  },
  {
    "id": "arXiv:2209.14609",
    "title": "Dataset Distillation using Parameter Pruning",
    "abstract": "The acquisition of advanced models relies on large datasets in many fields,\nwhich makes storing datasets and training models expensive. As a solution,\ndataset distillation can synthesize a small dataset such that models trained on\nit achieve high performance on par with the original large dataset. The\nrecently proposed dataset distillation method by matching network parameters\nhas been proved effective for several datasets. However, a few parameters in\nthe distillation process are difficult to match, which harms the distillation\nperformance. Based on this observation, this paper proposes a new method to\nsolve the problem using parameter pruning. The proposed method can synthesize\nmore robust distilled datasets and improve the distillation performance by\npruning difficult-to-match parameters in the distillation process. Experimental\nresults on three datasets show that the proposed method outperformed other SOTA\ndataset distillation methods.",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14609"
  },
  {
    "id": "arXiv:2209.14610",
    "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured  Mathematical Reasoning",
    "abstract": "Mathematical reasoning, a core ability of human intelligence, presents unique\nchallenges for machines in abstract thinking and logical reasoning. Recent\nlarge pre-trained language models such as GPT-3 have achieved remarkable\nprogress on mathematical reasoning tasks written in text form, such as math\nword problems (MWP). However, it is unknown if the models can handle more\ncomplex problems that involve math reasoning over heterogeneous information,\nsuch as tabular data. To fill the gap, we present Tabular Math Word Problems\n(TabMWP), a new dataset containing 38,431 open-domain grade-level problems that\nrequire mathematical reasoning on both textual and tabular data. Each question\nin TabMWP is aligned with a tabular context, which is presented as an image,\nsemi-structured text, and a structured table. There are two types of questions:\nfree-text and multi-choice, and each problem is annotated with gold solutions\nto reveal the multi-step reasoning process. We evaluate different pre-trained\nmodels on TabMWP, including the GPT-3 model in a few-shot setting. As earlier\nstudies suggest, since few-shot GPT-3 relies on the selection of in-context\nexamples, its performance is unstable and can degrade to near chance. The\nunstable issue is more severe when handling complex problems like TabMWP. To\nmitigate this, we further propose a novel approach, PromptPG, which utilizes\npolicy gradient to learn to select in-context examples from a small amount of\ntraining data and then constructs the corresponding prompt for the test\nexample. Experimental results show that our method outperforms the best\nbaseline by 5.31% on the accuracy metric and reduces the prediction variance\nsignificantly compared to random selection, which verifies its effectiveness in\nthe selection of in-context examples.",
    "descriptor": "\nComments: 24 pages, 18 figures, 8 tables. The data and code will be available at this https URL\n",
    "authors": [
      "Pan Lu",
      "Liang Qiu",
      "Kai-Wei Chang",
      "Ying Nian Wu",
      "Song-Chun Zhu",
      "Tanmay Rajpurohit",
      "Peter Clark",
      "Ashwin Kalyan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14610"
  },
  {
    "id": "arXiv:2209.14613",
    "title": "Proportional Multicalibration",
    "abstract": "Multicalibration is a desirable fairness criteria that constrains calibration\nerror among flexibly-defined groups in the data while maintaining overall\ncalibration. However, when outcome probabilities are correlated with group\nmembership, multicalibrated models can exhibit a higher percent calibration\nerror among groups with lower base rates than groups with higher base rates. As\na result, it remains possible for a decision-maker to learn to trust or\ndistrust model predictions for specific groups. To alleviate this, we propose\nproportional multicalibration, a criteria that constrains the percent\ncalibration error among groups and within prediction bins. We prove that\nsatisfying proportional multicalibration bounds a model's multicalibration as\nwell its differential calibration, a stronger fairness criteria inspired by the\nfairness notion of sufficiency. We provide an efficient algorithm for\npost-processing risk prediction models for proportional multicalibration and\nevaluate it empirically. We conduct simulation studies and investigate a\nreal-world application of PMC-postprocessing to prediction of emergency\ndepartment patient admissions. We observe that proportional multicalibration is\na promising criteria for controlling simultenous measures of calibration\nfairness of a model over intersectional groups with virtually no cost in terms\nof classification performance.",
    "descriptor": "",
    "authors": [
      "William La Cava",
      "Elle Lett",
      "Guangya Wan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.14613"
  },
  {
    "id": "arXiv:2209.14614",
    "title": "COMPILING: A Benchmark Dataset for Chinese Complexity Controllable  Definition Generation",
    "abstract": "The definition generation task aims to generate a word's definition within a\nspecific context automatically. However, owing to the lack of datasets for\ndifferent complexities, the definitions produced by models tend to keep the\nsame complexity level. This paper proposes a novel task of generating\ndefinitions for a word with controllable complexity levels. Correspondingly, we\nintroduce COMPILING, a dataset given detailed information about Chinese\ndefinitions, and each definition is labeled with its complexity levels. The\nCOMPILING dataset includes 74,303 words and 106,882 definitions. To the best of\nour knowledge, it is the largest dataset of the Chinese definition generation\ntask. We select various representative generation methods as baselines for this\ntask and conduct evaluations, which illustrates that our dataset plays an\noutstanding role in assisting models in generating different complexity-level\ndefinitions. We believe that the COMPILING dataset will benefit further\nresearch in complexity controllable definition generation.",
    "descriptor": "\nComments: Accepted by CCL 2022\n",
    "authors": [
      "Jiaxin Yuan",
      "Cunliang Kong",
      "Chenhui Xie",
      "Liner Yang",
      "Erhong Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14614"
  },
  {
    "id": "arXiv:2209.14624",
    "title": "Is Complexity Required for Neural Network Pruning? A Case Study on  Global Magnitude Pruning",
    "abstract": "Pruning neural networks has become popular in the last decade when it was\nshown that a large number of weights can be safely removed from modern neural\nnetworks without compromising accuracy. Numerous pruning methods have been\nproposed since then, each claiming to be better than the previous. Many\nstate-of-the-art (SOTA) techniques today rely on complex pruning methodologies\nutilizing importance scores, getting feedback through back-propagation or\nhaving heuristics-based pruning rules amongst others. We question this pattern\nof introducing complexity in order to achieve better pruning results. We\nbenchmark these SOTA techniques against Global Magnitude Pruning (Global MP), a\nnaive pruning baseline, to evaluate whether complexity is really needed to\nachieve higher performance. Global MP ranks weights in order of their\nmagnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one\nof the simplest pruning techniques. Surprisingly, we find that vanilla Global\nMP outperforms all the other SOTA techniques and achieves a new SOTA result. It\nalso achieves good performance on FLOPs sparsification, which we find is\nenhanced, when pruning is conducted in a gradual fashion. We also find that\nGlobal MP is generalizable across tasks, datasets and models with superior\nperformance. Moreover, a common issue that many pruning algorithms run into at\nhigh sparsity rates, namely, layer-collapse, can be easily fixed in Global MP\nby setting a minimum threshold of weights to be retained in each layer. Lastly,\nunlike many other SOTA techniques, Global MP does not require any additional\nalgorithm specific hyper-parameters and is very straightforward to tune and\nimplement. We showcase our findings on various models (WRN-28-8, ResNet-32,\nResNet-50, MobileNet-V1 and FastGRNN) and multiple datasets (CIFAR-10, ImageNet\nand HAR-2). Code is available at https://github.com/manasgupta-1/GlobalMP.",
    "descriptor": "",
    "authors": [
      "Manas Gupta",
      "Efe Camci",
      "Vishandi Rudy Keneta",
      "Abhishek Vaidyanathan",
      "Ritwik Kanodia",
      "Chuan-Sheng Foo",
      "Wu Min",
      "Lin Jie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14624"
  },
  {
    "id": "arXiv:2209.14627",
    "title": "An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation",
    "abstract": "Open-domain dialogue systems aim to interact with humans through natural\nlanguage texts in an open-ended fashion. However, the widely successful neural\nnetworks may not work well for dialogue systems, as they tend to generate\ngeneric responses. In this work, we propose an Equal-size Hard\nExpectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model\nfor diverse dialogue generation. Our algorithm assigns a sample to a decoder in\na hard manner and additionally imposes an equal-assignment constraint to ensure\nthat all decoders are well-trained. We provide detailed theoretical analysis to\njustify our approach. Further, experiments on two large-scale, open-domain\ndialogue datasets verify that our EqHard-EM algorithm generates high-quality\ndiverse responses.",
    "descriptor": "",
    "authors": [
      "Yuqiao Wen",
      "Yongchang Hao",
      "Yanshuai Cao",
      "Lili Mou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14627"
  },
  {
    "id": "arXiv:2209.14634",
    "title": "Hard thresholding hyperinterpolation over general regions",
    "abstract": "We propose a fully discrete hard thresholding polynomial approximation over a\ngeneral region, named hard thresholding hyperinterpolation (HTH). This\napproximation is a weighted $\\ell_0$-regularized discrete least squares\napproximation under the same conditions of hyperinterpolation. Given an\northonormal basis of a polynomial space of total-degree not exceeding $L$ and\nin view of exactness of a quadrature formula at degree $2L$, HTH approximates\nthe Fourier coefficients of a continuous function and obtains its coefficients\nby acting a hard thresholding operator on all approximated Fourier\ncoefficients. HTH is an efficient tool to deal with noisy data because of the\nbasis element selection ability. The main results of HTH for continuous and\nsmooth functions are twofold: the $L_2$ norm of HTH operator is bounded\nindependently of the polynomial degree; and the $L_2$ error bound of HTH is\ngreater than that of hyperinterpolation but HTH performs well in denoising. We\nconclude with some numerical experiments to demonstrate the denoising ability\nof HTH over intervals, discs, spheres, spherical triangles and cubes.",
    "descriptor": "\nComments: 23 pages, 5 figures\n",
    "authors": [
      "Congpei An",
      "Jia-Shu Ran"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14634"
  },
  {
    "id": "arXiv:2209.14635",
    "title": "Compressed Gastric Image Generation Based on Soft-Label Dataset  Distillation for Medical Data Sharing",
    "abstract": "Background and objective: Sharing of medical data is required to enable the\ncross-agency flow of healthcare information and construct high-accuracy\ncomputer-aided diagnosis systems. However, the large sizes of medical datasets,\nthe massive amount of memory of saved deep convolutional neural network (DCNN)\nmodels, and patients' privacy protection are problems that can lead to\ninefficient medical data sharing. Therefore, this study proposes a novel\nsoft-label dataset distillation method for medical data sharing. Methods: The\nproposed method distills valid information of medical image data and generates\nseveral compressed images with different data distributions for anonymous\nmedical data sharing. Furthermore, our method can extract essential weights of\nDCNN models to reduce the memory required to save trained models for efficient\nmedical data sharing. Results: The proposed method can compress tens of\nthousands of images into several soft-label images and reduce the size of a\ntrained model to a few hundredths of its original size. The compressed images\nobtained after distillation have been visually anonymized; therefore, they do\nnot contain the private information of the patients. Furthermore, we can\nrealize high-detection performance with a small number of compressed images.\nConclusions: The experimental results show that the proposed method can improve\nthe efficiency and security of medical data sharing.",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14635"
  },
  {
    "id": "arXiv:2209.14637",
    "title": "Tensor-Based Sketching Method for the Low-Rank Approximation of Data  Streams",
    "abstract": "Low-rank approximation in data streams is a fundamental and significant task\nin computing science, machine learning and statistics. Multiple streaming\nalgorithms have emerged over years and most of them are inspired by randomized\nalgorithms, more specifically, sketching methods. However, many algorithms are\nnot able to leverage information of data streams and consequently suffer from\nlow accuracy. Existing data-driven methods improve accuracy but the training\ncost is expensive in practice. In this paper, from a subspace perspective, we\npropose a tensor-based sketching method for low-rank approximation of data\nstreams. The proposed algorithm fully exploits the structure of data streams\nand obtains quasi-optimal sketching matrices by performing tensor decomposition\non training data. A series of experiments are carried out and show that the\nproposed tensor-based method can be more accurate and much faster than the\nprevious work.",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "Cuiyu Liu",
      "Chuanfu Xiao",
      "Mingshuo Ding",
      "Chao Yang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14637"
  },
  {
    "id": "arXiv:2209.14641",
    "title": "Scaling transformation of the multimode nonlinear Schr\u00f6dinger equation  for physics-informed neural networks",
    "abstract": "Single-mode optical fibers (SMFs) have become the backbone of modern\ncommunication systems. However, their throughput is expected to reach its\ntheoretical limit in the nearest future. Utilization of multimode fibers (MMFs)\nis considered as one of the most promising solutions rectifying this capacity\ncrunch. Nevertheless, differential equations describing light propagation in\nMMFs are a way more sophisticated than those for SMFs, which makes numerical\nmodelling of MMF-based systems computationally demanding and impractical for\nthe most part of realistic scenarios. Physics-informed neural networks (PINNs)\nare known to outperform conventional numerical approaches in various domains\nand have been successfully applied to the nonlinear Schr\\\"odinger equation\n(NLSE) describing light propagation in SMFs. A comprehensive study on\napplication of PINN to the multimode NLSE (MMNLSE) is still lacking though. To\nthe best of our knowledge, this paper is the first to deploy the paradigm of\nPINN for MMNLSE and to demonstrate that a straightforward implementation of\nPINNs by analogy with NLSE does not work out. We pinpoint all issues hindering\nPINN convergence and introduce a novel scaling transformation for the\nzero-order dispersion coefficient that makes PINN capture all relevant physical\neffects. Our simulations reveal good agreement with the split-step Fourier\n(SSF) method and extend numerically attainable propagation lengths up to\nseveral hundred meters. All major limitations are also highlighted.",
    "descriptor": "",
    "authors": [
      "Ivan Chuprov",
      "Dmitry Efremenko",
      "Jiexing Gao",
      "Pavel Anisimov",
      "Viacheslav Zemlyakov"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2209.14641"
  },
  {
    "id": "arXiv:2209.14642",
    "title": "A Coarse-to-fine Cascaded Evidence-Distillation Neural Network for  Explainable Fake News Detection",
    "abstract": "Existing fake news detection methods aim to classify a piece of news as true\nor false and provide veracity explanations, achieving remarkable performances.\nHowever, they often tailor automated solutions on manual fact-checked reports,\nsuffering from limited news coverage and debunking delays. When a piece of news\nhas not yet been fact-checked or debunked, certain amounts of relevant raw\nreports are usually disseminated on various media outlets, containing the\nwisdom of crowds to verify the news claim and explain its verdict. In this\npaper, we propose a novel Coarse-to-fine Cascaded Evidence-Distillation\n(CofCED) neural network for explainable fake news detection based on such raw\nreports, alleviating the dependency on fact-checked ones. Specifically, we\nfirst utilize a hierarchical encoder for web text representation, and then\ndevelop two cascaded selectors to select the most explainable sentences for\nverdicts on top of the selected top-K reports in a coarse-to-fine manner.\nBesides, we construct two explainable fake news datasets, which are publicly\navailable. Experimental results demonstrate that our model significantly\noutperforms state-of-the-art baselines and generates high-quality explanations\nfrom diverse evaluation perspectives.",
    "descriptor": "\nComments: Accepted by COLING 2022. The 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea\n",
    "authors": [
      "Zhiwei Yang",
      "Jing Ma",
      "Hechang Chen",
      "Hongzhan Lin",
      "Ziyang Luo",
      "Yi Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14642"
  },
  {
    "id": "arXiv:2209.14644",
    "title": "Increasing Model Generalizability for Unsupervised Domain Adaptation",
    "abstract": "A dominant approach for addressing unsupervised domain adaptation is to map\ndata points for the source and the target domains into an embedding space which\nis modeled as the output-space of a shared deep encoder. The encoder is trained\nto make the embedding space domain-agnostic to make a source-trained classifier\ngeneralizable on the target domain. A secondary mechanism to improve UDA\nperformance further is to make the source domain distribution more compact to\nimprove model generalizability. We demonstrate that increasing the interclass\nmargins in the embedding space can help to develop a UDA algorithm with\nimproved performance. We estimate the internally learned multi-modal\ndistribution for the source domain, learned as a result of pretraining, and use\nit to increase the interclass class separation in the source domain to reduce\nthe effect of domain shift. We demonstrate that using our approach leads to\nimproved model generalizability on four standard benchmark UDA image\nclassification datasets and compares favorably against exiting methods.",
    "descriptor": "\nComments: Presented 2022 Conference on Lifelong Learning Agents\n",
    "authors": [
      "Mohammad Rostami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14644"
  },
  {
    "id": "arXiv:2209.14645",
    "title": "Reducing Stress and Anxiety in the Metaverse: A Systematic Review of  Meditation, Mindfulness and Virtual Reality",
    "abstract": "Meditation, or mindfulness, is widely used to improve mental health. With the\nemergence of Virtual Reality technology, many studies have provided evidence\nthat meditation with VR can bring health benefits. However, to our knowledge,\nthere are no guidelines and comprehensive reviews in the literature on how to\nconduct such research in virtual reality. In order to understand the role of VR\ntechnology in meditation and future research opportunities, we conducted a\nsystematic literature review in the IEEE and ACM databases. Our process yielded\n19 eligible papers and we conducted a structured analysis. We understand the\nstate-of-art of meditation type, design consideration and VR and technology\nthrough these papers and conclude research opportunities and challenges for the\nfuture.",
    "descriptor": "",
    "authors": [
      "Xian Wang",
      "Xiaoyu Mo",
      "Mingming Fan",
      "Lik-Hang Lee",
      "Bertram E. Shi",
      "Pan Hui"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14645"
  },
  {
    "id": "arXiv:2209.14647",
    "title": "Bounded Future MS-TCN++ for surgical gesture recognition",
    "abstract": "In recent times there is a growing development of video based applications\nfor surgical purposes. Part of these applications can work offline after the\nend of the procedure, other applications must react immediately. However, there\nare cases where the response should be done during the procedure but some delay\nis acceptable. In the literature, the online-offline performance gap is known.\nOur goal in this study was to learn the performance-delay trade-off and design\nan MS-TCN++-based algorithm that can utilize this trade-off. To this aim, we\nused our open surgery simulation data-set containing 96 videos of 24\nparticipants that perform a suturing task on a variable tissue simulator. In\nthis study, we used video data captured from the side view. The Networks were\ntrained to identify the performed surgical gestures. The naive approach is to\nreduce the MS-TCN++ depth, as a result, the receptive field is reduced, and\nalso the number of required future frames is also reduced. We showed that this\nmethod is sub-optimal, mainly in the small delay cases. The second method was\nto limit the accessible future in each temporal convolution. This way, we have\nflexibility in the network design and as a result, we achieve significantly\nbetter performance than in the naive approach.",
    "descriptor": "\nComments: 17 pages, 7 figures, 1 table, Accepted to ECCV-MCV\n",
    "authors": [
      "Adam Goldbraikh",
      "Netanell Avisdris",
      "Carla M. Pugh",
      "Shlomi Laufer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14647"
  },
  {
    "id": "arXiv:2209.14649",
    "title": "Factor Graph Fusion of Raw GNSS Sensing with IMU and Lidar for Precise  Robot Localization without a Base Station",
    "abstract": "Accurate localization is a core component of a robot's navigation system. To\nthis end, global navigation satellite systems (GNSS) can provide absolute\nmeasurements outdoors and, therefore, eliminate long-term drift. However,\nfusing GNSS data with other sensor data is not trivial, especially when a robot\nmoves between areas with and without sky view. We propose a robust approach\nthat tightly fuses raw GNSS receiver data with inertial measurements and,\noptionally, lidar observations for precise and smooth mobile robot\nlocalization. A factor graph with two types of GNSS factors is proposed. First,\nfactors based on pseudoranges, which allow for global localization on Earth.\nSecond, factors based on carrier phases, which enable highly accurate relative\nlocalization, which is useful when other sensing modalities are challenged.\nUnlike traditional differential GNSS, this approach does not require a\nconnection to a base station. On a public urban driving dataset, our approach\nachieves accuracy comparable to a state-of-the-art algorithm that fuses visual\ninertial odometry with GNSS data -- despite our approach not using the camera,\njust inertial and GNSS data. We also demonstrate the robustness of our approach\nusing data from a car and a quadruped robot moving in environments with little\nsky visibility, such as a forest. The accuracy in the global Earth frame is\nstill 1-2 m, while the estimated trajectories are discontinuity-free and\nsmooth. We also show how lidar measurements can be tightly integrated. We\nbelieve this is the first system that fuses raw GNSS observations (as opposed\nto fixes) with lidar.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Jonas Beuchert",
      "Marco Camurri",
      "Maurice Fallon"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14649"
  },
  {
    "id": "arXiv:2209.14652",
    "title": "Backflipping with Miniature Quadcopters by Gaussian Process Based  Control and Planning",
    "abstract": "The paper proposes two control methods for performing a backflip maneuver\nwith miniature quadcopters. First, an existing feedforward control strategy\ndesigned specifically for backflipping is revised and improved. Bayesian\noptimization with surrogate Gaussian Process model is applied to find the\noptimal sequence of motion primitives by performing the flip maneuver\nrepeatedly in a simulation environment. The second method is based on\nclosed-loop control and it consists of two main steps: first a novel robust,\nadaptive controller is designed to provide reliable reference tracking even in\nthe case of model uncertainties. The controller is constructed by augmenting\nthe nominal model of the drone with a Gaussian Process that is tuned by\nmeasurement data. Second, an efficient trajectory planning algorithm is\nproposed, which designs feasible trajectories for the backflip maneuver by\nusing only quadratic programming. The two approaches are analyzed in\nsimulations and in real experiments using Bitcraze Crazyflie 2.1 quadcopters.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Control Systems Technology (2022)\n",
    "authors": [
      "P\u00e9ter Antal",
      "Tam\u00e1s P\u00e9ni",
      "Roland T\u00f3th"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14652"
  },
  {
    "id": "arXiv:2209.14655",
    "title": "Are we building the data discovery infrastructure researchers want?  Comparing perspectives of support specialists and researchers",
    "abstract": "Data discovery practices currently tend to be studied from the perspective of\nresearchers or the perspective of support specialists. This separation is\nproblematic, as it becomes easy for support specialists to build\ninfrastructures and services based on perceptions of researchers' practices,\nrather than the practices themselves. This paper brings together and analyze\nboth perspectives to support the building of effective infrastructures and\nservices for data discovery. This is a meta-synthesis of work the authors have\nconducted over the last six years investigating the data discovery practices of\nresearchers and support specialists, like data librarians. We bring together\ndata collected from in-depth interview studies with 6 support specialists in\nthe field of social science in Germany, with 21 social scientists in Singapore,\nan interview with 10 researchers and 3 support specialists from multiple\ndisciplines, a global survey with 1630 researchers and 47 support specialists\nfrom multiple disciplines, an observational study with 12 researchers from the\nfield of social science and a use case analysis of 25 support specialists from\nmultiple disciplines. We found that while there are many similarities in what\nresearchers and support specialists want and think about data discovery, there\nare some differences we have identified, most notably the interconnection of\ndata discovery with web search, literature search and social networks. We\nconclude by proposing recommendations for how different types of support work\ncan address these points of difference to better support researchers' data\ndiscovery practices.",
    "descriptor": "",
    "authors": [
      "Guangyuan Sun",
      "Tanja Friedrich",
      "Kathleen Gregory",
      "Brigitte Mathiak"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2209.14655"
  },
  {
    "id": "arXiv:2209.14662",
    "title": "A dichotomy for succinct representations of homomorphisms",
    "abstract": "The task of computing homomorphisms between two finite relational structures\n$\\mathcal{A}$ and $\\mathcal{B}$ is a well-studied question with numerous\napplications. Since the set $\\operatorname{Hom}(\\mathcal{A},\\mathcal{B})$ of\nall homomorphisms may be very large having a method of representing it in a\nsuccinct way, especially one which enables us to perform efficient enumeration\nand counting, could be extremely useful.\nOne simple yet powerful way of doing so is to decompose\n$\\operatorname{Hom}(\\mathcal{A},\\mathcal{B})$ using union and Cartesian\nproduct. Such data structures, called d-representations, have been introduced\nby Olteanu and Zavodny in the context of database theory. Their results also\nimply that if the treewidth of the left-hand side structure $\\mathcal{A}$ is\nbounded, then a d-representation of polynomial size can be found in polynomial\ntime. We show that for structures of bounded arity this is optimal: if the\ntreewidth is unbounded then there are instances where the size of any\nd-representation is superpolynomial. Along the way we develop tools for proving\nlower bounds on the size of d-representations, in particular we define a notion\nof reduction suitable for this context and prove an almost tight lower bound on\nthe size of d-representations of all $k$-cliques in a graph.",
    "descriptor": "",
    "authors": [
      "Christoph Berkholz",
      "Harry Vinall-Smeeth"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.14662"
  },
  {
    "id": "arXiv:2209.14667",
    "title": "Domain-aware Self-supervised Pre-training for Label-Efficient Meme  Analysis",
    "abstract": "Existing self-supervised learning strategies are constrained to either a\nlimited set of objectives or generic downstream tasks that predominantly target\nuni-modal applications. This has isolated progress for imperative multi-modal\napplications that are diverse in terms of complexity and domain-affinity, such\nas meme analysis. Here, we introduce two self-supervised pre-training methods,\nnamely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal\nhate-speech data during pre-training and (ii) perform self-supervised learning\nby incorporating multiple specialized pretext tasks, effectively catering to\nthe required complex multi-modal representation learning for meme analysis. We\nexperiment with different self-supervision strategies, including potential\nvariants that could help learn rich cross-modality representations and evaluate\nusing popular linear probing on the Hateful Memes task. The proposed solutions\nstrongly compete with the fully supervised baseline via label-efficient\ntraining while distinctly outperforming them on all three tasks of the Memotion\nchallenge with 0.18%, 23.64%, and 0.93% performance gain, respectively.\nFurther, we demonstrate the generalizability of the proposed solutions by\nreporting competitive performance on the HarMeme task. Finally, we empirically\nestablish the quality of the learned representations by analyzing task-specific\nlearning, using fewer labeled training samples, and arguing that the complexity\nof the self-supervision strategy and downstream task at hand are correlated.\nOur efforts highlight the requirement of better multi-modal self-supervision\nmethods involving specialized pretext tasks for efficient fine-tuning and\ngeneralizable performance.",
    "descriptor": "\nComments: Accepted at AACL-IJCNLP 2022 main conference. 9 Pages (main content); 6 Figures; 5 Tables and an Appendix\n",
    "authors": [
      "Shivam Sharma",
      "Mohd Khizir Siddiqui",
      "Md. Shad Akhtar",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2209.14667"
  },
  {
    "id": "arXiv:2209.14670",
    "title": "Towards Equalised Odds as Fairness Metric in Academic Performance  Prediction",
    "abstract": "The literature for fairness-aware machine learning knows a plethora of\ndifferent fairness notions. It is however wellknown, that it is impossible to\nsatisfy all of them, as certain notions contradict each other. In this paper,\nwe take a closer look at academic performance prediction (APP) systems and try\nto distil which fairness notions suit this task most. For this, we scan recent\nliterature proposing guidelines as to which fairness notion to use and apply\nthese guidelines onto APP. Our findings suggest equalised odds as most suitable\nnotion for APP, based on APP's WYSIWYG worldview as well as potential long-term\nimprovements for the population.",
    "descriptor": "\nComments: FATED'22: 2nd Workshop on Fairness, Accountability, and Transparency in Educational Data. July 2022. Durham, England\n",
    "authors": [
      "Jannik Dunkelau",
      "Manh Khoi Duong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.14670"
  },
  {
    "id": "arXiv:2209.14673",
    "title": "Chameleon Cache: Approximating Fully Associative Caches with Random  Replacement to Prevent Contention-Based Cache Attacks",
    "abstract": "Randomized, skewed caches (RSCs) such as CEASER-S have recently received much\nattention to defend against contention-based cache side channels. By\nrandomizing and regularly changing the mapping(s) of addresses to cache sets,\nthese techniques are designed to obfuscate the leakage of memory access\npatterns. However, new attack techniques, e.g., Prime+Prune+Probe, soon\ndemonstrated the limits of RSCs as they allow attackers to more quickly learn\nwhich addresses contend in the cache and use this information to circumvent the\nrandomization. To yet maintain side-channel resilience, RSCs must change the\nrandom mapping(s) more frequently with adverse effects on performance and\nimplementation complexity. This work aims to make randomization-based\napproaches more robust to allow for reduced re-keying rates and presents\nChameleon Cache. Chameleon Cache extends RSCs with a victim cache (VC) to\ndecouple contention in the RSC from evictions observed by the user. The VC\nallows Chameleon Cache to make additional use of the multiple mappings RSCs\nprovide to translate addresses to cache set indices: when a cache line is\nevicted from the RSC to the VC under one of its mappings, the VC automatically\nreinserts this evicted line back into the RSC by using a different mapping. As\na result, the effects of previous RSC set contention are hidden and Chameleon\nCache exhibits side-channel resistance and eviction patterns similar to fully\nassociative caches with random replacement. We show that Chameleon Cache has\nperformance overheads of < 1% and stress that VCs are more generically helpful\nto increase side-channel resistance and re-keying intervals of randomized\ncaches.",
    "descriptor": "\nComments: 12 pages, 9 figures, 6 algorithms, 1 table\n",
    "authors": [
      "Thomas Unterluggauer",
      "Austin Harris",
      "Scott Constable",
      "Fangfei Liu",
      "Carlos Rozas"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.14673"
  },
  {
    "id": "arXiv:2209.14684",
    "title": "A canonical correlation-based framework for performance analysis of  radio access networks",
    "abstract": "Data driven optimization and machine learning based performance diagnostics\nof radio access networks entails significant challenges arising not only from\nthe nature of underlying data sources but also due to complex spatio-temporal\nrelationships and interdependencies between cells due to user mobility and\nvarying traffic patterns. We discuss how to study these configuration and\nperformance management data sets and identify relationships between cells in\nterms of key performance indicators using multivariate analysis. To this end,\nwe leverage a novel framework based on canonical correlation analysis (CCA),\nwhich is a highly effective method for not only dimensionality reduction but\nalso for analyzing relationships across different sets of multivariate data. As\na case study, we discuss energy saving use-case based on cell shutdown in\ncommercial cellular networks, where we apply CCA to analyze the impact of\ncapacity cell shutdown on the KPIs of coverage cell in the same sector. Data\nfrom LTE Network is used to analyzed example case. We conclude that CCA is a\nviable approach for identifying key relationships not only between network\nplanning and configuration data, but also dynamic performance data, paving the\nway for endeavors such as dimensionality reduction, performance analysis, and\nroot cause analysis for performance diagnostics.",
    "descriptor": "\nComments: Accepted in Globecom 2022 Workshop - NetMan6G\n",
    "authors": [
      "Furqan Ahmed",
      "Muhammad Zeeshan Asghar",
      "Jyri H\u00e4m\u00e4l\u00e4inen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14684"
  },
  {
    "id": "arXiv:2209.14685",
    "title": "Self-stabilizing Total-order Broadcast",
    "abstract": "The problem of total-order (uniform reliable) broadcast is fundamental in\nfault-tolerant distributed computing since it abstracts a broad set of problems\nrequiring processes to uniformly deliver messages in the same order in which\nthey were sent. Existing solutions (that tolerate process failures) reduce the\ntotal-order broadcast problem to the one of multivalued consensus.\nOur study aims at the design of an even more reliable solution. We do so\nthrough the lenses of self-stabilization-a very strong notion of fault\ntolerance. In addition to node and communication failures, self-stabilizing\nalgorithms can recover after the occurrence of arbitrary transient faults;\nthese faults represent any violation of the assumptions according to which the\nsystem was designed to operate (as long as the algorithm code stays intact).\nThis work proposes the first (to the best of our knowledge) self-stabilizing\nalgorithm for total-order (uniform reliable) broadcast for asynchronous\nmessage-passing systems prone to process failures and transient faults. As we\nshow, the proposed solution facilitates the elegant construction of\nself-stabilizing state-machine replication using bounded memory.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2104.03129\n",
    "authors": [
      "Oskar Lundstr\u00f6m",
      "Michel Raynal",
      "Elad Michael Schiller"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.14685"
  },
  {
    "id": "arXiv:2209.14688",
    "title": "An Inductive Construction for Many-Valued Coalgebraic Modal Logic",
    "abstract": "In this paper, we present an abstract framework of many-valued modal logic\nwith the interpretation of atomic propositions and modal operators as predicate\nlifting over coalgebras for an endofunctor on the category of sets. It\ngeneralizes Pattinson's stratification method for colagebraic modal logic to\nthe many-valued setting. In contrast to standard techniques of canonical model\nconstruction and filtration, this method employs an induction principle to\nprove the soundness, completeness, and finite model property of the logics. As\na consequence, we can lift a restriction on the previous approach~\n\\cite{Lin2022} that requires the underlying language must have the expressive\npower to internalize the meta-level truth valuation operations.",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Chun-Yu Lin",
      "Churn-Jung Liau"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.14688"
  },
  {
    "id": "arXiv:2209.14690",
    "title": "Prompt-guided Scene Generation for 3D Zero-Shot Learning",
    "abstract": "Zero-shot learning on 3D point cloud data is a related underexplored problem\ncompared to its 2D image counterpart. 3D data brings new challenges for ZSL due\nto the unavailability of robust pre-trained feature extraction models. To\naddress this problem, we propose a prompt-guided 3D scene generation and\nsupervision method that augments 3D data to learn the network better, exploring\nthe complex interplay of seen and unseen objects. First, we merge point clouds\nof two 3D models in certain ways described by a prompt. The prompt acts like\nthe annotation describing each 3D scene. Later, we perform contrastive learning\nto train our proposed architecture in an end-to-end manner. We argue that 3D\nscenes can relate objects more efficiently than single objects because popular\nlanguage models (like BERT) can achieve high performance when objects appear in\na context. Our proposed prompt-guided scene generation method encapsulates data\naugmentation and prompt-based annotation/captioning to improve 3D ZSL\nperformance. We have achieved state-of-the-art ZSL and generalized ZSL\nperformance on synthetic (ModelNet40, ModelNet10) and real-scanned\n(ScanOjbectNN) 3D object datasets.",
    "descriptor": "",
    "authors": [
      "Majid Nasiri",
      "Ali Cheraghian",
      "Townim Faisal Chowdhury",
      "Sahar Ahmadi",
      "Morteza Saberi",
      "Shafin Rahman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14690"
  },
  {
    "id": "arXiv:2209.14692",
    "title": "Digital and Physical Face Attacks: Reviewing and One Step Further",
    "abstract": "With the rapid progress over the past five years, face authentication has\nbecome the most pervasive biometric recognition method. Thanks to the\nhigh-accuracy recognition performance and user-friendly usage, automatic face\nrecognition (AFR) has exploded into a plethora of practical applications over\ndevice unlocking, checking-in, and financial payment. In spite of the\ntremendous success of face authentication, a variety of face presentation\nattacks (FPA), such as print attacks, replay attacks, and 3D mask attacks, have\nraised pressing mistrust concerns. Besides physical face attacks, face\nvideos/images are vulnerable to a wide variety of digital attack techniques\nlaunched by malicious hackers, causing potential menace to the public at large.\nDue to the unrestricted access to enormous digital face images/videos and\ndisclosed easy-to-use face manipulation tools circulating on the internet,\nnon-expert attackers without any prior professional skills are able to readily\ncreate sophisticated fake faces, leading to numerous dangerous applications\nsuch as financial fraud, impersonation, and identity theft. This survey aims to\nbuild the integrity of face forensics by providing thorough analyses of\nexisting literature and highlighting the issues requiring further attention. In\nthis paper, we first comprehensively survey both physical and digital face\nattack types and datasets. Then, we review the latest and most advanced\nprogress on existing counter-attack methodologies and highlight their current\nlimits. Moreover, we outline possible future research directions for existing\nand upcoming challenges in the face forensics community. Finally, the necessity\nof joint physical and digital face attack detection has been discussed, which\nhas never been studied in previous surveys.",
    "descriptor": "",
    "authors": [
      "Chenqi Kong",
      "Shiqi Wang",
      "Haoliang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14692"
  },
  {
    "id": "arXiv:2209.14694",
    "title": "GROOT: Corrective Reward Optimization for Generative Sequential Labeling",
    "abstract": "Sequential labeling is a fundamental NLP task, forming the backbone of many\napplications. Supervised learning of Seq2Seq models (like T5) has shown great\nsuccess on these problems. However there remains a significant disconnect\nbetween the training objectives of these models vs the metrics and desiderata\nwe care about in practical applications. For example, a practical sequence\ntagging application may want to optimize for a certain precision-recall\ntrade-off (of the top-k predictions) which is quite different from the standard\nobjective of maximizing the likelihood of the gold labeled sequence. Thus to\nbridge this gap, we propose GROOT -- a simple yet effective framework for\nGenerative Reward Optimization Of Text sequences. GROOT works by training a\ngenerative sequential labeling model to match the decoder output distribution\nwith that of the (black-box) reward function. Using an iterative training\nregime, we first generate prediction candidates, then correct errors in them,\nand finally contrast those candidates (based on their reward values). As\ndemonstrated via extensive experiments on four public benchmarks, GROOT\nsignificantly improves all reward metrics. Furthermore, GROOT also leads to\nimprovements of the overall decoder distribution as evidenced by the quality\ngains of the top-$k$ candidates.",
    "descriptor": "",
    "authors": [
      "Kazuma Hashimoto",
      "Karthik Raman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14694"
  },
  {
    "id": "arXiv:2209.14697",
    "title": "Creative Painting with Latent Diffusion Models",
    "abstract": "Artistic painting has achieved significant progress during recent years by\napplying hundreds of GAN variants. However, adversarial training has been\nreported to be notoriously unstable and can lead to mode collapse. Recently,\ndiffusion models have achieved GAN-level sample quality without adversarial\ntraining. Using autoencoders to project the original images into compressed\nlatent spaces and cross attention enhanced U-Net as the backbone of diffusion,\nlatent diffusion models have achieved stable and high fertility image\ngeneration. In this paper, we focus on enhancing the creative painting ability\nof current latent diffusion models in two directions, textual condition\nextension and model retraining with Wikiart dataset. Through textual condition\nextension, users' input prompts are expanded in temporal and spacial directions\nfor deeper understanding and explaining the prompts. Wikiart dataset contains\n80K famous artworks drawn during recent 400 years by more than 1,000 famous\nartists in rich styles and genres. Through the retraining, we are able to ask\nthese artists to draw novel and creative painting on modern topics.",
    "descriptor": "\nComments: 17pages, 12 figures\n",
    "authors": [
      "Xianchao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14697"
  },
  {
    "id": "arXiv:2209.14698",
    "title": "Facial Landmark Predictions with Applications to Metaverse",
    "abstract": "This research aims to make metaverse characters more realistic by adding lip\nanimations learnt from videos in the wild. To achieve this, our approach is to\nextend Tacotron 2 text-to-speech synthesizer to generate lip movements together\nwith mel spectrogram in one pass. The encoder and gate layer weights are\npre-trained on LJ Speech 1.1 data set while the decoder is retrained on 93\nclips of TED talk videos extracted from LRS 3 data set. Our novel decoder\npredicts displacement in 20 lip landmark positions across time, using labels\nautomatically extracted by OpenFace 2.0 landmark predictor. Training converged\nin 7 hours using less than 5 minutes of video. We conducted ablation study for\nPre/Post-Net and pre-trained encoder weights to demonstrate the effectiveness\nof transfer learning between audio and visual speech data.",
    "descriptor": "",
    "authors": [
      "Qiao Han",
      "Jun Zhao",
      "Kwok-Yan Lam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.14698"
  },
  {
    "id": "arXiv:2209.14699",
    "title": "ARQ-based Average Consensus over Unreliable Directed Network Topologies",
    "abstract": "In this paper, we address the problem of discrete-time average consensus,\nwhere agents (nodes) exchange information over unreliable communication links.\nWe enhance the Robustified Ratio Consensus algorithm by embedding the Automatic\nRepeat ReQuest (ARQ) protocol used for error control of data transmissions, in\norder to allow the agents to reach asymptotic average consensus even when\noperating within unreliable directed networks. This strategy, apart from\nhandling time-varying delays induced by retransmissions of erroneous packets\n(that can be captured by the Robustified Ratio Consensus as well), it is also\npossible to handle packet drops that occur due to excess of a predefined packet\nretransmission limit imposed by the ARQ protocol. Invoking the ARQ protocol\nallows nodes to: (a) exploit the incoming error-free acknowledgement feedback\nsignals to initially acquire or later update their out-degree, (b) know whether\na packet has arrived or not, and (c) determine a local upper-bound on the\ndelays which is imposed by the retransmission limit. By augmenting the\nnetwork's corresponding weighted adjacency matrix, to handle time-varying (yet\nbounded) delays and possible packet drops, we show that nodes can make use of\nthe proposed algorithm, herein called the ARQ-based Ratio Consensus algorithm,\nto reach asymptotic average consensus, despite the fact that the communication\nlinks are unreliable. To the best of the authors' knowledge, this is the first\nconsensus algorithm that incorporates a communication protocol for error\ncontrol used in real communication systems with feedback.",
    "descriptor": "",
    "authors": [
      "Evagoras Makridis",
      "Themistoklis Charalambous",
      "Christoforos N. Hadjicostis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14699"
  },
  {
    "id": "arXiv:2209.14703",
    "title": "Lattice Linear Algorithms",
    "abstract": "This paper focuses on analyzing and differentiating between lattice linear\nproblems and algorithms. It introduces a new class of algorithms called\n\\textit{(fully) lattice linear algorithms}. A property of these algorithms is\nthat they induce a partial order among all states and form \\textit{multiple\nlattices}. An initial state locks in one of these lattices. We present a\nlattice linear self-stabilizing algorithm for minimal dominating set.",
    "descriptor": "",
    "authors": [
      "Arya Tanmay Gupta",
      "Sandeep S Kulkarni"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14703"
  },
  {
    "id": "arXiv:2209.14708",
    "title": "TruEyes: Utilizing Microtasks in Mobile Apps for Crowdsourced Labeling  of Machine Learning Datasets",
    "abstract": "The growing use of supervised machine learning in research and industry has\nincreased the need for labeled datasets. Crowdsourcing has emerged as a popular\nmethod to create data labels. However, working on large batches of tasks leads\nto worker fatigue, negatively impacting labeling quality. To address this, we\npresent TruEyes, a collaborative crowdsourcing system, enabling the\ndistribution of micro-tasks to mobile app users. TruEyes allows machine\nlearning practitioners to publish labeling tasks, mobile app developers to\nintegrate task ads for monetization, and users to label data instead of\nwatching advertisements. To evaluate the system, we conducted an experiment\nwith N=296 participants. Our results show that the quality of the labeled data\nis comparable to traditional crowdsourcing approaches and most users prefer\ntask ads over traditional ads. We discuss extensions to the system and address\nhow mobile advertisement space can be used as a productive resource in the\nfuture.",
    "descriptor": "",
    "authors": [
      "Chandramohan Sudar",
      "Michael Froehlich",
      "Florian Alt"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14708"
  },
  {
    "id": "arXiv:2209.14711",
    "title": "Low-Resolution Action Recognition for Tiny Actions Challenge",
    "abstract": "Tiny Actions Challenge focuses on understanding human activities in\nreal-world surveillance. Basically, there are two main difficulties for\nactivity recognition in this scenario. First, human activities are often\nrecorded at a distance, and appear in a small resolution without much\ndiscriminative clue. Second, these activities are naturally distributed in a\nlong-tailed way. It is hard to alleviate data bias for such heavy category\nimbalance. To tackle these problems, we propose a comprehensive recognition\nsolution in this paper. First, we train video backbones with data balance, in\norder to alleviate overfitting in the challenge benchmark. Second, we design a\ndual-resolution distillation framework, which can effectively guide\nlow-resolution action recognition by super-resolution knowledge. Finally, we\napply model en-semble with post-processing, which can further boost\nper-formance on the long-tailed categories. Our solution ranks Top-1 on the\nleaderboard.",
    "descriptor": "\nComments: This article is the report of the CVPR 2022 ActivityNet workshop Tiny Actions Challenge(this https URL). The time of the first submission to the organizers is June 6th\n",
    "authors": [
      "Boyu Chen",
      "Yu Qiao",
      "Yali Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14711"
  },
  {
    "id": "arXiv:2209.14714",
    "title": "Evolving Reference Architecture Description: Guidelines based on  ISO/IEC/IEEE 42010",
    "abstract": "The architectural design of software systems is not a trivial task, requiring\nsometimes large experience and knowledge accumulated for years. Reference\narchitectures have been increasingly adopted as a means to support such task,\nalso contributing to the standardization and evolution of these systems.\nAlthough considerable time and effort are devoted to design these\narchitectures, an outdated description is still found in several of them and,\nas a consequence, resulting in their non-continuation. This article presents\nguidelines to evolve the description of reference architectures, considering\ndifferent types of stakeholders and required tasks. To complement our statement\nthat the guidelines are correct by construction as they were grounded in widely\nknown international standard ISO/IEC/IEEE 42010 and literature, we also briefly\npresent a qualitative analysis comparing the guidelines with an ad hoc way\n(commonly occurred in reference architectures). We believe solutions like these\nguidelines are necessary and could further contribute to the sustainability and\nlongevity of reference architectures.",
    "descriptor": "\nComments: 17 pages, 2 figures, 2 algorithms, 11 tables\n",
    "authors": [
      "Edilson Soares Palma",
      "Elisa Yumi Nakagawa",
      "D\u00e9bora Maria Barroso Paiva",
      "Maria Istela Cagnin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.14714"
  },
  {
    "id": "arXiv:2209.14719",
    "title": "In Search of Projectively Equivariant Neural Networks",
    "abstract": "Equivariance of linear neural network layers is well studied. In this work,\nwe relax the equivariance condition to only be true in a projective sense. In\nparticular, we study the relation of projective and ordinary equivariance and\nshow that for important examples, the problems are in fact equivalent.\nThe rotation group in 3D acts projectively on the projective plane. We\nexperimentally study the practical importance of rotation equivariance when\ndesigning networks for filtering 2D-2D correspondences. Fully equivariant\nmodels perform poorly, and while a simple addition of invariant features to a\nstrong baseline yields improvements, this seems to not be due to improved\nequivariance.",
    "descriptor": "",
    "authors": [
      "Georg B\u00f6kman",
      "Axel Flinth",
      "Fredrik Kahl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14719"
  },
  {
    "id": "arXiv:2209.14727",
    "title": "FastPacket: Towards Pre-trained Packets Embedding based on FastText for  next-generation NIDS",
    "abstract": "New Attacks are increasingly used by attackers everyday but many of them are\nnot detected by Intrusion Detection Systems as most IDS ignore raw packet\ninformation and only care about some basic statistical information extracted\nfrom PCAP files. Using networking programs to extract fixed statistical\nfeatures from packets is good, but may not enough to detect nowadays\nchallenges. We think that it is time to utilize big data and deep learning for\nautomatic dynamic feature extraction from packets. It is time to get inspired\nby deep learning pre-trained models in computer vision and natural language\nprocessing, so security deep learning solutions will have its pre-trained\nmodels on big datasets to be used in future researches. In this paper, we\nproposed a new approach for embedding packets based on character-level\nembeddings, inspired by FastText success on text data. We called this approach\nFastPacket. Results are measured on subsets of CIC-IDS-2017 dataset, but we\nexpect promising results on big data pre-trained models. We suggest building\npre-trained FastPacket on MAWI big dataset and make it available to community,\nsimilar to FastText. To be able to outperform currently used NIDS, to start a\nnew era of packet-level NIDS that can better detect complex attacks.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2209.13961\n",
    "authors": [
      "Khloud Al Jallad"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.14727"
  },
  {
    "id": "arXiv:2209.14733",
    "title": "Hyper-Representations as Generative Models: Sampling Unseen Neural  Network Weights",
    "abstract": "Learning representations of neural network weights given a model zoo is an\nemerging and challenging area with many potential applications from model\ninspection, to neural architecture search or knowledge distillation. Recently,\nan autoencoder trained on a model zoo was able to learn a hyper-representation,\nwhich captures intrinsic and extrinsic properties of the models in the zoo. In\nthis work, we extend hyper-representations for generative use to sample new\nmodel weights. We propose layer-wise loss normalization which we demonstrate is\nkey to generate high-performing models and several sampling methods based on\nthe topology of hyper-representations. The models generated using our methods\nare diverse, performant and capable to outperform strong baselines as evaluated\non several downstream tasks: initialization, ensemble sampling and transfer\nlearning. Our results indicate the potential of knowledge aggregation from\nmodel zoos to new models via hyper-representations thereby paving the avenue\nfor novel research directions.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv admin note: text overlap with arXiv:2207.10951\n",
    "authors": [
      "Konstantin Sch\u00fcrholt",
      "Boris Knyazev",
      "Xavier Gir\u00f3-i-Nieto",
      "Damian Borth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14733"
  },
  {
    "id": "arXiv:2209.14734",
    "title": "DiGress: Discrete Denoising diffusion for graph generation",
    "abstract": "This work introduces DiGress, a discrete denoising diffusion model for\ngenerating graphs with categorical node and edge attributes. Our model defines\na diffusion process that progressively edits a graph with noise (adding or\nremoving edges, changing the categories), and a graph transformer network that\nlearns to revert this process. With these two ingredients in place, we reduce\ndistribution learning over graphs to a simple sequence of classification tasks.\nWe further improve sample quality by proposing a new Markovian noise model that\npreserves the marginal distribution of node and edge types during diffusion,\nand by adding auxiliary graph-theoretic features derived from the noisy graph\nat each diffusion step. Finally, we propose a guidance procedure for\nconditioning the generation on graph-level features. Overall, DiGress achieves\nstate-of-the-art performance on both molecular and non-molecular datasets, with\nup to 3x validity improvement on a dataset of planar graphs. In particular, it\nis the first model that scales to the large GuacaMol dataset containing 1.3M\ndrug-like molecules without using a molecule-specific representation such as\nSMILES or fragments.",
    "descriptor": "\nComments: 22 pages. Preprint, under review\n",
    "authors": [
      "Clement Vignac",
      "Igor Krawczuk",
      "Antoine Siraudin",
      "Bohan Wang",
      "Volkan Cevher",
      "Pascal Frossard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14734"
  },
  {
    "id": "arXiv:2209.14740",
    "title": "The Helmholtz equation with uncertainties in the wavenumber",
    "abstract": "We investigate the Helmholtz equation with suitable boundary conditions and\nuncertainties in the wavenumber. Thus the wavenumber is modeled as a random\nvariable or a random field. We discretize the Helmholtz equation using finite\ndifferences in space, which leads to a linear system of algebraic equations\nincluding random variables. A stochastic Galerkin method yields a deterministic\nlinear system of algebraic equations. This linear system is high-dimensional,\nsparse and complex symmetric but, in general, not hermitian. We therefore solve\nthis system iteratively with GMRES and propose two preconditioners: a complex\nshifted Laplace preconditioner and a mean value preconditioner. Both\npreconditioners reduce the number of iteration steps as well as the computation\ntime in our numerical experiments.",
    "descriptor": "",
    "authors": [
      "Roland Pulch",
      "Olivier S\u00e8te"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14740"
  },
  {
    "id": "arXiv:2209.14742",
    "title": "Learning Gradient-based Mixup towards Flatter Minima for Domain  Generalization",
    "abstract": "To address the distribution shifts between training and test data, domain\ngeneralization (DG) leverages multiple source domains to learn a model that\ngeneralizes well to unseen domains. However, existing DG methods generally\nsuffer from overfitting to the source domains, partly due to the limited\ncoverage of the expected region in feature space. Motivated by this, we propose\nto perform mixup with data interpolation and extrapolation to cover the\npotential unseen regions. To prevent the detrimental effects of unconstrained\nextrapolation, we carefully design a policy to generate the instance weights,\nnamed Flatness-aware Gradient-based Mixup (FGMix). The policy employs a\ngradient-based similarity to assign greater weights to instances that carry\nmore invariant information, and learns the similarity function towards flatter\nminima for better generalization. On the DomainBed benchmark, we validate the\nefficacy of various designs of FGMix and demonstrate its superiority over other\nDG algorithms.",
    "descriptor": "\nComments: 22 pages, 14 figures\n",
    "authors": [
      "Danni Peng",
      "Sinno Jialin Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14742"
  },
  {
    "id": "arXiv:2209.14743",
    "title": "Dataset Complexity Assessment Based on Cumulative Maximum Scaled Area  Under Laplacian Spectrum",
    "abstract": "Dataset complexity assessment aims to predict classification performance on a\ndataset with complexity calculation before training a classifier, which can\nalso be used for classifier selection and dataset reduction. The training\nprocess of deep convolutional neural networks (DCNNs) is iterative and\ntime-consuming because of hyperparameter uncertainty and the domain shift\nintroduced by different datasets. Hence, it is meaningful to predict\nclassification performance by assessing the complexity of datasets effectively\nbefore training DCNN models. This paper proposes a novel method called\ncumulative maximum scaled Area Under Laplacian Spectrum (cmsAULS), which can\nachieve state-of-the-art complexity assessment performance on six datasets.",
    "descriptor": "\nComments: Published as a journal paper at Springer MTAP\n",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14743"
  },
  {
    "id": "arXiv:2209.14745",
    "title": "A Multi-Agent Framework for the Asynchronous and Collaborative Extension  of Multitask ML Systems",
    "abstract": "Tradition ML development methodology does not enable a large number of\ncontributors, each with distinct objectives, to work collectively on the\ncreation and extension of a shared intelligent system. Enabling such a\ncollaborative methodology can accelerate the rate of innovation, increase ML\ntechnologies accessibility and enable the emergence of novel capabilities. We\nbelieve that this can be achieved through the definition of abstraction\nboundaries and a modularized representation of ML models and methods. We\npresent a multi-agent framework for collaborative and asynchronous extension of\ndynamic large-scale multitask intelligent systems.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2209.07326, arXiv:2205.12755\n",
    "authors": [
      "Andrea Gesmundo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multiagent Systems (cs.MA)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.14745"
  },
  {
    "id": "arXiv:2209.14750",
    "title": "Non-contrastive approaches to similarity learning: positive examples are  all you need",
    "abstract": "The similarity learning problem in the oil \\& gas industry aims to construct\na model that estimates similarity between interval measurements for logging\ndata. Previous attempts are mostly based on empirical rules, so our goal is to\nautomate this process and exclude expensive and time-consuming expert\nlabelling.\nOne of the approaches for similarity learning is self-supervised learning\n(SSL). In contrast to the supervised paradigm, this one requires little or no\nlabels for the data. Thus, we can learn such models even if the data labelling\nis absent or scarce. Nowadays, most SSL approaches are contrastive and\nnon-contrastive. However, due to possible wrong labelling of positive and\nnegative samples, contrastive methods don't scale well with the number of\nobjects. Non-contrastive methods don't rely on negative samples. Such\napproaches are actively used in the computer vision.\nWe introduce non-contrastive SSL for time series data. In particular, we\nbuild on top of BYOL and Barlow Twins methods that avoid using negative pairs\nand focus only on matching positive pairs. The crucial part of these methods is\nan augmentation strategy. Different augmentations of time series exist, while\ntheir effect on the performance can be both positive and negative. Our\naugmentation strategies and adaption for BYOL and Barlow Twins together allow\nus to achieve a higher quality (ARI $= 0.49$) than other self-supervised\nmethods (ARI $= 0.34$ only), proving usefulness of the proposed non-contrastive\nself-supervised approach for the interval similarity problem and time series\nrepresentation learning in general.",
    "descriptor": "",
    "authors": [
      "Alexander Marusov",
      "Valerii Baianov",
      "Alexey Zaytsev"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14750"
  },
  {
    "id": "arXiv:2209.14756",
    "title": "Unveiling the Real Performance of LPDDR5 Memories",
    "abstract": "LPDDR5 is the latest low-power DRAM standard and expected to be used in\nvarious application fields. The vendors have published promising peak\nbandwidths up to 50 % higher than those of the predecessor LPDDR4. In this\npaper we evaluate the best-case and worst-case real bandwidth utilization of\ndifferent LPDDR5 configurations and compare the results to corresponding LPDDR4\nconfigurations. We also show that an upgrade from LPDDR4 to LPDDR5 does not\nalways bring a bandwidth advantage and that some LPDDR5 configurations should\nbe avoided for specific workloads.",
    "descriptor": "\nComments: ACM/IEEE International Symposium on Memory Systems (MEMSYS 2022)\n",
    "authors": [
      "Lukas Steiner",
      "Matthias Jung",
      "Michael Huonker",
      "Norbert Wehn"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.14756"
  },
  {
    "id": "arXiv:2209.14757",
    "title": "Speeding Up Action Recognition Using Dynamic Accumulation of Residuals  in Compressed Domain",
    "abstract": "With the widespread use of installed cameras, video-based monitoring\napproaches have seized considerable attention for different purposes like\nassisted living. Temporal redundancy and the sheer size of raw videos are the\ntwo most common problematic issues related to video processing algorithms. Most\nof the existing methods mainly focused on increasing accuracy by exploring\nconsecutive frames, which is laborious and cannot be considered for real-time\napplications. Since videos are mostly stored and transmitted in compressed\nformat, these kinds of videos are available on many devices. Compressed videos\ncontain a multitude of beneficial information, such as motion vectors and\nquantized coefficients. Proper use of this available information can greatly\nimprove the video understanding methods' performance. This paper presents an\napproach for using residual data, available in compressed videos directly,\nwhich can be obtained by a light partially decoding procedure. In addition, a\nmethod for accumulating similar residuals is proposed, which dramatically\nreduces the number of processed frames for action recognition. Applying neural\nnetworks exclusively for accumulated residuals in the compressed domain\naccelerates performance, while the classification results are highly\ncompetitive with raw video approaches.",
    "descriptor": "",
    "authors": [
      "Ali Abdari",
      "Pouria Amirjan",
      "Azadeh Mansouri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14757"
  },
  {
    "id": "arXiv:2209.14761",
    "title": "On the Input-Output Behavior of a Geothermal Energy Storage:  Approximations by Model Order Reduction",
    "abstract": "In this paper we consider a geothermal energy storage in which the\nspatio-temporal temperature distribution is modeled by a heat equation with a\nconvection term. Such storages often are embedded in residential heating\nsystems and control and management require the knowledge of some aggregated\ncharacteristics of that temperature distribution in the storage. They describe\nthe input-output behaviour of the storage and the associated energy flows and\ntheir response to charging and discharging processes. We aim to derive an\nefficient approximative description of these characteristics by a\nlow-dimensional system of ODEs. This leads to a model order reduction problem\nfor a large scale linear system of ODEs arising from the semi-discretization of\nthe heat equation combined with a linear algebraic output equation. In a first\nstep we approximate the non time-invariant system of ODEs by a linear\ntime-invariant system. Then we apply Lyapunov balanced truncation model order\nreduction to approximate the output by a reduced-order system with only a few\nstate equations but almost the same input-output behavior. The paper presents\nresults of extensive numerical experiments showing the efficiency of the\napplied model order reduction methods. It turns out that only a few suitable\nchosen ODEs are sufficient to produce good approximations of the input-output\nbehaviour of the storage.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2104.05116\n",
    "authors": [
      "Paul Honore Takam",
      "Ralf Wunderlich"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14761"
  },
  {
    "id": "arXiv:2209.14764",
    "title": "Model Zoos: A Dataset of Diverse Populations of Neural Network Models",
    "abstract": "In the last years, neural networks (NN) have evolved from laboratory\nenvironments to the state-of-the-art for many real-world problems. It was shown\nthat NN models (i.e., their weights and biases) evolve on unique trajectories\nin weight space during training. Following, a population of such neural network\nmodels (referred to as model zoo) would form structures in weight space. We\nthink that the geometry, curvature and smoothness of these structures contain\ninformation about the state of training and can reveal latent properties of\nindividual models. With such model zoos, one could investigate novel approaches\nfor (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn\nrich representations of such populations, or (iv) exploit the model zoos for\ngenerative modelling of NN weights and biases. Unfortunately, the lack of\nstandardized model zoos and available benchmarks significantly increases the\nfriction for further research about populations of NNs. With this work, we\npublish a novel dataset of model zoos containing systematically generated and\ndiverse populations of NN models for further research. In total the proposed\nmodel zoo dataset is based on eight image datasets, consists of 27 model zoos\ntrained with varying hyperparameter combinations and includes 50'360 unique NN\nmodels as well as their sparsified twins, resulting in over 3'844'360 collected\nmodel states. Additionally, to the model zoo data we provide an in-depth\nanalysis of the zoos and provide benchmarks for multiple downstream tasks. The\ndataset can be found at www.modelzoos.cc.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks\n",
    "authors": [
      "Konstantin Sch\u00fcrholt",
      "Diyar Taskiran",
      "Boris Knyazev",
      "Xavier Gir\u00f3-i-Nieto",
      "Damian Borth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14764"
  },
  {
    "id": "arXiv:2209.14766",
    "title": "Outage Performance Analysis of Type-I HARQ Aided V2V Communications",
    "abstract": "Vehicle-to-vehicle (V2V) communications under dense urban environments\nusually experience severe keyhole fading effect, especially for multi-input\nmulti-output (MIMO) channels, which degrades the capacity and outage\nperformance due to the rank deficiency. To avoid these, the integration of MIMO\nand Type-I hybrid automatic repeat request (HARQ) is proposed to assist V2V\ncommunications in this paper. By using the Meijer G-function, the outage\nprobability of the proposed V2V communications system is derived in closed\nform. With the result, meaningful insights are gained by conducting the\nasymptotic outage analysis. Specifically, it is revealed that full-time\ndiversity order can be achieved, while full spatial diversity order is\nunreachable as compared to Type-I HARQ aided MIMO systems without keyhole\neffect. Moreover, we prove that the asymptotic outage probability is a\nmonotonically increasing and convex function of the transmission rate. Finally,\nthe analytical results are validated through extensive numerical experiments.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2209.12783\n",
    "authors": [
      "Huan Zhang",
      "Zhengtao Liao",
      "Zheng Shi",
      "Guanghua Yang",
      "Qingping Dou",
      "Shaodan Ma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14766"
  },
  {
    "id": "arXiv:2209.14767",
    "title": "Analysis of HARQ-IR over Time-Correlated Rayleigh Fading Channels",
    "abstract": "In this paper, performance of hybrid automatic repeat request with\nincremental redundancy (HARQ-IR) over Rayleigh fading channels is investigated.\nDifferent from prior analysis, time correlation in the channels is considered.\nUnder time-correlated fading channels, the mutual information in multiple HARQ\ntransmissions is correlated, making the analysis challenging. By using\npolynomial fitting technique, probability distribution function of the\naccumulated mutual information is derived. Three meaningful performance metrics\nincluding outage probability, average number of transmissions and long term\naverage throughput (LTAT) are then derived in closed-forms. Moreover, diversity\norder of HARQ-IR is also investigated. It is proved that full diversity can be\nachieved by HARQ-IR, i.e., the diversity order is equal to the number of\ntransmissions, even under time-correlated fading channels. These analytical\nresults are verified by simulations and enable the evaluation of the impact of\nvarious system parameters on the performance. Particularly, the results unveil\nthe negative impact of time correlation on the outage and throughput\nperformance. The results also show that although more transmissions would\nimprove the outage performance, they may not be beneficial to the LTAT when\ntime correlation is high. Optimal rate design to maximize the LTAT is finally\ndiscussed and significant LTAT improvement is demonstrated.",
    "descriptor": "",
    "authors": [
      "Zheng Shi",
      "Haichuan Ding",
      "Shaodan Ma",
      "Kam-Weng Tam"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14767"
  },
  {
    "id": "arXiv:2209.14768",
    "title": "Optimal Power Allocation for HARQ Schemes over Time-Correlated  Nakagami-m Fading Channels",
    "abstract": "This paper investigates the problem of power allocation for hybrid automatic\nrepeat request (HARQ) schemes over time-correlated Nakagami-m fading channels\nunder outage constraint. The presence of time correlation complicates the power\nallocation problem due to the involvement of multiple correlated fading\nchannels. Under a general time-correlated Nakagami-m fading channel with\nexponential correlation, outage probabilities for three widely adopted HARQ\nschemes, including Type I HARQ, HARQ with chase combining (HARQ-CC) and HARQ\nwith incremental redundancy (HARQ-IR), are first derived. With these results,\npower allocation schemes are proposed to minimize the average total\ntransmission power with guaranteed outage performance. Simulation results\ndemonstrate the accuracy of our outage analysis and the effectiveness of our\nproposed power allocation schemes. It is shown that our proposed power\nallocation schemes can achieve significant power savings when compared with\nfixed power allocation. Moreover, under practical low outage constraint, the\npower efficiency is further improved when the time correlation is reduced\nand/or the fading order is increased.",
    "descriptor": "",
    "authors": [
      "Zheng Shi",
      "Shaodan Ma",
      "Fen Hou",
      "Kam-Weng Tam",
      "Yik-Chung Wu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14768"
  },
  {
    "id": "arXiv:2209.14774",
    "title": "RECALL: Rehearsal-free Continual Learning for Object Classification",
    "abstract": "Convolutional neural networks show remarkable results in classification but\nstruggle with learning new things on the fly. We present a novel rehearsal-free\napproach, where a deep neural network is continually learning new unseen object\ncategories without saving any data of prior sequences. Our approach is called\nRECALL, as the network recalls categories by calculating logits for old\ncategories before training new ones. These are then used during training to\navoid changing the old categories. For each new sequence, a new head is added\nto accommodate the new categories. To mitigate forgetting, we present a\nregularization strategy where we replace the classification with a regression.\nMoreover, for the known categories, we propose a Mahalanobis loss that includes\nthe variances to account for the changing densities between known and unknown\ncategories. Finally, we present a novel dataset for continual learning,\nespecially suited for object recognition on a mobile robot (HOWS-CL-25),\nincluding 150,795 synthetic images of 25 household object categories. Our\napproach RECALL outperforms the current state of the art on CORe50 and\niCIFAR-100 and reaches the best performance on HOWS-CL-25.",
    "descriptor": "\nComments: Accepted as contributed paper at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Markus Knauer",
      "Maximilian Denninger",
      "Rudolph Triebel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14774"
  },
  {
    "id": "arXiv:2209.14775",
    "title": "On Constructing Spanners from Random Gaussian Projections",
    "abstract": "Graph sketching is a powerful paradigm for analyzing graph structure via\nlinear measurements introduced by Ahn, Guha, and McGregor (SODA'12) that has\nsince found numerous applications in streaming, distributed computing, and\nmassively parallel algorithms, among others. Graph sketching has proven to be\nquite successful for various problems such as connectivity, minimum spanning\ntrees, edge or vertex connectivity, and cut or spectral sparsifiers. Yet, the\nproblem of approximating shortest path metric of a graph, and specifically\ncomputing a spanner, is notably missing from the list of successes. This has\nturned the status of this fundamental problem into one of the most longstanding\nopen questions in this area.\nWe present a partial explanation of this lack of success by proving a strong\nlower bound for a large family of graph sketching algorithms that encompasses\nprior work on spanners and many (but importantly not also all) related\ncut-based problems mentioned above. Our lower bound matches the algorithmic\nbounds of the recent result of Filtser, Kapralov, and Nouri (SODA'21), up to\nlower order terms, for constructing spanners via the same graph sketching\nfamily. This establishes near-optimality of these bounds, at least restricted\nto this family of graph sketching techniques, and makes progress on a\nconjecture posed in this latter work.",
    "descriptor": "",
    "authors": [
      "Sepehr Assadi",
      "Michael Kapralov",
      "Huacheng Yu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14775"
  },
  {
    "id": "arXiv:2209.14777",
    "title": "Real-Time Scheduling of Machine Learning Operations on Heterogeneous  Neuromorphic SoC",
    "abstract": "Neuromorphic Systems-on-Chip (NSoCs) are becoming heterogeneous by\nintegrating general-purpose processors (GPPs) and neural processing units\n(NPUs) on the same SoC. For embedded systems, an NSoC may need to execute user\napplications built using a variety of machine learning models. We propose a\nreal-time scheduler, called PRISM, which can schedule machine learning models\non a heterogeneous NSoC either individually or concurrently to improve their\nsystem performance. PRISM consists of the following four key steps. First, it\nconstructs an interprocessor communication (IPC) graph of a machine learning\nmodel from a mapping and a self-timed schedule. Second, it creates a\ntransaction order for the communication actors and embeds this order into the\nIPC graph. Third, it schedules the graph on an NSoC by overlapping\ncommunication with the computation. Finally, it uses a Hill Climbing heuristic\nto explore the design space of mapping operations on GPPs and NPUs to improve\nthe performance. Unlike existing schedulers which use only the NPUs of an NSoC,\nPRISM improves performance by enabling batch, pipeline, and operation\nparallelism via exploiting a platform's heterogeneity. For use-cases with\nconcurrent applications, PRISM uses a heuristic resource sharing strategy and a\nnon-preemptive scheduling to reduce the expected wait time before concurrent\noperations can be scheduled on contending resources. Our extensive evaluations\nwith 20 machine learning workloads show that PRISM significantly improves the\nperformance per watt for both individual applications and use-cases when\ncompared to state-of-the-art schedulers.",
    "descriptor": "\nComments: To appear at MEMOCODE 2022\n",
    "authors": [
      "Anup Das"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.14777"
  },
  {
    "id": "arXiv:2209.14778",
    "title": "Batch Normalization Explained",
    "abstract": "A critically important, ubiquitous, and yet poorly understood ingredient in\nmodern deep networks (DNs) is batch normalization (BN), which centers and\nnormalizes the feature maps. To date, only limited progress has been made\nunderstanding why BN boosts DN learning and inference performance; work has\nfocused exclusively on showing that BN smooths a DN's loss landscape. In this\npaper, we study BN theoretically from the perspective of function\napproximation; we exploit the fact that most of today's state-of-the-art DNs\nare continuous piecewise affine (CPA) splines that fit a predictor to the\ntraining data via affine mappings defined over a partition of the input space\n(the so-called \"linear regions\"). {\\em We demonstrate that BN is an\nunsupervised learning technique that -- independent of the DN's weights or\ngradient-based learning -- adapts the geometry of a DN's spline partition to\nmatch the data.} BN provides a \"smart initialization\" that boosts the\nperformance of DN learning, because it adapts even a DN initialized with random\nweights to align its spline partition with the data. We also show that the\nvariation of BN statistics between mini-batches introduces a dropout-like\nrandom perturbation to the partition boundaries and hence the decision boundary\nfor classification problems. This per mini-batch perturbation reduces\noverfitting and improves generalization by increasing the margin between the\ntraining samples and the decision boundary.",
    "descriptor": "",
    "authors": [
      "Randall Balestriero",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Geometry (cs.CG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14778"
  },
  {
    "id": "arXiv:2209.14780",
    "title": "Perturbations and Subpopulations for Testing Robustness in Token-Based  Argument Unit Recognition",
    "abstract": "Argument Unit Recognition and Classification aims at identifying argument\nunits from text and classifying them as pro or against. One of the design\nchoices that need to be made when developing systems for this task is what the\nunit of classification should be: segments of tokens or full sentences.\nPrevious research suggests that fine-tuning language models on the token-level\nyields more robust results for classifying sentences compared to training on\nsentences directly. We reproduce the study that originally made this claim and\nfurther investigate what exactly token-based systems learned better compared to\nsentence-based ones. We develop systematic tests for analysing the behavioural\ndifferences between the token-based and the sentence-based system. Our results\nshow that token-based models are generally more robust than sentence-based\nmodels both on manually perturbed examples and on specific subpopulations of\nthe data.",
    "descriptor": "\nComments: Accepted at the 9th Workshop on Argument Mining, co-located with COLING 2022. Please cite the published version when available\n",
    "authors": [
      "Jonathan Kamp",
      "Lisa Beinborn",
      "Antske Fokkens"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14780"
  },
  {
    "id": "arXiv:2209.14781",
    "title": "Learning Parsimonious Dynamics for Generalization in Reinforcement  Learning",
    "abstract": "Humans are skillful navigators: We aptly maneuver through new places, realize\nwhen we are back at a location we have seen before, and can even conceive of\nshortcuts that go through parts of our environments we have never visited.\nCurrent methods in model-based reinforcement learning on the other hand\nstruggle with generalizing about environment dynamics out of the training\ndistribution. We argue that two principles can help bridge this gap: latent\nlearning and parsimonious dynamics. Humans tend to think about environment\ndynamics in simple terms -- we reason about trajectories not in reference to\nwhat we expect to see along a path, but rather in an abstract latent space,\ncontaining information about the places' spatial coordinates. Moreover, we\nassume that moving around in novel parts of our environment works the same way\nas in parts we are familiar with. These two principles work together in tandem:\nit is in the latent space that the dynamics show parsimonious characteristics.\nWe develop a model that learns such parsimonious dynamics. Using a variational\nobjective, our model is trained to reconstruct experienced transitions in a\nlatent space using locally linear transformations, while encouraged to invoke\nas few distinct transformations as possible. Using our framework, we\ndemonstrate the utility of learning parsimonious latent dynamics models in a\nrange of policy learning and planning tasks.",
    "descriptor": "",
    "authors": [
      "Tankred Saanum",
      "Eric Schulz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14781"
  },
  {
    "id": "arXiv:2209.14782",
    "title": "A case study of spatiotemporal forecasting techniques for weather  forecasting",
    "abstract": "The majority of real-world processes are spatiotemporal, and the data\ngenerated by them exhibits both spatial and temporal evolution. Weather is one\nof the most important processes that fall under this domain, and forecasting it\nhas become a crucial part of our daily routine. Weather data analysis is\nconsidered the most complex and challenging task. Although numerical weather\nprediction models are currently state-of-the-art, they are resource intensive\nand time-consuming. Numerous studies have proposed time-series-based models as\na viable alternative to numerical forecasts. Recent research has primarily\nfocused on forecasting weather at a specific location. Therefore, models can\nonly capture temporal correlations. This self-contained paper explores various\nmethods for regional data-driven weather forecasting, i.e., forecasting over\nmultiple latitude-longitude points to capture spatiotemporal correlations. The\nresults showed that spatiotemporal prediction models reduced computational cost\nwhile improving accuracy; in particular, the proposed tensor train dynamic mode\ndecomposition-based forecasting model has comparable accuracy to ConvLSTM\nwithout the need for training. We use the NASA POWER meteorological dataset to\nevaluate the models and compare them with the current state of the art.",
    "descriptor": "",
    "authors": [
      "Shakir Showkat Sofi",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Numerical Analysis (math.NA)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14782"
  },
  {
    "id": "arXiv:2209.14783",
    "title": "Training \u03b2-VAE by Aggregating a Learned Gaussian Posterior with a  Decoupled Decoder",
    "abstract": "The reconstruction loss and the Kullback-Leibler divergence (KLD) loss in a\nvariational autoencoder (VAE) often play antagonistic roles, and tuning the\nweight of the KLD loss in $\\beta$-VAE to achieve a balance between the two\nlosses is a tricky and dataset-specific task. As a result, current practices in\nVAE training often result in a trade-off between the reconstruction fidelity\nand the continuity$/$disentanglement of the latent space, if the weight $\\beta$\nis not carefully tuned. In this paper, we present intuitions and a careful\nanalysis of the antagonistic mechanism of the two losses, and propose, based on\nthe insights, a simple yet effective two-stage method for training a VAE.\nSpecifically, the method aggregates a learned Gaussian posterior $z \\sim\nq_{\\theta} (z|x)$ with a decoder decoupled from the KLD loss, which is trained\nto learn a new conditional distribution $p_{\\phi} (x|z)$ of the input data $x$.\nExperimentally, we show that the aggregated VAE maximally satisfies the\nGaussian assumption about the latent space, while still achieves a\nreconstruction error comparable to when the latent space is only loosely\nregularized by $\\mathcal{N}(\\mathbf{0},I)$. The proposed approach does not\nrequire hyperparameter (i.e., the KLD weight $\\beta$) tuning given a specific\ndataset as required in common VAE training practices. We evaluate the method\nusing a medical dataset intended for 3D skull reconstruction and shape\ncompletion, and the results indicate promising generative capabilities of the\nVAE trained using the proposed method. Besides, through guided manipulation of\nthe latent variables, we establish a connection between existing autoencoder\n(AE)-based approaches and generative approaches, such as VAE, for the shape\ncompletion problem. Codes and pre-trained weights are available at\nhttps://github.com/Jianningli/skullVAE",
    "descriptor": "",
    "authors": [
      "Jianning Li",
      "Jana Fragemann",
      "Seyed-Ahmad Ahmadi",
      "Jens Kleesiek",
      "Jan Egger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14783"
  },
  {
    "id": "arXiv:2209.14785",
    "title": "EMF-Aware MU-MIMO Beamforming in RIS-Aided Cellular Networks",
    "abstract": "Reconfigurable Intelligent Surfaces (RISs) are one of the key emerging 6th\nGeneration (6G) technologies that are expected to improve the link budgets\nbetween transmitters and receivers by adding artificial propagation paths. In\nsuch re-configured propagation environment, Downlink (DL) Multi-User\nMulti-Input Multi-Output (MU-MIMO) brings capacity improvement to cellular\nnetworks. It benefits from the spatial dimension offered by MIMO systems to\nenable simultaneous transmission of independent data streams to multiple users\non the same radio resources by applying appropriate Beamforming (BF) schemes.\nHowever, in some cases, serving the same subset of users for a long period of\ntime may cause some undesired regions where the average Electromagnetic Field\nExposure (EMFE) exceeds the regulatory limits. To address this challenge, we\npropose in this paper a novel Electromagnetic Field (EMF) aware MU-MIMO BF\nscheme that aims to optimize the overall capacity under EMF constraints in\nRIS-aided cellular networks.",
    "descriptor": "\nComments: 6 pages, 6 figures, to be published in IEEE GLOBECOM 2022\n",
    "authors": [
      "Yi Yu",
      "Rita Ibrahim",
      "Dinh-Thuy Phan-Huy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.14785"
  },
  {
    "id": "arXiv:2209.14788",
    "title": "Using models of baseline gameplay to design for physical rehabilitation",
    "abstract": "Modified digital games manage to drive motivation in repetitive exercises\nneeded for motor rehabilitation, however designing modifications that satisfy\nboth rehabilitation and engagement goals is challenging. We present a method\nwherein a statistical model of baseline gameplay identifies design\nconfigurations that emulate behaviours compatible with unmodified play. We\nillustrate this approach through a case study involving upper limb\nrehabilitation with a custom controller for a Pac-Man game. A participatory\ndesign workshop with occupational therapists defined two interaction parameters\nfor gameplay and rehabilitation adjustments. The parameters' effect on the\ninteraction was measured experimentally with 12 participants. We show that a\nlow-latency model, using both user input behaviour and internal game state,\nidentifies values for interaction parameters that reproduce baseline gameplay\nunder degraded control. We discuss how this method can be applied to\nsystematically balance gamification problems involving trade-offs between\nphysical requirements and subjectively engaging experiences.",
    "descriptor": "\nComments: 19 pages, 10 figures\n",
    "authors": [
      "Antoine Loriette",
      "Baptiste Caramiaux",
      "Sebastian Stein",
      "John H. Williamson"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14788"
  },
  {
    "id": "arXiv:2209.14792",
    "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
    "abstract": "We propose Make-A-Video -- an approach for directly translating the\ntremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video\n(T2V). Our intuition is simple: learn what the world looks like and how it is\ndescribed from paired text-image data, and learn how the world moves from\nunsupervised video footage. Make-A-Video has three advantages: (1) it\naccelerates training of the T2V model (it does not need to learn visual and\nmultimodal representations from scratch), (2) it does not require paired\ntext-video data, and (3) the generated videos inherit the vastness (diversity\nin aesthetic, fantastical depictions, etc.) of today's image generation models.\nWe design a simple yet effective way to build on T2I models with novel and\neffective spatial-temporal modules. First, we decompose the full temporal U-Net\nand attention tensors and approximate them in space and time. Second, we design\na spatial temporal pipeline to generate high resolution and frame rate videos\nwith a video decoder, interpolation model and two super resolution models that\ncan enable various applications besides T2V. In all aspects, spatial and\ntemporal resolution, faithfulness to text, and quality, Make-A-Video sets the\nnew state-of-the-art in text-to-video generation, as determined by both\nqualitative and quantitative measures.",
    "descriptor": "",
    "authors": [
      "Uriel Singer",
      "Adam Polyak",
      "Thomas Hayes",
      "Xi Yin",
      "Jie An",
      "Songyang Zhang",
      "Qiyuan Hu",
      "Harry Yang",
      "Oron Ashual",
      "Oran Gafni",
      "Devi Parikh",
      "Sonal Gupta",
      "Yaniv Taigman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14792"
  },
  {
    "id": "arXiv:2209.14795",
    "title": "ThreatPro: Multi-Layer Threat Analysis in the Cloud",
    "abstract": "Many effective Threat Analysis (TA) techniques exist that focus on analyzing\nthreats to targeted assets (e.g., components, services). These techniques\nconsider static interconnections among the assets. However, in dynamic\nenvironments, such as the Cloud, resources can instantiate, migrate across\nphysical hosts, or decommission to provide rapid resource elasticity to the\nusers. It is evident that existing TA techniques cannot address all these\nrequirements. In addition, there is an increasing number of complex\nmulti-layer/multi-asset attacks on Cloud systems, such as the Equifax data\nbreach. Hence, there is a need for threat analysis approaches that are designed\nto analyze threats in complex, dynamic, and multi-layer Cloud environments. In\nthis paper, we propose ThreatPro that addresses the analysis of multi-layer\nattacks and supports dynamic interconnections in the Cloud. ThreatPro\nfacilitates threat analysis by developing a technology-agnostic information\nflow model, which represents the Cloud's functionality through a set of\nconditional transitions. The model establishes the basis to capture the\nmulti-layer and dynamic interconnections during the life-cycle of a Virtual\nMachine (VM). Specifically, ThreatPro contributes in (a) enabling the\nexploration of a threat's behavior and its propagation across the Cloud, and\n(b) assessing the security of the Cloud by analyzing the impact of multiple\nthreats across various operational layers/assets. Using public information on\nthreats from the National Vulnerability Database (NVD), we validate ThreatPro's\ncapabilities, i.e., (a) identify and trace actual Cloud attacks and (b)\nspeculatively postulate alternate potential attack paths.",
    "descriptor": "\nComments: 32 pages, 14 figures\n",
    "authors": [
      "Salman Manzoor",
      "Antonios Gouglidis",
      "Matthew Bradbury",
      "Neeraj Suri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14795"
  },
  {
    "id": "arXiv:2209.14798",
    "title": "Fast Near-Field Beam Training for Extremely Large-Scale Array",
    "abstract": "In this letter, we study efficient near-field beam training design for the\nextremely large-scale array (XL-array) communication systems. Compared with the\nconventional far-field beam training method that searches for the best beam\ndirection only, the near-field beam training is more challenging since it\nrequires a beam search over both the angular and distance domains due to the\nspherical wavefront propagation model. To reduce the near-field beam-training\noverhead based on the two-dimensional exhaustive search, we propose in this\nletter a new two-phase beam training method that decomposes the two-dimensional\nsearch into two sequential phases. Specifically, in the first phase, the\ncandidate angles of the user is determined by a new method based on the\nconventional far-field codebook and angle-domain beam sweeping. Then, a\ncustomized polar-domain codebook is employed in the second phase to find the\nbest effective distance of the user given the shortlisted candidate angles.\nNumerical results show that our proposed two-phase beam training method\nsignificantly reduces the training overhead of the exhaustive search and yet\nachieves comparable beamforming performance for data transmission.",
    "descriptor": "\nComments: We proposed a novel two-phase near-field beam training method for the XL-array communication systems. The paper has been accepted by IEEE Wireless Communications Letters\n",
    "authors": [
      "Yunpu Zhang",
      "Xun Wu",
      "Changsheng You"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.14798"
  },
  {
    "id": "arXiv:2209.14804",
    "title": "Minimum Link Fencing",
    "abstract": "We study a variant of the geometric multicut problem, where we are given a\nset $\\mathcal{P}$ of colored and pairwise interior-disjoint polygons in the\nplane. The objective is to compute a set of simple closed polygon boundaries\n(fences) that separate the polygons in such a way that any two polygons that\nare enclosed by the same fence have the same color, and the total number of\nlinks of all fences is minimized. We call this the minimum link fencing (MLF)\nproblem and consider the natural case of bounded minimum link fencing (BMLF),\nwhere $\\mathcal{P}$ contains a polygon $Q$ that is unbounded in all directions\nand can be seen as an outer polygon. We show that BMLF is NP-hard in general\nand that it is XP-time solvable when each fence contains at most two polygons\nand the number of segments per fence is the parameter. Finally, we present an\n$O(n \\log n)$-time algorithm for the case that the convex hull of $\\mathcal{P}\n\\setminus \\{Q\\}$ does not intersect $Q$.",
    "descriptor": "",
    "authors": [
      "Sujoy Bhore",
      "Fabian Klute",
      "Maarten L\u00f6ffler",
      "Martin N\u00f6llenburg",
      "Soeren Terziadis",
      "Ana\u00efs Villedieu"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2209.14804"
  },
  {
    "id": "arXiv:2209.14808",
    "title": "Optimization of Functions Given in the Tensor Train Format",
    "abstract": "Tensor train (TT) format is a common approach for computationally efficient\nwork with multidimensional arrays, vectors, matrices, and discretized functions\nin a wide range of applications, including computational mathematics and\nmachine learning. In this work, we propose a new algorithm for TT-tensor\noptimization, which leads to very accurate approximations for the minimum and\nmaximum tensor element. The method consists in sequential tensor\nmultiplications of the TT-cores with an intelligent selection of candidates for\nthe optimum. We propose the probabilistic interpretation of the method, and\nmake estimates on its complexity and convergence. We perform extensive\nnumerical experiments with random tensors and various multivariable benchmark\nfunctions with the number of input dimensions up to $100$. Our approach\ngenerates a solution close to the exact optimum for all model problems, while\nthe running time is no more than $50$ seconds on a regular laptop.",
    "descriptor": "\nComments: 16 page, 3 figures, 3 tables\n",
    "authors": [
      "Andrei Chertkov",
      "Gleb Ryzhakov",
      "Georgii Novikov",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.14808"
  },
  {
    "id": "arXiv:2209.14812",
    "title": "Named Entity Recognition in Industrial Tables using Tabular Language  Models",
    "abstract": "Specialized transformer-based models for encoding tabular data have gained\ninterest in academia. Although tabular data is omnipresent in industry,\napplications of table transformers are still missing. In this paper, we study\nhow these models can be applied to an industrial Named Entity Recognition (NER)\nproblem where the entities are mentioned in tabular-structured spreadsheets.\nThe highly technical nature of spreadsheets as well as the lack of labeled data\npresent major challenges for fine-tuning transformer-based models. Therefore,\nwe develop a dedicated table data augmentation strategy based on available\ndomain-specific knowledge graphs. We show that this boosts performance in our\nlow-resource scenario considerably. Further, we investigate the benefits of\ntabular structure as inductive bias compared to tables as linearized sequences.\nOur experiments confirm that a table transformer outperforms other baselines\nand that its tabular inductive bias is vital for convergence of\ntransformer-based models.",
    "descriptor": "\nComments: EMNLP 2022 Industry Track\n",
    "authors": [
      "Aneta Koleva",
      "Martin Ringsquandl",
      "Mark Buckley",
      "Rakebul Hasan",
      "Volker Tresp"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14812"
  },
  {
    "id": "arXiv:2209.14819",
    "title": "SymmNeRF: Learning to Explore Symmetry Prior for Single-View View  Synthesis",
    "abstract": "We study the problem of novel view synthesis of objects from a single image.\nExisting methods have demonstrated the potential in single-view view synthesis.\nHowever, they still fail to recover the fine appearance details, especially in\nself-occluded areas. This is because a single view only provides limited\ninformation. We observe that manmade objects usually exhibit symmetric\nappearances, which introduce additional prior knowledge. Motivated by this, we\ninvestigate the potential performance gains of explicitly embedding symmetry\ninto the scene representation. In this paper, we propose SymmNeRF, a neural\nradiance field (NeRF) based framework that combines local and global\nconditioning under the introduction of symmetry priors. In particular, SymmNeRF\ntakes the pixel-aligned image features and the corresponding symmetric features\nas extra inputs to the NeRF, whose parameters are generated by a hypernetwork.\nAs the parameters are conditioned on the image-encoded latent codes, SymmNeRF\nis thus scene-independent and can generalize to new scenes. Experiments on\nsynthetic and realworld datasets show that SymmNeRF synthesizes novel views\nwith more details regardless of the pose transformation, and demonstrates good\ngeneralization when applied to unseen objects. Code is available at:\nhttps://github.com/xingyi-li/SymmNeRF.",
    "descriptor": "\nComments: Accepted by ACCV 2022\n",
    "authors": [
      "Xingyi Li",
      "Chaoyi Hong",
      "Yiran Wang",
      "Zhiguo Cao",
      "Ke Xian",
      "Guosheng Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14819"
  },
  {
    "id": "arXiv:2209.14821",
    "title": "Analyzing Diffusion as Serial Reproduction",
    "abstract": "Diffusion models are a class of generative models that learn to synthesize\nsamples by inverting a diffusion process that gradually maps data into noise.\nWhile these models have enjoyed great success recently, a full theoretical\nunderstanding of their observed properties is still lacking, in particular,\ntheir weak sensitivity to the choice of noise family and the role of adequate\nscheduling of noise levels for good synthesis. By identifying a correspondence\nbetween diffusion models and a well-known paradigm in cognitive science known\nas serial reproduction, whereby human agents iteratively observe and reproduce\nstimuli from memory, we show how the aforementioned properties of diffusion\nmodels can be explained as a natural consequence of this correspondence. We\nthen complement our theoretical analysis with simulations that exhibit these\nkey features. Our work highlights how classic paradigms in cognitive science\ncan shed light on state-of-the-art machine learning problems.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Raja Marjieh",
      "Ilia Sucholutsky",
      "Thomas A. Langlois",
      "Nori Jacoby",
      "Thomas L. Griffiths"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14821"
  },
  {
    "id": "arXiv:2209.14823",
    "title": "Physical Human-Robot Interaction Control of an Upper Limb Exoskeleton  with a Decentralized Neuro-Adaptive Control Scheme",
    "abstract": "In the concept of physical human-robot interaction (pHRI), the most important\ncriterion is the safety of a human operator interacting with a high degrees of\nfreedom (DoF) robot. Therefore, a robust control scheme is of high demand to\nestablish safe pHRI and stabilize nonlinear, high DoF systems. In this paper,\nan adaptive decentralized control strategy is designed to accomplish mentioned\nobjectives. To do so, human upper limb model and exoskeleton model are\ndecentralized and augmented at the subsystem level to be able to design a\ndecentralized control action. Moreover, human exogenous force (HEF) that can\nresist exoskeleton motion is estimated using radial basic function neural\nnetworks (RBFNNs). Estimating both human upper limb and robot rigid body\nparameters along with HEF estimation makes the controller adaptable to\ndifferent operators, ensuring their physical safety. The \\emph{barrier Lyapunov\nfunction} (BLF), on the other hand, is employed to guarantee that the robot\nwill work in a safe workspace while ensuring stability by adjusting the control\nlaw. Additionally, unknown actuator uncertainty and constraints are considered\nin this study to ensure a smooth and safe pHRI. Then, the asymptotic stability\nof the whole system is established by means of the \\emph{virtual stability}\nconcept and \\emph{virtual power flows} (VPFs). Numerical and experimental\nresults are provided and compared to PD controller to demonstrate the excellent\nperformance of the proposed controller. As a result, the proposed controller\naccomplished all the control objectives with nearly zero error and low computed\ntorque, ensuring physical safety in pHRI.",
    "descriptor": "",
    "authors": [
      "Mahdi Hejrati",
      "Jouni Mattila"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14823"
  },
  {
    "id": "arXiv:2209.14825",
    "title": "Trading off Quality for Efficiency of Community Detection: An Inductive  Method across Graphs",
    "abstract": "Many network applications can be formulated as NP-hard combinatorial\noptimization problems of community detection (CD). Due to the NP-hardness, to\nbalance the CD quality and efficiency remains a challenge. Most existing CD\nmethods are transductive, which are independently optimized only for the CD on\na single graph. Some of these methods use advanced machine learning techniques\nto obtain high-quality CD results but usually have high complexity. Other\napproaches use fast heuristic approximation to ensure low runtime but may\nsuffer from quality degradation. In contrast to these transductive methods, we\npropose an alternative inductive community detection (ICD) method across graphs\nof a system or scenario to alleviate the NP-hard challenge. ICD first conducts\nthe offline training of an adversarial dual GNN on historical graphs to capture\nkey properties of the system. The trained model is then directly generalized to\nnew unseen graphs for online CD without additional optimization, where a better\ntrade-off between quality and efficiency can be achieved. ICD can also capture\nthe permutation invariant community labels in the offline training and tackle\nthe online CD on new graphs with non-fixed number of nodes and communities.\nExperiments on a set of benchmarks demonstrate that ICD can achieve a\nsignificant trade-off between quality and efficiency over various baselines.",
    "descriptor": "",
    "authors": [
      "Meng Qin",
      "Chaorui Zhang",
      "Bo Bai",
      "Gong Zhang",
      "Dit-Yan Yeung"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14825"
  },
  {
    "id": "arXiv:2209.14826",
    "title": "Towards Lightweight Black-Box Attacks against Deep Neural Networks",
    "abstract": "Black-box attacks can generate adversarial examples without accessing the\nparameters of target model, largely exacerbating the threats of deployed deep\nneural networks (DNNs). However, previous works state that black-box attacks\nfail to mislead target models when their training data and outputs are\ninaccessible. In this work, we argue that black-box attacks can pose practical\nattacks in this extremely restrictive scenario where only several test samples\nare available. Specifically, we find that attacking the shallow layers of DNNs\ntrained on a few test samples can generate powerful adversarial examples. As\nonly a few samples are required, we refer to these attacks as lightweight\nblack-box attacks. The main challenge to promoting lightweight attacks is to\nmitigate the adverse impact caused by the approximation error of shallow\nlayers. As it is hard to mitigate the approximation error with few available\nsamples, we propose Error TransFormer (ETF) for lightweight attacks. Namely,\nETF transforms the approximation error in the parameter space into a\nperturbation in the feature space and alleviates the error by disturbing\nfeatures. In experiments, lightweight black-box attacks with the proposed ETF\nachieve surprising results. For example, even if only 1 sample per category\navailable, the attack success rate in lightweight black-box attacks is only\nabout 3% lower than that of the black-box attacks with complete training data.",
    "descriptor": "",
    "authors": [
      "Chenghao Sun",
      "Yonggang Zhang",
      "Wan Chaoqun",
      "Qizhou Wang",
      "Ya Li",
      "Tongliang Liu",
      "Bo Han",
      "Xinmei Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14826"
  },
  {
    "id": "arXiv:2209.14827",
    "title": "On the Convergence of AdaGrad on $\\R^{d}$: Beyond Convexity,  Non-Asymptotic Rate and Acceleration",
    "abstract": "Existing analysis of AdaGrad and other adaptive methods for smooth convex\noptimization is typically for functions with bounded domain diameter. In\nunconstrained problems, previous works guarantee an asymptotic convergence rate\nwithout an explicit constant factor that holds true for the entire function\nclass. Furthermore, in the stochastic setting, only a modified version of\nAdaGrad, different from the one commonly used in practice, in which the latest\ngradient is not used to update the stepsize, has been analyzed. Our paper aims\nat bridging these gaps and developing a deeper understanding of AdaGrad and its\nvariants in the standard setting of smooth convex functions as well as the more\ngeneral setting of quasar convex functions. First, we demonstrate new\ntechniques to explicitly bound the convergence rate of the vanilla AdaGrad for\nunconstrained problems in both deterministic and stochastic settings. Second,\nwe propose a variant of AdaGrad for which we can show the convergence of the\nlast iterate, instead of the average iterate. Finally, we give new accelerated\nadaptive algorithms and their convergence guarantee in the deterministic\nsetting with explicit dependency on the problem parameters, improving upon the\nasymptotic rate shown in previous works.",
    "descriptor": "",
    "authors": [
      "Zijian Liu",
      "Ta Duy Nguyen",
      "Alina Ene",
      "Huy L. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14827"
  },
  {
    "id": "arXiv:2209.14828",
    "title": "Denoising Diffusion Probabilistic Models for Styled Walking Synthesis",
    "abstract": "Generating realistic motions for digital humans is time-consuming for many\ngraphics applications. Data-driven motion synthesis approaches have seen solid\nprogress in recent years through deep generative models. These results offer\nhigh-quality motions but typically suffer in motion style diversity. For the\nfirst time, we propose a framework using the denoising diffusion probabilistic\nmodel (DDPM) to synthesize styled human motions, integrating two tasks into one\npipeline with increased style diversity compared with traditional motion\nsynthesis methods. Experimental results show that our system can generate\nhigh-quality and diverse walking motions.",
    "descriptor": "",
    "authors": [
      "Edmund J. C. Findlay",
      "Haozheng Zhang",
      "Ziyi Chang",
      "Hubert P. H. Shum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14828"
  },
  {
    "id": "arXiv:2209.14829",
    "title": "Lightweight Monocular Depth Estimation with an Edge Guided Network",
    "abstract": "Monocular depth estimation is an important task that can be applied to many\nrobotic applications. Existing methods focus on improving depth estimation\naccuracy via training increasingly deeper and wider networks, however these\nsuffer from large computational complexity. Recent studies found that edge\ninformation are important cues for convolutional neural networks (CNNs) to\nestimate depth. Inspired by the above observations, we present a novel\nlightweight Edge Guided Depth Estimation Network (EGD-Net) in this study. In\nparticular, we start out with a lightweight encoder-decoder architecture and\nembed an edge guidance branch which takes as input image gradients and\nmulti-scale feature maps from the backbone to learn the edge attention\nfeatures. In order to aggregate the context information and edge attention\nfeatures, we design a transformer-based feature aggregation module (TRFA). TRFA\ncaptures the long-range dependencies between the context information and edge\nattention features through cross-attention mechanism. We perform extensive\nexperiments on the NYU depth v2 dataset. Experimental results show that the\nproposed method runs about 96 fps on a Nvidia GTX 1080 GPU whilst achieving the\nstate-of-the-art performance in terms of accuracy.",
    "descriptor": "",
    "authors": [
      "Xingshuai Dong",
      "Matthew A. Garratt",
      "Sreenatha G. Anavatti",
      "Hussein A. Abbass",
      "Junyu Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14829"
  },
  {
    "id": "arXiv:2209.14831",
    "title": "Access Control with Encrypted Feature Maps for Object Detection Models",
    "abstract": "In this paper, we propose an access control method with a secret key for\nobject detection models for the first time so that unauthorized users without a\nsecret key cannot benefit from the performance of trained models. The method\nenables us not only to provide a high detection performance to authorized users\nbut to also degrade the performance for unauthorized users. The use of\ntransformed images was proposed for the access control of image classification\nmodels, but these images cannot be used for object detection models due to\nperformance degradation. Accordingly, in this paper, selected feature maps are\nencrypted with a secret key for training and testing models, instead of input\nimages. In an experiment, the protected models allowed authorized users to\nobtain almost the same performance as that of non-protected models but also\nwith robustness against unauthorized access without a key.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2206.05422\n",
    "authors": [
      "Teru Nagamori",
      "Hiroki Ito",
      "AprilPyone MaungMaung",
      "Hitoshi Kiya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14831"
  },
  {
    "id": "arXiv:2209.14835",
    "title": "Low-Latency Sliding Window Algorithms for Formal Languages",
    "abstract": "Low-latency sliding window algorithms for regular and context-free languages\nare studied, where latency refers to the worst-case time spent for a single\nwindow update or query. For every regular language $L$ it is shown that there\nexists a constant-latency solution that supports adding and removing symbols\nindependently on both ends of the window (the so-called two-way variable-size\nmodel). We prove that this result extends to all visibly pushdown languages.\nFor deterministic 1-counter languages we present a $\\mathcal{O}(\\log n)$\nlatency sliding window algorithm for the two-way variable-size model where $n$\nrefers to the window size. We complement these results with a conditional lower\nbound: there exists a fixed real-time deterministic context-free language $L$\nsuch that, assuming the OMV (online matrix vector multiplication) conjecture,\nthere is no sliding window algorithm for $L$ with latency $n^{1/2-\\epsilon}$\nfor any $\\epsilon>0$, even in the most restricted sliding window model (one-way\nfixed-size model). The above mentioned results all refer to the unit-cost RAM\nmodel with logarithmic word size. For regular languages we also present a\nrefined picture using word sizes $\\mathcal{O}(1)$, $\\mathcal{O}(\\log\\log n)$,\nand $\\mathcal{O}(\\log n)$.",
    "descriptor": "\nComments: A short version will be presented at the conference FSTTCS 2022\n",
    "authors": [
      "Moses Ganardi",
      "Louis Jachiet",
      "Markus Lohrey",
      "Thomas Schwentick"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2209.14835"
  },
  {
    "id": "arXiv:2209.14836",
    "title": "Deconstructing the structure of online conversations on Reddit",
    "abstract": "The Internet has made it easier for social scientists to study human behavior\nby analyzing their interactions on social media platforms. Many of these\nplatforms characterize conversations among users via threads, which induce a\ntree-like structure. The structural properties of these discussion trees, such\nas their width, depth, and size, can be used to make inferences regarding user\ndiscussion patterns and conversation dynamics. In this paper, we seek to\nunderstand the structure of these online discussions on Reddit. We characterize\nthe structure of these discussions via a set of global and local\ndiscussion-tree properties. The global features constitute information\nregarding the community/subreddit of a given post, whereas the local features\nare comprised of the properties of the post itself. We perform various\nstatistical analyses on a year's worth of Reddit data containing a quarter of a\nmillion posts and several million comments. These analyses allow us to tease\napart the relative contribution of a discussion post's global and local\nproperties and characterize the importance of specific individual features in\ndetermining the discussions' structural patterns. Our results indicate that\nboth local and global features explain a significant amount of structural\nvariation. Local features are collectively more important as they explain\nsignificantly more variation in the discussion trees' structural properties\nthan global features. However, there is significant heterogeneity in the impact\nof the various features. Several global features, e.g., the topic, age,\npopularity, and the redundancy of content in a subreddit, also play a crucial\nrole in understanding the specific properties of discussion trees.",
    "descriptor": "",
    "authors": [
      "Yulin Yu",
      "Paramveer Dhillon"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.14836"
  },
  {
    "id": "arXiv:2209.14842",
    "title": "Classification of Vocal Bursts for ACII 2022 A-VB-Type Competition using  Convolutional Network Networks and Deep Acoustic Embeddings",
    "abstract": "This report provides a brief description of our proposed solution for the\nVocal Burst Type classification task of the ACII 2022 Affective Vocal Bursts\n(A-VB) Competition. We experimented with two approaches as part of our solution\nfor the task at hand. The first of which is based on convolutional neural\nnetworks trained on Mel Spectrograms, and the second is based on average\npooling of deep acoustic embeddings from a pretrained wav2vec2 model. Our best\nperforming model achieves an unweighted average recall (UAR) of 0.5190 for the\ntest partition, compared to the chance-level UAR of 0.1250 and a baseline of\n0.4172. Thus, an improvement of around 20% over the challenge baseline. The\nresults reported in this document demonstrate the efficacy of our proposed\napproaches to solve the AV-B Type Classification task.",
    "descriptor": "\nComments: Report for our submission to the ACII 2022 Affective Vocal Bursts (A-VB) Competition\n",
    "authors": [
      "Muhammad Shehram Shah Syed",
      "Zafi Sherhan Syed",
      "Abbas Syed"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.14842"
  },
  {
    "id": "arXiv:2209.14843",
    "title": "Evaluating Research Dataset Recommendations in a Living Lab",
    "abstract": "The search for research datasets is as important as laborious. Due to the\nimportance of the choice of research data in further research, this decision\nmust be made carefully. Additionally, because of the growing amounts of data in\nalmost all areas, research data is already a central artifact in empirical\nsciences. Consequentially, research dataset recommendations can beneficially\nsupplement scientific publication searches. We formulated the recommendation\ntask as a retrieval problem by focussing on broad similarities between research\ndatasets and scientific publications. In a multistage approach, initial\nrecommendations were retrieved by the BM25 ranking function and dynamic\nqueries. Subsequently, the initial ranking was re-ranked utilizing click\nfeedback and document embeddings. The proposed system was evaluated live on\nreal user interaction data using the STELLA infrastructure in the LiLAS Lab at\nCLEF 2021. Our experimental system could efficiently be fine-tuned before the\nlive evaluation by pre-testing the system with a pseudo test collection based\non prior user interaction data from the live system. The results indicate that\nthe experimental system outperforms the other participating systems.",
    "descriptor": "\nComments: Best of 2021 Labs: LiLAS Lab Experimental IR Meets Multilinguality, Multimodality, and Interaction - 13th International Conference of the CLEF Association, CLEF 2022, Bologna, Italy, September 5-8, 2022, Proceedings\n",
    "authors": [
      "J\u00fcri Keller",
      "Leon Paul Mondrian Munz"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2209.14843"
  },
  {
    "id": "arXiv:2209.14848",
    "title": "An Economic Model Predictive Control Approach for Load Mitigation on  Multiple Tower Locations of Wind Turbines",
    "abstract": "The current trend in the evolution of wind turbines is to increase their\nrotor size in order to capture more power. This leads to taller, slender and\nmore flexible towers, which thus experience higher dynamical loads due to the\nturbine rotation and environmental factors. It is hence compelling to deploy\nadvanced control methods that can dynamically counteract such loads, especially\nat tower positions that are more prone to develop cracks or corrosion damages.\nStill, to the best of the authors' knowledge, little to no attention has been\npaid in the literature to load mitigation at multiple tower locations.\nFurthermore, there is a need for control schemes that can balance load\nreduction with optimization of power production. In this paper, we develop an\nEconomic Model Predictive Control (eMPC) framework to address such needs.\nFirst, we develop a linear modal model to account for the tower flexural\ndynamics. Then we incorporate it into an eMPC framework, where the dynamics of\nthe turbine rotation are expressed in energy terms. This allows us to obtain a\nconvex formulation, that is computationally attractive. Our control law is\ndesigned to avoid the 'turn-pike' behavior and guarantee recursive feasibility.\nWe demonstrate the performance of the proposed controller on a 5MW reference WT\nmodel: the results illustrate that the proposed controller is able to reduce\nthe tower loads at multiple locations, without significant effects to the\ngenerated power.",
    "descriptor": "\nComments: 6 pages, 6 figures, 61st IEEE Conference on Decision and Control 2022\n",
    "authors": [
      "Zhixin Feng",
      "Alexander J. Gallo",
      "Yichao Liu",
      "Atindriyo K. Pamososuryo",
      "Riccardo M.G. Ferrari",
      "Jan-Willem van Wingerden"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14848"
  },
  {
    "id": "arXiv:2209.14851",
    "title": "Meta Knowledge Condensation for Federated Learning",
    "abstract": "Existing federated learning paradigms usually extensively exchange\ndistributed models at a central solver to achieve a more powerful model.\nHowever, this would incur severe communication burden between a server and\nmultiple clients especially when data distributions are heterogeneous. As a\nresult, current federated learning methods often require a large number of\ncommunication rounds in training. Unlike existing paradigms, we introduce an\nalternative perspective to significantly decrease the communication cost in\nfederate learning. In this work, we first introduce a meta knowledge\nrepresentation method that extracts meta knowledge from distributed clients.\nThe extracted meta knowledge encodes essential information that can be used to\nimprove the current model. As the training progresses, the contributions of\ntraining samples to a federated model also vary. Thus, we introduce a dynamic\nweight assignment mechanism that enables samples to contribute adaptively to\nthe current model update. Then, informative meta knowledge from all active\nclients is sent to the server for model update. Training a model on the\ncombined meta knowledge without exposing original data among different clients\ncan significantly mitigate the heterogeneity issues. Moreover, to further\nameliorate data heterogeneity, we also exchange meta knowledge among clients as\nconditional initialization for local meta knowledge extraction. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmethod. Remarkably, our method outperforms the state-of-the-art by a large\nmargin (from $74.07\\%$ to $92.95\\%$) on MNIST with a restricted communication\nbudget (i.e. 10 rounds).",
    "descriptor": "",
    "authors": [
      "Ping Liu",
      "Xin Yu",
      "Joey Tianyi Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14851"
  },
  {
    "id": "arXiv:2209.14853",
    "title": "META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for  Unbounded Functions",
    "abstract": "We study the application of variance reduction (VR) techniques to general\nnon-convex stochastic optimization problems. In this setting, the recent work\nSTORM [Cutkosky-Orabona '19] overcomes the drawback of having to compute\ngradients of \"mega-batches\" that earlier VR methods rely on. There, STORM\nutilizes recursive momentum to achieve the VR effect and is then later made\nfully adaptive in STORM+ [Levy et al., '21], where full-adaptivity removes the\nrequirement for obtaining certain problem-specific parameters such as the\nsmoothness of the objective and bounds on the variance and norm of the\nstochastic gradients in order to set the step size. However, STORM+ crucially\nrelies on the assumption that the function values are bounded, excluding a\nlarge class of useful functions. In this work, we propose META-STORM, a\ngeneralized framework of STORM+ that removes this bounded function values\nassumption while still attaining the optimal convergence rate for non-convex\noptimization. META-STORM not only maintains full-adaptivity, removing the need\nto obtain problem specific parameters, but also improves the convergence rate's\ndependency on the problem parameters. Furthermore, META-STORM can utilize a\nlarge range of parameter settings that subsumes previous methods allowing for\nmore flexibility in a wider range of settings. Finally, we demonstrate the\neffectiveness of META-STORM through experiments across common deep learning\ntasks. Our algorithm improves upon the previous work STORM+ and is competitive\nwith widely used algorithms after the addition of per-coordinate update and\nexponential moving average heuristics.",
    "descriptor": "",
    "authors": [
      "Zijian Liu",
      "Ta Duy Nguyen",
      "Thien Hang Nguyen",
      "Alina Ene",
      "Huy L. Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14853"
  },
  {
    "id": "arXiv:2209.14854",
    "title": "TERMinator: A system for scientific texts processing",
    "abstract": "This paper is devoted to the extraction of entities and semantic relations\nbetween them from scientific texts, where we consider scientific terms as\nentities. In this paper, we present a dataset that includes annotations for two\ntasks and develop a system called TERMinator for the study of the influence of\nlanguage models on term recognition and comparison of different approaches for\nrelation extraction. Experiments show that language models pre-trained on the\ntarget language are not always show the best performance. Also adding some\nheuristic approaches may improve the overall quality of the particular task.\nThe developed tool and the annotated corpus are publicly available at\nhttps://github.com/iis-research-team/terminator and may be useful for other\nresearchers.",
    "descriptor": "",
    "authors": [
      "Elena Bruches",
      "Olga Tikhobaeva",
      "Yana Dementyeva",
      "Tatiana Batura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14854"
  },
  {
    "id": "arXiv:2209.14855",
    "title": "Continuous PDE Dynamics Forecasting with Implicit Neural Representations",
    "abstract": "Effective data-driven PDE forecasting methods often rely on fixed spatial and\n/ or temporal discretizations. This raises limitations in real-world\napplications like weather prediction where flexible extrapolation at arbitrary\nspatiotemporal locations is required. We address this problem by introducing a\nnew data-driven approach, DINo, that models a PDE's flow with continuous-time\ndynamics of spatially continuous functions. This is achieved by embedding\nspatial observations independently of their discretization via Implicit Neural\nRepresentations in a small latent space temporally driven by a learned ODE.\nThis separate and flexible treatment of time and space makes DINo the first\ndata-driven model to combine the following advantages. It extrapolates at\narbitrary spatial and temporal locations; it can learn from sparse irregular\ngrids or manifolds; at test time, it generalizes to new grids or resolutions.\nDINo outperforms alternative neural PDE forecasters in a variety of challenging\ngeneralization scenarios on representative PDE systems.",
    "descriptor": "",
    "authors": [
      "Yuan Yin",
      "Matthieu Kirchmeyer",
      "Jean-Yves Franceschi",
      "Alain Rakotomamonjy",
      "Patrick Gallinari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14855"
  },
  {
    "id": "arXiv:2209.14858",
    "title": "4D-StOP: Panoptic Segmentation of 4D LiDAR using Spatio-temporal Object  Proposal Generation and Aggregation",
    "abstract": "In this work, we present a new paradigm, called 4D-StOP, to tackle the task\nof 4D Panoptic LiDAR Segmentation. 4D-StOP first generates spatio-temporal\nproposals using voting-based center predictions, where each point in the 4D\nvolume votes for a corresponding center. These tracklet proposals are further\naggregated using learned geometric features. The tracklet aggregation method\neffectively generates a video-level 4D scene representation over the entire\nspace-time volume. This is in contrast to existing end-to-end trainable\nstate-of-the-art approaches which use spatio-temporal embeddings that are\nrepresented by Gaussian probability distributions. Our voting-based tracklet\ngeneration method followed by geometric feature-based aggregation generates\nsignificantly improved panoptic LiDAR segmentation quality when compared to\nmodeling the entire 4D volume using Gaussian probability distributions. 4D-StOP\nachieves a new state-of-the-art when applied to the SemanticKITTI test dataset\nwith a score of 63.9 LSTQ, which is a large (+7%) improvement compared to\ncurrent best-performing end-to-end trainable methods. The code and pre-trained\nmodels are available at: https://github.com/LarsKreuzberg/4D-StOP.",
    "descriptor": "\nComments: Accepted to the ECCV 2022 AVVision Workshop\n",
    "authors": [
      "Lars Kreuzberg",
      "Idil Esen Zulfikar",
      "Sabarinath Mahadevan",
      "Francis Engelmann",
      "Bastian Leibe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14858"
  },
  {
    "id": "arXiv:2209.14860",
    "title": "Bridging the Gap to Real-World Object-Centric Learning",
    "abstract": "Humans naturally decompose their environment into entities at the appropriate\nlevel of abstraction to act in the world. Allowing machine learning algorithms\nto derive this decomposition in an unsupervised way has become an important\nline of research. However, current methods are restricted to simulated data or\nrequire additional information in the form of motion or depth in order to\nsuccessfully discover objects. In this work, we overcome this limitation by\nshowing that reconstructing features from models trained in a self-supervised\nmanner is a sufficient training signal for object-centric representations to\narise in a fully unsupervised way. Our approach, DINOSAUR, significantly\nout-performs existing object-centric learning models on simulated data and is\nthe first unsupervised object-centric model that scales to real world-datasets\nsuch as COCO and PASCAL VOC. DINOSAUR is conceptually simple and shows\ncompetitive performance compared to more involved pipelines from the computer\nvision literature.",
    "descriptor": "",
    "authors": [
      "Maximilian Seitzer",
      "Max Horn",
      "Andrii Zadaianchuk",
      "Dominik Zietlow",
      "Tianjun Xiao",
      "Carl-Johann Simon-Gabriel",
      "Tong He",
      "Zheng Zhang",
      "Bernhard Sch\u00f6lkopf",
      "Thomas Brox",
      "Francesco Locatello"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14860"
  },
  {
    "id": "arXiv:2209.14861",
    "title": "Improving Understanding of Biocide Availability in Facades through  Immersive Analytics",
    "abstract": "The durability of facades is heavily affected by multiple factors like\nmicrobial growth and weather conditions among others. Biocides are often used\nto resist these factors and protect the facades. However, the biocides get\nwashed out due to rains and other factors like geometric structure of the\nfacade, orientation of the building. It is therefore, important to understand\nhow these factors affect the durability of facades, leading to a requirement of\nexpert analysis. In this paper, we propose a technical pipeline and a set of\ninteraction techniques to support data analysis within the immersive\nenvironment for our case study. Our technical pipeline mainly consists of three\nsteps: 3D reconstruction, embedding sensor data and visualization and\ninteraction techniques. We made a formative evaluation of our prototype to get\ninsights from microbiology, biology and VR experts. The remarks from the\nexperts and the results of the evaluation suggest that an immersive analytic\nsystem in our case study could be beneficial for both experts and non-expert\nusers.",
    "descriptor": "",
    "authors": [
      "Negar Nouri",
      "Snehanjali Kalamkar",
      "Forouzan Farzinnejad",
      "Verena Biener",
      "Fabian Schick",
      "Stefan Kalkhof",
      "Jens Grubert"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14861"
  },
  {
    "id": "arXiv:2209.14868",
    "title": "ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers  for Streaming Speech Recognition",
    "abstract": "The recurrent neural network transducer (RNN-T) is a prominent streaming\nend-to-end (E2E) ASR technology. In RNN-T, the acoustic encoder commonly\nconsists of stacks of LSTMs. Very recently, as an alternative to LSTM layers,\nthe Conformer architecture was introduced where the encoder of RNN-T is\nreplaced with a modified Transformer encoder composed of convolutional layers\nat the frontend and between attention layers. In this paper, we introduce a new\nstreaming ASR model, Convolutional Augmented Recurrent Neural Network\nTransducers (ConvRNN-T) in which we augment the LSTM-based RNN-T with a novel\nconvolutional frontend consisting of local and global context CNN encoders.\nConvRNN-T takes advantage of causal 1-D convolutional layers,\nsqueeze-and-excitation, dilation, and residual blocks to provide both global\nand local audio context representation to LSTM layers. We show ConvRNN-T\noutperforms RNN-T, Conformer, and ContextNet on Librispeech and in-house data.\nIn addition, ConvRNN-T offers less computational complexity compared to\nConformer. ConvRNN-T's superior accuracy along with its low footprint make it a\npromising candidate for on-device streaming ASR technologies.",
    "descriptor": "\nComments: This paper was presented in Interspeech 2022\n",
    "authors": [
      "Martin Radfar",
      "Rohit Barnwal",
      "Rupak Vignesh Swaminathan",
      "Feng-Ju Chang",
      "Grant P. Strimel",
      "Nathan Susanj",
      "Athanasios Mouchtaris"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.14868"
  },
  {
    "id": "arXiv:2209.14875",
    "title": "Accelerating Laboratory Automation Through Robot Skill Learning For  Sample Scraping",
    "abstract": "The potential use of robotics for laboratory experiments offers an attractive\nroute to alleviate scientists from tedious tasks while accelerating the process\nof obtaining new materials, where topical issues such as climate change and\ndisease risks worldwide would greatly benefit. While some experimental\nworkflows can already benefit from automation, it is common that sample\npreparation is still carried out manually due to the high level of motor\nfunction required when dealing with heterogeneous systems, e.g., different\ntools, chemicals, and glassware. A fundamental workflow in chemical fields is\ncrystallisation, where one application is polymorph screening, i.e., obtaining\na three dimensional molecular structure from a crystal. For this process, it is\nof utmost importance to recover as much of the sample as possible since\nsynthesising molecules is both costly in time and money. To this aim, chemists\nhave to scrape vials to retrieve sample contents prior to imaging plate\ntransfer. Automating this process is challenging as it goes beyond robotic\ninsertion tasks due to a fundamental requirement of having to execute\nfine-granular movements within a constrained environment that is the sample\nvial. Motivated by how human chemists carry out this process of scraping powder\nfrom vials, our work proposes a model-free reinforcement learning method for\nlearning a scraping policy, leading to a fully autonomous sample scraping\nprocedure. To realise that, we first create a simulation environment with a\nPanda Franka Emika robot using a laboratory scraper which is inserted into a\nsimulated vial, to demonstrate how a scraping policy can be learned\nsuccessfully. We then evaluate our method on a real robotic manipulator in\nlaboratory settings, and show that our method can autonomously scrape powder\nacross various setups.",
    "descriptor": "\nComments: 7 pages, 7 figures, submitted to IEEE International Conference on Robotics and Automation (ICRA 2023). Video: this https URL\n",
    "authors": [
      "Gabriella Pizzuto",
      "Hetong Wang",
      "Hatem Fakhruldeen",
      "Bei Peng",
      "Kevin S. Luck",
      "Andrew I. Cooper"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14875"
  },
  {
    "id": "arXiv:2209.14876",
    "title": "Repairing Bugs in Python Assignments Using Large Language Models",
    "abstract": "Students often make mistakes on their introductory programming assignments as\npart of their learning process. Unfortunately, providing custom repairs for\nthese mistakes can require a substantial amount of time and effort from class\ninstructors. Automated program repair (APR) techniques can be used to\nsynthesize such fixes. Prior work has explored the use of symbolic and neural\ntechniques for APR in the education domain. Both types of approaches require\neither substantial engineering efforts or large amounts of data and training.\nWe propose to use a large language model trained on code, such as Codex, to\nbuild an APR system -- MMAPR -- for introductory Python programming\nassignments. Our system can fix both syntactic and semantic mistakes by\ncombining multi-modal prompts, iterative querying, test-case-based selection of\nfew-shots, and program chunking. We evaluate MMAPR on 286 real student programs\nand compare to a baseline built by combining a state-of-the-art Python syntax\nrepair engine, BIFI, and state-of-the-art Python semantic repair engine for\nstudent assignments, Refactory. We find that MMAPR can fix more programs and\nproduce smaller patches on average.",
    "descriptor": "",
    "authors": [
      "Jialu Zhang",
      "Jos\u00e9 Cambronero",
      "Sumit Gulwani",
      "Vu Le",
      "Ruzica Piskac",
      "Gustavo Soares",
      "Gust Verbruggen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14876"
  },
  {
    "id": "arXiv:2209.14878",
    "title": "Enumerating Regular Languages in Constant Delay",
    "abstract": "We study the task, for a given language $L$, of enumerating the (generally\ninfinite) sequence of its words, without repetitions, while bounding the delay\nbetween two consecutive words. To allow for constant delay bounds, we assume a\nmodel where we produce each word by editing the preceding word with a small\nedit script, rather than writing out the word from scratch. In particular, this\nwitnesses that the language is orderable, i.e., we can write its words as an\ninfinite sequence such that the Levenshtein edit distance between any two\nconsecutive words is bounded by a constant. For instance, $(a+b)^*$ is\norderable (with a variant of the Gray code), but $a^* + b^*$ is not.\nWe characterize which regular languages are enumerable in this sense, and\nshow that this can be decided in PTIME in an input deterministic finite\nautomaton (DFA) for the language. In fact, we show that, given a DFA $A$\nrecognizing a language $L$, we can compute in PTIME automata $A_1, \\ldots, A_t$\nsuch that $L$ is partitioned as $L(A_1) \\sqcup \\ldots \\sqcup L(A_t)$ and every\n$L(A_i)$ is orderable in this sense. Further, we show that this is optimal,\ni.e., we cannot partition $L$ into less than $t$ orderable languages.\nIn the case where $L$ is orderable, we show that the ordering can be computed\nas a constant-delay algorithm: specifically, the algorithm runs in a suitable\npointer machine model, and produces a sequence of constant-length edit scripts\nto visit the words of $L$ without repetitions, with constant delay between each\nscript. In fact, we show that we can achieve this while only allowing the edit\noperations push and pop at the beginning and end of the word, which implies\nthat the word can in fact be maintained in a double-ended queue.\nWe also show results on the complexity of a related problem, and study the\nmodel where push-pop edits are only allowed at the end of the word.",
    "descriptor": "\nComments: 46 pages. Submitted\n",
    "authors": [
      "Antoine Amarilli",
      "Mika\u00ebl Monet"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14878"
  },
  {
    "id": "arXiv:2209.14879",
    "title": "OpenDSU: Digital Sovereignty in PharmaLedger",
    "abstract": "Distributed ledger networks, chiefly those based on blockchain technologies,\ncurrently are heralding a next generation of computer systems that aims to suit\nmodern users' demands. Over the recent years, several technologies for\nblockchains, off-chaining strategies, as well as decentralised and respectively\nself-sovereign identity systems have shot up so fast that standardisation of\nthe protocols is lagging behind, severely hampering the interoperability of\ndifferent approaches. Moreover, most of the currently available solutions for\ndistributed ledgers focus on either home users or enterprise use case\nscenarios, failing to provide integrative solutions addressing the needs of\nboth.\nHerein we introduce the OpenDSU platform that allows to interoperate generic\nblockchain technologies, organised - and possibly cascaded in a hierarchical\nfashion - in domains. To achieve this flexibility, we seamlessly integrated a\nset of well conceived OpenDSU components to orchestrate off-chain data with\ngranularly resolved and cryptographically secure access levels that are nested\nwith sovereign identities across the different domains.\nEmploying our platform to PharmaLedger, an inter-European network for the\nstandardisation of data handling in the pharmaceutical industry and in\nhealthcare, we demonstrate that OpenDSU can cope with generic demands of\nheterogeneous use cases in both, performance and handling substantially\ndifferent business policies. Importantly, whereas available solutions commonly\nrequire a pre-defined and fixed set of components, no such vendor lock-in\nrestrictions on the blockchain technology or identity system exist in OpenDSU,\nmaking systems built on it flexibly adaptable to new standards evolving in the\nfuture.",
    "descriptor": "\nComments: 18 pages, 8 figures\n",
    "authors": [
      "Cosmin Ursache",
      "Michael Sammeth",
      "S\u00eenic\u0103 Alboaie"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Software Engineering (cs.SE)",
      "Social and Information Networks (cs.SI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14879"
  },
  {
    "id": "arXiv:2209.14881",
    "title": "Sequential Attention for Feature Selection",
    "abstract": "Feature selection is the problem of selecting a subset of features for a\nmachine learning model that maximizes model quality subject to a resource\nbudget constraint. For neural networks, prior methods, including those based on\n$\\ell_1$ regularization, attention, and stochastic gates, typically select all\nof the features in one evaluation round, ignoring the residual value of the\nfeatures during selection (i.e., the marginal contribution of a feature\nconditioned on the previously selected features). We propose a feature\nselection algorithm called Sequential Attention that achieves state-of-the-art\nempirical results for neural networks. This algorithm is based on an efficient\nimplementation of greedy forward selection and uses attention weights at each\nstep as a proxy for marginal feature importance. We provide theoretical\ninsights into our Sequential Attention algorithm for linear regression models\nby showing that an adaptation to this setting is equivalent to the classical\nOrthogonal Matching Pursuit algorithm [PRK1993], and thus inherits all of its\nprovable guarantees. Lastly, our theoretical and empirical analyses provide new\nexplanations towards the effectiveness of attention and its connections to\noverparameterization, which might be of independent interest.",
    "descriptor": "",
    "authors": [
      "MohammadHossein Bateni",
      "Lin Chen",
      "Matthew Fahrbach",
      "Gang Fu",
      "Vahab Mirrokni",
      "Taisuke Yasuda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14881"
  },
  {
    "id": "arXiv:2209.14884",
    "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
    "abstract": "The fundamental goal of self-supervised learning (SSL) is to produce useful\nrepresentations of data without access to any labels for classifying the data.\nModern methods in SSL, which form representations based on known or constructed\nrelationships between samples, have been particularly effective at this task.\nHere, we aim to extend this framework to incorporate algorithms based on kernel\nmethods where embeddings are constructed by linear maps acting on the feature\nspace of a kernel. In this kernel regime, we derive methods to find the optimal\nform of the output representations for contrastive and non-contrastive loss\nfunctions. This procedure produces a new representation space with an inner\nproduct denoted as the induced kernel which generally correlates points which\nare related by an augmentation in kernel space and de-correlates points\notherwise. We analyze our kernel model on small datasets to identify common\nfeatures of self-supervised learning algorithms and gain theoretical insights\ninto their performance on downstream tasks.",
    "descriptor": "",
    "authors": [
      "Bobak T. Kiani",
      "Randall Balestriero",
      "Yubei Chen",
      "Seth Lloyd",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14884"
  },
  {
    "id": "arXiv:2209.14887",
    "title": "Learning Low-Frequency Motion Control for Robust and Dynamic Robot  Locomotion",
    "abstract": "Robotic locomotion is often approached with the goal of maximizing robustness\nand reactivity by increasing motion control frequency. We challenge this\nintuitive notion by demonstrating robust and dynamic locomotion with a learned\nmotion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The\nrobot is able to robustly and repeatably achieve a high heading velocity of 1.5\nm/s, traverse uneven terrain, and resist unexpected external perturbations. We\nfurther present a comparative analysis of deep reinforcement learning (RL)\nbased motion control policies trained and executed at frequencies ranging from\n5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to\nactuation latencies and variations in system dynamics. This is to the extent\nthat a successful sim-to-real transfer can be performed even without any\ndynamics randomization or actuation modeling. We support this claim through a\nset of rigorous empirical evaluations. Moreover, to assist reproducibility, we\nprovide the training and deployment code along with an extended analysis at\nhttps://ori-drs.github.io/lfmc/.",
    "descriptor": "\nComments: 7 pages, 9 figures and 2 tables\n",
    "authors": [
      "Siddhant Gangapurwala",
      "Luigi Campanaro",
      "Ioannis Havoutis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14887"
  },
  {
    "id": "arXiv:2209.14890",
    "title": "Mask-Guided Image Person Removal with Data Synthesis",
    "abstract": "As a special case of common object removal, image person removal is playing\nan increasingly important role in social media and criminal investigation\ndomains. Due to the integrity of person area and the complexity of human\nposture, person removal has its own dilemmas. In this paper, we propose a novel\nidea to tackle these problems from the perspective of data synthesis.\nConcerning the lack of dedicated dataset for image person removal, two dataset\nproduction methods are proposed to automatically generate images, masks and\nground truths respectively. Then, a learning framework similar to local image\ndegradation is proposed so that the masks can be used to guide the feature\nextraction process and more texture information can be gathered for final\nprediction. A coarse-to-fine training strategy is further applied to refine the\ndetails. The data synthesis and learning framework combine well with each\nother. Experimental results verify the effectiveness of our method\nquantitatively and qualitatively, and the trained network proves to have good\ngeneralization ability either on real or synthetic images.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Yunliang Jiang",
      "Chenyang Gu",
      "Zhenfeng Xue",
      "Xiongtao Zhang",
      "Yong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14890"
  },
  {
    "id": "arXiv:2209.14892",
    "title": "Shifted boundary polynomial corrections for compressible flows: high  order on curved domains using linear meshes",
    "abstract": "In this work we propose a simple but effective high order polynomial\ncorrection allowing to enhance the consistency of all kind of boundary\nconditions for the Euler equations (Dirichlet, characteristic far-field and\nslip-wall), both in 2D and 3D, preserving a high order of accuracy without the\nneed of curved meshes. The method proposed is a simplified reformulation of the\nShifted Boundary Method (SBM) and relies on a correction based on the\nextrapolated value of the in cell polynomial to the true geometry, thus not\nrequiring the explicit evaluation of high order Taylor series. Moreover, this\nstrategy could be easily implemented into any already existing finite element\nand finite volume code. Several validation tests are presented to prove the\nconvergence properties up to order four for 2D and 3D simulations with curved\nboundaries, as well as an effective extension to flows with shocks.",
    "descriptor": "",
    "authors": [
      "Mirco Ciallella",
      "Elena Gaburro",
      "Marco Lorini",
      "Mario Ricchiuto"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14892"
  },
  {
    "id": "arXiv:2209.14899",
    "title": "Generate-and-Retrieve: use your predictions to improve retrieval for  semantic parsing",
    "abstract": "A common recent approach to semantic parsing augments sequence-to-sequence\nmodels by retrieving and appending a set of training samples, called exemplars.\nThe effectiveness of this recipe is limited by the ability to retrieve\ninformative exemplars that help produce the correct parse, which is especially\nchallenging in low-resource settings. Existing retrieval is commonly based on\nsimilarity of query and exemplar inputs. We propose GandR, a retrieval\nprocedure that retrieves exemplars for which outputs are also similar.\nGandRfirst generates a preliminary prediction with input-based retrieval. Then,\nit retrieves exemplars with outputs similar to the preliminary prediction which\nare used to generate a final prediction. GandR sets the state of the art on\nmultiple low-resource semantic parsing tasks.",
    "descriptor": "\nComments: To appear in the proceedings of COLING 2022\n",
    "authors": [
      "Yury Zemlyanskiy",
      "Michiel de Jong",
      "Joshua Ainslie",
      "Panupong Pasupat",
      "Peter Shaw",
      "Linlu Qiu",
      "Sumit Sanghai",
      "Fei Sha"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14899"
  },
  {
    "id": "arXiv:2209.14900",
    "title": "Joint Optimization of Energy Consumption and Completion Time in  Federated Learning",
    "abstract": "Federated Learning (FL) is an intriguing distributed machine learning\napproach due to its privacy-preserving characteristics. To balance the\ntrade-off between energy and execution latency, and thus accommodate different\ndemands and application scenarios, we formulate an optimization problem to\nminimize a weighted sum of total energy consumption and completion time through\ntwo weight parameters. The optimization variables include bandwidth,\ntransmission power and CPU frequency of each device in the FL system, where all\ndevices are linked to a base station and train a global model collaboratively.\nThrough decomposing the non-convex optimization problem into two subproblems,\nwe devise a resource allocation algorithm to determine the bandwidth\nallocation, transmission power, and CPU frequency for each participating\ndevice. We further present the convergence analysis and computational\ncomplexity of the proposed algorithm. Numerical results show that our proposed\nalgorithm not only has better performance at different weight parameters (i.e.,\ndifferent demands) but also outperforms the state of the art.",
    "descriptor": "\nComments: This paper appears in the Proceedings of IEEE International Conference on Distributed Computing Systems (ICDCS) 2022. Please feel free to contact us for questions or remarks\n",
    "authors": [
      "Xinyu Zhou",
      "Jun Zhao",
      "Huimei Han",
      "Claude Guet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.14900"
  },
  {
    "id": "arXiv:2209.14901",
    "title": "DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language  Processing",
    "abstract": "The meaningful use of electronic health records (EHR) continues to progress\nin the digital era with clinical decision support systems augmented by\nartificial intelligence. A priority in improving provider experience is to\novercome information overload and reduce the cognitive burden so fewer medical\nerrors and cognitive biases are introduced during patient care. One major type\nof medical error is diagnostic error due to systematic or predictable errors in\njudgment that rely on heuristics. The potential for clinical natural language\nprocessing (cNLP) to model diagnostic reasoning in humans with forward\nreasoning from data to diagnosis and potentially reduce the cognitive burden\nand medical error has not been investigated. Existing tasks to advance the\nscience in cNLP have largely focused on information extraction and named entity\nrecognition through classification tasks. We introduce a novel suite of tasks\ncoined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for\ndeveloping and evaluating cNLP models with clinical diagnostic reasoning\nability. The suite includes six tasks from ten publicly available datasets\naddressing clinical text understanding, medical knowledge reasoning, and\ndiagnosis generation. DR.BENCH is the first clinical suite of tasks designed to\nbe a natural language generation framework to evaluate pre-trained language\nmodels. Experiments with state-of-the-art pre-trained generative language\nmodels using large general domain models and models that were continually\ntrained on a medical corpus demonstrate opportunities for improvement when\nevaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab\nrepository with a systematic approach to load and evaluate models for the cNLP\ncommunity.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Yanjun Gao",
      "Dmitriy Dligach",
      "Timothy Miller",
      "John Caskey",
      "Brihat Sharma",
      "Matthew M Churpek",
      "Majid Afshar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14901"
  },
  {
    "id": "arXiv:2209.14905",
    "title": "Variance Covariance Regularization Enforces Pairwise Independence in  Self-Supervised Representations",
    "abstract": "Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE\navoid collapse of their joint embedding architectures by constraining or\nregularizing the covariance matrix of their projector's output. This study\nhighlights important properties of such strategy, which we coin\nVariance-Covariance regularization (VCReg). More precisely, we show that VCReg\nenforces pairwise independence between the features of the learned\nrepresentation. This result emerges by bridging VCReg applied on the\nprojector's output to kernel independence criteria applied on the projector's\ninput. This provides the first theoretical motivations and explanations of\nVCReg. We empirically validate our findings where (i) we observe that SSL\nmethods employing VCReg learn visual representations with greater pairwise\nindependence than other methods, (i) we put in evidence which projector's\ncharacteristics favor pairwise independence, and show it to emerge\nindependently from learning the projector, (ii) we use these findings to obtain\nnontrivial performance gains for VICReg, (iii) we demonstrate that the scope of\nVCReg goes beyond SSL by using it to solve Independent Component Analysis. We\nhope that our findings will support the adoption of VCReg in SSL and beyond.",
    "descriptor": "",
    "authors": [
      "Gr\u00e9goire Mialon",
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14905"
  },
  {
    "id": "arXiv:2209.14907",
    "title": "Patients' Severity States Classification based on Electronic Health  Record (EHR) Data using Multiple Machine Learning and Deep Learning  Approaches",
    "abstract": "This research presents an examination of categorizing the severity states of\npatients based on their electronic health records during a certain time range\nusing multiple machine learning and deep learning approaches. The suggested\nmethod uses an EHR dataset collected from an open-source platform to categorize\nseverity. Some tools were used in this research, such as openRefine was used to\npre-process, RapidMiner was used for implementing three algorithms (Fast Large\nMargin, Generalized Linear Model, Multi-layer Feed-forward Neural Network) and\nTableau was used to visualize the data, for implementation of algorithms we\nused Google Colab. Here we implemented several supervised and unsupervised\nalgorithms along with semi-supervised and deep learning algorithms. The\nexperimental results reveal that hyperparameter-tuned Random Forest\noutperformed all the other supervised machine learning algorithms with 76%\naccuracy as well as Generalized Linear algorithm achieved the highest precision\nscore 78%, whereas the hyperparameter-tuned Hierarchical Clustering with 86%\nprecision score and Gaussian Mixture Model with 61% accuracy outperformed other\nunsupervised approaches. Dimensionality Reduction improved results a lot for\nmost unsupervised techniques. For implementing Deep Learning we employed a\nfeed-forward neural network (multi-layer) and the Fast Large Margin approach\nfor semi-supervised learning. The Fast Large Margin performed really well with\na recall score of 84% and an F1 score of 78%. Finally, the Multi-layer\nFeed-forward Neural Network performed admirably with 75% accuracy, 75%\nprecision, 87% recall, 81% F1 score.",
    "descriptor": "\nComments: 32 pages, 13 figures, and 14 tables\n",
    "authors": [
      "A. N. M. Sajedul Alam",
      "Rimi Reza",
      "Asir Abrar",
      "Tanvir Ahmed",
      "Salsabil Ahmed",
      "Shihab Sharar",
      "Annajiat Alim Rasel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14907"
  },
  {
    "id": "arXiv:2209.14910",
    "title": "Graph Modeling in Computer Assisted Automotive Development",
    "abstract": "We consider graph modeling for a knowledge graph for vehicle development,\nwith a focus on crash safety. An organized schema that incorporates information\nfrom various structured and unstructured data sources is provided, which\nincludes relevant concepts within the domain. In particular, we propose\nsemantics for crash computer aided engineering (CAE) data, which enables\nsearchability, filtering, recommendation, and prediction for crash CAE data\nduring the development process. This graph modeling considers the CAE data in\nthe context of the R\\&D development process and vehicle safety. Consequently,\nwe connect CAE data to the protocols that are used to assess vehicle safety\nperformances. The R\\&D process includes CAD engineering and safety attributes,\nwith a focus on multidisciplinary problem-solving. We describe previous efforts\nin graph modeling in comparison to our proposal, discuss its strengths and\nlimitations, and identify areas for future work.",
    "descriptor": "\nComments: ICKG 2022\n",
    "authors": [
      "Anahita Pakiman",
      "Jochen Garcke"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2209.14910"
  },
  {
    "id": "arXiv:2209.14914",
    "title": "Quantum invariants for the graph isomorphism problem",
    "abstract": "Graph Isomorphism is such an important problem in computer science, that it\nhas been widely studied over the last decades. It is well known that it belongs\nto NP class, but is not NP-complete. It is thought to be of comparable\ndifficulty to integer factorisation. The best known proved algorithm to solve\nthis problem in general, was proposed by L\\'aszl\\'o Babai and Eugene Luks in\n1983.\nRecently, there has been some research in the topic by using quantum\ncomputing, that also leads the present piece of research. In fact, we present a\nquantum computing algorithm that defines an invariant over Graph Isomorphism\ncharacterisation. This quantum algorithm is able to distinguish more\nnon-isomorphic graphs than most of the known invariants so far. The proof of\ncorrectness and some hints illustrating the extent and reason of the\nimprovement are also included in this paper.",
    "descriptor": "",
    "authors": [
      "Hern\u00e1n I. de la Cruz",
      "Fernando L. Pelayo",
      "Vicente Pascual",
      "Jose J. Paulet",
      "Fernando Cuartero",
      "Luis Llana",
      "Mauro Mezzini"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2209.14914"
  },
  {
    "id": "arXiv:2209.14915",
    "title": "Evaluating the temporal understanding of neural networks on event-based  action recognition with DVS-Gesture-Chain",
    "abstract": "Enabling artificial neural networks (ANNs) to have temporal understanding in\nvisual tasks is an essential requirement in order to achieve complete\nperception of video sequences. A wide range of benchmark datasets is available\nto allow for the evaluation of such capabilities when using conventional\nframe-based video sequences. In contrast, evaluating them for systems targeting\nneuromorphic data is still a challenge due to the lack of appropriate datasets.\nIn this work we define a new benchmark task for action recognition in\nevent-based video sequences, DVS-Gesture-Chain (DVS-GC), which is based on the\ntemporal combination of multiple gestures from the widely used DVS-Gesture\ndataset. This methodology allows to create datasets that are arbitrarily\ncomplex in the temporal dimension. Using our newly defined task, we evaluate\nthe spatio-temporal understanding of different feed-forward convolutional ANNs\nand convolutional Spiking Neural Networks (SNNs). Our study proves how the\noriginal DVS Gesture benchmark could be solved by networks without temporal\nunderstanding, unlike the new DVS-GC which demands an understanding of the\nordering of events. From there, we provide a study showing how certain elements\nsuch as spiking neurons or time-dependent weights allow for temporal\nunderstanding in feed-forward networks without the need for recurrent\nconnections. Code available at:\nhttps://github.com/VicenteAlex/DVS-Gesture-Chain",
    "descriptor": "",
    "authors": [
      "Alex Vicente-Sola",
      "Davide L. Manna",
      "Paul Kirkland",
      "Gaetano Di Caterina",
      "Trevor Bihl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14915"
  },
  {
    "id": "arXiv:2209.14916",
    "title": "Human Motion Diffusion Model",
    "abstract": "Natural and expressive human motion generation is the holy grail of computer\nanimation. It is a challenging task, due to the diversity of possible motion,\nhuman perceptual sensitivity to it, and the difficulty of accurately describing\nit. Therefore, current generative solutions are either low-quality or limited\nin expressiveness. Diffusion models, which have already shown remarkable\ngenerative capabilities in other domains, are promising candidates for human\nmotion due to their many-to-many nature, but they tend to be resource hungry\nand hard to control. In this paper, we introduce Motion Diffusion Model (MDM),\na carefully adapted classifier-free diffusion-based generative model for the\nhuman motion domain. MDM is transformer-based, combining insights from motion\ngeneration literature. A notable design-choice is the prediction of the sample,\nrather than the noise, in each diffusion step. This facilitates the use of\nestablished geometric losses on the locations and velocities of the motion,\nsuch as the foot contact loss. As we demonstrate, MDM is a generic approach,\nenabling different modes of conditioning, and different generation tasks. We\nshow that our model is trained with lightweight resources and yet achieves\nstate-of-the-art results on leading benchmarks for text-to-motion and\naction-to-motion. https://guytevet.github.io/mdm-page/ .",
    "descriptor": "",
    "authors": [
      "Guy Tevet",
      "Sigal Raab",
      "Brian Gordon",
      "Yonatan Shafir",
      "Amit H. Bermano",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2209.14916"
  },
  {
    "id": "arXiv:2209.14921",
    "title": "IvySyn: Automated Vulnerability Discovery for Deep Learning Frameworks",
    "abstract": "We present IvySyn: the first fully-automated framework for vulnerability\ndiscovery in Deep Learning (DL) frameworks. IvySyn leverages the\nstatically-typed nature of native APIs in order to automatically perform\ntype-aware mutation-based fuzzing on low-level kernel APIs. Given a set of\noffending inputs that trigger memory safety and fatal runtime errors in\nlow-level, native DL (C/C++) code, IvySyn automatically synthesizes code\nsnippets in high-level languages (e.g., in Python), which propagate offending\ninputs via high(er)-level APIs. Such code snippets essentially act as Proof of\nVulnerability, as they demonstrate the existence of bugs in native code that\nattackers can target through various high-level APIs. Our experimental\nevaluation shows that IvySyn significantly outperforms past approaches, both in\nterms of efficiency and effectiveness, in finding real vulnerabilities in\npopular DL frameworks. Specifically, we used IvySyn to test TensorFlow and\nPyTorch: although still an early research prototype, IvySyn has already helped\nthe corresponding TensorFlow and PyTorch framework developers to identify and\nfix 58 previously-unknown security vulnerabilities, and assign 36 unique CVEs.",
    "descriptor": "",
    "authors": [
      "Neophytos Christou",
      "Di Jin",
      "Vaggelis Atlidakis",
      "Baishakhi Ray",
      "Vasileios P. Kemerlis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14921"
  },
  {
    "id": "arXiv:2209.14922",
    "title": "GDIP: Gated Differentiable Image Processing for Object-Detection in  Adverse Conditions",
    "abstract": "Detecting objects under adverse weather and lighting conditions is crucial\nfor the safe and continuous operation of an autonomous vehicle, and remains an\nunsolved problem. We present a Gated Differentiable Image Processing (GDIP)\nblock, a domain-agnostic network architecture, which can be plugged into\nexisting object detection networks (e.g., Yolo) and trained end-to-end with\nadverse condition images such as those captured under fog and low lighting. Our\nproposed GDIP block learns to enhance images directly through the downstream\nobject detection loss. This is achieved by learning parameters of multiple\nimage pre-processing (IP) techniques that operate concurrently, with their\noutputs combined using weights learned through a novel gating mechanism. We\nfurther improve GDIP through a multi-stage guidance procedure for progressive\nimage enhancement. Finally, trading off accuracy for speed, we propose a\nvariant of GDIP that can be used as a regularizer for training Yolo, which\neliminates the need for GDIP-based image enhancement during inference,\nresulting in higher throughput and plausible real-world deployment. We\ndemonstrate significant improvement in detection performance over several\nstate-of-the-art methods through quantitative and qualitative studies on\nsynthetic datasets such as PascalVOC, and real-world foggy (RTTS) and\nlow-lighting (ExDark) datasets.",
    "descriptor": "\nComments: Submitted to ICRA2023. More information at this https URL\n",
    "authors": [
      "Sanket Kalwar",
      "Dhruv Patel",
      "Aakash Aanegola",
      "Krishna Reddy Konda",
      "Sourav Garg",
      "K Madhava Krishna"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14922"
  },
  {
    "id": "arXiv:2209.14923",
    "title": "Joint Convexity of Error Probability in Blocklength and Transmit Power  in the Finite Blocklength Regime",
    "abstract": "To support ultra-reliable and low-latency services for mission-critical\napplications, transmissions are usually carried via short blocklength codes,\ni.e., in the so-called finite blocklength (FBL) regime. Different from the\ninfinite blocklength regime where transmissions are assumed to be arbitrarily\nreliable at the Shannon's capacity, the reliability and capacity performances\nof an FBL transmission are impacted by the coding blocklength. The relationship\namong reliability, coding rate, blocklength and channel quality has recently\nbeen characterized in the literature, considering the FBL performance model. In\nthis paper, we follow this model, and prove the joint convexity of the FBL\nerror probability with respect to blocklength and transmit power within a\nregion of interest, as a key enabler for designing systems to achieve globally\noptimal performance levels. Moreover, we apply the joint convexity to general\nuse cases and efficiently solve the joint optimization problem in the setting\nwith multiple users. We also extend the applicability of the proposed approach\nby proving that the joint convexity still holds in fading channels, as well as\nin relaying networks. Via simulations, we validate our analytical results and\ndemonstrate the advantage of leveraging the joint convexity compared to other\ncommonly-applied approaches.",
    "descriptor": "\nComments: 33 pages, 7 figures, to be published in IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Yao Zhu",
      "Yulin Hu",
      "Xiaopeng Yuan",
      "M. Cenk Gursoy",
      "H. Vincent Poor",
      "Anke Schmeink"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.14923"
  },
  {
    "id": "arXiv:2209.14924",
    "title": "Chandojnanam: A Sanskrit Meter Identification and Utilization System",
    "abstract": "We present Chandoj\\~n\\=anam, a web-based Sanskrit meter (Chanda)\nidentification and utilization system. In addition to the core functionality of\nidentifying meters, it sports a friendly user interface to display the\nscansion, which is a graphical representation of the metrical pattern. The\nsystem supports identification of meters from uploaded images by using optical\ncharacter recognition (OCR) engines in the backend. It is also able to process\nentire text files at a time. The text can be processed in two modes, either by\ntreating it as a list of individual lines, or as a collection of verses. When a\nline or a verse does not correspond exactly to a known meter, Chandoj\\~n\\=anam\nis capable of finding fuzzy (i.e., approximate and close) matches based on\nsequence matching. This opens up the scope of a meter-based correction of\nerroneous digital corpora. The system is available for use at\nhttps://sanskrit.iitk.ac.in/jnanasangraha/chanda/, and the source code in the\nform of a Python library is made available at\nhttps://github.com/hrishikeshrt/chanda/.",
    "descriptor": "\nComments: to be published in \"18th World Sanskrit Conference (WSC 2023)\"\n",
    "authors": [
      "Hrishikesh Terdalkar",
      "Arnab Bhattacharya"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14924"
  },
  {
    "id": "arXiv:2209.14926",
    "title": "Domain-Unified Prompt Representations for Source-Free Domain  Generalization",
    "abstract": "Domain generalization (DG), aiming to make models work on unseen domains, is\na surefire way toward general artificial intelligence. Limited by the scale and\ndiversity of current DG datasets, it is difficult for existing methods to scale\nto diverse domains in open-world scenarios (e.g., science fiction and pixelate\nstyle). Therefore, the source-free domain generalization (SFDG) task is\nnecessary and challenging. To address this issue, we propose an approach based\non large-scale vision-language pretraining models (e.g., CLIP), which exploits\nthe extensive domain information embedded in it. The proposed scheme generates\ndiverse prompts from a domain bank that contains many more diverse domains than\nexisting DG datasets. Furthermore, our method yields domain-unified\nrepresentations from these prompts, thus being able to cope with samples from\nopen-world domains. Extensive experiments on mainstream DG datasets, namely\nPACS, VLCS, OfficeHome, and DomainNet, show that the proposed method achieves\ncompetitive performance compared to state-of-the-art (SOTA) DG methods that\nrequire source domain data for training. Besides, we collect a small datasets\nconsists of two domains to evaluate the open-world domain generalization\nability of the proposed method. The source code and the dataset will be made\npublicly available at\nhttps://github.com/muse1998/Source-Free-Domain-Generalization",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Hongjing Niu",
      "Hanting Li",
      "Feng Zhao",
      "Bin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14926"
  },
  {
    "id": "arXiv:2209.14927",
    "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a  Focus",
    "abstract": "Mobile UI understanding is important for enabling various interaction tasks\nsuch as UI automation and accessibility. Previous mobile UI modeling often\ndepends on the view hierarchy information of a screen, which directly provides\nthe structural data of the UI, with the hope to bypass challenging tasks of\nvisual modeling from screen pixels. However, view hierarchy is not always\navailable, and is often corrupted with missing object descriptions or\nmisaligned bounding box positions. As a result, although using view hierarchy\noffers some short-term gains, it may ultimately hinder the applicability and\nperformance of the model. In this paper, we propose Spotlight, a vision-only\napproach for mobile UI understanding. Specifically, we enhance a\nvision-language model that only takes the screenshot of the UI and a region of\ninterest on the screen -- the focus -- as the input. This general architecture\nis easily scalable and capable of performing a range of UI modeling tasks. Our\nexperiments show that our model obtains SoTA results on several representative\nUI tasks and outperforms previous methods that use both screenshots and view\nhierarchies as input. Furthermore, we explore the multi-task learning and\nfew-shot prompting capacity of the proposed models, demonstrating promising\nresults in the multi-task learning direction.",
    "descriptor": "",
    "authors": [
      "Gang Li",
      "Yang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14927"
  },
  {
    "id": "arXiv:2209.14930",
    "title": "Graph Anomaly Detection with Graph Neural Networks: Current Status and  Challenges",
    "abstract": "Graphs are used widely to model complex systems, and detecting anomalies in a\ngraph is an important task in the analysis of complex systems. Graph anomalies\nare patterns in a graph that do not conform to normal patterns expected of the\nattributes and/or structures of the graph. In recent years, graph neural\nnetworks (GNNs) have been studied extensively and have successfully performed\ndifficult machine learning tasks in node classification, link prediction, and\ngraph classification thanks to the highly expressive capability via message\npassing in effectively learning graph representations. To solve the graph\nanomaly detection problem, GNN-based methods leverage information about the\ngraph attributes (or features) and/or structures to learn to score anomalies\nappropriately. In this survey, we review the recent advances made in detecting\ngraph anomalies using GNN models. Specifically, we summarize GNN-based methods\naccording to the graph type (i.e., static and dynamic), the anomaly type (i.e.,\nnode, edge, subgraph, and whole graph), and the network architecture (e.g.,\ngraph autoencoder, graph convolutional network). To the best of our knowledge,\nthis survey is the first comprehensive review of graph anomaly detection\nmethods based on GNNs.",
    "descriptor": "\nComments: 9 pages, 2 figures, 1 tables; to appear in the IEEE Access (Please cite our journal version.)\n",
    "authors": [
      "Hwan Kim",
      "Byung Suk Lee",
      "Won-Yong Shin",
      "Sungsu Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.14930"
  },
  {
    "id": "arXiv:2209.14932",
    "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal  Features",
    "abstract": "In this paper we present a world model, which learns causal features using\nthe invariance principle. In particular, we use contrastive unsupervised\nlearning to learn the invariant causal features, which enforces invariance\nacross augmentations of irrelevant parts or styles of the observation. The\nworld-model-based reinforcement learning methods independently optimize\nrepresentation learning and the policy. Thus naive contrastive loss\nimplementation collapses due to a lack of supervisory signals to the\nrepresentation learning module. We propose an intervention invariant auxiliary\ntask to mitigate this issue. Specifically, we utilize depth prediction to\nexplicitly enforce the invariance and use data augmentation as style\nintervention on the RGB observation space. Our design leverages unsupervised\nrepresentation learning to learn the world model with invariant causal\nfeatures. Our proposed method significantly outperforms current\nstate-of-the-art model-based and model-free reinforcement learning methods on\nout-of-distribution point navigation tasks on the iGibson dataset. Moreover,\nour proposed model excels at the sim-to-real transfer of our perception\nlearning module. Finally, we evaluate our approach on the DeepMind control\nsuite and enforce invariance only implicitly since depth is not available.\nNevertheless, our proposed model performs on par with the state-of-the-art\ncounterpart.",
    "descriptor": "",
    "authors": [
      "Rudra P.K. Poudel",
      "Harit Pandya",
      "Roberto Cipolla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14932"
  },
  {
    "id": "arXiv:2209.14933",
    "title": "Training Normalizing Flows from Dependent Data",
    "abstract": "Normalizing flows are powerful non-parametric statistical models that\nfunction as a hybrid between density estimators and generative models. Current\nlearning algorithms for normalizing flows assume that data points are sampled\nindependently, an assumption that is frequently violated in practice, which may\nlead to erroneous density estimation and data generation. We propose a\nlikelihood objective of normalizing flows incorporating dependencies between\nthe data points, for which we derive a flexible and efficient learning\nalgorithm suitable for different dependency structures. We show that respecting\ndependencies between observations can improve empirical results on both\nsynthetic and real-world data.",
    "descriptor": "",
    "authors": [
      "Matthias Kirchler",
      "Christoph Lippert",
      "Marius Kloft"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14933"
  },
  {
    "id": "arXiv:2209.14934",
    "title": "Towards a sharp, structure preserving two-velocity model for two-phase  flow: transport of mass and momentum",
    "abstract": "The numerical modelling of convection dominated high density ratio two-phase\nflow poses several challenges, amongst which is resolving the relatively thin\nshear layer at the interface. To this end we propose a sharp discretisation of\nthe two-velocity model of the two-phase Navier-Stokes (NS) equations. This\nresults in the ability to model the shear layer, rather than resolving it, by\nallowing for a velocity discontinuity in the direction(s) tangential to the\ninterface.\nIn this paper we focus our attention on the transport of mass and momentum in\nthe presence of such a velocity discontinuity. We propose a generalisation of\nthe dimensionally unsplit geometric volume of fluid (VOF) method for the\nadvection of the interface in the two-velocity formulation. Sufficient\nconditions on the construction of donating regions are derived that ensure\nboundedness of the volume fraction for dimensionally unsplit advection methods.\nWe propose to interpolate the mass fluxes resulting from the dimensionally\nunsplit geometric VOF method for the advection of the staggered momentum field,\nresulting in semi-discrete energy conservation. Division of the momentum by the\nrespective mass, to obtain the velocity, is not always well-defined for nearly\nempty control volumes and therefore care is taken in the construction of the\nmomentum flux interpolant: our proposed flux interpolant guarantees that this\ndivision is always well-defined without being unnecessarily dissipative.\nBesides the newly proposed two-velocity model we also detail our exactly\nconservative (mass per phase and total linear momentum) implementation of the\none-velocity formulation of the two-phase NS equations, which will be used for\ncomparison.\nThe discretisation methods are validated using classical time-reversible flow\nfields, where in this paper the advection is uncoupled from the NS solver,\nwhich will be developed in a later paper.",
    "descriptor": "",
    "authors": [
      "Ronald A. Remmerswaal",
      "Arthur E. P. Veldman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14934"
  },
  {
    "id": "arXiv:2209.14935",
    "title": "Does Zero-Shot Reinforcement Learning Exist?",
    "abstract": "A zero-shot RL agent is an agent that can solve any RL task in a given\nenvironment, instantly with no additional planning or learning, after an\ninitial reward-free learning phase. This marks a shift from the reward-centric\nRL paradigm towards \"controllable\" agents that can follow arbitrary\ninstructions in an environment. Current RL agents can solve families of related\ntasks at best, or require planning anew for each task. Strategies for\napproximate zero-shot RL ave been suggested using successor features (SFs)\n[BBQ+ 18] or forward-backward (FB) representations [TO21], but testing has been\nlimited.\nAfter clarifying the relationships between these schemes, we introduce\nimproved losses and new SF models, and test the viability of zero-shot RL\nschemes systematically on tasks from the Unsupervised RL benchmark [LYL+21]. To\ndisentangle universal representation learning from exploration, we work in an\noffline setting and repeat the tests on several existing replay buffers.\nSFs appear to suffer from the choice of the elementary state features. SFs\nwith Laplacian eigenfunctions do well, while SFs based on auto-encoders,\ninverse curiosity, transition models, low-rank transition matrix, contrastive\nlearning, or diversity (APS), perform unconsistently. In contrast, FB\nrepresentations jointly learn the elementary and successor features from a\nsingle, principled criterion. They perform best and consistently across the\nboard, reaching 85% of supervised RL performance with a good replay buffer, in\na zero-shot manner.",
    "descriptor": "",
    "authors": [
      "Ahmed Touati",
      "J\u00e9r\u00e9my Rapin",
      "Yann Ollivier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14935"
  },
  {
    "id": "arXiv:2209.14940",
    "title": "Reinforcement Learning Algorithms: An Overview and Classification",
    "abstract": "The desire to make applications and machines more intelligent and the\naspiration to enable their operation without human interaction have been\ndriving innovations in neural networks, deep learning, and other machine\nlearning techniques. Although reinforcement learning has been primarily used in\nvideo games, recent advancements and the development of diverse and powerful\nreinforcement algorithms have enabled the reinforcement learning community to\nmove from playing video games to solving complex real-life problems in\nautonomous systems such as self-driving cars, delivery drones, and automated\nrobotics. Understanding the environment of an application and the algorithms'\nlimitations plays a vital role in selecting the appropriate reinforcement\nlearning algorithm that successfully solves the problem on hand in an efficient\nmanner. Consequently, in this study, we identify three main environment types\nand classify reinforcement learning algorithms according to those environment\ntypes. Moreover, within each category, we identify relationships between\nalgorithms. The overview of each algorithm provides insight into the\nalgorithms' foundations and reviews similarities and differences among\nalgorithms. This study provides a perspective on the field and helps\npractitioners and researchers to select the appropriate algorithm for their use\ncase.",
    "descriptor": "",
    "authors": [
      "Fadi AlMahamid",
      "Katarina Grolinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14940"
  },
  {
    "id": "arXiv:2209.14941",
    "title": "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual and  Language Learning",
    "abstract": "3D visual grounding aims to find the objects within point clouds mentioned by\nfree-form natural language descriptions with rich semantic components. However,\nexisting methods either extract the sentence-level features coupling all words,\nor focus more on object names, which would lose the word-level information or\nneglect other attributes. To alleviate this issue, we present EDA that\nExplicitly Decouples the textual attributes in a sentence and conducts Dense\nAlignment between such fine-grained language and point cloud objects.\nSpecifically, we first propose a text decoupling module to produce textual\nfeatures for every semantic component. Then, we design two losses to supervise\nthe dense matching between two modalities: the textual position alignment and\nobject semantic alignment. On top of that, we further introduce two new visual\ngrounding tasks, locating objects without object names and locating auxiliary\nobjects referenced in the descriptions, both of which can thoroughly evaluate\nthe model's dense alignment capacity. Through experiments, we achieve\nstate-of-the-art performance on two widely-adopted visual grounding datasets ,\nScanRefer and SR3D/NR3D, and obtain absolute leadership on our two\nnewly-proposed tasks. The code will be available at\nhttps://github.com/yanmin-wu/EDA.",
    "descriptor": "\nComments: 14 pages with 5 pages of supplementary material\n",
    "authors": [
      "Yanmin Wu",
      "Xinhua Cheng",
      "Renrui Zhang",
      "Zesen Cheng",
      "Jian Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14941"
  },
  {
    "id": "arXiv:2209.14945",
    "title": "Asynchronous Correspondences Between Hybrid Trajectory Semantics",
    "abstract": "We formalize the semantics of hybrid systems as sets of hybrid trajectories,\nincluding those generated by an hybrid transition system. We study the\nabstraction of hybrid trajectory semantics for verification, static analysis,\nand refinement. We mainly consider abstractions of hybrid semantics which\nestablish a correspondence between trajectories derived from a correspondence\nbetween states such as homomorphisms, simulations, bisimulations, and\npreservations with progress. We also consider abstractions that cannot be\ndefined stepwise like discretization. All these abstractions are Galois\nconnections between concrete and abstract hybrid trajectory or discrete trace\nsemantics. In contrast to semantic based abstractions, we investigate the\nproblematic trace-based composition of abstractions.",
    "descriptor": "",
    "authors": [
      "Patrick Cousot"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14945"
  },
  {
    "id": "arXiv:2209.14946",
    "title": "EiHi Net: Out-of-Distribution Generalization Paradigm",
    "abstract": "This paper develops a new EiHi net to solve the out-of-distribution (OoD)\ngeneralization problem in deep learning. EiHi net is a model learning paradigm\nthat can be blessed on any visual backbone. This paradigm can change the\nprevious learning method of the deep model, namely find out correlations\nbetween inductive sample features and corresponding categories, which suffers\nfrom pseudo correlations between indecisive features and labels. We fuse SimCLR\nand VIC-Reg via explicitly and dynamically establishing the original - positive\n- negative sample pair as a minimal learning element, the deep model\niteratively establishes a relationship close to the causal one between features\nand labels, while suppressing pseudo correlations. To further validate the\nproposed model, and strengthen the established causal relationships, we develop\na human-in-the-loop strategy, with few guidance samples, to prune the\nrepresentation space directly. Finally, it is shown that the developed EiHi net\nmakes significant improvements in the most difficult and typical OoD dataset\nNico, compared with the current SOTA results, without any domain ($e.g.$\nbackground, irrelevant features) information.",
    "descriptor": "\nComments: 30 pages, 7 figures\n",
    "authors": [
      "Qinglai Wei",
      "Beiming Yuan",
      "Diancheng Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14946"
  },
  {
    "id": "arXiv:2209.14951",
    "title": "Distributed decentralized receding horizon control for very large-scale  networks with application to LEO satellite mega-constellations",
    "abstract": "The implementation feasibility of decentralized control algorithms over very\nlarge-scale networks ushers in limiting constraints regarding communication,\ncomputational, and memory requirements. Thus, befitting control solutions must\nbe implemented distributively over the network of systems. In this paper, the\ndecentralized receding horizon control problem for very large-scale networks of\ndynamically decoupled systems with a common, possibly time-varying, control\nobjective is addressed. Each system is assumed to be modeled by linear\ntime-varying dynamics, which can be leveraged to approximate nonlinear systems\nabout successive points of operation. A distributed and decentralized receding\nhorizon control solution is put forward, which: i) takes communication delays\ninto account; ii) allows local communication exclusively; and iii) whose\ncomputational and memory requirements in each computational unit do not scale\nwith the dimension of the network. The scalability of the proposed solution\nenables emerging very large-scale applications of swarm robotics and networked\ncontrol. It is applied herein to the orbit control problem of LEO\nmega-constellations featuring high-fidelity numerical simulations for the\nStarlink constellation.",
    "descriptor": "",
    "authors": [
      "Leonardo Pedroso",
      "Pedro Batista"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14951"
  },
  {
    "id": "arXiv:2209.14952",
    "title": "CacheQL: Quantifying and Localizing Cache Side-Channel Vulnerabilities  in Production Software",
    "abstract": "Cache side-channel attacks extract secrets by examining how victim software\naccesses cache. To date, practical attacks on cryptosystems and media libraries\nare demonstrated under different scenarios, inferring secret keys and\nreconstructing private media data such as images.\nThis work first presents eight criteria for designing a full-fledged detector\nfor cache side-channel vulnerabilities. Then, we propose CacheQL, a novel\ndetector that meets all of these criteria. CacheQL precisely quantifies\ninformation leaks of binary code, by characterizing the distinguishability of\nlogged side channel traces. Moreover, CacheQL models leakage as a cooperative\ngame, allowing information leakage to be precisely distributed to program\npoints vulnerable to cache side channels. CacheQL is meticulously optimized to\nanalyze whole side channel traces logged from production software (where each\ntrace can have millions of records), and it alleviates randomness introduced by\ncryptographic blinding, ORAM, or real-world noises.\nOur evaluation quantifies side-channel leaks of production cryptographic and\nmedia software. We further localize vulnerabilities reported by previous\ndetectors and also identify a few hundred new leakage sites in recent OpenSSL\n(ver. 3.0.0), MbedTLS (ver. 3.0.0), Libgcrypt (ver. 1.9.4). Many of our\nlocalized program points are within the pre-processing modules of\ncryptosystems, which are not analyzed by existing works due to scalability. We\nalso localize vulnerabilities in Libjpeg (ver. 2.1.2) that leak privacy about\ninput images.",
    "descriptor": "\nComments: The extended version of a paper to appear in the Proceedings of the 32nd USENIX Security Symposium, 2023, (USENIX Security '23), 22 pages\n",
    "authors": [
      "Yuanyuan Yuan",
      "Zhibo Liu",
      "Shuai Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14952"
  },
  {
    "id": "arXiv:2209.14954",
    "title": "Generalized matrix nearness problems",
    "abstract": "We show that the global minimum solution of $\\lVert A - BXC \\rVert$ can be\nfound in closed-form with singular value decompositions and generalized\nsingular value decompositions for a variety of constraints on $X$ involving\nrank, norm, symmetry, two-sided product, and prescribed eigenvalue. This\nextends the solution of Friedland--Torokhti for the generalized\nrank-constrained approximation problem to other constraints as well as provides\nan alternative solution for rank constraint in terms of singular value\ndecompositions. For more complicated constraints on $X$ involving structures\nsuch as Toeplitz, Hankel, circulant, nonnegativity, stochasticity, positive\nsemidefiniteness, prescribed eigenvector, etc, we prove that a simple iterative\nmethod is linearly and globally convergent to the global minimum solution.",
    "descriptor": "\nComments: 18 pages, 2 figures\n",
    "authors": [
      "Zihao Li",
      "Lek-Heng Lim"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14954"
  },
  {
    "id": "arXiv:2209.14958",
    "title": "Co-Writing Screenplays and Theatre Scripts with Language Models: An  Evaluation by Industry Professionals",
    "abstract": "Language models are increasingly attracting interest from writers. However,\nsuch models lack long-range semantic coherence, limiting their usefulness for\nlongform creative writing. We address this limitation by applying language\nmodels hierarchically, in a system we call Dramatron. By building structural\ncontext via prompt chaining, Dramatron can generate coherent scripts and\nscreenplays complete with title, characters, story beats, location\ndescriptions, and dialogue. We illustrate Dramatron's usefulness as an\ninteractive co-creative system with a user study of 15 theatre and film\nindustry professionals. Participants co-wrote theatre scripts and screenplays\nwith Dramatron and engaged in open-ended interviews. We report critical\nreflections both from our interviewees and from independent reviewers who\nwatched stagings of the works to illustrate how both Dramatron and hierarchical\ntext generation could be useful for human-machine co-creativity. Finally, we\ndiscuss the suitability of Dramatron for co-creativity, ethical considerations\n-- including plagiarism and bias -- and participatory models for the design and\ndeployment of such tools.",
    "descriptor": "\nComments: 102 pages, 7 figures\n",
    "authors": [
      "Piotr Mirowski",
      "Kory W. Mathewson",
      "Jaylen Pittman",
      "Richard Evans"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14958"
  },
  {
    "id": "arXiv:2209.14965",
    "title": "DirectTracker: 3D Multi-Object Tracking Using Direct Image Alignment and  Photometric Bundle Adjustment",
    "abstract": "Direct methods have shown excellent performance in the applications of visual\nodometry and SLAM. In this work we propose to leverage their effectiveness for\nthe task of 3D multi-object tracking. To this end, we propose DirectTracker, a\nframework that effectively combines direct image alignment for the short-term\ntracking and sliding-window photometric bundle adjustment for 3D object\ndetection. Object proposals are estimated based on the sparse sliding-window\npointcloud and further refined using an optimization-based cost function that\ncarefully combines 3D and 2D cues to ensure consistency in image and world\nspace. We propose to evaluate 3D tracking using the recently introduced\nhigher-order tracking accuracy (HOTA) metric and the generalized intersection\nover union similarity measure to mitigate the limitations of the conventional\nuse of intersection over union for the evaluation of vision-based trackers. We\nperform evaluation on the KITTI Tracking benchmark for the Car class and show\ncompetitive performance in tracking objects both in 2D and 3D.",
    "descriptor": "\nComments: In Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS), 2022\n",
    "authors": [
      "Mariia Gladkova",
      "Nikita Korobov",
      "Nikolaus Demmel",
      "Aljo\u0161a O\u0161ep",
      "Laura Leal-Taix\u00e9",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14965"
  },
  {
    "id": "arXiv:2209.14969",
    "title": "Transfer Learning with Pretrained Remote Sensing Transformers",
    "abstract": "Although the remote sensing (RS) community has begun to pretrain transformers\n(intended to be fine-tuned on RS tasks), it is unclear how these models perform\nunder distribution shifts. Here, we pretrain a new RS transformer--called\nSatViT-V2--on 1.3 million satellite-derived RS images, then fine-tune it (along\nwith five other models) to investigate how it performs on distributions not\nseen during training. We split an expertly labeled land cover dataset into 14\ndatasets based on source biome. We train each model on each biome separately\nand test them on all other biomes. In all, this amounts to 1638 biome transfer\nexperiments. After fine-tuning, we find that SatViT-V2 outperforms SatViT-V1 by\n3.1% on in-distribution (matching biomes) and 2.8% on out-of-distribution\n(mismatching biomes) data. Additionally, we find that initializing fine-tuning\nfrom the linear probed solution (i.e., leveraging LPFT [1]) improves\nSatViT-V2's performance by another 1.2% on in-distribution and 2.4% on\nout-of-distribution data. Next, we find that pretrained RS transformers are\nbetter calibrated under distribution shifts than non-pretrained models and\nleveraging LPFT results in further improvements in model calibration. Lastly,\nwe find that five measures of distribution shift are moderately correlated with\nbiome transfer performance. We share code and pretrained model weights.\n(https://github.com/antofuller/SatViT)",
    "descriptor": "\nComments: Draft of manuscript that is being prepared for IEEE TGRS\n",
    "authors": [
      "Anthony Fuller",
      "Koreen Millard",
      "James R. Green"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14969"
  },
  {
    "id": "arXiv:2209.14970",
    "title": "3D Rendering Framework for Data Augmentation in Optical Character  Recognition",
    "abstract": "In this paper, we propose a data augmentation framework for Optical Character\nRecognition (OCR). The proposed framework is able to synthesize new viewing\nangles and illumination scenarios, effectively enriching any available OCR\ndataset. Its modular structure allows to be modified to match individual user\nrequirements. The framework enables to comfortably scale the enlargement factor\nof the available dataset. Furthermore, the proposed method is not restricted to\nsingle frame OCR but can also be applied to video OCR. We demonstrate the\nperformance of our framework by augmenting a 15% subset of the common Brno\nMobile OCR dataset. Our proposed framework is capable of leveraging the\nperformance of OCR applications especially for small datasets. Applying the\nproposed method, improvements of up to 2.79 percentage points in terms of\nCharacter Error Rate (CER), and up to 7.88 percentage points in terms of Word\nError Rate (WER) are achieved on the subset. Especially the recognition of\nchallenging text lines can be improved. The CER may be decreased by up to 14.92\npercentage points and the WER by up to 18.19 percentage points for this class.\nMoreover, we are able to achieve smaller error rates when training on the 15%\nsubset augmented with the proposed method than on the original non-augmented\nfull dataset.",
    "descriptor": "\nComments: IEEE International Symposium on Signals, Circuits and Systems (ISSCS), 1-4, July 2021\n",
    "authors": [
      "Andreas Spruck",
      "Maximiliane Hawesch",
      "Anatol Maier",
      "Christian Riess",
      "J\u00fcrgen Seiler",
      "Andr\u00e9 Kaup"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14970"
  },
  {
    "id": "arXiv:2209.14971",
    "title": "Hyperspectral Remote Sensing Benchmark Database for Oil Spill Detection  with an Isolation Forest-Guided Unsupervised Detector",
    "abstract": "Oil spill detection has attracted increasing attention in recent years since\nmarine oil spill accidents severely affect environments, natural resources, and\nthe lives of coastal inhabitants. Hyperspectral remote sensing images provide\nrich spectral information which is beneficial for the monitoring of oil spills\nin complex ocean scenarios. However, most of the existing approaches are based\non supervised and semi-supervised frameworks to detect oil spills from\nhyperspectral images (HSIs), which require a huge amount of effort to annotate\na certain number of high-quality training sets. In this study, we make the\nfirst attempt to develop an unsupervised oil spill detection method based on\nisolation forest for HSIs. First, considering that the noise level varies among\ndifferent bands, a noise variance estimation method is exploited to evaluate\nthe noise level of different bands, and the bands corrupted by severe noise are\nremoved. Second, kernel principal component analysis (KPCA) is employed to\nreduce the high dimensionality of the HSIs. Then, the probability of each pixel\nbelonging to one of the classes of seawater and oil spills is estimated with\nthe isolation forest, and a set of pseudo-labeled training samples is\nautomatically produced using the clustering algorithm on the detected\nprobability. Finally, an initial detection map can be obtained by performing\nthe support vector machine (SVM) on the dimension-reduced data, and then, the\ninitial detection result is further optimized with the extended random walker\n(ERW) model so as to improve the detection accuracy of oil spills. Experiments\non airborne hyperspectral oil spill data (HOSD) created by ourselves\ndemonstrate that the proposed method obtains superior detection performance\nwith respect to other state-of-the-art detection approaches.",
    "descriptor": "",
    "authors": [
      "Puhong Duan",
      "Xudong Kang",
      "Pedram Ghamisi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14971"
  },
  {
    "id": "arXiv:2209.14974",
    "title": "Greybox XAI: a Neural-Symbolic learning framework to produce  interpretable predictions for image classification",
    "abstract": "Although Deep Neural Networks (DNNs) have great generalization and prediction\ncapabilities, their functioning does not allow a detailed explanation of their\nbehavior. Opaque deep learning models are increasingly used to make important\npredictions in critical environments, and the danger is that they make and use\npredictions that cannot be justified or legitimized. Several eXplainable\nArtificial Intelligence (XAI) methods that separate explanations from machine\nlearning models have emerged, but have shortcomings in faithfulness to the\nmodel actual functioning and robustness. As a result, there is a widespread\nagreement on the importance of endowing Deep Learning models with explanatory\ncapabilities so that they can themselves provide an answer to why a particular\nprediction was made. First, we address the problem of the lack of universal\ncriteria for XAI by formalizing what an explanation is. We also introduced a\nset of axioms and definitions to clarify XAI from a mathematical perspective.\nFinally, we present the Greybox XAI, a framework that composes a DNN and a\ntransparent model thanks to the use of a symbolic Knowledge Base (KB). We\nextract a KB from the dataset and use it to train a transparent model (i.e., a\nlogistic regression). An encoder-decoder architecture is trained on RGB images\nto produce an output similar to the KB used by the transparent model. Once the\ntwo models are trained independently, they are used compositionally to form an\nexplainable predictive model. We show how this new architecture is accurate and\nexplainable in several datasets.",
    "descriptor": "\nComments: Accepted in Knowledge-Based Systems Journal\n",
    "authors": [
      "Adrien Bennetot",
      "Gianni Franchi",
      "Javier Del Ser",
      "Raja Chatila",
      "Natalia Diaz-Rodriguez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14974"
  },
  {
    "id": "arXiv:2209.14975",
    "title": "Causal Inference via Nonlinear Variable Decorrelation for Healthcare  Applications",
    "abstract": "Causal inference and model interpretability research are gaining increasing\nattention, especially in the domains of healthcare and bioinformatics. Despite\nrecent successes in this field, decorrelating features under nonlinear\nenvironments with human interpretable representations has not been adequately\ninvestigated. To address this issue, we introduce a novel method with a\nvariable decorrelation regularizer to handle both linear and nonlinear\nconfounding. Moreover, we employ association rules as new representations using\nassociation rule mining based on the original features to further proximate\nhuman decision patterns to increase model interpretability. Extensive\nexperiments are conducted on four healthcare datasets (one synthetically\ngenerated and three real-world collections on different diseases). Quantitative\nresults in comparison to baseline approaches on parameter estimation and\ncausality computation indicate the model's superior performance. Furthermore,\nexpert evaluation given by healthcare professionals validates the effectiveness\nand interpretability of the proposed model.",
    "descriptor": "",
    "authors": [
      "Junda Wang",
      "Weijian Li",
      "Han Wang",
      "Hanjia Lyu",
      "Caroline Thirukumaran",
      "Addisu Mesfin",
      "Jiebo Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14975"
  },
  {
    "id": "arXiv:2209.14976",
    "title": "Parameter-Conditioned Reachable Sets for Updating Safety Assurances  Online",
    "abstract": "Hamilton-Jacobi (HJ) reachability analysis is a powerful tool for analyzing\nthe safety of autonomous systems. However, the provided safety assurances are\noften predicated on the assumption that once deployed, the system or its\nenvironment does not evolve. Online, however, an autonomous system might\nexperience changes in system dynamics, control authority, external\ndisturbances, and/or the surrounding environment, requiring updated safety\nassurances. Rather than restarting the safety analysis from scratch, which can\nbe time-consuming and often intractable to perform online, we propose to\ncompute \\textit{parameter-conditioned} reachable sets. Assuming expected system\nand environment changes can be parameterized, we treat these parameters as\nvirtual states in the system and leverage recent advances in high-dimensional\nreachability analysis to solve the corresponding reachability problem offline.\nThis results in a family of reachable sets that is parameterized by the\nenvironment and system factors. Online, as these factors change, the system can\nsimply query the corresponding safety function from this family to ensure\nsystem safety, enabling a real-time update of the safety assurances. Through\nvarious simulation studies, we demonstrate the capability of our approach in\nmaintaining system safety despite the system and environment evolution.",
    "descriptor": "",
    "authors": [
      "Javier Borquez",
      "Kensuke Nakamura",
      "Somil Bansal"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.14976"
  },
  {
    "id": "arXiv:2209.14977",
    "title": "Transformer Meets Boundary Value Inverse Problems",
    "abstract": "A Transformer-based deep direct sampling method is proposed for solving a\nclass of boundary value inverse problem. A real-time reconstruction is achieved\nby evaluating the learned inverse operator between carefully designed data and\nthe reconstructed images. An effort is made to give a case study for a\nfundamental and critical question: whether and how one can benefit from the\ntheoretical structure of a mathematical problem to develop task-oriented and\nstructure-conforming deep neural network? Inspired by direct sampling methods\nfor inverse problems, the 1D boundary data are preprocessed by a partial\ndifferential equation-based feature map to yield 2D harmonic extensions in\ndifferent frequency input channels. Then, by introducing learnable non-local\nkernel, the approximation of direct sampling is recast to a modified attention\nmechanism. The proposed method is then applied to electrical impedance\ntomography, a well-known severely ill-posed nonlinear inverse problem. The new\nmethod achieves superior accuracy over its predecessors and contemporary\noperator learners, as well as shows robustness with respect to noise. This\nresearch shall strengthen the insights that the attention mechanism, despite\nbeing invented for natural language processing tasks, offers great flexibility\nto be modified in conformity with the a priori mathematical knowledge, which\nultimately leads to the design of more physics-compatible neural architectures.",
    "descriptor": "",
    "authors": [
      "Ruchi Guo",
      "Shuhao Cao",
      "Long Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14977"
  },
  {
    "id": "arXiv:2209.14981",
    "title": "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with  Latest Weight Averaging",
    "abstract": "Training vision or language models on large datasets can take days, if not\nweeks. We show that averaging the weights of the k latest checkpoints, each\ncollected at the end of an epoch, can speed up the training progression in\nterms of loss and accuracy by dozens of epochs, corresponding to time savings\nup to ~68 and ~30 GPU hours when training a ResNet50 on ImageNet and\nRoBERTa-Base model on WikiText-103, respectively. We also provide the code and\nmodel checkpoint trajectory to reproduce the results and facilitate research on\nreusing historical weights for faster convergence.",
    "descriptor": "",
    "authors": [
      "Jean Kaddour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14981"
  },
  {
    "id": "arXiv:2209.14987",
    "title": "No Free Lunch in \"Privacy for Free: How does Dataset Condensation Help  Privacy\"",
    "abstract": "New methods designed to preserve data privacy require careful scrutiny.\nFailure to preserve privacy is hard to detect, and yet can lead to catastrophic\nresults when a system implementing a ``privacy-preserving'' method is attacked.\nA recent work selected for an Outstanding Paper Award at ICML 2022 (Dong et\nal., 2022) claims that dataset condensation (DC) significantly improves data\nprivacy when training machine learning models. This claim is supported by\ntheoretical analysis of a specific dataset condensation technique and an\nempirical evaluation of resistance to some existing membership inference\nattacks.\nIn this note we examine the claims in the work of Dong et al. (2022) and\ndescribe major flaws in the empirical evaluation of the method and its\ntheoretical analysis. These flaws imply that their work does not provide\nstatistically significant evidence that DC improves the privacy of training ML\nmodels over a naive baseline. Moreover, previously published results show that\nDP-SGD, the standard approach to privacy preserving ML, simultaneously gives\nbetter accuracy and achieves a (provably) lower membership attack success rate.",
    "descriptor": "",
    "authors": [
      "Nicholas Carlini",
      "Vitaly Feldman",
      "Milad Nasr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14987"
  },
  {
    "id": "arXiv:2209.14988",
    "title": "DreamFusion: Text-to-3D using 2D Diffusion",
    "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion\nmodels trained on billions of image-text pairs. Adapting this approach to 3D\nsynthesis would require large-scale datasets of labeled 3D data and efficient\narchitectures for denoising 3D data, neither of which currently exist. In this\nwork, we circumvent these limitations by using a pretrained 2D text-to-image\ndiffusion model to perform text-to-3D synthesis. We introduce a loss based on\nprobability density distillation that enables the use of a 2D diffusion model\nas a prior for optimization of a parametric image generator. Using this loss in\na DeepDream-like procedure, we optimize a randomly-initialized 3D model (a\nNeural Radiance Field, or NeRF) via gradient descent such that its 2D\nrenderings from random angles achieve a low loss. The resulting 3D model of the\ngiven text can be viewed from any angle, relit by arbitrary illumination, or\ncomposited into any 3D environment. Our approach requires no 3D training data\nand no modifications to the image diffusion model, demonstrating the\neffectiveness of pretrained image diffusion models as priors.",
    "descriptor": "\nComments: see project page at this https URL\n",
    "authors": [
      "Ben Poole",
      "Ajay Jain",
      "Jonathan T. Barron",
      "Ben Mildenhall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14988"
  },
  {
    "id": "arXiv:2209.14990",
    "title": "Partially Observable RL with B-Stability: Unified Structural Condition  and Sharp Sample-Efficient Algorithms",
    "abstract": "Partial Observability -- where agents can only observe partial information\nabout the true underlying state of the system -- is ubiquitous in real-world\napplications of Reinforcement Learning (RL). Theoretically, learning a\nnear-optimal policy under partial observability is known to be hard in the\nworst case due to an exponential sample complexity lower bound. Recent work has\nidentified several tractable subclasses that are learnable with polynomial\nsamples, such as Partially Observable Markov Decision Processes (POMDPs) with\ncertain revealing or decodability conditions. However, this line of research is\nstill in its infancy, where (1) unified structural conditions enabling\nsample-efficient learning are lacking; (2) existing sample complexities for\nknown tractable subclasses are far from sharp; and (3) fewer sample-efficient\nalgorithms are available than in fully observable RL.\nThis paper advances all three aspects above for Partially Observable RL in\nthe general setting of Predictive State Representations (PSRs). First, we\npropose a natural and unified structural condition for PSRs called\n\\emph{B-stability}. B-stable PSRs encompasses the vast majority of known\ntractable subclasses such as weakly revealing POMDPs, low-rank\nfuture-sufficient POMDPs, decodable POMDPs, and regular PSRs. Next, we show\nthat any B-stable PSR can be learned with polynomial samples in relevant\nproblem parameters. When instantiated in the aforementioned subclasses, our\nsample complexities improve substantially over the current best ones. Finally,\nour results are achieved by three algorithms simultaneously: Optimistic Maximum\nLikelihood Estimation, Estimation-to-Decisions, and Model-Based Optimistic\nPosterior Sampling. The latter two algorithms are new for sample-efficient\nlearning of POMDPs/PSRs.",
    "descriptor": "",
    "authors": [
      "Fan Chen",
      "Yu Bai",
      "Song Mei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14990"
  },
  {
    "id": "arXiv:2209.14996",
    "title": "Multiple Modes for Continual Learning",
    "abstract": "Adapting model parameters to incoming streams of data is a crucial factor to\ndeep learning scalability. Interestingly, prior continual learning strategies\nin online settings inadvertently anchor their updated parameters to a local\nparameter subspace to remember old tasks, else drift away from the subspace and\nforget. From this observation, we formulate a trade-off between constructing\nmultiple parameter modes and allocating tasks per mode. Mode-Optimized Task\nAllocation (MOTA), our contributed adaptation strategy, trains multiple modes\nin parallel, then optimizes task allocation per mode. We empirically\ndemonstrate improvements over baseline continual learning strategies and across\nvarying distribution shifts, namely sub-population, domain, and task shift.",
    "descriptor": "",
    "authors": [
      "Siddhartha Datta",
      "Nigel Shadbolt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14996"
  },
  {
    "id": "arXiv:2209.14997",
    "title": "Optimistic MLE -- A Generic Model-based Algorithm for Partially  Observable Sequential Decision Making",
    "abstract": "This paper introduces a simple efficient learning algorithms for general\nsequential decision making. The algorithm combines Optimism for exploration\nwith Maximum Likelihood Estimation for model estimation, which is thus named\nOMLE. We prove that OMLE learns the near-optimal policies of an enormously rich\nclass of sequential decision making problems in a polynomial number of samples.\nThis rich class includes not only a majority of known tractable model-based\nReinforcement Learning (RL) problems (such as tabular MDPs, factored MDPs, low\nwitness rank problems, tabular weakly-revealing/observable POMDPs and\nmulti-step decodable POMDPs), but also many new challenging RL problems\nespecially in the partially observable setting that were not previously known\nto be tractable.\nNotably, the new problems addressed by this paper include (1) observable\nPOMDPs with continuous observation and function approximation, where we achieve\nthe first sample complexity that is completely independent of the size of\nobservation space; (2) well-conditioned low-rank sequential decision making\nproblems (also known as Predictive State Representations (PSRs)), which include\nand generalize all known tractable POMDP examples under a more intrinsic\nrepresentation; (3) general sequential decision making problems under SAIL\ncondition, which unifies our existing understandings of model-based RL in both\nfully observable and partially observable settings. SAIL condition is\nidentified by this paper, which can be viewed as a natural generalization of\nBellman/witness rank to address partial observability.",
    "descriptor": "",
    "authors": [
      "Qinghua Liu",
      "Praneeth Netrapalli",
      "Csaba Szepesvari",
      "Chi Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14997"
  },
  {
    "id": "arXiv:2209.15000",
    "title": "REST: REtrieve & Self-Train for generative action recognition",
    "abstract": "This work is on training a generative action/video recognition model whose\noutput is a free-form action-specific caption describing the video (rather than\nan action class label). A generative approach has practical advantages like\nproducing more fine-grained and human-readable output, and being naturally\nopen-world. To this end, we propose to adapt a pre-trained generative Vision &\nLanguage (V&L) Foundation Model for video/action recognition. While recently\nthere have been a few attempts to adapt V&L models trained with contrastive\nlearning (e.g. CLIP) for video/action, to the best of our knowledge, we propose\nthe very first method that sets outs to accomplish this goal for a generative\nmodel. We firstly show that direct fine-tuning of a generative model to produce\naction classes suffers from severe overfitting. To alleviate this, we introduce\nREST, a training framework consisting of two key components: an unsupervised\nmethod for adapting the generative model to action/video by means of\npseudo-caption generation and Self-training, i.e. without using any\naction-specific labels; (b) a Retrieval approach based on CLIP for discovering\na diverse set of pseudo-captions for each video to train the model.\nImportantly, we show that both components are necessary to obtain high\naccuracy. We evaluate REST on the problem of zero-shot action recognition where\nwe show that our approach is very competitive when compared to contrastive\nlearning-based methods. Code will be made available.",
    "descriptor": "",
    "authors": [
      "Adrian Bulat",
      "Enrique Sanchez",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15000"
  },
  {
    "id": "arXiv:2209.15001",
    "title": "Dilated Neighborhood Attention Transformer",
    "abstract": "Transformers are quickly becoming one of the most heavily applied deep\nlearning architectures across modalities, domains, and tasks. In vision, on top\nof ongoing efforts into plain transformers, hierarchical transformers have also\ngained significant attention, thanks to their performance and easy integration\ninto existing frameworks. These models typically employ localized attention\nmechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin\nTransformer's Shifted Window Self Attention. While effective at reducing self\nattention's quadratic complexity, local attention weakens two of the most\ndesirable properties of self attention: long range inter-dependency modeling,\nand global receptive field. In this paper, we introduce Dilated Neighborhood\nAttention (DiNA), a natural, flexible and efficient extension to NA that can\ncapture more global context and expand receptive fields exponentially at no\nadditional cost. NA's local attention and DiNA's sparse global attention\ncomplement each other, and therefore we introduce Dilated Neighborhood\nAttention Transformer (DiNAT), a new hierarchical vision transformer built upon\nboth. DiNAT variants enjoy significant improvements over attention-based\nbaselines such as NAT and Swin, as well as modern convolutional baseline\nConvNeXt. Our Large model is ahead of its Swin counterpart by 1.5% box AP in\nCOCO object detection, 1.3% mask AP in COCO instance segmentation, and 1.1%\nmIoU in ADE20K semantic segmentation, and faster in throughput. We believe\ncombinations of NA and DiNA have the potential to empower various tasks beyond\nthose presented in this paper. To support and encourage research in this\ndirection, in vision and beyond, we open-source our project at:\nhttps://github.com/SHI-Labs/Neighborhood-Attention-Transformer.",
    "descriptor": "",
    "authors": [
      "Ali Hassani",
      "Humphrey Shi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15001"
  },
  {
    "id": "arXiv:2209.15003",
    "title": "Compositional Semantic Parsing with Large Language Models",
    "abstract": "Humans can reason compositionally when presented with new tasks. Previous\nresearch shows that appropriate prompting techniques enable large language\nmodels (LLMs) to solve artificial compositional generalization tasks such as\nSCAN. In this work, we identify additional challenges in more realistic\nsemantic parsing tasks with larger vocabulary and refine these prompting\ntechniques to address them. Our best method is based on least-to-most\nprompting: it decomposes the problem using prompting-based syntactic parsing,\nthen uses this decomposition to select appropriate exemplars and to\nsequentially generate the semantic parse. This method allows us to set a new\nstate of the art for CFQ while requiring only 1% of the training data used by\ntraditional approaches. Due to the general nature of our approach, we expect\nsimilar efforts will lead to new results in other tasks and domains, especially\nfor knowledge-intensive applications.",
    "descriptor": "",
    "authors": [
      "Andrew Drozdov",
      "Nathanael Sch\u00e4rli",
      "Ekin Akyu\u00fcrek",
      "Nathan Scales",
      "Xinying Song",
      "Xinyun Chen",
      "Olivier Bousquet",
      "Denny Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15003"
  },
  {
    "id": "arXiv:2209.15006",
    "title": "Effective Vision Transformer Training: A Data-Centric Perspective",
    "abstract": "Vision Transformers (ViTs) have shown promising performance compared with\nConvolutional Neural Networks (CNNs), but the training of ViTs is much harder\nthan CNNs. In this paper, we define several metrics, including Dynamic Data\nProportion (DDP) and Knowledge Assimilation Rate (KAR), to investigate the\ntraining process, and divide it into three periods accordingly: formation,\ngrowth and exploration. In particular, at the last stage of training, we\nobserve that only a tiny portion of training examples is used to optimize the\nmodel. Given the data-hungry nature of ViTs, we thus ask a simple but important\nquestion: is it possible to provide abundant ``effective'' training examples at\nEVERY stage of training? To address this issue, we need to address two critical\nquestions, \\ie, how to measure the ``effectiveness'' of individual training\nexamples, and how to systematically generate enough number of ``effective''\nexamples when they are running out. To answer the first question, we find that\nthe ``difficulty'' of training samples can be adopted as an indicator to\nmeasure the ``effectiveness'' of training samples. To cope with the second\nquestion, we propose to dynamically adjust the ``difficulty'' distribution of\nthe training data in these evolution stages. To achieve these two purposes, we\npropose a novel data-centric ViT training framework to dynamically measure the\n``difficulty'' of training samples and generate ``effective'' samples for\nmodels at different training stages. Furthermore, to further enlarge the number\nof ``effective'' samples and alleviate the overfitting problem in the late\ntraining stage of ViTs, we propose a patch-level erasing strategy dubbed\nPatchErasing. Extensive experiments demonstrate the effectiveness of the\nproposed data-centric ViT training framework and techniques.",
    "descriptor": "",
    "authors": [
      "Benjia Zhou",
      "Pichao Wang",
      "Jun Wan",
      "Yanyan Liang",
      "Fan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15006"
  },
  {
    "id": "arXiv:2209.15007",
    "title": "Understanding Collapse in Non-Contrastive Learning",
    "abstract": "Contrastive methods have led a recent surge in the performance of\nself-supervised representation learning (SSL). Recent methods like BYOL or\nSimSiam purportedly distill these contrastive methods down to their essence,\nremoving bells and whistles, including the negative examples, that do not\ncontribute to downstream performance. These \"non-contrastive\" methods work\nsurprisingly well without using negatives even though the global minimum lies\nat trivial collapse. We empirically analyze these non-contrastive methods and\nfind that SimSiam is extraordinarily sensitive to dataset and model size. In\nparticular, SimSiam representations undergo partial dimensional collapse if the\nmodel is too small relative to the dataset size. We propose a metric to measure\nthe degree of this collapse and show that it can be used to forecast the\ndownstream task performance without any fine-tuning or labels. We further\nanalyze architectural design choices and their effect on the downstream\nperformance. Finally, we demonstrate that shifting to a continual learning\nsetting acts as a regularizer and prevents collapse, and a hybrid between\ncontinual and multi-epoch training can improve linear probe accuracy by as many\nas 18 percentage points using ResNet-18 on ImageNet.",
    "descriptor": "\nComments: Published at ECCV 2022\n",
    "authors": [
      "Alexander C. Li",
      "Alexei A. Efros",
      "Deepak Pathak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15007"
  },
  {
    "id": "arXiv:2209.14297",
    "title": "Using Multivariate Linear Regression for Biochemical Oxygen Demand  Prediction in Waste Water",
    "abstract": "There exist opportunities for Multivariate Linear Regression (MLR) in the\nprediction of Biochemical Oxygen Demand (BOD) in waste water, using the diverse\nwater quality parameters as the input variables. The goal of this work is to\nexamine the capability of MLR in prediction of BOD in waste water through four\ninput variables: Dissolved Oxygen (DO), Nitrogen, Fecal Coliform and Total\nColiform. The four input variables have higher correlation strength to BOD out\nof the seven parameters examined for the strength of correlation. Machine\nLearning (ML) was done with both 80% and 90% of the data as the training set\nand 20% and 10% as the test set respectively. MLR performance was evaluated\nthrough the coefficient of correlation (r), Root Mean Square Error (RMSE) and\nthe percentage accuracy in prediction of BOD. The performance indices for the\ninput variables of Dissolved Oxygen, Nitrogen, Fecal Coliform and Total\nColiform in prediction of BOD are: RMSE=6.77mg/L, r=0.60 and accuracy 70.3% for\ntraining dataset of 80% and RMSE=6.74mg/L, r=0.60 and accuracy of 87.5% for\ntraining set of 90% of the dataset. It was found that increasing the percentage\nof the training set above 80% of the dataset improved the accuracy of the model\nonly but did not have a significant impact on the prediction capacity of the\nmodel. The results showed that MLR model could be successfully employed in the\nestimation of BOD in waste water using appropriately selected input parameters.",
    "descriptor": "",
    "authors": [
      "Isaiah K. Mutai",
      "Kristof Van Laerhoven",
      "Nancy W. Karuri",
      "Robert K. Tewo"
    ],
    "subjectives": [
      "Other Quantitative Biology (q-bio.OT)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2209.14297"
  },
  {
    "id": "arXiv:2209.14328",
    "title": "Scalably learning quantum many-body Hamiltonians from dynamical data",
    "abstract": "The physics of a closed quantum mechanical system is governed by its\nHamiltonian. However, in most practical situations, this Hamiltonian is not\nprecisely known, and ultimately all there is are data obtained from\nmeasurements on the system. In this work, we introduce a highly scalable,\ndata-driven approach to learning families of interacting many-body Hamiltonians\nfrom dynamical data, by bringing together techniques from gradient-based\noptimization from machine learning with efficient quantum state representations\nin terms of tensor networks. Our approach is highly practical, experimentally\nfriendly, and intrinsically scalable to allow for system sizes of above 100\nspins. In particular, we demonstrate on synthetic data that the algorithm works\neven if one is restricted to one simple initial state, a small number of\nsingle-qubit observables, and time evolution up to relatively short times. For\nthe concrete example of the one-dimensional Heisenberg model our algorithm\nexhibits an error constant in the system size and scaling as the inverse square\nroot of the size of the data set.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Frederik Wilde",
      "Augustine Kshetrimayum",
      "Ingo Roth",
      "Dominik Hangleiter",
      "Ryan Sweke",
      "Jens Eisert"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Quantum Gases (cond-mat.quant-gas)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14328"
  },
  {
    "id": "arXiv:2209.14335",
    "title": "Text Independent Speaker Identification System for Access Control",
    "abstract": "Even human intelligence system fails to offer 100% accuracy in identifying\nspeeches from a specific individual. Machine intelligence is trying to mimic\nhumans in speaker identification problems through various approaches to speech\nfeature extraction and speech modeling techniques. This paper presents a\ntext-independent speaker identification system that employs Mel Frequency\nCepstral Coefficients (MFCC) for feature extraction and k-Nearest Neighbor\n(kNN) for classification. The maximum cross-validation accuracy obtained was\n60%. This will be improved upon in subsequent research.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Oluyemi E. Adetoyi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2209.14335"
  },
  {
    "id": "arXiv:2209.14355",
    "title": "Generalized Kernel Regularized Least Squares",
    "abstract": "Kernel Regularized Least Squares (KRLS) is a popular method for flexibly\nestimating models that may have complex relationships between variables.\nHowever, its usefulness to many researchers is limited for two reasons. First,\nexisting approaches are inflexible and do not allow KRLS to be combined with\ntheoretically-motivated extensions such as fixed effects or non-linear\noutcomes. Second, estimation is extremely computationally intensive for even\nmodestly sized datasets.\nOur paper addresses both concerns by introducing generalized KRLS (gKRLS). We\nnote that KRLS can be re-formulated as a hierarchical model thereby allowing\neasy inference and modular model construction. Computationally, we also\nimplement random sketching to dramatically accelerate estimation while\nincurring a limited penalty in estimation quality. We demonstrate that gKRLS\ncan be fit on datasets with tens of thousands of observations in under one\nminute. Further, state-of-the-art techniques that require fitting the model\nover a dozen times (e.g. meta-learners) can be estimated quickly.",
    "descriptor": "",
    "authors": [
      "Qing Chang",
      "Max Goplerud"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2209.14355"
  },
  {
    "id": "arXiv:2209.14358",
    "title": "The minimal canonical form of a tensor network",
    "abstract": "Tensor networks have a gauge degree of freedom on the virtual degrees of\nfreedom that are contracted. A canonical form is a choice of fixing this degree\nof freedom. For matrix product states, choosing a canonical form is a powerful\ntool, both for theoretical and numerical purposes. On the other hand, for\ntensor networks in dimension two or greater there is only limited understanding\nof the gauge symmetry. Here we introduce a new canonical form, the minimal\ncanonical form, which applies to projected entangled pair states (PEPS) in any\ndimension, and prove a corresponding fundamental theorem. Already for matrix\nproduct states this gives a new canonical form, while in higher dimensions it\nis the first rigorous definition of a canonical form valid for any choice of\ntensor. We show that two tensors have the same minimal canonical forms if and\nonly if they are gauge equivalent up to taking limits; moreover, this is the\ncase if and only if they give the same quantum state for any geometry. In\nparticular, this implies that the latter problem is decidable - in contrast to\nthe well-known undecidability for PEPS on grids. We also provide rigorous\nalgorithms for computing minimal canonical forms. To achieve this we draw on\ngeometric invariant theory and recent progress in theoretical computer science\nin non-commutative group optimization.",
    "descriptor": "\nComments: 51 pages, more than 8 figures\n",
    "authors": [
      "Arturo Acuaviva",
      "Visu Makam",
      "Harold Nieuwboer",
      "David P\u00e9rez-Garc\u00eda",
      "Friedrich Sittner",
      "Michael Walter",
      "Freek Witteveen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Data Structures and Algorithms (cs.DS)",
      "Mathematical Physics (math-ph)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2209.14358"
  },
  {
    "id": "arXiv:2209.14368",
    "title": "Repeated Prophet Inequality with Near-optimal Bounds",
    "abstract": "In modern sample-driven Prophet Inequality, an adversary chooses a sequence\nof $n$ items with values $v_1, v_2, \\ldots, v_n$ to be presented to a decision\nmaker (DM). The process follows in two phases. In the first phase (sampling\nphase), some items, possibly selected at random, are revealed to the DM, but\nshe can never accept them. In the second phase, the DM is presented with the\nother items in a random order and online fashion. For each item, she must make\nan irrevocable decision to either accept the item and stop the process or\nreject the item forever and proceed to the next item. The goal of the DM is to\nmaximize the expected value as compared to a Prophet (or offline algorithm)\nthat has access to all information. In this setting, the sampling phase has no\ncost and is not part of the optimization process. However, in many scenarios,\nthe samples are obtained as part of the decision-making process.\nWe model this aspect as a two-phase Prophet Inequality where an adversary\nchooses a sequence of $2n$ items with values $v_1, v_2, \\ldots, v_{2n}$ and the\nitems are randomly ordered. Finally, there are two phases of the Prophet\nInequality problem with the first $n$-items and the rest of the items,\nrespectively. We show that some basic algorithms achieve a ratio of at most\n$0.450$. We present an algorithm that achieves a ratio of at least $0.495$.\nFinally, we show that for every algorithm the ratio it can achieve is at most\n$0.502$. Hence our algorithm is near-optimal.",
    "descriptor": "",
    "authors": [
      "Krishnendu Chatterjee",
      "Mona Mohammadi",
      "Raimundo Saona"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.14368"
  },
  {
    "id": "arXiv:2209.14397",
    "title": "Variational Bayes for robust radar single object tracking",
    "abstract": "We address object tracking by radar and the robustness of the current\nstate-of-the-art methods to process outliers. The standard tracking algorithms\nextract detections from radar image space to use it in the filtering stage.\nFiltering is performed by a Kalman filter, which assumes Gaussian distributed\nnoise. However, this assumption does not account for large modeling errors and\nresults in poor tracking performance during abrupt motions. We take the\nGaussian Sum Filter (single-object variant of the Multi Hypothesis Tracker) as\nour baseline and propose a modification by modelling process noise with a\ndistribution that has heavier tails than a Gaussian. Variational Bayes provides\na fast, computationally cheap inference algorithm. Our simulations show that -\nin the presence of process outliers - the robust tracker outperforms the\nGaussian Sum filter when tracking single objects.",
    "descriptor": "\nComments: 6 pages, 8 figures. Published as part of the proceedings of the IEEE International Workshop on Signal Processing Systems 2022\n",
    "authors": [
      "Alp Sar\u0131",
      "Tak Kaneko",
      "Lense H.M. Swaenen",
      "Wouter M. Kouw"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14397"
  },
  {
    "id": "arXiv:2209.14414",
    "title": "Optimistic Posterior Sampling for Reinforcement Learning with Few  Samples and Tight Guarantees",
    "abstract": "We consider reinforcement learning in an environment modeled by an episodic,\nfinite, stage-dependent Markov decision process of horizon $H$ with $S$ states,\nand $A$ actions. The performance of an agent is measured by the regret after\ninteracting with the environment for $T$ episodes. We propose an optimistic\nposterior sampling algorithm for reinforcement learning (OPSRL), a simple\nvariant of posterior sampling that only needs a number of posterior samples\nlogarithmic in $H$, $S$, $A$, and $T$ per state-action pair. For OPSRL we\nguarantee a high-probability regret bound of order at most\n$\\widetilde{\\mathcal{O}}(\\sqrt{H^3SAT})$ ignoring $\\text{poly}\\log(HSAT)$\nterms. The key novel technical ingredient is a new sharp anti-concentration\ninequality for linear forms which may be of independent interest. Specifically,\nwe extend the normal approximation-based lower bound for Beta distributions by\nAlfers and Dinges [1984] to Dirichlet distributions. Our bound matches the\nlower bound of order $\\Omega(\\sqrt{H^3SAT})$, thereby answering the open\nproblems raised by Agrawal and Jia [2017b] for the episodic setting.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.07704\n",
    "authors": [
      "Daniil Tiapkin",
      "Denis Belomestny",
      "Daniele Calandriello",
      "Eric Moulines",
      "Remi Munos",
      "Alexey Naumov",
      "Mark Rowland",
      "Michal Valko",
      "Pierre Menard"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14414"
  },
  {
    "id": "arXiv:2209.14449",
    "title": "Parameterized Quantum Circuits with Quantum Kernels for Machine  Learning: A Hybrid Quantum-Classical Approach",
    "abstract": "Quantum machine learning (QML) is the use of quantum computing for the\ncomputation of machine learning algorithms. With the prevalence and importance\nof classical data, a hybrid quantum-classical approach to QML is called for.\nParameterized Quantum Circuits (PQCs), and particularly Quantum Kernel PQCs,\nare generally used in the hybrid approach to QML. In this paper we discuss some\nimportant aspects of PQCs with quantum kernels including PQCs, quantum kernels,\nquantum kernels with quantum advantage, and the trainability of quantum\nkernels. We conclude that quantum kernels with hybrid kernel methods, a.k.a.\nquantum kernel methods, offer distinct advantages as a hybrid approach to QML.\nNot only do they apply to Noisy Intermediate-Scale Quantum (NISQ) devices, but\nthey also can be used to solve all types of machine learning problems including\nregression, classification, clustering, and dimension reduction. Furthermore,\nbeyond quantum utility, quantum advantage can be attained if the quantum\nkernels, i.e., the quantum feature encodings, are classically intractable.",
    "descriptor": "",
    "authors": [
      "Daniel T. Chang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14449"
  },
  {
    "id": "arXiv:2209.14467",
    "title": "Reducing Positional Variance in Cross-sectional Abdominal CT Slices with  Deep Conditional Generative Models",
    "abstract": "2D low-dose single-slice abdominal computed tomography (CT) slice enables\ndirect measurements of body composition, which are critical to quantitatively\ncharacterizing health relationships on aging. However, longitudinal analysis of\nbody composition changes using 2D abdominal slices is challenging due to\npositional variance between longitudinal slices acquired in different years. To\nreduce the positional variance, we extend the conditional generative models to\nour C-SliceGen that takes an arbitrary axial slice in the abdominal region as\nthe condition and generates a defined vertebral level slice by estimating the\nstructural changes in the latent space. Experiments on 1170 subjects from an\nin-house dataset and 50 subjects from BTCV MICCAI Challenge 2015 show that our\nmodel can generate high quality images in terms of realism and similarity.\nExternal experiments on 20 subjects from the Baltimore Longitudinal Study of\nAging (BLSA) dataset that contains longitudinal single abdominal slices\nvalidate that our method can harmonize the slice positional variance in terms\nof muscle and visceral fat area. Our approach provides a promising direction of\nmapping slices from different vertebral levels to a target slice to reduce\npositional variance for single slice longitudinal analysis. The source code is\navailable at: https://github.com/MASILab/C-SliceGen.",
    "descriptor": "\nComments: 11 pages, 4 figures\n",
    "authors": [
      "Xin Yu",
      "Qi Yang",
      "Yucheng Tang",
      "Riqiang Gao",
      "Shunxing Bao",
      "LeonY. Cai",
      "Ho Hin Lee",
      "Yuankai Huo",
      "Ann Zenobia Moore",
      "Luigi Ferrucci",
      "Bennett A. Landman"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14467"
  },
  {
    "id": "arXiv:2209.14472",
    "title": "medigan: A Python Library of Pretrained Generative Models for Enriched  Data Access in Medical Imaging",
    "abstract": "Synthetic data generated by generative models can enhance the performance and\ncapabilities of data-hungry deep learning models in medical imaging. However,\nthere is (1) limited availability of (synthetic) datasets and (2) generative\nmodels are complex to train, which hinders their adoption in research and\nclinical applications. To reduce this entry barrier, we propose medigan, a\none-stop shop for pretrained generative models implemented as an open-source\nframework-agnostic Python library. medigan allows researchers and developers to\ncreate, increase, and domain-adapt their training data in just a few lines of\ncode. Guided by design decisions based on gathered end-user requirements, we\nimplement medigan based on modular components for generative model (i)\nexecution, (ii) visualisation, (iii) search & ranking, and (iv) contribution.\nThe library's scalability and design is demonstrated by its growing number of\nintegrated and readily-usable pretrained generative models consisting of 21\nmodels utilising 9 different Generative Adversarial Network architectures\ntrained on 11 datasets from 4 domains, namely, mammography, endoscopy, x-ray,\nand MRI. Furthermore, 3 applications of medigan are analysed in this work,\nwhich include (a) enabling community-wide sharing of restricted data, (b)\ninvestigating generative model evaluation metrics, and (c) improving clinical\ndownstream tasks. In (b), extending on common medical image synthesis\nassessment and reporting standards, we show Fr\\'echet Inception Distance\nvariability based on image normalisation and radiology-specific feature\nextraction.",
    "descriptor": "\nComments: 32 pages, 7 figures\n",
    "authors": [
      "Richard Osuala",
      "Grzegorz Skorupko",
      "Noussair Lazrak",
      "Lidia Garrucho",
      "Eloy Garc\u00eda",
      "Smriti Joshi",
      "Socayna Jouide",
      "Michael Rutherford",
      "Fred Prior",
      "Kaisar Kushibar",
      "Oliver Diaz",
      "Karim Lekadir"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14472"
  },
  {
    "id": "arXiv:2209.14501",
    "title": "On Quantum Speedups for Nonconvex Optimization via Quantum Tunneling  Walks",
    "abstract": "Classical algorithms are often not effective for solving nonconvex\noptimization problems where local minima are separated by high barriers. In\nthis paper, we explore possible quantum speedups for nonconvex optimization by\nleveraging the global effect of quantum tunneling. Specifically, we introduce a\nquantum algorithm termed the quantum tunneling walk (QTW) and apply it to\nnonconvex problems where local minima are approximately global minima. We show\nthat QTW achieves quantum speedup over classical stochastic gradient descents\n(SGD) when the barriers between different local minima are high but thin and\nthe minima are flat. Based on this observation, we construct a specific\ndouble-well landscape, where classical algorithms cannot efficiently hit one\ntarget well knowing the other well but QTW can when given proper initial states\nnear the known well. Finally, we corroborate our findings with numerical\nexperiments.",
    "descriptor": "\nComments: 38 pages, 10 figures\n",
    "authors": [
      "Yizhou Liu",
      "Weijie J. Su",
      "Tongyang Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.14501"
  },
  {
    "id": "arXiv:2209.14530",
    "title": "Low-Stabilizer-Complexity Quantum States Are Not Pseudorandom",
    "abstract": "We show that quantum states with \"low stabilizer complexity\" can be\nefficiently distinguished from Haar-random. Specifically, given an $n$-qubit\npure state $|\\psi\\rangle$, we give an efficient algorithm that distinguishes\nwhether $|\\psi\\rangle$ is (i) Haar-random or (ii) a state with stabilizer\nfidelity at least $\\frac{1}{k}$ (i.e., has fidelity at least $\\frac{1}{k}$ with\nsome stabilizer state), promised that one of these is the case. With black-box\naccess to $|\\psi\\rangle$, our algorithm uses $O\\!\\left( k^{12}\n\\log(1/\\delta)\\right)$ copies of $|\\psi\\rangle$ and $O\\!\\left(n k^{12}\n\\log(1/\\delta)\\right)$ time to succeed with probability at least $1-\\delta$,\nand, with access to a state preparation unitary for $|\\psi\\rangle$ (and its\ninverse), $O\\!\\left( k^{3} \\log(1/\\delta)\\right)$ queries and $O\\!\\left(n k^{3}\n\\log(1/\\delta)\\right)$ time suffice.\nAs a corollary, we prove that $\\omega(\\log(n))$ $T$-gates are necessary for\nany Clifford+$T$ circuit to prepare computationally pseudorandom quantum\nstates, a first-of-its-kind lower bound.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Sabee Grewal",
      "Vishnu Iyer",
      "William Kretschmer",
      "Daniel Liang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14530"
  },
  {
    "id": "arXiv:2209.14540",
    "title": "NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction",
    "abstract": "This paper proposes a novel and fast self-supervised solution for sparse-view\nCBCT reconstruction (Cone Beam Computed Tomography) that requires no external\ntraining data. Specifically, the desired attenuation coefficients are\nrepresented as a continuous function of 3D spatial coordinates, parameterized\nby a fully-connected deep neural network. We synthesize projections discretely\nand train the network by minimizing the error between real and synthesized\nprojections. A learning-based encoder entailing hash coding is adopted to help\nthe network capture high-frequency details. This encoder outperforms the\ncommonly used frequency-domain encoder in terms of having higher performance\nand efficiency, because it exploits the smoothness and sparsity of human\norgans. Experiments have been conducted on both human organ and phantom\ndatasets. The proposed method achieves state-of-the-art accuracy and spends\nreasonably short computation time.",
    "descriptor": "\nComments: MICCAI2022 (Oral)\n",
    "authors": [
      "Ruyi Zha",
      "Yanhao Zhang",
      "Hongdong Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14540"
  },
  {
    "id": "arXiv:2209.14566",
    "title": "Diffusion Adversarial Representation Learning for Self-supervised Vessel  Segmentation",
    "abstract": "Vessel segmentation in medical images is one of the important tasks in the\ndiagnosis of vascular diseases and therapy planning. Although learning-based\nsegmentation approaches have been extensively studied, a large amount of\nground-truth labels are required in supervised methods and confusing background\nstructures make neural networks hard to segment vessels in an unsupervised\nmanner. To address this, here we introduce a novel diffusion adversarial\nrepresentation learning (DARL) model that leverages a denoising diffusion\nprobabilistic model with adversarial learning, and apply it for vessel\nsegmentation. In particular, for self-supervised vessel segmentation, DARL\nlearns background image distribution using a diffusion module, which lets a\ngeneration module effectively provide vessel representations. Also, by\nadversarial learning based on the proposed switchable spatially-adaptive\ndenormalization, our model estimates synthetic fake vessel images as well as\nvessel segmentation masks, which further makes the model capture\nvessel-relevant semantic information. Once the proposed model is trained, the\nmodel generates segmentation masks by one step and can be applied to general\nvascular structure segmentation of coronary angiography and retinal images.\nExperimental results on various datasets show that our method significantly\noutperforms existing unsupervised and self-supervised methods in vessel\nsegmentation.",
    "descriptor": "",
    "authors": [
      "Boah Kim",
      "Yujin Oh",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14566"
  },
  {
    "id": "arXiv:2209.14568",
    "title": "Rethinking Counterfactual Explanations as Local and Regional  Counterfactual Policies",
    "abstract": "Among the challenges not yet resolved for Counterfactual Explanations (CE),\nthere are stability, synthesis of the various CE and the lack of\nplausibility/sparsity guarantees. From a more practical point of view, recent\nstudies show that the prescribed counterfactual recourses are often not\nimplemented exactly by the individuals and demonstrate that most\nstate-of-the-art CE algorithms are very likely to fail in this noisy\nenvironment. To address these issues, we propose a probabilistic framework that\ngives a sparse local counterfactual rule for each observation: we provide rules\nthat give a range of values that can change the decision with a given high\nprobability instead of giving diverse CE. In addition, the recourses derived\nfrom these rules are robust by construction. These local rules are aggregated\ninto a regional counterfactual rule to ensure the stability of the\ncounterfactual explanations across observations. Our local and regional rules\nguarantee that the recourses are faithful to the data distribution because our\nrules use a consistent estimator of the probabilities of changing the decision\nbased on a Random Forest. In addition, these probabilities give interpretable\nand sparse rules as we select the smallest set of variables having a given\nprobability of changing the decision. Codes for computing our counterfactual\nrules are available, and we compare their relevancy with standard CE and recent\nsimilar attempts.",
    "descriptor": "",
    "authors": [
      "Salim I. Amoukou",
      "Nicolas J.B Brunel"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14568"
  },
  {
    "id": "arXiv:2209.14577",
    "title": "Rectified Flow: A Marginal Preserving Approach to Optimal Transport",
    "abstract": "We present a flow-based approach to the optimal transport (OT) problem\nbetween two continuous distributions $\\pi_0,\\pi_1$ on $\\mathbb{R}^d$, of\nminimizing a transport cost $\\mathbb{E}[c(X_1-X_0)]$ in the set of couplings\n$(X_0,X_1)$ whose marginal distributions on $X_0,X_1$ equals $\\pi_0,\\pi_1$,\nrespectively, where $c$ is a cost function. Our method iteratively constructs a\nsequence of neural ordinary differentiable equations (ODE), each learned by\nsolving a simple unconstrained regression problem, which monotonically reduce\nthe transport cost while automatically preserving the marginal constraints.\nThis yields a monotonic interior approach that traverses inside the set of\nvalid couplings to decrease the transport cost, which distinguishes itself from\nmost existing approaches that enforce the coupling constraints from the\noutside. The main idea of the method draws from rectified flow, a recent\napproach that simultaneously decreases the whole family of transport costs\ninduced by convex functions $c$ (and is hence multi-objective in nature), but\nis not tailored to minimize a specific transport cost. Our method is a\nsingle-object variant of rectified flow that guarantees to solve the OT problem\nfor a fixed, user-specified convex cost function $c$.",
    "descriptor": "",
    "authors": [
      "Qiang Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14577"
  },
  {
    "id": "arXiv:2209.14604",
    "title": "Spherical Image Inpainting with Frame Transformation and Data-driven  Prior Deep Networks",
    "abstract": "Spherical image processing has been widely applied in many important fields,\nsuch as omnidirectional vision for autonomous cars, global climate modelling,\nand medical imaging. It is non-trivial to extend an algorithm developed for\nflat images to the spherical ones. In this work, we focus on the challenging\ntask of spherical image inpainting with deep learning-based regularizer.\nInstead of a naive application of existing models for planar images, we employ\na fast directional spherical Haar framelet transform and develop a novel\noptimization framework based on a sparsity assumption of the framelet\ntransform. Furthermore, by employing progressive encoder-decoder architecture,\na new and better-performed deep CNN denoiser is carefully designed and works as\nan implicit regularizer. Finally, we use a plug-and-play method to handle the\nproposed optimization model, which can be implemented efficiently by training\nthe CNN denoiser prior. Numerical experiments are conducted and show that the\nproposed algorithms can greatly recover damaged spherical images and achieve\nthe best performance over purely using deep learning denoiser and plug-and-play\nmodel.",
    "descriptor": "",
    "authors": [
      "Jianfei Li",
      "Chaoyan Huang",
      "Raymond Chan",
      "Han Feng",
      "Micheal Ng",
      "Tieyong Zeng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14604"
  },
  {
    "id": "arXiv:2209.14622",
    "title": "From geodesic extrapolation to a variational BDF2 scheme for Wasserstein  gradient flows",
    "abstract": "We introduce a time discretization for Wasserstein gradient flows based on\nthe classical Backward Differentiation Formula of order two. The main building\nblock of the scheme is the notion of geodesic extrapolation in the Wasserstein\nspace, which in general is not uniquely defined. We propose several possible\ndefinitions for such an operation, and we prove convergence of the resulting\nscheme to the limit PDE, in the case of the Fokker-Planck equation. For a\nspecific choice of extrapolation we also prove a more general result, that is\nconvergence towards EVI flows. Finally, we propose a variational finite volume\ndiscretization of the scheme which numerically achieves second order accuracy\nin both space and time.",
    "descriptor": "",
    "authors": [
      "Andrea Natale",
      "Gabriele Todeschi",
      "Thomas Gallou\u00ebt"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14622"
  },
  {
    "id": "arXiv:2209.14657",
    "title": "Correlated Feature Aggregation by Region Helps Distinguish Aggressive  from Indolent Clear Cell Renal Cell Carcinoma Subtypes on CT",
    "abstract": "Renal cell carcinoma (RCC) is a common cancer that varies in clinical\nbehavior. Indolent RCC is often low-grade without necrosis and can be monitored\nwithout treatment. Aggressive RCC is often high-grade and can cause metastasis\nand death if not promptly detected and treated. While most kidney cancers are\ndetected on CT scans, grading is based on histology from invasive biopsy or\nsurgery. Determining aggressiveness on CT images is clinically important as it\nfacilitates risk stratification and treatment planning. This study aims to use\nmachine learning methods to identify radiology features that correlate with\nfeatures on pathology to facilitate assessment of cancer aggressiveness on CT\nimages instead of histology. This paper presents a novel automated method,\nCorrelated Feature Aggregation By Region (CorrFABR), for classifying\naggressiveness of clear cell RCC by leveraging correlations between radiology\nand corresponding unaligned pathology images. CorrFABR consists of three main\nsteps: (1) Feature Aggregation where region-level features are extracted from\nradiology and pathology images, (2) Fusion where radiology features correlated\nwith pathology features are learned on a region level, and (3) Prediction where\nthe learned correlated features are used to distinguish aggressive from\nindolent clear cell RCC using CT alone as input. Thus, during training,\nCorrFABR learns from both radiology and pathology images, but during inference,\nCorrFABR will distinguish aggressive from indolent clear cell RCC using CT\nalone, in the absence of pathology images. CorrFABR improved classification\nperformance over radiology features alone, with an increase in binary\nclassification F1-score from 0.68 (0.04) to 0.73 (0.03). This demonstrates the\npotential of incorporating pathology disease characteristics for improved\nclassification of aggressiveness of clear cell RCC on CT images.",
    "descriptor": "\nComments: Submitted to Medical Image Analysis\n",
    "authors": [
      "Karin Stacke",
      "Indrani Bhattacharya",
      "Justin R. Tse",
      "James D. Brooks",
      "Geoffrey A. Sonn",
      "Mirabela Rusu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14657"
  },
  {
    "id": "arXiv:2209.14664",
    "title": "Causal inference in drug discovery and development",
    "abstract": "To discover new drugs is to seek and to prove causality. As an emerging\napproach leveraging human knowledge and creativity, data, and machine\nintelligence, causal inference holds the promise of reducing cognitive bias and\nimproving decision making in drug discovery. While it has been applied across\nthe value chain, the concepts and practice of causal inference remain obscure\nto many practitioners. This article offers a non-technical introduction to\ncausal inference, reviews its recent applications, and discusses opportunities\nand challenges of adopting the causal language in drug discovery and\ndevelopment.",
    "descriptor": "",
    "authors": [
      "Tom Michoel",
      "Jitao David Zhang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2209.14664"
  },
  {
    "id": "arXiv:2209.14687",
    "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
    "abstract": "Diffusion models have been recently studied as powerful generative inverse\nproblem solvers, owing to their high quality reconstructions and the ease of\ncombining existing iterative solvers. However, most works focus on solving\nsimple linear inverse problems in noiseless settings, which significantly\nunder-represents the complexity of real-world problems. In this work, we extend\ndiffusion solvers to efficiently handle general noisy (non)linear inverse\nproblems via the Laplace approximation of the posterior sampling.\nInterestingly, the resulting posterior sampling scheme is a blended version of\ndiffusion sampling with the manifold constrained gradient without a strict\nmeasurement consistency projection step, yielding a more desirable generative\npath in noisy settings compared to the previous studies. Our method\ndemonstrates that diffusion models can incorporate various measurement noise\nstatistics such as Gaussian and Poisson, and also efficiently handle noisy\nnonlinear inverse problems such as Fourier phase retrieval and non-uniform\ndeblurring.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Hyungjin Chung",
      "Jeongsol Kim",
      "Michael T. Mccann",
      "Marc L. Klasky",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14687"
  },
  {
    "id": "arXiv:2209.14728",
    "title": "Dependent Bayesian Lenses: Categories of Bidirectional Markov Kernels  with Canonical Bayesian Inversion",
    "abstract": "We generalise an existing construction of Bayesian Lenses to admit lenses\nbetween pairs of objects where the backwards object is dependent on states on\nthe forwards object (interpreted as probability distributions). This gives a\nnatural setting for studying stochastic maps with Bayesian inverses restricted\nto the points supported by a given prior. In order to state this formally we\ndevelop a proposed definition by Fritz of a support object in a Markov category\nand show that these give rise to a section into the category of dependent\nBayesian lenses encoding a more canonical notion of Bayesian inversion.",
    "descriptor": "\nComments: Work-in-progress preprint submitted to SYCO 9\n",
    "authors": [
      "Dylan Braithwaite",
      "Jules Hedges"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Logic in Computer Science (cs.LO)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.14728"
  },
  {
    "id": "arXiv:2209.14738",
    "title": "Optimal Stopping with Gaussian Processes",
    "abstract": "We propose a novel group of Gaussian Process based algorithms for fast\napproximate optimal stopping of time series with specific applications to\nfinancial markets. We show that structural properties commonly exhibited by\nfinancial time series (e.g., the tendency to mean-revert) allow the use of\nGaussian and Deep Gaussian Process models that further enable us to\nanalytically evaluate optimal stopping value functions and policies. We\nadditionally quantify uncertainty in the value function by propagating the\nprice model through the optimal stopping analysis. We compare and contrast our\nproposed methods against a sampling-based method, as well as a deep learning\nbased benchmark that is currently considered the state-of-the-art in the\nliterature. We show that our family of algorithms outperforms benchmarks on\nthree historical time series datasets that include intra-day and end-of-day\nequity asset prices as well as the daily US treasury yield curve rates.",
    "descriptor": "",
    "authors": [
      "Kshama Dwarakanath",
      "Danial Dervovic",
      "Peyman Tavallali",
      "Svitlana S Vyetrenko",
      "Tucker Balch"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14738"
  },
  {
    "id": "arXiv:2209.14754",
    "title": "On the Physics-Informed Neural Networks for Quantum Computers",
    "abstract": "Physics-Informed Neural Networks (PINN) emerged as a powerful tool for\nsolving scientific computing problems, ranging from the solution of Partial\nDifferential Equations to data assimilation tasks. One of the advantages of\nusing PINN is to leverage the usage of Machine Learning computational\nframeworks relying on the combined usage of CPUs and co-processors, such as\naccelerators, to achieve maximum performance. This work investigates the\nfeasibility and potential of using the Quantum Processing Unit (QPU)\nco-processor in PINNs. We design a simple Quantum PINN to solve the\none-dimensional Poisson problem using a Continuous Variable quantum computing\nframework. We discuss the impact of different optimizers, PINN residual\nformulation, and quantum neural network depth on the quantum PINN accuracy. We\nshow that the optimizer exploration of the training landscape in the case of\nquantum PINN is not as effective as in classical PINN, and basic SGD optimizers\noutperform adaptive and high-order optimizers. Finally, we highlight the\ndifference in methods and algorithms between quantum and classical PINNs and\noutline future research challenges for quantum PINN development.",
    "descriptor": "\nComments: submitted to Frontiers\n",
    "authors": [
      "Stefano Markidis"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.14754"
  },
  {
    "id": "arXiv:2209.14770",
    "title": "R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and  COVID-19 Classification",
    "abstract": "Restoration of poor quality images with a blended set of artifacts plays a\nvital role for a reliable diagnosis. Existing studies have focused on specific\nrestoration problems such as image deblurring, denoising, and exposure\ncorrection where there is usually a strong assumption on the artifact type and\nseverity. As a pioneer study in blind X-ray restoration, we propose a joint\nmodel for generic image restoration and classification: Restore-to-Classify\nGenerative Adversarial Networks (R2C-GANs). Such a jointly optimized model\nkeeps any disease intact after the restoration. Therefore, this will naturally\nlead to a higher diagnosis performance thanks to the improved X-ray image\nquality. To accomplish this crucial objective, we define the restoration task\nas an Image-to-Image translation problem from poor quality having noisy,\nblurry, or over/under-exposed images to high quality image domain. The proposed\nR2C-GAN model is able to learn forward and inverse transforms between the two\ndomains using unpaired training samples. Simultaneously, the joint\nclassification preserves the disease label during restoration. Moreover, the\nR2C-GANs are equipped with operational layers/neurons reducing the network\ndepth and further boosting both restoration and classification performances.\nThe proposed joint model is extensively evaluated over the QaTa-COV19 dataset\nfor Coronavirus Disease 2019 (COVID-19) classification. The proposed\nrestoration approach achieves over 90% F1-Score which is significantly higher\nthan the performance of any deep model. Moreover, in the qualitative analysis,\nthe restoration performance of R2C-GANs is approved by a group of medical\ndoctors. We share the software implementation at\nhttps://github.com/meteahishali/R2C-GAN.",
    "descriptor": "",
    "authors": [
      "Mete Ahishali",
      "Aysen Degerli",
      "Serkan Kiranyaz",
      "Tahir Hamid",
      "Rashid Mazhar",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14770"
  },
  {
    "id": "arXiv:2209.14790",
    "title": "Sparse PCA With Multiple Components",
    "abstract": "Sparse Principal Component Analysis is a cardinal technique for obtaining\ncombinations of features, or principal components (PCs), that explain the\nvariance of high-dimensional datasets in an interpretable manner. At its heart,\nthis involves solving a sparsity and orthogonality constrained convex\nmaximization problem, which is extremely computationally challenging. Most\nexisting work address sparse PCA via heuristics such as iteratively computing\none sparse PC and deflating the covariance matrix, which does not guarantee the\northogonality, let alone the optimality, of the resulting solution. We\nchallenge this status by reformulating the orthogonality conditions as rank\nconstraints and optimizing over the sparsity and rank constraints\nsimultaneously. We design tight semidefinite relaxations and propose tractable\nsecond-order cone versions of these relaxations which supply high-quality upper\nbounds. We also design valid second-order cone inequalities which hold when\neach PC's individual sparsity is specified, and demonstrate that these\ninequalities tighten our relaxations significantly. Moreover, we propose exact\nmethods and rounding mechanisms that exploit these relaxations' tightness to\nobtain solutions with a bound gap on the order of 1%-5% for real-world datasets\nwith p = 100s or 1000s of features and r \\in {2, 3} components. We investigate\nthe performance of our methods in spiked covariance settings and demonstrate\nthat simultaneously considering the orthogonality and sparsity constraints\nleads to improvements in the Area Under the ROC curve of 2%-8% compared to\nstate-of-the-art deflation methods. All in all, our approach solves sparse PCA\nproblems with multiple components to certifiable (near) optimality in a\npractically tractable fashion.",
    "descriptor": "\nComments: Submitted to Operations Research; comments or suggestions welcome\n",
    "authors": [
      "Ryan Cory-Wright",
      "Jean Pauphilet"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14790"
  },
  {
    "id": "arXiv:2209.14803",
    "title": "polyBERT: A chemical language model to enable fully machine-driven  ultrafast polymer informatics",
    "abstract": "Polymers are a vital part of everyday life. Their chemical universe is so\nlarge that it presents unprecedented opportunities as well as significant\nchallenges to identify suitable application-specific candidates. We present a\ncomplete end-to-end machine-driven polymer informatics pipeline that can search\nthis space for suitable candidates at unprecedented speed and accuracy. This\npipeline includes a polymer chemical fingerprinting capability called polyBERT\n(inspired by Natural Language Processing concepts), and a multitask learning\napproach that maps the polyBERT fingerprints to a host of properties. polyBERT\nis a chemical linguist that treats the chemical structure of polymers as a\nchemical language. The present approach outstrips the best presently available\nconcepts for polymer property prediction based on handcrafted fingerprint\nschemes in speed by two orders of magnitude while preserving accuracy, thus\nmaking it a strong candidate for deployment in scalable architectures including\ncloud infrastructures.",
    "descriptor": "",
    "authors": [
      "Christopher Kuenneth",
      "Rampi Ramprasad"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14803"
  },
  {
    "id": "arXiv:2209.14849",
    "title": "Federated Stain Normalization for Computational Pathology",
    "abstract": "Although deep federated learning has received much attention in recent years,\nprogress has been made mainly in the context of natural images and barely for\ncomputational pathology. However, deep federated learning is an opportunity to\ncreate datasets that reflect the data diversity of many laboratories. Further,\nthe effort of dataset construction can be divided among many. Unfortunately,\nexisting algorithms cannot be easily applied to computational pathology since\nprevious work presupposes that data distributions of laboratories must be\nsimilar. This is an unlikely assumption, mainly since different laboratories\nhave different staining styles. As a solution, we propose BottleGAN, a\ngenerative model that can computationally align the staining styles of many\nlaboratories and can be trained in a privacy-preserving manner to foster\nfederated learning in computational pathology. We construct a heterogenic\nmulti-institutional dataset based on the PESO segmentation dataset and improve\nthe IOU by 42\\% compared to existing federated learning algorithms. An\nimplementation of BottleGAN is available at\nhttps://github.com/MECLabTUDA/BottleGAN",
    "descriptor": "\nComments: Accepted for Poster at MICCAI2022\n",
    "authors": [
      "Nicolas Wagner",
      "Moritz Fuchs",
      "Yuri Tolkach",
      "Anirban Mukhopadhyay"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14849"
  },
  {
    "id": "arXiv:2209.14863",
    "title": "Neural Networks Efficiently Learn Low-Dimensional Representations with  SGD",
    "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary\nwidth using stochastic gradient descent (SGD) where the input\n$\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$\nfollows a multiple-index model, i.e.,\n$y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$\nwith a noisy link function $g$. We prove that the first-layer weights of the NN\nconverge to the $k$-dimensional principal subspace spanned by the vectors\n$\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with\nweight decay is used for training. This phenomenon has several important\nconsequences when $k \\ll d$. First, by employing uniform convergence on this\nsmaller subspace, we establish a generalization error bound of\n$\\mathcal{O}(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is\nindependent of the width of the NN. We further demonstrate that, SGD-trained\nReLU NNs can learn a single-index target of the form\n$y=f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle) + \\epsilon$ by recovering the\nprincipal direction, with a sample complexity linear in $d$ (up to log\nfactors), where $f$ is a monotonic function with at most polynomial growth, and\n$\\epsilon$ is the noise. This is in contrast to the known $d^{\\Omega(p)}$\nsample requirement to learn any degree $p$ polynomial in the kernel regime, and\nit shows that NNs trained with SGD can outperform the neural tangent kernel at\ninitialization. Finally, we also provide compressibility guarantees for NNs\nusing the approximate low-rank structure produced by SGD.",
    "descriptor": "",
    "authors": [
      "Alireza Mousavi-Hosseini",
      "Sejun Park",
      "Manuela Girotti",
      "Ioannis Mitliagkas",
      "Murat A. Erdogdu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14863"
  },
  {
    "id": "arXiv:2209.14888",
    "title": "A note on Cournot-Nash equilibria and Optimal Transport between unequal  dimensions",
    "abstract": "This note is devoted to study a class of games with a continuum of players\nfor which Cournot-Nash equilibria can be obtained by minimisation of some cost\nrelated to Optimal Transport. In particular we focus on the case of an Optimal\nTransport term between unequal dimension. We also present several numerical\nsimulations.",
    "descriptor": "",
    "authors": [
      "Luca Nenna",
      "Brendan Pass"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14888"
  },
  {
    "id": "arXiv:2209.14937",
    "title": "NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizers",
    "abstract": "Classical machine learning models such as deep neural networks are usually\ntrained by using Stochastic Gradient Descent-based (SGD) algorithms. The\nclassical SGD can be interpreted as a discretization of the stochastic gradient\nflow. In this paper we propose a novel, robust and accelerated stochastic\noptimizer that relies on two key elements: (1) an accelerated Nesterov-like\nStochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel\ntype discretization. The convergence and stability of the obtained method,\nreferred to as NAG-GS, are first studied extensively in the case of the\nminimization of a quadratic function. This analysis allows us to come up with\nan optimal step size (or learning rate) in terms of rate of convergence while\nensuring the stability of NAG-GS. This is achieved by the careful analysis of\nthe spectral radius of the iteration matrix and the covariance matrix at\nstationarity with respect to all hyperparameters of our method. We show that\nNAG-GS is competitive with state-of-the-art methods such as momentum SGD with\nweight decay and AdamW for the training of machine learning models such as the\nlogistic regression model, the residual networks models on standard computer\nvision datasets, and Transformers in the frame of the GLUE benchmark.",
    "descriptor": "\nComments: We study Nesterov acceleration for the Stochastic Differential Equation\n",
    "authors": [
      "Valentin Leplat",
      "Daniil Merkulov",
      "Aleksandr Katrutsa",
      "Daniel Bershatsky",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14937"
  },
  {
    "id": "arXiv:2209.14947",
    "title": "Controlling and scripting laboratory hardware with open-source,  intuitive interfaces: OpenFlexure Voice Control and OpenFlexure Blockly",
    "abstract": "Making user interaction with laboratory equipment more convenient and\nintuitive should promote experimental work and help researchers to complete\ntheir tasks efficiently. The most common form of interaction in current\ninstrumentation is either direct tactile, with buttons and knobs, or interfaced\nthrough a computer, using a mouse and keyboard. Scripting is another function\ntypical of smart and automated laboratory equipment, yet users are currently\nrequired to learn bespoke programming languages and libraries for individual\npieces of equipment. In this paper we present two open-source, novel and\nintuitive ways of interacting with and scripting laboratory equipment. We\nchoose the OpenFlexure family of microscopes as our exemplar, due to their\nopen-source nature and smart control system. Firstly, we demonstrate\n\"OpenFlexure Voice Control\" to enable users to control the microscope\nhands-free. Secondly, we present \"OpenFlexure Blockly\" which uses the Blockly\nVisual Programming Language to enable users to easily create scripts for the\nmicroscope, using a drag and drop web interface. We explain the design choices\nwhen developing these tools, and discuss more typical use cases and more\ngeneral applications.",
    "descriptor": "",
    "authors": [
      "Samuel McDermott",
      "Richard Bowman",
      "Kerrianne Harrington",
      "William Wadsworth",
      "Pietro Cicuta"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.14947"
  },
  {
    "id": "arXiv:2209.14967",
    "title": "Statistical Learning and Inverse Problems: An Stochastic Gradient  Approach",
    "abstract": "Inverse problems are paramount in Science and Engineering. In this paper, we\nconsider the setup of Statistical Inverse Problem (SIP) and demonstrate how\nStochastic Gradient Descent (SGD) algorithms can be used in the linear SIP\nsetting. We provide consistency and finite sample bounds for the excess risk.\nWe also propose a modification for the SGD algorithm where we leverage machine\nlearning methods to smooth the stochastic gradients and improve empirical\nperformance. We exemplify the algorithm in a setting of great interest\nnowadays: the Functional Linear Regression model. In this case we consider a\nsynthetic data example and examples with a real data classification problem.",
    "descriptor": "",
    "authors": [
      "Yuri S. Fonseca",
      "Yuri F. Saporito"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14967"
  },
  {
    "id": "arXiv:2209.14973",
    "title": "Deep Unfolding for Iterative Stripe Noise Removal",
    "abstract": "The non-uniform photoelectric response of infrared imaging systems results in\nfixed-pattern stripe noise being superimposed on infrared images, which\nseverely reduces image quality. As the applications of degraded infrared images\nare limited, it is crucial to effectively preserve original details. Existing\nimage destriping methods struggle to concurrently remove all stripe noise\nartifacts, preserve image details and structures, and balance real-time\nperformance. In this paper we propose a novel algorithm for destriping degraded\nimages, which takes advantage of neighbouring column signal correlation to\nremove independent column stripe noise. This is achieved through an iterative\ndeep unfolding algorithm where the estimated noise of one network iteration is\nused as input to the next iteration. This progression substantially reduces the\nsearch space of possible function approximations, allowing for efficient\ntraining on larger datasets. The proposed method allows for a more precise\nestimation of stripe noise to preserve scene details more accurately. Extensive\nexperimental results demonstrate that the proposed model outperforms existing\ndestriping methods on artificially corrupted images on both quantitative and\nqualitative assessments.",
    "descriptor": "\nComments: Accepted in the International Joint Conference on Neural Networks (IJCNN) track of the 2022 IEEE World Congress on Computational Intelligence (IEEE WCCI 2022)\n",
    "authors": [
      "Zeshan Fayyaz",
      "Daniel Platnick",
      "Hannan Fayyaz",
      "Nariman Farsad"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14973"
  },
  {
    "id": "arXiv:2209.14978",
    "title": "Enumeration of max-pooling responses with generalized permutohedra",
    "abstract": "We investigate the combinatorics of max-pooling layers, which are functions\nthat downsample input arrays by taking the maximum over shifted windows of\ninput coordinates, and which are commonly used in convolutional neural\nnetworks. We obtain results on the number of linearity regions of these\nfunctions by equivalently counting the number of vertices of certain Minkowski\nsums of simplices. We characterize the faces of such polytopes and obtain\ngenerating functions and closed formulas for the number of vertices and facets\nin a 1D max-pooling layer depending on the size of the pooling windows and\nstride, and for the number of vertices in a special case of 2D max-pooling.",
    "descriptor": "\nComments: 32 pages, 10 figures, 3 tables\n",
    "authors": [
      "Laura Escobar",
      "Patricio Gallardo",
      "Javier Gonz\u00e1lez-Anaya",
      "Jos\u00e9 L. Gonz\u00e1lez",
      "Guido Mont\u00fafar",
      "Alejandro H. Morales"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14978"
  },
  {
    "id": "arXiv:2209.14991",
    "title": "Equivariant maps from invariant functions",
    "abstract": "In equivariant machine learning the idea is to restrict the learning to a\nhypothesis class where all the functions are equivariant with respect to some\ngroup action. Irreducible representations or invariant theory are typically\nused to parameterize the space of such functions. In this note, we explicate a\ngeneral procedure, attributed to Malgrange, to express all polynomial maps\nbetween linear spaces that are equivariant with respect to the action of a\ngroup $G$, given a characterization of the invariant polynomials on a bigger\nspace. The method also parametrizes smooth equivariant maps in the case that\n$G$ is a compact Lie group.",
    "descriptor": "",
    "authors": [
      "Ben Blum-Smith",
      "Soledad Villar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14991"
  },
  {
    "id": "arXiv:2209.14993",
    "title": "Discrete Microlocal Morse Theory",
    "abstract": "We establish several results combining discrete Morse theory and microlocal\nsheaf theory in the setting of finite posets and simplicial complexes. Our\nprimary tool is a computationally tractable description of the bounded derived\ncategory of sheaves on a poset with the Alexandrov topology. We prove that each\nbounded complex of sheaves on a finite poset admits a unique (up to isomorphism\nof complexes) minimal injective resolution, and we provide algorithms for\ncomputing minimal injective resolutions, as well as several useful functors\nbetween derived categories of sheaves. For the constant sheaf on a simplicial\ncomplex, we give asymptotically tight bounds on the complexity of computing the\nminimal injective resolution with this algorithm. Our main result is a novel\ndefinition of the discrete microsupport of a bounded complex of sheaves on a\nfinite poset. We detail several foundational properties of the discrete\nmicrosupport, as well as a microlocal generalization of the discrete\nhomological Morse theorem and Morse inequalities.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.02609\n",
    "authors": [
      "Adam Brown",
      "Ondrej Draganov"
    ],
    "subjectives": [
      "General Topology (math.GN)",
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2209.14993"
  },
  {
    "id": "arXiv:1705.09379",
    "title": "Tensor rank is not multiplicative under the tensor product",
    "abstract": "Comments: Fixed a typo in Remark 9",
    "descriptor": "\nComments: Fixed a typo in Remark 9\n",
    "authors": [
      "Matthias Christandl",
      "Asger Kj\u00e6rulff Jensen",
      "Jeroen Zuiddam"
    ],
    "subjectives": [
      "Commutative Algebra (math.AC)",
      "Computational Complexity (cs.CC)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/1705.09379"
  },
  {
    "id": "arXiv:1908.01981",
    "title": "Monotonic Representations of Outerplanar Graphs as Edge Intersection  Graphs of Paths on a Grid",
    "abstract": "Monotonic Representations of Outerplanar Graphs as Edge Intersection  Graphs of Paths on a Grid",
    "descriptor": "",
    "authors": [
      "Eranda Cela",
      "Elisabeth Gaar"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1908.01981"
  },
  {
    "id": "arXiv:2003.11351",
    "title": "Topology and adjunction in promise constraint satisfaction",
    "abstract": "Comments: This merges and subsumes arXiv:1904.03214 and arXiv:1907.00872. After reviews",
    "descriptor": "\nComments: This merges and subsumes arXiv:1904.03214 and arXiv:1907.00872. After reviews\n",
    "authors": [
      "Andrei Krokhin",
      "Jakub Opr\u0161al",
      "Marcin Wrochna",
      "Stanislav \u017divn\u00fd"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Logic in Computer Science (cs.LO)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2003.11351"
  },
  {
    "id": "arXiv:2004.03107",
    "title": "The Economics of Social Data",
    "abstract": "The Economics of Social Data",
    "descriptor": "",
    "authors": [
      "Dirk Bergemann",
      "Alessandro Bonatti",
      "Tan Gan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2004.03107"
  },
  {
    "id": "arXiv:2009.07916",
    "title": "Causal Bandits without prior knowledge using separating sets",
    "abstract": "Causal Bandits without prior knowledge using separating sets",
    "descriptor": "",
    "authors": [
      "Arnoud A.W.M. de Kroon",
      "Danielle Belgrave",
      "Joris M. Mooij"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.07916"
  },
  {
    "id": "arXiv:2009.14233",
    "title": "Lip-reading with Densely Connected Temporal Convolutional Networks",
    "abstract": "Comments: WACV 2021. An improved code implementation is available at: this https URL",
    "descriptor": "\nComments: WACV 2021. An improved code implementation is available at: this https URL\n",
    "authors": [
      "Pingchuan Ma",
      "Yujiang Wang",
      "Jie Shen",
      "Stavros Petridis",
      "Maja Pantic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.14233"
  },
  {
    "id": "arXiv:2011.03574",
    "title": "Single-Node Attacks for Fooling Graph Neural Networks",
    "abstract": "Comments: Appeared in Neurocomputing",
    "descriptor": "\nComments: Appeared in Neurocomputing\n",
    "authors": [
      "Ben Finkelshtein",
      "Chaim Baskin",
      "Evgenii Zheltonozhskii",
      "Uri Alon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.03574"
  },
  {
    "id": "arXiv:2012.07497",
    "title": "Is FFT Fast Enough for Beyond-5G Communications?",
    "abstract": "Comments: IEEE Access, 2022",
    "descriptor": "\nComments: IEEE Access, 2022\n",
    "authors": [
      "Saulo Queiroz",
      "Jo\u00e3o P. Vilela",
      "Edmundo Monteiro"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.07497"
  },
  {
    "id": "arXiv:2101.00834",
    "title": "Symbolic Control for Stochastic Systems via Finite Parity Games",
    "abstract": "Comments: 54 pages, under review in the Elsevier journal of Nonlinear Analysis: Hybrid Systems",
    "descriptor": "\nComments: 54 pages, under review in the Elsevier journal of Nonlinear Analysis: Hybrid Systems\n",
    "authors": [
      "Rupak Majumdar",
      "Kaushik Mallik",
      "Anne-Kathrin Schmuck",
      "Sadegh Soudjani"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.00834"
  },
  {
    "id": "arXiv:2103.02728",
    "title": "Morality, Machines and the Interpretation Problem: A value-based,  Wittgensteinian approach to building Moral Agents",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Cosmin Badea",
      "Gregory Artus"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2103.02728"
  },
  {
    "id": "arXiv:2103.10881",
    "title": "Building Specifications in the Event-B Institution",
    "abstract": "Comments: 55 pages, 25 figures",
    "descriptor": "\nComments: 55 pages, 25 figures\n",
    "authors": [
      "Marie Farrell",
      "Rosemary Monahan",
      "James F. Power"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2103.10881"
  },
  {
    "id": "arXiv:2103.12541",
    "title": "A Survey on Multimodal Disinformation Detection",
    "abstract": "Comments: Accepted at COLING-2022, disinformation, misinformation, factuality, harmfulness, fake news, propaganda, multimodality, text, images, videos, network structure, temporality",
    "descriptor": "\nComments: Accepted at COLING-2022, disinformation, misinformation, factuality, harmfulness, fake news, propaganda, multimodality, text, images, videos, network structure, temporality\n",
    "authors": [
      "Firoj Alam",
      "Stefano Cresci",
      "Tanmoy Chakraborty",
      "Fabrizio Silvestri",
      "Dimiter Dimitrov",
      "Giovanni Da San Martino",
      "Shaden Shaar",
      "Hamed Firooz",
      "Preslav Nakov"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2103.12541"
  },
  {
    "id": "arXiv:2104.02864",
    "title": "Self-Supervised Learning for Gastritis Detection with Gastric X-ray  Images",
    "abstract": "Self-Supervised Learning for Gastritis Detection with Gastric X-ray  Images",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.02864"
  },
  {
    "id": "arXiv:2104.03952",
    "title": "Dataset Summarization by K Principal Concepts",
    "abstract": "Dataset Summarization by K Principal Concepts",
    "descriptor": "",
    "authors": [
      "Niv Cohen",
      "Yedid Hoshen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.03952"
  },
  {
    "id": "arXiv:2104.12039",
    "title": "Optimizing Regular Expressions via Rewrite-Guided Synthesis",
    "abstract": "Optimizing Regular Expressions via Rewrite-Guided Synthesis",
    "descriptor": "",
    "authors": [
      "Jedidiah McClurg",
      "Miles Claver",
      "Jackson Garner",
      "Jake Vossen",
      "Jordan Schmerge",
      "Mehmet E. Belviranli"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2104.12039"
  },
  {
    "id": "arXiv:2105.14559",
    "title": "Active Learning in Bayesian Neural Networks with Balanced Entropy  Learning Principle",
    "abstract": "Active Learning in Bayesian Neural Networks with Balanced Entropy  Learning Principle",
    "descriptor": "",
    "authors": [
      "Jae Oh Woo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.14559"
  },
  {
    "id": "arXiv:2106.03535",
    "title": "Graph Neural Networks in Network Neuroscience",
    "abstract": "Graph Neural Networks in Network Neuroscience",
    "descriptor": "",
    "authors": [
      "Alaa Bessadok",
      "Mohamed Ali Mahjoub",
      "Islem Rekik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.03535"
  },
  {
    "id": "arXiv:2106.04527",
    "title": "LaplaceNet: A Hybrid Graph-Energy Neural Network for Deep  Semi-Supervised Classification",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Philip Sellars",
      "Angelica I. Aviles-Rivero",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04527"
  },
  {
    "id": "arXiv:2106.04856",
    "title": "Strongly Sublinear Algorithms for Testing Pattern Freeness",
    "abstract": "Comments: 28 pages, 2 figures; We thank anonymous reviewers for finding a mistake in an earlier version of the paper and for comments that helped us considerably improve the presentation",
    "descriptor": "\nComments: 28 pages, 2 figures; We thank anonymous reviewers for finding a mistake in an earlier version of the paper and for comments that helped us considerably improve the presentation\n",
    "authors": [
      "Ilan Newman",
      "Nithin Varma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.04856"
  },
  {
    "id": "arXiv:2106.12531",
    "title": "Wavenumber-Division Multiplexing in Line-of-Sight Holographic MIMO  Communications",
    "abstract": "Comments: 16 pages, 12 figures, IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: 16 pages, 12 figures, IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Luca Sanguinetti",
      "Antonio A. D'Amico",
      "Merouane Debbah"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.12531"
  },
  {
    "id": "arXiv:2106.13150",
    "title": "Comparison of Consecutive and Re-stained Sections for Image Registration  in Histopathology",
    "abstract": "Comments: submitted, data available at this https URL",
    "descriptor": "\nComments: submitted, data available at this https URL\n",
    "authors": [
      "Johannes Lotz",
      "Nick Weiss",
      "Jeroen van der Laak",
      "Stefan Heldmann"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.13150"
  },
  {
    "id": "arXiv:2107.02579",
    "title": "Numerical Matrix Decomposition and its Modern Applications: A Rigorous  First Course",
    "abstract": "Numerical Matrix Decomposition and its Modern Applications: A Rigorous  First Course",
    "descriptor": "",
    "authors": [
      "Jun Lu"
    ],
    "subjectives": [
      "History and Overview (math.HO)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.02579"
  },
  {
    "id": "arXiv:2107.04010",
    "title": "A Decision Support System for Safer Airplane Landings: Predicting Runway  Conditions Using XGBoost and Explainable AI",
    "abstract": "A Decision Support System for Safer Airplane Landings: Predicting Runway  Conditions Using XGBoost and Explainable AI",
    "descriptor": "",
    "authors": [
      "Alise Danielle Midtfjord",
      "Riccardo De Bin",
      "Arne Bang Huseby"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2107.04010"
  },
  {
    "id": "arXiv:2108.00965",
    "title": "Privacy-Aware Rejection Sampling",
    "abstract": "Comments: 25 pages + references, 4 figures",
    "descriptor": "\nComments: 25 pages + references, 4 figures\n",
    "authors": [
      "Jordan Awan",
      "Vinayak Rao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2108.00965"
  },
  {
    "id": "arXiv:2108.01312",
    "title": "Learning Causal Models from Conditional Moment Restrictions by  Importance Weighting",
    "abstract": "Learning Causal Models from Conditional Moment Restrictions by  Importance Weighting",
    "descriptor": "",
    "authors": [
      "Masahiro Kato",
      "Masaaki Imaizumi",
      "Kenichiro McAlinn",
      "Haruo Kakehi",
      "Shota Yasui"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.01312"
  },
  {
    "id": "arXiv:2109.03283",
    "title": "Have a break from making decisions, have a MARS: The Multi-valued Action  Reasoning System",
    "abstract": "Have a break from making decisions, have a MARS: The Multi-valued Action  Reasoning System",
    "descriptor": "",
    "authors": [
      "Cosmin Badea"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2109.03283"
  },
  {
    "id": "arXiv:2109.13492",
    "title": "Following the Data, Not the Function: Rethinking Function Orchestration  in Serverless Computing",
    "abstract": "Comments: Accepted by NSDI'23",
    "descriptor": "\nComments: Accepted by NSDI'23\n",
    "authors": [
      "Minchen Yu",
      "Tingjia Cao",
      "Wei Wang",
      "Ruichuan Chen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2109.13492"
  },
  {
    "id": "arXiv:2109.14687",
    "title": "Guaranteed Rejection-free Sampling Method Using Past Behaviours for  Motion Planning of Autonomous Systems",
    "abstract": "Guaranteed Rejection-free Sampling Method Using Past Behaviours for  Motion Planning of Autonomous Systems",
    "descriptor": "",
    "authors": [
      "Thomas T. Enevoldsen",
      "Roberto Galeazzi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.14687"
  },
  {
    "id": "arXiv:2110.02424",
    "title": "Spectral Bias in Practice: The Role of Function Frequency in  Generalization",
    "abstract": "Spectral Bias in Practice: The Role of Function Frequency in  Generalization",
    "descriptor": "",
    "authors": [
      "Sara Fridovich-Keil",
      "Raphael Gontijo-Lopes",
      "Rebecca Roelofs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02424"
  },
  {
    "id": "arXiv:2110.06917",
    "title": "Extracting Dynamical Models from Data",
    "abstract": "Comments: 17 pages, 16 figures",
    "descriptor": "\nComments: 17 pages, 16 figures\n",
    "authors": [
      "Michael F. Zimmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06917"
  },
  {
    "id": "arXiv:2110.08343",
    "title": "Hyperseed: Unsupervised Learning with Vector Symbolic Architectures",
    "abstract": "Hyperseed: Unsupervised Learning with Vector Symbolic Architectures",
    "descriptor": "",
    "authors": [
      "Evgeny Osipov",
      "Sachin Kahawala",
      "Dilantha Haputhanthri",
      "Thimal Kempitiya",
      "Daswin De Silva",
      "Damminda Alahakoon",
      "Denis Kleyko"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08343"
  },
  {
    "id": "arXiv:2110.08387",
    "title": "Generated Knowledge Prompting for Commonsense Reasoning",
    "abstract": "Comments: ACL 2022 main conference",
    "descriptor": "\nComments: ACL 2022 main conference\n",
    "authors": [
      "Jiacheng Liu",
      "Alisa Liu",
      "Ximing Lu",
      "Sean Welleck",
      "Peter West",
      "Ronan Le Bras",
      "Yejin Choi",
      "Hannaneh Hajishirzi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08387"
  },
  {
    "id": "arXiv:2110.08465",
    "title": "Heterogeneous Graph-Based Multimodal Brain Network Learning",
    "abstract": "Heterogeneous Graph-Based Multimodal Brain Network Learning",
    "descriptor": "",
    "authors": [
      "Gen Shi",
      "Yifan Zhu",
      "Wenjin Liu",
      "Quanming Yao",
      "Xuesong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.08465"
  },
  {
    "id": "arXiv:2110.08604",
    "title": "Improving Implicit Sentiment Learning via Local Sentiment Aggregation",
    "abstract": "Comments: Source Code: this https URL",
    "descriptor": "\nComments: Source Code: this https URL\n",
    "authors": [
      "Heng Yang",
      "Ke Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08604"
  },
  {
    "id": "arXiv:2110.15155",
    "title": "Online Facility Location with Linear Delay",
    "abstract": "Online Facility Location with Linear Delay",
    "descriptor": "",
    "authors": [
      "Marcin Bienkowski",
      "Martin B\u00f6hm",
      "Jaros\u0142aw Byrka",
      "Jan Marcinkowski"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.15155"
  },
  {
    "id": "arXiv:2111.01254",
    "title": "Unique Games hardness of Quantum Max-Cut, and a conjectured  vector-valued Borell's inequality",
    "abstract": "Comments: 76 pages; v3 treats the vector-valued Borell's inequality as a conjecture rather than a theorem, due to an error in previous versions",
    "descriptor": "\nComments: 76 pages; v3 treats the vector-valued Borell's inequality as a conjecture rather than a theorem, due to an error in previous versions\n",
    "authors": [
      "Yeongwoo Hwang",
      "Joe Neeman",
      "Ojas Parekh",
      "Kevin Thompson",
      "John Wright"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2111.01254"
  },
  {
    "id": "arXiv:2111.01813",
    "title": "Spatial regionalization based on optimal information compression",
    "abstract": "Comments: In press at Communications Physics",
    "descriptor": "\nComments: In press at Communications Physics\n",
    "authors": [
      "Alec Kirkley"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2111.01813"
  },
  {
    "id": "arXiv:2111.08851",
    "title": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On  Conditional Probabilities",
    "abstract": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On  Conditional Probabilities",
    "descriptor": "",
    "authors": [
      "Xintong Shi",
      "Wenzhi Cao",
      "Sebastian Raschka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.08851"
  },
  {
    "id": "arXiv:2111.14251",
    "title": "False Data Injection Threats in Active Distribution Systems: A  Comprehensive Survey",
    "abstract": "False Data Injection Threats in Active Distribution Systems: A  Comprehensive Survey",
    "descriptor": "",
    "authors": [
      "Muhammad Akbar Husnoo",
      "Adnan Anwar",
      "Nasser Hosseinzadeh",
      "Shama Naz Islam",
      "Abdun Naser Mahmood",
      "Robin Doss"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.14251"
  },
  {
    "id": "arXiv:2111.14585",
    "title": "Similarity Contrastive Estimation for Self-Supervised Soft Contrastive  Learning",
    "abstract": "Comments: Accepted to IEEE Winter Conference on Applications of Computer Vision (WACV) 2023",
    "descriptor": "\nComments: Accepted to IEEE Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Julien Denize",
      "Jaonary Rabarisoa",
      "Astrid Orcesi",
      "Romain H\u00e9rault",
      "St\u00e9phane Canu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.14585"
  },
  {
    "id": "arXiv:2112.00804",
    "title": "PreViTS: Contrastive Pretraining with Video Tracking Supervision",
    "abstract": "Comments: To be presented at WACV 2023",
    "descriptor": "\nComments: To be presented at WACV 2023\n",
    "authors": [
      "Brian Chen",
      "Ramprasaath R. Selvaraju",
      "Shih-Fu Chang",
      "Juan Carlos Niebles",
      "Nikhil Naik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00804"
  },
  {
    "id": "arXiv:2112.02545",
    "title": "New Properties and Invariants of Harmonic Polygons",
    "abstract": "Comments: 18 pages, 9 figures, 3 tables, 8 videos",
    "descriptor": "\nComments: 18 pages, 9 figures, 3 tables, 8 videos\n",
    "authors": [
      "Ronaldo Garcia",
      "Dan Reznik",
      "Pedro Roitman"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2112.02545"
  },
  {
    "id": "arXiv:2112.03237",
    "title": "From Coarse to Fine-grained Concept based Discrimination for Phrase  Detection",
    "abstract": "From Coarse to Fine-grained Concept based Discrimination for Phrase  Detection",
    "descriptor": "",
    "authors": [
      "Maan Qraitem",
      "Bryan A. Plummer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.03237"
  },
  {
    "id": "arXiv:2112.05149",
    "title": "DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion  Model",
    "abstract": "DiffuseMorph: Unsupervised Deformable Image Registration Using Diffusion  Model",
    "descriptor": "",
    "authors": [
      "Boah Kim",
      "Inhwa Han",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.05149"
  },
  {
    "id": "arXiv:2112.11734",
    "title": "D-HYPR: Harnessing Neighborhood Modeling and Asymmetry Preservation for  Digraph Representation Learning",
    "abstract": "Comments: CIKM 2022",
    "descriptor": "\nComments: CIKM 2022\n",
    "authors": [
      "Honglu Zhou",
      "Advith Chegu",
      "Samuel S. Sohn",
      "Zuohui Fu",
      "Gerard de Melo",
      "Mubbasir Kapadia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.11734"
  },
  {
    "id": "arXiv:2112.14927",
    "title": "An Empirical Study of Security Practices for Microservices Systems",
    "abstract": "Comments: 22 pages, 5 images, 8 tables, Manuscript submitted to a Journal (2022)",
    "descriptor": "\nComments: 22 pages, 5 images, 8 tables, Manuscript submitted to a Journal (2022)\n",
    "authors": [
      "Ali Rezaei Nasab",
      "Mojtaba Shahin",
      "Seyed Ali Hoseyni Raviz",
      "Peng Liang",
      "Amir Mashmool",
      "Valentina Lenarduzzi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.14927"
  },
  {
    "id": "arXiv:2201.00074",
    "title": "Engagement Outweighs Exposure to Partisan and Unreliable News within  Google Search",
    "abstract": "Comments: Updated to latest version of manuscript. Abstract preview is trimmed to fit arXiv 1,920 character limit",
    "descriptor": "\nComments: Updated to latest version of manuscript. Abstract preview is trimmed to fit arXiv 1,920 character limit\n",
    "authors": [
      "Ronald E. Robertson",
      "Jon Green",
      "Damian J. Ruck",
      "Katherine Ognyanova",
      "Christo Wilson",
      "David Lazer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.00074"
  },
  {
    "id": "arXiv:2201.09191",
    "title": "Revisiting Global Pooling through the Lens of Optimal Transport",
    "abstract": "Revisiting Global Pooling through the Lens of Optimal Transport",
    "descriptor": "",
    "authors": [
      "Minjie Cheng",
      "Hongteng Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09191"
  },
  {
    "id": "arXiv:2201.12151",
    "title": "Unsupervised Learning From Incomplete Measurements for Inverse Problems",
    "abstract": "Unsupervised Learning From Incomplete Measurements for Inverse Problems",
    "descriptor": "",
    "authors": [
      "Juli\u00e1n Tachella",
      "Dongdong Chen",
      "Mike Davies"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12151"
  },
  {
    "id": "arXiv:2202.00769",
    "title": "Distributional Reinforcement Learning via Sinkhorn Iterations",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2110.03155",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.03155\n",
    "authors": [
      "Ke Sun",
      "Yingnan Zhao",
      "Yi Liu",
      "Wulong Liu",
      "Bei Jiang",
      "Linglong Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00769"
  },
  {
    "id": "arXiv:2202.03008",
    "title": "Algorithms that get old : the case of generative deep neural networks",
    "abstract": "Algorithms that get old : the case of generative deep neural networks",
    "descriptor": "",
    "authors": [
      "Gabriel Turinici"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03008"
  },
  {
    "id": "arXiv:2202.04041",
    "title": "Physics-informed neural networks for solving parametric magnetostatic  problems",
    "abstract": "Comments: 12 pages, 10 figures",
    "descriptor": "\nComments: 12 pages, 10 figures\n",
    "authors": [
      "Andr\u00e9s Beltr\u00e1n-Pulido",
      "Ilias Bilionis",
      "Dionysios Aliprantis"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2202.04041"
  },
  {
    "id": "arXiv:2202.06767",
    "title": "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training  Benchmark",
    "abstract": "Comments: Accepted by NeurIPS 2022 Track Datasets and Benchmarks",
    "descriptor": "\nComments: Accepted by NeurIPS 2022 Track Datasets and Benchmarks\n",
    "authors": [
      "Jiaxi Gu",
      "Xiaojun Meng",
      "Guansong Lu",
      "Lu Hou",
      "Minzhe Niu",
      "Xiaodan Liang",
      "Lewei Yao",
      "Runhui Huang",
      "Wei Zhang",
      "Xin Jiang",
      "Chunjing Xu",
      "Hang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06767"
  },
  {
    "id": "arXiv:2202.07697",
    "title": "A new discrete theory of pseudoconvexity",
    "abstract": "A new discrete theory of pseudoconvexity",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs Keszegh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2202.07697"
  },
  {
    "id": "arXiv:2202.08548",
    "title": "From Utility to Capability: A New Paradigm to Conceptualize and Develop  Inclusive PETs",
    "abstract": "Comments: 16 Pages, 2 Figures",
    "descriptor": "\nComments: 16 Pages, 2 Figures\n",
    "authors": [
      "Partha Das Chowdhury",
      "Andres Dominguez",
      "Kopo M. Ramokapane",
      "Awais Rashid"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2202.08548"
  },
  {
    "id": "arXiv:2202.12276",
    "title": "On the influence of stochastic roundoff errors on the convergence of the  gradient descent method with low-precision floating-point computation",
    "abstract": "On the influence of stochastic roundoff errors on the convergence of the  gradient descent method with low-precision floating-point computation",
    "descriptor": "",
    "authors": [
      "Lu Xia",
      "Stefano Massei",
      "Michiel E. Hochstenbach",
      "Barry Koren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.12276"
  },
  {
    "id": "arXiv:2202.13894",
    "title": "Spherical cap discrepancy of perturbed lattices under the Lambert  projection",
    "abstract": "Comments: A new constant M^Q(K) needed to be added to the main theorem and Section 3 has been extended by an algorithm. The proof of Theorem 1 was adjusted to incorporate the value M^Q(K). All proofs have been streamlined and arguments better presented. Many typos have been corrected and a reference to directional discrepancy was added",
    "descriptor": "\nComments: A new constant M^Q(K) needed to be added to the main theorem and Section 3 has been extended by an algorithm. The proof of Theorem 1 was adjusted to incorporate the value M^Q(K). All proofs have been streamlined and arguments better presented. Many typos have been corrected and a reference to directional discrepancy was added\n",
    "authors": [
      "Damir Ferizovi\u0107"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Analysis and ODEs (math.CA)"
    ],
    "url": "https://arxiv.org/abs/2202.13894"
  },
  {
    "id": "arXiv:2203.00669",
    "title": "Hierarchical Reinforcement Learning with AI Planning Models",
    "abstract": "Comments: 30 pages, 15 figures",
    "descriptor": "\nComments: 30 pages, 15 figures\n",
    "authors": [
      "Junkyu Lee",
      "Michael Katz",
      "Don Joven Agravante",
      "Miao Liu",
      "Geraud Nangue Tasse",
      "Tim Klinger",
      "Shirin Sohrabi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.00669"
  },
  {
    "id": "arXiv:2203.01726",
    "title": "Ensembles of Vision Transformers as a New Paradigm for Automated  Classification in Ecology",
    "abstract": "Comments: To appear in Scientific Reports",
    "descriptor": "\nComments: To appear in Scientific Reports\n",
    "authors": [
      "S. Kyathanahally",
      "T. Hardeman",
      "M. Reyes",
      "E. Merz",
      "T. Bulas",
      "P. Brun",
      "F. Pomati",
      "M. Baity-Jesi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.01726"
  },
  {
    "id": "arXiv:2203.01769",
    "title": "PeerSum: A Peer Review Dataset for Abstractive Multi-document  Summarization",
    "abstract": "Comments: This is because the paper has changed so much and the arxiv paper no longer represents the PeerSum",
    "descriptor": "\nComments: This is because the paper has changed so much and the arxiv paper no longer represents the PeerSum\n",
    "authors": [
      "Miao Li",
      "Jianzhong Qi",
      "Jey Han Lau"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.01769"
  },
  {
    "id": "arXiv:2203.06798",
    "title": "The Role of Local Steps in Local SGD",
    "abstract": "The Role of Local Steps in Local SGD",
    "descriptor": "",
    "authors": [
      "Tiancheng Qin",
      "S. Rasoul Etesami",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.06798"
  },
  {
    "id": "arXiv:2203.10749",
    "title": "STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic  flow prediction in Intelligent Transportation Systems",
    "abstract": "STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic  flow prediction in Intelligent Transportation Systems",
    "descriptor": "",
    "authors": [
      "Wei Zhao",
      "Shiqi Zhang",
      "Bing Zhou",
      "Bei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.10749"
  },
  {
    "id": "arXiv:2203.11555",
    "title": "Gradient flows and randomised thresholding: sparse inversion and  classification",
    "abstract": "Gradient flows and randomised thresholding: sparse inversion and  classification",
    "descriptor": "",
    "authors": [
      "Jonas Latz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2203.11555"
  },
  {
    "id": "arXiv:2203.15021",
    "title": "Few-Shot Object Detection with Fully Cross-Transformer",
    "abstract": "Comments: CVPR 2022 (Oral). Code is available at this https URL",
    "descriptor": "\nComments: CVPR 2022 (Oral). Code is available at this https URL\n",
    "authors": [
      "Guangxing Han",
      "Jiawei Ma",
      "Shiyuan Huang",
      "Long Chen",
      "Shih-Fu Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2203.15021"
  },
  {
    "id": "arXiv:2203.15483",
    "title": "Representing 'how you say' with 'what you say': English corpus of  focused speech and text reflecting corresponding implications",
    "abstract": "Comments: A revised version of the one published at INTERSPEECH2022. The following parts have been modified: Section 2.2.2. The variance for screening paraphrases from 1.5 to 1.6 and the number of speech data and recorders; Section 2.4. The number of paraphrases from 1698 to 1697; Section 3, Table2. Values in two cells (Negation, P from 0.14 to 0.12) and (Leftward, P from 0.10 to 0.12)",
    "descriptor": "\nComments: A revised version of the one published at INTERSPEECH2022. The following parts have been modified: Section 2.2.2. The variance for screening paraphrases from 1.5 to 1.6 and the number of speech data and recorders; Section 2.4. The number of paraphrases from 1698 to 1697; Section 3, Table2. Values in two cells (Negation, P from 0.14 to 0.12) and (Leftward, P from 0.10 to 0.12)\n",
    "authors": [
      "Naoaki Suzuki",
      "Satoshi Nakamura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.15483"
  },
  {
    "id": "arXiv:2203.16870",
    "title": "A Convex Optimal Control Framework for Autonomous Vehicle Intersection  Crossing",
    "abstract": "Comments: 16 pages, 11 figures. This work has been accepted by the IEEE Transactions on Intelligent Transportation Systems. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: 16 pages, 11 figures. This work has been accepted by the IEEE Transactions on Intelligent Transportation Systems. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Xiao Pan",
      "Boli Chen",
      "Stelios Timotheou",
      "Simos A. Evangelou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.16870"
  },
  {
    "id": "arXiv:2204.02311",
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "abstract": "PaLM: Scaling Language Modeling with Pathways",
    "descriptor": "",
    "authors": [
      "Aakanksha Chowdhery",
      "Sharan Narang",
      "Jacob Devlin",
      "Maarten Bosma",
      "Gaurav Mishra",
      "Adam Roberts",
      "Paul Barham",
      "Hyung Won Chung",
      "Charles Sutton",
      "Sebastian Gehrmann",
      "Parker Schuh",
      "Kensen Shi",
      "Sasha Tsvyashchenko",
      "Joshua Maynez",
      "Abhishek Rao",
      "Parker Barnes",
      "Yi Tay",
      "Noam Shazeer",
      "Vinodkumar Prabhakaran",
      "Emily Reif",
      "Nan Du",
      "Ben Hutchinson",
      "Reiner Pope",
      "James Bradbury",
      "Jacob Austin",
      "Michael Isard",
      "Guy Gur-Ari",
      "Pengcheng Yin",
      "Toju Duke",
      "Anselm Levskaya",
      "Sanjay Ghemawat",
      "Sunipa Dev",
      "Henryk Michalewski",
      "Xavier Garcia",
      "Vedant Misra",
      "Kevin Robinson",
      "Liam Fedus",
      "Denny Zhou",
      "Daphne Ippolito",
      "David Luan",
      "Hyeontaek Lim",
      "Barret Zoph",
      "Alexander Spiridonov",
      "Ryan Sepassi",
      "David Dohan",
      "Shivani Agrawal",
      "Mark Omernick",
      "Andrew M. Dai",
      "Thanumalayan Sankaranarayana Pillai",
      "Marie Pellat",
      "Aitor Lewkowycz",
      "Erica Moreira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.02311"
  },
  {
    "id": "arXiv:2204.02782",
    "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse  Molecular Simulation Datasets",
    "abstract": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse  Molecular Simulation Datasets",
    "descriptor": "",
    "authors": [
      "Johannes Gasteiger",
      "Muhammed Shuaibi",
      "Anuroop Sriram",
      "Stephan G\u00fcnnemann",
      "Zachary Ulissi",
      "C. Lawrence Zitnick",
      "Abhishek Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.02782"
  },
  {
    "id": "arXiv:2204.06759",
    "title": "Iterative Inner/outer Approximations for Scalable Semidefinite Programs  using Block Factor-width-two Matrices",
    "abstract": "Comments: 8 pages, 6 figures. Code is available through this https URL",
    "descriptor": "\nComments: 8 pages, 6 figures. Code is available through this https URL\n",
    "authors": [
      "Feng-Yi Liao",
      "Yang Zheng"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.06759"
  },
  {
    "id": "arXiv:2204.06843",
    "title": "Surface Similarity Parameter: A New Machine Learning Loss Metric for  Oscillatory Spatio-Temporal Data",
    "abstract": "Comments: 19 pages, 7 figures, submitted to Elsevier Neural Networks",
    "descriptor": "\nComments: 19 pages, 7 figures, submitted to Elsevier Neural Networks\n",
    "authors": [
      "Mathies Wedler",
      "Merten Stender",
      "Marco Klein",
      "Svenja Ehlers",
      "Norbert Hoffmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.06843"
  },
  {
    "id": "arXiv:2204.08790",
    "title": "ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented  Visual Models",
    "abstract": "Comments: NeurIPS 2022 (Datasets and Benchmarks Track). The first two authors contribute equally. Benchmark page: this https URL",
    "descriptor": "\nComments: NeurIPS 2022 (Datasets and Benchmarks Track). The first two authors contribute equally. Benchmark page: this https URL\n",
    "authors": [
      "Chunyuan Li",
      "Haotian Liu",
      "Liunian Harold Li",
      "Pengchuan Zhang",
      "Jyoti Aneja",
      "Jianwei Yang",
      "Ping Jin",
      "Houdong Hu",
      "Zicheng Liu",
      "Yong Jae Lee",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.08790"
  },
  {
    "id": "arXiv:2204.10306",
    "title": "Performance and limitations of the QAOA at constant levels on large  sparse hypergraphs and spin glass models",
    "abstract": "Comments: 13+47 pages, updated introduction",
    "descriptor": "\nComments: 13+47 pages, updated introduction\n",
    "authors": [
      "Joao Basso",
      "David Gamarnik",
      "Song Mei",
      "Leo Zhou"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.10306"
  },
  {
    "id": "arXiv:2204.11641",
    "title": "Cryptography Is Not Enough: Relay Attacks on Authenticated GNSS Signals",
    "abstract": "Cryptography Is Not Enough: Relay Attacks on Authenticated GNSS Signals",
    "descriptor": "",
    "authors": [
      "Maryam Motallebighomi",
      "Harshad Sathaye",
      "Mridula Singh",
      "Aanjhan Ranganathan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.11641"
  },
  {
    "id": "arXiv:2204.13940",
    "title": "PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient  Descent",
    "abstract": "PnP-ReG: Learned Regularizing Gradient for Plug-and-Play Gradient  Descent",
    "descriptor": "",
    "authors": [
      "Rita Fermanian",
      "Mikael Le Pendu",
      "Christine Guillemot"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13940"
  },
  {
    "id": "arXiv:2205.01554",
    "title": "Exploring Proxying QUIC and HTTP/3 for Satellite Communication",
    "abstract": "Comments: The final publication is available at IEEE Xplore via this https URL",
    "descriptor": "\nComments: The final publication is available at IEEE Xplore via this https URL\n",
    "authors": [
      "Mike Kosek",
      "Hendrik Cech",
      "Vaibhav Bajpai",
      "J\u00f6rg Ott"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.01554"
  },
  {
    "id": "arXiv:2205.02885",
    "title": "Adversarial confound regression and uncertainty measurements to classify  heterogeneous clinical MRI in Mass General Brigham",
    "abstract": "Adversarial confound regression and uncertainty measurements to classify  heterogeneous clinical MRI in Mass General Brigham",
    "descriptor": "",
    "authors": [
      "Matthew Leming",
      "Sudeshna Das",
      "Hyungsoon Im"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2205.02885"
  },
  {
    "id": "arXiv:2205.05591",
    "title": "Predicting hot-electron free energies from ground-state data",
    "abstract": "Predicting hot-electron free energies from ground-state data",
    "descriptor": "",
    "authors": [
      "Chiheb Ben Mahmoud",
      "Federico Grasselli",
      "Michele Ceriotti"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05591"
  },
  {
    "id": "arXiv:2205.06029",
    "title": "Information flow estimation: a study of news on Twitter",
    "abstract": "Information flow estimation: a study of news on Twitter",
    "descriptor": "",
    "authors": [
      "Tobin South",
      "Bridget Smart",
      "Matthew Roughan",
      "Lewis Mitchell"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Information Theory (cs.IT)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.06029"
  },
  {
    "id": "arXiv:2205.07848",
    "title": "Power and limitations of single-qubit native quantum neural networks",
    "abstract": "Comments: 22 pages including appendix. To appear at NeurIPS 2022",
    "descriptor": "\nComments: 22 pages including appendix. To appear at NeurIPS 2022\n",
    "authors": [
      "Zhan Yu",
      "Hongshun Yao",
      "Mujin Li",
      "Xin Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.07848"
  },
  {
    "id": "arXiv:2205.07972",
    "title": "Sparse Visual Counterfactual Explanations in Image Space",
    "abstract": "Sparse Visual Counterfactual Explanations in Image Space",
    "descriptor": "",
    "authors": [
      "Valentyn Boreiko",
      "Maximilian Augustin",
      "Francesco Croce",
      "Philipp Berens",
      "Matthias Hein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.07972"
  },
  {
    "id": "arXiv:2205.08207",
    "title": "DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual  Odometry in Dynamic Scenes",
    "abstract": "DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual  Odometry in Dynamic Scenes",
    "descriptor": "",
    "authors": [
      "Baosheng Zhang",
      "Ya Wang",
      "Xiaoguang Ma",
      "Hongjun Ma",
      "Chunbo Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.08207"
  },
  {
    "id": "arXiv:2205.09809",
    "title": "Calibration Matters: Tackling Maximization Bias in Large-scale  Advertising Recommendation Systems",
    "abstract": "Calibration Matters: Tackling Maximization Bias in Large-scale  Advertising Recommendation Systems",
    "descriptor": "",
    "authors": [
      "Yewen Fan",
      "Nian Si",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.09809"
  },
  {
    "id": "arXiv:2205.12808",
    "title": "Mirror Descent Maximizes Generalized Margin and Can Be Implemented  Efficiently",
    "abstract": "Mirror Descent Maximizes Generalized Margin and Can Be Implemented  Efficiently",
    "descriptor": "",
    "authors": [
      "Haoyuan Sun",
      "Kwangjun Ahn",
      "Christos Thrampoulidis",
      "Navid Azizan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12808"
  },
  {
    "id": "arXiv:2205.12891",
    "title": "Scheduling to Optimize Sojourn Time of Successful Jobs",
    "abstract": "Comments: We found that theorem III.4 is incorrect and we have a different theorem for the performance of the algorithm",
    "descriptor": "\nComments: We found that theorem III.4 is incorrect and we have a different theorem for the performance of the algorithm\n",
    "authors": [
      "Yuan Yao",
      "Marco Paolieri",
      "Leana Golubchik"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2205.12891"
  },
  {
    "id": "arXiv:2205.13187",
    "title": "Relaxed Fixed Point Iterations for Matrix Equations Arising in Markov  Chains Modeling",
    "abstract": "Relaxed Fixed Point Iterations for Matrix Equations Arising in Markov  Chains Modeling",
    "descriptor": "",
    "authors": [
      "Luca Gemignani",
      "Beatrice Meini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.13187"
  },
  {
    "id": "arXiv:2205.13450",
    "title": "Variance-Aware Sparse Linear Bandits",
    "abstract": "Variance-Aware Sparse Linear Bandits",
    "descriptor": "",
    "authors": [
      "Yan Dai",
      "Ruosong Wang",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13450"
  },
  {
    "id": "arXiv:2205.13573",
    "title": "Efficient Approximation of Gromov-Wasserstein Distance using Importance  Sparsification",
    "abstract": "Efficient Approximation of Gromov-Wasserstein Distance using Importance  Sparsification",
    "descriptor": "",
    "authors": [
      "Mengyu Li",
      "Jun Yu",
      "Hongteng Xu",
      "Cheng Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13573"
  },
  {
    "id": "arXiv:2205.14977",
    "title": "Fast Nonlinear Vector Quantile Regression",
    "abstract": "Comments: 35 pages, 15 figures, code: this https URL",
    "descriptor": "\nComments: 35 pages, 15 figures, code: this https URL\n",
    "authors": [
      "Aviv A. Rosenberg",
      "Sanketh Vedula",
      "Yaniv Romano",
      "Alex M. Bronstein"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.14977"
  },
  {
    "id": "arXiv:2205.15419",
    "title": "Fool SHAP with Stealthily Biased Sampling",
    "abstract": "Fool SHAP with Stealthily Biased Sampling",
    "descriptor": "",
    "authors": [
      "Gabriel Laberge",
      "Ulrich A\u00efvodji",
      "Satoshi Hara",
      "Mario Marchand.",
      "Foutse Khomh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15419"
  },
  {
    "id": "arXiv:2205.15549",
    "title": "VC Theoretical Explanation of Double Descent",
    "abstract": "VC Theoretical Explanation of Double Descent",
    "descriptor": "",
    "authors": [
      "Eng Hock Lee",
      "Vladimir Cherkassky"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15549"
  },
  {
    "id": "arXiv:2205.15638",
    "title": "Differentiable Invariant Causal Discovery",
    "abstract": "Comments: 22 pages, 11 figures",
    "descriptor": "\nComments: 22 pages, 11 figures\n",
    "authors": [
      "Yu Wang",
      "An Zhang",
      "Xiang Wang",
      "Yancheng Yuan",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.15638"
  },
  {
    "id": "arXiv:2206.00152",
    "title": "Human-AI Shared Control via Policy Dissection",
    "abstract": "Human-AI Shared Control via Policy Dissection",
    "descriptor": "",
    "authors": [
      "Quanyi Li",
      "Zhenghao Peng",
      "Haibin Wu",
      "Lan Feng",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00152"
  },
  {
    "id": "arXiv:2206.01342",
    "title": "Understanding the Role of Nonlinearity in Training Dynamics of  Contrastive Learning",
    "abstract": "Understanding the Role of Nonlinearity in Training Dynamics of  Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Yuandong Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01342"
  },
  {
    "id": "arXiv:2206.03019",
    "title": "The Survival Bandit Problem",
    "abstract": "The Survival Bandit Problem",
    "descriptor": "",
    "authors": [
      "Charles Riou",
      "Junya Honda",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.03019"
  },
  {
    "id": "arXiv:2206.04277",
    "title": "On Transfer Learning in Functional Linear Regression",
    "abstract": "Comments: 31 pages",
    "descriptor": "\nComments: 31 pages\n",
    "authors": [
      "Haotian Lin",
      "Matthew Reimherr"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04277"
  },
  {
    "id": "arXiv:2206.04596",
    "title": "Linear Delta Arrays for Compliant Dexterous Distributed Manipulation",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Sarvesh Patil",
      "Tony Tao",
      "Tess Hellebrekers",
      "Oliver Kroemer",
      "F. Zeynep Temel"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04596"
  },
  {
    "id": "arXiv:2206.04951",
    "title": "Evolutionary Echo State Network: evolving reservoirs in the Fourier  space",
    "abstract": "Comments: This manuscript was accepted at the 2022 International Joint Conference on Neural Networks (IJCNN 2022)",
    "descriptor": "\nComments: This manuscript was accepted at the 2022 International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "Sebastian Basterrech",
      "Gerardo Rubino"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04951"
  },
  {
    "id": "arXiv:2206.06354",
    "title": "Differentiable and Transportable Structure Learning",
    "abstract": "Differentiable and Transportable Structure Learning",
    "descriptor": "",
    "authors": [
      "Jeroen Berrevoets",
      "Nabeel Seedat",
      "Fergus Imrie",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.06354"
  },
  {
    "id": "arXiv:2206.06602",
    "title": "Deep Isolation Forest for Anomaly Detection",
    "abstract": "Comments: 14 pages, 7 figures; the source code is available at this https URL",
    "descriptor": "\nComments: 14 pages, 7 figures; the source code is available at this https URL\n",
    "authors": [
      "Hongzuo Xu",
      "Guansong Pang",
      "Yijie Wang",
      "Yongjun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06602"
  },
  {
    "id": "arXiv:2206.06783",
    "title": "Characteristic Mode Decomposition Using the Scattering Dyadic in  Arbitrary Full-Wave Solvers",
    "abstract": "Comments: 11 pages, 10 figures, with supplementary material (github)",
    "descriptor": "\nComments: 11 pages, 10 figures, with supplementary material (github)\n",
    "authors": [
      "Miloslav Capek",
      "Johan Lundgren",
      "Mats Gustafsson",
      "Kurt Schab",
      "Lukas Jelinek"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Classical Physics (physics.class-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.06783"
  },
  {
    "id": "arXiv:2206.07836",
    "title": "Personal Entity, Concept, and Named Entity Linking in Conversations",
    "abstract": "Personal Entity, Concept, and Named Entity Linking in Conversations",
    "descriptor": "",
    "authors": [
      "Hideaki Joko",
      "Faegheh Hasibi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.07836"
  },
  {
    "id": "arXiv:2206.08515",
    "title": "ComENet: Towards Complete and Efficient Message Passing for 3D Molecular  Graphs",
    "abstract": "Comments: The paper has been accepted by NeurIPS 2022. You can also cite the conference version",
    "descriptor": "\nComments: The paper has been accepted by NeurIPS 2022. You can also cite the conference version\n",
    "authors": [
      "Limei Wang",
      "Yi Liu",
      "Yuchao Lin",
      "Haoran Liu",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08515"
  },
  {
    "id": "arXiv:2206.10485",
    "title": "Optimal homotopy reconstruction results \u00e0 la Niyogi, Smale, and  Weinberger",
    "abstract": "Comments: 21 pages, 12 figures",
    "descriptor": "\nComments: 21 pages, 12 figures\n",
    "authors": [
      "Dominique Attali",
      "Hana Dal Poz Kou\u0159imsk\u00e1",
      "Christopher Fillmore",
      "Ishika Ghosh",
      "Andr\u00e9 Lieutier",
      "Elizabeth Stephenson",
      "Mathijs Wintraecken"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2206.10485"
  },
  {
    "id": "arXiv:2206.11239",
    "title": "FedorAS: Federated Architecture Search under system heterogeneity",
    "abstract": "FedorAS: Federated Architecture Search under system heterogeneity",
    "descriptor": "",
    "authors": [
      "Lukasz Dudziak",
      "Stefanos Laskaridis",
      "Javier Fernandez-Marques"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.11239"
  },
  {
    "id": "arXiv:2206.12361",
    "title": "Robustness to corruption in pre-trained Bayesian neural networks",
    "abstract": "Robustness to corruption in pre-trained Bayesian neural networks",
    "descriptor": "",
    "authors": [
      "Xi Wang",
      "Laurence Aitchison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.12361"
  },
  {
    "id": "arXiv:2206.15033",
    "title": "A Causal Approach to Detecting Multivariate Time-series Anomalies and  Root Causes",
    "abstract": "Comments: 19 pages, 9 figures",
    "descriptor": "\nComments: 19 pages, 9 figures\n",
    "authors": [
      "Wenzhuo Yang",
      "Kun Zhang",
      "Steven C.H. Hoi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.15033"
  },
  {
    "id": "arXiv:2207.01294",
    "title": "A New Index for Clustering Evaluation Based on Density Estimation",
    "abstract": "A New Index for Clustering Evaluation Based on Density Estimation",
    "descriptor": "",
    "authors": [
      "Gangli Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.01294"
  },
  {
    "id": "arXiv:2207.01431",
    "title": "Breaking Bad News in the Era of Artificial Intelligence and Algorithmic  Medicine: An Exploration of Disclosure and its Ethical Justification using  the Hedonic Calculus",
    "abstract": "Breaking Bad News in the Era of Artificial Intelligence and Algorithmic  Medicine: An Exploration of Disclosure and its Ethical Justification using  the Hedonic Calculus",
    "descriptor": "",
    "authors": [
      "Benjamin Post",
      "Cosmin Badea",
      "Aldo Faisal",
      "Stephen J. Brett"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2207.01431"
  },
  {
    "id": "arXiv:2207.06718",
    "title": "Hardware-in-the-Loop Simulation for Evaluating Communication Impacts on  the Wireless-Network-Controlled Robots",
    "abstract": "Comments: 6 pages, 11 figures, to appear in 48th Annual Conference of the Industrial Electronics Society IECON 2022 Conference",
    "descriptor": "\nComments: 6 pages, 11 figures, to appear in 48th Annual Conference of the Industrial Electronics Society IECON 2022 Conference\n",
    "authors": [
      "Honghao Lv",
      "Zhibo Pang",
      "Ming Xiao",
      "Geng Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2207.06718"
  },
  {
    "id": "arXiv:2207.09324",
    "title": "Signed Network Embedding with Application to Simultaneous Detection of  Communities and Anomalies",
    "abstract": "Comments: 24 pages, 4 figures. The appendix containing technical proof is not included, and will be uploaded in the future",
    "descriptor": "\nComments: 24 pages, 4 figures. The appendix containing technical proof is not included, and will be uploaded in the future\n",
    "authors": [
      "Haoran Zhang",
      "Junhui Wang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.09324"
  },
  {
    "id": "arXiv:2208.00817",
    "title": "DSLA: Dynamic smooth label assignment for efficient anchor-free object  detection",
    "abstract": "Comments: single column, 33 pages, 7 figures, accepted by Pattern Recognition",
    "descriptor": "\nComments: single column, 33 pages, 7 figures, accepted by Pattern Recognition\n",
    "authors": [
      "Hu Su",
      "Yonghao He",
      "Rui Jiang",
      "Jiabin Zhang",
      "Wei Zou",
      "Bin Fan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.00817"
  },
  {
    "id": "arXiv:2208.01695",
    "title": "PolarFly: A Cost-Effective and Flexible Low-Diameter Topology",
    "abstract": "Comments: To appear at The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC) 2022",
    "descriptor": "\nComments: To appear at The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC) 2022\n",
    "authors": [
      "Kartik Lakhotia",
      "Maciej Besta",
      "Laura Monroe",
      "Kelly Isham",
      "Patrick Iff",
      "Torsten Hoefler",
      "Fabrizio Petrini"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2208.01695"
  },
  {
    "id": "arXiv:2208.01864",
    "title": "Pyramidal Denoising Diffusion Probabilistic Models",
    "abstract": "Pyramidal Denoising Diffusion Probabilistic Models",
    "descriptor": "",
    "authors": [
      "Dohoon Ryu",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.01864"
  },
  {
    "id": "arXiv:2208.01886",
    "title": "Quantifying Temporal Privacy Leakage in Continuous Event Data Publishing",
    "abstract": "Quantifying Temporal Privacy Leakage in Continuous Event Data Publishing",
    "descriptor": "",
    "authors": [
      "Majid Rafiei",
      "Gamal Elkoumy",
      "Wil M.P. van der Aalst"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.01886"
  },
  {
    "id": "arXiv:2208.03211",
    "title": "Why do networks have inhibitory/negative connections?",
    "abstract": "Comments: Submitted",
    "descriptor": "\nComments: Submitted\n",
    "authors": [
      "Qingyang Wang",
      "Michael A. Powell",
      "Ali Geisa",
      "Eric Bridgeford",
      "Joshua T. Vogelstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2208.03211"
  },
  {
    "id": "arXiv:2208.03505",
    "title": "\"All of them claim to be the best\": Multi-perspective study of VPN users  and VPN providers",
    "abstract": "Comments: Accepted to appear at USENIX Security Symposium 2023 (32nd USENIX Security Symposium, 2023)",
    "descriptor": "\nComments: Accepted to appear at USENIX Security Symposium 2023 (32nd USENIX Security Symposium, 2023)\n",
    "authors": [
      "Reethika Ramesh",
      "Anjali Vyas",
      "Roya Ensafi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.03505"
  },
  {
    "id": "arXiv:2208.06340",
    "title": "Real numbers equally compressible in every base",
    "abstract": "Comments: Article was restructured. Table of contents and table of notations and terminology were added",
    "descriptor": "\nComments: Article was restructured. Table of contents and table of notations and terminology were added\n",
    "authors": [
      "Satyadev Nandakumar",
      "Subin Pulari"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2208.06340"
  },
  {
    "id": "arXiv:2208.10159",
    "title": "Prompt-Matched Semantic Segmentation",
    "abstract": "Prompt-Matched Semantic Segmentation",
    "descriptor": "",
    "authors": [
      "Lingbo Liu",
      "Bruce X.B. Yu",
      "Jianlong Chang",
      "Qi Tian",
      "Chang-Wen Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.10159"
  },
  {
    "id": "arXiv:2208.12184",
    "title": "Decentralized Nonlinear Control of Redundant Upper Limb Exoskeleton with  Natural Adaptation Law",
    "abstract": "Comments: Manuscript is accepted to publish in 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)",
    "descriptor": "\nComments: Manuscript is accepted to publish in 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)\n",
    "authors": [
      "Mahdi Hejrati",
      "Jouni Mattila"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2208.12184"
  },
  {
    "id": "arXiv:2208.14488",
    "title": "Constraining Representations Yields Models That Know What They Don't  Know",
    "abstract": "Constraining Representations Yields Models That Know What They Don't  Know",
    "descriptor": "",
    "authors": [
      "Joao Monteiro",
      "Pau Rodriguez",
      "Pierre-Andre Noel",
      "Issam Laradji",
      "David Vazquez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.14488"
  },
  {
    "id": "arXiv:2209.01383",
    "title": "Training Strategies for Improved Lip-reading",
    "abstract": "Comments: ICASSP 2022. Code is available at this https URL",
    "descriptor": "\nComments: ICASSP 2022. Code is available at this https URL\n",
    "authors": [
      "Pingchuan Ma",
      "Yujiang Wang",
      "Stavros Petridis",
      "Jie Shen",
      "Maja Pantic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.01383"
  },
  {
    "id": "arXiv:2209.02765",
    "title": "Depression Symptoms Modelling from Social Media Text: A Semi-supervised  Learning Approach",
    "abstract": "Comments: Title and relevant changes are made",
    "descriptor": "\nComments: Title and relevant changes are made\n",
    "authors": [
      "Nawshad Farruque",
      "Randy Goebel",
      "Sudhakar Sivapalan",
      "Osmar Zaiane"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.02765"
  },
  {
    "id": "arXiv:2209.03807",
    "title": "Hardware Accelerator and Neural Network Co-Optimization for  Ultra-Low-Power Audio Processing Devices",
    "abstract": "Comments: Accepted Version for: EUROMICRO DSD 2022",
    "descriptor": "\nComments: Accepted Version for: EUROMICRO DSD 2022\n",
    "authors": [
      "Christoph Gerum",
      "Adrian Frischknecht",
      "Tobias Hald",
      "Paul Palomero Bernardo",
      "Konstantin L\u00fcbeck",
      "Oliver Bringmann"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.03807"
  },
  {
    "id": "arXiv:2209.06415",
    "title": "DMCA: Dense Multi-agent Navigation using Attention and Communication",
    "abstract": "DMCA: Dense Multi-agent Navigation using Attention and Communication",
    "descriptor": "",
    "authors": [
      "Senthil Hariharan Arul",
      "Amrit Singh Bedi",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.06415"
  },
  {
    "id": "arXiv:2209.06620",
    "title": "Distributionally Robust Offline Reinforcement Learning with Linear  Function Approximation",
    "abstract": "Comments: First two authors contribute equally",
    "descriptor": "\nComments: First two authors contribute equally\n",
    "authors": [
      "Xiaoteng Ma",
      "Zhipeng Liang",
      "Jose Blanchet",
      "Mingwen Liu",
      "Li Xia",
      "Jiheng Zhang",
      "Qianchuan Zhao",
      "Zhengyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.06620"
  },
  {
    "id": "arXiv:2209.07497",
    "title": "On Power Set Axiom",
    "abstract": "Comments: Minor changes. 4 pages",
    "descriptor": "\nComments: Minor changes. 4 pages\n",
    "authors": [
      "Leonid A. Levin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.07497"
  },
  {
    "id": "arXiv:2209.07518",
    "title": "Distribution Aware Metrics for Conditional Natural Language Generation",
    "abstract": "Distribution Aware Metrics for Conditional Natural Language Generation",
    "descriptor": "",
    "authors": [
      "David M Chan",
      "Yiming Ni",
      "David A Ross",
      "Sudheendra Vijayanarasimhan",
      "Austin Myers",
      "John Canny"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.07518"
  },
  {
    "id": "arXiv:2209.08378",
    "title": "Inducing Early Neural Collapse in Deep Neural Networks for Improved  Out-of-Distribution Detection",
    "abstract": "Comments: 11 pages, preprint, pending review with TMLR",
    "descriptor": "\nComments: 11 pages, preprint, pending review with TMLR\n",
    "authors": [
      "Jarrod Haas",
      "William Yolland",
      "Bernhard Rabus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.08378"
  },
  {
    "id": "arXiv:2209.08646",
    "title": "DeepTOP: Deep Threshold-Optimal Policy for MDPs and RMABs",
    "abstract": "Comments: Accepted for publication in NeurIPS 2022",
    "descriptor": "\nComments: Accepted for publication in NeurIPS 2022\n",
    "authors": [
      "Khaled Nakhleh",
      "I-Hong Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.08646"
  },
  {
    "id": "arXiv:2209.08809",
    "title": "One of Many: Assessing User-level Effects of Moderation Interventions on  r/The_Donald",
    "abstract": "One of Many: Assessing User-level Effects of Moderation Interventions on  r/The_Donald",
    "descriptor": "",
    "authors": [
      "Amaury Trujillo",
      "Stefano Cresci"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.08809"
  },
  {
    "id": "arXiv:2209.08868",
    "title": "Snowmass 2021 Computational Frontier CompF4 Topical Group Report:  Storage and Processing Resource Access",
    "abstract": "Comments: Snowmass 2021 Computational Frontier CompF4 topical group report. v2: Expanded introduction. Updated author list. 52 pages, 6 figures",
    "descriptor": "\nComments: Snowmass 2021 Computational Frontier CompF4 topical group report. v2: Expanded introduction. Updated author list. 52 pages, 6 figures\n",
    "authors": [
      "W. Bhimji",
      "D. Carder",
      "E. Dart",
      "J. Duarte",
      "I. Fisk",
      "R. Gardner",
      "C. Guok",
      "B. Jayatilaka",
      "T. Lehman",
      "M. Lin",
      "C. Maltzahn",
      "S. McKee",
      "M.S. Neubauer",
      "O. Rind",
      "O. Shadura",
      "N.V. Tran",
      "P. van Gemmeren",
      "G. Watts",
      "B.A. Weaver",
      "F. W\u00fcrthwein"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Lattice (hep-lat)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2209.08868"
  },
  {
    "id": "arXiv:2209.08926",
    "title": "Convergence of the number of period sets in strings",
    "abstract": "Comments: 14 pages, 1 figure, 2 tables, 12 bibliographic references; version 2: added a Related works section with additional references",
    "descriptor": "\nComments: 14 pages, 1 figure, 2 tables, 12 bibliographic references; version 2: added a Related works section with additional references\n",
    "authors": [
      "Eric Rivals",
      "Michelle Sweering",
      "Pengfei Wang"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.08926"
  },
  {
    "id": "arXiv:2209.09203",
    "title": "Look where you look! Saliency-guided Q-networks for visual RL tasks",
    "abstract": "Comments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), Nov 2022, New Orleans, United States",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), Nov 2022, New Orleans, United States\n",
    "authors": [
      "David Bertoin",
      "Adil Zouitine",
      "Mehdi Zouitine",
      "Emmanuel Rachelson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.09203"
  },
  {
    "id": "arXiv:2209.09338",
    "title": "Revisiting Embeddings for Graph Neural Networks",
    "abstract": "Revisiting Embeddings for Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "S. Purchase",
      "A. Zhao",
      "R. D. Mullins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.09338"
  },
  {
    "id": "arXiv:2209.09509",
    "title": "Data structures for topologically sound higher-dimensional diagram  rewriting",
    "abstract": "Comments: Final version submitted for proceedings of ACT 2022",
    "descriptor": "\nComments: Final version submitted for proceedings of ACT 2022\n",
    "authors": [
      "Amar Hadzihasanovic",
      "Diana Kessler"
    ],
    "subjectives": [
      "Category Theory (math.CT)",
      "Data Structures and Algorithms (cs.DS)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.09509"
  },
  {
    "id": "arXiv:2209.09731",
    "title": "Early Application Experiences on a Modern GPU-Accelerated Arm-based HPC  Platform",
    "abstract": "Early Application Experiences on a Modern GPU-Accelerated Arm-based HPC  Platform",
    "descriptor": "",
    "authors": [
      "Wael Elwasif",
      "Sergei Bastrakov",
      "Spencer H. Bryngelson",
      "Michael Bussmann",
      "Sunita Chandrasekaran",
      "Florina Ciorba",
      "M. A. Clark",
      "Alexander Debus",
      "William Godoy",
      "Nick Hagerty",
      "Jeff Hammond",
      "David Hardy",
      "J. Austin Harris",
      "Oscar Hernandez",
      "Balint Joo",
      "Sebastian Keller",
      "Paul Kent",
      "Henry Le Berre",
      "Damien Lebrun-Grandie",
      "Elijah MacCarthy",
      "Ver\u00f3nica G. Melesse Vergara",
      "Bronson Messer",
      "Ross Miller",
      "Sarp Oral",
      "Jean-Guillaume Piccinali",
      "Anand Radhakrishnan",
      "Osman Simsek",
      "Filippo Spiga",
      "Klaus Steiniger",
      "Jan Stephan",
      "John E. Stone",
      "Christian Trott",
      "Ren\u00e9 Widera",
      "Jeffrey Young"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.09731"
  },
  {
    "id": "arXiv:2209.09868",
    "title": "Streaming Encoding Algorithms for Scalable Hyperdimensional Computing",
    "abstract": "Comments: Fixes some typos and formatting issues",
    "descriptor": "\nComments: Fixes some typos and formatting issues\n",
    "authors": [
      "Anthony Thomas",
      "Behnam Khaleghi",
      "Gopi Krishna Jha",
      "Sanjoy Dasgupta",
      "Nageen Himayat",
      "Ravi Iyer",
      "Nilesh Jain",
      "Tajana Rosing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.09868"
  },
  {
    "id": "arXiv:2209.10063",
    "title": "Generate rather than Retrieve: Large Language Models are Strong Context  Generators",
    "abstract": "Comments: Preprint v2 (fix typos in v1), 22 pages",
    "descriptor": "\nComments: Preprint v2 (fix typos in v1), 22 pages\n",
    "authors": [
      "Wenhao Yu",
      "Dan Iter",
      "Shuohang Wang",
      "Yichong Xu",
      "Mingxuan Ju",
      "Soumya Sanyal",
      "Chenguang Zhu",
      "Michael Zeng",
      "Meng Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.10063"
  },
  {
    "id": "arXiv:2209.10428",
    "title": "An NWDAF Approach to 5G Core Network Signaling Traffic: Analysis and  Characterization",
    "abstract": "Comments: Accepted in IEEE GlobeCom 2022",
    "descriptor": "\nComments: Accepted in IEEE GlobeCom 2022\n",
    "authors": [
      "Dimitrios Michael Manias",
      "Ali Chouman",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.10428"
  },
  {
    "id": "arXiv:2209.10873",
    "title": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian  Preserving Flows",
    "abstract": "Turning Normalizing Flows into Monge Maps with Geodesic Gaussian  Preserving Flows",
    "descriptor": "",
    "authors": [
      "Guillaume Morel",
      "Lucas Drumetz",
      "Nicolas Courty",
      "Fran\u00e7ois Rousseau",
      "Simon Benaichouche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.10873"
  },
  {
    "id": "arXiv:2209.11222",
    "title": "Concept Activation Regions: A Generalized Framework For Concept-Based  Explanations",
    "abstract": "Comments: Presented at NeurIPS 2022",
    "descriptor": "\nComments: Presented at NeurIPS 2022\n",
    "authors": [
      "Jonathan Crabb\u00e9",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.11222"
  },
  {
    "id": "arXiv:2209.12374",
    "title": "High moment and pathwise error estimates for fully discrete mixed finite  element approximattions of stochastic Navier-Stokes equations with additive  noise",
    "abstract": "Comments: 36 pages",
    "descriptor": "\nComments: 36 pages\n",
    "authors": [
      "Xiaobing Feng",
      "Liet Vo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.12374"
  },
  {
    "id": "arXiv:2209.12643",
    "title": "PiFold: Toward effective and efficient protein inverse folding",
    "abstract": "PiFold: Toward effective and efficient protein inverse folding",
    "descriptor": "",
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.12643"
  },
  {
    "id": "arXiv:2209.13020",
    "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial  Intelligence with Humans",
    "abstract": "Comments: Draft; Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20",
    "descriptor": "\nComments: Draft; Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20\n",
    "authors": [
      "John J Nay"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13020"
  },
  {
    "id": "arXiv:2209.13167",
    "title": "A Morphology Focused Diffusion Probabilistic Model for Synthesis of  Histopathology Images",
    "abstract": "A Morphology Focused Diffusion Probabilistic Model for Synthesis of  Histopathology Images",
    "descriptor": "",
    "authors": [
      "Puria Azadi Moghadam",
      "Sanne Van Dalen",
      "Karina C. Martin",
      "Jochen Lennerz",
      "Stephen Yip",
      "Hossein Farahani",
      "Ali Bashashati"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.13167"
  },
  {
    "id": "arXiv:2209.13187",
    "title": "DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech  Entity Linking",
    "abstract": "Comments: Accepted to NLPCC2022, 10 pages",
    "descriptor": "\nComments: Accepted to NLPCC2022, 10 pages\n",
    "authors": [
      "Shen Huang",
      "Yuchen Zhai",
      "Xinwei Long",
      "Yong Jiang",
      "Xiaobin Wang",
      "Yin Zhang",
      "Pengjun Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13187"
  },
  {
    "id": "arXiv:2209.13263",
    "title": "Error Rate and Ergodic Capacity of RF-FSO System with Partial Relay  Selection in the Presence of Pointing Errors",
    "abstract": "Comments: Published at Optics Communications",
    "descriptor": "\nComments: Published at Optics Communications\n",
    "authors": [
      "Milica I. Petkovic",
      "Imran Shafique Ansari",
      "Goran T. Djordjevic",
      "Khalid A. Qaraqe"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.13263"
  },
  {
    "id": "arXiv:2209.13415",
    "title": "Shape optimization for the Laplacian eigenvalue over triangles and its  application to interpolation error constant estimation",
    "abstract": "Shape optimization for the Laplacian eigenvalue over triangles and its  application to interpolation error constant estimation",
    "descriptor": "",
    "authors": [
      "Ryoki Endo",
      "Xuefeng Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.13415"
  },
  {
    "id": "arXiv:2209.13476",
    "title": "Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with  Extremely Limited Labels",
    "abstract": "Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with  Extremely Limited Labels",
    "descriptor": "",
    "authors": [
      "Chenyu You",
      "Weicheng Dai",
      "Fenglin Liu",
      "Haoran Su",
      "Xiaoran Zhang",
      "Lawrence Staib",
      "James S. Duncan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13476"
  },
  {
    "id": "arXiv:2209.13523",
    "title": "Watch What You Pretrain For: Targeted, Transferable Adversarial Examples  on Self-Supervised Speech Recognition models",
    "abstract": "Watch What You Pretrain For: Targeted, Transferable Adversarial Examples  on Self-Supervised Speech Recognition models",
    "descriptor": "",
    "authors": [
      "Raphael Olivier",
      "Hadi Abdullah",
      "Bhiksha Raj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.13523"
  },
  {
    "id": "arXiv:2209.13570",
    "title": "Hierarchical Sliced Wasserstein Distance",
    "abstract": "Comments: 30 pages, 7 figures, 6 tables. arXiv admin note: text overlap with arXiv:2204.01188",
    "descriptor": "\nComments: 30 pages, 7 figures, 6 tables. arXiv admin note: text overlap with arXiv:2204.01188\n",
    "authors": [
      "Khai Nguyen",
      "Tongzheng Ren",
      "Huy Nguyen",
      "Litu Rout",
      "Tan Nguyen",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13570"
  },
  {
    "id": "arXiv:2209.13592",
    "title": "DVGAN: Stabilize Wasserstein GAN training for time-domain Gravitational  Wave physics",
    "abstract": "Comments: 10 pages, 6 figures, 3 tables",
    "descriptor": "\nComments: 10 pages, 6 figures, 3 tables\n",
    "authors": [
      "Tom Dooney",
      "Stefano Bromuri",
      "Lyana Curier"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Detectors (physics.ins-det)"
    ],
    "url": "https://arxiv.org/abs/2209.13592"
  },
  {
    "id": "arXiv:2209.13708",
    "title": "Falsification before Extrapolation in Causal Effect Estimation",
    "abstract": "Comments: Conference on Neural Information Processing Systems, 2022 (to appear)",
    "descriptor": "\nComments: Conference on Neural Information Processing Systems, 2022 (to appear)\n",
    "authors": [
      "Zeshan Hussain",
      "Michael Oberst",
      "Ming-Chieh Shih",
      "David Sontag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13708"
  },
  {
    "id": "arXiv:2209.13767",
    "title": "Internet Outage Detection using Passive Analysis (Poster Abstract and  Poster)",
    "abstract": "Internet Outage Detection using Passive Analysis (Poster Abstract and  Poster)",
    "descriptor": "",
    "authors": [
      "Asma Enayet",
      "John Heidemann"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.13767"
  },
  {
    "id": "arXiv:2209.13768",
    "title": "Disruptive Changes in Field Equation Modeling: A Simple Interface for  Wafer Scale Engines",
    "abstract": "Comments: 22 pages, 5 figures, 2 Tables",
    "descriptor": "\nComments: 22 pages, 5 figures, 2 Tables\n",
    "authors": [
      "Mino Woo",
      "Terry Jordan",
      "Robert Schreiber",
      "Ilya Sharapov",
      "Shaheer Muhammad",
      "Abhishek Koneru",
      "Michael James",
      "Dirk Van Essendelft"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)",
      "Mathematical Software (cs.MS)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2209.13768"
  },
  {
    "id": "arXiv:2209.14145",
    "title": "Multi-scale Attention Network for Single Image Super-Resolution",
    "abstract": "Multi-scale Attention Network for Single Image Super-Resolution",
    "descriptor": "",
    "authors": [
      "Yan Wang",
      "Yusen Li",
      "Gang Wang",
      "Xiaoguang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.14145"
  },
  {
    "id": "arXiv:2209.14281",
    "title": "Multilingual Search with Subword TF-IDF",
    "abstract": "Multilingual Search with Subword TF-IDF",
    "descriptor": "",
    "authors": [
      "Artit Wangperawong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14281"
  }
]