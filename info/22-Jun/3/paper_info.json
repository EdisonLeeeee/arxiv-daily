[
  {
    "id": "arXiv:2206.00667",
    "title": "How Biased is Your Feature?: Computing Fairness Influence Functions with  Global Sensitivity Analysis",
    "abstract": "Fairness in machine learning has attained significant focus due to the\nwidespread application of machine learning in high-stake decision-making tasks.\nUnless regulated with a fairness objective, machine learning classifiers might\ndemonstrate unfairness/bias towards certain demographic populations in the\ndata. Thus, the quantification and mitigation of the bias induced by\nclassifiers have become a central concern. In this paper, we aim to quantify\nthe influence of different features on the bias of a classifier. To this end,\nwe propose a framework of Fairness Influence Function (FIF), and compute it as\na scaled difference of conditional variances in the prediction of the\nclassifier. We also instantiate an algorithm, FairXplainer, that uses variance\ndecomposition among the subset of features and a local regressor to compute\nFIFs accurately, while also capturing the intersectional effects of the\nfeatures. Our experimental analysis validates that FairXplainer captures the\ninfluences of both individual features and higher-order feature interactions,\nestimates the bias more accurately than existing local explanation methods, and\ndetects the increase/decrease in bias due to affirmative/punitive actions in\nthe classifier.",
    "descriptor": "",
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S. Meel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00667"
  },
  {
    "id": "arXiv:2206.00669",
    "title": "Bayesian Learning to Discover Mathematical Operations in Governing  Equations of Dynamic Systems",
    "abstract": "Discovering governing equations from data is critical for diverse scientific\ndisciplines as they can provide insights into the underlying phenomenon of\ndynamic systems. This work presents a new representation for governing\nequations by designing the Mathematical Operation Network (MathONet) with a\ndeep neural network-like hierarchical structure. Specifically, the MathONet is\nstacked by several layers of unary operations (e.g., sin, cos, log) and binary\noperations (e.g., +,-), respectively. An initialized MathONet is typically\nregarded as a super-graph with a redundant structure, a sub-graph of which can\nyield the governing equation. We develop a sparse group Bayesian learning\nalgorithm to extract the sub-graph by employing structurally constructed priors\nover the redundant mathematical operations. By demonstrating the chaotic Lorenz\nsystem, Lotka-Volterra system, and Kolmogorov-Petrovsky-Piskunov system, the\nproposed method can discover the ordinary differential equations (ODEs) and\npartial differential equations (PDEs) from the observations given limited\nmathematical operations, without any prior knowledge on possible expressions of\nthe ODEs and PDEs.",
    "descriptor": "",
    "authors": [
      "Hongpeng Zhou",
      "Wei Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.00669"
  },
  {
    "id": "arXiv:2206.00679",
    "title": "Why Did This Model Forecast This Future? Closed-Form Temporal Saliency  Towards Causal Explanations of Probabilistic Forecasts",
    "abstract": "Forecasting tasks surrounding the dynamics of low-level human behavior are of\nsignificance to multiple research domains. In such settings, methods for\nexplaining specific forecasts can enable domain experts to gain insights into\nthe predictive relationships between behaviors. In this work, we introduce and\naddress the following question: given a probabilistic forecasting model how can\nwe identify observed windows that the model considers salient when making its\nforecasts? We build upon a general definition of information-theoretic saliency\ngrounded in human perception and extend it to forecasting settings by\nleveraging a crucial attribute of the domain: a single observation can result\nin multiple valid futures. We propose to express the saliency of an observed\nwindow in terms of the differential entropy of the resulting predicted future\ndistribution. In contrast to existing methods that either require explicit\ntraining of the saliency mechanism or access to the internal states of the\nforecasting model, we obtain a closed-form solution for the saliency map for\ncommonly used density functions in probabilistic forecasting. We empirically\ndemonstrate how our framework can recover salient observed windows from head\npose features for the sample task of speaking-turn forecasting using a\nsynthesized conversation dataset.",
    "descriptor": "",
    "authors": [
      "Chirag Raman",
      "Hayley Hung",
      "Marco Loog"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00679"
  },
  {
    "id": "arXiv:2206.00686",
    "title": "Federated Learning in Non-IID Settings Aided by Differentially Private  Synthetic Data",
    "abstract": "Federated learning (FL) is a privacy-promoting framework that enables\npotentially large number of clients to collaboratively train machine learning\nmodels. In a FL system, a server coordinates the collaboration by collecting\nand aggregating clients' model updates while the clients' data remains local\nand private. A major challenge in federated learning arises when the local data\nis heterogeneous -- the setting in which performance of the learned global\nmodel may deteriorate significantly compared to the scenario where the data is\nidentically distributed across the clients. In this paper we propose FedDPMS\n(Federated Differentially Private Means Sharing), an FL algorithm in which\nclients deploy variational auto-encoders to augment local datasets with data\nsynthesized using differentially private means of latent data representations\ncommunicated by a trusted server. Such augmentation ameliorates effects of data\nheterogeneity across the clients without compromising privacy. Our experiments\non deep image classification tasks demonstrate that FedDPMS outperforms\ncompeting state-of-the-art FL methods specifically designed for heterogeneous\ndata settings.",
    "descriptor": "",
    "authors": [
      "Huancheng Chen",
      "Haris Vikalo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.00686"
  },
  {
    "id": "arXiv:2206.00694",
    "title": "Meta-SysId: A Meta-Learning Approach for Simultaneous Identification and  Prediction",
    "abstract": "In this paper, we propose Meta-SysId, a meta-learning approach to model sets\nof systems that have behavior governed by common but unknown laws and that\ndifferentiate themselves by their context. Inspired by classical\nmodeling-and-identification approaches, Meta-SysId learns to represent the\ncommon law through shared parameters and relies on online optimization to\ncompute system-specific context. Compared to optimization-based meta-learning\nmethods, the separation between class parameters and context variables reduces\nthe computational burden while allowing batch computations and a simple\ntraining scheme. We test Meta-SysId on polynomial regression, time-series\nprediction, model-based control, and real-world traffic prediction domains,\nempirically finding it outperforms or is competitive with meta-learning\nbaselines.",
    "descriptor": "\nComments: 9 pages, 8 figures\n",
    "authors": [
      "Junyoung Park",
      "Federico Berto",
      "Arec Jamgochian",
      "Mykel J. Kochenderfer",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00694"
  },
  {
    "id": "arXiv:2206.00695",
    "title": "Know Your Boundaries: The Necessity of Explicit Behavioral Cloning in  Offline RL",
    "abstract": "We introduce an offline reinforcement learning (RL) algorithm that explicitly\nclones a behavior policy to constrain value learning. In offline RL, it is\noften important to prevent a policy from selecting unobserved actions, since\nthe consequence of these actions cannot be presumed without additional\ninformation about the environment. One straightforward way to implement such a\nconstraint is to explicitly model a given data distribution via behavior\ncloning and directly force a policy not to select uncertain actions. However,\nmany offline RL methods instantiate the constraint indirectly -- for example,\npessimistic value estimation -- due to a concern about errors when modeling a\npotentially complex behavior policy. In this work, we argue that it is not only\nviable but beneficial to explicitly model the behavior policy for offline RL\nbecause the constraint can be realized in a stable way with the trained model.\nWe first suggest a theoretical framework that allows us to incorporate\nbehavior-cloned models into value-based offline RL methods, enjoying the\nstrength of both explicit behavior cloning and value learning. Then, we propose\na practical method utilizing a score-based generative model for behavior\ncloning. With the proposed method, we show state-of-the-art performance on\nseveral datasets within the D4RL and Robomimic benchmarks and achieve\ncompetitive performance across all datasets tested.",
    "descriptor": "",
    "authors": [
      "Wonjoon Goo",
      "Scott Niekum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00695"
  },
  {
    "id": "arXiv:2206.00699",
    "title": "Studying the Practices of Deploying Machine Learning Projects on Docker",
    "abstract": "Docker is a containerization service that allows for convenient deployment of\nwebsites, databases, applications' APIs, and machine learning (ML) models with\na few lines of code. Studies have recently explored the use of Docker for\ndeploying general software projects with no specific focus on how Docker is\nused to deploy ML-based projects.\nIn this study, we conducted an exploratory study to understand how Docker is\nbeing used to deploy ML-based projects. As the initial step, we examined the\ncategories of ML-based projects that use Docker. We then examined why and how\nthese projects use Docker, and the characteristics of the resulting Docker\nimages. Our results indicate that six categories of ML-based projects use\nDocker for deployment, including ML Applications, MLOps/ AIOps, Toolkits, DL\nFrameworks, Models, and Documentation. We derived the taxonomy of 21 major\ncategories representing the purposes of using Docker, including those specific\nto models such as model management tasks (e.g., testing, training). We then\nshowed that ML engineers use Docker images mostly to help with the platform\nportability, such as transferring the software across the operating systems,\nruntimes such as GPU, and language constraints. However, we also found that\nmore resources may be required to run the Docker images for building ML-based\nsoftware projects due to the large number of files contained in the image\nlayers with deeply nested directories. We hope to shed light on the emerging\npractices of deploying ML software projects using containers and highlight\naspects that should be improved.",
    "descriptor": "",
    "authors": [
      "Moses Openja",
      "Forough Majidi",
      "Foutse Khomh",
      "Bhagya Chembakottu",
      "Heng Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00699"
  },
  {
    "id": "arXiv:2206.00700",
    "title": "RoCourseNet: Distributionally Robust Training of a Prediction Aware  Recourse Model",
    "abstract": "Counterfactual (CF) explanations for machine learning (ML) models are\npreferred by end-users, as they explain the predictions of ML models by\nproviding a recourse case to individuals who are adversely impacted by\npredicted outcomes. Existing CF explanation methods generate recourses under\nthe assumption that the underlying target ML model remains stationary over\ntime. However, due to commonly occurring distributional shifts in training\ndata, ML models constantly get updated in practice, which might render\npreviously generated recourses invalid and diminish end-users trust in our\nalgorithmic framework. To address this problem, we propose RoCourseNet, a\ntraining framework that jointly optimizes for predictions and robust recourses\nto future data shifts. We have three main contributions: (i) We propose a novel\nvirtual data shift (VDS) algorithm to find worst-case shifted ML models by\nexplicitly considering the worst-case data shift in the training dataset. (ii)\nWe leverage adversarial training to solve a novel tri-level optimization\nproblem inside RoCourseNet, which simultaneously generates predictions and\ncorresponding robust recourses. (iii) Finally, we evaluate RoCourseNet's\nperformance on three real-world datasets and show that RoCourseNet outperforms\nstate-of-the-art baselines by 10% in generating robust CF explanations.",
    "descriptor": "",
    "authors": [
      "Hangzhi Guo",
      "Feiran Jia",
      "Jinghui Chen",
      "Anna Squicciarini",
      "Amulya Yadav"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00700"
  },
  {
    "id": "arXiv:2206.00701",
    "title": "What Changed? Investigating Debiasing Methods using Causal Mediation  Analysis",
    "abstract": "Previous work has examined how debiasing language models affect downstream\ntasks, specifically, how debiasing techniques influence task performance and\nwhether debiased models also make impartial predictions in downstream tasks or\nnot. However, what we don't understand well yet is why debiasing methods have\nvarying impacts on downstream tasks and how debiasing techniques affect\ninternal components of language models, i.e., neurons, layers, and attentions.\nIn this paper, we decompose the internal mechanisms of debiasing language\nmodels with respect to gender by applying causal mediation analysis to\nunderstand the influence of debiasing methods on toxicity detection as a\ndownstream task. Our findings suggest a need to test the effectiveness of\ndebiasing methods with different bias metrics, and to focus on changes in the\nbehavior of certain components of the models, e.g.,first two layers of language\nmodels, and attention heads.",
    "descriptor": "",
    "authors": [
      "Sullam Jeoung",
      "Jana Diesner"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00701"
  },
  {
    "id": "arXiv:2206.00702",
    "title": "Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal  Search",
    "abstract": "Complex reasoning problems contain states that vary in the computational cost\nrequired to determine a good action plan. Taking advantage of this property, we\npropose Adaptive Subgoal Search (AdaSubS), a search method that adaptively\nadjusts the planning horizon. To this end, AdaSubS generates diverse sets of\nsubgoals at different distances. A verification mechanism is employed to filter\nout unreachable subgoals swiftly and thus allowing to focus on feasible further\nsubgoals. In this way, AdaSubS benefits from the efficiency of planning with\nlonger subgoals and the fine control with the shorter ones. We show that\nAdaSubS significantly surpasses hierarchical planning algorithms on three\ncomplex reasoning tasks: Sokoban, the Rubik's Cube, and inequality proving\nbenchmark INT, setting new state-of-the-art on INT.",
    "descriptor": "",
    "authors": [
      "Micha\u0142 Zawalski",
      "Micha\u0142 Tyrolski",
      "Konrad Czechowski",
      "Damian Stachura",
      "Piotr Pi\u0119kos",
      "Tomasz Odrzyg\u00f3\u017ad\u017a",
      "Yuhuai Wu",
      "\u0141ukasz Kuci\u0144ski",
      "Piotr Mi\u0142o\u015b"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00702"
  },
  {
    "id": "arXiv:2206.00705",
    "title": "Dense Crowd Flow-Informed Path Planning",
    "abstract": "Both pedestrian and robot comfort are of the highest priority whenever a\nrobot is placed in an environment containing human beings. In the case of\npedestrian-unaware mobile robots this desire for safety leads to the freezing\nrobot problem, where a robot confronted with a large dynamic group of obstacles\n(such as a crowd of pedestrians) would determine all forward navigation unsafe\ncausing the robot to stop in place. In order to navigate in a socially\ncompliant manner while avoiding the freezing robot problem we are interested in\nunderstanding the flow of pedestrians in crowded scenarios. By treating the\npedestrians in the crowd as particles moved along by the crowd itself we can\nmodel the system as a time dependent flow field. From this flow field we can\nextract different flow segments that reflect the motion patterns emerging from\nthe crowd. These motion patterns can then be accounted for during the control\nand navigation of a mobile robot allowing it to move safely within the flow of\nthe crowd to reach a desired location within or beyond the flow.\nWe combine flow-field extraction with a discrete heuristic search to create\nFlow-Informed path planning (FIPP). We provide empirical results showing that\nwhen compared against a trajectory-rollout local path planner, a robot using\nFIPP was able not only to reach its goal more quickly but also was shown to be\nmore socially compliant than a robot using traditional techniques both in\nsimulation and on real robots.",
    "descriptor": "",
    "authors": [
      "Emily Pruc",
      "Shlomo Zilberstein",
      "Joydeep Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00705"
  },
  {
    "id": "arXiv:2206.00708",
    "title": "The Boundary Element Method for Acoustic Transmission with Nonconforming  Grids",
    "abstract": "Acoustic wave propagation through a homogeneous material embedded in an\nunbounded medium can be formulated as a boundary integral equation and\naccurately solved with the boundary element method. The computational\nefficiency deteriorates at high frequencies due to the increase in mesh size\nwith a fixed number of elements per wavelength and ill-conditioning of the\nlinear system due to high material contrasts. This study presents the design of\nboundary element methods feasible for nonconforming surface meshes at the\nmaterial interface. The nonconforming algorithm allows for independent grid\ngeneration, which improves flexibility and reduces the degrees of freedom. It\nworks for different boundary integral formulations for Helmholtz transmission\nproblems, operator preconditioning, and coupling with finite element solvers.\nThe extensive numerical benchmarks at canonical configurations and an acoustic\nfoam model confirm the significant improvements in computational efficiency\nwhen employing the nonconforming grid coupling in the boundary element method.",
    "descriptor": "",
    "authors": [
      "Elwin van 't Wout"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.00708"
  },
  {
    "id": "arXiv:2206.00711",
    "title": "Learning to Solve PDE-constrained Inverse Problems with Graph Networks",
    "abstract": "Learned graph neural networks (GNNs) have recently been established as fast\nand accurate alternatives for principled solvers in simulating the dynamics of\nphysical systems. In many application domains across science and engineering,\nhowever, we are not only interested in a forward simulation but also in solving\ninverse problems with constraints defined by a partial differential equation\n(PDE). Here we explore GNNs to solve such PDE-constrained inverse problems.\nGiven a sparse set of measurements, we are interested in recovering the initial\ncondition or parameters of the PDE. We demonstrate that GNNs combined with\nautodecoder-style priors are well-suited for these tasks, achieving more\naccurate estimates of initial conditions or physical parameters than other\nlearned approaches when applied to the wave equation or Navier-Stokes\nequations. We also demonstrate computational speedups of up to 90x using GNNs\ncompared to principled solvers. Project page:\nhttps://cyanzhao42.github.io/LearnInverseProblem",
    "descriptor": "",
    "authors": [
      "Qingqing Zhao",
      "David B. Lindell",
      "Gordon Wetzstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00711"
  },
  {
    "id": "arXiv:2206.00716",
    "title": "Not so immutable: Upgradeability of Smart Contracts on Ethereum",
    "abstract": "A smart contract that is deployed to a blockchain system like Ethereum is,\nunder reasonable circumstances, expected to be immutable and tamper-proof. This\nis both a feature (promoting integrity and transparency) and a bug (preventing\nsecurity patches and feature updates). Modern smart contracts use software\ntricks to enable upgradeability, raising the research questions of how\nupgradeability is achieved and who is authorized to make changes. In this\npaper, we summarize and evaluate six upgradeability patterns. We develop a\nmeasurement framework for finding how many upgradeable contracts are on\nEthereum that use certain prominent upgrade patters. We find 1.4 million proxy\ncontracts which 8,225 of them are unique upgradeable proxy contracts. We also\nmeasure how they implement access control over their upgradeability: about 50%\nare controlled by a single Externally Owned Address (EOA), and about 14% are\ncontrolled by multi-signature wallets in which a limited number of persons can\nchange the whole logic of the contract.",
    "descriptor": "",
    "authors": [
      "Mehdi Salehi",
      "Jeremy Clark",
      "Mohammad Mannan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.00716"
  },
  {
    "id": "arXiv:2206.00717",
    "title": "K-Receiver Wiretap Channel: Optimal Encoding Order and Signaling Design",
    "abstract": "The K-receiver wiretap channel is a channel model where a transmitter\nbroadcasts K independent messages to K intended receivers while keeping them\nsecret from an eavesdropper. The capacity region of the K-receiver\nmultiple-input multiple-output (MIMO) wiretap channel has been characterized by\nusing dirty-paper coding and stochastic encoding. However, K factorial encoding\norders may need to be enumerated to evaluate the capacity region, which makes\nthe problem intractable. In addition, even though the capacity region is known,\nthe optimal signaling to achieve the capacity region is unknown. In this paper,\nwe determine one optimal encoding order to achieve every point on the capacity\nregion, and thus reduce the encoding complexity K factorial times. We prove\nthat the optimal decoding order for the K-receiver MIMO wiretap channel is the\nsame as that for the MIMO broadcast channel without secrecy. To be specific,\nthe descending weight ordering in the weighted sum-rate (WSR) maximization\nproblem determines the optimal encoding order. Next, to reach the border of the\nsecrecy capacity region, we form a WSR maximization problem and apply the block\nsuccessive maximization method to solve this nonconvex problem and find the\ninput covariance matrices corresponding to each message. Numerical results are\nused to verify the optimality of the encoding order and to demonstrate the\nefficacy of the proposed signaling design.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.06412\n",
    "authors": [
      "Yue Qi",
      "Mojtaba Vaezi",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.00717"
  },
  {
    "id": "arXiv:2206.00718",
    "title": "Context-Driven Detection of Invertebrate Species in Deep-Sea Video",
    "abstract": "Each year, underwater remotely operated vehicles (ROVs) collect thousands of\nhours of video of unexplored ocean habitats revealing a plethora of information\nregarding biodiversity on Earth. However, fully utilizing this information\nremains a challenge as proper annotations and analysis require trained\nscientists time, which is both limited and costly. To this end, we present a\nDataset for Underwater Substrate and Invertebrate Analysis (DUSIA), a benchmark\nsuite and growing large-scale dataset to train, validate, and test methods for\ntemporally localizing four underwater substrates as well as temporally and\nspatially localizing 59 underwater invertebrate species. DUSIA currently\nincludes over ten hours of footage across 25 videos captured in 1080p at 30 fps\nby an ROV following pre planned transects across the ocean floor near the\nChannel Islands of California. Each video includes annotations indicating the\nstart and end times of substrates across the video in addition to counts of\nspecies of interest. Some frames are annotated with precise bounding box\nlocations for invertebrate species of interest, as seen in Figure 1. To our\nknowledge, DUSIA is the first dataset of its kind for deep sea exploration,\nwith video from a moving camera, that includes substrate annotations and\ninvertebrate species that are present at significant depths where sunlight does\nnot penetrate. Additionally, we present the novel context-driven object\ndetector (CDD) where we use explicit substrate classification to influence an\nobject detection network to simultaneously predict a substrate and species\nclass influenced by that substrate. We also present a method for improving\ntraining on partially annotated bounding box frames. Finally, we offer a\nbaseline method for automating the counting of invertebrate species of\ninterest.",
    "descriptor": "",
    "authors": [
      "R. Austin McEver",
      "Bowen Zhang",
      "Connor Levenson",
      "A S M Iftekhar",
      "B.S. Manjunath"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00718"
  },
  {
    "id": "arXiv:2206.00719",
    "title": "Dataset Distillation using Neural Feature Regression",
    "abstract": "Dataset distillation aims to learn a small synthetic dataset that preserves\nmost of the information from the original dataset. Dataset distillation can be\nformulated as a bi-level meta-learning problem where the outer loop optimizes\nthe meta-dataset and the inner loop trains a model on the distilled data.\nMeta-gradient computation is one of the key challenges in this formulation, as\ndifferentiating through the inner loop learning procedure introduces\nsignificant computation and memory costs. In this paper, we address these\nchallenges using neural Feature Regression with Pooling (FRePo), achieving the\nstate-of-the-art performance with an order of magnitude less memory requirement\nand two orders of magnitude faster training than previous methods. The proposed\nalgorithm is analogous to truncated backpropagation through time with a pool of\nmodels to alleviate various types of overfitting in dataset distillation. FRePo\nsignificantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and\nImageNet-1K. Furthermore, we show that high-quality distilled data can greatly\nimprove various downstream applications, such as continual learning and\nmembership inference defense.",
    "descriptor": "",
    "authors": [
      "Yongchao Zhou",
      "Ehsan Nezhadarya",
      "Jimmy Ba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00719"
  },
  {
    "id": "arXiv:2206.00726",
    "title": "Cooperative Multi-Agent Trajectory Generation with Modular Bayesian  Optimization",
    "abstract": "We present a modular Bayesian optimization framework that efficiently\ngenerates time-optimal trajectories for a cooperative multi-agent system, such\nas a team of UAVs. Existing methods for multi-agent trajectory generation often\nrely on overly conservative constraints to reduce the complexity of this\nhigh-dimensional planning problem, leading to suboptimal solutions. We propose\na novel modular structure for the Bayesian optimization model that consists of\nmultiple Gaussian process surrogate models that represent the dynamic\nfeasibility and collision avoidance constraints. This modular structure\nalleviates the stark increase in computational cost with problem dimensionality\nand enables the use of minimal constraints in the joint optimization of the\nmulti-agent trajectories. The efficiency of the algorithm is further improved\nby introducing a scheme for simultaneous evaluation of the Bayesian\noptimization acquisition function and random sampling. The modular BayesOpt\nalgorithm was applied to optimize multi-agent trajectories through six unique\nenvironments using multi-fidelity evaluations from various data sources. It was\nfound that the resulting trajectories are faster than those obtained from two\nbaseline methods. The optimized trajectories were validated in real-world\nexperiments using four quadcopters that fly within centimeters of each other at\nspeeds up to 7.4 m/s.",
    "descriptor": "\nComments: Accepted to appear at Robotics: Science and Systems 2022. Video at this https URL\n",
    "authors": [
      "Gilhyun Ryou",
      "Ezra Tal",
      "Sertac Karaman"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.00726"
  },
  {
    "id": "arXiv:2206.00730",
    "title": "The Phenomenon of Policy Churn",
    "abstract": "We identify and study the phenomenon of policy churn, that is, the rapid\nchange of the greedy policy in value-based reinforcement learning. Policy churn\noperates at a surprisingly rapid pace, changing the greedy action in a large\nfraction of states within a handful of learning updates (in a typical deep RL\nset-up such as DQN on Atari). We characterise the phenomenon empirically,\nverifying that it is not limited to specific algorithm or environment\nproperties. A number of ablations help whittle down the plausible explanations\non why churn occurs to just a handful, all related to deep learning. Finally,\nwe hypothesise that policy churn is a beneficial but overlooked form of\nimplicit exploration that casts $\\epsilon$-greedy exploration in a fresh light,\nnamely that $\\epsilon$-noise plays a much smaller role than expected.",
    "descriptor": "",
    "authors": [
      "Tom Schaul",
      "Andr\u00e9 Barreto",
      "John Quan",
      "Georg Ostrovski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00730"
  },
  {
    "id": "arXiv:2206.00734",
    "title": "Measuring Discrimination Abilities of Monk Parakeets Between Discreet  and Continuous Quantities Through a Digital Life Enrichment Application",
    "abstract": "Ain et al. measured three African Grey (Psittacus erithacus) parrot's\ndiscrimination abilities between discreet and continuous quantities. Some\nfeatures of their experimental protocol make it difficult to apply to other\nsubjects and/or species without introducing a risk for some bias, as subjects\ncould read cues from the experimenter (even though the study's subjects\nprobably did not). Can digital life enrichment techniques permit us to\nreplicate their results with other species with less risk for experimental\nbias, with a better precision, and at lower cost? Inspired by previous informal\ndigital life enrichment experiments with parrots, we designed and tested a web\napplication to digitally replicate and extend Ain et al.'s experimental setup.\nWe were able to obtain similar results to theirs for two individuals from a\ndistinct species, Monk Parakeets (Myiopsitta Monachus), with increased\nguarantees against potential experimental biases, in a way which should allow\nto replicate such experiments at larger scale and at a much lower cost.",
    "descriptor": "\nComments: Long preliminary version\n",
    "authors": [
      "J\u00e9r\u00e9my Barbay",
      "Fabi\u00e1n Ja\u00f1a",
      "Crist\u00f3bal Sepulveda \u00c1lvarez"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2206.00734"
  },
  {
    "id": "arXiv:2206.00735",
    "title": "Cascaded Video Generation for Videos In-the-Wild",
    "abstract": "Videos can be created by first outlining a global view of the scene and then\nadding local details. Inspired by this idea we propose a cascaded model for\nvideo generation which follows a coarse to fine approach. First our model\ngenerates a low resolution video, establishing the global scene structure,\nwhich is then refined by subsequent cascade levels operating at larger\nresolutions. We train each cascade level sequentially on partial views of the\nvideos, which reduces the computational complexity of our model and makes it\nscalable to high-resolution videos with many frames. We empirically validate\nour approach on UCF101 and Kinetics-600, for which our model is competitive\nwith the state-of-the-art. We further demonstrate the scaling capabilities of\nour model and train a three-level model on the BDD100K dataset which generates\n256x256 pixels videos with 48 frames.",
    "descriptor": "\nComments: Accepted to the 26th International Conference on Pattern Recognition (ICPR 2022). arXiv admin note: substantial text overlap with arXiv:2106.02719\n",
    "authors": [
      "Lluis Castrejon",
      "Nicolas Ballas",
      "Aaron Courville"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00735"
  },
  {
    "id": "arXiv:2206.00737",
    "title": "Walk for Learning: A Random Walk Approach for Federated Learning from  Heterogeneous Data",
    "abstract": "We consider the problem of a Parameter Server (PS) that wishes to learn a\nmodel that fits data distributed on the nodes of a graph. We focus on Federated\nLearning (FL) as a canonical application. One of the main challenges of FL is\nthe communication bottleneck between the nodes and the parameter server. A\npopular solution in the literature is to allow each node to do several local\nupdates on the model in each iteration before sending it back to the PS. While\nthis mitigates the communication bottleneck, the statistical heterogeneity of\nthe data owned by the different nodes has proven to delay convergence and bias\nthe model. In this work, we study random walk (RW) learning algorithms for\ntackling the communication and data heterogeneity problems. The main idea is to\nleverage available direct connections among the nodes themselves, which are\ntypically \"cheaper\" than the communication to the PS. In a random walk, the\nmodel is thought of as a \"baton\" that is passed from a node to one of its\nneighbors after being updated in each iteration. The challenge in designing the\nRW is the data heterogeneity and the uncertainty about the data distributions.\nIdeally, we would want to visit more often nodes that hold more informative\ndata. We cast this problem as a sleeping multi-armed bandit (MAB) to design a\nnear-optimal node sampling strategy that achieves variance-reduced gradient\nestimates and approaches sub-linearly the optimal sampling strategy. Based on\nthis framework, we present an adaptive random walk learning algorithm. We\nprovide theoretical guarantees on its convergence. Our numerical results\nvalidate our theoretical findings and show that our algorithm outperforms\nexisting random walk algorithms.",
    "descriptor": "",
    "authors": [
      "Ghadir Ayache",
      "Venkat Dassari",
      "Salim El Rouayheb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00737"
  },
  {
    "id": "arXiv:2206.00738",
    "title": "Composition of Relational Features with an Application to Explaining  Black-Box Predictors",
    "abstract": "Relational machine learning programs like those developed in Inductive Logic\nProgramming (ILP) offer several advantages: (1) The ability to model complex\nrelationships amongst data instances; (2) The use of domain-specific relations\nduring model construction; and (3) The models constructed are human-readable,\nwhich is often one step closer to being human-understandable. However, these\nILP-like methods have not been able to capitalise fully on the rapid hardware,\nsoftware and algorithmic developments fuelling current developments in deep\nneural networks. In this paper, we treat relational features as functions and\nuse the notion of generalised composition of functions to derive complex\nfunctions from simpler ones. We formulate the notion of a set of\n$\\text{M}$-simple features in a mode language $\\text{M}$ and identify two\ncomposition operators ($\\rho_1$ and $\\rho_2$) from which all possible complex\nfeatures can be derived. We use these results to implement a form of\n\"explainable neural network\" called Compositional Relational Machines, or CRMs,\nwhich are labelled directed-acyclic graphs. The vertex-label for any vertex $j$\nin the CRM contains a feature-function $f_j$ and a continuous activation\nfunction $g_j$. If $j$ is a \"non-input\" vertex, then $f_j$ is the composition\nof features associated with vertices in the direct predecessors of $j$. Our\nfocus is on CRMs in which input vertices (those without any direct\npredecessors) all have $\\text{M}$-simple features in their vertex-labels. We\nprovide a randomised procedure for constructing and learning such CRMs. Using a\nnotion of explanations based on the compositional structure of features in a\nCRM, we provide empirical evidence on synthetic data of the ability to identify\nappropriate explanations; and demonstrate the use of CRMs as 'explanation\nmachines' for black-box models that do not provide explanations for their\npredictions.",
    "descriptor": "\nComments: 45 pages, submitted to Machine Learning Journal\n",
    "authors": [
      "Ashwin Srinivasan",
      "A Baskar",
      "Tirtharaj Dash",
      "Devanshu Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.00738"
  },
  {
    "id": "arXiv:2206.00744",
    "title": "A Log-Linear Time Sequential Optimal Calibration Algorithm for Quantized  Isotonic L2 Regression",
    "abstract": "We study the sequential calibration of estimations in a quantized isotonic L2\nregression setting. We start by showing that the optimal calibrated quantized\nestimations can be acquired from the traditional isotonic L2 regression\nsolution. We modify the traditional PAVA algorithm to create calibrators for\nboth batch and sequential optimization of the quantized isotonic regression\nproblem. Our algorithm can update the optimal quantized monotone mapping for\nthe samples observed so far in linear space and logarithmic time per new\nunordered sample.",
    "descriptor": "",
    "authors": [
      "Kaan Gokcesu",
      "Hakan Gokcesu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00744"
  },
  {
    "id": "arXiv:2206.00746",
    "title": "Residual Multiplicative Filter Networks for Multiscale Reconstruction",
    "abstract": "Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON\noffer some control over the frequency spectrum used to represent continuous\nsignals such as images or 3D volumes. Yet, they are not readily applicable to\nproblems for which coarse-to-fine estimation is required, including various\ninverse problems in which coarse-to-fine optimization plays a key role in\navoiding poor local minima. We introduce a new coordinate network architecture\nand training scheme that enables coarse-to-fine optimization with fine-grained\ncontrol over the frequency support of learned reconstructions. This is achieved\nwith two key innovations. First, we incorporate skip connections so that\nstructure at one scale is preserved when fitting finer-scale structure. Second,\nwe propose a novel initialization scheme to provide control over the model\nfrequency spectrum at each stage of optimization. We demonstrate how these\nmodifications enable multiscale optimization for coarse-to-fine fitting to\nnatural images. We then evaluate our model on synthetically generated datasets\nfor the the problem of single-particle cryo-EM reconstruction. We learn high\nresolution multiscale structures, on par with the state-of-the art.",
    "descriptor": "",
    "authors": [
      "Shayan Shekarforoush",
      "David B. Lindell",
      "David J. Fleet",
      "Marcus A. Brubaker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00746"
  },
  {
    "id": "arXiv:2206.00747",
    "title": "SolarGAN: Synthetic Annual Solar Irradiance Time Series on Urban  Building Facades via Deep Generative Networks",
    "abstract": "Building Integrated Photovoltaics (BIPV) is a promising technology to\ndecarbonize urban energy systems via harnessing solar energy available on\nbuilding envelopes. While methods to assess solar irradiation, especially on\nrooftops, are well established, the assessment on building facades usually\ninvolves a higher effort due to more complex urban features and obstructions.\nThe drawback of existing physics-based simulation programs is that they require\nsignificant manual modelling effort and computing time for generating time\nresolved deterministic results. Yet, solar irradiation is highly intermittent\nand representing its inherent uncertainty may be required for designing robust\nBIPV energy systems. Targeting on these drawbacks, this paper proposes a\ndata-driven model based on Deep Generative Networks (DGN) to efficiently\ngenerate high-fidelity stochastic ensembles of annual hourly solar irradiance\ntime series on building facades with uncompromised spatiotemporal resolution at\nthe urban scale. The only input required is easily obtainable, simple fisheye\nimages as categorical shading masks captured from 3D models. In principle, even\nactual photographs of urban contexts can be utilized, given they are\nsemantically segmented. Our validations exemplify the high fidelity of the\ngenerated time series when compared to the physics-based simulator. To\ndemonstrate the model's relevance for urban energy planning, we showcase its\npotential for generative design by parametrically altering characteristic\nfeatures of the urban environment and producing corresponding time series on\nbuilding facades under different climatic contexts in real-time.",
    "descriptor": "",
    "authors": [
      "Yufei Zhang",
      "Arno Schl\u00fcter",
      "Christoph Waibel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00747"
  },
  {
    "id": "arXiv:2206.00751",
    "title": "Disturbance Observer Based Frequency & Voltage Regulation for RES  Integrated Uncertain Power Systems",
    "abstract": "This paper proposes a disturbance-observer-based control (DOBC) scheme for\nfrequency and voltage regulation for a renewable energy sources (RES)\nintegrated power systems. The proposed approach acts a feed-forward control\nwhich improves the dynamic performance of the conventional\nproportional-integral-derivative (PID) controller. Robustness of the proposed\ncontrol scheme has been validated through simulations under worst-case and\nstochastic uncertainties to mitigate real-time variability in RES output and\nload. The performance of the proposed control technique is compared to well\nestablished technique in presence of communication delay and white noise.",
    "descriptor": "",
    "authors": [
      "Himanshu Grover",
      "Ashu Verma",
      "T.S.Bhatti"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00751"
  },
  {
    "id": "arXiv:2206.00752",
    "title": "Algorithmic Applications of Tree-Cut Width",
    "abstract": "The recently introduced graph parameter tree-cut width plays a similar role\nwith respect to immersions as the graph parameter treewidth plays with respect\nto minors. In this paper, we provide the first algorithmic applications of\ntree-cut width to hard combinatorial problems. Tree-cut width is known to be\nlower-bounded by a function of treewidth, but it can be much larger and hence\nhas the potential to facilitate the efficient solution of problems that are not\nknown to be fixed-parameter tractable (FPT) when parameterized by treewidth. We\nintroduce the notion of nice tree-cut decompositions and provide FPT algorithms\nfor the showcase problems Capacitated Vertex Cover, Capacitated Dominating Set,\nand Imbalance parameterized by the tree-cut width of an input graph. On the\nother hand, we show that List Coloring, Precoloring Extension, and Boolean CSP\n(the latter parameterized by the tree-cut width of the incidence graph) are\nW[1]-hard and hence unlikely to be fixed-parameter tractable when parameterized\nby tree-cut width.",
    "descriptor": "\nComments: Full version to appear in the Siam Journal on Discrete Mathematics\n",
    "authors": [
      "Robert Ganian",
      "Eun Jung Kim",
      "Stefan Szeider"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.00752"
  },
  {
    "id": "arXiv:2206.00756",
    "title": "Energy-Efficient Hybrid Offloading for Backscatter-Assisted Wirelessly  Powered MEC with Reconfigurable Intelligent Surfaces",
    "abstract": "We investigate a wireless power transfer (WPT)-based backscatter-mobile edge\ncomputing (MEC) network with a {reconfigurable intelligent surface (RIS)}.In\nthis network, wireless devices (WDs) offload task bits and harvest energy, and\nthey can switch between backscatter communication (BC) and active transmission\n(AT) modes. We exploit the RIS to maximize energy efficiency (EE). To this end,\nwe optimize the time/power allocations, local computing frequencies, execution\ntimes, backscattering coefficients, and RIS phase shifts.} This goal results in\na multi-objective optimization problem (MOOP) with conflicting objectives.\nThus, we simultaneously maximize system throughput and minimize energy\nconsumption via the Tchebycheff method, transforming into two single-objective\noptimization problems (SOOPs). For throughput maximization, we exploit\nalternating optimization (AO) to yield two sub-problems. For the first one, we\nderive closed-form resource allocations. For the second one, we design the RIS\nphase shifts via semi-definite relaxation, a difference of convex functions\nprogramming, majorization minimization techniques, and a penalty function for\nenforcing a rank-one solution. For energy minimization, we derive closed-form\nresource allocations. We demonstrate the gains over several benchmarks. For\ninstance, with a $20$-element RIS, EE can be as high as 3 (Mbits/Joule), a\n150\\% improvement over the no-RIS case (achieving only 2 (Mbits/Joule)).",
    "descriptor": "\nComments: The final version of this paper was accepted by IEEE Transactions on Mobile Computing\n",
    "authors": [
      "S. Zargari",
      "C. Tellambura",
      "S. Herath"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.00756"
  },
  {
    "id": "arXiv:2206.00757",
    "title": "A technical note for a Shor's algorithm by phase estimation",
    "abstract": "The objective of this paper concerns at first the motivation and the method\nof Shor's algorithm including an excursion into quantum mechanics and quantum\ncomputing introducing an algorithmic description of the method. The corner\nstone of the Shor's algorithm is the modular exponentiation that is the most\ncomputational component (in time and space). Second, a linear depth unit based\non phase estimation is introduced and a description of a generic version of a\nmodular multiplier based on phases is introduced to build block of a modular\nexponentiation circuit. Our proposal includes numerical experiments achieved on\nboth the IBM simulator using the Qiskit library and on quantum physical\noptimizers provided by IBM.",
    "descriptor": "",
    "authors": [
      "G\u00e9rard Fleury",
      "Philippe Lacomme"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.00757"
  },
  {
    "id": "arXiv:2206.00759",
    "title": "Merlin-Arthur Classifiers: Formal Interpretability with Interactive  Black Boxes",
    "abstract": "We present a new theoretical framework for making black box classifiers such\nas Neural Networks interpretable, basing our work on clear assumptions and\nguarantees. In our setting, which is inspired by the Merlin-Arthur protocol\nfrom Interactive Proof Systems, two functions cooperate to achieve a\nclassification together: the \\emph{prover} selects a small set of features as a\ncertificate and presents it to the \\emph{classifier}. Including a second,\nadversarial prover allows us to connect a game-theoretic equilibrium to\ninformation-theoretic guarantees on the exchanged features. We define notions\nof completeness and soundness that enable us to lower bound the mutual\ninformation between features and class. To demonstrate good agreement between\ntheory and practice, we support our framework by providing numerical\nexperiments for Neural Network classifiers, explicitly calculating the mutual\ninformation of features with respect to the class.",
    "descriptor": "\nComments: 26 pages, 14 figures, 2 tables, 1 algorithm\n",
    "authors": [
      "Stephan W\u00e4ldchen",
      "Kartikey Sharma",
      "Max Zimmer",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00759"
  },
  {
    "id": "arXiv:2206.00761",
    "title": "On Reinforcement Learning and Distribution Matching for Fine-Tuning  Language Models with no Catastrophic Forgetting",
    "abstract": "The availability of large pre-trained models is changing the landscape of\nMachine Learning research and practice, moving from a training-from-scratch to\na fine-tuning paradigm. While in some applications the goal is to \"nudge\" the\npre-trained distribution towards preferred outputs, in others it is to steer it\ntowards a different distribution over the sample space. Two main paradigms have\nemerged to tackle this challenge: Reward Maximization (RM) and, more recently,\nDistribution Matching (DM). RM applies standard Reinforcement Learning (RL)\ntechniques, such as Policy Gradients, to gradually increase the reward signal.\nDM prescribes to first make explicit the target distribution that the model is\nfine-tuned to approximate. Here we explore the theoretical connections between\nthe two paradigms, and show that methods such as KL-control developed for RM\ncan also be construed as belonging to DM. We further observe that while DM\ndiffers from RM, it can suffer from similar training difficulties, such as high\ngradient variance. We leverage connections between the two paradigms to import\nthe concept of baseline into DM methods. We empirically validate the benefits\nof adding a baseline on an array of controllable language generation tasks such\nas constraining topic, sentiment, and gender distributions in texts sampled\nfrom a language model. We observe superior performance in terms of constraint\nsatisfaction, stability and sample efficiency.",
    "descriptor": "",
    "authors": [
      "Tomasz Korbak",
      "Hady Elsahar",
      "Germ\u00e1n Kruszewski",
      "Marc Dymetman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00761"
  },
  {
    "id": "arXiv:2206.00769",
    "title": "Defense Against Gradient Leakage Attacks via Learning to Obscure Data",
    "abstract": "Federated learning is considered as an effective privacy-preserving learning\nmechanism that separates the client's data and model training process. However,\nfederated learning is still under the risk of privacy leakage because of the\nexistence of attackers who deliberately conduct gradient leakage attacks to\nreconstruct the client data. Recently, popular strategies such as gradient\nperturbation methods and input encryption methods have been proposed to defend\nagainst gradient leakage attacks. Nevertheless, these defenses can either\ngreatly sacrifice the model performance, or be evaded by more advanced attacks.\nIn this paper, we propose a new defense method to protect the privacy of\nclients' data by learning to obscure data. Our defense method can generate\nsynthetic samples that are totally distinct from the original samples, but they\ncan also maximally preserve their predictive features and guarantee the model\nperformance. Furthermore, our defense strategy makes the gradient leakage\nattack and its variants extremely difficult to reconstruct the client data.\nThrough extensive experiments, we show that our proposed defense method obtains\nbetter privacy protection while preserving high accuracy compared with\nstate-of-the-art methods.",
    "descriptor": "\nComments: 13 pages, 2 figures\n",
    "authors": [
      "Yuxuan Wan",
      "Han Xu",
      "Xiaorui Liu",
      "Jie Ren",
      "Wenqi Fan",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00769"
  },
  {
    "id": "arXiv:2206.00770",
    "title": "Winning the 3rd Japan Automotive AI Challenge -- Autonomous Racing with  the Autoware.Auto Open Source Software Stack",
    "abstract": "The 3rd Japan Automotive AI Challenge was an international online autonomous\nracing challenge where 164 teams competed in December 2021. This paper outlines\nthe winning strategy to this competition, and the advantages and challenges of\nusing the Autoware.Auto open source autonomous driving platform for multi-agent\nracing. Our winning approach includes a lane-switching opponent overtaking\nstrategy, a global raceline optimization, and the integration of various tools\nfrom Autoware.Auto including a Model-Predictive Controller. We describe the use\nof perception, planning and control modules for high-speed racing applications\nand provide experience-based insights on working with Autoware.Auto. While our\napproach is a rule-based strategy that is suitable for non-interactive\nopponents, it provides a good reference and benchmark for learning-enabled\napproaches.",
    "descriptor": "\nComments: Accepted at Autoware Workshop at IV 2022\n",
    "authors": [
      "Zirui Zang",
      "Renukanandan Tumu",
      "Johannes Betz",
      "Hongrui Zheng",
      "Rahul Mangharam"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00770"
  },
  {
    "id": "arXiv:2206.00771",
    "title": "Dynamic Linear Transformer for 3D Biomedical Image Segmentation",
    "abstract": "Transformer-based neural networks have surpassed promising performance on\nmany biomedical image segmentation tasks due to a better global information\nmodeling from the self-attention mechanism. However, most methods are still\ndesigned for 2D medical images while ignoring the essential 3D volume\ninformation. The main challenge for 3D transformer-based segmentation methods\nis the quadratic complexity introduced by the self-attention mechanism\n\\cite{vaswani2017attention}. In this paper, we propose a novel transformer\narchitecture for 3D medical image segmentation using an encoder-decoder style\narchitecture with linear complexity. Furthermore, we newly introduce a dynamic\ntoken concept to further reduce the token numbers for self-attention\ncalculation. Taking advantage of the global information modeling, we provide\nuncertainty maps from different hierarchy stages. We evaluate this method on\nmultiple challenging CT pancreas segmentation datasets. Our promising results\nshow that our novel 3D Transformer-based segmentor could provide promising\nhighly feasible segmentation performance and accurate uncertainty\nquantification using single annotation. Code is available\nhttps://github.com/freshman97/LinTransUNet.",
    "descriptor": "\nComments: 8 Pages\n",
    "authors": [
      "Zheyuan Zhang",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00771"
  },
  {
    "id": "arXiv:2206.00772",
    "title": "On the reversibility of adversarial attacks",
    "abstract": "Adversarial attacks modify images with perturbations that change the\nprediction of classifiers. These modified images, known as adversarial\nexamples, expose the vulnerabilities of deep neural network classifiers. In\nthis paper, we investigate the predictability of the mapping between the\nclasses predicted for original images and for their corresponding adversarial\nexamples. This predictability relates to the possibility of retrieving the\noriginal predictions and hence reversing the induced misclassification. We\nrefer to this property as the reversibility of an adversarial attack, and\nquantify reversibility as the accuracy in retrieving the original class or the\ntrue class of an adversarial example. We present an approach that reverses the\neffect of an adversarial attack on a classifier using a prior set of\nclassification results. We analyse the reversibility of state-of-the-art\nadversarial attacks on benchmark classifiers and discuss the factors that\naffect the reversibility.",
    "descriptor": "",
    "authors": [
      "Chau Yi Li",
      "Ricardo S\u00e1nchez-Matilla",
      "Ali Shahin Shamsabadi",
      "Riccardo Mazzon",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.00772"
  },
  {
    "id": "arXiv:2206.00773",
    "title": "Assessing the trade-off between prediction accuracy and interpretability  for topic modeling on energetic materials corpora",
    "abstract": "As the amount and variety of energetics research increases, machine aware\ntopic identification is necessary to streamline future research pipelines. The\nmakeup of an automatic topic identification process consists of creating\ndocument representations and performing classification. However, the\nimplementation of these processes on energetics research imposes new\nchallenges. Energetics datasets contain many scientific terms that are\nnecessary to understand the context of a document but may require more complex\ndocument representations. Secondly, the predictions from classification must be\nunderstandable and trusted by the chemists within the pipeline. In this work,\nwe study the trade-off between prediction accuracy and interpretability by\nimplementing three document embedding methods that vary in computational\ncomplexity. With our accuracy results, we also introduce local interpretability\nmodel-agnostic explanations (LIME) of each prediction to provide a localized\nunderstanding of each prediction and to validate classifier decisions with our\nteam of energetics experts. This study was carried out on a novel labeled\nenergetics dataset created and validated by our team of energetics experts.",
    "descriptor": "\nComments: Accepted for publication in the 25th International Seminar New Trends in Research of Energetic Materials (NTREM 2022 proceedings)\n",
    "authors": [
      "Monica Puerto",
      "Mason Kellett",
      "Rodanthi Nikopoulou",
      "Mark D. Fuge",
      "Ruth Doherty",
      "Peter W. Chung",
      "Zois Boukouvalas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00773"
  },
  {
    "id": "arXiv:2206.00774",
    "title": "Distributed Training for Deep Learning Models On An Edge Computing  Network Using ShieldedReinforcement Learning",
    "abstract": "Edge devices with local computation capability has made distributed deep\nlearning training on edges possible. In such method, the cluster head of a\ncluster of edges schedules DL training jobs from the edges. Using such\ncentralized scheduling method, the cluster head knows all loads of edges, which\ncan avoid overloading the cluster edges, but the head itself may become\noverloaded. To handle this problem, we propose a multi-agent RL (MARL) system\nthat enables each edge to schedule its jobs using RL. However, without\ncoordination among edges, action collision may occur, in which multiple edges\nschedule tasks to the same edge and make it overloaded. For this reason, we\npropose a system called Shielded ReinfOrcement learning (RL) based DL training\non Edges (SROLE). In SROLE, the shield deployed in an edge checks action\ncollisions and provides alternative actions to avoid collisions. As the central\nshield for entire cluster may become a bottleneck, we further propose a\ndecentralized shielding method, where different shields are responsible for\ndifferent regions in the cluster and they coordinate to avoid action collisions\non the region boundaries. Our emulation and real device experiments show SROLE\nreduces training time by 59% compared to MARL and centralized RL.",
    "descriptor": "\nComments: Accepted in 2022 International Conference on Distributed Computing Systems (ICDCS)\n",
    "authors": [
      "Tanmoy Sen",
      "Haiying Shen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.00774"
  },
  {
    "id": "arXiv:2206.00776",
    "title": "Robots in healthcare as envisioned by care professionals",
    "abstract": "As AI-enabled robots enter the realm of healthcare and caregiving, it is\nimportant to consider how they will address the dimensions of care and how they\nwill interact not just with the direct receivers of assistance, but also with\nthose who provide it (e.g., caregivers, healthcare providers etc.). Caregiving\nin its best form addresses challenges in a multitude of dimensions of a\nperson's life: from physical, to social-emotional and sometimes even\nexistential dimensions (such as issues surrounding life and death). In this\nstudy we use semi-structured qualitative interviews administered to healthcare\nprofessions with multidisciplinary backgrounds (physicians, public health\nprofessionals, social workers, and chaplains) to understand their expectations\nregarding the possible roles robots may play in the healthcare ecosystem in the\nfuture. We found that participants drew inspiration in their mental models of\nrobots from both works of science fiction but also from existing commercial\nrobots. Participants envisioned roles for robots in the full spectrum of care,\nfrom physical to social-emotional and even existential-spiritual dimensions,\nbut also pointed out numerous limitations that robots have in being able to\nprovide comprehensive humanistic care. While no dimension of care was deemed as\nexclusively the realm of humans, participants stressed the importance of\ncaregiving humans as the primary providers of comprehensive care, with robots\nassisting with more narrowly focused tasks. Throughout the paper we point out\nthe encouraging confluence of ideas between the expectations of healthcare\nproviders and research trends in the human-robot interaction (HRI) literature.",
    "descriptor": "",
    "authors": [
      "Fran Soljacic",
      "Meia Chita-Tegmark",
      "Theresa Law",
      "Matthias Scheutz"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.00776"
  },
  {
    "id": "arXiv:2206.00777",
    "title": "Visual Navigation for Autonomous Vehicles: An Open-source Hands-on  Robotics Course at MIT",
    "abstract": "This paper reports on the development, execution, and open-sourcing of a new\nrobotics course at MIT. The course is a modern take on \"Visual Navigation for\nAutonomous Vehicles\" (VNAV) and targets first-year graduate students and senior\nundergraduates with prior exposure to robotics. VNAV has the goal of preparing\nthe students to perform research in robotics and vision-based navigation, with\nemphasis on drones and self-driving cars. The course spans the entire\nautonomous navigation pipeline; as such, it covers a broad set of topics,\nincluding geometric control and trajectory optimization, 2D and 3D computer\nvision, visual and visual-inertial odometry, place recognition, simultaneous\nlocalization and mapping, and geometric deep learning for perception. VNAV has\nthree key features. First, it bridges traditional computer vision and robotics\ncourses by exposing the challenges that are specific to embodied intelligence,\ne.g., limited computation and need for just-in-time and robust perception to\nclose the loop over control and decision making. Second, it strikes a balance\nbetween depth and breadth by combining rigorous technical notes (including\ntopics that are less explored in typical robotics courses, e.g., on-manifold\noptimization) with slides and videos showcasing the latest research results.\nThird, it provides a compelling approach to hands-on robotics education by\nleveraging a physical drone platform (mostly suitable for small residential\ncourses) and a photo-realistic Unity-based simulator (open-source and scalable\nto large online courses). VNAV has been offered at MIT in the Falls of\n2018-2021 and is now publicly available on MIT OpenCourseWare (OCW).",
    "descriptor": "\nComments: This paper has been accepted for publication at the IEEE Integrated STEM Education Conference\n",
    "authors": [
      "Luca Carlone",
      "Kasra Khosoussi",
      "Vasileios Tzoumas",
      "Golnaz Habibi",
      "Markus Ryll",
      "Rajat Talak",
      "Jingnan Shi",
      "Pasquale Antonante"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.00777"
  },
  {
    "id": "arXiv:2206.00781",
    "title": "Near-Optimal Search Time in $\u03b4$-Optimal Space",
    "abstract": "Two recent lower bounds on the compressiblity of repetitive sequences,\n$\\delta \\le \\gamma$, have received much attention. It has been shown that a\nstring $S[1..n]$ can be represented within the optimal\n$O(\\delta\\log\\frac{n}{\\delta})$ space, and further, that within that space one\ncan find all the $occ$ occurrences in $S$ of any pattern of length $m$ in time\n$O(m\\log n + occ \\log^\\epsilon n)$ for any constant $\\epsilon>0$. Instead, the\nnear-optimal search time $O(m+(occ+1)\\log^\\epsilon n)$ was achieved only within\n$O(\\gamma\\log\\frac{n}{\\gamma})$ space. Both results are based on considerably\ndifferent locally consistent parsing techniques. The question of whether the\nbetter search time could be obtained within the $\\delta$-optimal space was\nopen. In this paper we prove that both techniques can indeed be combined in\norder to obtain the best of both worlds, $O(m+(occ+1)\\log^\\epsilon n)$ search\ntime within $O(\\delta\\log\\frac{n}{\\delta})$ space.",
    "descriptor": "",
    "authors": [
      "Tomasz Kociumaka",
      "Gonzalo Navarro",
      "Francisco Olivares"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.00781"
  },
  {
    "id": "arXiv:2206.00783",
    "title": "Core-periphery Models for Hypergraphs",
    "abstract": "We introduce a random hypergraph model for core-periphery structure. By\nleveraging our model's sufficient statistics, we develop a novel statistical\ninference algorithm that is able to scale to large hypergraphs with runtime\nthat is practically linear wrt. the number of nodes in the graph after a\npreprocessing step that is almost linear in the number of hyperedges, as well\nas a scalable sampling algorithm. Our inference algorithm is capable of\nlearning embeddings that correspond to the reputation (rank) of a node within\nthe hypergraph. We also give theoretical bounds on the size of the core of\nhypergraphs generated by our model. We experiment with hypergraph data that\nrange to $\\sim 10^5$ hyperedges mined from the Microsoft Academic Graph, Stack\nExchange, and GitHub and show that our model outperforms baselines wrt.\nproducing good fits.",
    "descriptor": "\nComments: Accepted at as a research track paper at KDD 2022\n",
    "authors": [
      "Marios Papachristou",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00783"
  },
  {
    "id": "arXiv:2206.00785",
    "title": "Delivering Document Conversion as a Cloud Service with High Throughput  and Responsiveness",
    "abstract": "Document understanding is a key business process in the data-driven economy\nsince documents are central to knowledge discovery and business insights.\nConverting documents into a machine-processable format is a particular\nchallenge here due to their huge variability in formats and complex structure.\nAccordingly, many algorithms and machine-learning methods emerged to solve\nparticular tasks such as Optical Character Recognition (OCR), layout analysis,\ntable-structure recovery, figure understanding, etc. We observe the adoption of\nsuch methods in document understanding solutions offered by all major cloud\nproviders. Yet, publications outlining how such services are designed and\noptimized to scale in the cloud are scarce. In this paper, we focus on the case\nof document conversion to illustrate the particular challenges of scaling a\ncomplex data processing pipeline with a strong reliance on machine-learning\nmethods on cloud infrastructure. Our key objective is to achieve high\nscalability and responsiveness for different workload profiles in a\nwell-defined resource budget. We outline the requirements, design, and\nimplementation choices of our document conversion service and reflect on the\nchallenges we faced. Evidence for the scaling behavior and resource efficiency\nis provided for two alternative workload distribution strategies and deployment\nconfigurations. Our best-performing method achieves sustained throughput of\nover one million PDF pages per hour on 3072 CPU cores across 192 nodes.",
    "descriptor": "\nComments: 11 pages, 7 figures, to be published in IEEE CLOUD 2022\n",
    "authors": [
      "Christoph Auer",
      "Michele Dolfi",
      "Andr\u00e9 Carvalho",
      "Cesar Berrospi Ramis",
      "Peter W. J. Staar"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.00785"
  },
  {
    "id": "arXiv:2206.00786",
    "title": "Neural Decoding with Optimization of Node Activations",
    "abstract": "The problem of maximum likelihood decoding with a neural decoder for\nerror-correcting code is considered. It is shown that the neural decoder can be\nimproved with two novel loss terms on the node's activations. The first loss\nterm imposes a sparse constraint on the node's activations. Whereas, the second\nloss term tried to mimic the node's activations from a teacher decoder which\nhas better performance. The proposed method has the same run time complexity\nand model size as the neural Belief Propagation decoder, while improving the\ndecoding performance by up to $1.1dB$ on BCH codes.",
    "descriptor": "",
    "authors": [
      "Eliya Nachmani",
      "Yair Be'ery"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.00786"
  },
  {
    "id": "arXiv:2206.00787",
    "title": "On the Generalization of Neural Combinatorial Optimization Heuristics",
    "abstract": "Neural Combinatorial Optimization approaches have recently leveraged the\nexpressiveness and flexibility of deep neural networks to learn efficient\nheuristics for hard Combinatorial Optimization (CO) problems. However, most of\nthe current methods lack generalization: for a given CO problem, heuristics\nwhich are trained on instances with certain characteristics underperform when\ntested on instances with different characteristics. While some previous works\nhave focused on varying the training instances properties, we postulate that a\none-size-fit-all model is out of reach. Instead, we formalize solving a CO\nproblem over a given instance distribution as a separate learning task and\ninvestigate meta-learning techniques to learn a model on a variety of tasks, in\norder to optimize its capacity to adapt to new tasks. Through extensive\nexperiments, on two CO problems, using both synthetic and realistic instances,\nwe show that our proposed meta-learning approach significantly improves the\ngeneralization of two state-of-the-art models.",
    "descriptor": "",
    "authors": [
      "Sahil Manchanda",
      "Sofia Michel",
      "Darko Drakulic",
      "Jean-Marc Andreoli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00787"
  },
  {
    "id": "arXiv:2206.00788",
    "title": "Information-based matching explains the diversity of cooperation among  different populations",
    "abstract": "This paper introduces a bilateral matching mechanism to explain why different\npopulations have different levels of cooperation. The traditional game theory\nassumes that individuals can acquire their neighbor's information without cost\nafter generating information. In fact, the environment and cognition of\npopulations often limit the magnitude of information received by individuals.\nOur model divides information dynamics into two processes: generation and\ndissemination. After generating, information starts to disseminate in the\npopulation. Individuals match and interact with each other based on the\ninformation received and then confirm partnerships, which differs from\ntraditional research's unilateral partner selection process. Specifically, we\nfind a function to simulate two constraints of information acquisition in\ndifferent populations: information dissemination cost and cognition competence.\nThese two kinds of constraints affect the choice of partnership and then the\nevolution of cooperation. The game evolved under the condition of information\nconstraints. Through large-scale Monte Carlo simulations, we find that\ninformation dissemination and cognition underlie the evolution of cooperation.\nThe lower cost of information dissemination and the more valid cognition of\ninformation, the higher level of cooperation. Moreover, deviations in cognition\namong individuals more sensitively determine the equilibrium cooperation\ndensity. As the deviations increase, cooperation density decreases\nsignificantly. This paper provides a new explanation for the diversity of\ncooperation among populations with different information dissemination costs\nand cognition competence.",
    "descriptor": "",
    "authors": [
      "Xiaoming Gong"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.00788"
  },
  {
    "id": "arXiv:2206.00789",
    "title": "Integrating Unikernel Optimizations in a General Purpose OS",
    "abstract": "We explore if unikernel techniques can be integrated into a general-purpose\nOS while preserving its battle-tested code, development community, and\necosystem of tools, applications, and hardware support. Our prototype\ndemonstrates both a path to integrate unikernel techniques in Linux and that\nsuch techniques can result in significant performance advantages. With a\nre-compilation and link to our modified kernel, applications show modest\nperformance gains. Expert developers can optimize the application to call\ninternal kernel functionality and optimize across the application/kernel\nboundary for more significant gains. While only one process can be optimized,\nstandard scripts can be used to launch it, and other processes can run\nalongside it, enabling the use of standard user-level tools (prof, bash,...)\nand support for both virtual and physical servers. The changes to the Linux\nkernel are modest (1250 LOC) and largely part of a configuration build target.",
    "descriptor": "",
    "authors": [
      "Ali Raza",
      "Thomas Unger",
      "Matthew Boyd",
      "Eric Munson",
      "Parul Sohal",
      "Ulrich Drepper",
      "Richard Jones",
      "Daniel Bristot de Oliveira",
      "Larry Woodman",
      "Renato Mancuso",
      "Jonathan Appavoo",
      "Orran Krieger"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)"
    ],
    "url": "https://arxiv.org/abs/2206.00789"
  },
  {
    "id": "arXiv:2206.00790",
    "title": "Efficient Self-supervised Vision Pretraining with Local Masked  Reconstruction",
    "abstract": "Self-supervised learning for computer vision has achieved tremendous progress\nand improved many downstream vision tasks such as image classification,\nsemantic segmentation, and object detection. Among these, generative\nself-supervised vision learning approaches such as MAE and BEiT show promising\nperformance. However, their global masked reconstruction mechanism is\ncomputationally demanding. To address this issue, we propose local masked\nreconstruction (LoMaR), a simple yet effective approach that performs masked\nreconstruction within a small window of 7$\\times$7 patches on a simple\nTransformer encoder, improving the trade-off between efficiency and accuracy\ncompared to global masked reconstruction over the entire image. Extensive\nexperiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K\nclassification, outperforming MAE by 0.5%. After finetuning the pretrained\nLoMaR on 384$\\times$384 images, it can reach 85.4% top-1 accuracy, surpassing\nMAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\\text{AP}^\\text{box}$ on\nobject detection and 0.5 $\\text{AP}^\\text{mask}$ on instance segmentation.\nLoMaR is especially more computation-efficient on pretraining high-resolution\nimages, e.g., it is 3.1$\\times$ faster than MAE with 0.2% higher classification\naccuracy on pretraining 448$\\times$448 images. This local masked reconstruction\nlearning mechanism can be easily integrated into any other generative\nself-supervised learning approach. Our code will be publicly available.",
    "descriptor": "",
    "authors": [
      "Jun Chen",
      "Ming Hu",
      "Boyang Li",
      "Mohamed Elhoseiny"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00790"
  },
  {
    "id": "arXiv:2206.00792",
    "title": "Channel Codes for Relayless Networks with General Message Access  Structure",
    "abstract": "Channel codes for relayless networks with the general message access\nstructure is introduced. It is shown that the multi-letter characterized\ncapacity region of this network is achievable with this code. The capacity\nregion is characterized in terms of entropy functions and provides an\nalternative to the regions introduced by [Somekh-Baruch and Verd\\'u,\nISIT2006][Muramatsu and Miyake, ISITA2018].",
    "descriptor": "\nComments: 26 pages, The author is planning to submit this paper to 2023 IEEE International Symposium on Information Theory (ISIT2023)\n",
    "authors": [
      "Jun Muramatsu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00792"
  },
  {
    "id": "arXiv:2206.00795",
    "title": "Adaptive Sampling-based Motion Planning with Control Barrier Functions",
    "abstract": "Sampling-based algorithms, such as Rapidly Exploring Random Trees (RRT) and\nits variants, have been used extensively for motion planning. Control barrier\nfunctions (CBFs) have been recently proposed to synthesize controllers for\nsafety-critical systems. In this paper, we combine the effectiveness of\nRRT-based algorithms with the safety guarantees provided by CBFs in a method\ncalled CBF-RRT$^\\ast$. CBFs are used for local trajectory planning for\nRRT$^\\ast$, avoiding explicit collision checking of the extended paths. We\nprove that CBF-RRT$^\\ast$ preserves the probabilistic completeness of\nRRT$^\\ast$. Furthermore, in order to improve the sampling efficiency of the\nalgorithm, we equip the algorithm with an adaptive sampling procedure, which is\nbased on the cross-entropy method (CEM) for importance sampling (IS). The\nprocedure exploits the tree of samples to focus the sampling in promising\nregions of the configuration space. We demonstrate the efficacy of the proposed\nalgorithms through simulation examples.",
    "descriptor": "\nComments: Submitted to CDC2022\n",
    "authors": [
      "Ahmad Ahmad",
      "Calin Belta",
      "Roberto Tron"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00795"
  },
  {
    "id": "arXiv:2206.00796",
    "title": "Stabilizing Q-learning with Linear Architectures for Provably Efficient  Learning",
    "abstract": "The $Q$-learning algorithm is a simple and widely-used stochastic\napproximation scheme for reinforcement learning, but the basic protocol can\nexhibit instability in conjunction with function approximation. Such\ninstability can be observed even with linear function approximation. In\npractice, tools such as target networks and experience replay appear to be\nessential, but the individual contribution of each of these mechanisms is not\nwell understood theoretically. This work proposes an exploration variant of the\nbasic $Q$-learning protocol with linear function approximation. Our modular\nanalysis illustrates the role played by each algorithmic tool that we adopt: a\nsecond order update rule, a set of target networks, and a mechanism akin to\nexperience replay. Together, they enable state of the art regret bounds on\nlinear MDPs while preserving the most prominent feature of the algorithm,\nnamely a space complexity independent of the number of step elapsed. We show\nthat the performance of the algorithm degrades very gracefully under a novel\nand more permissive notion of approximation error. The algorithm also exhibits\na form of instance-dependence, in that its performance depends on the\n\"effective\" feature dimension.",
    "descriptor": "\nComments: Appears in ICML 2022\n",
    "authors": [
      "Andrea Zanette",
      "Martin J. Wainwright"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00796"
  },
  {
    "id": "arXiv:2206.00798",
    "title": "Multi-scale frequency separation network for image deblurring",
    "abstract": "Image deblurring aims to restore the detailed texture information or\nstructures from the blurry images, which has become an indispensable step in\nmany computer-vision tasks. Although various methods have been proposed to deal\nwith the image deblurring problem, most of them treated the blurry image as a\nwhole and neglected the characteristics of different image frequencies. In this\npaper, we present a new method called multi-scale frequency separation network\n(MSFS-Net) for image deblurring. MSFS-Net introduces the frequency separation\nmodule (FSM) into an encoder-decoder network architecture to capture the low\nand high-frequency information of image at multiple scales. Then, a simple\ncycle-consistency strategy and a sophisticated contrastive learning module\n(CLM) are respectively designed to retain the low-frequency information and\nrecover the high-frequency information during deblurring. At last, the features\nof different scales are fused by a cross-scale feature fusion module (CSFFM).\nExtensive experiments on benchmark datasets show that the proposed network\nachieves state-of-the-art performance.",
    "descriptor": "",
    "authors": [
      "Yanni Zhang",
      "Qiang Li",
      "Miao Qi",
      "Di Liu",
      "Jun Kong",
      "Jianzhong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00798"
  },
  {
    "id": "arXiv:2206.00799",
    "title": "Federated Learning under Distributed Concept Drift",
    "abstract": "Federated Learning (FL) under distributed concept drift is a largely\nunexplored area. Although concept drift is itself a well-studied phenomenon, it\nposes particular challenges for FL, because drifts arise staggered in time and\nspace (across clients). Our work is the first to explicitly study data\nheterogeneity in both dimensions. We first demonstrate that prior solutions to\ndrift adaptation, with their single global model, are ill-suited to staggered\ndrifts, necessitating multi-model solutions. We identify the problem of drift\nadaptation as a time-varying clustering problem, and we propose two new\nclustering algorithms for reacting to drifts based on local drift detection and\nhierarchical clustering. Empirical evaluation shows that our solutions achieve\nsignificantly higher accuracy than existing baselines, and are comparable to an\nidealized algorithm with oracle knowledge of the ground-truth clustering of\nclients to concepts at each time step.",
    "descriptor": "\nComments: 18 pages. Submitted to NeurIPS 2022\n",
    "authors": [
      "Ellango Jothimurugesan",
      "Kevin Hsieh",
      "Jianyu Wang",
      "Gauri Joshi",
      "Phillip B. Gibbons"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00799"
  },
  {
    "id": "arXiv:2206.00800",
    "title": "CcHarmony: Color-checker based Image Harmonization Dataset",
    "abstract": "Image harmonization targets at adjusting the foreground in a composite image\nto make it compatible with the background, producing a more realistic and\nharmonious image. Training deep image harmonization network requires abundant\ntraining data, but it is extremely difficult to acquire training pairs of\ncomposite images and ground-truth harmonious images. Therefore, existing works\nturn to adjust the foreground appearance in a real image to create a synthetic\ncomposite image. However, such adjustment may not faithfully reflect the\nnatural illumination change of foreground. In this work, we explore a novel\ntransitive way to construct image harmonization dataset. Specifically, based on\nthe existing datasets with recorded illumination information, we first convert\nthe foreground in a real image to the standard illumination condition, and then\nconvert it to another illumination condition, which is combined with the\noriginal background to form a synthetic composite image. In this manner, we\nconstruct an image harmonization dataset called ccHarmony, which is named after\ncolor checker (cc). The dataset is available at\nhttps://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.",
    "descriptor": "",
    "authors": [
      "Haoxu Huang",
      "Li Niu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00800"
  },
  {
    "id": "arXiv:2206.00803",
    "title": "Robust recovery of low-rank matrices and low-tubal-rank tensors from  noisy sketches",
    "abstract": "A common approach for compressing large-scale data is through matrix\nsketching. In this work, we consider the problem of recovering low-rank\nmatrices from two noisy sketches using the double sketching algorithm discussed\nin Fazel et al. (2008). Using tools from non-asymptotic random matrix theory,\nwe provide the first theoretical guarantees characterizing the error between\nthe output of the double sketch algorithm and the ground truth low-rank matrix.\nWe apply our result to the problems of low-rank matrix approximation and\nlow-tubal-rank tensor recovery.",
    "descriptor": "\nComments: 16 pages, 2 figures\n",
    "authors": [
      "Anna Ma",
      "Dominik St\u00f6ger",
      "Yizhe Zhu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2206.00803"
  },
  {
    "id": "arXiv:2206.00804",
    "title": "Learning code summarization from a small and local dataset",
    "abstract": "Foundation models (e.g., CodeBERT, GraphCodeBERT, CodeT5) work well for many\nsoftware engineering tasks. These models are pre-trained (using\nself-supervision) with billions of code tokens, and then fine-tuned with\nhundreds of thousands of labeled examples, typically drawn from many projects.\nHowever, software phenomena can be very project-specific. Vocabulary, and other\nphenomena vary substantially with each project. Thus, training on\nproject-specific data, and testing on the same project, is a promising idea.\nThis hypothesis has to be evaluated carefully, e.g., in a time-series setting,\nto prevent training-test leakage. We compare several models and training\napproaches, including same-project training, cross-project training, training a\nmodel especially designed to be sample efficient (and thus prima facie\nwell-suited for learning in a limited-sample same-project setting) and a\nmaximalist hybrid approach, fine-tuning first on many projects in many\nlanguages and then training on the same-project. We find that the maximalist\nhybrid setting provides consistent, substantial gains over the\nstate-of-the-art, on many different projects in both Java and Python.",
    "descriptor": "",
    "authors": [
      "Toufique Ahmed",
      "Premkumar Devanbu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00804"
  },
  {
    "id": "arXiv:2206.00806",
    "title": "XBound-Former: Toward Cross-scale Boundary Modeling in Transformers",
    "abstract": "Skin lesion segmentation from dermoscopy images is of great significance in\nthe quantitative analysis of skin cancers, which is yet challenging even for\ndermatologists due to the inherent issues, i.e., considerable size, shape and\ncolor variation, and ambiguous boundaries. Recent vision transformers have\nshown promising performance in handling the variation through global context\nmodeling. Still, they have not thoroughly solved the problem of ambiguous\nboundaries as they ignore the complementary usage of the boundary knowledge and\nglobal contexts. In this paper, we propose a novel cross-scale boundary-aware\ntransformer, \\textbf{XBound-Former}, to simultaneously address the variation\nand boundary problems of skin lesion segmentation. XBound-Former is a purely\nattention-based network and catches boundary knowledge via three specially\ndesigned learners. We evaluate the model on two skin lesion datasets,\nISIC-2016\\&PH$^2$ and ISIC-2018, where our model consistently outperforms other\nconvolution- and transformer-based models, especially on the boundary-wise\nmetrics. We extensively verify the generalization ability of polyp lesion\nsegmentation that has similar characteristics, and our model can also yield\nsignificant improvement compared to the latest models.",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Jiacheng Wang",
      "Fei Chen",
      "Yuxi Ma",
      "Liansheng Wang",
      "Zhaodong Fei",
      "Jianwei Shuai",
      "Xiangdong Tang",
      "Qichao Zhou",
      "Jing Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00806"
  },
  {
    "id": "arXiv:2206.00807",
    "title": "Applied Federated Learning: Architectural Design for Robust and  Efficient Learning in Privacy Aware Settings",
    "abstract": "The classical machine learning paradigm requires the aggregation of user data\nin a central location where machine learning practitioners can preprocess data,\ncalculate features, tune models and evaluate performance. The advantage of this\napproach includes leveraging high performance hardware (such as GPUs) and the\nability of machine learning practitioners to do in depth data analysis to\nimprove model performance. However, these advantages may come at a cost to data\nprivacy. User data is collected, aggregated, and stored on centralized servers\nfor model development. Centralization of data poses risks, including a\nheightened risk of internal and external security incidents as well as\naccidental data misuse. Federated learning with differential privacy is\ndesigned to avoid the server-side centralization pitfall by bringing the ML\nlearning step to users' devices. Learning is done in a federated manner where\neach mobile device runs a training loop on a local copy of a model. Updates\nfrom on-device models are sent to the server via encrypted communication and\nthrough differential privacy to improve the global model. In this paradigm,\nusers' personal data remains on their devices. Surprisingly, model training in\nthis manner comes at a fairly minimal degradation in model performance.\nHowever, federated learning comes with many other challenges due to its\ndistributed nature, heterogeneous compute environments and lack of data\nvisibility. This paper explores those challenges and outlines an architectural\ndesign solution we are exploring and testing to productionize federated\nlearning at Meta scale.",
    "descriptor": "",
    "authors": [
      "Branislav Stojkovic",
      "Jonathan Woodbridge",
      "Zhihan Fang",
      "Jerry Cai",
      "Andrey Petrov",
      "Sathya Iyer",
      "Daoyu Huang",
      "Patrick Yau",
      "Arvind Sastha Kumar",
      "Hitesh Jawa",
      "Anamita Guha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00807"
  },
  {
    "id": "arXiv:2206.00809",
    "title": "Distilling Knowledge from Object Classification to Aesthetics Assessment",
    "abstract": "In this work, we point out that the major dilemma of image aesthetics\nassessment (IAA) comes from the abstract nature of aesthetic labels. That is, a\nvast variety of distinct contents can correspond to the same aesthetic label.\nOn the one hand, during inference, the IAA model is required to relate various\ndistinct contents to the same aesthetic label. On the other hand, when\ntraining, it would be hard for the IAA model to learn to distinguish different\ncontents merely with the supervision from aesthetic labels, since aesthetic\nlabels are not directly related to any specific content. To deal with this\ndilemma, we propose to distill knowledge on semantic patterns for a vast\nvariety of image contents from multiple pre-trained object classification (POC)\nmodels to an IAA model. Expecting the combination of multiple POC models can\nprovide sufficient knowledge on various image contents, the IAA model can\neasier learn to relate various distinct contents to a limited number of\naesthetic labels. By supervising an end-to-end single-backbone IAA model with\nthe distilled knowledge, the performance of the IAA model is significantly\nimproved by 4.8% in SRCC compared to the version trained only with ground-truth\naesthetic labels. On specific categories of images, the SRCC improvement\nbrought by the proposed method can achieve up to 7.2%. Peer comparison also\nshows that our method outperforms 10 previous IAA methods.",
    "descriptor": "",
    "authors": [
      "Jingwen Hou",
      "Henghui Ding",
      "Weisi Lin",
      "Weide Liu",
      "Yuming Fang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00809"
  },
  {
    "id": "arXiv:2206.00810",
    "title": "Offline Reinforcement Learning with Differential Privacy",
    "abstract": "The offline reinforcement learning (RL) problem is often motivated by the\nneed to learn data-driven decision policies in financial, legal and healthcare\napplications. However, the learned policy could retain sensitive information of\nindividuals in the training data (e.g., treatment and outcome of patients),\nthus susceptible to various privacy risks. We design offline RL algorithms with\ndifferential privacy guarantees which provably prevent such risks. These\nalgorithms also enjoy strong instance-dependent learning bounds under both\ntabular and linear Markov decision process (MDP) settings. Our theory and\nsimulation suggest that the privacy guarantee comes at (almost) no drop in\nutility comparing to the non-private counterpart for a medium-size dataset.",
    "descriptor": "\nComments: 50 pages\n",
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00810"
  },
  {
    "id": "arXiv:2206.00812",
    "title": "Modeling sRGB Camera Noise with Normalizing Flows",
    "abstract": "Noise modeling and reduction are fundamental tasks in low-level computer\nvision. They are particularly important for smartphone cameras relying on small\nsensors that exhibit visually noticeable noise. There has recently been renewed\ninterest in using data-driven approaches to improve camera noise models via\nneural networks. These data-driven approaches target noise present in the\nraw-sensor image before it has been processed by the camera's image signal\nprocessor (ISP). Modeling noise in the RAW-rgb domain is useful for improving\nand testing the in-camera denoising algorithm; however, there are situations\nwhere the camera's ISP does not apply denoising or additional denoising is\ndesired when the RAW-rgb domain image is no longer available. In such cases,\nthe sensor noise propagates through the ISP to the final rendered image encoded\nin standard RGB (sRGB). The nonlinear steps on the ISP culminate in a\nsignificantly more complex noise distribution in the sRGB domain and existing\nraw-domain noise models are unable to capture the sRGB noise distribution. We\npropose a new sRGB-domain noise model based on normalizing flows that is\ncapable of learning the complex noise distribution found in sRGB images under\nvarious ISO levels. Our normalizing flows-based approach outperforms other\nmodels by a large margin in noise modeling and synthesis tasks. We also show\nthat image denoisers trained on noisy images synthesized with our noise model\noutperforms those trained with noise from baselines models.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Shayan Kousha",
      "Ali Maleky",
      "Michael S. Brown",
      "Marcus A. Brubaker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.00812"
  },
  {
    "id": "arXiv:2206.00820",
    "title": "NIPQ: Noise Injection Pseudo Quantization for Automated DNN Optimization",
    "abstract": "The optimization of neural networks in terms of computation cost and memory\nfootprint is crucial for their practical deployment on edge devices. In this\nwork, we propose a novel quantization-aware training (QAT) scheme called noise\ninjection pseudo quantization (NIPQ). NIPQ is implemented based on pseudo\nquantization noise (PQN) and has several advantages. First, both activation and\nweight can be quantized based on a unified framework. Second, the\nhyper-parameters of quantization (e.g., layer-wise bit-width and quantization\ninterval) are automatically tuned. Third, after QAT, the network has robustness\nagainst quantization, thereby making it easier to deploy in practice. To\nvalidate the superiority of the proposed algorithm, we provide extensive\nanalysis and conduct diverse experiments for various vision applications. Our\ncomprehensive experiments validate the outstanding performance of the proposed\nalgorithm in several aspects.",
    "descriptor": "",
    "authors": [
      "Sein Park",
      "Junhyuk So",
      "Juncheol Shin",
      "Eunhyeok Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00820"
  },
  {
    "id": "arXiv:2206.00823",
    "title": "Beyond accuracy: generalization properties of bio-plausible temporal  credit assignment rules",
    "abstract": "To unveil how the brain learns, ongoing work seeks biologically-plausible\napproximations of gradient descent algorithms for training recurrent neural\nnetworks (RNNs). Yet, beyond task accuracy, it is unclear if such learning\nrules converge to solutions that exhibit different levels of generalization\nthan their nonbiologically-plausible counterparts. Leveraging results from deep\nlearning theory based on loss landscape curvature, we ask: how do\nbiologically-plausible gradient approximations affect generalization? We first\ndemonstrate that state-of-the-art biologically-plausible learning rules for\ntraining RNNs exhibit worse and more variable generalization performance\ncompared to their machine learning counterparts that follow the true gradient\nmore closely. Next, we verify that such generalization performance is\ncorrelated significantly with loss landscape curvature, and we show that\nbiologically-plausible learning rules tend to approach high-curvature regions\nin synaptic weight space. Using tools from dynamical systems, we derive\ntheoretical arguments and present a theorem explaining this phenomenon. This\npredicts our numerical results, and explains why biologically-plausible rules\nlead to worse and more variable generalization properties. Finally, we suggest\npotential remedies that could be used by the brain to mitigate this effect. To\nour knowledge, our analysis is the first to identify the reason for this\ngeneralization gap between artificial and biologically-plausible learning\nrules, which can help guide future investigations into how the brain learns\nsolutions that generalize.",
    "descriptor": "",
    "authors": [
      "Yuhan Helena Liu",
      "Arna Ghosh",
      "Blake A. Richards",
      "Eric Shea-Brown",
      "Guillaume Lajoie"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2206.00823"
  },
  {
    "id": "arXiv:2206.00825",
    "title": "Nonconformal Domain Decomposition Method Based on the Hybrid SIE-PDE  Formulation for Flexible Transverse Magnetic Analysis",
    "abstract": "A nonconformal domain decomposition method based on the hybrid surface\nintegral equation partial differential equation (SIE-PDE) formulation is\nproposed to solve the transverse magnetic electromagnetic problems. In the\nhybrid SIE-PDE formulation, an equivalent model with only the electric current\ndensity is first constructed, and then is embedded into the inhomogeneous\nHelmholtz equation as an excitation. A connection matrix, which couples the\ninterfaces of the SIE and PDE domains, is carefully designed to support\nnonconformal meshes. Since meshes in each domain are independently generated,\nit is much more efficient and flexible to model multiscale and complex\nstructures compared with the original hybrid SIE-PDE formulation with conformal\nmeshes. The proposed formulation is efficient, flexible and easy to implement.\nIts accuracy, efficiency and flexibility are validated by three numerical\nexamples.",
    "descriptor": "",
    "authors": [
      "Aipeng Sun",
      "Zekun Zhu",
      "Shunchuan Yang",
      "Zhizhang Chen"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.00825"
  },
  {
    "id": "arXiv:2206.00826",
    "title": "BayesFormer: Transformer with Uncertainty Estimation",
    "abstract": "Transformer has become ubiquitous due to its dominant performance in various\nNLP and image processing tasks. However, it lacks understanding of how to\ngenerate mathematically grounded uncertainty estimates for transformer\narchitectures. Models equipped with such uncertainty estimates can typically\nimprove predictive performance, make networks robust, avoid over-fitting and\nused as acquisition function in active learning. In this paper, we introduce\nBayesFormer, a Transformer model with dropouts designed by Bayesian theory. We\nproposed a new theoretical framework to extend the approximate variational\ninference-based dropout to Transformer-based architectures. Through extensive\nexperiments, we validate the proposed architecture in four paradigms and show\nimprovements across the board: language modeling and classification,\nlong-sequence understanding, machine translation and acquisition function for\nactive learning.",
    "descriptor": "",
    "authors": [
      "Karthik Abinav Sankararaman",
      "Sinong Wang",
      "Han Fang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00826"
  },
  {
    "id": "arXiv:2206.00827",
    "title": "Universal Polar Coding for Parallel Gaussian Channels with Non-Binary  Inputs and Its Applications to HARQ and MIMO",
    "abstract": "In this paper, we first propose an universal polar coding scheme for parallel\nGaussian channels with non-binary inputs. It is assumed that the encoder knows\nonly the sum capacity of M parallel channels instead of the capacity of any\nsingle channel. By decomposing each parallel channel into T = [log2r] sub\nchannels, we therefore obtain MT binary sub-channels. A super polar coding\nscheme that across all sub-channels is then proposed. This scheme can achieve\nthe sum capacity when the block length is sufficiently large. We have also\ndiscussed the applications of parallel polar coding design for both the HARQ\nand MIMO systems. It is shown that a capacity-achieving HARQ scheme can be\nobtained for block fading channel and a capacity-achieving MIMO design that\nrequires only the feedback of the sum rate of all MIMO layers can also be\nattained.",
    "descriptor": "",
    "authors": [
      "Bin Li",
      "Jiaqi Gu",
      "Huazi Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00827"
  },
  {
    "id": "arXiv:2206.00828",
    "title": "Decentralized temperature and storage volume control in multi-producer  district heating",
    "abstract": "Modern district heating technologies have a great potential to make the\nenergy sector more flexible and sustainable due to their capabilities to use\nenergy sources of varied nature and to efficiently store energy for subsequent\nuse. Central control tasks within these systems for the efficient and safe\ndistribution of heat refer to the stabilization of overall system temperatures\nand the regulation of storage units state of charge. These are challenging\ngoals when the networked and nonlinear nature of district heating system models\nis taken into consideration. In this letter, for district heating systems with\nmultiple, distributed heat producers, we propose a decentralized control scheme\nto provably meet said tasks stably.",
    "descriptor": "",
    "authors": [
      "Juan E. Machado",
      "Joel Ferguson",
      "Michele Cucuzzella",
      "Jacquelien M.A. Scherpen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00828"
  },
  {
    "id": "arXiv:2206.00830",
    "title": "Progressive Purification for Instance-Dependent Partial Label Learning",
    "abstract": "Partial label learning (PLL) aims to train multi-class classifiers from\ninstances with partial labels (PLs)-a PL for an instance is a set of candidate\nlabels where a fixed but unknown candidate is the true label. In the last few\nyears, the instance-independent generation process of PLs has been extensively\nstudied, on the basis of which many practical and theoretical advances have\nbeen made in PLL, whereas relatively less attention has been paid to the\npractical setting of instance-dependent PLs, namely, the PL depends not only on\nthe true label but the instance itself. In this paper, we propose a\ntheoretically grounded and practically effective approach called PrOgressive\nPurification (POP) for instance-dependent PLL: in each epoch, POP updates the\nlearning model while purifying each PL for the next epoch of the model training\nby progressively moving out false candidate labels. Theoretically, we prove\nthat POP enlarges the region appropriately fast where the model is reliable,\nand eventually approximates the Bayes optimal classifier with mild assumptions;\ntechnically, POP is flexible with arbitrary losses and compatible with deep\nnetworks, so that the previous advanced PLL losses can be embedded in it and\nthe performance is often significantly improved.",
    "descriptor": "",
    "authors": [
      "Ning Xu",
      "Jiaqi Lv",
      "Biao Liu",
      "Congyu Qiao",
      "Xin Geng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00830"
  },
  {
    "id": "arXiv:2206.00832",
    "title": "Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning  Rates",
    "abstract": "Benchmarking the tradeoff between neural network accuracy and training time\nis computationally expensive. Here we show how a multiplicative cyclic learning\nrate schedule can be used to construct a tradeoff curve in a single training\nrun. We generate cyclic tradeoff curves for combinations of training methods\nsuch as Blurpool, Channels Last, Label Smoothing and MixUp, and highlight how\nthese cyclic tradeoff curves can be used to evaluate the effects of algorithmic\nchoices on network training efficiency.",
    "descriptor": "\nComments: 8 pages, 7 figures\n",
    "authors": [
      "Jacob Portes",
      "Davis Blalock",
      "Cory Stephenson",
      "Jonathan Frankle"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00832"
  },
  {
    "id": "arXiv:2206.00833",
    "title": "Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic  Algorithm",
    "abstract": "Natural actor-critic (NAC) and its variants, equipped with the representation\npower of neural networks, have demonstrated impressive empirical success in\nsolving Markov decision problems with large state spaces. In this paper, we\npresent a finite-time analysis of NAC with neural network approximation, and\nidentify the roles of neural networks, regularization and optimization\ntechniques (e.g., gradient clipping and averaging) to achieve provably good\nperformance in terms of sample complexity, iteration complexity and\noverparametrization bounds for the actor and the critic. In particular, we\nprove that (i) entropy regularization and averaging ensure stability by\nproviding sufficient exploration to avoid near-deterministic and strictly\nsuboptimal policies and (ii) regularization leads to sharp sample complexity\nand network width bounds in the regularized MDPs, yielding a favorable\nbias-variance tradeoff in policy optimization. In the process, we identify the\nimportance of uniform approximation power of the actor neural network to\nachieve global optimality in policy optimization due to distributional shift.",
    "descriptor": "",
    "authors": [
      "Semih Cayci",
      "Niao He",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00833"
  },
  {
    "id": "arXiv:2206.00835",
    "title": "Reinforcement learning based parameters adaption method for particle  swarm optimization",
    "abstract": "Particle swarm optimization (PSO) is a well-known optimization algorithm that\nshows good performance in solving different optimization problems. However, PSO\nusually suffers from slow convergence. In this article, a reinforcement\nlearning-based online parameters adaption method(RLAM) is developed to enhance\nPSO in convergence by designing a network to control the coefficients of PSO.\nMoreover, based on RLAM, a new RLPSO is designed.\nIn order to investigate the performance of RLAM and RLPSO, experiments on 28\nCEC 2013 benchmark functions are carried out when comparing with other online\nadaption method and PSO variants. The reported computational results show that\nthe proposed RLAM is efficient and effictive and that the the proposed RLPSO is\nmore superior compared with several state-of-the-art PSO variants.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Yin ShiYuan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.00835"
  },
  {
    "id": "arXiv:2206.00838",
    "title": "Bi-convolution matrix factorization algorithm based on improved ConvMF",
    "abstract": "With the rapid development of information technology, \"information overload\"\nhas become the main theme that plagues people's online life. As an effective\ntool to help users quickly search for useful information, a personalized\nrecommendation is more and more popular among people. In order to solve the\nsparsity problem of the traditional matrix factorization algorithm and the\nproblem of low utilization of review document information, this paper proposes\na Bicon-vMF algorithm based on improved ConvMF. This algorithm uses two\nparallel convolutional neural networks to extract deep features from the user\nreview set and item review set respectively and fuses these features into the\ndecomposition of the rating matrix, so as to construct the user latent model\nand the item latent model more accurately. The experimental results show that\ncompared with traditional recommendation algorithms like PMF, ConvMF, and\nDeepCoNN, the method proposed in this paper has lower prediction error and can\nachieve a better recommendation effect. Specifically, compared with the\nprevious three algorithms, the prediction errors of the algorithm proposed in\nthis paper are reduced by 45.8%, 16.6%, and 34.9%, respectively.",
    "descriptor": "",
    "authors": [
      "Peiyu Liu",
      "Junping Du",
      "Zhe Xue",
      "Ang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.00838"
  },
  {
    "id": "arXiv:2206.00843",
    "title": "DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware  Efficiency of Compact Neural Networks",
    "abstract": "Efficient deep neural network (DNN) models equipped with compact operators\n(e.g., depthwise convolutions) have shown great potential in reducing DNNs'\ntheoretical complexity (e.g., the total number of weights/operations) while\nmaintaining a decent model accuracy. However, existing efficient DNNs are still\nlimited in fulfilling their promise in boosting real-hardware efficiency, due\nto their commonly adopted compact operators' low hardware utilization. In this\nwork, we open up a new compression paradigm for developing real-hardware\nefficient DNNs, leading to boosted hardware efficiency while maintaining model\naccuracy. Interestingly, we observe that while some DNN layers' activation\nfunctions help DNNs' training optimization and achievable accuracy, they can be\nproperly removed after training without compromising the model accuracy.\nInspired by this observation, we propose a framework dubbed DepthShrinker,\nwhich develops hardware-friendly compact networks via shrinking the basic\nbuilding blocks of existing efficient DNNs that feature irregular computation\npatterns into dense ones with much improved hardware utilization and thus\nreal-hardware efficiency. Excitingly, our DepthShrinker framework delivers\nhardware-friendly compact networks that outperform both state-of-the-art\nefficient DNNs and compression techniques, e.g., a 3.06\\% higher accuracy and\n1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method\nMetaPruning. Our codes are available at:\nhttps://github.com/RICE-EIC/DepthShrinker.",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Yonggan Fu",
      "Haichuan Yang",
      "Jiayi Yuan",
      "Meng Li",
      "Cheng Wan",
      "Raghuraman Krishnamoorthi",
      "Vikas Chandra",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00843"
  },
  {
    "id": "arXiv:2206.00845",
    "title": "Hyperspherical Consistency Regularization",
    "abstract": "Recent advances in contrastive learning have enlightened diverse applications\nacross various semi-supervised fields. Jointly training supervised learning and\nunsupervised learning with a shared feature encoder becomes a common scheme.\nThough it benefits from taking advantage of both feature-dependent information\nfrom self-supervised learning and label-dependent information from supervised\nlearning, this scheme remains suffering from bias of the classifier. In this\nwork, we systematically explore the relationship between self-supervised\nlearning and supervised learning, and study how self-supervised learning helps\nrobust data-efficient deep learning. We propose hyperspherical consistency\nregularization (HCR), a simple yet effective plug-and-play method, to\nregularize the classifier using feature-dependent information and thus avoid\nbias from labels. Specifically, HCR first projects logits from the classifier\nand feature projections from the projection head on the respective hypersphere,\nthen it enforces data points on hyperspheres to have similar structures by\nminimizing binary cross entropy of pairwise distances' similarity metrics.\nExtensive experiments on semi-supervised and weakly-supervised learning\ndemonstrate the effectiveness of our method, by showing superior performance\nwith HCR.",
    "descriptor": "\nComments: Accepted by CVPR 2022\n",
    "authors": [
      "Cheng Tan",
      "Zhangyang Gao",
      "Lirong Wu",
      "Siyuan Li",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00845"
  },
  {
    "id": "arXiv:2206.00846",
    "title": "Faster Rates of Convergence to Stationary Points in Differentially  Private Optimization",
    "abstract": "We study the problem of approximating stationary points of Lipschitz and\nsmooth functions under $(\\varepsilon,\\delta)$-differential privacy (DP) in both\nthe finite-sum and stochastic settings. A point $\\widehat{w}$ is called an\n$\\alpha$-stationary point of a function $F:\\mathbb{R}^d\\rightarrow\\mathbb{R}$\nif $\\|\\nabla F(\\widehat{w})\\|\\leq \\alpha$. We provide a new efficient algorithm\nthat finds an\n$\\tilde{O}\\big(\\big[\\frac{\\sqrt{d}}{n\\varepsilon}\\big]^{2/3}\\big)$-stationary\npoint in the finite-sum setting, where $n$ is the number of samples. This\nimproves on the previous best rate of\n$\\tilde{O}\\big(\\big[\\frac{\\sqrt{d}}{n\\varepsilon}\\big]^{1/2}\\big)$. We also\ngive a new construction that improves over the existing rates in the stochastic\noptimization setting, where the goal is to find approximate stationary points\nof the population risk. Our construction finds a\n$\\tilde{O}\\big(\\frac{1}{n^{1/3}} +\n\\big[\\frac{\\sqrt{d}}{n\\varepsilon}\\big]^{1/2}\\big)$-stationary point of the\npopulation risk in time linear in $n$. Furthermore, under the additional\nassumption of convexity, we completely characterize the sample complexity of\nfinding stationary points of the population risk (up to polylog factors) and\nshow that the optimal rate on population stationarity is $\\tilde\n\\Theta\\big(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d}}{n\\varepsilon}\\big)$. Finally, we\nshow that our methods can be used to provide dimension-independent rates of\n$O\\big(\\frac{1}{\\sqrt{n}}+\\min\\big(\\big[\\frac{\\sqrt{rank}}{n\\varepsilon}\\big]^{2/3},\\frac{1}{(n\\varepsilon)^{2/5}}\\big)\\big)$\non population stationarity for Generalized Linear Models (GLM), where $rank$ is\nthe rank of the design matrix, which improves upon the previous best known\nrate.",
    "descriptor": "",
    "authors": [
      "Raman Arora",
      "Raef Bassily",
      "Tom\u00e1s Gonz\u00e1lez",
      "Crist\u00f3bal Guzm\u00e1n",
      "Michael Menart",
      "Enayat Ullah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00846"
  },
  {
    "id": "arXiv:2206.00847",
    "title": "TSTR: Too Short to Represent, Summarize with Details! Intro-Guided  Extended Summary Generation",
    "abstract": "Many scientific papers such as those in arXiv and PubMed data collections\nhave abstracts with varying lengths of 50-1000 words and average length of\napproximately 200 words, where longer abstracts typically convey more\ninformation about the source paper. Up to recently, scientific summarization\nresearch has typically focused on generating short, abstract-like summaries\nfollowing the existing datasets used for scientific summarization. In domains\nwhere the source text is relatively long-form, such as in scientific documents,\nsuch summary is not able to go beyond the general and coarse overview and\nprovide salient information from the source document. The recent interest to\ntackle this problem motivated curation of scientific datasets, arXiv-Long and\nPubMed-Long, containing human-written summaries of 400-600 words, hence,\nproviding a venue for research in generating long/extended summaries. Extended\nsummaries facilitate a faster read while providing details beyond coarse\ninformation. In this paper, we propose TSTR, an extractive summarizer that\nutilizes the introductory information of documents as pointers to their salient\ninformation. The evaluations on two existing large-scale extended summarization\ndatasets indicate statistically significant improvement in terms of Rouge and\naverage Rouge (F1) scores (except in one case) as compared to strong baselines\nand state-of-the-art. Comprehensive human evaluations favor our generated\nextended summaries in terms of cohesion and completeness.",
    "descriptor": "\nComments: 9 pages, NAACL 2022 Long Paper\n",
    "authors": [
      "Sajad Sotudeh",
      "Nazli Goharian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.00847"
  },
  {
    "id": "arXiv:2206.00851",
    "title": "Finite Element Complexes in Two Dimensions",
    "abstract": "Two-dimensional finite element complexes with various smoothness, including\nthe de Rham complex, the curldiv complex, the elasticity complex, and the\ndivdiv complex, are systematically constructed in this work. First smooth\nscalar finite elements in two dimensions are developed based on a\nnon-overlapping decomposition of the simplicial lattice and the Bernstein basis\nof the polynomial space. Smoothness at vertices is more than doubled than that\nat edges. Then the finite element de Rham complexes with various smoothness are\ndevised using smooth finite elements with smoothness parameters satisfying\ncertain relations. Finally, finite element elasticity complexes and finite\nelement divdiv complexes are derived from finite element de Rham complexes by\nusing the Bernstein-Gelfand-Gelfand (BGG) framework. Additionally, some finite\nelement divdiv complexes are constructed without BGG framework. Dimension count\nplays an important role for verifying the exactness of two-dimensional finite\nelement complexes.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Long Chen",
      "Xuehai Huang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.00851"
  },
  {
    "id": "arXiv:2206.00856",
    "title": "MentSum: A Resource for Exploring Summarization of Mental Health Online  Posts",
    "abstract": "Mental health remains a significant challenge of public health worldwide.\nWith increasing popularity of online platforms, many use the platforms to share\ntheir mental health conditions, express their feelings, and seek help from the\ncommunity and counselors. Some of these platforms, such as Reachout, are\ndedicated forums where the users register to seek help. Others such as Reddit\nprovide subreddits where the users publicly but anonymously post their mental\nhealth distress. Although posts are of varying length, it is beneficial to\nprovide a short, but informative summary for fast processing by the counselors.\nTo facilitate research in summarization of mental health online posts, we\nintroduce Mental Health Summarization dataset, MentSum, containing over 24k\ncarefully selected user posts from Reddit, along with their short user-written\nsummary (called TLDR) in English from 43 mental health subreddits. This\ndomain-specific dataset could be of interest not only for generating short\nsummaries on Reddit, but also for generating summaries of posts on the\ndedicated mental health forums such as Reachout. We further evaluate both\nextractive and abstractive state-of-the-art summarization baselines in terms of\nRouge scores, and finally conduct an in-depth human evaluation study of both\nuser-written and system-generated summaries, highlighting challenges in this\nresearch.",
    "descriptor": "\nComments: 8 pages, LREC 2022 Long Paper\n",
    "authors": [
      "Sajad Sotudeh",
      "Nazli Goharian",
      "Zachary Young"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.00856"
  },
  {
    "id": "arXiv:2206.00859",
    "title": "Disentangled Generation Network for Enlarged License Plate Recognition  and A Unified Dataset",
    "abstract": "License plate recognition plays a critical role in many practical\napplications, but license plates of large vehicles are difficult to be\nrecognized due to the factors of low resolution, contamination, low\nillumination, and occlusion, to name a few. To overcome the above factors, the\ntransportation management department generally introduces the enlarged license\nplate behind the rear of a vehicle. However, enlarged license plates have high\ndiversity as they are non-standard in position, size, and style. Furthermore,\nthe background regions contain a variety of noisy information which greatly\ndisturbs the recognition of license plate characters. Existing works have not\nstudied this challenging problem. In this work, we first address the enlarged\nlicense plate recognition problem and contribute a dataset containing 9342\nimages, which cover most of the challenges of real scenes. However, the created\ndata are still insufficient to train deep methods of enlarged license plate\nrecognition, and building large-scale training data is very time-consuming and\nhigh labor cost. To handle this problem, we propose a novel task-level\ndisentanglement generation framework based on the Disentangled Generation\nNetwork (DGNet), which disentangles the generation into the text generation and\nbackground generation in an end-to-end manner to effectively ensure diversity\nand integrity, for robust enlarged license plate recognition. Extensive\nexperiments on the created dataset are conducted, and we demonstrate the\neffectiveness of the proposed approach in three representative text recognition\nframeworks.",
    "descriptor": "\nComments: Submission to TIP\n",
    "authors": [
      "Chenglong Li",
      "Xiaobin Yang",
      "Guohao Wang",
      "Aihua Zheng",
      "Chang Tan",
      "Ruoran Jia",
      "Jin Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00859"
  },
  {
    "id": "arXiv:2206.00860",
    "title": "Self-Consistency of the Fokker-Planck Equation",
    "abstract": "The Fokker-Planck equation (FPE) is the partial differential equation that\ngoverns the density evolution of the It\\^o process and is of great importance\nto the literature of statistical physics and machine learning. The FPE can be\nregarded as a continuity equation where the change of the density is completely\ndetermined by a time varying velocity field. Importantly, this velocity field\nalso depends on the current density function. As a result, the ground-truth\nvelocity field can be shown to be the solution of a fixed-point equation, a\nproperty that we call self-consistency. In this paper, we exploit this concept\nto design a potential function of the hypothesis velocity fields, and prove\nthat, if such a function diminishes to zero during the training procedure, the\ntrajectory of the densities generated by the hypothesis velocity fields\nconverges to the solution of the FPE in the Wasserstein-2 sense. The proposed\npotential function is amenable to neural-network based parameterization as the\nstochastic gradient with respect to the parameter can be efficiently computed.\nOnce a parameterized model, such as Neural Ordinary Differential Equation is\ntrained, we can generate the entire trajectory to the FPE.",
    "descriptor": "\nComments: Accepted to COLT 2022. The code can be found at this https URL\n",
    "authors": [
      "Zebang Shen",
      "Zhenfu Wang",
      "Satyen Kale",
      "Alejandro Ribeiro",
      "Aim Karbasi",
      "Hamed Hassani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00860"
  },
  {
    "id": "arXiv:2206.00861",
    "title": "Dynamic Structure Estimation from Bandit Feedback",
    "abstract": "This work present novel method for structure estimation of an underlying\ndynamical system. We tackle problems of estimating dynamic structure from\nbandit feedback contaminated by sub-Gaussian noise. In particular, we focus on\nperiodically behaved discrete dynamical system in the Euclidean space, and\ncarefully identify certain obtainable subset of full information of the\nperiodic structure. We then derive a sample complexity bound for periodic\nstructure estimation. Technically, asymptotic results for exponential sums are\nadopted to effectively average out the noise effects while preventing the\ninformation to be estimated from vanishing. For linear systems, the use of the\nWeyl sum further allows us to extract eigenstructures. Our theoretical claims\nare experimentally validated on simulations of toy examples, including Cellular\nAutomata.",
    "descriptor": "",
    "authors": [
      "Motoya Ohnishi",
      "Isao Ishikawa",
      "Yuko Kuroki",
      "Masahiro Ikeda"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00861"
  },
  {
    "id": "arXiv:2206.00867",
    "title": "Stochastic Deep-Ritz for Parametric Uncertainty Quantification",
    "abstract": "Scientific machine learning has become an increasingly popular tool for\nsolving high dimensional differential equations and constructing surrogates of\ncomplex physical models. In this work, we propose a deep learning based\nnumerical method for solving elliptic partial differential equations (PDE) with\nrandom coefficients. We elucidate the stochastic variational formulation for\nthe problem by recourse to the direct method of calculus of variations. The\nformulation allows us to reformulate the random coefficient PDE into a\nstochastic optimization problem, subsequently solved by a combination of Monte\nCarlo sampling and deep-learning approximation. The resulting method is simple\nyet powerful. We carry out numerical experiments to demonstrate the efficiency\nand accuracy of the proposed method.",
    "descriptor": "\nComments: 19 pages, 11 figures\n",
    "authors": [
      "Ting Wang",
      "Jaroslaw Knap"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.00867"
  },
  {
    "id": "arXiv:2206.00868",
    "title": "6G Survey on Challenges, Requirements, Applications, Key Enabling  Technologies, Use Cases, AI integration issues and Security aspects",
    "abstract": "The fifth-generation (5G) network is likely to bring in high data rates, more\nreliability, and low delays for mobile, personal and local area networks.\nAlongside the rapid growth of smart wireless sensing and communication\ntechnologies, data traffic has significantly risen, and existing 5G networks\nare not fully capable of supporting future massive data traffic in terms of\nservices, storage, and processing. To meet the forthcoming challenges, the\nresearch community is investigating the Terahertz-based sixth-generation (6G)\nwireless network which is supposed to be offered for industrial usage in around\n10 years. This is the right time to explore and learn about various 6G aspects\nthat will play a key role in the successful execution and implementation of 6G\nnetworks in the future. This survey provides a review of specifications,\nrequirements, applications, enabling technologies including disruptive and\ninnovative, integration of 6G with advanced architectures and networks like\nsoftware-defined networks (SDN), network functions virtualization (NFV),\ncloud/fog computing, etc, artificial intelligence (AI) oriented technologies,\nprivacy and security issues and solutions, and potential futuristic use cases:\nvirtual reality, smart healthcare and Industry 5.0. Furthermore, based on the\nconducted review, challenges and future research directions are highlighted to\naid the deployment of 6G networks.",
    "descriptor": "",
    "authors": [
      "Muhammad Sajjad Akbar",
      "Zawar Hussain",
      "Quan Z. Sheng",
      "Subhas Mukhopadhyay"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.00868"
  },
  {
    "id": "arXiv:2206.00873",
    "title": "Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with  Feedback Graphs",
    "abstract": "This study considers online learning with general directed feedback graphs.\nFor this problem, we present best-of-both-worlds algorithms that achieve nearly\ntight regret bounds for adversarial environments as well as poly-logarithmic\nregret bounds for stochastic environments. As Alon et al. [2015] have shown,\ntight regret bounds depend on the structure of the feedback graph:\n\\textit{strongly observable} graphs yield minimax regret of $\\tilde{\\Theta}(\n\\alpha^{1/2} T^{1/2} )$, while \\textit{weakly observable} graphs induce minimax\nregret of $\\tilde{\\Theta}( \\delta^{1/3} T^{2/3} )$, where $\\alpha$ and\n$\\delta$, respectively, represent the independence number of the graph and the\ndomination number of a certain portion of the graph. Our proposed algorithm for\nstrongly observable graphs has a regret bound of $\\tilde{O}( \\alpha^{1/2}\nT^{1/2} ) $ for adversarial environments, as well as of $ {O} ( \\frac{\\alpha\n(\\ln T)^3 }{\\Delta_{\\min}} ) $ for stochastic environments, where\n$\\Delta_{\\min}$ expresses the minimum suboptimality gap. This result resolves\nan open question raised by Erez and Koren [2021]. We also provide an algorithm\nfor weakly observable graphs that achieves a regret bound of $\\tilde{O}(\n\\delta^{1/3}T^{2/3} )$ for adversarial environments and poly-logarithmic regret\nfor stochastic environments. The proposed algorithms are based on the\nfollow-the-perturbed-leader approach combined with newly designed update rules\nfor learning rates.",
    "descriptor": "",
    "authors": [
      "Shinji Ito",
      "Taira Tsuchiya",
      "Junya Honda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00873"
  },
  {
    "id": "arXiv:2206.00874",
    "title": "Age of Information in Reservation Multi-Access Networks with Stochastic  Arrivals",
    "abstract": "This paper investigates the Age of Information (AoI) performance of Frame\nSlotted ALOHA with Reservation and Data slots (FSA-RD). We consider a symmetric\nmulti-access network where each user transmits its randomly generated status\nupdates to an access point in a framed manner. Each frame consists of one\nreservation slot and several data slots. The reservation slot is made up of\nsome mini-slots. In each reservation slot, users, with a status update packet\nto transmit, randomly send short reservation packets in one of the mini-slots\nto contend for data slots of the frame. The data slots are assigned to those\nusers that succeed in reservation slot. To provide insights in optimizing the\ninformation freshness of FSA-RD, we manage to derive a closed-form expression\nof the average AoI under FSA-RD by applying a recursive method. Numerical\nresults validate the analytical expression and demonstrate the influence of the\nframe size and reservation probability on the average AoI. We finally perform a\ncomparison between the AoI performance of FSA-RD with optimized frame size and\nreservation probability, and that of slotted ALOHA with optimized transmission\nprobability. The comparison results show that FSA-RD can effectively reduce the\nAoI performance of multi-access networks, especially when the status arrival\nrate of the network becomes large.",
    "descriptor": "\nComments: Paper accepted by IEEE ISIT 2022\n",
    "authors": [
      "Qian Wang",
      "He Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.00874"
  },
  {
    "id": "arXiv:2206.00877",
    "title": "EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &  Accelerator Co-Design",
    "abstract": "Eye tracking has become an essential human-machine interaction modality for\nproviding immersive experience in numerous virtual and augmented reality\n(VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and\nenhanced visual privacy. However, existing eye tracking systems are still\nlimited by their: (1) large form-factor largely due to the adopted bulky\nlens-based cameras; and (2) high communication cost required between the camera\nand backend processor, thus prohibiting their more extensive applications. To\nthis end, we propose a lensless FlatCam-based eye tracking algorithm and\naccelerator co-design framework dubbed EyeCoD to enable eye tracking systems\nwith a much reduced form-factor and boosted system efficiency without\nsacrificing the tracking accuracy, paving the way for next-generation eye\ntracking solutions. On the system level, we advocate the use of lensless\nFlatCams to facilitate the small form-factor need in mobile eye tracking\nsystems. On the algorithm level, EyeCoD integrates a predict-then-focus\npipeline that first predicts the region-of-interest (ROI) via segmentation and\nthen only focuses on the ROI parts to estimate gaze directions, greatly\nreducing redundant computations and data movements. On the hardware level, we\nfurther develop a dedicated accelerator that (1) integrates a novel workload\norchestration between the aforementioned segmentation and gaze estimation\nmodels, (2) leverages intra-channel reuse opportunities for depth-wise layers,\nand (3) utilizes input feature-wise partition to save activation memory size.\nOn-silicon measurement validates that our EyeCoD consistently reduces both the\ncommunication and computation costs, leading to an overall system speedup of\n10.95x, 3.21x, and 12.85x over CPUs, GPUs, and a prior-art eye tracking\nprocessor called CIS-GEP, respectively, while maintaining the tracking\naccuracy.",
    "descriptor": "\nComments: Accepted by ISCA 2022\n",
    "authors": [
      "Haoran You",
      "Cheng Wan",
      "Yang Zhao",
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Jiayi Yuan",
      "Shang Wu",
      "Shunyao Zhang",
      "Yongan Zhang",
      "Chaojian Li",
      "Vivek Boominathan",
      "Ashok Veeraraghavan",
      "Ziyun Li",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.00877"
  },
  {
    "id": "arXiv:2206.00878",
    "title": "EfficientNeRF: Efficient Neural Radiance Fields",
    "abstract": "Neural Radiance Fields (NeRF) has been wildly applied to various tasks for\nits high-quality representation of 3D scenes. It takes long per-scene training\ntime and per-image testing time. In this paper, we present EfficientNeRF as an\nefficient NeRF-based method to represent 3D scene and synthesize novel-view\nimages. Although several ways exist to accelerate the training or testing\nprocess, it is still difficult to much reduce time for both phases\nsimultaneously. We analyze the density and weight distribution of the sampled\npoints then propose valid and pivotal sampling at the coarse and fine stage,\nrespectively, to significantly improve sampling efficiency. In addition, we\ndesign a novel data structure to cache the whole scene during testing to\naccelerate the rendering speed. Overall, our method can reduce over 88\\% of\ntraining time, reach rendering speed of over 200 FPS, while still achieving\ncompetitive accuracy. Experiments prove that our method promotes the\npracticality of NeRF in the real world and enables many applications.",
    "descriptor": "",
    "authors": [
      "Tao Hu",
      "Shu Liu",
      "Yilun Chen",
      "Tiancheng Shen",
      "Jiaya Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00878"
  },
  {
    "id": "arXiv:2206.00886",
    "title": "Watch Out for the Safety-Threatening Actors: Proactively Mitigating  Safety Hazards",
    "abstract": "Despite the successful demonstration of autonomous vehicles (AVs), such as\nself-driving cars, ensuring AV safety remains a challenging task. Although some\nactors influence an AV's driving decisions more than others, current approaches\npay equal attention to each actor on the road. An actor's influence on the AV's\ndecision can be characterized in terms of its ability to decrease the number of\nsafe navigational choices for the AV. In this work, we propose a safety threat\nindicator (STI) using counterfactual reasoning to estimate the importance of\neach actor on the road with respect to its influence on the AV's safety. We use\nthis indicator to (i) characterize the existing real-world datasets to identify\nrare hazardous scenarios as well as the poor performance of existing\ncontrollers in such scenarios; and (ii) design an RL based safety mitigation\ncontroller to proactively mitigate the safety hazards those actors pose to the\nAV. Our approach reduces the accident rate for the state-of-the-art AV agent(s)\nin rare hazardous scenarios by more than 70%.",
    "descriptor": "",
    "authors": [
      "Saurabh Jha",
      "Shengkun Cui",
      "Zbigniew Kalbarczyk",
      "Ravishankar K. Iyer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00886"
  },
  {
    "id": "arXiv:2206.00893",
    "title": "Leveraging Systematic Knowledge of 2D Transformations",
    "abstract": "The existing deep learning models suffer from out-of-distribution (o.o.d.)\nperformance drop in computer vision tasks. In comparison, humans have a\nremarkable ability to interpret images, even if the scenes in the images are\nrare, thanks to the systematicity of acquired knowledge. This work focuses on\n1) the acquisition of systematic knowledge of 2D transformations, and 2)\narchitectural components that can leverage the learned knowledge in image\nclassification tasks in an o.o.d. setting. With a new training methodology\nbased on synthetic datasets that are constructed under the causal framework,\nthe deep neural networks acquire knowledge from semantically different domains\n(e.g. even from noise), and exhibit certain level of systematicity in parameter\nestimation experiments. Based on this, a novel architecture is devised\nconsisting of a classifier, an estimator and an identifier (abbreviated as\n\"CED\"). By emulating the \"hypothesis-verification\" process in human visual\nperception, CED improves the classification accuracy significantly on test sets\nunder covariate shift.",
    "descriptor": "",
    "authors": [
      "Jiachen Kang",
      "Wenjing Jia",
      "Xiangjian He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00893"
  },
  {
    "id": "arXiv:2206.00897",
    "title": "xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture  Imagery",
    "abstract": "Unsustainable fishing practices worldwide pose a major threat to marine\nresources and ecosystems. Identifying vessels that evade monitoring systems --\nknown as \"dark vessels\" -- is key to managing and securing the health of marine\nenvironments. With the rise of satellite-based synthetic aperture radar (SAR)\nimaging and modern machine learning (ML), it is now possible to automate\ndetection of dark vessels day or night, under all-weather conditions. SAR\nimages, however, require domain-specific treatment and is not widely accessible\nto the ML community. Moreover, the objects (vessels) are small and sparse,\nchallenging traditional computer vision approaches. We present the largest\nlabeled dataset for training ML models to detect and characterize vessels from\nSAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the\nSentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The\nimages are annotated using a combination of automated and manual analysis.\nCo-located bathymetry and wind state rasters accompany every SAR image. We\nprovide an overview of the results from the xView3 Computer Vision Challenge,\nan international competition using xView3-SAR for ship detection and\ncharacterization at large scale. We release the data (https://iuu.xview.us/)\nand code (https://github.com/DIUx-xView) to support ongoing development and\nevaluation of ML approaches for this important application.",
    "descriptor": "\nComments: 9 pages (21 with references and supplement). In review\n",
    "authors": [
      "Fernando Paolo",
      "Tsu-ting Tim Lin",
      "Ritwik Gupta",
      "Bryce Goodman",
      "Nirav Patel",
      "Daniel Kuster",
      "David Kroodsma",
      "Jared Dunnmon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.00897"
  },
  {
    "id": "arXiv:2206.00901",
    "title": "Musical Instrument Recognition by XGBoost Combining Feature Fusion",
    "abstract": "Musical instrument classification is one of the focuses of Music Information\nRetrieval (MIR). In order to solve the problem of poor performance of current\nmusical instrument classification models, we propose a musical instrument\nclassification algorithm based on multi-channel feature fusion and XGBoost.\nBased on audio feature extraction and fusion of the dataset, the features are\ninput into the XGBoost model for training; secondly, we verified the superior\nperformance of the algorithm in the musical instrument classification task by\ncom-paring different feature combinations and several classical machine\nlearning models such as Naive Bayes. The algorithm achieves an accuracy of\n97.65% on the Medley-solos-DB dataset, outperforming existing models. The\nexperiments provide a reference for feature selection in feature engineering\nfor musical instrument classification.",
    "descriptor": "",
    "authors": [
      "Yijie Liu",
      "Yanfang Yin",
      "Qigang Zhu",
      "Wenzhuo Cui"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.00901"
  },
  {
    "id": "arXiv:2206.00902",
    "title": "MISSU: 3D Medical Image Segmentation via Self-distilling TransUNet",
    "abstract": "U-Nets have achieved tremendous success in medical image segmentation.\nNevertheless, it may suffer limitations in global (long-range) contextual\ninteractions and edge-detail preservation. In contrast, Transformer has an\nexcellent ability to capture long-range dependencies by leveraging the\nself-attention mechanism into the encoder. Although Transformer was born to\nmodel the long-range dependency on the extracted feature maps, it still suffers\nfrom extreme computational and spatial complexities in processing\nhigh-resolution 3D feature maps. This motivates us to design the efficiently\nTransformer-based UNet model and study the feasibility of Transformer-based\nnetwork architectures for medical image segmentation tasks. To this end, we\npropose to self-distill a Transformer-based UNet for medical image\nsegmentation, which simultaneously learns global semantic information and local\nspatial-detailed features. Meanwhile, a local multi-scale fusion block is first\nproposed to refine fine-grained details from the skipped connections in the\nencoder by the main CNN stem through self-distillation, only computed during\ntraining and removed at inference with minimal overhead. Extensive experiments\non BraTS 2019 and CHAOS datasets show that our MISSU achieves the best\nperformance over previous state-of-the-art methods. Code and models are\navailable at \\url{https://github.com/wangn123/MISSU.git}",
    "descriptor": "",
    "authors": [
      "Nan Wang",
      "Shaohui Lin",
      "Xiaoxiao Li",
      "Ke Li",
      "Yunhang Shen",
      "Yue Gao",
      "Lizhuang Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00902"
  },
  {
    "id": "arXiv:2206.00903",
    "title": "Satisfiability of Quantified Boolean Announcements",
    "abstract": "Dynamic epistemic logics consider formal representations of agents'\nknowledge, and how the knowledge of agents changes in response to informative\nevents, such as public announcements. Quantifying over informative events\nallows us to ask whether it is possible to achieve some state of knowledge, and\nhas important applications in synthesising secure communication protocols.\nHowever, quantifying over quite simple informative events, public\nannouncements, is not computable: such an arbitrary public announcement logic,\nAPAL, has an undecidable satisfiability problem. Here we consider even simpler\ninformative events called Boolean announcements, where announcements are\nrestricted to be a Boolean combination of atomic propositions. The logic is\ncalled Boolean arbitrary public announcement logic, BAPAL. A companion paper\nprovides a complete finitary axiomatization, and related expressivity results,\nfor BAPAL. In this work the satisfiability problem for BAPAL is shown to\ndecidable, and also that BAPAL does not have the finite model property.",
    "descriptor": "",
    "authors": [
      "Hans van Ditmarsch",
      "Tim French",
      "Rustam Galimullin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.00903"
  },
  {
    "id": "arXiv:2206.00906",
    "title": "NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model  with Logic Regularization",
    "abstract": "The symptom checking systems inquire users for their symptoms and perform a\nrapid and affordable medical assessment of their condition. The basic symptom\nchecking systems based on Bayesian methods, decision trees, or information gain\nmethods are easy to train and do not require significant computational\nresources. However, their drawbacks are low relevance of proposed symptoms and\ninsufficient quality of diagnostics. The best results on these tasks are\nachieved by reinforcement learning models. Their weaknesses are the difficulty\nof developing and training such systems and limited applicability to cases with\nlarge and sparse decision spaces. We propose a new approach based on the\nsupervised learning of neural models with logic regularization that combines\nthe advantages of the different methods. Our experiments on real and synthetic\ndata show that the proposed approach outperforms the best existing methods in\nthe accuracy of diagnosis when the number of diagnoses and symptoms is large.",
    "descriptor": "\nComments: Published in the proceedings of the conference \"Artificial Intelligence in Medicine 2022\"\n",
    "authors": [
      "Aleksandr Nesterov",
      "Bulat Ibragimov",
      "Dmitriy Umerenkov",
      "Artem Shelmanov",
      "Galina Zubkova",
      "Vladimir Kokh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.00906"
  },
  {
    "id": "arXiv:2206.00910",
    "title": "A Real-time Critical-scenario-generation Framework for Testing  Autonomous Driving System",
    "abstract": "In order to find the most likely failure scenarios which may occur under\ncertain given operation domain, critical-scenario-based test is supposed as an\neffective and widely used method, which gives suggestions for designers to\nimprove the developing algorithm. However, for the state of art,\ncritical-scenario generation approaches commonly utilize random-search or\nreinforcement learning methods to generate series of scenarios for a specific\nalgorithm, which takes amounts of computing resource for testing a developing\ntarget that is always changing, and inapplicable for testing a real-time\nsystem. In this paper, we proposed a real-time critical-scenario-generation\n(RTCSG) framework to address the above challenges. In our framework, an\naggressive-driving algorithm is proposed in controlling the virtual agent\nvehicles, a specially designed cost function is presented to guide scenarios to\nevolve towards critical conditions, and a self-adaptive coefficient iteration\nis designed that enable the approach to operate successfully in different\nconditions. With our proposed method, the critical-scenarios can be directly\ngenerated for the target under test which is a black-box system, and the\nreal-time critical-scenario test can be brought into reality. The simulation\nresults show that our approach is able to obtain more critical scenarios in\nmost conditions than current methods, with a higher stability of success. For a\nreal-time testing, our approach improves the efficiency around 16 times.",
    "descriptor": "\nComments: 7pages,9 figures, 21 references\n",
    "authors": [
      "Yizhou Xie",
      "Kunpeng Dai",
      "Yong Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.00910"
  },
  {
    "id": "arXiv:2206.00913",
    "title": "Mask-Guided Divergence Loss Improves the Generalization and Robustness  of Deep Neural Network",
    "abstract": "Deep neural network (DNN) with dropout can be regarded as an ensemble model\nconsisting of lots of sub-DNNs (i.e., an ensemble sub-DNN where the sub-DNN is\nthe remaining part of the DNN after dropout), and through increasing the\ndiversity of the ensemble sub-DNN, the generalization and robustness of the DNN\ncan be effectively improved. In this paper, a mask-guided divergence loss\nfunction (MDL), which consists of a cross-entropy loss term and an orthogonal\nterm, is proposed to increase the diversity of the ensemble sub-DNN by the\nadded orthogonal term. Particularly, the mask technique is introduced to assist\nin generating the orthogonal term for avoiding overfitting of the diversity\nlearning. The theoretical analysis and extensive experiments on 4 datasets\n(i.e., MNIST, FashionMNIST, CIFAR10, and CIFAR100) manifest that MDL can\nimprove the generalization and robustness of standard training and adversarial\ntraining. For CIFAR10 and CIFAR100, in standard training, the maximum\nimprovement of accuracy is $1.38\\%$ on natural data, $30.97\\%$ on FGSM (i.e.,\nFast Gradient Sign Method) attack, $38.18\\%$ on PGD (i.e., Projected Gradient\nDescent) attack. While in adversarial training, the maximum improvement is\n$1.68\\%$ on natural data, $4.03\\%$ on FGSM attack and $2.65\\%$ on PGD attack.",
    "descriptor": "",
    "authors": [
      "Xiangyuan Yang",
      "Jie Lin",
      "Hanlin Zhang",
      "Xinyu Yang",
      "Peng Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00913"
  },
  {
    "id": "arXiv:2206.00920",
    "title": "Federated Learning with a Sampling Algorithm under Isoperimetry",
    "abstract": "Federated learning uses a set of techniques to efficiently distribute the\ntraining of a machine learning algorithm across several devices, who own the\ntraining data. These techniques critically rely on reducing the communication\ncost -- the main bottleneck -- between the devices and a central server.\nFederated learning algorithms usually take an optimization approach: they are\nalgorithms for minimizing the training loss subject to communication (and\nother) constraints. In this work, we instead take a Bayesian approach for the\ntraining task, and propose a communication-efficient variant of the Langevin\nalgorithm to sample a posteriori. The latter approach is more robust and\nprovides more knowledge of the \\textit{a posteriori} distribution than its\noptimization counterpart. We analyze our algorithm without assuming that the\ntarget distribution is strongly log-concave. Instead, we assume the weaker log\nSobolev inequality, which allows for nonconvexity.",
    "descriptor": "",
    "authors": [
      "Lukang Sun",
      "Adil Salim",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00920"
  },
  {
    "id": "arXiv:2206.00921",
    "title": "A Scalable Shannon Entropy Estimator",
    "abstract": "We revisit the well-studied problem of estimating the Shannon entropy of a\nprobability distribution, now given access to a probability-revealing\nconditional sampling oracle. In this model, the oracle takes as input the\nrepresentation of a set $S$ and returns a sample from the distribution obtained\nby conditioning on $S$, together with the probability of that sample in the\ndistribution. Our work is motivated by applications of such algorithms in\nQuantitative Information Flow analysis (QIF) in programming-language-based\nsecurity. Here, information-theoretic quantities capture the effort required on\nthe part of an adversary to obtain access to confidential information. These\napplications demand accurate measurements when the entropy is small. Existing\nalgorithms that do not use conditional samples require a number of queries that\nscale inversely with the entropy, which is unacceptable in this regime, and\nindeed, a lower bound by Batu et al.(STOC 2002) established that no algorithm\nusing only sampling and evaluation oracles can obtain acceptable performance.\nOn the other hand, prior work in the conditional sampling model by Chakraborty\net al.(SICOMP 2016) only obtained a high-order polynomial query complexity,\n$\\mathcal{O}(\\frac{m^7}{\\epsilon^8}\\log\\frac{1}{\\delta})$ queries, to obtain\nadditive $\\epsilon$-approximations on a domain of size $\\mathcal{O}(2^m)$.\nWe obtain multiplicative $(1+\\epsilon)$-approximations using only\n$\\mathcal{O}(\\frac{m}{\\epsilon^2}\\log\\frac{1}{\\delta})$ queries to the\nprobability-revealing conditional sampling oracle. Indeed, moreover, we obtain\nsmall, explicit constants, and demonstrate that our algorithm obtains a\nsubstantial improvement in practice over the previous state-of-the-art methods\nused for entropy estimation in QIF.",
    "descriptor": "\nComments: 24 pages, 1 figure, A preliminary version of this work appears at CAV, 2022\n",
    "authors": [
      "Priyanka Golia",
      "Brendan Juba",
      "Kuldeep S. Meel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00921"
  },
  {
    "id": "arXiv:2206.00923",
    "title": "Modeling Image Composition for Complex Scene Generation",
    "abstract": "We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&patch-level and\nobject-level&patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Zuopeng Yang",
      "Daqing Liu",
      "Chaoyue Wang",
      "Jie Yang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00923"
  },
  {
    "id": "arXiv:2206.00924",
    "title": "FACM: Correct the Output of Deep Neural Network with Middle Layers  Features against Adversarial Samples",
    "abstract": "In the strong adversarial attacks against deep neural network (DNN), the\noutput of DNN will be misclassified if and only if the last feature layer of\nthe DNN is completely destroyed by adversarial samples, while our studies found\nthat the middle feature layers of the DNN can still extract the effective\nfeatures of the original normal category in these adversarial attacks. To this\nend, in this paper, a middle $\\bold{F}$eature layer $\\bold{A}$nalysis and\n$\\bold{C}$onditional $\\bold{M}$atching prediction distribution (FACM) model is\nproposed to increase the robustness of the DNN against adversarial samples\nthrough correcting the output of DNN with the features extracted by the middle\nlayers of DNN. In particular, the middle $\\bold{F}$eature layer\n$\\bold{A}$nalysis (FA) module, the conditional matching prediction distribution\n(CMPD) module and the output decision module are included in our FACM model to\ncollaboratively correct the classification of adversarial samples. The\nexperiments results show that, our FACM model can significantly improve the\nrobustness of the naturally trained model against various attacks, and our FA\nmodel can significantly improve the robustness of the adversarially trained\nmodel against white-box attacks with weak transferability and black box attacks\nwhere FA model includes the FA module and the output decision module, not the\nCMPD module.",
    "descriptor": "",
    "authors": [
      "Xiangyuan Yang",
      "Jie Lin",
      "Hanlin Zhang",
      "Xinyu Yang",
      "Peng Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00924"
  },
  {
    "id": "arXiv:2206.00927",
    "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling  in Around 10 Steps",
    "abstract": "Diffusion probabilistic models (DPMs) are emerging powerful generative\nmodels. Despite their high-quality generation performance, DPMs still suffer\nfrom their slow sampling as they generally need hundreds or thousands of\nsequential function evaluations (steps) of large neural networks to draw a\nsample. Sampling from DPMs can be viewed alternatively as solving the\ncorresponding diffusion ordinary differential equations (ODEs). In this work,\nwe propose an exact formulation of the solution of diffusion ODEs. The\nformulation analytically computes the linear part of the solution, rather than\nleaving all terms to black-box ODE solvers as adopted in previous works. By\napplying change-of-variable, the solution can be equivalently simplified to an\nexponentially weighted integral of the neural network. Based on our\nformulation, we propose DPM-Solver, a fast dedicated high-order solver for\ndiffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for\nboth discrete-time and continuous-time DPMs without any further training.\nExperimental results show that DPM-Solver can generate high-quality samples in\nonly 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in\n10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10\ndataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art\ntraining-free samplers on various datasets.",
    "descriptor": "",
    "authors": [
      "Cheng Lu",
      "Yuhao Zhou",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00927"
  },
  {
    "id": "arXiv:2206.00929",
    "title": "The ParlaSent-BCS dataset of sentiment-annotated parliamentary debates  from Bosnia-Herzegovina, Croatia, and Serbia",
    "abstract": "Expression of sentiment in parliamentary debates is deemed to be\nsignificantly different from that on social media or in product reviews. This\npaper adds to an emerging body of research on parliamentary debates with a\ndataset of sentences annotated for detection sentiment polarity in political\ndiscourse. We sample the sentences for annotation from the proceedings of three\nSoutheast European parliaments: Croatia, Bosnia-Herzegovina, and Serbia. A\nsix-level schema is applied to the data with the aim of training a\nclassification model for the detection of sentiment in parliamentary\nproceedings. Krippendorff's alpha measuring the inter-annotator agreement\nranges from 0.6 for the six-level annotation schema to 0.75 for the three-level\nschema and 0.83 for the two-level schema. Our initial experiments on the\ndataset show that transformer models perform significantly better than those\nusing a simpler architecture. Furthermore, regardless of the similarity of the\nthree languages, we observe differences in performance across different\nlanguages. Performing parliament-specific training and evaluation shows that\nthe main reason for the differing performance between parliaments seems to be\nthe different complexity of the automatic classification task, which is not\nobservable in annotator performance. Language distance does not seem to play\nany role neither in annotator nor in automatic classification performance. We\nrelease the dataset and the best-performing model under permissive licences.",
    "descriptor": "\nComments: 8 pages, submitted to JT-DH 2022 (Language Technologies and Digital Humanities 2022) conference, number 4293\n",
    "authors": [
      "Michal Mochtak",
      "Peter Rupnik",
      "Nikola Ljube\u0161i\u010d"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.00929"
  },
  {
    "id": "arXiv:2206.00930",
    "title": "Predicting Physical Object Properties from Video",
    "abstract": "We present a novel approach to estimating physical properties of objects from\nvideo. Our approach consists of a physics engine and a correction estimator.\nStarting from the initial observed state, object behavior is simulated forward\nin time. Based on the simulated and observed behavior, the correction estimator\nthen determines refined physical parameters for each object. The method can be\niterated for increased precision. Our approach is generic, as it allows for the\nuse of an arbitrary - not necessarily differentiable - physics engine and\ncorrection estimator. For the latter, we evaluate both gradient-free\nhyperparameter optimization and a deep convolutional neural network. We\ndemonstrate faster and more robust convergence of the learned method in several\nsimulated 2D scenarios focusing on bin situations.",
    "descriptor": "\nComments: accepted for International Joint Conference on Neural Networks (IJCNN) 2022\n",
    "authors": [
      "Martin Link",
      "Max Schwarz",
      "Sven Behnke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.00930"
  },
  {
    "id": "arXiv:2206.00931",
    "title": "Generating Sparse Counterfactual Explanations For Multivariate Time  Series",
    "abstract": "Since neural networks play an increasingly important role in critical\nsectors, explaining network predictions has become a key research topic.\nCounterfactual explanations can help to understand why classifier models decide\nfor particular class assignments and, moreover, how the respective input\nsamples would have to be modified such that the class prediction changes.\nPrevious approaches mainly focus on image and tabular data. In this work we\npropose SPARCE, a generative adversarial network (GAN) architecture that\ngenerates SPARse Counterfactual Explanations for multivariate time series. Our\napproach provides a custom sparsity layer and regularizes the counterfactual\nloss function in terms of similarity, sparsity, and smoothness of trajectories.\nWe evaluate our approach on real-world human motion datasets as well as a\nsynthetic time series interpretability benchmark. Although we make\nsignificantly sparser modifications than other approaches, we achieve\ncomparable or better performance on all metrics. Moreover, we demonstrate that\nour approach predominantly modifies salient time steps and features, leaving\nnon-salient inputs untouched.",
    "descriptor": "\nComments: 10 pages, 6 figures. Preprint. Under review\n",
    "authors": [
      "Jana Lang",
      "Martin Giese",
      "Winfried Ilg",
      "Sebastian Otte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00931"
  },
  {
    "id": "arXiv:2206.00934",
    "title": "Deep neural networks can stably solve high-dimensional, noisy,  non-linear inverse problems",
    "abstract": "We study the problem of reconstructing solutions of inverse problems with\nneural networks when only noisy data is available. We assume the problem can be\nmodeled with an infinite-dimensional forward operator that is not continuously\ninvertible. Then, we restrict this forward operator to finite-dimensional\nspaces so that the inverse is Lipschitz continuous. For the inverse operator,\nwe demonstrate that there exists a neural network which is a robust-to-noise\napproximation of the function. In addition, we show that these neural networks\ncan be learned from appropriately perturbed training data. We demonstrate the\nadmissibility of this approach to a wide range of inverse problems of practical\ninterest. Numerical examples are given that support the theoretical findings.",
    "descriptor": "",
    "authors": [
      "Andr\u00e9s Felipe Lerma Pineda",
      "Philipp Christian Petersen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00934"
  },
  {
    "id": "arXiv:2206.00938",
    "title": "Exploiting Near-Data Processing to Accelerate Time Series Analysis",
    "abstract": "Time series analysis is a key technique for extracting and predicting events\nin domains as diverse as epidemiology, genomics, neuroscience, environmental\nsciences, economics, and more. Matrix profile, the state-of-the-art algorithm\nto perform time series analysis, computes the most similar subsequence for a\ngiven query subsequence within a sliced time series. Matrix profile has low\narithmetic intensity, but it typically operates on large amounts of time series\ndata. In current computing systems, this data needs to be moved between the\noff-chip memory units and the on-chip computation units for performing matrix\nprofile. This causes a major performance bottleneck as data movement is\nextremely costly in terms of both execution time and energy.\nIn this work, we present NATSA, the first Near-Data Processing accelerator\nfor time series analysis. The key idea is to exploit modern 3D-stacked High\nBandwidth Memory (HBM) to enable efficient and fast specialized matrix profile\ncomputation near memory, where time series data resides. NATSA provides three\nkey benefits: 1) quickly computing the matrix profile for a wide range of\napplications by building specialized energy-efficient floating-point arithmetic\nprocessing units close to HBM, 2) improving the energy efficiency and execution\ntime by reducing the need for data movement over slow and energy-hungry buses\nbetween the computation units and the memory units, and 3) analyzing time\nseries data at scale by exploiting low-latency, high-bandwidth, and\nenergy-efficient memory access provided by HBM. Our experimental evaluation\nshows that NATSA improves performance by up to 14.2x (9.9x on average) and\nreduces energy by up to 27.2x (19.4x on average), over the state-of-the-art\nmulti-core implementation. NATSA also improves performance by 6.3x and reduces\nenergy by 10.2x over a general-purpose NDP platform with 64 in-order cores.",
    "descriptor": "\nComments: To appear in ISVLSI 2022 Special Session on Processing in Memory. arXiv admin note: text overlap with arXiv:2010.02079\n",
    "authors": [
      "Ivan Fernandez",
      "Ricardo Quislant",
      "Christina Giannoula",
      "Mohammed Alser",
      "Juan G\u00f3mez-Luna",
      "Eladio Guti\u00e9rrez",
      "Oscar Plata",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.00938"
  },
  {
    "id": "arXiv:2206.00941",
    "title": "Improving Diffusion Models for Inverse Problems using Manifold  Constraints",
    "abstract": "Recently, diffusion models have been used to solve various inverse problems\nin an unsupervised manner with appropriate modifications to the sampling\nprocess. However, the current solvers, which recursively apply a reverse\ndiffusion step followed by a measurement consistency step, often produce\nsub-optimal results. By studying the generative sampling path, here we show\nthat current solvers throw the sample path off the data manifold, and hence the\nerror accumulates. To address this, we propose an additional correction term\ninspired by the manifold constraint, which can be used synergistically with the\nprevious solvers to make the iterations close to the manifold. The proposed\nmanifold constraint is straightforward to implement within a few lines of code,\nyet boosts the performance by a surprisingly large margin. With extensive\nexperiments, we show that our method is superior to the previous methods both\ntheoretically and empirically, producing promising results in many applications\nsuch as image inpainting, colorization, and sparse-view computed tomography.",
    "descriptor": "",
    "authors": [
      "Hyungjin Chung",
      "Byeongsu Sim",
      "Dohoon Ryu",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00941"
  },
  {
    "id": "arXiv:2206.00942",
    "title": "A Serverless Engine for High Energy Physics Distributed Analysis",
    "abstract": "The Large Hadron Collider (LHC) at CERN has generated in the last decade an\nunprecedented volume of data for the High-Energy Physics (HEP) field.\nScientific collaborations interested in analysing such data very often require\ncomputing power beyond a single machine. This issue has been tackled\ntraditionally by running analyses in distributed environments using stateful,\nmanaged batch computing systems. While this approach has been effective so far,\ncurrent estimates for future computing needs of the field present large scaling\nchallenges. Such a managed approach may not be the only viable way to tackle\nthem and an interesting alternative could be provided by serverless\narchitectures, to enable an even larger scaling potential.\nThis work describes a novel approach to running real HEP scientific\napplications through a distributed serverless computing engine. The engine is\nbuilt upon ROOT, a well-established HEP data analysis software, and distributes\nits computations to a large pool of concurrent executions on Amazon Web\nServices Lambda Serverless Platform. Thanks to the developed tool, physicists\nare able to access datasets stored at CERN (also those that are under\nrestricted access policies) and process it on remote infrastructures outside of\ntheir typical environment. The analysis of the serverless functions is\nmonitored at runtime to gather performance metrics, both for data- and\ncomputation-intensive workloads.",
    "descriptor": "\nComments: 10 pages, CCGRID 2022\n",
    "authors": [
      "Jacek Ku\u015bnierz",
      "Vincenzo Eduardo Padulano",
      "Maciej Malawski",
      "Kamil Burkiewicz",
      "Enric Tejedor Saavedra",
      "Pedro Alonso-Jord\u00e1",
      "Michael Pitt",
      "Valentina Avati"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.00942"
  },
  {
    "id": "arXiv:2206.00944",
    "title": "Feature Space Particle Inference for Neural Network Ensembles",
    "abstract": "Ensembles of deep neural networks demonstrate improved performance over\nsingle models. For enhancing the diversity of ensemble members while keeping\ntheir performance, particle-based inference methods offer a promising approach\nfrom a Bayesian perspective. However, the best way to apply these methods to\nneural networks is still unclear: seeking samples from the weight-space\nposterior suffers from inefficiency due to the over-parameterization issues,\nwhile seeking samples directly from the function-space posterior often results\nin serious underfitting. In this study, we propose optimizing particles in the\nfeature space where the activation of a specific intermediate layer lies to\naddress the above-mentioned difficulties. Our method encourages each member to\ncapture distinct features, which is expected to improve ensemble prediction\nrobustness. Extensive evaluation on real-world datasets shows that our model\nsignificantly outperforms the gold-standard Deep Ensembles on various metrics,\nincluding accuracy, calibration, and robustness. Code is available at\nhttps://github.com/DensoITLab/featurePI .",
    "descriptor": "\nComments: ICML2022\n",
    "authors": [
      "Shingo Yashima",
      "Teppei Suzuki",
      "Kohta Ishikawa",
      "Ikuro Sato",
      "Rei Kawakami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00944"
  },
  {
    "id": "arXiv:2206.00945",
    "title": "Algorithmic Fairness and Structural Injustice: Insights from Feminist  Political Philosophy",
    "abstract": "Data-driven predictive algorithms are widely used to automate and guide\nhigh-stake decision making such as bail and parole recommendation, medical\nresource distribution, and mortgage allocation. Nevertheless, harmful outcomes\nbiased against vulnerable groups have been reported. The growing research field\nknown as 'algorithmic fairness' aims to mitigate these harmful biases. Its\nprimary methodology consists in proposing mathematical metrics to address the\nsocial harms resulting from an algorithm's biased outputs. The metrics are\ntypically motivated by -- or substantively rooted in -- ideals of distributive\njustice, as formulated by political and legal philosophers. The perspectives of\nfeminist political philosophers on social justice, by contrast, have been\nlargely neglected. Some feminist philosophers have criticized the paradigm of\ndistributive justice and have proposed corrective amendments to surmount its\nlimitations. The present paper brings some key insights of feminist political\nphilosophy to algorithmic fairness. The paper has three goals. First, I show\nthat algorithmic fairness does not accommodate structural injustices in its\ncurrent scope. Second, I defend the relevance of structural injustices -- as\npioneered in the contemporary philosophical literature by Iris Marion Young --\nto algorithmic fairness. Third, I take some steps in developing the paradigm of\n'responsible algorithmic fairness' to correct for errors in the current scope\nand implementation of algorithmic fairness.",
    "descriptor": "\nComments: This paper is accepted for publication in the Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES 22)\n",
    "authors": [
      "Atoosa Kasirzadeh"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.00945"
  },
  {
    "id": "arXiv:2206.00946",
    "title": "An efficient thermal lattice Boltzmann method for simulating  three-dimensional liquid-vapor phase change",
    "abstract": "In this paper, a multiple-relaxation-time lattice Boltzmann (LB) approach is\ndeveloped for the simulation of three-dimensional (3D) liquid-vapor phase\nchange based on the pseudopotential model. In contrast to some existing 3D\nthermal LB models for liquid-vapor phase change, the present approach has two\nadvantages: for one thing, the current approach does not require calculating\nthe gradient of volumetric heat capacity [i.e., $\\nabla \\left( {\\rho {c_v}}\n\\right)$], and for another, the current approach is constructed based on the\nseven discrete velocities in three dimensions (D3Q7), making the current\nthermal LB model more efficient and easy to implement. Also, based on the\nscheme proposed by Zhou and He [Phys Fluids 9:1591-1598, 1997], a pressure\nboundary condition for the D3Q19 lattice is proposed to model the multiphase\nflow in open systems. The current method is then validated by considering the\ntemperature distribution in a 3D saturated liquid-vapor system, the $d^2$ law\nand the droplet evaporation on a heated surface. It is observed that the\nnumerical results fit well with the analytical solutions, the results of the\nfinite difference method and the experimental data. Our numerical results\nindicate that the present approach is reliable and efficient in dealing with\nthe 3D liquid-vapor phase change.",
    "descriptor": "",
    "authors": [
      "Jiangxu Huang",
      "Lei Wang",
      "Kun He",
      "Changsheng Huang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.00946"
  },
  {
    "id": "arXiv:2206.00947",
    "title": "A Bhattacharyya Coefficient-Based Framework for Noise Model-Aware Random  Walker Image Segmentation",
    "abstract": "One well established method of interactive image segmentation is the random\nwalker algorithm. Considerable research on this family of segmentation methods\nhas been continuously conducted in recent years with numerous applications.\nThese methods are common in using a simple Gaussian weight function which\ndepends on a parameter that strongly influences the segmentation performance.\nIn this work we propose a general framework of deriving weight functions based\non probabilistic modeling. This framework can be concretized to cope with\nvirtually any well-defined noise model. It eliminates the critical parameter\nand thus avoids time-consuming parameter search. We derive the specific weight\nfunctions for common noise types and show their superior performance on\nsynthetic data as well as different biomedical image data (MRI images from the\nNYU fastMRI dataset, larvae images acquired with the FIM technique). Our\nframework can also be used in multiple other applications, e.g., the graph cut\nalgorithm and its extensions.",
    "descriptor": "\nComments: Dominik Drees and Florian Eilers contributed equally to this work\n",
    "authors": [
      "Dominik Drees",
      "Florian Eilers",
      "Ang Bian",
      "Xiaoyi Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.00947"
  },
  {
    "id": "arXiv:2206.00960",
    "title": "SparseDet: Towards End-to-End 3D Object Detection",
    "abstract": "In this paper, we propose SparseDet for end-to-end 3D object detection from\npoint cloud. Existing works on 3D object detection rely on dense object\ncandidates over all locations in a 3D or 2D grid following the mainstream\nmethods for object detection in 2D images. However, this dense paradigm\nrequires expertise in data to fulfill the gap between label and detection. As a\nnew detection paradigm, SparseDet maintains a fixed set of learnable proposals\nto represent latent candidates and directly perform classification and\nlocalization for 3D objects through stacked transformers. It demonstrates that\neffective 3D object detection can be achieved with none of post-processing such\nas redundant removal and non-maximum suppression. With a properly designed\nnetwork, SparseDet achieves highly competitive detection accuracy while running\nwith a more efficient speed of 34.5 FPS. We believe this end-to-end paradigm of\nSparseDet will inspire new thinking on the sparsity of 3D object detection.",
    "descriptor": "",
    "authors": [
      "Jianhong Han",
      "Zhaoyi Wan",
      "Zhe Liu",
      "Jie Feng",
      "Bingfeng Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00960"
  },
  {
    "id": "arXiv:2206.00962",
    "title": "Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language  Detection",
    "abstract": "We study the selection of transfer languages for automatic abusive language\ndetection. Instead of preparing a dataset for every language, we demonstrate\nthe effectiveness of cross-lingual transfer learning for zero-shot abusive\nlanguage detection. This way we can use existing data from higher-resource\nlanguages to build better detection systems for low-resource languages. Our\ndatasets are from seven different languages from three language families. We\nmeasure the distance between the languages using several language similarity\nmeasures, especially by quantifying the World Atlas of Language Structures. We\nshow that there is a correlation between linguistic similarity and classifier\nperformance. This discovery allows us to choose an optimal transfer language\nfor zero shot abusive language detection.",
    "descriptor": "",
    "authors": [
      "Juuso Eronen",
      "Michal Ptaszynski",
      "Fumito Masui",
      "Masaki Arata",
      "Gniewosz Leliwa",
      "Michal Wroczynski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.00962"
  },
  {
    "id": "arXiv:2206.00968",
    "title": "Real-Time, Constant-Space, Constant-Randomness Verifiers",
    "abstract": "We study the class of languages that have membership proofs which can be\nverified by real-time finite-state machines using only a constant number of\nrandom bits, regardless of the size of their inputs. Since any further\nrestriction on the verifiers would preclude the verification of nonregular\nlanguages, this is the tightest computational budget which allows the checking\nof externally provided proofs to have meaningful use. We show that all\nlanguages that can be recognized by two-head one-way deterministic finite\nautomata have such membership proofs. For any $k>0$, there exist languages that\ncannot be recognized by any $k$-head one-way nondeterministic finite automaton,\nbut that are nonetheless real-time verifiable in this sense. The set of\nnonpalindromes, which cannot be recognized by any one-way multihead\ndeterministic finite automaton, is also demonstrated to be verifiable within\nthese restrictions.",
    "descriptor": "\nComments: 12 pages, 1 figure, accepted for presentation in the conference CIAA 2022, and published in its proceedings\n",
    "authors": [
      "\u00d6zdeniz Dolu",
      "Nevzat Ersoy",
      "M. Utkan Gezer",
      "A. C. Cem Say"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.00968"
  },
  {
    "id": "arXiv:2206.00971",
    "title": "CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework  Using CNN, Visual Transformer and Multilayer Perceptron",
    "abstract": "Cervical cancer is the seventh most common cancer among all the cancers\nworldwide and the fourth most common cancer among women. Cervical cytopathology\nimage classification is an important method to diagnose cervical cancer. Manual\nscreening of cytopathology images is time-consuming and error-prone. The\nemergence of the automatic computer-aided diagnosis system solves this problem.\nThis paper proposes a framework called CVM-Cervix based on deep learning to\nperform cervical cell classification tasks. It can analyze pap slides quickly\nand accurately. CVM-Cervix first proposes a Convolutional Neural Network module\nand a Visual Transformer module for local and global feature extraction\nrespectively, then a Multilayer Perceptron module is designed to fuse the local\nand global features for the final classification. Experimental results show the\neffectiveness and potential of the proposed CVM-Cervix in the field of cervical\nPap smear image classification. In addition, according to the practical needs\nof clinical work, we perform a lightweight post-processing to compress the\nmodel.",
    "descriptor": "",
    "authors": [
      "Wanli Liu",
      "Chen Li",
      "Ning Xu",
      "Tao Jiang",
      "Md Mamunur Rahaman",
      "Hongzan Sun",
      "Xiangchen Wu",
      "Weiming Hu",
      "Haoyuan Chen",
      "Changhao Sun",
      "Yudong Yao",
      "Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00971"
  },
  {
    "id": "arXiv:2206.00975",
    "title": "A Fast Randomized Algorithm for computing the Null Space",
    "abstract": "Randomized algorithms in numerical linear algebra can be fast, scalable and\nrobust. This paper examines the effect of sketching on the right singular\nvectors corresponding to the smallest singular values of a tall-skinny matrix.\nWe devise a fast algorithm for finding the trailing right singular vectors\nusing randomization and examine the quality of the solution using\nmultiplicative perturbation theory. For an $m\\times n$ ($m\\geq n$) matrix, the\nalgorithm runs with complexity $O(mn\\log n +n^3)$ which is faster than the\nstandard $O(mn^2)$ methods. In applications, numerical experiments show great\nspeedups including a $30\\times$ speedup for the AAA algorithm and $10\\times$\nspeedup for the total least squares problem.",
    "descriptor": "\nComments: 19 pages, 3 figures, 2 tables\n",
    "authors": [
      "Yuji Nakatsukasa",
      "Taejun Park"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.00975"
  },
  {
    "id": "arXiv:2206.00976",
    "title": "Distributed Edge Coloring in Time Polylogarithmic in $\u0394$",
    "abstract": "We provide new deterministic algorithms for the edge coloring problem, which\nis one of the classic and highly studied distributed local symmetry breaking\nproblems. As our main result, we show that a $(2\\Delta-1)$-edge coloring can be\ncomputed in time $\\mathrm{poly}\\log\\Delta + O(\\log^* n)$ in the LOCAL model.\nThis improves a result of Balliu, Kuhn, and Olivetti [PODC '20], who gave an\nalgorithm with a quasi-polylogarithmic dependency on $\\Delta$. We further show\nthat in the CONGEST model, an $(8+\\varepsilon)\\Delta$-edge coloring can be\ncomputed in $\\mathrm{poly}\\log\\Delta + O(\\log^* n)$ rounds. The best previous\n$O(\\Delta)$-edge coloring algorithm that can be implemented in the CONGEST\nmodel is by Barenboim and Elkin [PODC '11] and it computes a\n$2^{O(1/\\varepsilon)}\\Delta$-edge coloring in time $O(\\Delta^\\varepsilon +\n\\log^* n)$ for any $\\varepsilon\\in(0,1]$.",
    "descriptor": "",
    "authors": [
      "Alkida Balliu",
      "Sebastian Brandt",
      "Fabian Kuhn",
      "Dennis Olivetti"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.00976"
  },
  {
    "id": "arXiv:2206.00979",
    "title": "Graph Kernels Based on Multi-scale Graph Embeddings",
    "abstract": "Graph kernels are conventional methods for computing graph similarities.\nHowever, most of the R-convolution graph kernels face two challenges: 1) They\ncannot compare graphs at multiple different scales, and 2) they do not consider\nthe distributions of substructures when computing the kernel matrix. These two\nchallenges limit their performances. To mitigate the two challenges, we propose\na novel graph kernel called the Multi-scale Path-pattern Graph kernel (MPG), at\nthe heart of which is the multi-scale path-pattern node feature map. Each\nelement of the path-pattern node feature map is the number of occurrences of a\npath-pattern around a node. A path-pattern is constructed by the concatenation\nof all the node labels in a path of a truncated BFS tree rooted at each node.\nSince the path-pattern node feature map can only compare graphs at local\nscales, we incorporate into it the multiple different scales of the graph\nstructure, which are captured by the truncated BFS trees of different depth. We\nuse the Wasserstein distance to compute the similarity between the multi-scale\npath-pattern node feature maps of two graphs, considering the distributions of\nsubstructures. We empirically validate MPG on various benchmark graph datasets\nand demonstrate that it achieves state-of-the-art performance.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Wei Ye",
      "Hao Tian",
      "Qijun Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00979"
  },
  {
    "id": "arXiv:2206.00983",
    "title": "On the Effectiveness of Knowledge Graph Embeddings: a Rule Mining  Approach",
    "abstract": "We study the effectiveness of Knowledge Graph Embeddings (KGE) for knowledge\ngraph (KG) completion with rule mining. More specifically, we mine rules from\nKGs before and after they have been completed by a KGE to compare possible\ndifferences in the rules extracted. We apply this method to classical KGEs\napproaches, in particular, TransE, DistMult and ComplEx. Our experiments\nindicate that there can be huge differences between the extracted rules,\ndepending on the KGE approach for KG completion. In particular, after the\nTransE completion, several spurious rules were extracted.",
    "descriptor": "",
    "authors": [
      "Johanna J\u00f8sang",
      "Ricardo Guimar\u00e3es",
      "Ana Ozaki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00983"
  },
  {
    "id": "arXiv:2206.00991",
    "title": "StopNet: Scalable Trajectory and Occupancy Prediction for Urban  Autonomous Driving",
    "abstract": "We introduce a motion forecasting (behavior prediction) method that meets the\nlatency requirements for autonomous driving in dense urban environments without\nsacrificing accuracy. A whole-scene sparse input representation allows StopNet\nto scale to predicting trajectories for hundreds of road agents with reliable\nlatency. In addition to predicting trajectories, our scene encoder lends itself\nto predicting whole-scene probabilistic occupancy grids, a complementary output\nrepresentation suitable for busy urban environments. Occupancy grids allow the\nAV to reason collectively about the behavior of groups of agents without\nprocessing their individual trajectories. We demonstrate the effectiveness of\nour sparse input representation and our model in terms of computation and\naccuracy over three datasets. We further show that co-training consistent\ntrajectory and occupancy predictions improves upon state-of-the-art performance\nunder standard metrics.",
    "descriptor": "",
    "authors": [
      "Jinkyu Kim",
      "Reza Mahjourian",
      "Scott Ettinger",
      "Mayank Bansal",
      "Brandyn White",
      "Ben Sapp",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00991"
  },
  {
    "id": "arXiv:2206.00994",
    "title": "A new fluid-based strategy for the connection of non-matching lattice  materials",
    "abstract": "We present a new algorithm for the design of the connection region between\ndifferent lattice materials. We solve a Stokes-type topology optimization\nproblem on a narrow morphing region to smoothly connect two different unit\ncells. The proposed procedure turns out to be effective and provides a local\nre-design of the materials, leading to a very mild modification of the\nmechanical behaviour characterizing the original lattices. The robustness of\nthe algorithm is assessed in terms of sensitivity of the final layout to\ndifferent parameters. Both the cases of Cartesian and non-Cartesian morphing\nregions are successfully investigated.",
    "descriptor": "",
    "authors": [
      "Nicola Ferro",
      "Simona Perotto",
      "Matteo Gavazzoni"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.00994"
  },
  {
    "id": "arXiv:2206.00995",
    "title": "Lie Complexity of Sturmian Words",
    "abstract": "Bell and Shallit recently introduced the Lie complexity of an infinite word s\nas the function counting for each length the number of conjugacy classes of\nwords whose elements are all factors of s. We give a formula for the Lie\ncomplexity of Sturmian words.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Alessandro De Luca",
      "Gabriele Fici"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.00995"
  },
  {
    "id": "arXiv:2206.00997",
    "title": "Is Mapping Necessary for Realistic PointGoal Navigation?",
    "abstract": "Can an autonomous agent navigate in a new environment without building an\nexplicit map?\nFor the task of PointGoal navigation ('Go to $\\Delta x$, $\\Delta y$') under\nidealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the\nanswer is a clear 'yes' - map-less neural models composed of task-agnostic\ncomponents (CNNs and RNNs) trained with large-scale reinforcement learning\nachieve 100% Success on a standard dataset (Gibson). However, for PointNav in a\nrealistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open\nquestion; one we tackle in this paper. The strongest published result for this\ntask is 71.7% Success.\nFirst, we identify the main (perhaps, only) cause of the drop in performance:\nthe absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D\nsensing and actuation noise achieves 99.8% Success (Gibson-v2 val). This\nsuggests that (to paraphrase a meme) robust visual odometry is all we need for\nrealistic PointNav; if we can achieve that, we can ignore the sensing and\nactuation noise.\nWith that as our operating hypothesis, we scale the dataset and model size,\nand develop human-annotation-free data-augmentation techniques to train models\nfor visual odometry. We advance the state of art on the Habitat Realistic\nPointNav Challenge from 71% to 94% Success (+32, 4% relative) and 53% to 74%\nSPL (+39, 6% relative). While our approach does not saturate or 'solve' this\ndataset, this strong improvement combined with promising zero-shot sim2real\ntransfer (to a LoCoBot) provides evidence consistent with the hypothesis that\nexplicit mapping may not be necessary for navigation, even in a realistic\nsetting.",
    "descriptor": "",
    "authors": [
      "Ruslan Partsey",
      "Erik Wijmans",
      "Naoki Yokoyama",
      "Oles Dobosevych",
      "Dhruv Batra",
      "Oleksandr Maksymets"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00997"
  },
  {
    "id": "arXiv:2206.01002",
    "title": "Introducing One Sided Margin Loss for Solving Classification Problems in  Deep Networks",
    "abstract": "This paper introduces a new loss function, OSM (One-Sided Margin), to solve\nmaximum-margin classification problems effectively. Unlike the hinge loss, in\nOSM the margin is explicitly determined with corresponding hyperparameters and\nthen the classification problem is solved. In experiments, we observe that\nusing OSM loss leads to faster training speeds and better accuracies than\nbinary and categorical cross-entropy in several commonly used deep models for\nclassification and optical character recognition problems.\nOSM has consistently shown better classification accuracies over\ncross-entropy and hinge losses for small to large neural networks. it has also\nled to a more efficient training procedure. We achieved state-of-the-art\naccuracies for small networks on several benchmark datasets of\nCIFAR10(98.82\\%), CIFAR100(91.56\\%), Flowers(98.04\\%), Stanford Cars(93.91\\%)\nwith considerable improvements over other loss functions. Moreover, the\naccuracies are rather better than cross-entropy and hinge loss for large\nnetworks. Therefore, we strongly believe that OSM is a powerful alternative to\nhinge and cross-entropy losses to train deep neural networks on classification\ntasks.",
    "descriptor": "",
    "authors": [
      "Ali Karimi",
      "Zahra Mousavi Kouzehkanan",
      "Reshad Hosseini",
      "Hadi Asheri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01002"
  },
  {
    "id": "arXiv:2206.01003",
    "title": "Shortest Path Networks for Graph Property Prediction",
    "abstract": "Most graph neural network models rely on a particular message passing\nparadigm, where the idea is to iteratively propagate node representations of a\ngraph to each node in the direct neighborhood. While very prominent, this\nparadigm leads to information propagation bottlenecks, as information is\nrepeatedly compressed at intermediary node representations, which causes loss\nof information, making it practically impossible to gather meaningful signals\nfrom distant nodes. To address this issue, we propose shortest path message\npassing neural networks, where the node representations of a graph are\npropagated to each node in the shortest path neighborhoods. In this setting,\nnodes can directly communicate between each other even if they are not\nneighbors, breaking the information bottleneck and hence leading to more\nadequately learned representations. Theoretically, our framework generalizes\nmessage passing neural networks, resulting in provably more expressive models.\nEmpirically, we verify the capacity of a basic model of this framework on\ndedicated synthetic experiments, and on real-world graph classification and\nregression benchmarks, obtaining several state-of-the-art results.",
    "descriptor": "",
    "authors": [
      "Ralph Abboud",
      "Radoslav Dimitrov",
      "\u0130smail \u0130lkan Ceylan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01003"
  },
  {
    "id": "arXiv:2206.01004",
    "title": "Nonlinear Equalization for Optical Communications Based on  Entropy-Regularized Mean Square Error",
    "abstract": "An entropy-regularized mean square error (MSE-X) cost function is proposed\nfor nonlinear equalization of short-reach optical channels. For a coherent\noptical transmission experiment, MSE-X achieves the same bit error rate as the\nstandard MSE cost function and a significantly higher achievable information\nrate.",
    "descriptor": "",
    "authors": [
      "Francesca Diedolo",
      "Georg B\u00f6cherer",
      "Maximilian Sch\u00e4dler",
      "Stefano Calabr\u00f3"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.01004"
  },
  {
    "id": "arXiv:2206.01008",
    "title": "Approximate Network Motif Mining Via Graph Learning",
    "abstract": "Frequent and structurally related subgraphs, also known as network motifs,\nare valuable features of many graph datasets. However, the high computational\ncomplexity of identifying motif sets in arbitrary datasets (motif mining) has\nlimited their use in many real-world datasets. By automatically leveraging\nstatistical properties of datasets, machine learning approaches have shown\npromise in several tasks with combinatorial complexity and are therefore a\npromising candidate for network motif mining. In this work we seek to\nfacilitate the development of machine learning approaches aimed at motif\nmining. We propose a formulation of the motif mining problem as a node\nlabelling task. In addition, we build benchmark datasets and evaluation metrics\nwhich test the ability of models to capture different aspects of motif\ndiscovery such as motif number, size, topology, and scarcity. Next, we propose\nMotiFiesta, a first attempt at solving this problem in a fully differentiable\nmanner with promising results on challenging baselines. Finally, we demonstrate\nthrough MotiFiesta that this learning setting can be applied simultaneously to\ngeneral-purpose data mining and interpretable feature extraction for graph\nclassification tasks.",
    "descriptor": "",
    "authors": [
      "Carlos Oliver",
      "Dexiong Chen",
      "Vincent Mallet",
      "Pericles Philippopoulos",
      "Karsten Borgwardt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01008"
  },
  {
    "id": "arXiv:2206.01009",
    "title": "Unified Recurrence Modeling for Video Action Anticipation",
    "abstract": "Forecasting future events based on evidence of current conditions is an\ninnate skill of human beings, and key for predicting the outcome of any\ndecision making. In artificial vision for example, we would like to predict the\nnext human action before it happens, without observing the future video frames\nassociated to it. Computer vision models for action anticipation are expected\nto collect the subtle evidence in the preamble of the target actions. In prior\nstudies recurrence modeling often leads to better performance, the strong\ntemporal inference is assumed to be a key element for reasonable prediction. To\nthis end, we propose a unified recurrence modeling for video action\nanticipation via message passing framework. The information flow in space-time\ncan be described by the interaction between vertices and edges, and the changes\nof vertices for each incoming frame reflects the underlying dynamics. Our model\nleverages self-attention as the building blocks for each of the message passing\nfunctions. In addition, we introduce different edge learning strategies that\ncan be end-to-end optimized to gain better flexibility for the connectivity\nbetween vertices. Our experimental results demonstrate that our proposed method\noutperforms previous works on the large-scale EPIC-Kitchen dataset.",
    "descriptor": "",
    "authors": [
      "Tsung-Ming Tai",
      "Giuseppe Fiameni",
      "Cheng-Kuang Lee",
      "Simon See",
      "Oswald Lanz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01009"
  },
  {
    "id": "arXiv:2206.01010",
    "title": "Long-tailed Recognition by Learning from Latent Categories",
    "abstract": "In this work, we address the challenging task of long-tailed image\nrecognition. Previous long-tailed recognition methods commonly focus on the\ndata augmentation or re-balancing strategy of the tail classes to give more\nattention to tail classes during the model training. However, due to the\nlimited training images for tail classes, the diversity of tail class images is\nstill restricted, which results in poor feature representations. In this work,\nwe hypothesize that common latent features among the head and tail classes can\nbe used to give better feature representation. Motivated by this, we introduce\na Latent Categories based long-tail Recognition (LCReg) method. Specifically,\nwe propose to learn a set of class-agnostic latent features shared among the\nhead and tail classes. Then, we implicitly enrich the training sample diversity\nvia applying semantic data augmentation to the latent features. Extensive\nexperiments on five long-tailed image recognition datasets demonstrate that our\nproposed LCReg is able to significantly outperform previous methods and achieve\nstate-of-the-art results.",
    "descriptor": "",
    "authors": [
      "Weide Liu",
      "Zhonghua Wu",
      "Yiming Wang",
      "Henghui Ding",
      "Fayao Liu",
      "Jie Lin",
      "Guosheng Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01010"
  },
  {
    "id": "arXiv:2206.01011",
    "title": "Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov  Decision Processes",
    "abstract": "Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes\na parameterized policy model for an expected return using gradient ascent.\nGiven a well-parameterized policy model, such as a neural network model, with\nappropriate initial parameters, the PG algorithms work well even when\nenvironment does not have the Markov property. Otherwise, they can be trapped\non a plateau or suffer from peakiness effects. As another successful RL\napproach, algorithms based on Monte-Carlo Tree Search (MCTS), which include\nAlphaZero, have obtained groundbreaking results especially on the board game\nplaying domain. They are also suitable to be applied to non-Markov decision\nprocesses. However, since the standard MCTS does not have the ability to learn\nstate representation, the size of the tree-search space can be too large to\nsearch. In this work, we examine a mixture policy of PG and MCTS to complement\neach other's difficulties and take advantage of them. We derive conditions for\nasymptotic convergence with results of a two-timescale stochastic approximation\nand propose an algorithm that satisfies these conditions. The effectivity of\nthe proposed methods is verified through numerical experiments on non-Markov\ndecision processes.",
    "descriptor": "",
    "authors": [
      "Tetsuro Morimura",
      "Kazuhiro Ota",
      "Kenshi Abe",
      "Peinan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01011"
  },
  {
    "id": "arXiv:2206.01014",
    "title": "Suggestive Annotation of Brain MR Images with Gradient-guided Sampling",
    "abstract": "Machine learning has been widely adopted for medical image analysis in recent\nyears given its promising performance in image segmentation and classification\ntasks. The success of machine learning, in particular supervised learning,\ndepends on the availability of manually annotated datasets. For medical imaging\napplications, such annotated datasets are not easy to acquire, it takes a\nsubstantial amount of time and resource to curate an annotated medical image\nset. In this paper, we propose an efficient annotation framework for brain MR\nimages that can suggest informative sample images for human experts to\nannotate. We evaluate the framework on two different brain image analysis\ntasks, namely brain tumour segmentation and whole brain segmentation.\nExperiments show that for brain tumour segmentation task on the BraTS 2019\ndataset, training a segmentation model with only 7% suggestively annotated\nimage samples can achieve a performance comparable to that of training on the\nfull dataset. For whole brain segmentation on the MALC dataset, training with\n42% suggestively annotated image samples can achieve a comparable performance\nto training on the full dataset. The proposed framework demonstrates a\npromising way to save manual annotation cost and improve data efficiency in\nmedical imaging applications.",
    "descriptor": "\nComments: Manuscript accepted by MedIA\n",
    "authors": [
      "Chengliang Dai",
      "Shuo Wang",
      "Yuanhan Mo",
      "Elsa Angelini",
      "Yike Guo",
      "Wenjia Bai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01014"
  },
  {
    "id": "arXiv:2206.01015",
    "title": "An extended range of stable flux reconstruction schemes on  quadrilaterals for various polynomial bases",
    "abstract": "An extended range of energy stable flux reconstruction schemes, developed\nusing a summation-by-parts approach, is presented on quadrilateral elements for\nvarious sets of polynomial bases. For the maximal order bases, a new set of\ncorrection functions which result in stable schemes is found. However, for a\nrange of orders it is shown that only a single correction function can be cast\nas a tensor-product. Subsequently, correction functions are identified using a\ngeneralised analytic framework that results in stable schemes for total order\nand approximate Euclidean order polynomial bases on quadrilaterals -- which\nhave not previously been explored in the context of flux reconstruction. It is\nshown that the approximate Euclidean order basis can provide similar numerical\naccuracy as the maximal order basis but with fewer points per element, and thus\nlower cost.",
    "descriptor": "",
    "authors": [
      "Will Trojak",
      "Rob Watson",
      "Peter Vincent"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.01015"
  },
  {
    "id": "arXiv:2206.01017",
    "title": "Structured Two-stream Attention Network for Video Question Answering",
    "abstract": "To date, visual question answering (VQA) (i.e., image QA and video QA) is\nstill a holy grail in vision and language understanding, especially for video\nQA. Compared with image QA that focuses primarily on understanding the\nassociations between image region-level details and corresponding questions,\nvideo QA requires a model to jointly reason across both spatial and long-range\ntemporal structures of a video as well as text to provide an accurate answer.\nIn this paper, we specifically tackle the problem of video QA by proposing a\nStructured Two-stream Attention network, namely STA, to answer a free-form or\nopen-ended natural language question about the content of a given video. First,\nwe infer rich long-range temporal structures in videos using our structured\nsegment component and encode text features. Then, our structured two-stream\nattention component simultaneously localizes important visual instance, reduces\nthe influence of background video and focuses on the relevant text. Finally,\nthe structured two-stream fusion component incorporates different segments of\nquery and video aware context representation and infers the answers.\nExperiments on the large-scale video QA dataset \\textit{TGIF-QA} show that our\nproposed method significantly surpasses the best counterpart (i.e., with one\nrepresentation for the video input) by 13.0%, 13.5%, 11.0% and 0.3 for Action,\nTrans., TrameQA and Count tasks. It also outperforms the best competitor (i.e.,\nwith two representations) on the Action, Trans., TrameQA tasks by 4.1%, 4.7%,\nand 5.1%.",
    "descriptor": "",
    "authors": [
      "Lianli Gao",
      "Pengpeng Zeng",
      "Jingkuan Song",
      "Yuan-Fang Li",
      "Wu Liu",
      "Tao Mei",
      "Heng Tao Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01017"
  },
  {
    "id": "arXiv:2206.01020",
    "title": "STORM: A Model for Sustainably Onboarding Software Testers",
    "abstract": "Recruiting and onboarding software testing professionals are complex and cost\nintensive activities. Whether onboarding is successful and sustainable depends\non both the employee as well as the organization and is influenced by a number\nof often highly individual factors. Therefore, we propose the Software Testing\nOnboarding Model (STORM) for sustainably onboarding software testing\nprofessionals based on existing frameworks and models taking into account\nonboarding processes, sustainability, and test processes. We provide detailed\ninstructions on how to use the model and apply it to real-world onboarding\nprocesses in two industrial case studies.",
    "descriptor": "",
    "authors": [
      "Tobias Lorey",
      "Stefan Mohacsi",
      "Armin Beer",
      "Michael Felderer"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.01020"
  },
  {
    "id": "arXiv:2206.01022",
    "title": "Learning Disentangled Representations for Counterfactual Regression via  Mutual Information Minimization",
    "abstract": "Learning individual-level treatment effect is a fundamental problem in causal\ninference and has received increasing attention in many areas, especially in\nthe user growth area which concerns many internet companies. Recently,\ndisentangled representation learning methods that decompose covariates into\nthree latent factors, including instrumental, confounding and adjustment\nfactors, have witnessed great success in treatment effect estimation. However,\nit remains an open problem how to learn the underlying disentangled factors\nprecisely. Specifically, previous methods fail to obtain independent\ndisentangled factors, which is a necessary condition for identifying treatment\neffect. In this paper, we propose Disentangled Representations for\nCounterfactual Regression via Mutual Information Minimization (MIM-DRCFR),\nwhich uses a multi-task learning framework to share information when learning\nthe latent factors and incorporates MI minimization learning criteria to ensure\nthe independence of these factors. Extensive experiments including public\nbenchmarks and real-world industrial user growth datasets demonstrate that our\nmethod performs much better than state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Mingyuan Cheng",
      "Xinru Liao",
      "Quan Liu",
      "Bin Ma",
      "Jian Xu",
      "Bo Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01022"
  },
  {
    "id": "arXiv:2206.01024",
    "title": "A Constraint and Object Oriented Fifth Generation Programming Language  and its Compiler and Runtime System",
    "abstract": "Since the advent of LISP, the fifth generation programming language has\ndeveloped for decades. However, compared with the fourth generation programming\nlanguage, the fifth generation programming language has not been widely used\nbecause of its obscure semantics, rigorous representation of problems, and\nlimited inference ability. For this reason, COOL (Constraint and Object Ordered\nLanguage), a fifth generation programming language proposed in this paper,\novercomes the problems of intuitive semantics, rigorous restrictions on\nhandling problem conditions, and improves the inference ability of previous\nfifth generation programming languages. Specific improvements are as follows:\nFirst, COOL supports process-oriented and object-oriented for easy application\nin production projects; Second, COOL supports expression as function\ndeclaration and function return, which improves language affinity for\nmathematical formulas, and supports embedding function parameters into function\nname strings to make function naming closer to natural languages. Make\nmathematical problems easier to describe; Third, COOL introduces a weighting\nmechanism and accelerates the inference process through cumulative weighting.\nFourth, COOL introduces the concepts of forward and reverse functions in\nprogramming so that computers can infer and execute problems with logical\nsequential constraints. Fifth, the computer can deduce the reverse solution\nprocess by using the forward solution process through the back-tracking\nalgorithm and the dynamic programming algorithm, so that the computer can\ndeduce the problem with time-sequential constraints. Sixth, the pre-execution\nstep is introduced to separate the inference and function query process of the\nprogram from the execution process, so as to improve the execution speed of the\nprogram.",
    "descriptor": "\nComments: 22 pages, 8 figures, submitted to Chinese Journal of Computers\n",
    "authors": [
      "Han Jipeng",
      "Lichen Zhihang"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.01024"
  },
  {
    "id": "arXiv:2206.01028",
    "title": "Impact of Sampling on Locally Differentially Private Data Collection",
    "abstract": "With the recent bloom of data, there is a huge surge in threats against\nindividuals' private information. Various techniques for optimizing\nprivacy-preserving data analysis are at the focus of research in the recent\nyears. In this paper, we analyse the impact of sampling on the utility of the\nstandard techniques of frequency estimation, which is at the core of\nlarge-scale data analysis, of the locally deferentially private data-release\nunder a pure protocol. We study the case in a distributed environment of data\nsharing where the values are reported by various nodes to the central server,\ne.g., cross-device Federated Learning. We show that if we introduce some random\nsampling of the nodes in order to reduce the cost of communication, the\nstandard existing estimators fail to remain unbiased. We propose a new unbiased\nestimator in the context of sampling each node with certain probability and\ncompute various statistical summaries of the data using it. We propose a way of\nsampling each node with personalized sampling probabilities as a step to\nfurther generalisation, which leads to some interesting open questions in the\nend. We analyse the accuracy of our proposed estimators on synthetic datasets\nto gather some insight on the trade-off between communication cost, privacy,\nand utility.",
    "descriptor": "",
    "authors": [
      "Sayan Biswas",
      "Graham Cormode",
      "Carsten Maple"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01028"
  },
  {
    "id": "arXiv:2206.01032",
    "title": "A more abstract bounded exploration postulate",
    "abstract": "In article \"Sequential abstract state machines capture sequential\nalgorithms\", one of us axiomatized sequential algorithms by means of three\npostulates: sequential time, abstract state, and bounded exploration\npostulates. Here we give a more abstract version of the bounded exploration\npostulate which is closer in spirit to the abstract state postulate. In the\npresence of the sequential time and abstract state postulates, our postulate is\nequivalent to the original bounded exploration postulate.",
    "descriptor": "",
    "authors": [
      "Yuri Gurevich",
      "Tatiana Yavorskaya"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.01032"
  },
  {
    "id": "arXiv:2206.01034",
    "title": "Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to  DNNs",
    "abstract": "Most existing deep neural networks (DNNs) are easily disturbed by slight\nnoise. As far as we know, there are few researches on physical adversarial\nattack technology by deploying lighting equipment. The light-based physical\nadversarial attack technology has excellent covertness, which brings great\nsecurity risks to many applications based on deep neural networks (such as\nautomatic driving technology). Therefore, we propose a robust physical\nadversarial attack technology with excellent covertness, called adversarial\nlaser point (AdvLS), which optimizes the physical parameters of laser point\nthrough genetic algorithm to perform physical adversarial attack. It realizes\nrobust and covert physical adversarial attack by using low-cost laser\nequipment. As far as we know, AdvLS is the first light-based adversarial attack\ntechnology that can perform physical adversarial attacks in the daytime. A\nlarge number of experiments in the digital and physical environments show that\nAdvLS has excellent robustness and concealment. In addition, through in-depth\nanalysis of the experimental data, we find that the adversarial perturbations\ngenerated by AdvLS have superior adversarial attack migration. The experimental\nresults show that AdvLS impose serious interference to the advanced deep neural\nnetworks, we call for the attention of the proposed physical adversarial attack\ntechnology.",
    "descriptor": "",
    "authors": [
      "Chengyin Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01034"
  },
  {
    "id": "arXiv:2206.01038",
    "title": "A Survey on Video Action Recognition in Sports: Datasets, Methods and  Applications",
    "abstract": "To understand human behaviors, action recognition based on videos is a common\napproach. Compared with image-based action recognition, videos provide much\nmore information. Reducing the ambiguity of actions and in the last decade,\nmany works focused on datasets, novel models and learning approaches have\nimproved video action recognition to a higher level. However, there are\nchallenges and unsolved problems, in particular in sports analytics where data\ncollection and labeling are more sophisticated, requiring sport professionals\nto annotate data. In addition, the actions could be extremely fast and it\nbecomes difficult to recognize them. Moreover, in team sports like football and\nbasketball, one action could involve multiple players, and to correctly\nrecognize them, we need to analyse all players, which is relatively\ncomplicated. In this paper, we present a survey on video action recognition for\nsports analytics. We introduce more than ten types of sports, including team\nsports, such as football, basketball, volleyball, hockey and individual sports,\nsuch as figure skating, gymnastics, table tennis, tennis, diving and badminton.\nThen we compare numerous existing frameworks for sports analysis to present\nstatus quo of video action recognition in both team sports and individual\nsports. Finally, we discuss the challenges and unsolved problems in this area\nand to facilitate sports analytics, we develop a toolbox using PaddlePaddle,\nwhich supports football, basketball, table tennis and figure skating action\nrecognition.",
    "descriptor": "\nComments: 26 pages. The toolbox is available at this https URL\n",
    "authors": [
      "Fei Wu",
      "Qingzhong Wang",
      "Jian Bian",
      "Haoyi Xiong",
      "Ning Ding",
      "Feixiang Lu",
      "Jun Cheng",
      "Dejing Dou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.01038"
  },
  {
    "id": "arXiv:2206.01041",
    "title": "End-to-End Security for Distributed Event-Driven Enclave Applications on  Heterogeneous TEEs",
    "abstract": "This paper presents an approach to provide strong assurance of the secure\nexecution of distributed event-driven applications on shared infrastructures,\nwhile relying on a small Trusted Computing Base. We build upon and extend\nsecurity primitives provided by Trusted Execution Environments (TEEs) to\nguarantee authenticity and integrity properties of applications, and to secure\ncontrol of input and output devices. More specifically, we guarantee that if an\noutput is produced by the application, it was allowed to be produced by the\napplication's source code based on an authentic trace of inputs.\nWe present an integrated open-source framework to develop, deploy, and use\nsuch applications across heterogeneous TEEs. Beyond authenticity and integrity,\nour framework optionally provides confidentiality and a notion of availability,\nand facilitates software development at a high level of abstraction over the\nplatform-specific TEE layer. We support event-driven programming to develop\ndistributed enclave applications in Rust and C for heterogeneous TEE, including\nIntel SGX, ARM TrustZone and Sancus.\nIn this article we discuss the workings of our approach, the extensions we\nmade to the Sancus processor, and the integration of our development model with\ncommercial TEEs. Our evaluation of security and performance aspects show that\nTEEs, together with our programming model, form a basis for powerful security\narchitectures for dependable systems in domains such as Industrial Control\nSystems and the Internet of Things, illustrating our framework's unique\nsuitability for a broad range of use cases which combine cloud processing,\nmobile and edge devices, and lightweight sensing and actuation.",
    "descriptor": "\nComments: 35 pages, submitted to ACM Transactions on Privacy and Security, first co-authorship between Gianluca Scopelliti and Sepideh Pouyanrad, source code available at this https URL\n",
    "authors": [
      "Gianluca Scopelliti",
      "Sepideh Pouyanrad",
      "Job Noorman",
      "Fritz Alder",
      "Christoph Baumann",
      "Frank Piessens",
      "Jan Tobias M\u00fchlberg"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01041"
  },
  {
    "id": "arXiv:2206.01044",
    "title": "Artificial Open World for Evaluating AGI: a Conceptual Design",
    "abstract": "How to evaluate Artificial General Intelligence (AGI) is a critical problem\nthat is discussed and unsolved for a long period. In the research of narrow AI,\nthis seems not a severe problem, since researchers in that field focus on some\nspecific problems as well as one or some aspects of cognition, and the criteria\nfor evaluation are explicitly defined. By contrast, an AGI agent should solve\nproblems that are never-encountered by both agents and developers. However,\nonce a developer tests and debugs the agent with a problem, the\nnever-encountered problem becomes the encountered problem, as a result, the\nproblem is solved by the developers to some extent, exploiting their\nexperience, rather than the agents. This conflict, as we call the trap of\ndevelopers' experience, leads to that this kind of problems is probably hard to\nbecome an acknowledged criterion. In this paper, we propose an evaluation\nmethod named Artificial Open World, aiming to jump out of the trap. The\nintuition is that most of the experience in the actual world should not be\nnecessary to be applied to the artificial world, and the world should be open\nin some sense, such that developers are unable to perceive the world and solve\nproblems by themselves before testing, though after that they are allowed to\ncheck all the data. The world is generated in a similar way as the actual\nworld, and a general form of problems is proposed. A metric is proposed aiming\nto quantify the progress of research. This paper describes the conceptual\ndesign of the Artificial Open World, though the formalization and the\nimplementation are left to the future.",
    "descriptor": "",
    "authors": [
      "Bowen Xu",
      "Quansheng Ren"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01044"
  },
  {
    "id": "arXiv:2206.01051",
    "title": "Multi-stage Moving Target Defense: A Security-enhanced D-FACTS  Implementation Approach",
    "abstract": "In recent studies, moving target defense (MTD) has been applied to detect\nfalse data injection (FDI) attacks using distributed flexible AC transmission\nsystem (D-FACTS) devices. However, the inherent conflict between the security\ngoals of MTD (i.e., detecting FDI attacks) and the economic goals of D-FACTS\ndevices (i.e., reducing power losses) would impede the application of MTD in\nreal systems. Moreover, the detection capabilities of existing MTDs are often\ninsufficient. This paper proposes a multi-stage MTD (MMTD) approach to resolve\nthese two issues by adding a group of designed security-oriented schemes before\nD-FACTS' economic-oriented scheme to detect FDI attacks. We keep these\nsecurity-oriented schemes for a very short time interval and then revert to the\neconomic-oriented scheme for the remaining time to ensure the economic\nrequirements. We prove that a designed MMTD can significantly improve the\ndetection capability compared to existing one-stage MTDs. We find the supremum\nof MMTD's detection capability and study its relationship with system topology\nand D-FACTS deployment. Meanwhile, a greedy algorithm is proposed to search the\nMMTD strategy to reach this supremum. Simulation results show that the proposed\nMMTD can achieve the supremum against FDI attacks while outperforming current\nMTD strategies on economic indicators.",
    "descriptor": "\nComments: 10 pages, 10 figures\n",
    "authors": [
      "Jiazhou Wang",
      "Jue Tian",
      "Yang Liu",
      "Xiaohong Guan",
      "Dong Yang",
      "Ting Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.01051"
  },
  {
    "id": "arXiv:2206.01061",
    "title": "FV-UPatches: Enhancing Universality in Finger Vein Recognition",
    "abstract": "Many deep learning-based models have been introduced in finger vein\nrecognition in recent years. These solutions, however, suffer from data\ndependency and are difficult to achieve model generalization. To address this\nproblem, we are inspired by the idea of domain adaptation and propose a\nuniversal learning-based framework, which achieves generalization while\ntraining with limited data. To reduce differences between data distributions, a\ncompressed U-Net is introduced as a domain mapper to map the raw region of\ninterest image onto a target domain. The concentrated target domain is a\nunified feature space for the subsequent matching, in which a local descriptor\nmodel SOSNet is employed to embed patches into descriptors measuring the\nsimilarity of matching pairs. In the proposed framework, the domain mapper is\nan approximation to a specific extraction function thus the training is only a\none-time effort with limited data. Moreover, the local descriptor model can be\ntrained to be representative enough based on a public dataset of\nnon-finger-vein images. The whole pipeline enables the framework to be well\ngeneralized, making it possible to enhance universality and helps to reduce\ncosts of data collection, tuning and retraining. The comparable experimental\nresults to state-of-the-art (SOTA) performance in five public datasets prove\nthe effectiveness of the proposed framework. Furthermore, the framework shows\napplication potential in other vein-based biometric recognition as well.",
    "descriptor": "",
    "authors": [
      "Ziyan Chen",
      "Jiazhen Liu",
      "Changwen Cao",
      "Changlong Jin",
      "Hakil Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01061"
  },
  {
    "id": "arXiv:2206.01062",
    "title": "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis",
    "abstract": "Accurate document layout analysis is a key requirement for high-quality PDF\ndocument conversion. With the recent availability of public, large ground-truth\ndatasets such as PubLayNet and DocBank, deep-learning models have proven to be\nvery effective at layout detection and segmentation. While these datasets are\nof adequate size to train such models, they severely lack in layout variability\nsince they are sourced from scientific article repositories such as PubMed and\narXiv only. Consequently, the accuracy of the layout segmentation drops\nsignificantly when these models are applied on more challenging and diverse\nlayouts. In this paper, we present \\textit{DocLayNet}, a new, publicly\navailable, document-layout annotation dataset in COCO format. It contains 80863\nmanually annotated pages from diverse data sources to represent a wide\nvariability in layouts. For each PDF page, the layout annotations provide\nlabelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also\nprovides a subset of double- and triple-annotated pages to determine the\ninter-annotator agreement. In multiple experiments, we provide baseline\naccuracy scores (in mAP) for a set of popular object detection models. We also\ndemonstrate that these models fall approximately 10\\% behind the\ninter-annotator agreement. Furthermore, we provide evidence that DocLayNet is\nof sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and\nDocLayNet, showing that layout predictions of the DocLayNet-trained models are\nmore robust and thus the preferred choice for general-purpose document-layout\nanalysis.",
    "descriptor": "\nComments: 9 pages, 6 figures, 5 tables. Accepted paper at SIGKDD 2022 conference\n",
    "authors": [
      "Birgit Pfitzmann",
      "Christoph Auer",
      "Michele Dolfi",
      "Ahmed S Nassar",
      "Peter W J Staar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01062"
  },
  {
    "id": "arXiv:2206.01067",
    "title": "Practical Adversarial Multivalid Conformal Prediction",
    "abstract": "We give a simple, generic conformal prediction method for sequential\nprediction that achieves target empirical coverage guarantees against\nadversarially chosen data. It is computationally lightweight -- comparable to\nsplit conformal prediction -- but does not require having a held-out validation\nset, and so all data can be used for training models from which to derive a\nconformal score. It gives stronger than marginal coverage guarantees in two\nways. First, it gives threshold calibrated prediction sets that have correct\nempirical coverage even conditional on the threshold used to form the\nprediction set from the conformal score. Second, the user can specify an\narbitrary collection of subsets of the feature space -- possibly intersecting\n-- and the coverage guarantees also hold conditional on membership in each of\nthese subsets. We call our algorithm MVP, short for MultiValid Prediction. We\ngive both theory and an extensive set of empirical evaluations.",
    "descriptor": "\nComments: Code for our experiments can be found at: this https URL\n",
    "authors": [
      "Osbert Bastani",
      "Varun Gupta",
      "Christopher Jung",
      "Georgy Noarov",
      "Ramya Ramalingam",
      "Aaron Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01067"
  },
  {
    "id": "arXiv:2206.01071",
    "title": "Partitura: A Python Package for Symbolic Music Processing",
    "abstract": "Partitura is a lightweight Python package for handling symbolic musical\ninformation. It provides easy access to features commonly used in music\ninformation retrieval tasks, like note arrays (lists of timed pitched events)\nand 2D piano roll matrices, as well as other score elements such as time and\nkey signatures, performance directives, and repeat structures. Partitura can\nload musical scores (in MEI, MusicXML, Kern, and MIDI formats), MIDI\nperformances, and score-to-performance alignments. The package includes some\ntools for music analysis, such as automatic pitch spelling, key signature\nidentification, and voice separation. Partitura is an open-source project and\nis available at https://github.com/CPJKU/partitura/.",
    "descriptor": "",
    "authors": [
      "Carlos Cancino-Chac\u00f3n",
      "Silvan David Peter",
      "Emmanouil Karystinaios",
      "Francesco Foscarin",
      "Maarten Grachten",
      "Gerhard Widmer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Digital Libraries (cs.DL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.01071"
  },
  {
    "id": "arXiv:2206.01077",
    "title": "The Power of Amortized Recourse for Online Graph Problems",
    "abstract": "In this work, we study graph problems with monotone-sum objectives. We\npropose a general two-fold greedy algorithm that references\n$\\alpha$-approximation algorithms (where $\\alpha \\ge 1$) to achieve $(t \\cdot\n\\alpha)$-competitiveness while incurring at most\n$\\frac{w_{\\text{max}}\\cdot(t+1)}{\\min\\{1, w_\\text{min}\\}\\cdot(t-1)}$ amortized\nrecourse, where $w_{\\text{max}}$ and $w_{\\text{min}}$ are the largest value and\nthe smallest positive value that can be assigned to an element in the sum. We\nfurther refine this trade-off between competitive ratio and amortized recourse\nfor three classical graph problems. For Independent Set, we refine the analysis\nof our general algorithm and show that $t$-competitiveness can be achieved with\n$\\frac{t}{t-1}$ amortized recourse. For Maximum Matching, we use an existing\nalgorithm with limited greed to show that $t$-competitiveness can be achieved\nwith $\\frac{(2-t^*)}{(t^*-1)(3-t^*)}+\\frac{t^*-1}{3-t^*}$ amortized recourse,\nwhere $t^*$ is the largest number such that $t^*= 1 +\\frac{1}{j} \\leq t$ for\nsome integer $j$. For Vertex Cover, we introduce a polynomial-time algorithm\nthat further limits greed to show that $(2 -\n\\frac{2}{\\texttt{OPT}})$-competitiveness, where $\\texttt{OPT}$ is the size of\nthe optimal vertex cover, can be achieved with at most $\\frac{10}{3}$ amortized\nrecourse by a potential function argument. We remark that this online result\ncan be used as an offline approximation result (without violating the unique\ngames conjecture) to improve upon that of Monien and Speckenmeyer for graphs\ncontaining odd cycles of length no less than $2k + 3$, using an algorithm that\nis also constructive.",
    "descriptor": "",
    "authors": [
      "Hsiang-Hsuan Liu",
      "Jonathan Toole-Charignon"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.01077"
  },
  {
    "id": "arXiv:2206.01078",
    "title": "Deep Transformer Q-Networks for Partially Observable Reinforcement  Learning",
    "abstract": "Real-world reinforcement learning tasks often involve some form of partial\nobservability where the observations only give a partial or noisy view of the\ntrue state of the world. Such tasks typically require some form of memory,\nwhere the agent has access to multiple past observations, in order to perform\nwell. One popular way to incorporate memory is by using a recurrent neural\nnetwork to access the agent's history. However, recurrent neural networks in\nreinforcement learning are often fragile and difficult to train, susceptible to\ncatastrophic forgetting and sometimes fail completely as a result. In this\nwork, we propose Deep Transformer Q-Networks (DTQN), a novel architecture\nutilizing transformers and self-attention to encode an agent's history. DTQN is\ndesigned modularly, and we compare results against several modifications to our\nbase model. Our experiments demonstrate the transformer can solve partially\nobservable tasks faster and more stably than previous recurrent approaches.",
    "descriptor": "",
    "authors": [
      "Kevin Esslinger",
      "Robert Platt",
      "Christopher Amato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01078"
  },
  {
    "id": "arXiv:2206.01079",
    "title": "When does return-conditioned supervised learning work for offline  reinforcement learning?",
    "abstract": "Several recent works have proposed a class of algorithms for the offline\nreinforcement learning (RL) problem that we will refer to as return-conditioned\nsupervised learning (RCSL). RCSL algorithms learn the distribution of actions\nconditioned on both the state and the return of the trajectory. Then they\ndefine a policy by conditioning on achieving high return. In this paper, we\nprovide a rigorous study of the capabilities and limitations of RCSL, something\nwhich is crucially missing in previous work. We find that RCSL returns the\noptimal policy under a set of assumptions that are stronger than those needed\nfor the more traditional dynamic programming-based algorithms. We provide\nspecific examples of MDPs and datasets that illustrate the necessity of these\nassumptions and the limits of RCSL. Finally, we present empirical evidence that\nthese limitations will also cause issues in practice by providing illustrative\nexperiments in simple point-mass environments and on datasets from the D4RL\nbenchmark.",
    "descriptor": "",
    "authors": [
      "David Brandfonbrener",
      "Alberto Bietti",
      "Jacob Buckman",
      "Romain Laroche",
      "Joan Bruna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01079"
  },
  {
    "id": "arXiv:2206.01081",
    "title": "Revisiting the General Identifiability Problem",
    "abstract": "We revisit the problem of general identifiability originally introduced in\n[Lee et al., 2019] for causal inference and note that it is necessary to add\npositivity assumption of observational distribution to the original definition\nof the problem. We show that without such an assumption the rules of\ndo-calculus and consequently the proposed algorithm in [Lee et al., 2019] are\nnot sound. Moreover, adding the assumption will cause the completeness proof in\n[Lee et al., 2019] to fail. Under positivity assumption, we present a new\nalgorithm that is provably both sound and complete. A nice property of this new\nalgorithm is that it establishes a connection between general identifiability\nand classical identifiability by Pearl [1995] through decomposing the general\nidentifiability problem into a series of classical identifiability\nsub-problems.",
    "descriptor": "\nComments: 22 pages, 4 figures, 2 Algorithms, UAI 2022\n",
    "authors": [
      "Yaroslav Kivva",
      "Ehsan Mokhtarian",
      "Jalal Etesami",
      "Negar Kiyavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01081"
  },
  {
    "id": "arXiv:2206.01085",
    "title": "Incorporating Explicit Uncertainty Estimates into Deep Offline  Reinforcement Learning",
    "abstract": "Most theoretically motivated work in the offline reinforcement learning\nsetting requires precise uncertainty estimates. This requirement restricts the\nalgorithms derived in that work to the tabular and linear settings where such\nestimates exist. In this work, we develop a novel method for incorporating\nscalable uncertainty estimates into an offline reinforcement learning algorithm\ncalled deep-SPIBB that extends the SPIBB family of algorithms to environments\nwith larger state and action spaces. We use recent innovations in uncertainty\nestimation from the deep learning community to get more scalable uncertainty\nestimates to plug into deep-SPIBB. While these uncertainty estimates do not\nallow for the same theoretical guarantees as in the tabular case, we argue that\nthe SPIBB mechanism for incorporating uncertainty is more robust and flexible\nthan pessimistic approaches that incorporate the uncertainty as a value\nfunction penalty. We bear this out empirically, showing that deep-SPIBB\noutperforms pessimism based approaches with access to the same uncertainty\nestimates and performs at least on par with a variety of other strong baselines\nacross several environments and datasets.",
    "descriptor": "",
    "authors": [
      "David Brandfonbrener",
      "Remi Tachet des Combes",
      "Romain Laroche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01085"
  },
  {
    "id": "arXiv:2206.01087",
    "title": "Enriching a Fashion Knowledge Graph from Product Textual Descriptions",
    "abstract": "Knowledge Graphs offer a very useful and powerful structure for representing\ninformation, consequently, they have been adopted as the backbone for many\napplications in e-commerce scenarios. In this paper, we describe an application\nof existing techniques for enriching thelarge-scale Fashion Knowledge Graph\n(FKG) that we build at Farfetch. In particular, we apply techniques for named\nentity recognition (NER) and entity linking (EL) in order to extract and link\nrich metadata from product textual descriptions to entities in the FKG. Having\na complete and enriched FKG as an e-commerce backbone can have a highly\nvaluable impact on downstream applications such as search and recommendations.\nHowever, enriching a Knowledge Graph in the fashion domain has its own\nchallenges. Data representation is different from a more generic KG, like\nWikidata and Yago, as entities (e.g. product attributes) are too specific to\nthe domain, and long textual descriptions are not readily available. Data\nitself is also scarce, as labelling datasets to train supervised models is a\nvery laborious task. Even more, fashion products display a high variability and\nrequire an intricate ontology of attributes to link to. We use a transfer\nlearning based approach to train an NER module on a small amount of manually\nlabeled data, followed by an EL module that links the previously identified\nnamed entities to the appropriate entities within the FKG. Experiments using a\npre-trained model show that it is possible to achieve 89.75% accuracy in NER\neven with a small manually labeled dataset. Moreover, the EL module, despite\nrelying on simple rule-based or ML models (due to lack of training data), is\nable to link relevant attributes to products, thus automatically enriching the\nFKG.",
    "descriptor": "\nComments: Presented at the International Workshop on Knowledge Graph Generation from Text (ESWC 2022)\n",
    "authors": [
      "Jo\u00e3o Barroca",
      "Abhishek Shivkumar",
      "Beatriz Quintino Ferreira",
      "Evgeny Sherkhonov",
      "Jo\u00e3o Faria"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.01087"
  },
  {
    "id": "arXiv:2206.01094",
    "title": "A DTCWT-SVD Based Video Watermarking resistant to frame rate conversion",
    "abstract": "Videos can be easily tampered, copied and redistributed by attackers for\nillegal and monetary usage. Such behaviors severely jeopardize the interest of\ncontent owners. Despite huge efforts made in digital video watermarking for\ncopyright protection, typical distortions in video transmission including\nsignal attacks, geometric attacks and temporal synchronization attacks can\nstill easily erase the embedded signal. Among them, temporal synchronization\nattacks which include frame dropping, frame insertion and frame rate conversion\nis one of the most prevalent attacks. To address this issue, we present a new\nvideo watermarking based on joint Dual-Tree Cosine Wavelet Transformation\n(DTCWT) and Singular Value Decomposition (SVD), which is resistant to frame\nrate conversion. We first extract a set of candidate coefficient by applying\nSVD decomposition after DTCWT transform. Then, we simulate the watermark\nembedding by adjusting the shape of candidate coefficient. Finally, we perform\ngroup-level watermarking that includes moderate temporal redundancy to resist\ntemporal desynchronization attacks. Extensive experimental results show that\nthe proposed scheme is more resilient to temporal desynchronization attacks and\nperforms better than the existing blind video watermarking schemes.",
    "descriptor": "",
    "authors": [
      "Yifei Wang",
      "Qichao Ying",
      "Zhenxing Qian",
      "Sheng Li",
      "Xinpeng Zhang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01094"
  },
  {
    "id": "arXiv:2206.01101",
    "title": "Weakly Supervised Representation Learning with Sparse Perturbations",
    "abstract": "The theory of representation learning aims to build methods that provably\ninvert the data generating process with minimal domain knowledge or any source\nof supervision. Most prior approaches require strong distributional assumptions\non the latent variables and weak supervision (auxiliary information such as\ntimestamps) to provide provable identification guarantees. In this work, we\nshow that if one has weak supervision from observations generated by sparse\nperturbations of the latent variables--e.g. images in a reinforcement learning\nenvironment where actions move individual sprites--identification is achievable\nunder unknown continuous latent distributions. We show that if the\nperturbations are applied only on mutually exclusive blocks of latents, we\nidentify the latents up to those blocks. We also show that if these\nperturbation blocks overlap, we identify latents up to the smallest blocks\nshared across perturbations. Consequently, if there are blocks that intersect\nin one latent variable only, then such latents are identified up to permutation\nand scaling. We propose a natural estimation procedure based on this theory and\nillustrate it on low-dimensional synthetic and image-based experiments.",
    "descriptor": "",
    "authors": [
      "Kartik Ahuja",
      "Jason Hartford",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01101"
  },
  {
    "id": "arXiv:2206.01102",
    "title": "A temporal chrominance trigger for clean-label backdoor attack against  anti-spoof rebroadcast detection",
    "abstract": "We propose a stealthy clean-label video backdoor attack against Deep Learning\n(DL)-based models aiming at detecting a particular class of spoofing attacks,\nnamely video rebroadcast attacks. The injected backdoor does not affect\nspoofing detection in normal conditions, but induces a misclassification in the\npresence of a specific triggering signal. The proposed backdoor relies on a\ntemporal trigger altering the average chrominance of the video sequence. The\nbackdoor signal is designed by taking into account the peculiarities of the\nHuman Visual System (HVS) to reduce the visibility of the trigger, thus\nincreasing the stealthiness of the backdoor. To force the network to look at\nthe presence of the trigger in the challenging clean-label scenario, we choose\nthe poisoned samples used for the injection of the backdoor following a\nso-called Outlier Poisoning Strategy (OPS). According to OPS, the triggering\nsignal is inserted in the training samples that the network finds more\ndifficult to classify. The effectiveness of the proposed backdoor attack and\nits generality are validated experimentally on different datasets and\nanti-spoofing rebroadcast detection architectures.",
    "descriptor": "",
    "authors": [
      "Wei Guo",
      "Benedetta Tondi",
      "Mauro Barni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01102"
  },
  {
    "id": "arXiv:2206.01104",
    "title": "The match file format: Encoding Alignments between Scores and  Performances",
    "abstract": "This paper presents the specifications of match: a file format that extends a\nMIDI human performance with note-, beat-, and downbeat-level alignments to a\ncorresponding musical score. This enables advanced analyses of the performance\nthat are relevant for various tasks, such as expressive performance modeling,\nscore following, music transcription, and performer classification. The match\nfile includes a set of score-related descriptors that makes it usable also as a\nbare-bones score representation. For applications that require the use of\nstructural score elements (e.g., voices, parts, beams, slurs), the match file\ncan be easily combined with the symbolic score. To support the practical\napplication of our work, we release a corrected and upgraded version of the\nVienna4x22 dataset of scores and performances aligned with match files.",
    "descriptor": "",
    "authors": [
      "Francesco Foscarin",
      "Emmanouil Karystinaios",
      "Silvan David Peter",
      "Carlos Cancino-Chac\u00f3n",
      "Maarten Grachten",
      "Gerhard Widmer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Digital Libraries (cs.DL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.01104"
  },
  {
    "id": "arXiv:2206.01106",
    "title": "Robustness to Label Noise Depends on the Shape of the Noise Distribution  in Feature Space",
    "abstract": "Machine learning classifiers have been demonstrated, both empirically and\ntheoretically, to be robust to label noise under certain conditions -- notably\nthe typical assumption is that label noise is independent of the features given\nthe class label. We provide a theoretical framework that generalizes beyond\nthis typical assumption by modeling label noise as a distribution over feature\nspace. We show that both the scale and the shape of the noise distribution\ninfluence the posterior likelihood; and the shape of the noise distribution has\na stronger impact on classification performance if the noise is concentrated in\nfeature space where the decision boundary can be moved. For the special case of\nuniform label noise (independent of features and the class label), we show that\nthe Bayes optimal classifier for $c$ classes is robust to label noise until the\nratio of noisy samples goes above $\\frac{c-1}{c}$ (e.g. 90% for 10 classes),\nwhich we call the tipping point. However, for the special case of\nclass-dependent label noise (independent of features given the class label),\nthe tipping point can be as low as 50%. Most importantly, we show that when the\nnoise distribution targets decision boundaries (label noise is directly\ndependent on feature space), classification robustness can drop off even at a\nsmall scale of noise. Even when evaluating recent label-noise mitigation\nmethods we see reduced accuracy when label noise is dependent on features.\nThese findings explain why machine learning often handles label noise well if\nthe noise distribution is uniform in feature-space; yet it also points to the\ndifficulty of overcoming label noise when it is concentrated in a region of\nfeature space where a decision boundary can move.",
    "descriptor": "",
    "authors": [
      "Diane Oyen",
      "Michal Kucer",
      "Nick Hengartner",
      "Har Simrat Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01106"
  },
  {
    "id": "arXiv:2206.01109",
    "title": "Game-theoretic Utility Tree for Multi-Robot Cooperative Pursuit Strategy",
    "abstract": "Underlying relationships among multiagent systems (MAS) in hazardous\nscenarios can be represented as game-theoretic models. In adversarial\nenvironments, the adversaries can be intentional or unintentional based on\ntheir needs and motivations. Agents will adopt suitable decision-making\nstrategies to maximize their current needs and minimize their expected costs.\nThis paper proposes and extends the new hierarchical network-based model,\ntermed Game-theoretic Utility Tree (GUT), to arrive at a cooperative pursuit\nstrategy to catch an evader in the Pursuit-Evasion game domain. We verify and\ndemonstrate the performance of the proposed method using the Robotarium\nplatform compared to the conventional constant bearing (CB) and pure pursuit\n(PP) strategies. The experiments demonstrated the effectiveness of the GUT, and\nthe performances validated that the GUT could effectively organize cooperation\nstrategies, helping the group with fewer advantages achieve higher performance.",
    "descriptor": "",
    "authors": [
      "Qin Yang",
      "Ramviyas Parasuraman"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.01109"
  },
  {
    "id": "arXiv:2206.01111",
    "title": "MorphQ: Metamorphic Testing of Quantum Computing Platforms",
    "abstract": "As quantum computing is becoming increasingly popular, the underlying quantum\ncomputing platforms are growing both in ability and complexity. This growth may\ncause bugs in the platforms, which hinders the adoption of quantum computing.\nUnfortunately, testing quantum computing platforms is challenging due to the\nrelatively small number of existing quantum programs and because of the oracle\nproblem, i.e., a lack of specifications of the expected behavior of programs.\nThis paper presents MorphQ, the first metamorphic testing approach for quantum\ncomputing platforms. Our two key contributions are (i) a program generator that\ncreates a large and diverse set of valid (i.e., non-crashing) quantum programs,\nand (ii) set of program transformations that exploit quantum-specific\nmetamorphic relationships to alleviate the oracle problem. Evaluating the\napproach by testing the popular Qiskit platform shows that the approach creates\nover 50k program pairs within two days, many of which expose crashes.\nInspecting the crashes, we find twelve bugs, eight of which have already been\nconfirmed. MorphQ widens the slim portfolio of testing techniques of quantum\ncomputing platforms, helping to create a reliable software stack for this\nincreasingly important field.",
    "descriptor": "\nComments: 12 pages, 13 figures\n",
    "authors": [
      "Matteo Paltenghi",
      "Michael Pradel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.01111"
  },
  {
    "id": "arXiv:2206.01121",
    "title": "The Loop of the Rings: A Distributed Cooperative System",
    "abstract": "We introduce a decentralized and distributed collaborative environment\ndenoted by $LoR$, which stands for \\emph{\"The Loop of the Rings\"}. The $LoR$\nsystem provides a secure, user-friendly cooperative environment for users who\ncan offer particular services to each other. The system manages to provide\nreliability and security using randomized technics in a well-structured\nenvironment. These technics together provide consensus and trust for the groups\nof collaborator parties. This platform carries a blockchain-based distributed\ndatabase to save important information. The $LoR$ system deals with cooperation\nrather than transactions. The system should manage and verify the collaboration\nbetween each group of participants who work with each other from the start to\nthe end of the collaboration. The unique structure of the $LoR$ system makes it\na secure and reliable middleware between a (distributed) database and a service\nprovider system. Such a service provider could be a freelancer or an IoT\nmanagement system. The 5G-related services can be organized to be managed by\nthe $LoR$ platform.",
    "descriptor": "",
    "authors": [
      "Arash Vaezi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01121"
  },
  {
    "id": "arXiv:2206.01122",
    "title": "Super-resolving 2D stress tensor field conserving equilibrium  constraints using physics informed U-Net",
    "abstract": "In a finite element analysis, using a large number of grids is important to\nobtain accurate results, but is a resource-consuming task. Aiming to real-time\nsimulation and optimization, it is desired to obtain fine grid analysis results\nwithin a limited resource. This paper proposes a super-resolution method that\npredicts a stress tensor field in a high-resolution from low-resolution contour\nplots by utilizing a U-Net-based neural network which is called PI-UNet. In\naddition, the proposed model minimizes the residual of the equilibrium\nconstraints so that it outputs a physically reasonable solution. The proposed\nnetwork is trained with FEM results of simple shapes, and is validated with a\ncomplicated realistic shape to evaluate generalization capability. Although\nESRGAN is a standard model for image super-resolution, the proposed U-Net based\nmodel outperforms ESRGAN model in the stress tensor prediction task.",
    "descriptor": "",
    "authors": [
      "Kazuo Yonekura",
      "Kento Maruoka",
      "Kyoku Tyou",
      "Katsuyuki Suzuki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01122"
  },
  {
    "id": "arXiv:2206.01125",
    "title": "Prefix Conditioning Unifies Language and Label Supervision",
    "abstract": "Vision-language contrastive learning suggests a new learning paradigm by\nleveraging a large amount of image-caption-pair data. The caption supervision\nexcels at providing wide coverage in vocabulary that enables strong zero-shot\nimage recognition performance. On the other hand, label supervision offers to\nlearn more targeted visual representations that are label-oriented and can\ncover rare categories. To gain the complementary advantages of both kinds of\nsupervision for contrastive image-caption pre-training, recent works have\nproposed to convert class labels into a sentence with pre-defined templates\ncalled prompts. However, a naive unification of the real caption and the prompt\nsentences could lead to a complication in learning, as the distribution shift\nin text may not be handled properly in the language encoder. In this work, we\npropose a simple yet effective approach to unify these two types of supervision\nusing prefix tokens that inform a language encoder of the type of the input\nsentence (e.g., caption or prompt) at training time. Our method is generic and\ncan be easily integrated into existing VL pre-training objectives such as CLIP\nor UniCL. In experiments, we show that this simple technique dramatically\nimproves the performance in zero-shot image recognition accuracy of the\npre-trained model.",
    "descriptor": "",
    "authors": [
      "Kuniaki Saito",
      "Kihyuk Sohn",
      "Xiang Zhang",
      "Chun-Liang Li",
      "Chen-Yu Lee",
      "Kate Saenko",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01125"
  },
  {
    "id": "arXiv:2206.01127",
    "title": "VL-BEiT: Generative Vision-Language Pretraining",
    "abstract": "We introduce a vision-language foundation model called VL-BEiT, which is a\nbidirectional multimodal Transformer learned by generative pretraining. Our\nminimalist solution conducts masked prediction on both monomodal and multimodal\ndata with a shared Transformer. Specifically, we perform masked vision-language\nmodeling on image-text pairs, masked language modeling on texts, and masked\nimage modeling on images. VL-BEiT is learned from scratch with one unified\npretraining task, one shared backbone, and one-stage training. Our method is\nconceptually simple and empirically effective. Experimental results show that\nVL-BEiT obtains strong results on various vision-language benchmarks, such as\nvisual question answering, visual reasoning, and image-text retrieval.\nMoreover, our method learns transferable visual features, achieving competitive\nperformance on image classification, and semantic segmentation.",
    "descriptor": "",
    "authors": [
      "Hangbo Bao",
      "Wenhui Wang",
      "Li Dong",
      "Furu Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.01127"
  },
  {
    "id": "arXiv:2206.01131",
    "title": "Predictive Multiplicity in Probabilistic Classification",
    "abstract": "For a prediction task, there may exist multiple models that perform almost\nequally well. This multiplicity complicates how we typically develop and deploy\nmachine learning models. We study how multiplicity affects predictions -- i.e.,\npredictive multiplicity -- in probabilistic classification. We introduce new\nmeasures for this setting and present optimization-based methods to compute\nthese measures for convex empirical risk minimization problems like logistic\nregression. We apply our methodology to gain insight into why predictive\nmultiplicity arises. We study the incidence and prevalence of predictive\nmultiplicity in real-world risk assessment tasks. Our results emphasize the\nneed to report multiplicity more widely.",
    "descriptor": "",
    "authors": [
      "Jamelle Watson-Daniels",
      "David C. Parkes",
      "Berk Ustun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01131"
  },
  {
    "id": "arXiv:2206.01132",
    "title": "A Communication-efficient Algorithm with Linear Convergence for  Federated Minimax Learning",
    "abstract": "In this paper, we study a large-scale multi-agent minimax optimization\nproblem, which models many interesting applications in statistical learning and\ngame theory, including Generative Adversarial Networks (GANs). The overall\nobjective is a sum of agents' private local objective functions. We first\nanalyze an important special case, empirical minimax problem, where the overall\nobjective approximates a true population minimax risk by statistical samples.\nWe provide generalization bounds for learning with this objective through\nRademacher complexity analysis. Then, we focus on the federated setting, where\nagents can perform local computation and communicate with a central server.\nMost existing federated minimax algorithms either require communication per\niteration or lack performance guarantees with the exception of Local Stochastic\nGradient Descent Ascent (SGDA), a multiple-local-update descent ascent\nalgorithm which guarantees convergence under a diminishing stepsize. By\nanalyzing Local SGDA under the ideal condition of no gradient noise, we show\nthat generally it cannot guarantee exact convergence with constant stepsizes\nand thus suffers from slow rates of convergence. To tackle this issue, we\npropose FedGDA-GT, an improved Federated (Fed) Gradient Descent Ascent (GDA)\nmethod based on Gradient Tracking (GT). When local objectives are Lipschitz\nsmooth and strongly-convex-strongly-concave, we prove that FedGDA-GT converges\nlinearly with a constant stepsize to global $\\epsilon$-approximation solution\nwith $\\mathcal{O}(\\log (1/\\epsilon))$ rounds of communication, which matches\nthe time complexity of centralized GDA method. Finally, we numerically show\nthat FedGDA-GT outperforms Local SGDA.",
    "descriptor": "",
    "authors": [
      "Zhenyu Sun",
      "Ermin Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.01132"
  },
  {
    "id": "arXiv:2206.01134",
    "title": "Vygotskian Autotelic Artificial Intelligence: Language and Culture  Internalization for Human-Like AI",
    "abstract": "Building autonomous artificial agents able to grow open-ended repertoires of\nskills is one of the fundamental goals of AI. To that end, a promising\ndevelopmental approach recommends the design of intrinsically motivated agents\nthat learn new skills by generating and pursuing their own goals - autotelic\nagents. However, existing algorithms still show serious limitations in terms of\ngoal diversity, exploration, generalization or skill composition. This\nperspective calls for the immersion of autotelic agents into rich\nsocio-cultural worlds. We focus on language especially, and how its structure\nand content may support the development of new cognitive functions in\nartificial agents, just like it does in humans. Indeed, most of our skills\ncould not be learned in isolation. Formal education teaches us to reason\nsystematically, books teach us history, and YouTube might teach us how to cook.\nCrucially, our values, traditions, norms and most of our goals are cultural in\nessence. This knowledge, and some argue, some of our cognitive functions such\nas abstraction, compositional imagination or relational thinking, are formed\nthrough linguistic and cultural interactions. Inspired by the work of Vygotsky,\nwe suggest the design of Vygotskian autotelic agents able to interact with\nothers and, more importantly, able to internalize these interactions to\ntransform them into cognitive tools supporting the development of new cognitive\nfunctions. This perspective paper proposes a new AI paradigm in the quest for\nartificial lifelong skill discovery. It justifies the approach by uncovering\nexamples of new artificial cognitive functions emerging from interactions\nbetween language and embodiment in recent works at the intersection of deep\nreinforcement learning and natural language processing. Looking forward, it\nhighlights future opportunities and challenges for Vygotskian Autotelic AI\nresearch.",
    "descriptor": "",
    "authors": [
      "C\u00e9dric Colas",
      "Tristan Karch",
      "Cl\u00e9ment Moulin-Frier",
      "Pierre-Yves Oudeyer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01134"
  },
  {
    "id": "arXiv:2206.01136",
    "title": "Transforming medical imaging with Transformers? A comparative review of  key properties, current progresses, and future perspectives",
    "abstract": "Transformer, the latest technological advance of deep learning, has gained\nprevalence in natural language processing or computer vision. Since medical\nimaging bear some resemblance to computer vision, it is natural to inquire\nabout the status quo of Transformers in medical imaging and ask the question:\ncan the Transformer models transform medical imaging? In this paper, we attempt\nto make a response to the inquiry. After a brief introduction of the\nfundamentals of Transformers, especially in comparison with convolutional\nneural networks (CNNs), and highlighting key defining properties that\ncharacterize the Transformers, we offer a comprehensive review of the\nstate-of-the-art Transformer-based approaches for medical imaging and exhibit\ncurrent research progresses made in the areas of medical image segmentation,\nrecognition, detection, registration, reconstruction, enhancement, etc. In\nparticular, what distinguishes our review lies in its organization based on the\nTransformer's key defining properties, which are mostly derived from comparing\nthe Transformer and CNN, and its type of architecture, which specifies the\nmanner in which the Transformer and CNN are combined, all helping the readers\nto best understand the rationale behind the reviewed approaches. We conclude\nwith discussions of future perspectives.",
    "descriptor": "",
    "authors": [
      "Jun Li",
      "Junyu Chen",
      "Yucheng Tang",
      "Bennett A. Landman",
      "S. Kevin Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01136"
  },
  {
    "id": "arXiv:2206.01137",
    "title": "Finding the Right Recipe for Low Resource Domain Adaptation in Neural  Machine Translation",
    "abstract": "General translation models often still struggle to generate accurate\ntranslations in specialized domains. To guide machine translation practitioners\nand characterize the effectiveness of domain adaptation methods under different\ndata availability scenarios, we conduct an in-depth empirical exploration of\nmonolingual and parallel data approaches to domain adaptation of pre-trained,\nthird-party, NMT models in settings where architecture change is impractical.\nWe compare data centric adaptation methods in isolation and combination. We\nstudy method effectiveness in very low resource (8k parallel examples) and\nmoderately low resource (46k parallel examples) conditions and propose an\nensemble approach to alleviate reductions in original domain translation\nquality. Our work includes three domains: consumer electronic, clinical, and\nbiomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We\nalso make concrete recommendations for achieving high in-domain performance and\nrelease our consumer electronic and medical domain datasets for all languages\nand make our code publicly available.",
    "descriptor": "",
    "authors": [
      "Virginia Adams",
      "Sandeep Subramanian",
      "Mike Chrzanowski",
      "Oleksii Hrinchuk",
      "Oleksii Kuchaiev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01137"
  },
  {
    "id": "arXiv:2206.01146",
    "title": "Block-Parallel Systolic-Array Architecture for 2-D NTT-based Fragile  Watermark Embedding",
    "abstract": "Number-theoretic transforms (NTTs) have been applied in the fragile\nwatermarking of digital images. A block-parallel systolic-array architecture is\nproposed for watermarking based on the 2-D special Hartley NTT (HNTT). The\nproposed core employs two 2-D special HNTT hardware cores, each using digital\narithmetic over $\\mathrm{GF}(3)$, and processes $4\\times4$ blocks of pixels in\nparallel every clock cycle. Prototypes are operational on a Xilinx Sx35-10ff668\nFPGA device. The maximum estimated throughput of the FPGA circuit is 100\nmillion $4\\times4$ HNTT fragile watermarked blocks per second, when clocked at\n100 MHz. Potential applications exist in high-traffic back-end servers dealing\nwith large amounts of protected digital images requiring authentication, in\nremote-sensing for high-security surveillance applications, in real-time video\nprocessing of information of a sensitive nature or matters of national\nsecurity, in video/photographic content management of corporate clients, in\nauthenticating multimedia for the entertainment industry, in the authentication\nof electronic evidence material, and in real-time news streaming.",
    "descriptor": "\nComments: 11 pages, 4 figures\n",
    "authors": [
      "H. P. L. Arjuna Madanayake",
      "R. J. Cintra",
      "V. S. Dimitrov",
      "L. Bruton"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.01146"
  },
  {
    "id": "arXiv:2206.01149",
    "title": "Engineering Compact Data Structures for Rank and Select Queries on Bit  Vectors",
    "abstract": "Bit vectors are fundamental building blocks of many succinct data structures.\nThey can be used to represent graphs, are an important part of many text\nindices in the form of the wavelet tree, and can be used to encode ordered\nsequences of integers as Elias-Fano codes. To do so, two queries have to be\nanswered: namely rank and select queries. Given a position in the bit vector, a\nrank query returns the number of 1-bits before that position. A select query,\ngiven a parameter $j$, returns the position of the $j$-th 1-bit. On a length-n\nbit vector, both queries can be answered in $O(1)$ time and require $o(n)$ bits\nof additional space. In practice, the smallest (uncompressed) rank and select\ndata structure cs-poppy has a space overhead of $\\approx$ 3.51% [Zhou et al.,\nSEA 13]. In this paper, we present an improved rank and select data structure\nthat has the same space overhead but can answer queries up to 8% (rank) and\n16.5% (select) faster compared with cs-poppy.",
    "descriptor": "",
    "authors": [
      "Florian Kurpicz"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.01149"
  },
  {
    "id": "arXiv:2206.01153",
    "title": "Multi-View Active Fine-Grained Recognition",
    "abstract": "As fine-grained visual classification (FGVC) being developed for decades,\ngreat works related have exposed a key direction -- finding discriminative\nlocal regions and revealing subtle differences. However, unlike identifying\nvisual contents within static images, for recognizing objects in the real\nphysical world, discriminative information is not only present within seen\nlocal regions but also hides in other unseen perspectives. In other words, in\naddition to focusing on the distinguishable part from the whole, for efficient\nand accurate recognition, it is required to infer the key perspective with a\nfew glances, e.g., people may recognize a \"Benz AMG GT\" with a glance of its\nfront and then know that taking a look at its exhaust pipe can help to tell\nwhich year's model it is. In this paper, back to reality, we put forward the\nproblem of active fine-grained recognition (AFGR) and complete this study in\nthree steps: (i) a hierarchical, multi-view, fine-grained vehicle dataset is\ncollected as the testbed, (ii) a simple experiment is designed to verify that\ndifferent perspectives contribute differently for FGVC and different categories\nown different discriminative perspective, (iii) a policy-gradient-based\nframework is adopted to achieve efficient recognition with active view\nselection. Comprehensive experiments demonstrate that the proposed method\ndelivers a better performance-efficient trade-off than previous FGVC methods\nand advanced neural networks.",
    "descriptor": "",
    "authors": [
      "Ruoyi Du",
      "Wenqing Yu",
      "Heqing Wang",
      "Dongliang Chang",
      "Ting-En Lin",
      "Yongbin Li",
      "Zhanyu Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01153"
  },
  {
    "id": "arXiv:2206.01160",
    "title": "DE-Net: Dynamic Text-guided Image Editing Adversarial Networks",
    "abstract": "Text-guided image editing models have shown remarkable results. However,\nthere remain two problems. First, they employ fixed manipulation modules for\nvarious editing requirements (e.g., color changing, texture changing, content\nadding and removing), which result in over-editing or insufficient editing.\nSecond, they do not clearly distinguish between text-required parts and\ntext-irrelevant parts, which leads to inaccurate editing. To solve these\nlimitations, we propose: (i) a Dynamic Editing Block (DEBlock) which combines\nspatial- and channel-wise manipulations dynamically for various editing\nrequirements. (ii) a Combination Weights Predictor (CWP) which predicts the\ncombination weights for DEBlock according to the inference on text and visual\nfeatures. (iii) a Dynamic text-adaptive Convolution Block (DCBlock) which\nqueries source image features to distinguish text-required parts and\ntext-irrelevant parts. Extensive experiments demonstrate that our DE-Net\nachieves excellent performance and manipulates source images more effectively\nand accurately. Code is available at \\url{https://github.com/tobran/DE-Net}.",
    "descriptor": "",
    "authors": [
      "Ming Tao",
      "Bing-Kun Bao",
      "Hao Tang",
      "Fei Wu",
      "Longhui Wei",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.01160"
  },
  {
    "id": "arXiv:2206.01161",
    "title": "Optimizing Relevance Maps of Vision Transformers Improves Robustness",
    "abstract": "It has been observed that visual classification models often rely mostly on\nthe image background, neglecting the foreground, which hurts their robustness\nto distribution changes. To alleviate this shortcoming, we propose to monitor\nthe model's relevancy signal and manipulate it such that the model is focused\non the foreground object. This is done as a finetuning step, involving\nrelatively few samples consisting of pairs of images and their associated\nforeground masks. Specifically, we encourage the model's relevancy map (i) to\nassign lower relevance to background regions, (ii) to consider as much\ninformation as possible from the foreground, and (iii) we encourage the\ndecisions to have high confidence. When applied to Vision Transformer (ViT)\nmodels, a marked improvement in robustness to domain shifts is observed.\nMoreover, the foreground masks can be obtained automatically, from a\nself-supervised variant of the ViT model itself; therefore no additional\nsupervision is required.",
    "descriptor": "",
    "authors": [
      "Hila Chefer",
      "Idan Schwartz",
      "Lior Wolf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01161"
  },
  {
    "id": "arXiv:2206.01162",
    "title": "Posterior Coreset Construction with Kernelized Stein Discrepancy for  Model-Based Reinforcement Learning",
    "abstract": "In this work, we propose a novel ${\\bf K}$ernelized ${\\bf S}$tein\nDiscrepancy-based Posterior Sampling for ${\\bf RL}$ algorithm (named\n$\\texttt{KSRL}$) which extends model-based RL based upon posterior sampling\n(PSRL) in several ways: we (i) relax the need for any smoothness or Gaussian\nassumptions, allowing for complex mixture models; (ii) ensure it is applicable\nto large-scale training by incorporating a compression step such that the\nposterior consists of a \\emph{Bayesian coreset} of only statistically\nsignificant past state-action pairs; and (iii) develop a novel regret analysis\nof PSRL based upon integral probability metrics, which, under a smoothness\ncondition on the constructed posterior, can be evaluated in closed form as the\nkernelized Stein discrepancy (KSD). Consequently, we are able to improve the\n$\\mathcal{O}(H^{3/2}d\\sqrt{T})$ {regret} of PSRL to\n$\\mathcal{O}(H^{3/2}\\sqrt{T})$, where $d$ is the input dimension, $H$ is the\nepisode length, and $T$ is the total number of episodes experienced,\nalleviating a linear dependence on $d$ . Moreover, we theoretically establish a\ntrade-off between regret rate with posterior representational complexity via\nintroducing a compression budget parameter $\\epsilon$ based on KSD, and\nestablish a lower bound on the required complexity for consistency of the\nmodel. Experimentally, we observe that this approach is competitive with\nseveral state of the art RL methodologies, with substantive improvements in\ncomputation time. Experimentally, we observe that this approach is competitive\nwith several state of the art RL methodologies, and can achieve up-to $50\\%$\nreduction in wall clock time in some continuous control environments.",
    "descriptor": "",
    "authors": [
      "Souradip Chakraborty",
      "Amrit Singh Bedi",
      "Alec Koppel",
      "Brian M. Sadler",
      "Furong Huang",
      "Pratap Tokekar",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01162"
  },
  {
    "id": "arXiv:2206.01167",
    "title": "Sparse Mixed Linear Regression with Guarantees: Taming an Intractable  Problem with Invex Relaxation",
    "abstract": "In this paper, we study the problem of sparse mixed linear regression on an\nunlabeled dataset that is generated from linear measurements from two different\nregression parameter vectors. Since the data is unlabeled, our task is not only\nto figure out a good approximation of the regression parameter vectors but also\nto label the dataset correctly. In its original form, this problem is NP-hard.\nThe most popular algorithms to solve this problem (such as\nExpectation-Maximization) have a tendency to stuck at local minima. We provide\na novel invex relaxation for this intractable problem which leads to a solution\nwith provable theoretical guarantees. This relaxation enables exact recovery of\ndata labels. Furthermore, we recover a close approximation of the regression\nparameter vectors which match the true parameter vectors in support and sign.\nOur formulation uses a carefully constructed primal dual witnesses framework\nfor the invex problem. Furthermore, we show that the sample complexity of our\nmethod is only logarithmic in terms of the dimension of the regression\nparameter vectors.",
    "descriptor": "",
    "authors": [
      "Adarsh Barik",
      "Jean Honorio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01167"
  },
  {
    "id": "arXiv:2206.01175",
    "title": "Robust Longitudinal Control for Vehicular Autonomous Platoons Using Deep  Reinforcement Learning",
    "abstract": "In the last few years, researchers have applied machine learning strategies\nin the context of vehicular platoons to increase the safety and efficiency of\ncooperative transportation. Reinforcement Learning methods have been employed\nin the longitudinal spacing control of Cooperative Adaptive Cruise Control\nsystems, but to date, none of those studies have addressed problems of\ndisturbance rejection in such scenarios. Characteristics such as uncertain\nparameters in the model and external interferences may prevent agents from\nreaching null-spacing errors when traveling at cruising speed. On the other\nhand, complex communication topologies lead to specific training processes that\ncan not be generalized to other contexts, demanding re-training every time the\nconfiguration changes. Therefore, in this paper, we propose an approach to\ngeneralize the training process of a vehicular platoon, such that the\nacceleration command of each agent becomes independent of the network topology.\nAlso, we have modeled the acceleration input as a term with integral action,\nsuch that the Convolutional Neural Network is capable of learning corrective\nactions when the states are disturbed by unknown effects. We illustrate the\neffectiveness of our proposal with experiments using different network\ntopologies, uncertain parameters, and external forces. Comparative analyses, in\nterms of the steady-state error and overshoot response, were conducted against\nthe state-of-the-art literature. The findings offer new insights concerning\ngeneralization and robustness of using Reinforcement Learning in the control of\nautonomous platoons.",
    "descriptor": "",
    "authors": [
      "Armando Alves Neto",
      "Leonardo Amaral Mozelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01175"
  },
  {
    "id": "arXiv:2206.01176",
    "title": "From Cities to Series: Complex Networks and Deep Learning for Improved  Spatial and Temporal Analytics*",
    "abstract": "Graphs have often been used to answer questions about the interaction between\nreal-world entities by taking advantage of their capacity to represent complex\ntopologies. Complex networks are known to be graphs that capture such\nnon-trivial topologies; they are able to represent human phenomena such as\nepidemic processes, the dynamics of populations, and the urbanization of\ncities. The investigation of complex networks has been extrapolated to many\nfields of science, with particular emphasis on computing techniques, including\nartificial intelligence. In such a case, the analysis of the interaction\nbetween entities of interest is transposed to the internal learning of\nalgorithms, a paradigm whose investigation is able to expand the state of the\nart in Computer Science. By exploring this paradigm, this thesis puts together\ncomplex networks and machine learning techniques to improve the understanding\nof the human phenomena observed in pandemics, pendular migration, and street\nnetworks. Accordingly, we contribute with: (i) a new neural network\narchitecture capable of modeling dynamic processes observed in spatial and\ntemporal data with applications in epidemics propagation, weather forecasting,\nand patient monitoring in intensive care units; (ii) a machine-learning\nmethodology for analyzing and predicting links in the scope of human mobility\nbetween all the cities of Brazil; and, (iii) techniques for identifying\ninconsistencies in the urban planning of cities while tracking the most\ninfluential vertices, with applications over Brazilian and worldwide cities. We\nobtained results sustained by sound evidence of advances to the state of the\nart in artificial intelligence, rigorous formalisms, and ample experimentation.\nOur findings rely upon real-world applications in a range of domains,\ndemonstrating the applicability of our methodologies.",
    "descriptor": "\nComments: This piece refers to an extended abstract of the Ph.D. thesis under the same name defended in the Graduate Program in Computer Science and Computational Mathematics (PPG-CCMC) of the Institute of Mathematics and Statistics (ICMC) from the University of Sao Paulo (USP) - Brazil on July 12, 2021. The authors here listed refer to the Ph.D. candidate and his advisor, respectively\n",
    "authors": [
      "Gabriel Spadon",
      "Jose F. Rodrigues-Jr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.01176"
  },
  {
    "id": "arXiv:2206.01178",
    "title": "Deep Learning on Implicit Neural Datasets",
    "abstract": "Implicit neural representations (INRs) have become fast, lightweight tools\nfor storing continuous data, but to date there is no general method for\nlearning directly with INRs as a data representation. We introduce a principled\ndeep learning framework for learning and inference directly with INRs of any\ntype without reverting to grid-based features or operations. Our INR-Nets\nevaluate INRs on a low discrepancy sequence, enabling quasi-Monte Carlo (QMC)\nintegration throughout the network. We prove INR-Nets are universal\napproximators on a large class of maps between $L^2$ functions. Additionally,\nINR-Nets have convergent gradients under the empirical measure, enabling\nbackpropagation. We design INR-Nets as a continuous generalization of discrete\nnetworks, enabling them to be initialized with pre-trained models. We\ndemonstrate learning of INR-Nets on classification (INR$\\to$label) and\nsegmentation (INR$\\to$INR) tasks.",
    "descriptor": "",
    "authors": [
      "Clinton J. Wang",
      "Polina Golland"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.01178"
  },
  {
    "id": "arXiv:2206.01186",
    "title": "ORC: Network Group-based Knowledge Distillation using Online Role Change",
    "abstract": "In knowledge distillation, since a single, omnipotent teacher network cannot\nsolve all problems, multiple teacher-based knowledge distillations have been\nstudied recently. However, sometimes their improvements are not as good as\nexpected because some immature teachers may transfer the false knowledge to the\nstudent. In this paper, to overcome this limitation and take the efficacy of\nthe multiple networks, we divide the multiple networks into teacher and student\ngroups, respectively. That is, the student group is a set of immature networks\nthat require learning the teacher's knowledge, while the teacher group consists\nof the selected networks that have performed well. Furthermore, according to\nour online role change strategy, the top-ranked networks in the student group\nare able to promote to the teacher group at every iteration and vice versa.\nAfter training the teacher group using the error images of the student group to\nrefine the teacher group's knowledge, we transfer the collective knowledge from\nthe teacher group to the student group successfully. We verify the superiority\nof the proposed method on CIFAR-10 and CIFAR-100, which achieves high\nperformance. We further show the generality of our method with various backbone\narchitectures such as resent, wrn, vgg, mobilenet, and shufflenet.",
    "descriptor": "",
    "authors": [
      "Junyong Choi",
      "Hyeon Cho",
      "Seockhwa Jeong",
      "Wonjun Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01186"
  },
  {
    "id": "arXiv:2206.01188",
    "title": "Control hubs of complex networks and a polynomial-time identification  algorithm",
    "abstract": "Unveiling the underlying control principles of complex networks is one of the\nultimate goals of network science. We introduce a novel concept, control hub,\nto reveal a cornerstone of the control structure of a network. The control hubs\nof a network are the nodes that lie in the middle of a control path in every\ncontrol scheme of the network. We present a theorem based on graph theory for\nidentifying control hubs without computing all control schemes. We develop an\nalgorithm to identify all control hubs in O(N0.5L) time complexity for a\nnetwork of N nodes and L links.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Xizhe Zhang",
      "Chunyu Pan",
      "Weixiong Zhang"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.01188"
  },
  {
    "id": "arXiv:2206.01191",
    "title": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "abstract": "Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, e.g., attention mechanism, ViT-based\nmodels are generally times slower than lightweight convolutional networks.\nTherefore, the deployment of ViT for real-time applications is particularly\nchallenging, especially on resource-constrained hardware such as mobile\ndevices. Recent efforts try to reduce the computation complexity of ViT through\nnetwork architecture search or hybrid design with MobileNet block, yet the\ninference speed is still unsatisfactory. This leads to an important question:\ncan transformers run as fast as MobileNet while obtaining high performance? To\nanswer this, we first revisit the network architecture and operators used in\nViT-based models and identify inefficient designs. Then we introduce a\ndimension-consistent pure transformer (without MobileNet blocks) as design\nparadigm. Finally, we perform latency-driven slimming to get a series of final\nmodels dubbed EfficientFormer. Extensive experiments show the superiority of\nEfficientFormer in performance and speed on mobile devices. Our fastest model,\nEfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6\nms inference latency on iPhone 12 (compiled with CoreML), which is even a bit\nfaster than MobileNetV2 (1.7 ms, 71.8% top-1), and our largest model,\nEfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work\nproves that properly designed transformers can reach extremely low latency on\nmobile devices while maintaining high performance",
    "descriptor": "",
    "authors": [
      "Yanyu Li",
      "Geng Yuan",
      "Yang Wen",
      "Eric Hu",
      "Georgios Evangelidis",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01191"
  },
  {
    "id": "arXiv:2206.01192",
    "title": "Uniqueness and Complexity of Inverse MDP Models",
    "abstract": "What is the action sequence aa'a\" that was likely responsible for reaching\nstate s\"' (from state s) in 3 steps? Addressing such questions is important in\ncausal reasoning and in reinforcement learning. Inverse \"MDP\" models\np(aa'a\"|ss\"') can be used to answer them. In the traditional \"forward\" view,\ntransition \"matrix\" p(s'|sa) and policy {\\pi}(a|s) uniquely determine\n\"everything\": the whole dynamics p(as'a's\"a\"...|s), and with it, the\naction-conditional state process p(s's\"...|saa'a\"), the multi-step inverse\nmodels p(aa'a\"...|ss^i), etc. If the latter is our primary concern, a natural\nquestion, analogous to the forward case is to which extent 1-step inverse model\np(a|ss') plus policy {\\pi}(a|s) determine the multi-step inverse models or even\nthe whole dynamics. In other words, can forward models be inferred from inverse\nmodels or even be side-stepped. This work addresses this question and\nvariations thereof, and also whether there are efficient decision/inference\nalgorithms for this.",
    "descriptor": "\nComments: 34 pages, 4 fgures\n",
    "authors": [
      "Marcus Hutter",
      "Steven Hansen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.01192"
  },
  {
    "id": "arXiv:2206.01197",
    "title": "Hard Negative Sampling Strategies for Contrastive Representation  Learning",
    "abstract": "One of the challenges in contrastive learning is the selection of appropriate\n\\textit{hard negative} examples, in the absence of label information. Random\nsampling or importance sampling methods based on feature similarity often lead\nto sub-optimal performance. In this work, we introduce UnReMix, a hard negative\nsampling strategy that takes into account anchor similarity, model uncertainty\nand representativeness. Experimental results on several benchmarks show that\nUnReMix improves negative sample selection, and subsequently downstream\nperformance when compared to state-of-the-art contrastive learning methods.",
    "descriptor": "",
    "authors": [
      "Afrina Tabassum",
      "Muntasir Wahed",
      "Hoda Eldardiry",
      "Ismini Lourentzou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01197"
  },
  {
    "id": "arXiv:2206.01198",
    "title": "Pruning-as-Search: Efficient Neural Architecture Search via Channel  Pruning and Structural Reparameterization",
    "abstract": "Neural architecture search (NAS) and network pruning are widely studied\nefficient AI techniques, but not yet perfect. NAS performs exhaustive candidate\narchitecture search, incurring tremendous search cost. Though (structured)\npruning can simply shrink model dimension, it remains unclear how to decide the\nper-layer sparsity automatically and optimally. In this work, we revisit the\nproblem of layer-width optimization and propose Pruning-as-Search (PaS), an\nend-to-end channel pruning method to search out desired sub-network\nautomatically and efficiently. Specifically, we add a depth-wise binary\nconvolution to learn pruning policies directly through gradient descent. By\ncombining the structural reparameterization and PaS, we successfully searched\nout a new family of VGG-like and lightweight networks, which enable the\nflexibility of arbitrary width with respect to each layer instead of each\nstage. Experimental results show that our proposed architecture outperforms\nprior arts by around $1.0\\%$ top-1 accuracy under similar inference speed on\nImageNet-1000 classification task. Furthermore, we demonstrate the\neffectiveness of our width search on complex tasks including instance\nsegmentation and image translation. Code and models are released.",
    "descriptor": "",
    "authors": [
      "Yanyu Li",
      "Pu Zhao",
      "Geng Yuan",
      "Xue Lin",
      "Yanzhi Wang",
      "Xin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01198"
  },
  {
    "id": "arXiv:2206.01201",
    "title": "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual  Question Answering",
    "abstract": "This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationships are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA.",
    "descriptor": "",
    "authors": [
      "Yuanze Lin",
      "Yujia Xie",
      "Dongdong Chen",
      "Yichong Xu",
      "Chenguang Zhu",
      "Lu Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.01201"
  },
  {
    "id": "arXiv:2206.01202",
    "title": "Unveiling The Mask of Position-Information Pattern Through the Mist of  Image Features",
    "abstract": "Recent studies show that paddings in convolutional neural networks encode\nabsolute position information which can negatively affect the model performance\nfor certain tasks. However, existing metrics for quantifying the strength of\npositional information remain unreliable and frequently lead to erroneous\nresults. To address this issue, we propose novel metrics for measuring (and\nvisualizing) the encoded positional information. We formally define the encoded\ninformation as PPP (Position-information Pattern from Padding) and conduct a\nseries of experiments to study its properties as well as its formation. The\nproposed metrics measure the presence of positional information more reliably\nthan the existing metrics based on PosENet and a test in F-Conv. We also\ndemonstrate that for any extant (and proposed) padding schemes, PPP is\nprimarily a learning artifact and is less dependent on the characteristics of\nthe underlying padding schemes.",
    "descriptor": "",
    "authors": [
      "Chieh Hubert Lin",
      "Hsin-Ying Lee",
      "Hung-Yu Tseng",
      "Maneesh Singh",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01202"
  },
  {
    "id": "arXiv:2206.01203",
    "title": "Semantic Instance Segmentation of 3D Scenes Through Weak Bounding Box  Supervision",
    "abstract": "Current 3D segmentation methods heavily rely on large-scale point-cloud\ndatasets, which are notoriously laborious to annotate. Few attempts have been\nmade to circumvent the need for dense per-point annotations. In this work, we\nlook at weakly-supervised 3D instance semantic segmentation. The key idea is to\nleverage 3D bounding box labels which are easier and faster to annotate.\nIndeed, we show that it is possible to train dense segmentation models using\nonly weak bounding box labels. At the core of our method, Box2Mask, lies a deep\nmodel, inspired by classical Hough voting, that directly votes for bounding box\nparameters, and a clustering method specifically tailored to bounding box\nvotes. This goes beyond commonly used center votes, which would not fully\nexploit the bounding box annotations. On ScanNet test, our weakly supervised\nmodel attains leading performance among other weakly supervised approaches (+18\nmAP50). Remarkably, it also achieves 97% of the performance of fully supervised\nmodels. To prove the practicality of our approach, we show segmentation results\non the recently released ARKitScenes dataset which is annotated with 3D\nbounding boxes only, and obtain, for the first time, compelling 3D instance\nsegmentation results.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Julian Chibane",
      "Francis Engelmann",
      "Tuan Anh Tran",
      "Gerard Pons-Moll"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01203"
  },
  {
    "id": "arXiv:2206.01204",
    "title": "Siamese Image Modeling for Self-Supervised Vision Representation  Learning",
    "abstract": "Self-supervised learning (SSL) has delivered superior performance on a\nvariety of downstream vision tasks. Two main-stream SSL frameworks have been\nproposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).\nID pulls together the representations of different views from the same image,\nwhile avoiding feature collapse. It does well on linear probing but is inferior\nin detection performance. On the other hand, MIM reconstructs the original\ncontent given a masked image. It excels at dense prediction but fails to\nperform well on linear probing. Their distinctions are caused by neglecting the\nrepresentation requirements of either semantic alignment or spatial\nsensitivity. Specifically, we observe that (1) semantic alignment demands\nsemantically similar views to be projected into nearby representation, which\ncan be achieved by contrasting different views with strong augmentations; (2)\nspatial sensitivity requires to model the local structure within an image.\nPredicting dense representations with masked image is therefore beneficial\nbecause it models the conditional distribution of image content. Driven by\nthese analysis, we propose Siamese Image Modeling (SIM), which predicts the\ndense representations of an augmented view, based on another masked view from\nthe same image but with different augmentations. Our method uses a Siamese\nnetwork with two branches. The online branch encodes the first view, and\npredicts the second view's representation according to the relative positions\nbetween these two views. The target branch produces the target by encoding the\nsecond view. In this way, we are able to achieve comparable linear probing and\ndense prediction performances with ID and MIM, respectively. We also\ndemonstrate that decent linear probing result can be obtained without a global\nloss. Code shall be released.",
    "descriptor": "",
    "authors": [
      "Chenxin Tao",
      "Xizhou Zhu",
      "Gao Huang",
      "Yu Qiao",
      "Xiaogang Wang",
      "Jifeng Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01204"
  },
  {
    "id": "arXiv:2012.05503",
    "title": "Geometric algorithms for sampling the flux space of metabolic networks",
    "abstract": "Systems Biology is a fundamental field and paradigm that introduces a new era\nin Biology. The crux of its functionality and usefulness relies on metabolic\nnetworks that model the reactions occurring inside an organism and provide the\nmeans to understand the underlying mechanisms that govern biological systems.\nEven more, metabolic networks have a broader impact that ranges from resolution\nof ecosystems to personalized medicine.The analysis of metabolic networks is a\ncomputational geometry oriented field as one of the main operations they depend\non is sampling uniformly points from polytopes; the latter provides a\nrepresentation of the steady states of the metabolic networks. However, the\npolytopes that result from biological data are of very high dimension (to the\norder of thousands) and in most, if not all, the cases are considerably skinny.\nTherefore, to perform uniform random sampling efficiently in this setting, we\nneed a novel algorithmic and computational framework specially tailored for the\nproperties of metabolic networks.We present a complete software framework to\nhandle sampling in metabolic networks. Its backbone is a Multiphase Monte Carlo\nSampling (MMCS) algorithm that unifies rounding and sampling in one pass,\nobtaining both upon termination. It exploits an improved variant of the\nBilliard Walk that enjoys faster arithmetic complexity per step. We demonstrate\nthe efficiency of our approach by performing extensive experiments on various\nmetabolic networks. Notably, sampling on the most complicated human metabolic\nnetwork accessible today, Recon3D, corresponding to a polytope of dimension 5\n335 took less than 30 hours. To our knowledge, that is out of reach for\nexisting software.",
    "descriptor": "\nComments: The 37th International Symposium on Computational Geometry (SoCG), Jun 2021, Buffalo, United States\n",
    "authors": [
      "Apostolos Chalkis",
      "Vissarion Fisikopoulos",
      "Elias Tsigaridas",
      "Haris Zafeiropoulos"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computational Geometry (cs.CG)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2012.05503"
  },
  {
    "id": "arXiv:2206.00668",
    "title": "Learning to Untangle Genome Assembly with Graph Convolutional Networks",
    "abstract": "A quest to determine the complete sequence of a human DNA from telomere to\ntelomere started three decades ago and was finally completed in 2021. This\naccomplishment was a result of a tremendous effort of numerous experts who\nengineered various tools and performed laborious manual inspection to achieve\nthe first gapless genome sequence. However, such method can hardly be used as a\ngeneral approach to assemble different genomes, especially when the assembly\nspeed is critical given the large amount of data. In this work, we explore a\ndifferent approach to the central part of the genome assembly task that\nconsists of untangling a large assembly graph from which a genomic sequence\nneeds to be reconstructed. Our main motivation is to reduce human-engineered\nheuristics and use deep learning to develop more generalizable reconstruction\ntechniques. Precisely, we introduce a new learning framework to train a graph\nconvolutional network to resolve assembly graphs by finding a correct path\nthrough them. The training is supervised with a dataset generated from the\nresolved CHM13 human sequence and tested on assembly graphs built using real\nhuman PacBio HiFi reads. Experimental results show that a model, trained on\nsimulated graphs generated solely from a single chromosome, is able to\nremarkably resolve all other chromosomes. Moreover, the model outperforms\nhand-crafted heuristics from a state-of-the-art \\textit{de novo} assembler on\nthe same graphs. Reconstructed chromosomes with graph networks are more\naccurate on nucleotide level, report lower number of contigs, higher genome\nreconstructed fraction and NG50/NGA50 assessment metrics.",
    "descriptor": "",
    "authors": [
      "Lovro Vr\u010dek",
      "Xavier Bresson",
      "Thomas Laurent",
      "Martin Schmitz",
      "Mile \u0160iki\u0107"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00668"
  },
  {
    "id": "arXiv:2206.00706",
    "title": "Split-kl and PAC-Bayes-split-kl Inequalities",
    "abstract": "We present a new concentration of measure inequality for sums of independent\nbounded random variables, which we name a split-kl inequality. The inequality\ncombines the combinatorial power of the kl inequality with ability to exploit\nlow variance. While for Bernoulli random variables the kl inequality is tighter\nthan the Empirical Bernstein, for random variables taking values inside a\nbounded interval and having low variance the Empirical Bernstein inequality is\ntighter than the kl. The proposed split-kl inequality yields the best of both\nworlds. We discuss an application of the split-kl inequality to bounding excess\nlosses. We also derive a PAC-Bayes-split-kl inequality and use a synthetic\nexample and several UCI datasets to compare it with the PAC-Bayes-kl, PAC-Bayes\nEmpirical Bernstein, PAC-Bayes Unexpected Bernstein, and PAC-Bayes Empirical\nBennett inequalities.",
    "descriptor": "",
    "authors": [
      "Yi-Shan Wu",
      "Yevgeny Seldin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00706"
  },
  {
    "id": "arXiv:2206.00707",
    "title": "Collaborative Learning of Distributions under Heterogeneity and  Communication Constraints",
    "abstract": "In modern machine learning, users often have to collaborate to learn\ndistributions that generate the data. Communication can be a significant\nbottleneck. Prior work has studied homogeneous users -- i.e., whose data follow\nthe same discrete distribution -- and has provided optimal\ncommunication-efficient methods. However, these methods rely heavily on\nhomogeneity, and are less applicable in the common case when users' discrete\ndistributions are heterogeneous. Here we consider a natural and tractable model\nof heterogeneity, where users' discrete distributions only vary sparsely, on a\nsmall number of entries. We propose a novel two-stage method named SHIFT:\nFirst, the users collaborate by communicating with the server to learn a\ncentral distribution; relying on methods from robust statistics. Then, the\nlearned central distribution is fine-tuned to estimate the individual\ndistributions of users. We show that SHIFT is minimax optimal in our model of\nheterogeneity and under communication constraints. Further, we provide\nexperimental results using both synthetic data and $n$-gram frequency\nestimation in the text domain, which corroborate its efficiency.",
    "descriptor": "",
    "authors": [
      "Xinmeng Huang",
      "Donghwan Lee",
      "Edgar Dobriban",
      "Hamed Hassani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00707"
  },
  {
    "id": "arXiv:2206.00727",
    "title": "(Machine) Learning What Policies Value",
    "abstract": "When a policy prioritizes one person over another, is it because they benefit\nmore, or because they are preferred? This paper develops a method to uncover\nthe values consistent with observed allocation decisions. We use machine\nlearning methods to estimate how much each individual benefits from an\nintervention, and then reconcile its allocation with (i) the welfare weights\nassigned to different people; (ii) heterogeneous treatment effects of the\nintervention; and (iii) weights on different outcomes. We demonstrate this\napproach by analyzing Mexico's PROGRESA anti-poverty program. The analysis\nreveals that while the program prioritized certain subgroups -- such as\nindigenous households -- the fact that those groups benefited more implies that\nthey were in fact assigned a lower welfare weight. The PROGRESA case\nillustrates how the method makes it possible to audit existing policies, and to\ndesign future policies that better align with values.",
    "descriptor": "",
    "authors": [
      "Daniel Bj\u00f6rkegren",
      "Joshua E. Blumenstock",
      "Samsun Knight"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00727"
  },
  {
    "id": "arXiv:2206.00743",
    "title": "Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax  Optimization",
    "abstract": "Adaptive algorithms like AdaGrad and AMSGrad are successful in nonconvex\noptimization owing to their parameter-agnostic ability -- requiring no a priori\nknowledge about problem-specific parameters nor tuning of learning rates.\nHowever, when it comes to nonconvex minimax optimization, direct extensions of\nsuch adaptive optimizers without proper time-scale separation may fail to work\nin practice. We provide such an example proving that the simple combination of\nGradient Descent Ascent (GDA) with adaptive stepsizes can diverge if the\nprimal-dual stepsize ratio is not carefully chosen; hence, a fortiori, such\nadaptive extensions are not parameter-agnostic. To address the issue, we\nformally introduce a Nested Adaptive framework, NeAda for short, that carries\nan inner loop for adaptively maximizing the dual variable with controllable\nstopping criteria and an outer loop for adaptively minimizing the primal\nvariable. Such mechanism can be equipped with off-the-shelf adaptive optimizers\nand automatically balance the progress in the primal and dual variables.\nTheoretically, for nonconvex-strongly-concave minimax problems, we show that\nNeAda can achieve the near-optimal $\\tilde{O}(\\epsilon^{-2})$ and\n$\\tilde{O}(\\epsilon^{-4})$ gradient complexities respectively in the\ndeterministic and stochastic settings, without prior information on the\nproblem's smoothness and strong concavity parameters. To the best of our\nknowledge, this is the first algorithm that simultaneously achieves\nnear-optimal convergence rates and parameter-agnostic adaptation in the\nnonconvex minimax setting. Numerically, we further illustrate the robustness of\nthe NeAda family with experiments on simple test functions and a real-world\napplication.",
    "descriptor": "",
    "authors": [
      "Junchi Yang",
      "Xiang Li",
      "Niao He"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00743"
  },
  {
    "id": "arXiv:2206.00766",
    "title": "Simulating the 1976 Teton Dam Failure using Geoclaw and HEC-RAS and  comparing with Historical Observations",
    "abstract": "Dam failures occur worldwide, often from factors including aging structures,\nextreme hydrologic loading, and design oversights related to the changing\nclimate. Understanding and mitigating risk to downstream inhabited areas\nrequire developing and improving low-cost high-fidelity tools, such as\nnumerical models, which allow emergency managers to predict the consequences of\ndam failures better. Two-dimensional (2D) depth-averaged hydraulic models can\nprovide valuable insights into the importance of breach parameters or\ndownstream flow characteristics, but historical studies considering historic\nfailures using real topographies are less common in literature. This study\ncompares Geoclaw, a 2D hydraulic model with adaptive mesh refinement\ncapabilities, to an industry-standard software HEC-RAS (Hydrologic Engineering\nCenter - River Analysis System) using the 1976 Teton Dam failure as a case\nstudy. The suitability of Geoclaw for dam failure modeling is determined based\non its capability to resolve inundation extent and flood wave arrival times.\nThis study performs sensitivity analyses of the HEC-RAS model to compare an\ninstantaneous dam breach assumption with a time-dependent breach formation for\nquantifying the model uncertainty. We find the 2D Geoclaw dam-break model\nresults compare reasonably with historical gauge and field observational data\nand HEC-RAS results. The model demonstrates stability and relatively low\ncomputational costs. Our findings highlight opportunities for future work, with\nthe Geoclaw software performance supporting continued studies to evaluate\nperformance. Outcomes of this study will assist dam owners, floodplain\nmanagers, and emergency managers by providing an additional tool for estimating\nthe impacts of dam failures to protect lives and infrastructure downstream.",
    "descriptor": "",
    "authors": [
      "Hannah Spero",
      "Donna Calhoun",
      "Michael Shubert"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.00766"
  },
  {
    "id": "arXiv:2206.00775",
    "title": "Adaptive Local Neighborhood-based Neural Networks for MR Image  Reconstruction from Undersampled Data",
    "abstract": "Recent medical image reconstruction techniques focus on generating\nhigh-quality medical images suitable for clinical use at the lowest possible\ncost and with the fewest possible adverse effects on patients. Recent works\nhave shown significant promise for reconstructing MR images from sparsely\nsampled k-space data using deep learning. In this work, we propose a technique\nthat rapidly estimates deep neural networks directly at reconstruction time by\nfitting them on small adaptively estimated neighborhoods of a training set. In\nbrief, our algorithm alternates between searching for neighbors in a data set\nthat are similar to the test reconstruction, and training a local network on\nthese neighbors followed by updating the test reconstruction. Because our\nreconstruction model is learned on a dataset that is structurally similar to\nthe image being reconstructed rather than being fit on a large, diverse\ntraining set, it is more adaptive to new scans. It can also handle changes in\ntraining sets and flexible scan settings, while being relatively fast. Our\napproach, dubbed LONDN-MRI, was validated on the FastMRI multi-coil knee data\nset using deep unrolled reconstruction networks. Reconstructions were performed\nat four fold and eight fold undersampling of k-space with 1D variable-density\nrandom phase-encode undersampling masks. Our results demonstrate that our\nproposed locally-trained method produces higher-quality reconstructions\ncompared to models trained globally on larger datasets.",
    "descriptor": "",
    "authors": [
      "Shijun Liang",
      "Anish Lahiri",
      "Saiprasad Ravishankar"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00775"
  },
  {
    "id": "arXiv:2206.00778",
    "title": "Efficient and Self-Recursive Delay Vandermonde Algorithm for Multi-Beam  Antenna Arrays",
    "abstract": "This paper presents a self-contained factorization for the delay Vandermonde\nmatrix (DVM), which is the super class of the discrete Fourier transform, using\nsparse and companion matrices. An efficient DVM algorithm is proposed to reduce\nthe complexity of radio-frequency (RF) $N$-beam analog beamforming systems.\nThere exist applications for wideband multi-beam beamformers in wireless\ncommunication networks such as 5G/6G systems, system capacity can be improved\nby exploiting the improvement of the signal to noise ratio (SNR) using coherent\nsummation of propagating waves based on their directions of propagation. The\npresence of a multitude of RF beams allows multiple independent wireless links\nto be established at high SNR, or used in conjunction with multiple-input\nmultiple-output (MIMO) wireless systems, with the overall goal of improving\nsystem SNR and therefore capacity. To realize such multi-beam beamformers at\nacceptable analog circuit complexities, we use sparse factorization of the DVM\nin order to derive a low arithmetic complexity DVM algorithm. The paper also\nestablishes an error bound and stability analysis of the proposed DVM\nalgorithm. The proposed efficient DVM algorithm is aimed at implementation\nusing analog realizations. For purposes of evaluation, the algorithm can be\nrealized using both digital hardware as well as software defined radio\nplatforms.",
    "descriptor": "\nComments: 25 pages, 2 figures\n",
    "authors": [
      "S. M. Perera",
      "A. Madanayake",
      "R. J. Cintra"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.00778"
  },
  {
    "id": "arXiv:2206.00779",
    "title": "Radix-2 Self-Recursive Sparse Factorizations of Delay Vandermonde  Matrices for Wideband Multi-Beam Antenna Arrays",
    "abstract": "This paper presents a self-contained factorization for the Vandermonde\nmatrices associated with true-time delay based wideband analog multi-beam\nbeamforming using antenna arrays. The proposed factorization contains sparse\nand orthogonal matrices. Novel self-recursive radix-2 algorithms for\nVandermonde matrices associated with true time delay based delay-sum\nfilterbanks are presented to reduce the circuit complexity of multi-beam analog\nbeamforming systems. The proposed algorithms for Vandermonde matrices by a\nvector attain $\\mathcal{O}(N \\log N)$ delay-amplifier circuit counts. Error\nbounds for the Vandermode matrices associated with true-time delay are\nestablished and then analyzed for numerical stability. The potential for\nreal-world circuit implementation of the proposed algorithms will be shown\nthrough signal flow graphs that are the starting point for high-frequency\nanalog circuit realizations.",
    "descriptor": "\nComments: 20 pages, 1 figure\n",
    "authors": [
      "S. M. Perera",
      "A. Madanayake",
      "R. J. Cintra"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.00779"
  },
  {
    "id": "arXiv:2206.00794",
    "title": "Sequential Bayesian Neural Subnetwork Ensembles",
    "abstract": "Deep neural network ensembles that appeal to model diversity have been used\nsuccessfully to improve predictive performance and model robustness in several\napplications. Whereas, it has recently been shown that sparse subnetworks of\ndense models can match the performance of their dense counterparts and increase\ntheir robustness while effectively decreasing the model complexity. However,\nmost ensembling techniques require multiple parallel and costly evaluations and\nhave been proposed primarily with deterministic models, whereas sparsity\ninduction has been mostly done through ad-hoc pruning. We propose sequential\nensembling of dynamic Bayesian neural subnetworks that systematically reduce\nmodel complexity through sparsity-inducing priors and generate diverse\nensembles in a single forward pass of the model. The ensembling strategy\nconsists of an exploration phase that finds high-performing regions of the\nparameter space and multiple exploitation phases that effectively exploit the\ncompactness of the sparse model to quickly converge to different minima in the\nenergy landscape corresponding to high-performing subnetworks yielding diverse\nensembles. We empirically demonstrate that our proposed approach surpasses the\nbaselines of the dense frequentist and Bayesian ensemble models in prediction\naccuracy, uncertainty estimation, and out-of-distribution (OoD) robustness on\nCIFAR10, CIFAR100 datasets, and their out-of-distribution variants: CIFAR10-C,\nCIFAR100-C induced by corruptions. Furthermore, we found that our approach\nproduced the most diverse ensembles compared to the approaches with a single\nforward pass and even compared to the approaches with multiple forward passes\nin some cases.",
    "descriptor": "",
    "authors": [
      "Sanket Jantre",
      "Sandeep Madireddy",
      "Shrijita Bhattacharya",
      "Tapabrata Maiti",
      "Prasanna Balaprakash"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.00794"
  },
  {
    "id": "arXiv:2206.00801",
    "title": "Indeterminacy in Latent Variable Models: Characterization and Strong  Identifiability",
    "abstract": "Most modern latent variable and probabilistic generative models, such as the\nvariational autoencoder (VAE), have certain indeterminacies that are\nunresolvable even with an infinite amount of data. Recent applications of such\nmodels have indicated the need for \\textit{strongly} identifiable models, in\nwhich an observation corresponds to a unique latent code. Progress has been\nmade towards reducing model indeterminacies while maintaining flexibility, most\nnotably by the iVAE (arXiv:1907.04809 [stat.ML]), which excludes many -- but\nnot all -- indeterminacies. We construct a full theoretical framework for\nanalyzing the indeterminacies of latent variable models, and characterize them\nprecisely in terms of properties of the generator functions and the latent\nvariable prior distributions. To illustrate, we apply the framework to better\nunderstand the structure of recent identifiability results. We then investigate\nhow we might specify strongly identifiable latent variable models, and\nconstruct two such classes of models. One is a straightforward modification of\niVAE; the other uses ideas from optimal transport and leads to novel models and\nconnections to recent work.",
    "descriptor": "",
    "authors": [
      "Quanhan Xi",
      "Benjamin Bloem-Reddy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00801"
  },
  {
    "id": "arXiv:2206.00831",
    "title": "Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm  and Casorati Matrix Nuclear Norm Regularizations",
    "abstract": "Low-rank tensor models have been applied in accelerating dynamic magnetic\nresonance imaging (dMRI). Recently, a new tensor nuclear norm based on t-SVD\nhas been proposed and applied to tensor completion. Inspired by the different\nproperties of the tensor nuclear norm (TNN) and the Casorati matrix nuclear\nnorm (MNN), we introduce a combined TNN and Casorati MNN regularizations\nframework to reconstruct dMRI, which we term as TMNN. The proposed method\nsimultaneously exploits the spatial structure and the temporal correlation of\nthe dynamic MR data. The optimization problem can be efficiently solved by the\nalternating direction method of multipliers (ADMM). In order to further improve\nthe computational efficiency, we develop a fast algorithm under the Cartesian\nsampling scenario. Numerical experiments based on cardiac cine MRI and\nperfusion MRI data demonstrate the performance improvement over the traditional\nCasorati nuclear norm regularization method.",
    "descriptor": "\nComments: 4 pages, 3 figures, 1 table, accepted in IEEE ISBI 2022\n",
    "authors": [
      "Yinghao Zhang",
      "Yue Hu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00831"
  },
  {
    "id": "arXiv:2206.00850",
    "title": "Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network  (DTLR-Net)",
    "abstract": "While low-rank matrix prior has been exploited in dynamic MR image\nreconstruction and has obtained satisfying performance, low-rank tensors models\nhave recently emerged as powerful alternative representations for\nthree-dimensional dynamic MR datasets. In this paper, we introduce a\nmodel-based deep learning network by learning the tensor low-rank prior of the\ncardiac dynamic MR images. Instead of representing the dynamic dataset as a\nlow-rank tensor directly, we propose a learned transformation operator to\nexploit the tensor low-rank property in a transform domain. In particular, by\ngeneralizing the t-SVD tensor decomposition into a unitary transformed t-SVD,\nwe define a transformed tensor nuclear norm (TTNN) to enforce the tensor\nlow-rankness. The dynamic MRI reconstruction problem is thus formulated using a\nTTNN regularized optimization problem. An iterative algorithm based on ADMM\nused to minimize the cost is unrolled into a deep network, where the transform\nis learned using convolutional neural networks (CNNs) to promote the\nreconstruction quality in the feature domain. Experimental results on cardiac\ncine MRI reconstruction demonstrate that the proposed framework is able to\nprovide improved recovery results compared with the state-of-the-art\nalgorithms.",
    "descriptor": "\nComments: 5 pages, 2 figures, 1 tabel, summited to IEEE ICIP 2022\n",
    "authors": [
      "Yinghao Zhang",
      "Peng Li",
      "Yue Hu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00850"
  },
  {
    "id": "arXiv:2206.00853",
    "title": "Masked Bayesian Neural Networks : Computation and Optimality",
    "abstract": "As data size and computing power increase, the architectures of deep neural\nnetworks (DNNs) have been getting more complex and huge, and thus there is a\ngrowing need to simplify such complex and huge DNNs. In this paper, we propose\na novel sparse Bayesian neural network (BNN) which searches a good DNN with an\nappropriate complexity. We employ the masking variables at each node which can\nturn off some nodes according to the posterior distribution to yield a nodewise\nsparse DNN. We devise a prior distribution such that the posterior distribution\nhas theoretical optimalities (i.e. minimax optimality and adaptiveness), and\ndevelop an efficient MCMC algorithm. By analyzing several benchmark datasets,\nwe illustrate that the proposed BNN performs well compared to other existing\nmethods in the sense that it discovers well condensed DNN architectures with\nsimilar prediction accuracy and uncertainty quantification compared to large\nDNNs.",
    "descriptor": "",
    "authors": [
      "Insung Kong",
      "Dongyoon Yang",
      "Jongjin Lee",
      "Ilsang Ohn",
      "Yongdai Kim"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00853"
  },
  {
    "id": "arXiv:2206.00858",
    "title": "Bayesian Inference of Stochastic Dynamical Networks",
    "abstract": "Network inference has been extensively studied in several fields, such as\nsystems biology and social sciences. Learning network topology and internal\ndynamics is essential to understand mechanisms of complex systems. In\nparticular, sparse topologies and stable dynamics are fundamental features of\nmany real-world continuous-time networks. Given that usually only a partial set\nof nodes are able to observe, in this paper, we consider linear continuous-time\nsystems to depict networks since they can model unmeasured nodes via transfer\nfunctions. Additionally, measurements tend to be noisy and with low and varying\nsampling frequencies. For this reason, we consider continuous-time models (CT)\nsince discrete-time approximations often require fine-grained measurements and\nuniform sampling steps. The developed method applies dynamical structure\nfunctions (DSFs) derived from linear stochastic differential equations (SDEs)\nto describe networks of measured nodes. Further, a numerical sampling method,\npreconditioned Crank-Nicolson (pCN), is used to refine coarse-grained\ntrajectories to improve inference accuracy. The simulation conducted on random\nand ring networks, and a synthetic biological network illustrate that our\nmethod achieves state-of-the-art performance compared with group sparse\nBayesian learning (GSBL), BINGO, kernel-based methods, dynGENIE3, GENIE3 and\nARNI. In particular, these are challenging networks, suggesting that the\ndeveloped method can be applied under a wide range of contexts.",
    "descriptor": "\nComments: 12 pages, 2 figures, and 7 tables\n",
    "authors": [
      "Yasen Wang",
      "Junyang Jin",
      "Jorge Goncalves"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00858"
  },
  {
    "id": "arXiv:2206.00866",
    "title": "Extension and Validation of 4D Model for Improving the Accuracy of  Modulation-Dependent Nonlinear Interference",
    "abstract": "Existing nonlinear interference (NLI) model underestimates the NLI of dual\npolarization four-dimensional (4D) modulation in long-haul transmission, due to\nthe ignorance of nonlinearity caused by signal-ASE interaction. We propose an\nenhanced 4D model by lifting an underlying assumption, which could improve the\nperformance prediction accuracy.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Zhiwei Liang",
      "Bin Chen",
      "Yi Lei",
      "Gabriele Liga",
      "Alex Alvarado"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.00866"
  },
  {
    "id": "arXiv:2206.00885",
    "title": "Coordinated Double Machine Learning",
    "abstract": "Double machine learning is a statistical method for leveraging complex\nblack-box models to construct approximately unbiased treatment effect estimates\ngiven observational data with high-dimensional covariates, under the assumption\nof a partially linear model. The idea is to first fit on a subset of the\nsamples two non-linear predictive models, one for the continuous outcome of\ninterest and one for the observed treatment, and then to estimate a linear\ncoefficient for the treatment using the remaining samples through a simple\northogonalized regression. While this methodology is flexible and can\naccommodate arbitrary predictive models, typically trained independently of one\nanother, this paper argues that a carefully coordinated learning algorithm for\ndeep neural networks may reduce the estimation bias. The improved empirical\nperformance of the proposed method is demonstrated through numerical\nexperiments on both simulated and real data.",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Nitai Fingerhut",
      "Matteo Sesia",
      "Yaniv Romano"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.00885"
  },
  {
    "id": "arXiv:2206.00888",
    "title": "Squeezeformer: An Efficient Transformer for Automatic Speech Recognition",
    "abstract": "The recently proposed Conformer model has become the de facto backbone model\nfor various downstream speech tasks based on its hybrid attention-convolution\narchitecture that captures both local and global features. However, through a\nseries of systematic studies, we find that the Conformer architecture's design\nchoices are not optimal. After reexamining the design choices for both the\nmacro and micro-architecture of Conformer, we propose the Squeezeformer model,\nwhich consistently outperforms the state-of-the-art ASR models under the same\ntraining schemes. In particular, for the macro-architecture, Squeezeformer\nincorporates (i) the Temporal U-Net structure, which reduces the cost of the\nmulti-head attention modules on long sequences, and (ii) a simpler block\nstructure of feed-forward module, followed up by multi-head attention or\nconvolution modules, instead of the Macaron structure proposed in Conformer.\nFurthermore, for the micro-architecture, Squeezeformer (i) simplifies the\nactivations in the convolutional block, (ii) removes redundant Layer\nNormalization operations, and (iii) incorporates an efficient depth-wise\ndownsampling layer to efficiently sub-sample the input signal. Squeezeformer\nachieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate on\nLibrispeech test-other without external language models. This is 3.1%, 1.4%,\nand 0.6% better than Conformer-CTC with the same number of FLOPs. Our code is\nopen-sourced and available online.",
    "descriptor": "",
    "authors": [
      "Sehoon Kim",
      "Amir Gholami",
      "Albert Shaw",
      "Nicholas Lee",
      "Karttikeya Mangalam",
      "Jitendra Malik",
      "Michael W. Mahoney",
      "Kurt Keutzer"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.00888"
  },
  {
    "id": "arXiv:2206.00939",
    "title": "Gradient flow dynamics of shallow ReLU networks for square loss and  orthogonal inputs",
    "abstract": "The training of neural networks by gradient descent methods is a cornerstone\nof the deep learning revolution. Yet, despite some recent progress, a complete\ntheory explaining its success is still missing. This article presents, for\northogonal input vectors, a precise description of the gradient flow dynamics\nof training one-hidden layer ReLU neural networks for the mean squared error at\nsmall initialisation. In this setting, despite non-convexity, we show that the\ngradient flow converges to zero loss and characterise its implicit bias towards\nminimum variation norm. Furthermore, some interesting phenomena are\nhighlighted: a quantitative description of the initial alignment phenomenon and\na proof that the process follows a specific saddle to saddle dynamics.",
    "descriptor": "",
    "authors": [
      "Etienne Boursier",
      "Loucas Pillaud-Vivien",
      "Nicolas Flammarion"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00939"
  },
  {
    "id": "arXiv:2206.00951",
    "title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining  Unsupervised and Supervised Phonetic Representations",
    "abstract": "This paper proposes a multilingual speech synthesis method which combines\nunsupervised phonetic representations (UPR) and supervised phonetic\nrepresentations (SPR) to avoid reliance on the pronunciation dictionaries of\ntarget languages. In this method, a pretrained wav2vec 2.0 model is adopted to\nextract UPRs and a language-independent automatic speech recognition (LI-ASR)\nmodel is built with a connectionist temporal classification (CTC) loss to\nextract segment-level SPRs from the audio data of target languages. Then, an\nacoustic model is designed, which first predicts UPRs and SPRs from texts\nseparately and then combines the predicted UPRs and SPRs to generate\nmel-spectrograms. The results of our experiments on six languages show that the\nproposed method outperformed the methods that directly predicted\nmel-spectrograms from character or phoneme sequences and the ablated models\nthat utilized only UPRs or SPRs.",
    "descriptor": "\nComments: Submitted to Interspeech 2022\n",
    "authors": [
      "Chang Liu",
      "Zhen-Hua Ling",
      "Ling-Hui Chen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.00951"
  },
  {
    "id": "arXiv:2206.00970",
    "title": "Self-supervised Learning of Audio Representations from Audio-Visual Data  using Spatial Alignment",
    "abstract": "Learning from audio-visual data offers many possibilities to express\ncorrespondence between the audio and visual content, similar to the human\nperception that relates aural and visual information. In this work, we present\na method for self-supervised representation learning based on audio-visual\nspatial alignment (AVSA), a more sophisticated alignment task than the\naudio-visual correspondence (AVC). In addition to the correspondence, AVSA also\nlearns from the spatial location of acoustic and visual content. Based on\n360$^\\text{o}$ video and Ambisonics audio, we propose selection of visual\nobjects using object detection, and beamforming of the audio signal towards the\ndetected objects, attempting to learn the spatial alignment between objects and\nthe sound they produce. We investigate the use of spatial audio features to\nrepresent the audio input, and different audio formats: Ambisonics, mono, and\nstereo. Experimental results show a 10 $\\%$ improvement on AVSA for the first\norder ambisonics intensity vector (FOA-IV) in comparison with log-mel\nspectrogram features; the addition of object-oriented crops also brings\nsignificant performance increases for the human action recognition downstream\ntask. A number of audio-only downstream tasks are devised for testing the\neffectiveness of the learnt audio feature representation, obtaining performance\ncomparable to state-of-the-art methods on acoustic scene classification from\nambisonic and binaural audio.",
    "descriptor": "",
    "authors": [
      "Shanshan Wang",
      "Archontis Politis",
      "Annamaria Mesaros",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.00970"
  },
  {
    "id": "arXiv:2206.00973",
    "title": "Primal-dual extrapolation methods for monotone inclusions under local  Lipschitz continuity with applications to variational inequality, conic  constrained saddle point, and convex conic optimization problems",
    "abstract": "In this paper we consider a class of structured monotone inclusion (MI)\nproblems that consist of finding a zero in the sum of two monotone operators,\nin which one is maximal monotone while another is locally Lipschitz continuous.\nIn particular, we first propose a primal-dual extrapolation (PDE) method for\nsolving a structured strongly MI problem by modifying the classical\nforward-backward splitting method by using a point and operator extrapolation\ntechnique, in which the parameters are adaptively updated by a backtracking\nline search scheme. The proposed PDE method is almost parameter-free, equipped\nwith a verifiable termination criterion, and enjoys an operation complexity of\n${\\cal O}(\\log \\epsilon^{-1})$, measured by the amount of fundamental\noperations consisting only of evaluations of one operator and resolvent of\nanother operator, for finding an $\\epsilon$-residual solution of the structured\nstrongly MI problem. We then propose another PDE method for solving a\nstructured non-strongly MI problem by applying the above PDE method to\napproximately solve a sequence of structured strongly MI problems. The\nresulting PDE method is parameter-free, equipped with a verifiable termination\ncriterion, and enjoys an operation complexity of ${\\cal O}(\\epsilon^{-1}\\log\n\\epsilon^{-1})$ for finding an $\\epsilon$-residual solution of the structured\nnon-strongly MI problem. As a consequence, we apply the latter PDE method to\nconvex conic optimization, conic constrained saddle point, and variational\ninequality problems, and obtain complexity results for finding an\n$\\epsilon$-KKT or $\\epsilon$-residual solution of them under local Lipschitz\ncontinuity. To the best of our knowledge, no prior studies were conducted to\ninvestigate methods with complexity guarantees for solving the aforementioned\nproblems under local Lipschitz continuity. All the complexity results obtained\nin this paper are entirely new.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Zhaosong Lu",
      "Sanyou Mei"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00973"
  },
  {
    "id": "arXiv:2206.00978",
    "title": "First demonstration of a post-quantum key-exchange with a nanosatellite",
    "abstract": "We demonstrate a post-quantum key-exchange with the nanosatellite SpooQy-1 in\nlow Earth orbit using Kyber-512, a lattice-based key-encapsulation mechanism\nand a round three finalist in the NIST PQC standardization process. Our\nfirmware solution runs on an on-board computer that is based on the Atmel AVR32\nRISC microcontroller, a widely used platform for nanosatellites. We uploaded\nthe new firmware with a 436.2 MHz UHF link using the CubeSat Space Protocol\n(CSP) and performed the steps of the key exchange in several passes over\nSwitzerland. The shared secret key generated in this experiment could\npotentially be used to encrypt RF links with AES-256. This implementation\ndemonstrates the feasibility of a quantum-safe authenticated key-exchange and\nencryption system on SWaP constrained nanosatellites.",
    "descriptor": "\nComments: 9 pages, 8 figures, to be published at 36th Annual Small Satellite Conference, August 6-11, 2022, Logan, UT, USA\n",
    "authors": [
      "Simon M. Burkhardt",
      "Ayesha Reezwana",
      "Tanvirul Islam",
      "Willi Meier",
      "Alexander Ling",
      "Christoph F. Wildfeuer"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.00978"
  },
  {
    "id": "arXiv:2206.01018",
    "title": "Score-Based Generative Models Detect Manifolds",
    "abstract": "Score-based generative models (SGMs) need to approximate the scores $\\nabla\n\\log p_t$ of the intermediate distributions as well as the final distribution\n$p_T$ of the forward process. The theoretical underpinnings of the effects of\nthese approximations are still lacking. We find precise conditions under which\nSGMs are able to produce samples from an underlying (low-dimensional) data\nmanifold $\\mathcal{M}$. This assures us that SGMs are able to generate the\n\"right kind of samples\". For example, taking $\\mathcal{M}$ to be the subset of\nimages of faces, we find conditions under which the SGM robustly produces an\nimage of a face, even though the relative frequencies of these images might not\naccurately represent the true data generating distribution. Moreover, this\nanalysis is a first step towards understanding the generalization properties of\nSGMs: Taking $\\mathcal{M}$ to be the set of all training samples, our results\nprovide a precise description of when the SGM memorizes its training data.",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "Jakiw Pidstrigach"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2206.01018"
  },
  {
    "id": "arXiv:2206.01029",
    "title": "Trajectory of Mini-Batch Momentum: Batch Size Saturation and Convergence  in High Dimensions",
    "abstract": "We analyze the dynamics of large batch stochastic gradient descent with\nmomentum (SGD+M) on the least squares problem when both the number of samples\nand dimensions are large. In this setting, we show that the dynamics of SGD+M\nconverge to a deterministic discrete Volterra equation as dimension increases,\nwhich we analyze. We identify a stability measurement, the implicit\nconditioning ratio (ICR), which regulates the ability of SGD+M to accelerate\nthe algorithm. When the batch size exceeds this ICR, SGD+M converges linearly\nat a rate of $\\mathcal{O}(1/\\sqrt{\\kappa})$, matching optimal full-batch\nmomentum (in particular performing as well as a full-batch but with a fraction\nof the size). For batch sizes smaller than the ICR, in contrast, SGD+M has\nrates that scale like a multiple of the single batch SGD rate. We give explicit\nchoices for the learning rate and momentum parameter in terms of the Hessian\nspectra that achieve this performance.",
    "descriptor": "",
    "authors": [
      "Kiwon Lee",
      "Andrew N. Cheng",
      "Courtney Paquette",
      "Elliot Paquette"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01029"
  },
  {
    "id": "arXiv:2206.01059",
    "title": "Recognizing the Commuting Graph of a Finite Group",
    "abstract": "In this paper we study the realizability question for commuting graphs of\nfinite groups: Given an undirected graph $X$ is it the commuting graph of a\ngroup $G$? And if so, to determine such a group. We seek efficient algorithms\nfor this problem. We make some general observations on this problem, and obtain\na polynomial-time algorithm for the case of extraspecial groups.",
    "descriptor": "",
    "authors": [
      "V. Arvind",
      "Peter. J. Cameron"
    ],
    "subjectives": [
      "Group Theory (math.GR)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.01059"
  },
  {
    "id": "arXiv:2206.01068",
    "title": "Min orderings and list homomorphism dichotomies for signed and unsigned  graphs",
    "abstract": "The CSP dichotomy conjecture has been recently established, but a number of\nother dichotomy questions remain open, including the dichotomy classification\nof list homomorphism problems for signed graphs. Signed graphs arise naturally\nin many contexts, including for instance nowhere-zero flows for graphs embedded\nin non-orientable surfaces. For a fixed signed graph $\\widehat{H}$, the list\nhomomorphism problem asks whether an input signed graph $\\widehat{G}$ with\nlists $L(v) \\subseteq V(\\widehat{H}), v \\in V(\\widehat{G}),$ admits a\nhomomorphism $f$ to $\\widehat{H}$ with all $f(v) \\in L(v), v \\in\nV(\\widehat{G})$. Usually, a dichotomy classification is easier to obtain for\nlist homomorphisms than for homomorphisms, but in the context of signed graphs\na structural classification of the complexity of list homomorphism problems has\nnot even been conjectured, even though the classification of the complexity of\nhomomorphism problems is known. Kim and Siggers have conjectured a structural\nclassification in the special case of \"weakly balanced\" signed graphs, and\nproved it for reflexive signed graphs. We confirm the conjecture for\nirreflexive signed graphs; this generalizes previous results on weakly balanced\nsigned trees, and weakly balanced separable signed graphs. Our proof depends on\nfirst deriving a new result on extensions of min orderings of (unsigned)\nbipartite graphs.",
    "descriptor": "",
    "authors": [
      "Jan Bok",
      "Richard Brewster",
      "Pavol Hell",
      "Nikola Jedli\u010dkov\u00e1",
      "Arash Rafiey"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.01068"
  },
  {
    "id": "arXiv:2206.01088",
    "title": "Machine Learning-based Lung and Colon Cancer Detection using Deep  Feature Extraction and Ensemble Learning",
    "abstract": "Cancer is a fatal disease caused by a combination of genetic diseases and a\nvariety of biochemical abnormalities. Lung and colon cancer have emerged as two\nof the leading causes of death and disability in humans. The histopathological\ndetection of such malignancies is usually the most important component in\ndetermining the best course of action. Early detection of the ailment on either\nfront considerably decreases the likelihood of mortality. Machine learning and\ndeep learning techniques can be utilized to speed up such cancer detection,\nallowing researchers to study a large number of patients in a much shorter\namount of time and at a lower cost. In this research work, we introduced a\nhybrid ensemble feature extraction model to efficiently identify lung and colon\ncancer. It integrates deep feature extraction and ensemble learning with\nhigh-performance filtering for cancer image datasets. The model is evaluated on\nhistopathological (LC25000) lung and colon datasets. According to the study\nfindings, our hybrid model can detect lung, colon, and (lung and colon) cancer\nwith accuracy rates of 99.05%, 100%, and 99.30%, respectively. The study's\nfindings show that our proposed strategy outperforms existing models\nsignificantly. Thus, these models could be applicable in clinics to support the\ndoctor in the diagnosis of cancers.",
    "descriptor": "\nComments: Accepted for publication in the Special Issue of Expert Systems with Applications (IF:6.954, Cite:12.70) How to Cite: Md. Alamin Talukder, Md. Manowarul Islam, Md Ashraf Uddin, Arnisha Akhter, Khondokar Fida Hasan, Mohammad Ali Moni. \"Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning\", Expert Systems with Applications. 2022 June 1\n",
    "authors": [
      "Md. Alamin Talukder",
      "Md. Manowarul Islam",
      "Md Ashraf Uddin",
      "Arnisha Akhter",
      "Khondokar Fida Hasan",
      "Mohammad Ali Moni"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01088"
  },
  {
    "id": "arXiv:2206.01092",
    "title": "Combining Machine Learning and Agent-Based Modeling to Study Biomedical  Systems",
    "abstract": "Agent-based modeling (ABM) is a well-established paradigm for simulating\ncomplex systems via interactions between constituent entities. Machine learning\n(ML) refers to approaches whereby statistical algorithms 'learn' from data on\ntheir own, without imposing a priori theories of system behavior. Biological\nsystems -- from molecules, to cells, to entire organisms -- consist of vast\nnumbers of entities, governed by complex webs of interactions that span many\nspatiotemporal scales and exhibit nonlinearity, stochasticity and intricate\ncoupling between entities. The macroscopic properties and collective dynamics\nof such systems are difficult to capture via continuum modelling and mean-field\nformalisms. ABM takes a 'bottom-up' approach that obviates these difficulties\nby enabling one to easily propose and test a set of well-defined 'rules' to be\napplied to the individual entities (agents) in a system. Evaluating a system\nand propagating its state over discrete time-steps effectively simulates the\nsystem, allowing observables to be computed and system properties to be\nanalyzed. Because the rules that govern an ABM can be difficult to abstract and\nformulate from experimental data, there is an opportunity to use ML to help\ninfer optimal, system-specific ABM rules. Once such rule-sets are devised, ABM\ncalculations can generate a wealth of data, and ML can be applied there too --\ne.g., to probe statistical measures that meaningfully describe a system's\nstochastic properties. As an example of synergy in the other direction (from\nABM to ML), ABM simulations can generate realistic datasets for training ML\nalgorithms (e.g., for regularization, to mitigate overfitting). In these ways,\none can envision various synergistic ABM$\\rightleftharpoons$ML loops. This\nreview summarizes how ABM and ML have been integrated in contexts that span\nspatial scales from the cellular to population-level scale epidemiology.",
    "descriptor": "\nComments: 31 pages, 1 table, 6 figures\n",
    "authors": [
      "Nikita Sivakumar",
      "Cameron Mura",
      "Shayn M. Peirce"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Cell Behavior (q-bio.CB)"
    ],
    "url": "https://arxiv.org/abs/2206.01092"
  },
  {
    "id": "arXiv:2206.01095",
    "title": "Clipped Stochastic Methods for Variational Inequalities with  Heavy-Tailed Noise",
    "abstract": "Stochastic first-order methods such as Stochastic Extragradient (SEG) or\nStochastic Gradient Descent-Ascent (SGDA) for solving smooth minimax problems\nand, more generally, variational inequality problems (VIP) have been gaining a\nlot of attention in recent years due to the growing popularity of adversarial\nformulations in machine learning. However, while high-probability convergence\nbounds are known to reflect the actual behavior of stochastic methods more\naccurately, most convergence results are provided in expectation. Moreover, the\nonly known high-probability complexity results have been derived under\nrestrictive sub-Gaussian (light-tailed) noise and bounded domain Assump.\n[Juditsky et al., 2011]. In this work, we prove the first high-probability\ncomplexity results with logarithmic dependence on the confidence level for\nstochastic methods for solving monotone and structured non-monotone VIPs with\nnon-sub-Gaussian (heavy-tailed) noise and unbounded domains. In the monotone\ncase, our results match the best-known ones in the light-tails case [Juditsky\net al., 2011], and are novel for structured non-monotone problems such as\nnegative comonotone, quasi-strongly monotone, and/or star-cocoercive ones. We\nachieve these results by studying SEG and SGDA with clipping. In addition, we\nnumerically validate that the gradient noise of many practical GAN formulations\nis heavy-tailed and show that clipping improves the performance of SEG/SGDA.",
    "descriptor": "\nComments: 71 pages, 16 figures\n",
    "authors": [
      "Eduard Gorbunov",
      "Marina Danilova",
      "David Dobre",
      "Pavel Dvurechensky",
      "Alexander Gasnikov",
      "Gauthier Gidel"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01095"
  },
  {
    "id": "arXiv:2206.01096",
    "title": "A Dual-fusion Semantic Segmentation Framework With GAN For SAR Images",
    "abstract": "Deep learning based semantic segmentation is one of the popular methods in\nremote sensing image segmentation. In this paper, a network based on the widely\nused encoderdecoder architecture is proposed to accomplish the synthetic\naperture radar (SAR) images segmentation. With the better representation\ncapability of optical images, we propose to enrich SAR images with generated\noptical images via the generative adversative network (GAN) trained by numerous\nSAR and optical images. These optical images can be used as expansions of\noriginal SAR images, thus ensuring robust result of segmentation. Then the\noptical images generated by the GAN are stitched together with the\ncorresponding real images. An attention module following the stitched data is\nused to strengthen the representation of the objects. Experiments indicate that\nour method is efficient compared to other commonly used methods",
    "descriptor": "\nComments: 4 pages,4 figures, 2022 IEEE International Geoscience and Remote Sensing Symposium\n",
    "authors": [
      "Donghui Li",
      "Jia Liu",
      "Fang Liu",
      "Wenhua Zhang",
      "Andi Zhang",
      "Wenfei Gao",
      "Jiao Shi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01096"
  },
  {
    "id": "arXiv:2206.01103",
    "title": "Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images",
    "abstract": "Image noise modeling is a long-standing problem with many applications in\ncomputer vision. Early attempts that propose simple models, such as\nsignal-independent additive white Gaussian noise or the heteroscedastic\nGaussian noise model (a.k.a., camera noise level function) are not sufficient\nto learn the complex behavior of the camera sensor noise. Recently, more\ncomplex learning-based models have been proposed that yield better results in\nnoise synthesis and downstream tasks, such as denoising. However, their\ndependence on supervised data (i.e., paired clean images) is a limiting factor\ngiven the challenges in producing ground-truth images. This paper proposes a\nframework for training a noise model and a denoiser simultaneously while\nrelying only on pairs of noisy images rather than noisy/clean paired image\ndata. We apply this framework to the training of the Noise Flow architecture.\nThe noise synthesis and density estimation results show that our framework\noutperforms previous signal-processing-based noise models and is on par with\nits supervised counterpart. The trained denoiser is also shown to significantly\nimprove upon both supervised and weakly supervised baseline denoising\napproaches. The results indicate that the joint training of a denoiser and a\nnoise model yields significant improvements in the denoiser.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Ali Maleky",
      "Shayan Kousha",
      "Michael S. Brown",
      "Marcus A. Brubaker"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01103"
  },
  {
    "id": "arXiv:2206.01118",
    "title": "Comparing Conventional and Deep Feature Models for Classifying Fundus  Photography of Hemorrhages",
    "abstract": "Diabetic retinopathy is an eye-related pathology creating abnormalities and\ncausing visual impairment, proper treatment of which requires identifying\nirregularities. This research uses a hemorrhage detection method and compares\nclassification of conventional and deep features. Especially, method identifies\nhemorrhage connected with blood vessels or reside at retinal border and\nreported challenging. Initially, adaptive brightness adjustment and contrast\nenhancement rectify degraded images. Prospective locations of hemorrhages are\nestimated by a Gaussian matched filter, entropy thresholding, and morphological\noperation. Hemorrhages are segmented by a novel technique based on regional\nvariance of intensities. Features are then extracted by conventional methods\nand deep models for training support vector machines, and results evaluated.\nEvaluation metrics for each model are promising, but findings suggest that\ncomparatively, deep models are more effective than conventional features.",
    "descriptor": "",
    "authors": [
      "Tamoor Aziz",
      "Chalie Charoenlarpnopparut",
      "Srijidtra Mahapakulchai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01118"
  },
  {
    "id": "arXiv:2206.01152",
    "title": "Causal Structure Learning: a Combinatorial Perspective",
    "abstract": "In this review, we discuss approaches for learning causal structure from\ndata, also called causal discovery. In particular, we focus on approaches for\nlearning directed acyclic graphs (DAGs) and various generalizations which allow\nfor some variables to be unobserved in the available data. We devote special\nattention to two fundamental combinatorial aspects of causal structure\nlearning. First, we discuss the structure of the search space over causal\ngraphs. Second, we discuss the structure of equivalence classes over causal\ngraphs, i.e., sets of graphs which represent what can be learned from\nobservational data alone, and how these equivalence classes can be refined by\nadding interventional data.",
    "descriptor": "",
    "authors": [
      "Chandler Squires",
      "Caroline Uhler"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01152"
  },
  {
    "id": "arXiv:2206.01163",
    "title": "Invertible Neural Networks for Graph Prediction",
    "abstract": "In this work, we address conditional generation using deep invertible neural\nnetworks. This is a type of problem where one aims to infer the most probable\ninputs $X$ given outcomes $Y$. We call our method \\textit{invertible graph\nneural network} (iGNN) due to the primary focus on generating node features on\ngraph data. A notable feature of our proposed methods is that during network\ntraining, we revise the typically-used loss objective in normalizing flow and\nconsider Wasserstein-2 regularization to facilitate the training process.\nAlgorithmic-wise, we adopt an end-to-end training approach since our objective\nis to address prediction and generation in the forward and backward processes\nat once through a single model. Theoretically, we characterize the conditions\nfor identifiability of a true mapping, the existence and invertibility of the\nmapping, and the expressiveness of iGNN in learning the mapping.\nExperimentally, we verify the performance of iGNN on both simulated and\nreal-data datasets. We demonstrate through extensive numerical experiments that\niGNN shows clear improvement over competing conditional generation benchmarks\non high-dimensional and/or non-convex data.",
    "descriptor": "",
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01163"
  },
  {
    "id": "arXiv:2206.01164",
    "title": "Authentication of quantum key distribution with post-quantum  cryptography and replay attacks",
    "abstract": "With the development of quantum computers, traditional cryptographic systems\nare facing more and more serious security threats. Fortunately, quantum key\ndistribution (QKD) and post-quantum cryptography (PQC) are two cryptographic\nmechanisms with quantum-resistant security, and both will become important\nsolutions for future information security. However, neither of them is perfect,\nand they are complementary. Quantum key distribution has unconditional security\nthat post-quantum cryptography does not have, and PQC can provide secure and\nconvenient authentication for QKD networks. In this paper, we propose two\nprotocols based on PQC to realize the full authentication of the QKD data\npost-processing, and we only need to assume the short-term security of PQC\nalgorithm to ensure the long-term quantum resistant security of distributed\nkeys. We found that for the above two authentication protocols, attackers\ncannot successfully implement replay attacks. These authentication protocols\ncan solve the problems of the current pre-shared key authentication in the\napplication of large-scale quantum key distribution networks, and are expected\nto realize a key distribution mechanism with practical operability and quantum\nresistant security, which will be beneficial to promote the deployment and\napplication of quantum key distribution networks.",
    "descriptor": "\nComments: 7 pages, 2 figures\n",
    "authors": [
      "Liu-Jun Wang",
      "You-Yang Zhou",
      "Jian-Ming Yin",
      "Qing Chen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01164"
  },
  {
    "id": "arXiv:1609.05867",
    "title": "Fully Dynamic Connectivity in $O(\\log n(\\log\\log n)^2)$ Amortized  Expected Time",
    "abstract": "Fully Dynamic Connectivity in $O(\\log n(\\log\\log n)^2)$ Amortized  Expected Time",
    "descriptor": "",
    "authors": [
      "Shang-En Huang",
      "Dawei Huang",
      "Tsvi Kopelowitz",
      "Seth Pettie",
      "Mikkel Thorup"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1609.05867"
  },
  {
    "id": "arXiv:1801.00329",
    "title": "ZOOpt: Toolbox for Derivative-Free Optimization",
    "abstract": "Comments: SCIENCE CHINA Information Sciences, 2022. Codes: this https URL",
    "descriptor": "\nComments: SCIENCE CHINA Information Sciences, 2022. Codes: this https URL\n",
    "authors": [
      "Yu-Ren Liu",
      "Yi-Qi Hu",
      "Hong Qian",
      "Chao Qian",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1801.00329"
  },
  {
    "id": "arXiv:1803.09941",
    "title": "Iteration-complexity of first-order augmented Lagrangian methods for  convex conic programming",
    "abstract": "Comments: Needs substantial revision",
    "descriptor": "\nComments: Needs substantial revision\n",
    "authors": [
      "Zhaosong Lu",
      "Zirui Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1803.09941"
  },
  {
    "id": "arXiv:1808.01640",
    "title": "Principles of perceptual grouping: implications for image-guided surgery",
    "abstract": "Principles of perceptual grouping: implications for image-guided surgery",
    "descriptor": "",
    "authors": [
      "Birgitta Dresp-Langley"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/1808.01640"
  },
  {
    "id": "arXiv:1810.09177",
    "title": "Compositional Coding Capsule Network with K-Means Routing for Text  Classification",
    "abstract": "Comments: the paper is accepted by Pattern Recognition Letters, please refer this https URL for an updated version",
    "descriptor": "\nComments: the paper is accepted by Pattern Recognition Letters, please refer this https URL for an updated version\n",
    "authors": [
      "Hao Ren",
      "Hong Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1810.09177"
  },
  {
    "id": "arXiv:1911.05350",
    "title": "Exponential Convergence Rates of Classification Errors on Learning with  SGD and Random Features",
    "abstract": "Comments: AISTATS2021",
    "descriptor": "\nComments: AISTATS2021\n",
    "authors": [
      "Shingo Yashima",
      "Atsushi Nitanda",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1911.05350"
  },
  {
    "id": "arXiv:2001.10608",
    "title": "You, Me, and IoT: How Internet-Connected Consumer Devices Affect  Interpersonal Relationships",
    "abstract": "Comments: 28 pages, 5 figures, 5 tables, 1 supplemental PDF. Camera-ready version for journal publication. Original title: \"You, Me, and IoT: How Internet-Connected Home Devices Affect Interpersonal Relationships\"",
    "descriptor": "\nComments: 28 pages, 5 figures, 5 tables, 1 supplemental PDF. Camera-ready version for journal publication. Original title: \"You, Me, and IoT: How Internet-Connected Home Devices Affect Interpersonal Relationships\"\n",
    "authors": [
      "Noah Apthorpe",
      "Pardis Emami-Naeini",
      "Arunesh Mathur",
      "Marshini Chetty",
      "Nick Feamster"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2001.10608"
  },
  {
    "id": "arXiv:2004.08282",
    "title": "Distributed Lower Bounds for Ruling Sets",
    "abstract": "Distributed Lower Bounds for Ruling Sets",
    "descriptor": "",
    "authors": [
      "Alkida Balliu",
      "Sebastian Brandt",
      "Dennis Olivetti"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2004.08282"
  },
  {
    "id": "arXiv:2005.06392",
    "title": "On the Global Convergence Rates of Softmax Policy Gradient Methods",
    "abstract": "Comments: 64 pages, 5 figures. Published in ICML 2020",
    "descriptor": "\nComments: 64 pages, 5 figures. Published in ICML 2020\n",
    "authors": [
      "Jincheng Mei",
      "Chenjun Xiao",
      "Csaba Szepesvari",
      "Dale Schuurmans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.06392"
  },
  {
    "id": "arXiv:2005.08304",
    "title": "Understanding Nesterov's Acceleration via Proximal Point Method",
    "abstract": "Comments: 14 pages; Presented at SIAM Symposium on Simplicity in Algorithms (SOSA22), January 10 - 11, 2022",
    "descriptor": "\nComments: 14 pages; Presented at SIAM Symposium on Simplicity in Algorithms (SOSA22), January 10 - 11, 2022\n",
    "authors": [
      "Kwangjun Ahn",
      "Suvrit Sra"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2005.08304"
  },
  {
    "id": "arXiv:2006.12938",
    "title": "Multi-source Domain Adaptation via Weighted Joint Distributions Optimal  Transport",
    "abstract": "Comments: Accepted at UAI 2022",
    "descriptor": "\nComments: Accepted at UAI 2022\n",
    "authors": [
      "Rosanna Turrisi",
      "R\u00e9mi Flamary",
      "Alain Rakotomamonjy",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.12938"
  },
  {
    "id": "arXiv:2006.16924",
    "title": "Quantum algorithm for Petz recovery channels and pretty good  measurements",
    "abstract": "Comments: v2: 10 pages, accepted for publication in Physical Review Letters",
    "descriptor": "\nComments: v2: 10 pages, accepted for publication in Physical Review Letters\n",
    "authors": [
      "Andr\u00e1s Gily\u00e9n",
      "Seth Lloyd",
      "Iman Marvian",
      "Yihui Quek",
      "Mark M. Wilde"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "High Energy Physics - Theory (hep-th)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2006.16924"
  },
  {
    "id": "arXiv:2007.10090",
    "title": "Meet MASKS: A novel Multi-Classifier's verification approach",
    "abstract": "Comments: 34 pages, 12 figures, 1 table",
    "descriptor": "\nComments: 34 pages, 12 figures, 1 table\n",
    "authors": [
      "Amirhoshang Hoseinpour Dehkordi",
      "Majid Alizadeh",
      "Ali Movaghar"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2007.10090"
  },
  {
    "id": "arXiv:2009.01367",
    "title": "Bridging the Gap: Unifying the Training and Evaluation of Neural Network  Binary Classifiers",
    "abstract": "Bridging the Gap: Unifying the Training and Evaluation of Neural Network  Binary Classifiers",
    "descriptor": "",
    "authors": [
      "Nathan Tsoi",
      "Kate Candon",
      "Deyuan Li",
      "Yofti Milkessa",
      "Marynel V\u00e1zquez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.01367"
  },
  {
    "id": "arXiv:2010.09295",
    "title": "A Scalable Nyquist Stability Criterion with Application to Power System  Small-Signal Stability",
    "abstract": "A Scalable Nyquist Stability Criterion with Application to Power System  Small-Signal Stability",
    "descriptor": "",
    "authors": [
      "Joakim Bj\u00f6rk",
      "Karl Henrik Johansson"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.09295"
  },
  {
    "id": "arXiv:2012.05874",
    "title": "Hindsight and Sequential Rationality of Correlated Play",
    "abstract": "Comments: Corrected technical report for the paper with the same title in the proceedings of the thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21), February 2-9, 2021, Virtual. 27 pages and 16 figures",
    "descriptor": "\nComments: Corrected technical report for the paper with the same title in the proceedings of the thirty-fifth AAAI Conference on Artificial Intelligence (AAAI-21), February 2-9, 2021, Virtual. 27 pages and 16 figures\n",
    "authors": [
      "Dustin Morrill",
      "Ryan D'Orazio",
      "Reca Sarfati",
      "Marc Lanctot",
      "James R. Wright",
      "Amy Greenwald",
      "Michael Bowling"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.05874"
  },
  {
    "id": "arXiv:2012.12179",
    "title": "Timely Monitoring of Dynamic Sources with Observations from Multiple  Wireless Sensors",
    "abstract": "Comments: Submitted for publication",
    "descriptor": "\nComments: Submitted for publication\n",
    "authors": [
      "Anders E. Kal\u00f8r",
      "Petar Popovski"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2012.12179"
  },
  {
    "id": "arXiv:2101.02120",
    "title": "Ludii Game Logic Guide",
    "abstract": "Ludii Game Logic Guide",
    "descriptor": "",
    "authors": [
      "\u00c9ric Piette",
      "Cameron Browne",
      "Dennis J. N. J. Soemers"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.02120"
  },
  {
    "id": "arXiv:2101.05151",
    "title": "Temporal Knowledge Graph Forecasting with Neural ODE",
    "abstract": "Comments: accepted at EMNLP 2021",
    "descriptor": "\nComments: accepted at EMNLP 2021\n",
    "authors": [
      "Zhen Han",
      "Zifeng Ding",
      "Yunpu Ma",
      "Yujia Gu",
      "Volker Tresp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.05151"
  },
  {
    "id": "arXiv:2101.07413",
    "title": "Dynamic Privacy Budget Allocation Improves Data Efficiency of  Differentially Private Gradient Descent",
    "abstract": "Comments: Accepted to FAccT'22",
    "descriptor": "\nComments: Accepted to FAccT'22\n",
    "authors": [
      "Junyuan Hong",
      "Zhangyang Wang",
      "Jiayu Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.07413"
  },
  {
    "id": "arXiv:2101.11108",
    "title": "New Riemannian preconditioned algorithms for tensor completion via  polyadic decomposition",
    "abstract": "Comments: 27 Pages, 10 figures, 4 tables",
    "descriptor": "\nComments: 27 Pages, 10 figures, 4 tables\n",
    "authors": [
      "Shuyu Dong",
      "Bin Gao",
      "Yu Guan",
      "Fran\u00e7ois Glineur"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2101.11108"
  },
  {
    "id": "arXiv:2102.03382",
    "title": "SkillBot: Identifying Risky Content for Children in Alexa Skills",
    "abstract": "SkillBot: Identifying Risky Content for Children in Alexa Skills",
    "descriptor": "",
    "authors": [
      "Tu Le",
      "Danny Yuxing Huang",
      "Noah Apthorpe",
      "Yuan Tian"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2102.03382"
  },
  {
    "id": "arXiv:2102.06973",
    "title": "Efficient Deviation Types and Learning for Hindsight Rationality in  Extensive-Form Games",
    "abstract": "Comments: Corrected technical report for the paper with the same title in the proceedings of the thirty-eighth International Conference on Machine Learning (ICML 2021), virtual. 43 pages and 6 figures",
    "descriptor": "\nComments: Corrected technical report for the paper with the same title in the proceedings of the thirty-eighth International Conference on Machine Learning (ICML 2021), virtual. 43 pages and 6 figures\n",
    "authors": [
      "Dustin Morrill",
      "Ryan D'Orazio",
      "Marc Lanctot",
      "James R. Wright",
      "Michael Bowling",
      "Amy Greenwald"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.06973"
  },
  {
    "id": "arXiv:2103.02136",
    "title": "Toward a Scalable Upper Bound for a CVaR-LQ Problem",
    "abstract": "Comments: This version of the article makes almost-everywhere notions explicit",
    "descriptor": "\nComments: This version of the article makes almost-everywhere notions explicit\n",
    "authors": [
      "Margaret P. Chapman",
      "Laurent Lessard"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.02136"
  },
  {
    "id": "arXiv:2104.07719",
    "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with  Attentive Feature Alignment",
    "abstract": "Comments: AAAI 2022 (Oral). Code is available at this https URL",
    "descriptor": "\nComments: AAAI 2022 (Oral). Code is available at this https URL\n",
    "authors": [
      "Guangxing Han",
      "Shiyuan Huang",
      "Jiawei Ma",
      "Yicheng He",
      "Shih-Fu Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2104.07719"
  },
  {
    "id": "arXiv:2104.11426",
    "title": "Regularized Nonlinear Regression for Simultaneously Selecting and  Estimating Key Model Parameters",
    "abstract": "Comments: 13 pages, 4 figures, 2 Tables",
    "descriptor": "\nComments: 13 pages, 4 figures, 2 Tables\n",
    "authors": [
      "Kyubaek Yoon",
      "Hojun You",
      "Wei-Ying Wu",
      "Chae Young Lim",
      "Jongeun Choi",
      "Connor Boss",
      "Ahmed Ramadan",
      "John M. Popovich Jr.",
      "Jacek Cholewicki",
      "N. Peter Reeves",
      "Clark J. Radcliffe"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.11426"
  },
  {
    "id": "arXiv:2105.06072",
    "title": "Leveraging Non-uniformity in First-order Non-convex Optimization",
    "abstract": "Comments: 48 pages, 10 figures. Accepted at ICML 2021",
    "descriptor": "\nComments: 48 pages, 10 figures. Accepted at ICML 2021\n",
    "authors": [
      "Jincheng Mei",
      "Yue Gao",
      "Bo Dai",
      "Csaba Szepesvari",
      "Dale Schuurmans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.06072"
  },
  {
    "id": "arXiv:2105.13015",
    "title": "A recursive representation for decoupling time-state dependent jumps  from jump-diffusion processes",
    "abstract": "Comments: 21 pages, 2 figures",
    "descriptor": "\nComments: 21 pages, 2 figures\n",
    "authors": [
      "Qinjing Qiu",
      "Reiichiro Kawai"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.13015"
  },
  {
    "id": "arXiv:2106.05682",
    "title": "DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced  Semi-Supervised Learning",
    "abstract": "Comments: CVPR 2022; Project page: this https URL",
    "descriptor": "\nComments: CVPR 2022; Project page: this https URL\n",
    "authors": [
      "Youngtaek Oh",
      "Dong-Jin Kim",
      "In So Kweon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05682"
  },
  {
    "id": "arXiv:2106.08171",
    "title": "Evaluating Modules in Graph Contrastive Learning",
    "abstract": "Evaluating Modules in Graph Contrastive Learning",
    "descriptor": "",
    "authors": [
      "Ganqu Cui",
      "Yufeng Du",
      "Cheng Yang",
      "Jie Zhou",
      "Liang Xu",
      "Xing Zhou",
      "Xingyi Cheng",
      "Zhiyuan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08171"
  },
  {
    "id": "arXiv:2106.08928",
    "title": "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent  Neural Networks",
    "abstract": "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent  Neural Networks",
    "descriptor": "",
    "authors": [
      "Leo Kozachkov",
      "Michaela Ennis",
      "Jean-Jacques Slotine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.08928"
  },
  {
    "id": "arXiv:2106.09517",
    "title": "Dynamic Knowledge Distillation With Noise Elimination for RGB-D Salient  Object Detection",
    "abstract": "Dynamic Knowledge Distillation With Noise Elimination for RGB-D Salient  Object Detection",
    "descriptor": "",
    "authors": [
      "Guangyu Ren",
      "Yinxiao Yu",
      "Hengyan Liu",
      "Tania Stathaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.09517"
  },
  {
    "id": "arXiv:2106.11190",
    "title": "A Power-Pool-Based Power Control in Semi-Grant-Free NOMA Transmission",
    "abstract": "A Power-Pool-Based Power Control in Semi-Grant-Free NOMA Transmission",
    "descriptor": "",
    "authors": [
      "Muhammad Fayaz",
      "Wenqiang Yi",
      "Yuanwei Liu",
      "Arumugam Nallanathan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2106.11190"
  },
  {
    "id": "arXiv:2106.11299",
    "title": "Boundary Graph Neural Networks for 3D Simulations",
    "abstract": "Boundary Graph Neural Networks for 3D Simulations",
    "descriptor": "",
    "authors": [
      "Andreas Mayr",
      "Sebastian Lehner",
      "Arno Mayrhofer",
      "Christoph Kloss",
      "Sepp Hochreiter",
      "Johannes Brandstetter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.11299"
  },
  {
    "id": "arXiv:2106.15910",
    "title": "Graph Signal Restoration Using Nested Deep Algorithm Unrolling",
    "abstract": "Graph Signal Restoration Using Nested Deep Algorithm Unrolling",
    "descriptor": "",
    "authors": [
      "Masatoshi Nagahama",
      "Koki Yamada",
      "Yuichi Tanaka",
      "Stanley H. Chan",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.15910"
  },
  {
    "id": "arXiv:2107.01378",
    "title": "Learning Efficient Vision Transformers via Fine-Grained Manifold  Distillation",
    "abstract": "Learning Efficient Vision Transformers via Fine-Grained Manifold  Distillation",
    "descriptor": "",
    "authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Ding Jia",
      "Kai Han",
      "Yehui Tang",
      "Chao Zhang",
      "Han Hu",
      "Yunhe Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.01378"
  },
  {
    "id": "arXiv:2107.07015",
    "title": "Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI  Interactions",
    "abstract": "Comments: Conference on Artificial Intelligence, Ethics, and Society (AIES 2022)",
    "descriptor": "\nComments: Conference on Artificial Intelligence, Ethics, and Society (AIES 2022)\n",
    "authors": [
      "Kailas Vodrahalli",
      "Roxana Daneshjou",
      "Tobias Gerstenberg",
      "James Zou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2107.07015"
  },
  {
    "id": "arXiv:2107.12838",
    "title": "Graph Autoencoders for Embedding Learning in Brain Networks and Major  Depressive Disorder Identification",
    "abstract": "Graph Autoencoders for Embedding Learning in Brain Networks and Major  Depressive Disorder Identification",
    "descriptor": "",
    "authors": [
      "Fuad Noman",
      "Chee-Ming Ting",
      "Hakmook Kang",
      "Raphael C.-W. Phan",
      "Brian D. Boyd",
      "Warren D. Taylor",
      "Hernando Ombao"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.12838"
  },
  {
    "id": "arXiv:2107.14126",
    "title": "The Complexity of Growing a Graph",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "George B. Mertzios",
      "Othon Michail",
      "George Skretas",
      "Paul G. Spirakis",
      "Michail Theofilatos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2107.14126"
  },
  {
    "id": "arXiv:2107.14221",
    "title": "Can't See The Forest for the Trees: Navigating Metric Spaces by Bounded  Hop-Diameter Spanners",
    "abstract": "Comments: Abstract truncated to fit arXiv limits",
    "descriptor": "\nComments: Abstract truncated to fit arXiv limits\n",
    "authors": [
      "Omri Kahalon",
      "Hung Le",
      "Lazar Milenkovic",
      "Shay Solomon"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2107.14221"
  },
  {
    "id": "arXiv:2107.14264",
    "title": "An Information-Theoretic Approach to Joint Sensing and Communication",
    "abstract": "An Information-Theoretic Approach to Joint Sensing and Communication",
    "descriptor": "",
    "authors": [
      "Mehrasa Ahmadipour",
      "Mari Kobayashi",
      "Miche`le Wigger",
      "Giuseppe Caire"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.14264"
  },
  {
    "id": "arXiv:2108.05713",
    "title": "Towards real-world navigation with deep differentiable planners",
    "abstract": "Comments: Published in CVPR 2022 (Conference on Computer Vision and Pattern Recognition)",
    "descriptor": "\nComments: Published in CVPR 2022 (Conference on Computer Vision and Pattern Recognition)\n",
    "authors": [
      "Shu Ishida",
      "Jo\u00e3o F. Henriques"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.05713"
  },
  {
    "id": "arXiv:2108.06844",
    "title": "Rate-Splitting Multiple Access for Downlink MIMO: A Generalized Power  Iteration Approach",
    "abstract": "Comments: submitted to possible IEEE publication",
    "descriptor": "\nComments: submitted to possible IEEE publication\n",
    "authors": [
      "Jeonghun Park",
      "Jinseok Choi",
      "Namyoon Lee",
      "Wonjae Shin",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2108.06844"
  },
  {
    "id": "arXiv:2108.07926",
    "title": "Collaboration Equilibrium in Federated Learning",
    "abstract": "Comments: This paper is accepted by SIGKDD2022",
    "descriptor": "\nComments: This paper is accepted by SIGKDD2022\n",
    "authors": [
      "Sen Cui",
      "Jian Liang",
      "Weishen Pan",
      "Kun Chen",
      "Changshui Zhang",
      "Fei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.07926"
  },
  {
    "id": "arXiv:2108.09190",
    "title": "Supervised Contrastive Learning for Interpretable Long-Form Document  Matching",
    "abstract": "Supervised Contrastive Learning for Interpretable Long-Form Document  Matching",
    "descriptor": "",
    "authors": [
      "Akshita Jha",
      "Vineeth Rakesh",
      "Jaideep Chandrashekar",
      "Adithya Samavedhi",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2108.09190"
  },
  {
    "id": "arXiv:2108.09875",
    "title": "Anarchic Federated Learning",
    "abstract": "Comments: Accepted for Long Presentation in ICML 2022",
    "descriptor": "\nComments: Accepted for Long Presentation in ICML 2022\n",
    "authors": [
      "Haibo Yang",
      "Xin Zhang",
      "Prashant Khanduri",
      "Jia Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2108.09875"
  },
  {
    "id": "arXiv:2108.10392",
    "title": "A generalized stacked reinforcement learning method for sampled systems",
    "abstract": "A generalized stacked reinforcement learning method for sampled systems",
    "descriptor": "",
    "authors": [
      "Pavel Osinenko",
      "Dmitrii Dobriborsci",
      "Grigory Yaremenko"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.10392"
  },
  {
    "id": "arXiv:2109.00389",
    "title": "Optimization problems in graphs with locational uncertainty",
    "abstract": "Optimization problems in graphs with locational uncertainty",
    "descriptor": "",
    "authors": [
      "Marin Bougeret",
      "J\u00e9r\u00e9my Omer",
      "Michael Poss"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2109.00389"
  },
  {
    "id": "arXiv:2109.03025",
    "title": "Linear equations for unordered data vectors in $[D]^k\\to{}Z^d$",
    "abstract": "Comments: 39 pages",
    "descriptor": "\nComments: 39 pages\n",
    "authors": [
      "Piotr Hofman",
      "Jakub R\u00f3\u017cycki"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2109.03025"
  },
  {
    "id": "arXiv:2109.04566",
    "title": "SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural  Networks",
    "abstract": "Comments: 7 pages, 5 figures",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Kiran Karra",
      "Chace Ashcraft",
      "Cash Costello"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04566"
  },
  {
    "id": "arXiv:2109.06375",
    "title": "Adaptive Constrained Kinematic Control using Partial or Complete  Task-Space Measurements",
    "abstract": "Comments: Accepted on T-RO 2022, 16 Pages",
    "descriptor": "\nComments: Accepted on T-RO 2022, 16 Pages\n",
    "authors": [
      "Murilo Marques Marinho",
      "Bruno Vilhena Adorno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.06375"
  },
  {
    "id": "arXiv:2109.11941",
    "title": "Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and  Landmark Localization on 3D Intraoral Scans",
    "abstract": "Comments: 9 pages, 8 figures, accepted by IEEE TMI",
    "descriptor": "\nComments: 9 pages, 8 figures, accepted by IEEE TMI\n",
    "authors": [
      "Tai-Hsien Wu",
      "Chunfeng Lian",
      "Sanghee Lee",
      "Matthew Pastewait",
      "Christian Piers",
      "Jie Liu",
      "Fang Wang",
      "Li Wang",
      "Chiung-Ying Chiu",
      "Wenchi Wang",
      "Christina Jackson",
      "Wei-Lun Chao",
      "Dinggang Shen",
      "Ching-Chang Ko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.11941"
  },
  {
    "id": "arXiv:2110.00546",
    "title": "Discovery of interpretable structural model errors by combining Bayesian  sparse regression and data assimilation: A chaotic Kuramoto-Sivashinsky test  case",
    "abstract": "Comments: 9 pages, 2 figures",
    "descriptor": "\nComments: 9 pages, 2 figures\n",
    "authors": [
      "Rambod Mojgani",
      "Ashesh Chattopadhyay",
      "Pedram Hassanzadeh"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.00546"
  },
  {
    "id": "arXiv:2110.00643",
    "title": "Distributed $\u0394$-Coloring Plays Hide-and-Seek",
    "abstract": "Distributed $\u0394$-Coloring Plays Hide-and-Seek",
    "descriptor": "",
    "authors": [
      "Alkida Balliu",
      "Sebastian Brandt",
      "Fabian Kuhn",
      "Dennis Olivetti"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.00643"
  },
  {
    "id": "arXiv:2110.02027",
    "title": "ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning",
    "abstract": "Comments: Accetpted at ICML 2022",
    "descriptor": "\nComments: Accetpted at ICML 2022\n",
    "authors": [
      "Jun Xia",
      "Lirong Wu",
      "Ge Wang",
      "Jintao Chen",
      "Stan Z.Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.02027"
  },
  {
    "id": "arXiv:2110.02488",
    "title": "ABC: Attention with Bounded-memory Control",
    "abstract": "ABC: Attention with Bounded-memory Control",
    "descriptor": "",
    "authors": [
      "Hao Peng",
      "Jungo Kasai",
      "Nikolaos Pappas",
      "Dani Yogatama",
      "Zhaofeng Wu",
      "Lingpeng Kong",
      "Roy Schwartz",
      "Noah A. Smith"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.02488"
  },
  {
    "id": "arXiv:2110.02711",
    "title": "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image  Manipulation",
    "abstract": "Comments: Accepted to CVPR 2022",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Gwanghyun Kim",
      "Taesung Kwon",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02711"
  },
  {
    "id": "arXiv:2110.02771",
    "title": "DNN-assisted Particle-based Bayesian Joint Synchronization and  Localization",
    "abstract": "DNN-assisted Particle-based Bayesian Joint Synchronization and  Localization",
    "descriptor": "",
    "authors": [
      "Meysam Goodarzi",
      "Vladica Sark",
      "Nebojsa Maletic",
      "Jes\u00fas Guti\u00e9rrez",
      "Giuseppe Caire",
      "Eckhard Grass"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.02771"
  },
  {
    "id": "arXiv:2110.03605",
    "title": "Robust Feature-Level Adversaries are Interpretability Tools",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Stephen Casper",
      "Max Nadeau",
      "Dylan Hadfield-Menell",
      "Gabriel Kreiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03605"
  },
  {
    "id": "arXiv:2110.04079",
    "title": "A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection",
    "abstract": "Comments: 18 pages, 5 figures. Published by Computer-Aided Civil and Infrastructure Engineering (CACIE). Open access from this https URL",
    "descriptor": "\nComments: 18 pages, 5 figures. Published by Computer-Aided Civil and Infrastructure Engineering (CACIE). Open access from this https URL\n",
    "authors": [
      "Yongqi Dong",
      "Sandeep Patil",
      "Bart van Arem",
      "Haneen Farah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.04079"
  },
  {
    "id": "arXiv:2110.05088",
    "title": "Privacy-Preserving Feature Selection with Fully Homomorphic Encryption",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Shinji Ono",
      "Jun Takata",
      "Masaharu Kataoka",
      "Tomohiro I",
      "Kilho Shin",
      "Hiroshi Sakamoto"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.05088"
  },
  {
    "id": "arXiv:2110.11524",
    "title": "Sequential Voting with Relational Box Fields for Active Object Detection",
    "abstract": "Comments: In CVPR 2022. Project: this https URL",
    "descriptor": "\nComments: In CVPR 2022. Project: this https URL\n",
    "authors": [
      "Qichen Fu",
      "Xingyu Liu",
      "Kris M. Kitani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.11524"
  },
  {
    "id": "arXiv:2110.13374",
    "title": "Defining Blockchain Governance Principles: A Comprehensive Framework",
    "abstract": "Comments: Submitted to Information Systems, Elsevier",
    "descriptor": "\nComments: Submitted to Information Systems, Elsevier\n",
    "authors": [
      "Yue Liu",
      "Qinghua Lu",
      "Guangsheng Yu",
      "Hye-Young Paik",
      "Liming Zhu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.13374"
  },
  {
    "id": "arXiv:2110.14144",
    "title": "Physically Explainable CNN for SAR Image Classification",
    "abstract": "Physically Explainable CNN for SAR Image Classification",
    "descriptor": "",
    "authors": [
      "Zhongling Huang",
      "Xiwen Yao",
      "Ying Liu",
      "Corneliu Octavian Dumitru",
      "Mihai Datcu",
      "Junwei Han"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14144"
  },
  {
    "id": "arXiv:2110.15532",
    "title": "Contact Transfer: A Direct, User-Driven Method for Human to Robot  Transfer of Grasps and Manipulations",
    "abstract": "Contact Transfer: A Direct, User-Driven Method for Human to Robot  Transfer of Grasps and Manipulations",
    "descriptor": "",
    "authors": [
      "Arjun Lakshmipathy",
      "Dominik Bauer",
      "Cornelia Bauer",
      "Nancy S. Pollard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.15532"
  },
  {
    "id": "arXiv:2111.00537",
    "title": "A network-based approach to QAnon user dynamics and topic diversity  during the COVID-19 infodemic",
    "abstract": "Comments: accepted by APSIPA Transactions on Signal and Information Processing (Special issue: Multi-Disciplinary Dis/Misinformation Analysis and Countermeasures), 2022",
    "descriptor": "\nComments: accepted by APSIPA Transactions on Signal and Information Processing (Special issue: Multi-Disciplinary Dis/Misinformation Analysis and Countermeasures), 2022\n",
    "authors": [
      "Wentao Xu",
      "Kazutoshi Sasahara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2111.00537"
  },
  {
    "id": "arXiv:2111.00898",
    "title": "Availability Attacks Create Shortcuts",
    "abstract": "Comments: Published as a research track paper at KDD 2022",
    "descriptor": "\nComments: Published as a research track paper at KDD 2022\n",
    "authors": [
      "Da Yu",
      "Huishuai Zhang",
      "Wei Chen",
      "Jian Yin",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2111.00898"
  },
  {
    "id": "arXiv:2111.04881",
    "title": "Combining machine learning with physics: A framework for tracking and  sorting multiple dark solitons",
    "abstract": "Comments: 13 pages, 9 figures",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Shangjie Guo",
      "Sophia M. Koh",
      "Amilson R. Fritsch",
      "I. B. Spielman",
      "Justyna P. Zwolak"
    ],
    "subjectives": [
      "Quantum Gases (cond-mat.quant-gas)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2111.04881"
  },
  {
    "id": "arXiv:2111.07845",
    "title": "Metric dimension on sparse graphs and its applications to zero forcing  sets",
    "abstract": "Metric dimension on sparse graphs and its applications to zero forcing  sets",
    "descriptor": "",
    "authors": [
      "Nicolas Bousquet",
      "Quentin Deschamps",
      "Aline Parreau",
      "Ignacio M. Pelayo"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2111.07845"
  },
  {
    "id": "arXiv:2112.01693",
    "title": "Optimism brings accurate perception in Iterated Prisoner's Dilemma",
    "abstract": "Comments: 9 pages, 5 figures",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Orhun Gorkem",
      "Haluk O. Bingol"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2112.01693"
  },
  {
    "id": "arXiv:2112.03093",
    "title": "Communication Beyond Transmitting Bits: Semantics-Guided Source and  Channel Coding",
    "abstract": "Communication Beyond Transmitting Bits: Semantics-Guided Source and  Channel Coding",
    "descriptor": "",
    "authors": [
      "Jincheng Dai",
      "Ping Zhang",
      "Kai Niu",
      "Sixian Wang",
      "Zhongwei Si",
      "Xiaoqi Qin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2112.03093"
  },
  {
    "id": "arXiv:2112.04351",
    "title": "Sentiment Analysis and Effect of COVID-19 Pandemic using College  SubReddit Data",
    "abstract": "Sentiment Analysis and Effect of COVID-19 Pandemic using College  SubReddit Data",
    "descriptor": "",
    "authors": [
      "Tian Yan",
      "Fang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.04351"
  },
  {
    "id": "arXiv:2112.05141",
    "title": "Exploring the Equivalence of Siamese Self-Supervised Learning via A  Unified Gradient Framework",
    "abstract": "Comments: CVPR2022",
    "descriptor": "\nComments: CVPR2022\n",
    "authors": [
      "Chenxin Tao",
      "Honghui Wang",
      "Xizhou Zhu",
      "Jiahua Dong",
      "Shiji Song",
      "Gao Huang",
      "Jifeng Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.05141"
  },
  {
    "id": "arXiv:2112.06920",
    "title": "Boosting Independent Component Analysis",
    "abstract": "Boosting Independent Component Analysis",
    "descriptor": "",
    "authors": [
      "Yunpeng Li",
      "ZhaoHui Ye"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.06920"
  },
  {
    "id": "arXiv:2112.08615",
    "title": "Knowledge-Augmented Language Models for Cause-Effect Relation  Classification",
    "abstract": "Comments: Accepted to Commonsense Representation and Reasoning (CSRR) @ ACL 2022",
    "descriptor": "\nComments: Accepted to Commonsense Representation and Reasoning (CSRR) @ ACL 2022\n",
    "authors": [
      "Pedram Hosseini",
      "David A. Broniatowski",
      "Mona Diab"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08615"
  },
  {
    "id": "arXiv:2112.08907",
    "title": "Inherently Explainable Reinforcement Learning in Natural Language",
    "abstract": "Inherently Explainable Reinforcement Learning in Natural Language",
    "descriptor": "",
    "authors": [
      "Xiangyu Peng",
      "Mark O. Riedl",
      "Prithviraj Ammanabrolu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08907"
  },
  {
    "id": "arXiv:2112.09072",
    "title": "Sampling Trade-Offs in Duty-Cycled Systems for Air Quality Low-Cost  Sensors",
    "abstract": "Comments: Article published in Sensors MDPI special issue: \"Air Quality Internet of Things Devices\"",
    "descriptor": "\nComments: Article published in Sensors MDPI special issue: \"Air Quality Internet of Things Devices\"\n",
    "authors": [
      "Pau Ferrer-Cid",
      "Julio Garcia-Calvete",
      "Aina Main-Nadal",
      "Zhe Ye",
      "Jose M. Barcelo-Ordinas",
      "Jorge Garcia-Vidal"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2112.09072"
  },
  {
    "id": "arXiv:2112.09332",
    "title": "WebGPT: Browser-assisted question-answering with human feedback",
    "abstract": "Comments: 32 pages",
    "descriptor": "\nComments: 32 pages\n",
    "authors": [
      "Reiichiro Nakano",
      "Jacob Hilton",
      "Suchir Balaji",
      "Jeff Wu",
      "Long Ouyang",
      "Christina Kim",
      "Christopher Hesse",
      "Shantanu Jain",
      "Vineet Kosaraju",
      "William Saunders",
      "Xu Jiang",
      "Karl Cobbe",
      "Tyna Eloundou",
      "Gretchen Krueger",
      "Kevin Button",
      "Matthew Knight",
      "Benjamin Chess",
      "John Schulman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09332"
  },
  {
    "id": "arXiv:2112.10852",
    "title": "The effective noise of Stochastic Gradient Descent",
    "abstract": "Comments: 7 pages + appendix, 5 figures",
    "descriptor": "\nComments: 7 pages + appendix, 5 figures\n",
    "authors": [
      "Francesca Mignacco",
      "Pierfrancesco Urbani"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.10852"
  },
  {
    "id": "arXiv:2112.12342",
    "title": "Individual and Group-wise Classroom Seating Experience: Effects on  Student Engagement in Different Courses",
    "abstract": "Individual and Group-wise Classroom Seating Experience: Effects on  Student Engagement in Different Courses",
    "descriptor": "",
    "authors": [
      "Nan Gao",
      "Mohammad Saiedur Rahaman",
      "Wei Shao",
      "Kaixin Ji",
      "Flora D. Salim"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2112.12342"
  },
  {
    "id": "arXiv:2112.13985",
    "title": "LatteGAN: Visually Guided Language Attention for Multi-Turn  Text-Conditioned Image Manipulation",
    "abstract": "LatteGAN: Visually Guided Language Attention for Multi-Turn  Text-Conditioned Image Manipulation",
    "descriptor": "",
    "authors": [
      "Shoya Matsumori",
      "Yuki Abe",
      "Kosuke Shingyouchi",
      "Komei Sugiura",
      "Michita Imai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.13985"
  },
  {
    "id": "arXiv:2112.15217",
    "title": "VisRecall: Quantifying Information Visualisation Recallability via  Question Answering",
    "abstract": "Comments: 10 pages, 10 figures",
    "descriptor": "\nComments: 10 pages, 10 figures\n",
    "authors": [
      "Yao Wang",
      "Chuhan Jiao",
      "Mihai B\u00e2ce",
      "Andreas Bulling"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2112.15217"
  },
  {
    "id": "arXiv:2112.15577",
    "title": "How Infinitely Wide Neural Networks Benefit from Multi-task Learning --  an Exact Macroscopic Characterization",
    "abstract": "Comments: 9 pages + appendix",
    "descriptor": "\nComments: 9 pages + appendix\n",
    "authors": [
      "Jakob Heiss",
      "Josef Teichmann",
      "Hanna Wutte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2112.15577"
  },
  {
    "id": "arXiv:2201.03916",
    "title": "Automated Reinforcement Learning (AutoRL): A Survey and Open Problems",
    "abstract": "Comments: Published in JAIR. Co-first authors and co-last authors are listed in alphabetical order",
    "descriptor": "\nComments: Published in JAIR. Co-first authors and co-last authors are listed in alphabetical order\n",
    "authors": [
      "Jack Parker-Holder",
      "Raghu Rajan",
      "Xingyou Song",
      "Andr\u00e9 Biedenkapp",
      "Yingjie Miao",
      "Theresa Eimer",
      "Baohe Zhang",
      "Vu Nguyen",
      "Roberto Calandra",
      "Aleksandra Faust",
      "Frank Hutter",
      "Marius Lindauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.03916"
  },
  {
    "id": "arXiv:2201.05745",
    "title": "Deep Optimal Transport for Domain Adaptation on SPD Manifolds",
    "abstract": "Comments: 14 pages, 7 figures, and 6 tables; This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: 14 pages, 7 figures, and 6 tables; This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Ce Ju",
      "Cuntai Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.05745"
  },
  {
    "id": "arXiv:2201.06446",
    "title": "Privacy-Preserving Maximum Matching on General Graphs and its  Application to Enable Privacy-Preserving Kidney Exchange",
    "abstract": "Comments: This is the extended version of the paper that appeared in the 12th ACM Conference on Data and Application Security and Privacy (CODASPY'22), April 24-26, 2022, Baltimore-Washington DC Area, United States, this https URL",
    "descriptor": "\nComments: This is the extended version of the paper that appeared in the 12th ACM Conference on Data and Application Security and Privacy (CODASPY'22), April 24-26, 2022, Baltimore-Washington DC Area, United States, this https URL\n",
    "authors": [
      "Malte Breuer",
      "Ulrike Meyer",
      "Susanne Wetzel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.06446"
  },
  {
    "id": "arXiv:2201.06651",
    "title": "Limited Information Shared Control: A Potential Game Approach",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Balint Varga",
      "Jairo Inga",
      "Soeren Hohmann"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.06651"
  },
  {
    "id": "arXiv:2201.08418",
    "title": "SoftDropConnect (SDC) -- Effective and Efficient Quantification of the  Network Uncertainty in Deep MR Image Analysis",
    "abstract": "SoftDropConnect (SDC) -- Effective and Efficient Quantification of the  Network Uncertainty in Deep MR Image Analysis",
    "descriptor": "",
    "authors": [
      "Qing Lyu",
      "Christopher T. Whitlow",
      "Ge Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.08418"
  },
  {
    "id": "arXiv:2201.10633",
    "title": "Informative Path Planning to Estimate Quantiles for Environmental  Analysis",
    "abstract": "Comments: 8 pages, 9 figures",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Isabel M. Rayas Fern\u00e1ndez",
      "Christopher E. Denniston",
      "David A. Caron",
      "Gaurav S. Sukhatme"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.10633"
  },
  {
    "id": "arXiv:2201.10842",
    "title": "Voronoi cell analysis: The shapes of particle systems",
    "abstract": "Comments: 14 pages, 12 figures",
    "descriptor": "\nComments: 14 pages, 12 figures\n",
    "authors": [
      "Emanuel A. Lazar",
      "Jiayin Lu",
      "Chris H. Rycroft"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Geometry (cs.CG)",
      "Numerical Analysis (math.NA)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.10842"
  },
  {
    "id": "arXiv:2201.11300",
    "title": "Geo-obfuscation Mechanisms for Spatial Crowdsourcing via Multi-Objective  Evolutionary Optimization",
    "abstract": "Geo-obfuscation Mechanisms for Spatial Crowdsourcing via Multi-Objective  Evolutionary Optimization",
    "descriptor": "",
    "authors": [
      "Shun Zhang",
      "Tao Zhang",
      "Zhili Chen",
      "Stan Z. Li",
      "Shenghui Cheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.11300"
  },
  {
    "id": "arXiv:2201.12514",
    "title": "Composing a surrogate observation operator for sequential data  assimilation",
    "abstract": "Composing a surrogate observation operator for sequential data  assimilation",
    "descriptor": "",
    "authors": [
      "Kosuke Akita",
      "Yuto Miyatake",
      "Daisuke Furihata"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2201.12514"
  },
  {
    "id": "arXiv:2202.00293",
    "title": "Phase diagram of Stochastic Gradient Descent in high-dimensional  two-layer neural networks",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Rodrigo Veiga",
      "Ludovic Stephan",
      "Bruno Loureiro",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.00293"
  },
  {
    "id": "arXiv:2202.00661",
    "title": "A Fair Comparison of Two Popular Flat Minima Optimizers: Stochastic  Weight Averaging vs. Sharpness-Aware Minimization",
    "abstract": "A Fair Comparison of Two Popular Flat Minima Optimizers: Stochastic  Weight Averaging vs. Sharpness-Aware Minimization",
    "descriptor": "",
    "authors": [
      "Jean Kaddour",
      "Linqing Liu",
      "Ricardo Silva",
      "Matt J. Kusner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00661"
  },
  {
    "id": "arXiv:2202.01769",
    "title": "Improving Automatic Complexity Analysis of Integer Programs",
    "abstract": "Improving Automatic Complexity Analysis of Integer Programs",
    "descriptor": "",
    "authors": [
      "J\u00fcrgen Giesl",
      "Nils Lommen",
      "Marcel Hark",
      "Fabian Meyer"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2202.01769"
  },
  {
    "id": "arXiv:2202.01771",
    "title": "Pre-Trained Language Models for Interactive Decision-Making",
    "abstract": "Pre-Trained Language Models for Interactive Decision-Making",
    "descriptor": "",
    "authors": [
      "Shuang Li",
      "Xavier Puig",
      "Chris Paxton",
      "Yilun Du",
      "Clinton Wang",
      "Linxi Fan",
      "Tao Chen",
      "De-An Huang",
      "Ekin Aky\u00fcrek",
      "Anima Anandkumar",
      "Jacob Andreas",
      "Igor Mordatch",
      "Antonio Torralba",
      "Yuke Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.01771"
  },
  {
    "id": "arXiv:2202.02110",
    "title": "New Inner and Outer Bounds for Gaussian Broadcast Channels with  Heterogeneous Blocklength Constraints",
    "abstract": "Comments: 4 Figures",
    "descriptor": "\nComments: 4 Figures\n",
    "authors": [
      "Marcel Mross",
      "Pin-Hsun Lin",
      "Eduard A. Jorswieck"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.02110"
  },
  {
    "id": "arXiv:2202.02989",
    "title": "Graph Self-supervised Learning with Accurate Discrepancy Learning",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Dongki Kim",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.02989"
  },
  {
    "id": "arXiv:2202.03091",
    "title": "Auto-Lambda: Disentangling Dynamic Task Relationships",
    "abstract": "Comments: Published at TMLR 2022. Project Page: this https URL Code: this https URL",
    "descriptor": "\nComments: Published at TMLR 2022. Project Page: this https URL Code: this https URL\n",
    "authors": [
      "Shikun Liu",
      "Stephen James",
      "Andrew J. Davison",
      "Edward Johns"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.03091"
  },
  {
    "id": "arXiv:2202.03799",
    "title": "What are the best systems? New perspectives on NLP Benchmarking",
    "abstract": "What are the best systems? New perspectives on NLP Benchmarking",
    "descriptor": "",
    "authors": [
      "Pierre Colombo",
      "Nathan Noiry",
      "Ekhine Irurozki",
      "Stephan Clemencon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.03799"
  },
  {
    "id": "arXiv:2202.05262",
    "title": "Locating and Editing Factual Associations in GPT",
    "abstract": "Comments: 28 pages, 23 figures. Code and data at this https URL",
    "descriptor": "\nComments: 28 pages, 23 figures. Code and data at this https URL\n",
    "authors": [
      "Kevin Meng",
      "David Bau",
      "Alex Andonian",
      "Yonatan Belinkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05262"
  },
  {
    "id": "arXiv:2202.05983",
    "title": "Uncalibrated Models Can Improve Human-AI Collaboration",
    "abstract": "Comments: 19 pages, 10 figures, in submission",
    "descriptor": "\nComments: 19 pages, 10 figures, in submission\n",
    "authors": [
      "Kailas Vodrahalli",
      "Tobias Gerstenberg",
      "James Zou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05983"
  },
  {
    "id": "arXiv:2202.07409",
    "title": "A Lower Bounding Framework for Motion Planning amid Dynamic Obstacles in  2D",
    "abstract": "Comments: WAFR 2022, 16 pages, 6 figures",
    "descriptor": "\nComments: WAFR 2022, 16 pages, 6 figures\n",
    "authors": [
      "Zhongqiang Ren",
      "Sivakumar Rathinam",
      "Howie Choset"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.07409"
  },
  {
    "id": "arXiv:2202.07716",
    "title": "Learning Model Predictive Control for Quadrotors",
    "abstract": "Comments: Guanrui Li and Alex Tunchez contributed equally to this paper. This paper has been accepted to the 2022 IEEE International Conference on Robotics and Automation. Please cite this paper with the standard IEEE Conference format. Link to the Video: this https URL",
    "descriptor": "\nComments: Guanrui Li and Alex Tunchez contributed equally to this paper. This paper has been accepted to the 2022 IEEE International Conference on Robotics and Automation. Please cite this paper with the standard IEEE Conference format. Link to the Video: this https URL\n",
    "authors": [
      "Guanrui Li",
      "Alex Tunchez",
      "Giuseppe Loianno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.07716"
  },
  {
    "id": "arXiv:2202.09710",
    "title": "A Barrier Certificate-based Simplex Architecture with Application to  Microgrids",
    "abstract": "A Barrier Certificate-based Simplex Architecture with Application to  Microgrids",
    "descriptor": "",
    "authors": [
      "Amol Damare",
      "Shouvik Roy",
      "Scott A. Smolka",
      "Scott D. Stoller"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.09710"
  },
  {
    "id": "arXiv:2202.10752",
    "title": "Event-Triggered Tracking Control of Networked Multi-Agent Systems",
    "abstract": "Comments: 16 pages, 8 figures, 1 table, Accepted by IEEE Transactions on Automatic Control. arXiv admin note: text overlap with arXiv:2110.09786",
    "descriptor": "\nComments: 16 pages, 8 figures, 1 table, Accepted by IEEE Transactions on Automatic Control. arXiv admin note: text overlap with arXiv:2110.09786\n",
    "authors": [
      "Wei Ren",
      "Dimos V. Dimarogonas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2202.10752"
  },
  {
    "id": "arXiv:2202.11089",
    "title": "Counterfactual Phenotyping with Censored Time-to-Events",
    "abstract": "Comments: KDD 2022 Applied Data Science Paper",
    "descriptor": "\nComments: KDD 2022 Applied Data Science Paper\n",
    "authors": [
      "Chirag Nagpal",
      "Mononito Goswami",
      "Keith Dufendach",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.11089"
  },
  {
    "id": "arXiv:2202.11678",
    "title": "Bayesian Model Selection, the Marginal Likelihood, and Generalization",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Sanae Lotfi",
      "Pavel Izmailov",
      "Gregory Benton",
      "Micah Goldblum",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.11678"
  },
  {
    "id": "arXiv:2202.12002",
    "title": "Rare Gems: Finding Lottery Tickets at Initialization",
    "abstract": "Rare Gems: Finding Lottery Tickets at Initialization",
    "descriptor": "",
    "authors": [
      "Kartik Sreenivasan",
      "Jy-yong Sohn",
      "Liu Yang",
      "Matthew Grinde",
      "Alliot Nagle",
      "Hongyi Wang",
      "Eric Xing",
      "Kangwook Lee",
      "Dimitris Papailiopoulos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.12002"
  },
  {
    "id": "arXiv:2202.13884",
    "title": "Numeric Lyndon-based feature embedding of sequencing reads for machine  learning approaches",
    "abstract": "Numeric Lyndon-based feature embedding of sequencing reads for machine  learning approaches",
    "descriptor": "",
    "authors": [
      "Paola Bonizzoni",
      "Matteo Costantini",
      "Clelia De Felice",
      "Alessia Petescia",
      "Yuri Pirola",
      "Marco Previtali",
      "Raffaella Rizzi",
      "Jens Stoye",
      "Rocco Zaccagnino",
      "Rosalba Zizza"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.13884"
  },
  {
    "id": "arXiv:2203.01877",
    "title": "Query Processing on Tensor Computation Runtimes",
    "abstract": "Query Processing on Tensor Computation Runtimes",
    "descriptor": "",
    "authors": [
      "Dong He",
      "Supun Nakandala",
      "Dalitso Banda",
      "Rathijit Sen",
      "Karla Saur",
      "Kwanghyun Park",
      "Carlo Curino",
      "Jes\u00fas Camacho-Rodr\u00edguez",
      "Konstantinos Karanasos",
      "Matteo Interlandi"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.01877"
  },
  {
    "id": "arXiv:2203.02298",
    "title": "Intrinsically-Motivated Reinforcement Learning: A Brief Introduction",
    "abstract": "Comments: 40 pages, 25 figures",
    "descriptor": "\nComments: 40 pages, 25 figures\n",
    "authors": [
      "Mingqi Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.02298"
  },
  {
    "id": "arXiv:2203.02635",
    "title": "Training privacy-preserving video analytics pipelines by suppressing  features that reveal information about private attributes",
    "abstract": "Training privacy-preserving video analytics pipelines by suppressing  features that reveal information about private attributes",
    "descriptor": "",
    "authors": [
      "Chau Yi Li",
      "Andrea Cavallaro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.02635"
  },
  {
    "id": "arXiv:2203.04203",
    "title": "AssistQ: Affordance-centric Question-driven Task Completion for  Egocentric Assistant",
    "abstract": "Comments: 22 pages. Equal contribution: Benita Wong, Joya Chen, You Wu; Corresponding author: Mike Zheng Shou",
    "descriptor": "\nComments: 22 pages. Equal contribution: Benita Wong, Joya Chen, You Wu; Corresponding author: Mike Zheng Shou\n",
    "authors": [
      "Benita Wong",
      "Joya Chen",
      "You Wu",
      "Stan Weixian Lei",
      "Dongxing Mao",
      "Difei Gao",
      "Mike Zheng Shou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04203"
  },
  {
    "id": "arXiv:2203.05395",
    "title": "Annotation Efficient Person Re-Identification with Diverse Cluster-Based  Pair Selection",
    "abstract": "Annotation Efficient Person Re-Identification with Diverse Cluster-Based  Pair Selection",
    "descriptor": "",
    "authors": [
      "Lantian Xue",
      "Yixiong Zou",
      "Peixi Peng",
      "Yonghong Tian",
      "Tiejun Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05395"
  },
  {
    "id": "arXiv:2203.09210",
    "title": "Universal Conditional Masked Language Pre-training for Neural Machine  Translation",
    "abstract": "Comments: Accepted to ACL 2022 Main conference",
    "descriptor": "\nComments: Accepted to ACL 2022 Main conference\n",
    "authors": [
      "Pengfei Li",
      "Liangyou Li",
      "Meng Zhang",
      "Minghao Wu",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.09210"
  },
  {
    "id": "arXiv:2203.10348",
    "title": "Font Generation with Missing Impression Labels",
    "abstract": "Comments: Accepted ICPR2022",
    "descriptor": "\nComments: Accepted ICPR2022\n",
    "authors": [
      "Seiya Matsuda",
      "Akisato Kimura",
      "Seiichi Uchida"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.10348"
  },
  {
    "id": "arXiv:2203.12798",
    "title": "DPar2: Fast and Scalable PARAFAC2 Decomposition for Irregular Dense  Tensors",
    "abstract": "Comments: 14 pages, 11 figures. To appear at the 38th IEEE International Conference on Data Engineering (ICDE '22)",
    "descriptor": "\nComments: 14 pages, 11 figures. To appear at the 38th IEEE International Conference on Data Engineering (ICDE '22)\n",
    "authors": [
      "Jun-Gi Jang",
      "U Kang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2203.12798"
  },
  {
    "id": "arXiv:2203.13262",
    "title": "Interpretability of Neural Network With Physiological Mechanisms",
    "abstract": "Comments: Updated a new version",
    "descriptor": "\nComments: Updated a new version\n",
    "authors": [
      "Anna Zou",
      "Zhiyuan Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2203.13262"
  },
  {
    "id": "arXiv:2204.01100",
    "title": "Strong convergence rates of an explicit scheme for stochastic  Cahn-Hilliard equation with additive noise",
    "abstract": "Comments: 24 pages, 3 figures",
    "descriptor": "\nComments: 24 pages, 3 figures\n",
    "authors": [
      "Meng Cai",
      "Ruisheng Qi",
      "Xiaojie Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2204.01100"
  },
  {
    "id": "arXiv:2204.01640",
    "title": "APP: Anytime Progressive Pruning",
    "abstract": "Comments: 21 pages including 4 pages of references. Preprint version",
    "descriptor": "\nComments: 21 pages including 4 pages of references. Preprint version\n",
    "authors": [
      "Diganta Misra",
      "Bharat Runwal",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Irina Rish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.01640"
  },
  {
    "id": "arXiv:2204.02113",
    "title": "A Restricted SVD type CUR Decomposition for Matrix Triplets",
    "abstract": "A Restricted SVD type CUR Decomposition for Matrix Triplets",
    "descriptor": "",
    "authors": [
      "Perfect Y. Gidisu",
      "Michiel E. Hochstenbach"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.02113"
  },
  {
    "id": "arXiv:2204.05009",
    "title": "VWR2A: A Very-Wide-Register Reconfigurable-Array Architecture for  Low-Power Embedded Devices",
    "abstract": "VWR2A: A Very-Wide-Register Reconfigurable-Array Architecture for  Low-Power Embedded Devices",
    "descriptor": "",
    "authors": [
      "Beno\u00eet Walter Denkinger",
      "Miguel Pe\u00f3n-Quir\u00f3s",
      "Mario Konijnenburg",
      "David Atienza",
      "Francky Catthoor"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2204.05009"
  },
  {
    "id": "arXiv:2204.11484",
    "title": "AQuaMoHo: Localized Low-Cost Outdoor Air Quality Sensing over a  Thermo-Hygrometer",
    "abstract": "Comments: 26 Pages, 17 Figures, Journal",
    "descriptor": "\nComments: 26 Pages, 17 Figures, Journal\n",
    "authors": [
      "Prithviraj Pramanik",
      "Prasenjit Karmakar",
      "Praveen Kumar Sharma",
      "Soumyajit Chatterjee",
      "Abhijit Roy",
      "Santanu Mandal",
      "Subrata Nandi",
      "Sandip Chakraborty",
      "Mousumi Saha",
      "Sujoy Saha"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.11484"
  },
  {
    "id": "arXiv:2204.13369",
    "title": "Joint Sum Rate and Blocklength Optimization in RIS-aided Short Packet  URLLC Systems",
    "abstract": "Comments: Accepted in IEEE Communications Letters",
    "descriptor": "\nComments: Accepted in IEEE Communications Letters\n",
    "authors": [
      "Ramin Hashemi",
      "Samad Ali",
      "Nurul Huda Mahmood",
      "Matti Latva-aho"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2204.13369"
  },
  {
    "id": "arXiv:2204.13388",
    "title": "Coping with Byzantine Processes and a Message Adversary: Modularity  Helps!",
    "abstract": "Coping with Byzantine Processes and a Message Adversary: Modularity  Helps!",
    "descriptor": "",
    "authors": [
      "Davide Frey",
      "Michel Raynal",
      "Fran\u00e7ois Ta\u00efani",
      "Timoth\u00e9 Albouy"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2204.13388"
  },
  {
    "id": "arXiv:2205.03040",
    "title": "Fusion: Efficient and Secure Inference Resilient to Malicious Server and  Curious Clients",
    "abstract": "Comments: The experimental comparison results shown in the Abstract have been updated due to the changes in the experimental setting",
    "descriptor": "\nComments: The experimental comparison results shown in the Abstract have been updated due to the changes in the experimental setting\n",
    "authors": [
      "Caiqin Dong",
      "Jian Weng",
      "Yao Tong",
      "Jia-Nan Liu",
      "Anjia Yang",
      "Yudan Cheng",
      "Shun Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.03040"
  },
  {
    "id": "arXiv:2205.03295",
    "title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of  Explanations",
    "abstract": "Comments: Published in FAccT 2022",
    "descriptor": "\nComments: Published in FAccT 2022\n",
    "authors": [
      "Aparna Balagopalan",
      "Haoran Zhang",
      "Kimia Hamidieh",
      "Thomas Hartvigsen",
      "Frank Rudzicz",
      "Marzyeh Ghassemi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.03295"
  },
  {
    "id": "arXiv:2205.03960",
    "title": "$\u03b1$NAS: Neural Architecture Search using Property Guided Synthesis",
    "abstract": "Comments: Our code is available at this https URL",
    "descriptor": "\nComments: Our code is available at this https URL\n",
    "authors": [
      "Charles Jin",
      "Phitchaya Mangpo Phothilimthana",
      "Sudip Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.03960"
  },
  {
    "id": "arXiv:2205.04774",
    "title": "Automorphism Shuffles for Graphs and Hypergraphs and Its Applications",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Kazumasa Shinagawa",
      "Kengo Miyamoto"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.04774"
  },
  {
    "id": "arXiv:2205.06655",
    "title": "Unified Modeling of Multi-Domain Multi-Device ASR Systems",
    "abstract": "Comments: Submitted to Interspeech 2022",
    "descriptor": "\nComments: Submitted to Interspeech 2022\n",
    "authors": [
      "Soumyajit Mitra",
      "Swayambhu Nath Ray",
      "Bharat Padi",
      "Arunasish Sen",
      "Raghavendra Bilgi",
      "Harish Arsikere",
      "Shalini Ghosh",
      "Ajay Srinivasamurthy",
      "Sri Garimella"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.06655"
  },
  {
    "id": "arXiv:2205.07320",
    "title": "Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective",
    "abstract": "Analyzing Lottery Ticket Hypothesis from PAC-Bayesian Theory Perspective",
    "descriptor": "",
    "authors": [
      "Keitaro Sakamoto",
      "Issei Sato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.07320"
  },
  {
    "id": "arXiv:2205.07756",
    "title": "The Influence of Dimensions on the Complexity of Computing Decision  Trees",
    "abstract": "Comments: 13 pages, 8 figures",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Stephen G. Kobourov",
      "Maarten L\u00f6ffler",
      "Fabrizio Montecchiani",
      "Marcin Pilipczuk",
      "Ignaz Rutter",
      "Raimund Seidel",
      "Manuel Sorge",
      "Jules Wulms"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.07756"
  },
  {
    "id": "arXiv:2205.07890",
    "title": "On the Difficulty of Defending Self-Supervised Learning against Model  Extraction",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Adam Dziedzic",
      "Nikita Dhawan",
      "Muhammad Ahmad Kaleem",
      "Jonas Guan",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.07890"
  },
  {
    "id": "arXiv:2205.08314",
    "title": "Self-Sovereign Identity as a Service: Architecture in Practice",
    "abstract": "Self-Sovereign Identity as a Service: Architecture in Practice",
    "descriptor": "",
    "authors": [
      "Yepeng Ding",
      "Hiroyuki Sato"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.08314"
  },
  {
    "id": "arXiv:2205.10233",
    "title": "RigoBERTa: A State-of-the-Art Language Model For Spanish",
    "abstract": "RigoBERTa: A State-of-the-Art Language Model For Spanish",
    "descriptor": "",
    "authors": [
      "Alejandro Vaca Serrano",
      "Guillem Garcia Subies",
      "Helena Montoro Zamorano",
      "Nuria Aldama Garcia",
      "Doaa Samy",
      "David Betancur Sanchez",
      "Antonio Moreno Sandoval",
      "Marta Guerrero Nieto",
      "Alvaro Barbero Jimenez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10233"
  },
  {
    "id": "arXiv:2205.10956",
    "title": "CIRCLE: Continual Repair across Programming Languages",
    "abstract": "Comments: This paper was accepted by ISSTA2022",
    "descriptor": "\nComments: This paper was accepted by ISSTA2022\n",
    "authors": [
      "Wei Yuan",
      "Quanjun Zhang",
      "Tieke He",
      "Chunrong Fang",
      "Nguyen Quoc Viet Hung",
      "Xiaodong Hao",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.10956"
  },
  {
    "id": "arXiv:2205.12031",
    "title": "Efficient Deviation Types and Learning for Hindsight Rationality in  Extensive-Form Games: Corrections",
    "abstract": "Comments: Please see version 4 of arXiv:2102.06973 (arXiv:2102.06973v4). This submission was a version of that paper with highlighted corrections. After submitting, I figured out that it would be better to submit this report as another version of arXiv:2102.06973",
    "descriptor": "\nComments: Please see version 4 of arXiv:2102.06973 (arXiv:2102.06973v4). This submission was a version of that paper with highlighted corrections. After submitting, I figured out that it would be better to submit this report as another version of arXiv:2102.06973\n",
    "authors": [
      "Dustin Morrill",
      "Ryan D'Orazio",
      "Marc Lanctot",
      "James R. Wright",
      "Michael Bowling",
      "Amy R. Greenwald"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12031"
  },
  {
    "id": "arXiv:2205.12693",
    "title": "Contrastive Learning with Boosted Memorization",
    "abstract": "Contrastive Learning with Boosted Memorization",
    "descriptor": "",
    "authors": [
      "Zhihan Zhou",
      "Jiangchao Yao",
      "Yanfeng Wang",
      "Bo Han",
      "Ya Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12693"
  },
  {
    "id": "arXiv:2205.12905",
    "title": "Analytics of Business Time Series Using Machine Learning and Bayesian  Inference",
    "abstract": "Comments: Survey article. arXiv admin note: text overlap with arXiv:2201.02034, arXiv:2201.02058, arXiv:2201.02729, arXiv:2201.02049, arXiv:2004.01489",
    "descriptor": "\nComments: Survey article. arXiv admin note: text overlap with arXiv:2201.02034, arXiv:2201.02058, arXiv:2201.02729, arXiv:2201.02049, arXiv:2004.01489\n",
    "authors": [
      "Bohdan M. Pavlyshenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12905"
  },
  {
    "id": "arXiv:2205.13219",
    "title": "Penalizing Proposals using Classifiers for Semi-Supervised Object  Detection",
    "abstract": "Comments: The paper is under consideration at Computer Vision and Image Understanding",
    "descriptor": "\nComments: The paper is under consideration at Computer Vision and Image Understanding\n",
    "authors": [
      "Somnath Hazra",
      "Pallab Dasgupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13219"
  },
  {
    "id": "arXiv:2205.13525",
    "title": "Kernel Ridgeless Regression is Inconsistent in Low Dimensions",
    "abstract": "Kernel Ridgeless Regression is Inconsistent in Low Dimensions",
    "descriptor": "",
    "authors": [
      "Daniel Beaglehole",
      "Mikhail Belkin",
      "Parthe Pandit"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13525"
  },
  {
    "id": "arXiv:2205.13559",
    "title": "HashPIM: High-Throughput SHA-3 via Memristive Digital  Processing-in-Memory",
    "abstract": "Comments: Accepted to International Conference on Modern Circuits and Systems Technologies (MOCAST) 2022",
    "descriptor": "\nComments: Accepted to International Conference on Modern Circuits and Systems Technologies (MOCAST) 2022\n",
    "authors": [
      "Batel Oved",
      "Orian Leitersdorf",
      "Ronny Ronen",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.13559"
  },
  {
    "id": "arXiv:2205.13916",
    "title": "Ranking Binary Unlabelled Necklaces in Polynomial Time",
    "abstract": "Comments: Short version to appear at the 24th International Conference on Descriptional Complexity of Formal systems",
    "descriptor": "\nComments: Short version to appear at the 24th International Conference on Descriptional Complexity of Formal systems\n",
    "authors": [
      "Duncan Adamson"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2205.13916"
  },
  {
    "id": "arXiv:2205.14118",
    "title": "Efficient textual explanations for complex road and traffic scenarios  based on semantic segmentation",
    "abstract": "Efficient textual explanations for complex road and traffic scenarios  based on semantic segmentation",
    "descriptor": "",
    "authors": [
      "Yiyue Zhao",
      "Xinyu Yun",
      "Chen Chai",
      "Zhiyu Liu",
      "Wenxuan Fan",
      "Xiao Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14118"
  },
  {
    "id": "arXiv:2205.14295",
    "title": "Is Lip Region-of-Interest Sufficient for Lipreading?",
    "abstract": "Comments: preprint",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Jing-Xuan Zhang",
      "Gen-Shun Wan",
      "Jia Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.14295"
  },
  {
    "id": "arXiv:2205.14474",
    "title": "DeepRM: Deep Recurrent Matching for 6D Pose Refinement",
    "abstract": "Comments: 6 pages, 2 figures",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Alexander Avery",
      "Andreas Savakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14474"
  },
  {
    "id": "arXiv:2205.14959",
    "title": "Dataset Condensation via Efficient Synthetic-Data Parameterization",
    "abstract": "Comments: ICML 2022; Codes at this https URL",
    "descriptor": "\nComments: ICML 2022; Codes at this https URL\n",
    "authors": [
      "Jang-Hyun Kim",
      "Jinuk Kim",
      "Seong Joon Oh",
      "Sangdoo Yun",
      "Hwanjun Song",
      "Joonhyun Jeong",
      "Jung-Woo Ha",
      "Hyun Oh Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14959"
  },
  {
    "id": "arXiv:2205.15049",
    "title": "Metrizing Fairness",
    "abstract": "Metrizing Fairness",
    "descriptor": "",
    "authors": [
      "Yves Rychener",
      "Bahar Taskesen",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15049"
  },
  {
    "id": "arXiv:2205.15140",
    "title": "FiltPIM: In-Memory Filter for DNA Sequencing",
    "abstract": "Comments: Published in 2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",
    "descriptor": "\nComments: Published in 2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)\n",
    "authors": [
      "Marcel Khalifa",
      "Rotem Ben-Hur",
      "Ronny Ronen",
      "Orian Leitersdorf",
      "Leonid Yavits",
      "Shahar Kvatinsky"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15140"
  },
  {
    "id": "arXiv:2205.15146",
    "title": "Batch Normalization Is Blind to the First and Second Derivatives of the  Loss",
    "abstract": "Batch Normalization Is Blind to the First and Second Derivatives of the  Loss",
    "descriptor": "",
    "authors": [
      "Zhanpeng Zhou",
      "Wen Shen",
      "Huixin Chen",
      "Ling Tang",
      "Quanshi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15146"
  },
  {
    "id": "arXiv:2205.15258",
    "title": "Transparency, Governance and Regulation of Algorithmic Tools Deployed in  the Criminal Justice System: a UK Case Study",
    "abstract": "Comments: AAAI/ACM Conference on AI, Ethics, and Society 2022",
    "descriptor": "\nComments: AAAI/ACM Conference on AI, Ethics, and Society 2022\n",
    "authors": [
      "Miri Zilka",
      "Holli Sargeant",
      "Adrian Weller"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2205.15258"
  },
  {
    "id": "arXiv:2205.15294",
    "title": "Efficient $\u03a6$-Regret Minimization in Extensive-Form Games via Online  Mirror Descent",
    "abstract": "Efficient $\u03a6$-Regret Minimization in Extensive-Form Games via Online  Mirror Descent",
    "descriptor": "",
    "authors": [
      "Yu Bai",
      "Chi Jin",
      "Song Mei",
      "Ziang Song",
      "Tiancheng Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15294"
  },
  {
    "id": "arXiv:2205.15397",
    "title": "Minimax Optimal Online Imitation Learning via Replay Estimation",
    "abstract": "Minimax Optimal Online Imitation Learning via Replay Estimation",
    "descriptor": "",
    "authors": [
      "Gokul Swamy",
      "Nived Rajaraman",
      "Matthew Peng",
      "Sanjiban Choudhury",
      "J. Andrew Bagnell",
      "Zhiwei Steven Wu",
      "Jiantao Jiao",
      "Kannan Ramchandran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15397"
  },
  {
    "id": "arXiv:2205.15436",
    "title": "Fairness in the First Stage of Two-Stage Recommender Systems",
    "abstract": "Fairness in the First Stage of Two-Stage Recommender Systems",
    "descriptor": "",
    "authors": [
      "Lequn Wang",
      "Thorsten Joachims"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15436"
  },
  {
    "id": "arXiv:2205.15747",
    "title": "Adversarial synthesis based data-augmentation for code-switched spoken  language identification",
    "abstract": "Comments: 9 pages, 8 figures, updated",
    "descriptor": "\nComments: 9 pages, 8 figures, updated\n",
    "authors": [
      "Parth Shastri",
      "Chirag Patil",
      "Poorval Wanere",
      "Dr. Shrinivas Mahajan",
      "Dr. Abhishek Bhatt",
      "Dr. Hardik Sailor"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15747"
  },
  {
    "id": "arXiv:2205.15912",
    "title": "Efficient Algorithms for Sorting in Trees",
    "abstract": "Efficient Algorithms for Sorting in Trees",
    "descriptor": "",
    "authors": [
      "Jishnu Roychoudhury",
      "Jatin Yadav"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.15912"
  },
  {
    "id": "arXiv:2206.00216",
    "title": "THE-X: Privacy-Preserving Transformer Inference with Homomorphic  Encryption",
    "abstract": "Comments: Findings of ACL 2022",
    "descriptor": "\nComments: Findings of ACL 2022\n",
    "authors": [
      "Tianyu Chen",
      "Hangbo Bao",
      "Shaohan Huang",
      "Li Dong",
      "Binxing Jiao",
      "Daxin Jiang",
      "Haoyi Zhou",
      "Jianxin Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.00216"
  },
  {
    "id": "arXiv:2206.00242",
    "title": "CrossCBR: Cross-view Contrastive Learning for Bundle Recommendation",
    "abstract": "Comments: 9 pages, 5 figures, 5 tables",
    "descriptor": "\nComments: 9 pages, 5 figures, 5 tables\n",
    "authors": [
      "Yunshan Ma",
      "Yingzhi He",
      "An Zhang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.00242"
  },
  {
    "id": "arXiv:2206.00252",
    "title": "Interpretable Deep Learning Classifier by Detection of Prototypical  Parts on Kidney Stones Images",
    "abstract": "Comments: Extended abstract accepted at LatinX in Computer Vision Research Workshop, at CVPR 2022",
    "descriptor": "\nComments: Extended abstract accepted at LatinX in Computer Vision Research Workshop, at CVPR 2022\n",
    "authors": [
      "Daniel Flores-Araiza",
      "Francisco Lopez-Tiro",
      "Elias Villalvazo-Avila",
      "Jonathan El-Beze",
      "Jacques Hubert",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00252"
  },
  {
    "id": "arXiv:2206.00277",
    "title": "Task-Specific Expert Pruning for Sparse Mixture-of-Experts",
    "abstract": "Comments: under review",
    "descriptor": "\nComments: under review\n",
    "authors": [
      "Tianyu Chen",
      "Shaohan Huang",
      "Yuan Xie",
      "Binxing Jiao",
      "Daxin Jiang",
      "Haoyi Zhou",
      "Jianxin Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00277"
  },
  {
    "id": "arXiv:2206.00316",
    "title": "Model Generation with Provable Coverability for Offline Reinforcement  Learning",
    "abstract": "Comments: 10 pages, 8 figures",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Chengxing Jia",
      "Hao Yin",
      "Chenxiao Gao",
      "Tian Xu",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00316"
  },
  {
    "id": "arXiv:2206.00339",
    "title": "Adaptive time integration of mechanical forces in center-based models  for biological cell populations",
    "abstract": "Adaptive time integration of mechanical forces in center-based models  for biological cell populations",
    "descriptor": "",
    "authors": [
      "Per L\u00f6tstedt",
      "Sonja Mathias"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2206.00339"
  },
  {
    "id": "arXiv:2206.00381",
    "title": "The statistical nature of h-index of a network node",
    "abstract": "The statistical nature of h-index of a network node",
    "descriptor": "",
    "authors": [
      "Yan Liu",
      "Mudi Jiang",
      "Lianyu Hu",
      "Zengyou He"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.00381"
  },
  {
    "id": "arXiv:2206.00415",
    "title": "Learning Invariant Visual Representations for Compositional Zero-Shot  Learning",
    "abstract": "Learning Invariant Visual Representations for Compositional Zero-Shot  Learning",
    "descriptor": "",
    "authors": [
      "Tian Zhang",
      "Kongming Liang",
      "Ruoyi Du",
      "Xian Sun",
      "Zhanyu Ma",
      "Jun Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00415"
  },
  {
    "id": "arXiv:2206.00439",
    "title": "Algorithmic Foundation of Deep X-Risk Optimization",
    "abstract": "Algorithmic Foundation of Deep X-Risk Optimization",
    "descriptor": "",
    "authors": [
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00439"
  },
  {
    "id": "arXiv:2206.00535",
    "title": "Deepfake Caricatures: Amplifying attention to artifacts increases  deepfake detection by humans and machines",
    "abstract": "Comments: 9 pages, 5 figures, 4 tables",
    "descriptor": "\nComments: 9 pages, 5 figures, 4 tables\n",
    "authors": [
      "Camilo Fosco",
      "Emilie Josephs",
      "Alex Andonian",
      "Allen Lee",
      "Xi Wang",
      "Aude Oliva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.00535"
  },
  {
    "id": "arXiv:2206.00592",
    "title": "Stopping Silent Sneaks: Defending against Malicious Mixes with  Topological Engineering",
    "abstract": "Stopping Silent Sneaks: Defending against Malicious Mixes with  Topological Engineering",
    "descriptor": "",
    "authors": [
      "Xinshu Ma",
      "Florentin Rochet",
      "Tariq Elahi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.00592"
  },
  {
    "id": "arXiv:2206.00595",
    "title": "Logic-Based Ethical Planning",
    "abstract": "Logic-Based Ethical Planning",
    "descriptor": "",
    "authors": [
      "Umberto Grandi",
      "Emiliano Lorini",
      "Timothy Parker",
      "Rachid Alami"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00595"
  }
]