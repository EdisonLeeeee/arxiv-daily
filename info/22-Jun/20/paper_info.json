[
  {
    "id": "arXiv:2206.08369",
    "title": "Embarrassingly Parallel Independent Training of Multi-Layer Perceptrons  with Heterogeneous Architectures",
    "abstract": "The definition of a Neural Network architecture is one of the most critical\nand challenging tasks to perform. In this paper, we propose ParallelMLPs.\nParallelMLPs is a procedure to enable the training of several independent\nMultilayer Perceptron Neural Networks with a different number of neurons and\nactivation functions in parallel by exploring the principle of locality and\nparallelization capabilities of modern CPUs and GPUs. The core idea of this\ntechnique is to use a Modified Matrix Multiplication that replaces an ordinal\nmatrix multiplication by two simple matrix operations that allow separate and\nindependent paths for gradient flowing, which can be used in other scenarios.\nWe have assessed our algorithm in simulated datasets varying the number of\nsamples, features and batches using 10,000 different models. We achieved a\ntraining speedup from 1 to 4 orders of magnitude if compared to the sequential\napproach.",
    "descriptor": "",
    "authors": [
      "Felipe Costa Farias",
      "Teresa Bernarda Ludermir",
      "Carmelo Jose Albanez Bastos-Filho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08369"
  },
  {
    "id": "arXiv:2206.08370",
    "title": "Assessing the wall energy efficiency design under climate change using  POD reduced order model",
    "abstract": "Within the environmental context, numerical modeling is a promising approach\nto assessing the energy efficiency of buildings. Resilient buildings need to be\ndesigned, and capable of adapting to future extreme heat. Simulations are\nrequired assuming a one-dimensional heat transfer problem through walls and a\nsimulation horizon of several years (nearly 30). The computational cost\nassociated with such modeling is quite significant and model reduction methods\nare worth investigating. The objective is to propose a reliable reduced-order\nmodel for such long-term simulations. For this, an alternative model reduction\napproach is investigated, assuming a known Proper Orthogonal Decomposition\nreduced basis for time, and not for space as usual. The model enables computing\nparametric solutions using basis interpolation on the tangent space of the\n\\textsc{Grassmann} manifold. Three study cases are considered to verify the\nefficiency of the \\revision{reduced-order} model. Results highlight that the\nmodel has a satisfying accuracy of $10^{\\,-3}\\,$ compared to reference\nsolutions. The last case study focuses on the wall energy efficiency design\nunder climate change according to a \\revision{four-dimensional} parameter\nspace. The latter is composed of the load material emissivity, heat capacity,\nthermal conductivity, and thickness insulation layer. Simulations are carried\nover $30$ years considering climate change. The solution minimizing the wall\nwork rate is determined with a computational ratio of $0.1\\%$ compared to\nstandard approaches.",
    "descriptor": "",
    "authors": [
      "Julien Berger",
      "Cyrille Allery",
      "Ana\u00efs Machard"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.08370"
  },
  {
    "id": "arXiv:2206.08371",
    "title": "Surface transfer coefficients estimation for heat conduction problem  using the Bayesian framework",
    "abstract": "This work deals with an inverse two-dimensional nonlinear heat conduction\nproblem to determine the top and lateral surface transfer coefficients. For\nthis, the \\textsc{B}ayesian framework with the \\textsc{M}arkov Chain\n\\textsc{M}onte \\textsc{C}arlo algorithm is used to determine the posterior\ndistribution of unknown parameters. To handle the computational burden, a\nlumped one-dimensional model is proposed. The lumped model approximations are\nconsidered within the parameter estimation procedure thanks to the\nApproximation Error Model. The experiments are carried out for several\nconfigurations of chamber ventilator speed. Experimental observations are\nobtained through a complete measurement uncertainty propagation. By solving the\ninverse problem, accurate probability distributions are determined. Additional\ninvestigations are performed to demonstrate the reliability of the lumped\nmodel, in terms of accuracy and computational gains.",
    "descriptor": "",
    "authors": [
      "Julien Berger",
      "Clemence Legros"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.08371"
  },
  {
    "id": "arXiv:2206.08392",
    "title": "Ultrametric Smale's $\u03b1$-theory",
    "abstract": "We present a version of Smale's $\\alpha$-theory for ultrametric fields, such\nas the $p$-adics and their extensions, which gives us a multivariate version of\nHensel's lemma.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Jazz G. Suchen",
      "Josu\u00e9 Tonelli-Cueto"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08392"
  },
  {
    "id": "arXiv:2206.08394",
    "title": "Powershap: A Power-full Shapley Feature Selection Method",
    "abstract": "Feature selection is a crucial step in developing robust and powerful machine\nlearning models. Feature selection techniques can be divided into two\ncategories: filter and wrapper methods. While wrapper methods commonly result\nin strong predictive performances, they suffer from a large computational\ncomplexity and therefore take a significant amount of time to complete,\nespecially when dealing with high-dimensional feature sets. Alternatively,\nfilter methods are considerably faster, but suffer from several other\ndisadvantages, such as (i) requiring a threshold value, (ii) not taking into\naccount intercorrelation between features, and (iii) ignoring feature\ninteractions with the model. To this end, we present powershap, a novel wrapper\nfeature selection method, which leverages statistical hypothesis testing and\npower calculations in combination with Shapley values for quick and intuitive\nfeature selection. Powershap is built on the core assumption that an\ninformative feature will have a larger impact on the prediction compared to a\nknown random feature. Benchmarks and simulations show that powershap\noutperforms other filter methods with predictive performances on par with\nwrapper methods while being significantly faster, often even reaching half or a\nthird of the execution time. As such, powershap provides a competitive and\nquick algorithm that can be used by various models in different domains.\nFurthermore, powershap is implemented as a plug-and-play and open-source\nsklearn component, enabling easy integration in conventional data science\npipelines. User experience is even further enhanced by also providing an\nautomatic mode that automatically tunes the hyper-parameters of the powershap\nalgorithm, allowing to use the algorithm without any configuration needed.",
    "descriptor": "\nComments: Accepted at ECML PKDD 2022\n",
    "authors": [
      "Jarne Verhaeghe",
      "Jeroen Van Der Donckt",
      "Femke Ongenae",
      "Sofie Van Hoecke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08394"
  },
  {
    "id": "arXiv:2206.08396",
    "title": "TACO: A Tree-based Approach to Customizing Location Obfuscation based on  User Policies",
    "abstract": "A large body of literature exists for studying Location obfuscation in\ndifferent contexts. However, the obfuscation functions generated by existing\nsystems are not easily customizable by end users. Users might find it difficult\nto understand the parameters involved (e.g., obfuscation range and granularity\nof location representation) and set realistic trade-offs between privacy and\nutility. In this paper, we propose a new framework called, TACO, i.e.,\nTree-based Approach to Customizing location Obfuscation, which can generate\nlocation obfuscation functions that provide strong privacy guarantees while\nbeing easily customizable via user-specified policies. First, we develop a\nsemantic representation of a given region using tree structure. These data\nstructures assist users in specifying their privacy requirements using\npolicies. Second, we design a rigorous privacy model based on\nGeo-Indistinguishability for TACO using this tree structure. Third, we\nimplement enforcement techniques in TACO to translate user policies to\nappropriate parameters and generate a robust, customized obfuscation function\nfor each user. Finally, we carry out experiments on real world datasets to\nevaluate the effectiveness of the framework under different settings.",
    "descriptor": "",
    "authors": [
      "Primal Pappachan",
      "Chenxi Qiu",
      "Anna Squicciarini",
      "Vishnu Sharma Hunsur Manjunath"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08396"
  },
  {
    "id": "arXiv:2206.08403",
    "title": "Learning to Teach Fairness-aware Deep Multi-task Learning",
    "abstract": "Fairness-aware learning mainly focuses on single task learning (STL). The\nfairness implications of multi-task learning (MTL) have only recently been\nconsidered and a seminal approach has been proposed that considers the\nfairness-accuracy trade-off for each task and the performance trade-off among\ndifferent tasks. Instead of a rigid fairness-accuracy trade-off formulation, we\npropose a flexible approach that learns how to be fair in a MTL setting by\nselecting which objective (accuracy or fairness) to optimize at each step. We\nintroduce the L2T-FMT algorithm that is a teacher-student network trained\ncollaboratively; the student learns to solve the fair MTL problem while the\nteacher instructs the student to learn from either accuracy or fairness,\ndepending on what is harder to learn for each task. Moreover, this dynamic\nselection of which objective to use at each step for each task reduces the\nnumber of trade-off weights from 2T to T, where T is the number of tasks. Our\nexperiments on three real datasets show that L2T-FMT improves on both fairness\n(12-19%) and accuracy (up to 2%) over state-of-the-art approaches.",
    "descriptor": "\nComments: Accepted to be published in the Proceedings of the \"European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD)\", Sept. 19th to 23rd 2022\n",
    "authors": [
      "Arjun Roy",
      "Eirini Ntoutsi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08403"
  },
  {
    "id": "arXiv:2206.08405",
    "title": "Going Deeper than Tracking: a Survey of Computer-Vision Based  Recognition of Animal Pain and Affective States",
    "abstract": "Advances in animal motion tracking and pose recognition have been a game\nchanger in the study of animal behavior. Recently, an increasing number of\nworks go 'deeper' than tracking, and address automated recognition of animals'\ninternal states such as emotions and pain with the aim of improving animal\nwelfare, making this a timely moment for a systematization of the field. This\npaper provides a comprehensive survey of computer vision-based research on\nrecognition of affective states and pain in animals, addressing both facial and\nbodily behavior analysis. We summarize the efforts that have been presented so\nfar within this topic -- classifying them across different dimensions,\nhighlight challenges and research gaps, and provide best practice\nrecommendations for advancing the field, and some future directions for\nresearch.",
    "descriptor": "",
    "authors": [
      "Sofia Broom\u00e9",
      "Marcelo Feighelstein",
      "Anna Zamansky",
      "Gabriel Carreira Lencioni",
      "Pia Haubro Andersen",
      "Francisca Pessanha",
      "Marwa Mahmoud",
      "Hedvig Kjellstr\u00f6m",
      "Albert Ali Salah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08405"
  },
  {
    "id": "arXiv:2206.08406",
    "title": "Predicting Hate Intensity of Twitter Conversation Threads",
    "abstract": "Tweets are the most concise form of communication in online social media,\nwherein a single tweet has the potential to make or break the discourse of the\nconversation. Online hate speech is more accessible than ever, and stifling its\npropagation is of utmost importance for social media companies and users for\ncongenial communication. Most of the research barring a recent few has focused\non classifying an individual tweet regardless of the tweet thread/context\nleading up to that point. One of the classical approaches to curb hate speech\nis to adopt a reactive strategy after the hate speech postage. The ex-post\nfacto strategy results in neglecting subtle posts that do not show the\npotential to instigate hate speech on their own but may portend in the\nsubsequent discussion ensuing in the post's replies. In this paper, we propose\nDRAGNET++, which aims to predict the intensity of hatred that a tweet can bring\nin through its reply chain in the future. It uses the semantic and propagating\nstructure of the tweet threads to maximize the contextual information leading\nup to and the fall of hate intensity at each subsequent tweet. We explore three\npublicly available Twitter datasets -- Anti-Racism contains the reply tweets of\na collection of social media discourse on racist remarks during US political\nand Covid-19 background; Anti-Social presents a dataset of 40 million tweets\namidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents\nTwitter datasets collated based on anti-Asian behaviours during COVID-19\npandemic. All the curated datasets consist of structural graph information of\nthe Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art\nbaselines significantly. It beats the best baseline by an 11\\% margin on the\nPerson correlation coefficient and a decrease of 25\\% on RMSE for the\nAnti-Racism dataset with a similar performance on the other two datasets.",
    "descriptor": "\nComments: 22 pages, 10 figures, 3 tables\n",
    "authors": [
      "Qing Meng",
      "Tharun Suresh",
      "Roy Ka-Wei Lee",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08406"
  },
  {
    "id": "arXiv:2206.08407",
    "title": "Deep Multi-Task Models for Misogyny Identification and Categorization on  Arabic Social Media",
    "abstract": "The prevalence of toxic content on social media platforms, such as hate\nspeech, offensive language, and misogyny, presents serious challenges to our\ninterconnected society. These challenging issues have attracted widespread\nattention in Natural Language Processing (NLP) community. In this paper, we\npresent the submitted systems to the first Arabic Misogyny Identification\nshared task. We investigate three multi-task learning models as well as their\nsingle-task counterparts. In order to encode the input text, our models rely on\nthe pre-trained MARBERT language model. The overall obtained results show that\nall our submitted models have achieved the best performances (top three ranked\nsubmissions) in both misogyny identification and categorization tasks.",
    "descriptor": "",
    "authors": [
      "Abdelkader El Mahdaouy",
      "Abdellah El Mekki",
      "Ahmed Oumar",
      "Hajar Mousannif",
      "Ismail Berrada"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08407"
  },
  {
    "id": "arXiv:2206.08409",
    "title": "Control Barrier Functionals: Safety-critical Control for Time Delay  Systems",
    "abstract": "This work presents a theoretical framework for the safety-critical control of\ntime delay systems. The theory of control barrier functions, that provides\nformal safety guarantees for delay-free systems, is extended to systems with\nstate delay. The notion of control barrier functionals is introduced to attain\nformal safety guarantees, by enforcing the forward invariance of safe sets\ndefined in the infinite dimensional state space. The proposed framework is able\nto handle multiple delays and distributed delays both in the dynamics and in\nthe safety condition, and provides an affine constraint on the control input\nthat yields provable safety. This constraint can be incorporated into\noptimization problems to synthesize pointwise optimal and provable safe\ncontrollers. The applicability of the proposed method is demonstrated by\nnumerical simulation examples.",
    "descriptor": "\nComments: Submitted to the International Journal of Robust and Nonlinear Control (JRNC). 25 pages, 3 figures\n",
    "authors": [
      "Adam K. Kiss",
      "Tamas G. Molnar",
      "Aaron D. Ames",
      "Gabor Orosz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08409"
  },
  {
    "id": "arXiv:2206.08412",
    "title": "Consensus Formation Tracking for Multiple AUV Systems Using Distributed  Bioinspired Sliding Mode Control",
    "abstract": "Consensus formation tracking of multiple autonomous underwater vehicles\n(AUVs) subject to nonlinear and uncertain dynamics is a challenging problem in\nrobotics. To tackle this challenge, a distributed bioinspired sliding mode\ncontroller is proposed in this paper. First, the conventional sliding mode\ncontroller (SMC) is presented, and the consensus problem is addressed on the\nbasis of graph theory. Next, to tackle the high frequency chattering issue in\nSMC scheme and meanwhile improve the robustness to the noises, a bioinspired\napproach is introduced, in which a neural dynamic model is employed to replace\nthe nonlinear sign or saturation function in the synthesis of conventional\nsliding mode controllers. Furthermore, the input-to-state stability of the\nresulting closed-loop system is proved in the presence of bounded lumped\ndisturbance by the Lyapunov stability theory. Finally, simulation experiments\nare conducted to demonstrate the effectiveness of the proposed distributed\nformation control protocol.",
    "descriptor": "",
    "authors": [
      "Tao Yan",
      "Zhe Xu",
      "Simon X. Yang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08412"
  },
  {
    "id": "arXiv:2206.08413",
    "title": "Recursion does not always help",
    "abstract": "We show that adding recursion does not increase the total numerical functions\ndefinable in the typed $\\lambda\\beta\\eta$-calculus or the, possibly partial,\nnumerical functions definable in the $\\lambda\\Omega$-calculus.",
    "descriptor": "",
    "authors": [
      "Gordon Plotkin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08413"
  },
  {
    "id": "arXiv:2206.08415",
    "title": "CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended  Sarcasm Detection in English and Arabic",
    "abstract": "Sarcasm is a form of figurative language where the intended meaning of a\nsentence differs from its literal meaning. This poses a serious challenge to\nseveral Natural Language Processing (NLP) applications such as Sentiment\nAnalysis, Opinion Mining, and Author Profiling. In this paper, we present our\nparticipating system to the intended sarcasm detection task in English and\nArabic languages. Our system\\footnote{The source code of our system is\navailable at \\url{https://github.com/AbdelkaderMH/iSarcasmEval}} consists of\nthree deep learning-based models leveraging two existing pre-trained language\nmodels for Arabic and English. We have participated in all sub-tasks. Our\nofficial submissions achieve the best performance on sub-task A for Arabic\nlanguage and rank second in sub-task B. For sub-task C, our system is ranked\n7th and 11th on Arabic and English datasets, respectively.",
    "descriptor": "",
    "authors": [
      "Abdelkader El Mahdaouy",
      "Abdellah El Mekki",
      "Kabil Essefar",
      "Abderrahman Skiredj",
      "Ismail Berrada"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08415"
  },
  {
    "id": "arXiv:2206.08416",
    "title": "A IETI-DP method for discontinuous Galerkin discretizations in  Isogeometric Analysis with inexact local solvers",
    "abstract": "We construct solvers for an isogeometric multi-patch discretization, where\nthe patches are coupled via a discontinuous Galerkin approach, which allows the\nconsideration of discretizations that do not match on the interfaces. We solve\nthe resulting linear system using a Dual-Primal IsogEometric Tearing and\nInterconnecting (IETI-DP) method. We are interested in solving the arising\npatch-local problems using iterative solvers since this allows the reduction of\nthe memory footprint. We solve the patch-local problems approximately using the\nFast Diagonalization method, which is known to be robust in the grid size and\nthe spline degree. To obtain the tensor structure needed for the application of\nthe Fast Diagonalization method, we introduce an orthogonal splitting of the\nlocal function spaces. We present a convergence theory that confirms that the\ncondition number of the preconditioned system only grows poly-logarithmically\nwith the grid size. The numerical experiments confirm this finding. Moreover,\nthey show that the convergence of the overall solver only mildly depends on the\nspline degree. We observe a mild reduction of the computational times and a\nsignificant reduction of the memory requirements in comparison to standard\nIETI-DP solvers using sparse direct solvers for the local subproblems.\nFurthermore, the experiments indicate good scaling behavior on distributed\nmemory machines.",
    "descriptor": "",
    "authors": [
      "Monica Montardini",
      "Giancarlo Sangalli",
      "Rainer Schneckenleitner",
      "Stefan Takacs",
      "Mattia Tani"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08416"
  },
  {
    "id": "arXiv:2206.08419",
    "title": "Time Reversal for 6G Spatiotemporal Focusing: Recent Experiments,  Opportunities, and Challenges",
    "abstract": "Late visions and trends for the future sixth Generation (6G) of wireless\ncommunications advocate, among other technologies, towards the deployment of\nnetwork nodes with extreme numbers of antennas and up to terahertz frequencies,\nas means to enable various immersive applications. However, these technologies\nimpose several challenges in the design of radio-frequency front-ends and\nbeamforming architectures, as well as of ultra-wideband waveforms and\ncomputationally efficient transceiver signal processing. In this article, we\nrevisit the Time Reversal (TR) technique, which was initially experimented in\nacoustics, in the context of large-bandwidth 6G wireless communications,\ncapitalizing on its high resolution spatiotemporal focusing realized with low\ncomplexity transceivers. We first overview representative state-of-the-art in\nTR-based wireless communications, identifying the key competencies and\nrequirements of TR for efficient operation. Recent and novel experimental\nsetups and results for the spatiotemporal focusing capability of TR at the\ncarrier frequencies $2.5$, $36$, and $273$ GHz are then presented,\ndemonstrating in quantitative ways the technique's effectiveness in these very\ndifferent frequency bands, as well as the roles of the available bandwidth and\nthe number of transmit antennas. We also showcase the TR potential for\nrealizing low complexity multi-user communications. The opportunities arising\nfrom TR-based wireless communications as well as the challenges for finding\ntheir place in 6G networks, also in conjunction with other complementary\ncandidate technologies, are highlighted.",
    "descriptor": "\nComments: 7 pages, 4 figures, submitted to an IEEE Magazine\n",
    "authors": [
      "George C. Alexandropoulos",
      "Ali Mokh",
      "Ramin Khayatzadeh",
      "Julien de Rosny",
      "Mohamed Kamoun",
      "Abdelwaheb Ourir",
      "Arnaud Tourin",
      "Mathias Fink",
      "M\u00e9rouane Debbah"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08419"
  },
  {
    "id": "arXiv:2206.08422",
    "title": "Real-time motion amplification on mobile devices",
    "abstract": "A simple motion amplification algorithm suitable for real-time applications\non mobile devices is presented. It is based on motion enhancement by moving\naverage differencing (MEMAD), a temporal high-pass filter for video streams.\nMEMAD can amplify small moving objects or subtle motion in larger objects. It\nis computationally sufficiently simple to be implemented in real time on\nsmartphones. In the specific implementation as an Android phone app, MEMAD is\ndemonstrated on examples chosen such as to motivate applications in the\nengineering, biological, and medical sciences.",
    "descriptor": "\nComments: Supplemental data at this https URL\n",
    "authors": [
      "Henning U. Voss"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08422"
  },
  {
    "id": "arXiv:2206.08423",
    "title": "IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering  in Indoor Scenes",
    "abstract": "Indoor scenes exhibit significant appearance variations due to myriad\ninteractions between arbitrarily diverse object shapes, spatially-changing\nmaterials, and complex lighting. Shadows, highlights, and inter-reflections\ncaused by visible and invisible light sources require reasoning about\nlong-range interactions for inverse rendering, which seeks to recover the\ncomponents of image formation, namely, shape, material, and lighting. In this\nwork, our intuition is that the long-range attention learned by transformer\narchitectures is ideally suited to solve longstanding challenges in\nsingle-image inverse rendering. We demonstrate with a specific instantiation of\na dense vision transformer, IRISformer, that excels at both single-task and\nmulti-task reasoning required for inverse rendering. Specifically, we propose a\ntransformer architecture to simultaneously estimate depths, normals,\nspatially-varying albedo, roughness and lighting from a single image of an\nindoor scene. Our extensive evaluations on benchmark datasets demonstrate\nstate-of-the-art results on each of the above tasks, enabling applications like\nobject insertion and material editing in a single unconstrained real image,\nwith greater photorealism than prior works. Code and data are publicly released\nat https://github.com/ViLab-UCSD/IRISformer.",
    "descriptor": "\nComments: CVPR 22 camera ready version with supplementary\n",
    "authors": [
      "Rui Zhu",
      "Zhengqin Li",
      "Janarbek Matai",
      "Fatih Porikli",
      "Manmohan Chandraker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08423"
  },
  {
    "id": "arXiv:2206.08425",
    "title": "DialogueScript: Using Dialogue Agents to Produce a Script",
    "abstract": "We present a novel approach to generating scripts by using agents with\ndifferent personality types. To manage character interaction in the script, we\nemploy simulated dramatic networks. Automatic and human evaluation on multiple\ncriteria shows that our approach outperforms a vanilla-GPT2-based baseline. We\nfurther introduce a new metric to evaluate dialogue consistency based on\nnatural language inference and demonstrate its validity.",
    "descriptor": "\nComments: Non-archival paper at the 4th Workshop on Narrative Understanding (WNU 2022)\n",
    "authors": [
      "Patr\u00edcia Schmidtov\u00e1",
      "D\u00e1vid Javorsk\u00fd",
      "Christi\u00e1n Mikl\u00e1\u0161",
      "Tom\u00e1\u0161 Musil",
      "Rudolf Rosa",
      "Ond\u0159ej Du\u0161ek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08425"
  },
  {
    "id": "arXiv:2206.08427",
    "title": "SATBench: Benchmarking the speed-accuracy tradeoff in object recognition  by humans and dynamic neural networks",
    "abstract": "The core of everyday tasks like reading and driving is active object\nrecognition. Attempts to model such tasks are currently stymied by the\ninability to incorporate time. People show a flexible tradeoff between speed\nand accuracy and this tradeoff is a crucial human skill. Deep neural networks\nhave emerged as promising candidates for predicting peak human object\nrecognition performance and neural activity. However, modeling the temporal\ndimension i.e., the speed-accuracy tradeoff (SAT), is essential for them to\nserve as useful computational models for how humans recognize objects. To this\nend, we here present the first large-scale (148 observers, 4 neural networks, 8\ntasks) dataset of the speed-accuracy tradeoff (SAT) in recognizing ImageNet\nimages. In each human trial, a beep, indicating the desired reaction time,\nsounds at a fixed delay after the image is presented, and observer's response\ncounts only if it occurs near the time of the beep. In a series of blocks, we\ntest many beep latencies, i.e., reaction times. We observe that human accuracy\nincreases with reaction time and proceed to compare its characteristics with\nthe behavior of several dynamic neural networks that are capable of\ninference-time adaptive computation. Using FLOPs as an analog for reaction\ntime, we compare networks with humans on curve-fit error, category-wise\ncorrelation, and curve steepness, and conclude that cascaded dynamic neural\nnetworks are a promising model of human reaction time in object recognition\ntasks.",
    "descriptor": "\nComments: 19 pages, 12 figures. Under Review at NeurIPS Datasets and Benchmarks Track 2022\n",
    "authors": [
      "Ajay Subramanian",
      "Sara Price",
      "Omkar Kumbhar",
      "Elena Sizikova",
      "Najib J. Majaj",
      "Denis G. Pelli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08427"
  },
  {
    "id": "arXiv:2206.08428",
    "title": "EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation  and Relighting of Human Eyes",
    "abstract": "A unique challenge in creating high-quality animatable and relightable 3D\navatars of people is modeling human eyes. The challenge of synthesizing eyes is\nmultifold as it requires 1) appropriate representations for the various\ncomponents of the eye and the periocular region for coherent viewpoint\nsynthesis, capable of representing diffuse, refractive and highly reflective\nsurfaces, 2) disentangling skin and eye appearance from environmental\nillumination such that it may be rendered under novel lighting conditions, and\n3) capturing eyeball motion and the deformation of the surrounding skin to\nenable re-gazing. These challenges have traditionally necessitated the use of\nexpensive and cumbersome capture setups to obtain high-quality results, and\neven then, modeling of the eye region holistically has remained elusive. We\npresent a novel geometry and appearance representation that enables\nhigh-fidelity capture and photorealistic animation, view synthesis and\nrelighting of the eye region using only a sparse set of lights and cameras. Our\nhybrid representation combines an explicit parametric surface model for the\neyeball with implicit deformable volumetric representations for the periocular\nregion and the interior of the eye. This novel hybrid model has been designed\nto address the various parts of that challenging facial area - the explicit\neyeball surface allows modeling refraction and high-frequency specular\nreflection at the cornea, whereas the implicit representation is well suited to\nmodel lower-frequency skin reflection via spherical harmonics and can represent\nnon-surface structures such as hair or diffuse volumetric bodies, both of which\nare a challenge for explicit surface models. We show that for high-resolution\nclose-ups of the eye, our model can synthesize high-fidelity animated gaze from\nnovel views under unseen illumination conditions.",
    "descriptor": "\nComments: 16 pages, 16 figures, 1 table, to be published in ACM Transactions on Graphics (TOG) (Volume: 41, Issue: 4), 2022\n",
    "authors": [
      "Gengyan Li",
      "Abhimitra Meka",
      "Franziska M\u00fcller",
      "Marcel C. B\u00fchler",
      "Otmar Hilliges"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08428"
  },
  {
    "id": "arXiv:2206.08429",
    "title": "Scalable Temporal Localization of Sensitive Activities in Movies and TV  Episodes",
    "abstract": "To help customers make better-informed viewing choices, video-streaming\nservices try to moderate their content and provide more visibility into which\nportions of their movies and TV episodes contain age-appropriate material\n(e.g., nudity, sex, violence, or drug-use). Supervised models to localize these\nsensitive activities require large amounts of clip-level labeled data which is\nhard to obtain, while weakly-supervised models to this end usually do not offer\ncompetitive accuracy. To address this challenge, we propose a novel Coarse2Fine\nnetwork designed to make use of readily obtainable video-level weak labels in\nconjunction with sparse clip-level labels of age-appropriate activities. Our\nmodel aggregates frame-level predictions to make video-level classifications\nand is therefore able to leverage sparse clip-level labels along with\nvideo-level labels. Furthermore, by performing frame-level predictions in a\nhierarchical manner, our approach is able to overcome the label-imbalance\nproblem caused due to the rare-occurrence nature of age-appropriate content. We\npresent comparative results of our approach using 41,234 movies and TV episodes\n(~3 years of video-content) from 521 sub-genres and 250 countries making it by\nfar the largest-scale empirical analysis of age-appropriate activity\nlocalization in long-form videos ever published. Our approach offers 107.2%\nrelative mAP improvement (from 5.5% to 11.4%) over existing state-of-the-art\nactivity-localization approaches.",
    "descriptor": "",
    "authors": [
      "Xiang Hao",
      "Jingxiang Chen",
      "Shixing Chen",
      "Ahmed Saad",
      "Raffay Hamid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08429"
  },
  {
    "id": "arXiv:2206.08432",
    "title": "GraphScale: Scalable Bandwidth-Efficient Graph Processing on FPGAs",
    "abstract": "Recent advances in graph processing on FPGAs promise to alleviate performance\nbottlenecks with irregular memory access patterns. Such bottlenecks challenge\nperformance for a growing number of important application areas like machine\nlearning and data analytics. While FPGAs denote a promising solution through\nflexible memory hierarchies and massive parallelism, we argue that current\ngraph processing accelerators either use the off-chip memory bandwidth\ninefficiently or do not scale well across memory channels.\nIn this work, we propose GraphScale, a scalable graph processing framework\nfor FPGAs. For the first time, GraphScale combines multi-channel memory with\nasynchronous graph processing (i.e., for fast convergence on results) and a\ncompressed graph representation (i.e., for efficient usage of memory bandwidth\nand reduced memory footprint). GraphScale solves common graph problems like\nbreadth-first search, PageRank, and weakly-connected components through modular\nuser-defined functions, a novel two-dimensional partitioning scheme, and a\nhigh-performance two-level crossbar design.",
    "descriptor": "",
    "authors": [
      "Jonas Dann",
      "Daniel Ritter",
      "Holger Fr\u00f6ning"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.08432"
  },
  {
    "id": "arXiv:2206.08434",
    "title": "The Case for a Wholistic Serverless Programming Paradigm and Full Stack  Automation for AI and Beyond -- The Philosophy of Jaseci and Jac",
    "abstract": "In this work, the case is made for a wholistic top-down re-envisioning of the\nsystem stack from the programming language level down through the system\narchitecture to bridge this complexity gap. The key goal of our design is to\naddress the critical need for the programmer to articulate solutions with\nhigher level abstractions at the problem level while having the runtime system\nstack subsume and hide a broad scope of diffuse sub-applications and\ninter-machine resources. This work also presents the design of a\nproduction-grade realization of such a system stack architecture called Jaseci,\nand corresponding programming language Jac. Jac and Jaseci has been released as\nopen source and has been leveraged by real product teams to accelerate\ndeveloping and deploying sophisticated AI products and other applications at\nscale. Jac has been utilized in commercial production environments to\naccelerate AI development timelines by ~10x, with the Jaseci runtime automating\nthe decisions and optimizations typically falling in the scope of manual\nengineering roles on a team such as what should and should not be a\nmicroservice and changing those dynamically.",
    "descriptor": "",
    "authors": [
      "Jason Mars"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.08434"
  },
  {
    "id": "arXiv:2206.08436",
    "title": "Phase Identification of Distribution System Users Through a MILP  Extension of State Estimation",
    "abstract": "To address the challenges and exploit the opportunities that the\ndecarbonization of the energy sector is bringing about, advanced distribution\nnetwork management and operation strategies are being developed. Many of these\nrequire accurate network models to work effectively, including user phase\nconnectivity. However, such information is often not available. This paper\nproposes a novel method to identify the phase connectivity of single- and\nthree-phase distribution consumers using smart meter measurements. The method\nis based on state estimation and mixed-integer linear programming, and requires\nshorter measurement collection times compared to statistical and machine\nlearning-based techniques, for the same level of accuracy. Consequently, the\nduration of measurement efforts/campaigns aimed at obtaining this type of\ninformation reduces, accelerating the system knowledge acquisition process and\npotentially reducing the campaigns' costs. A technique is illustrated that\nallows to solve large networks in acceptable time despite the considerable\nnumber of integer variables. Extensive computational results are presented for\npublicly available low voltage feeders.",
    "descriptor": "",
    "authors": [
      "Marta Vanin",
      "Tom Van Acker",
      "Reinhilde D'hulst",
      "Dirk Van Hertem"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08436"
  },
  {
    "id": "arXiv:2206.08441",
    "title": "GAAMA 2.0: An Integrated System that Answers Boolean and Extractive  Question",
    "abstract": "Recent machine reading comprehension datasets include extractive and boolean\nquestions but current approaches do not offer integrated support for answering\nboth question types. We present a multilingual machine reading comprehension\nsystem and front-end demo that handles boolean questions by providing both a\nYES/NO answer and highlighting supporting evidence, and handles extractive\nquestions by highlighting the answer in the passage. Our system, GAAMA 2.0, is\nranked first on the Tydi QA leaderboard at the time of this writing. We\ncontrast two different implementations of our approach. The first includes\nseveral independent stacks of transformers allowing easy deployment of each\ncomponent. The second is a single stack of transformers utilizing adapters to\nreduce GPU memory footprint in a resource-constrained environment.",
    "descriptor": "",
    "authors": [
      "Scott McCarley",
      "Mihaela Bornea",
      "Sara Rosenthal",
      "Anthony Ferritto",
      "Md Arafat Sultan",
      "Avirup Sil",
      "Radu Florian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08441"
  },
  {
    "id": "arXiv:2206.08442",
    "title": "Understanding Decision-Time vs. Background Planning in Model-Based  Reinforcement Learning",
    "abstract": "In model-based reinforcement learning, an agent can leverage a learned model\nto improve its way of behaving in different ways. Two prevalent approaches are\ndecision-time planning and background planning. In this study, we are\ninterested in understanding under what conditions and in which settings one of\nthese two planning styles will perform better than the other in domains that\nrequire fast responses. After viewing them through the lens of dynamic\nprogramming, we first consider the classical instantiations of these planning\nstyles and provide theoretical results and hypotheses on which one will perform\nbetter in the pure planning, planning & learning, and transfer learning\nsettings. We then consider the modern instantiations of these planning styles\nand provide hypotheses on which one will perform better in the last two of the\nconsidered settings. Lastly, we perform several illustrative experiments to\nempirically validate both our theoretical results and hypotheses. Overall, our\nfindings suggest that even though decision-time planning does not perform as\nwell as background planning in their classical instantiations, in their modern\ninstantiations, it can perform on par or better than background planning in\nboth the planning & learning and transfer learning settings.",
    "descriptor": "",
    "authors": [
      "Safa Alver",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08442"
  },
  {
    "id": "arXiv:2206.08445",
    "title": "Enriching Abusive Language Detection with Community Context",
    "abstract": "Uses of pejorative expressions can be benign or actively empowering. When\nmodels for abuse detection misclassify these expressions as derogatory, they\ninadvertently censor productive conversations held by marginalized groups. One\nway to engage with non-dominant perspectives is to add context around\nconversations. Previous research has leveraged user- and thread-level features,\nbut it often neglects the spaces within which productive conversations take\nplace. Our paper highlights how community context can improve classification\noutcomes in abusive language detection. We make two main contributions to this\nend. First, we demonstrate that online communities cluster by the nature of\ntheir support towards victims of abuse. Second, we establish how community\ncontext improves accuracy and reduces the false positive rates of\nstate-of-the-art abusive language classifiers. These findings suggest a\npromising direction for context-aware models in abusive language research.",
    "descriptor": "",
    "authors": [
      "Jana Kurrek",
      "Haji Mohammad Saleem",
      "Derek Ruths"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08445"
  },
  {
    "id": "arXiv:2206.08446",
    "title": "Methods for Estimating and Improving Robustness of Language Models",
    "abstract": "Despite their outstanding performance, large language models (LLMs) suffer\nnotorious flaws related to their preference for simple, surface-level textual\nrelations over full semantic complexity of the problem. This proposal\ninvestigates a common denominator of this problem in their weak ability to\ngeneralise outside of the training domain. We survey diverse research\ndirections providing estimations of model generalisation ability and find that\nincorporating some of these measures in the training objectives leads to\nenhanced distributional robustness of neural models. Based on these findings,\nwe present future research directions towards enhancing the robustness of LLMs.",
    "descriptor": "\nComments: Thesis proposal, accepted & to appear in NAACL SRW 2022\n",
    "authors": [
      "Michal \u0160tef\u00e1nik"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08446"
  },
  {
    "id": "arXiv:2206.08448",
    "title": "Empirical Bayesian Approaches for Robust Constraint-based Causal  Discovery under Insufficient Data",
    "abstract": "Causal discovery is to learn cause-effect relationships among variables given\nobservational data and is important for many applications. Existing causal\ndiscovery methods assume data sufficiency, which may not be the case in many\nreal world datasets. As a result, many existing causal discovery methods can\nfail under limited data. In this work, we propose Bayesian-augmented\nfrequentist independence tests to improve the performance of constraint-based\ncausal discovery methods under insufficient data: 1) We firstly introduce a\nBayesian method to estimate mutual information (MI), based on which we propose\na robust MI based independence test; 2) Secondly, we consider the Bayesian\nestimation of hypothesis likelihood and incorporate it into a well-defined\nstatistical test, resulting in a robust statistical testing based independence\ntest. We apply proposed independence tests to constraint-based causal discovery\nmethods and evaluate the performance on benchmark datasets with insufficient\nsamples. Experiments show significant performance improvement in terms of both\naccuracy and efficiency over SOTA methods.",
    "descriptor": "\nComments: Accepted to IJCAI 2022\n",
    "authors": [
      "Zijun Cui",
      "Naiyu Yin",
      "Yuru Wang",
      "Qiang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08448"
  },
  {
    "id": "arXiv:2206.08450",
    "title": "Active Fairness Auditing",
    "abstract": "The fast spreading adoption of machine learning (ML) by companies across\nindustries poses significant regulatory challenges. One such challenge is\nscalability: how can regulatory bodies efficiently audit these ML models,\nensuring that they are fair? In this paper, we initiate the study of\nquery-based auditing algorithms that can estimate the demographic parity of ML\nmodels in a query-efficient manner. We propose an optimal deterministic\nalgorithm, as well as a practical randomized, oracle-efficient algorithm with\ncomparable guarantees. Furthermore, we make inroads into understanding the\noptimal query complexity of randomized active fairness estimation algorithms.\nOur first exploration of active fairness estimation aims to put AI governance\non firmer theoretical foundations.",
    "descriptor": "\nComments: 34 pages; 2 figures; ICML 2022\n",
    "authors": [
      "Tom Yan",
      "Chicheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08450"
  },
  {
    "id": "arXiv:2206.08451",
    "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine  Learning Models and Defences",
    "abstract": "Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm,\nmaking even the most complex machine learning models available for clients via\ne.g. a pay-per-query principle. This allows users to avoid time-consuming\nprocesses of data collection, hyperparameter tuning, and model training.\nHowever, by giving their customers access to the (predictions of their) models,\nMLaaS providers endanger their intellectual property, such as sensitive\ntraining data, optimised hyperparameters, or learned model parameters.\nAdversaries can create a copy of the model with (almost) identical behavior\nusing the the prediction labels only. While many variants of this attack have\nbeen described, only scattered defence strategies have been proposed,\naddressing isolated threats. This raises the necessity for a thorough\nsystematisation of the field of model stealing, to arrive at a comprehensive\nunderstanding why these attacks are successful, and how they could be\nholistically defended against. We address this by categorising and comparing\nmodel stealing attacks, assessing their performance, and exploring\ncorresponding defence techniques in different settings. We propose a taxonomy\nfor attack and defence approaches, and provide guidelines on how to select the\nright attack or defence strategy based on the goal and available resources.\nFinally, we analyse which defences are rendered less effective by current\nattack strategies.",
    "descriptor": "\nComments: Under review at ACM Computing Surveys\n",
    "authors": [
      "Daryna Oliynyk",
      "Rudolf Mayer",
      "Andreas Rauber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08451"
  },
  {
    "id": "arXiv:2206.08452",
    "title": "GOOD: A Graph Out-of-Distribution Benchmark",
    "abstract": "Out-of-distribution (OOD) learning deals with scenarios in which training and\ntest data follow different distributions. Although general OOD problems have\nbeen intensively studied in machine learning, graph OOD is only an emerging\narea of research. Currently, there lacks a systematic benchmark tailored to\ngraph OOD method evaluation. In this work, we aim at developing an OOD\nbenchmark, known as GOOD, for graphs specifically. We explicitly make\ndistinctions between covariate and concept shifts and design data splits that\naccurately reflect different shifts. We consider both graph and node prediction\ntasks as there are key differences when designing shifts. Overall, GOOD\ncontains 8 datasets with 14 domain selections. When combined with covariate,\nconcept, and no shifts, we obtain 42 different splits. We provide performance\nresults on 7 commonly used baseline methods with 10 random runs. This results\nin 294 dataset-model combinations in total. Our results show significant\nperformance gaps between in-distribution and OOD settings. Our results also\nshed light on different performance trends between covariate and concept shifts\nby different methods. Our GOOD benchmark is a growing project and expects to\nexpand in both quantity and variety of resources as the area develops. The GOOD\nbenchmark can be accessed via\n$\\href{https://github.com/divelab/GOOD/}{\\text{https://github.com/divelab/GOOD/}}$.",
    "descriptor": "",
    "authors": [
      "Shurui Gui",
      "Xiner Li",
      "Limei Wang",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08452"
  },
  {
    "id": "arXiv:2206.08454",
    "title": "Quantifying Feature Contributions to Overall Disparity Using Information  Theory",
    "abstract": "When a machine-learning algorithm makes biased decisions, it can be helpful\nto understand the sources of disparity to explain why the bias exists. Towards\nthis, we examine the problem of quantifying the contribution of each individual\nfeature to the observed disparity. If we have access to the decision-making\nmodel, one potential approach (inspired from intervention-based approaches in\nexplainability literature) is to vary each individual feature (while keeping\nthe others fixed) and use the resulting change in disparity to quantify its\ncontribution. However, we may not have access to the model or be able to\ntest/audit its outputs for individually varying features. Furthermore, the\ndecision may not always be a deterministic function of the input features\n(e.g., with human-in-the-loop). For these situations, we might need to explain\ncontributions using purely distributional (i.e., observational) techniques,\nrather than interventional. We ask the question: what is the \"potential\"\ncontribution of each individual feature to the observed disparity in the\ndecisions when the exact decision-making mechanism is not accessible? We first\nprovide canonical examples (thought experiments) that help illustrate the\ndifference between distributional and interventional approaches to explaining\ncontributions, and when either is better suited. When unable to intervene on\nthe inputs, we quantify the \"redundant\" statistical dependency about the\nprotected attribute that is present in both the final decision and an\nindividual feature, by leveraging a body of work in information theory called\nPartial Information Decomposition. We also perform a simple case study to show\nhow this technique could be applied to quantify contributions.",
    "descriptor": "\nComments: Presented at the AAAI-22 Workshop on Information-Theoretic Methods for Causal Inference and Discovery in March 2022\n",
    "authors": [
      "Sanghamitra Dutta",
      "Praveen Venkatesh",
      "Pulkit Grover"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08454"
  },
  {
    "id": "arXiv:2206.08455",
    "title": "Local overlap reduction procedure for dynamic ensemble selection",
    "abstract": "Class imbalance is a characteristic known for making learning more\nchallenging for classification models as they may end up biased towards the\nmajority class. A promising approach among the ensemble-based methods in the\ncontext of imbalance learning is Dynamic Selection (DS). DS techniques single\nout a subset of the classifiers in the ensemble to label each given unknown\nsample according to their estimated competence in the area surrounding the\nquery. Because only a small region is taken into account in the selection\nscheme, the global class disproportion may have less impact over the system's\nperformance. However, the presence of local class overlap may severely hinder\nthe DS techniques' performance over imbalanced distributions as it not only\nexacerbates the effects of the under-representation but also introduces\nambiguous and possibly unreliable samples to the competence estimation process.\nThus, in this work, we propose a DS technique which attempts to minimize the\neffects of the local class overlap during the classifier selection procedure.\nThe proposed method iteratively removes from the target region the instance\nperceived as the hardest to classify until a classifier is deemed competent to\nlabel the query sample. The known samples are characterized using instance\nhardness measures that quantify the local class overlap. Experimental results\nshow that the proposed technique can significantly outperform the baseline as\nwell as several other DS techniques, suggesting its suitability for dealing\nwith class under-representation and overlap. Furthermore, the proposed\ntechnique still yielded competitive results when using an under-sampled, less\noverlapped version of the labelled sets, specially over the problems with a\nhigh proportion of minority class samples in overlap areas. Code available at\nhttps://github.com/marianaasouza/lords.",
    "descriptor": "\nComments: Paper accepted to the 2022 International Joint Conference on Neural Networks\n",
    "authors": [
      "Mariana A. Souza",
      "Robert Sabourin",
      "George D. C. Cavalcanti",
      "Rafael M. O. Cruz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08455"
  },
  {
    "id": "arXiv:2206.08460",
    "title": "TUSK: Task-Agnostic Unsupervised Keypoints",
    "abstract": "Existing unsupervised methods for keypoint learning rely heavily on the\nassumption that a specific keypoint type (e.g. elbow, digit, abstract geometric\nshape) appears only once in an image. This greatly limits their applicability,\nas each instance must be isolated before applying the method-an issue that is\nnever discussed or evaluated. We thus propose a novel method to learn\nTask-agnostic, UnSupervised Keypoints (TUSK) which can deal with multiple\ninstances. To achieve this, instead of the commonly-used strategy of detecting\nmultiple heatmaps, each dedicated to a specific keypoint type, we use a single\nheatmap for detection, and enable unsupervised learning of keypoint types\nthrough clustering. Specifically, we encode semantics into the keypoints by\nteaching them to reconstruct images from a sparse set of keypoints and their\ndescriptors, where the descriptors are forced to form distinct clusters in\nfeature space around learned prototypes. This makes our approach amenable to a\nwider range of tasks than any previous unsupervised keypoint method: we show\nexperiments on multiple-instance detection and classification, object\ndiscovery, and landmark detection-all unsupervised-with performance on par with\nthe state of the art, while also being able to deal with multiple instances.",
    "descriptor": "",
    "authors": [
      "Yuhe Jin",
      "Weiwei Sun",
      "Jan Hosang",
      "Eduard Trulls",
      "Kwang Moo Yi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08460"
  },
  {
    "id": "arXiv:2206.08462",
    "title": "Recursive Neural Programs: Variational Learning of Image Grammars and  Part-Whole Hierarchies",
    "abstract": "Human vision involves parsing and representing objects and scenes using\nstructured representations based on part-whole hierarchies. Computer vision and\nmachine learning researchers have recently sought to emulate this capability\nusing capsule networks, reference frames and active predictive coding, but a\ngenerative model formulation has been lacking. We introduce Recursive Neural\nPrograms (RNPs), which, to our knowledge, is the first neural generative model\nto address the part-whole hierarchy learning problem. RNPs model images as\nhierarchical trees of probabilistic sensory-motor programs that recursively\nreuse learned sensory-motor primitives to model an image within different\nreference frames, forming recursive image grammars. We express RNPs as\nstructured variational autoencoders (sVAEs) for inference and sampling, and\ndemonstrate parts-based parsing, sampling and one-shot transfer learning for\nMNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's\nexpressive power. Our results show that RNPs provide an intuitive and\nexplainable way of composing objects and scenes, allowing rich compositionality\nand intuitive interpretations of objects in terms of part-whole hierarchies.",
    "descriptor": "\nComments: Code repository up soon :)\n",
    "authors": [
      "Ares Fisher",
      "Rajesh P.N. Rao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08462"
  },
  {
    "id": "arXiv:2206.08464",
    "title": "PRANC: Pseudo RAndom Networks for Compacting deep models",
    "abstract": "Communication becomes a bottleneck in various distributed Machine Learning\nsettings. Here, we propose a novel training framework that leads to highly\nefficient communication of models between agents. In short, we train our\nnetwork to be a linear combination of many pseudo-randomly generated frozen\nmodels. For communication, the source agent transmits only the `seed' scalar\nused to generate the pseudo-random `basis' networks along with the learned\nlinear mixture coefficients. Our method, denoted as PRANC, learns almost\n$100\\times$ fewer parameters than a deep model and still performs well on\nseveral datasets and architectures. PRANC enables 1) efficient communication of\nmodels between agents, 2) efficient model storage, and 3) accelerated inference\nby generating layer-wise weights on the fly. We test PRANC on CIFAR-10,\nCIFAR-100, tinyImageNet, and ImageNet-100 with various architectures like\nAlexNet, LeNet, ResNet18, ResNet20, and ResNet56 and demonstrate a massive\nreduction in the number of parameters while providing satisfactory performance\non these benchmark datasets. The code is available\n\\href{https://github.com/UCDvision/PRANC}{https://github.com/UCDvision/PRANC}",
    "descriptor": "",
    "authors": [
      "Parsa Nooralinejad",
      "Ali Abbasi",
      "Soheil Kolouri",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08464"
  },
  {
    "id": "arXiv:2206.08468",
    "title": "Belief-Desire-Intention (BDI) Multi-agent System for Cloud Marketplace  Negotiation",
    "abstract": "With the evolution of cloud computing, there has been a rise of large\nenterprises extending their infrastructure and workloads into the public cloud.\nThis paper proposes a full-fledged framework for a Belief-Desire-Intention\n(BDI) multi-agent-based cloud marketplace system for cloud resources. Each\nparty in the cloud marketplace system supports a BDI agent for autonomous\ndecision making and negotiation to facilitate automated buying and selling of\nresources. Additionally, multiple BDI agents from an enterprise competing for\nthe same cloud resource can consult with each other via Master Negotiation\nClearing House to minimize the overall cost function for the enterprise while\nnegotiating for a cloud resource. The cloud marketplace system is further\naugmented with assignments of behavior norm and reputation index to the agents\nto facilitate trust among them.",
    "descriptor": "\nComments: 19th International Conference on Distributed Computing and Artificial Intelligence\n",
    "authors": [
      "Saurabh Deochake"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.08468"
  },
  {
    "id": "arXiv:2206.08472",
    "title": "Combined Plant and Controller Optimization of an Underwater Energy  Harvesting Kite System",
    "abstract": "This paper presents the formulation and results for a control-aware\noptimization of the combined geometric and structural design of an\nenergy-harvesting underwater kite. Because kite-based energy-harvesting\nsystems, both airborne and underwater, possess strong coupling between\nclosed-loop flight control, geometric design, and structural design,\nconsideration of all three facets of the design within a single co-design\nframework is highly desirable. However, while prior literature has addressed\none or two attributes of the design at a time, the present work constitutes the\nfirst comprehensive effort aimed at addressing all three. In particular,\nfocusing on the goals of power maximization and mass minimization, we present a\nco-design formulation that fuses a geometric optimization tool, structural\noptimization tool, and closed-loop flight efficiency map. The resulting\nintegrated co-design tool is used to address two mathematical optimization\nformulations that exhibit subtle differences: a Pareto optimal formulation and\na dual-objective formulation that focuses on a weighted power-to-mass ratio as\nthe techno-economic metric of merit. Based on the resulting geometric and\nstructural designs, using a medium-fidelity closed-loop simulation tool, the\nproposed formulation is shown to achieve more than three times the\npower-to-mass ratio of a previously published, un-optimized benchmark design.",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Kartik Naik",
      "Sumedh Beknalkar",
      "James Reed",
      "Andre Mazzoleni",
      "Hosam Fathy",
      "Chris Vermillion"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08472"
  },
  {
    "id": "arXiv:2206.08473",
    "title": "A Robust Stacking Framework for Training Deep Graph Models with  Multifaceted Node Features",
    "abstract": "Graph Neural Networks (GNNs) with numerical node features and graph structure\nas inputs have demonstrated superior performance on various supervised learning\ntasks with graph data. However the numerical node features utilized by GNNs are\ncommonly extracted from raw data which is of text or tabular\n(numeric/categorical) type in most real-world applications. The best models for\nsuch data types in most standard supervised learning settings with IID\n(non-graph) data are not simple neural network layers and thus are not easily\nincorporated into a GNN. Here we propose a robust stacking framework that fuses\ngraph-aware propagation with arbitrary models intended for IID data, which are\nensembled and stacked in multiple layers. Our layer-wise framework leverages\nbagging and stacking strategies to enjoy strong generalization, in a manner\nwhich effectively mitigates label leakage and overfitting. Across a variety of\ngraph datasets with tabular/text node features, our method achieves comparable\nor superior performance relative to both tabular/text and graph neural network\nmodels, as well as existing state-of-the-art hybrid strategies that combine the\ntwo.",
    "descriptor": "",
    "authors": [
      "Jiuhai Chen",
      "Jonas Mueller",
      "Vassilis N. Ioannidis",
      "Tom Goldstein",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08473"
  },
  {
    "id": "arXiv:2206.08474",
    "title": "XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence",
    "abstract": "Recent advances in machine learning have significantly improved the\nunderstanding of source code data and achieved good performance on a number of\ndownstream tasks. Open source repositories like GitHub enable this process with\nrich unlabeled code data. However, the lack of high quality labeled data has\nlargely hindered the progress of several code related tasks, such as program\ntranslation, summarization, synthesis, and code search. This paper introduces\nXLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for\ncross-lingual code intelligence. Our dataset contains fine-grained parallel\ndata from 8 languages (7 commonly used programming languages and English), and\nsupports 10 cross-lingual code tasks. To the best of our knowledge, it is the\nlargest parallel dataset for source code both in terms of size and the number\nof languages. We also provide the performance of several state-of-the-art\nbaseline models for each task. We believe this new dataset can be a valuable\nasset for the research community and facilitate the development and validation\nof new methods for cross-lingual code intelligence.",
    "descriptor": "\nComments: 20 pages, 11 tables, 2 figures\n",
    "authors": [
      "Ming Zhu",
      "Aneesh Jain",
      "Karthik Suresh",
      "Roshan Ravindran",
      "Sindhu Tipirneni",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08474"
  },
  {
    "id": "arXiv:2206.08476",
    "title": "Zero-Shot AutoML with Pretrained Models",
    "abstract": "Given a new dataset D and a low compute budget, how should we choose a\npre-trained model to fine-tune to D, and set the fine-tuning hyperparameters\nwithout risking overfitting, particularly if D is small? Here, we extend\nautomated machine learning (AutoML) to best make these choices. Our\ndomain-independent meta-learning approach learns a zero-shot surrogate model\nwhich, at test time, allows to select the right deep learning (DL) pipeline\n(including the pre-trained model and fine-tuning hyperparameters) for a new\ndataset D given only trivial meta-features describing D such as image\nresolution or the number of classes. To train this zero-shot model, we collect\nperformance data for many DL pipelines on a large collection of datasets and\nmeta-train on this data to minimize a pairwise ranking objective. We evaluate\nour approach under the strict time limit of the vision track of the ChaLearn\nAutoDL challenge benchmark, clearly outperforming all challenge contenders.",
    "descriptor": "",
    "authors": [
      "Ekrem \u00d6zt\u00fcrk",
      "Fabio Ferreira",
      "Hadi S. Jomaa",
      "Lars Schmidt-Thieme",
      "Josif Grabocka",
      "Frank Hutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08476"
  },
  {
    "id": "arXiv:2206.08477",
    "title": "Backdoor Attacks on Vision Transformers",
    "abstract": "Vision Transformers (ViT) have recently demonstrated exemplary performance on\na variety of vision tasks and are being used as an alternative to CNNs. Their\ndesign is based on a self-attention mechanism that processes images as a\nsequence of patches, which is quite different compared to CNNs. Hence it is\ninteresting to study if ViTs are vulnerable to backdoor attacks. Backdoor\nattacks happen when an attacker poisons a small part of the training data for\nmalicious purposes. The model performance is good on clean test images, but the\nattacker can manipulate the decision of the model by showing the trigger at\ntest time. To the best of our knowledge, we are the first to show that ViTs are\nvulnerable to backdoor attacks. We also find an intriguing difference between\nViTs and CNNs - interpretation algorithms effectively highlight the trigger on\ntest images for ViTs but not for CNNs. Based on this observation, we propose a\ntest-time image blocking defense for ViTs which reduces the attack success rate\nby a large margin. Code is available here:\nhttps://github.com/UCDvision/backdoor_transformer.git",
    "descriptor": "",
    "authors": [
      "Akshayvarun Subramanya",
      "Aniruddha Saha",
      "Soroush Abbasi Koohpayegani",
      "Ajinkya Tejankar",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08477"
  },
  {
    "id": "arXiv:2206.08478",
    "title": "Classification of datasets with imputed missing values: does imputation  quality matter?",
    "abstract": "Classifying samples in incomplete datasets is a common aim for machine\nlearning practitioners, but is non-trivial. Missing data is found in most\nreal-world datasets and these missing values are typically imputed using\nestablished methods, followed by classification of the now complete, imputed,\nsamples. The focus of the machine learning researcher is then to optimise the\ndownstream classification performance. In this study, we highlight that it is\nimperative to consider the quality of the imputation. We demonstrate how the\ncommonly used measures for assessing quality are flawed and propose a new class\nof discrepancy scores which focus on how well the method recreates the overall\ndistribution of the data. To conclude, we highlight the compromised\ninterpretability of classifier models trained using poorly imputed data.",
    "descriptor": "\nComments: 17 pages, 10 figures, 30 supplementary pages\n",
    "authors": [
      "Tolou Shadbahr",
      "Michael Roberts",
      "Jan Stanczuk",
      "Julian Gilbey",
      "Philip Teare",
      "S\u00f6ren Dittmer",
      "Matthew Thorpe",
      "Ramon Vinas Torne",
      "Evis Sala",
      "Pietro Lio",
      "Mishal Patel",
      "AIX-COVNET Collaboration",
      "James H.F. Rudd",
      "Tuomas Mirtti",
      "Antti Rannikko",
      "John A.D. Aston",
      "Jing Tang",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08478"
  },
  {
    "id": "arXiv:2206.08479",
    "title": "Modifying the Asynchronous Jacobi Method for Data Corruption Resilience",
    "abstract": "Moving scientific computation from high-performance computing (HPC) and cloud\ncomputing (CC) environments to devices on the edge, where data can be collected\nby streamlined computing devices that are physically near instruments of\ninterest, has garnered tremendous interest in recent years. Such edge computing\nenvironments can operate on data in-situ instead of requiring the collection of\ndata in HPC and/or CC facilities, offering enticing benefits that include\navoiding costs of transmission over potentially unreliable or slow networks,\nincreased data privacy, and real-time data analysis. Before such benefits can\nbe realized at scale, new fault tolerances approaches must be developed to\naddress the inherent unreliability of edge computing environments, because the\ntraditional approaches used by HPC and CC are not generally applicable to edge\ncomputing. Those traditional approaches commonly utilize checkpoint-and-restart\nand/or redundant-computation strategies that are not feasible for edge\ncomputing environments where data storage is limited and synchronization is\ncostly. Motivated by prior algorithm-based fault tolerance approaches, an\nasynchronous Jacobi (ASJ) variant is developed herein with resilience to data\ncorruption by leveraging existing convergence theory. The ASJ variant rejects\nsolution approximations from neighbor devices if the distance between two\nsuccessive approximations violates an analytic bound. Numerical results show\nthe ASJ variant restores convergence in the presence of certain types of\nnatural and malicious data corruption.",
    "descriptor": "\nComments: Submitted to SIAM SISC June 15, 2022\n",
    "authors": [
      "Christopher J. Vogl",
      "Zach Atkins",
      "Alyson Fox",
      "Agnieszka Miedlar",
      "Colin Ponce"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08479"
  },
  {
    "id": "arXiv:2206.08482",
    "title": "GMI-DRL: Empowering Multi-GPU Deep Reinforcement Learning with GPU  Spatial Multiplexing",
    "abstract": "With the increasing popularity of robotics in industrial control and\nautonomous driving, deep reinforcement learning (DRL) raises the attention of\nvarious fields. However, DRL computation on the modern powerful GPU platform is\nstill inefficient due to its heterogeneous workloads and interleaved execution\nparadigm. To this end, we propose GMI-DRL, a systematic design to accelerate\nmulti-GPU DRL via GPU spatial multiplexing. We introduce a novel design of\nresource-adjustable GPU multiplexing instances (GMIs) to match the actual needs\nof DRL tasks, an adaptive GMI management strategy to simultaneously achieve\nhigh GPU utilization and computation throughput, and a highly efficient\ninter-GMI communication support to meet the demands of various DRL\ncommunication patterns. Comprehensive experiments reveal that GMI-DRL\noutperforms state-of-the-art NVIDIA Isaac Gym with NCCL (up to 2.81X) and\nHorovod (up to 2.34X) support in training throughput on the latest DGX-A100\nplatform. Our work provides an initial user experience with GPU spatial\nmultiplexing in processing heterogeneous workloads with a mixture of\ncomputation and communication.",
    "descriptor": "",
    "authors": [
      "Yuke Wang",
      "Boyuan Feng",
      "Zheng Wang",
      "Tong Geng",
      "Ang Li",
      "Yufei Ding"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.08482"
  },
  {
    "id": "arXiv:2206.08485",
    "title": "Modeling Gender Differences in Membership Change in Open Source Software  Projects",
    "abstract": "Gender diversity in open source software development continues to be a topic\nof growing interest among researchers, practitioners, and organizations. To\ndate, research has revealed disparities in participation between developers on\nthe basis of gender, with women being significantly underrepresented in open\nsource development. Using a large data set curated for studies of diversity in\nopen source projects, we contribute to this body of work by characterizing the\nrelationship between gender-based participation differences and group\ncomposition in GitHub. We found that contributors identified as women and\ncontributors of unknown gender have a shorter tenure in open source projects\ncompared to those identified as men. Additionally, at the team level, we found\nthat project teams with mixed-gender composition were associated with lower\nturnover and teams with greater disparity in the distribution of platform\ntenure were associated with higher turnover. Finally, our case study reveals\nthat when looking at the entire team, GitHub users were more likely to remain\nin a project rather than leave after contributing but when looking at women\nspecifically, users were more likely to leave rather than remain after\ncontributing to a project.",
    "descriptor": "\nComments: 10 pages, 2 figures, 7 tables\n",
    "authors": [
      "Olivia B. Newton",
      "Jihye Song"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.08485"
  },
  {
    "id": "arXiv:2206.08486",
    "title": "Interpretable AMR-Based Question Decomposition for Multi-hop Question  Answering",
    "abstract": "Effective multi-hop question answering (QA) requires reasoning over multiple\nscattered paragraphs and providing explanations for answers. Most existing\napproaches cannot provide an interpretable reasoning process to illustrate how\nthese models arrive at an answer. In this paper, we propose a Question\nDecomposition method based on Abstract Meaning Representation (QDAMR) for\nmulti-hop QA, which achieves interpretable reasoning by decomposing a multi-hop\nquestion into simpler sub-questions and answering them in order. Since\nannotating the decomposition is expensive, we first delegate the complexity of\nunderstanding the multi-hop question to an AMR parser. We then achieve the\ndecomposition of a multi-hop question via segmentation of the corresponding AMR\ngraph based on the required reasoning type. Finally, we generate sub-questions\nusing an AMR-to-Text generation model and answer them with an off-the-shelf QA\nmodel. Experimental results on HotpotQA demonstrate that our approach is\ncompetitive for interpretable reasoning and that the sub-questions generated by\nQDAMR are well-formed, outperforming existing question-decomposition-based\nmulti-hop QA approaches.",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Zhenyun Deng",
      "Yonghua Zhu",
      "Yang Chen",
      "Michael Witbrock",
      "Patricia Riddle"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08486"
  },
  {
    "id": "arXiv:2206.08487",
    "title": "High-Speed Accurate Robot Control using Learned Forward Kinodynamics and  Non-linear Least Squares Optimization",
    "abstract": "Accurate control of robots in the real world requires a control system that\nis capable of taking into account the kinodynamic interactions of the robot\nwith its environment. At high speeds, the dependence of the movement of the\nrobot on these kinodynamic interactions becomes more pronounced, making\nhigh-speed, accurate robot control a challenging problem. Previous work has\nshown that learning the inverse kinodynamics (IKD) of the robot can be helpful\nfor high-speed robot control. However a learned inverse kinodynamic model can\nonly be applied to a limited class of control problems, and different control\nproblems require the learning of a new IKD model. In this work we present a new\nformulation for accurate, high-speed robot control that makes use of a learned\nforward kinodynamic (FKD) model and non-linear least squares optimization. By\nnature of the formulation, this approach is extensible to a wide array of\ncontrol problems without requiring the retraining of a new model. We\ndemonstrate the ability of this approach to accurately control a scale\none-tenth robot car at high speeds, and show improved results over baselines.",
    "descriptor": "",
    "authors": [
      "Pranav Atreya",
      "Haresh Karnan",
      "Kavan Singh Sikand",
      "Xuesu Xiao",
      "Garrett Warnell",
      "Sadegh Rabiee",
      "Peter Stone",
      "Joydeep Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08487"
  },
  {
    "id": "arXiv:2206.08488",
    "title": "Controllable Image Enhancement",
    "abstract": "Editing flat-looking images into stunning photographs requires skill and\ntime. Automated image enhancement algorithms have attracted increased interest\nby generating high-quality images without user interaction. However, the\nquality assessment of a photograph is subjective. Even in tone and color\nadjustments, a single photograph of auto-enhancement is challenging to fit user\npreferences which are subtle and even changeable. To address this problem, we\npresent a semiautomatic image enhancement algorithm that can generate\nhigh-quality images with multiple styles by controlling a few parameters. We\nfirst disentangle photo retouching skills from high-quality images and build an\nefficient enhancement system for each skill. Specifically, an encoder-decoder\nframework encodes the retouching skills into latent codes and decodes them into\nthe parameters of image signal processing (ISP) functions. The ISP functions\nare computationally efficient and consist of only 19 parameters. Despite our\napproach requiring multiple inferences to obtain the desired result,\nexperimental results present that the proposed method achieves state-of-the-art\nperformances on the benchmark dataset for image quality and model efficiency.",
    "descriptor": "",
    "authors": [
      "Heewon Kim",
      "Kyoung Mu Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08488"
  },
  {
    "id": "arXiv:2206.08489",
    "title": "Debugging using Orthogonal Gradient Descent",
    "abstract": "In this report we consider the following problem: Given a trained model that\nis partially faulty, can we correct its behaviour without having to train the\nmodel from scratch? In other words, can we ``debug\" neural networks similar to\nhow we address bugs in our mathematical models and standard computer code. We\nbase our approach on the hypothesis that debugging can be treated as a two-task\ncontinual learning problem. In particular, we employ a modified version of a\ncontinual learning algorithm called Orthogonal Gradient Descent (OGD) to\ndemonstrate, via two simple experiments on the MNIST dataset, that we can\nin-fact \\textit{unlearn} the undesirable behaviour while retaining the general\nperformance of the model, and we can additionally \\textit{relearn} the\nappropriate behaviour, both without having to train the model from scratch.",
    "descriptor": "",
    "authors": [
      "Narsimha Chilkuri",
      "Chris Eliasmith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08489"
  },
  {
    "id": "arXiv:2206.08491",
    "title": "Revisiting Self-Distillation",
    "abstract": "Knowledge distillation is the procedure of transferring \"knowledge\" from a\nlarge model (the teacher) to a more compact one (the student), often being used\nin the context of model compression. When both models have the same\narchitecture, this procedure is called self-distillation. Several works have\nanecdotally shown that a self-distilled student can outperform the teacher on\nheld-out data. In this work, we systematically study self-distillation in a\nnumber of settings. We first show that even with a highly accurate teacher,\nself-distillation allows a student to surpass the teacher in all cases.\nSecondly, we revisit existing theoretical explanations of (self) distillation\nand identify contradicting examples, revealing possible drawbacks of these\nexplanations. Finally, we provide an alternative explanation for the dynamics\nof self-distillation through the lens of loss landscape geometry. We conduct\nextensive experiments to show that self-distillation leads to flatter minima,\nthereby resulting in better generalization.",
    "descriptor": "",
    "authors": [
      "Minh Pham",
      "Minsu Cho",
      "Ameya Joshi",
      "Chinmay Hegde"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08491"
  },
  {
    "id": "arXiv:2206.08492",
    "title": "TKIL: Tangent Kernel Approach for Class Balanced Incremental Learning",
    "abstract": "When learning new tasks in a sequential manner, deep neural networks tend to\nforget tasks that they previously learned, a phenomenon called catastrophic\nforgetting. Class incremental learning methods aim to address this problem by\nkeeping a memory of a few exemplars from previously learned tasks, and\ndistilling knowledge from them. However, existing methods struggle to balance\nthe performance across classes since they typically overfit the model to the\nlatest task. In our work, we propose to address these challenges with the\nintroduction of a novel methodology of Tangent Kernel for Incremental Learning\n(TKIL) that achieves class-balanced performance. The approach preserves the\nrepresentations across classes and balances the accuracy for each class, and as\nsuch achieves better overall accuracy and variance. TKIL approach is based on\nNeural Tangent Kernel (NTK), which describes the convergence behavior of neural\nnetworks as a kernel function in the limit of infinite width. In TKIL, the\ngradients between feature layers are treated as the distance between the\nrepresentations of these layers and can be defined as Gradients Tangent Kernel\nloss (GTK loss) such that it is minimized along with averaging weights. This\nallows TKIL to automatically identify the task and to quickly adapt to it\nduring inference. Experiments on CIFAR-100 and ImageNet datasets with various\nincremental learning settings show that these strategies allow TKIL to\noutperform existing state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Jinlin Xiang",
      "Eli Shlizerman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08492"
  },
  {
    "id": "arXiv:2206.08493",
    "title": "Nonconforming finite elements for the Brinkman and $-\\text{curl}\u0394  \\text{curl}$ problems on cubical meshes",
    "abstract": "We propose two families of nonconforming elements on cubical meshes: one for\nthe $-\\text{curl}\\Delta\\text{curl}$ problem and the other for the Brinkman\nproblem. The element for the $-\\text{curl}\\Delta\\text{curl}$ problem is the\nfirst nonconforming element on cubical meshes. The element for the Brinkman\nproblem can yield a uniformly stable finite element method with respect to the\nparameter $\\nu$. The lowest-order elements for the\n$-\\text{curl}\\Delta\\text{curl}$ and the Brinkman problems have 48 and 30\ndegrees of freedom, respectively.\nThe two families of elements are subspaces of $H(\\text{curl};\\Omega)$ and\n$H(\\text{div};\\Omega)$, and they, as nonconforming approximation to\n$H(\\text{gradcurl};\\Omega)$ and $[H^1(\\Omega)]^3$, can form a discrete Stokes\ncomplex together with the Lagrange element and the $L^2$ element.",
    "descriptor": "",
    "authors": [
      "Qian Zhang",
      "Min Zhang",
      "Zhimin Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08493"
  },
  {
    "id": "arXiv:2206.08494",
    "title": "Factorization Approach for Sparse Spatio-Temporal Brain-Computer  Interface",
    "abstract": "Recently, advanced technologies have unlimited potential in solving various\nproblems with a large amount of data. However, these technologies have yet to\nshow competitive performance in brain-computer interfaces (BCIs) which deal\nwith brain signals. Basically, brain signals are difficult to collect in large\nquantities, in particular, the amount of information would be sparse in\nspontaneous BCIs. In addition, we conjecture that high spatial and temporal\nsimilarities between tasks increase the prediction difficulty. We define this\nproblem as sparse condition. To solve this, a factorization approach is\nintroduced to allow the model to obtain distinct representations from latent\nspace. To this end, we propose two feature extractors: A class-common module is\ntrained through adversarial learning acting as a generator; Class-specific\nmodule utilizes loss function generated from classification so that features\nare extracted with traditional methods. To minimize the latent space shared by\nthe class-common and class-specific features, the model is trained under\northogonal constraint. As a result, EEG signals are factorized into two\nseparate latent spaces. Evaluations were conducted on a single-arm motor\nimagery dataset. From the results, we demonstrated that factorizing the EEG\nsignal allows the model to extract rich and decisive features under sparse\ncondition.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Byeong-Hoo Lee",
      "Jeong-Hyun Cho",
      "Byoung-Hee Kwon",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08494"
  },
  {
    "id": "arXiv:2206.08495",
    "title": "Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid  Rank Valuations",
    "abstract": "We study fair allocation of indivisible goods when agents have matroid rank\nvaluations. Our main contribution is a simple algorithm based on the colloquial\nYankee Swap procedure that computes provably fair and efficient Lorenz\ndominating allocations. While there exist polynomial time algorithms to compute\nsuch allocations, our proposed method improves on them in two ways. (a) Our\napproach is easy to understand and does not use complex matroid optimization\nalgorithms as subroutines. (b) Our approach is scalable; it is provably faster\nthan all known algorithms to compute Lorenz dominating allocations. These two\nproperties are key to the adoption of algorithms in any real fair allocation\nsetting; our contribution brings us one step closer to this goal.",
    "descriptor": "",
    "authors": [
      "Vignesh Viswanathan",
      "Yair Zick"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08495"
  },
  {
    "id": "arXiv:2206.08496",
    "title": "Self-Supervised Contrastive Pre-Training For Time Series via  Time-Frequency Consistency",
    "abstract": "Pre-training on time series poses a unique challenge due to the potential\nmismatch between pre-training and target domains, such as shifts in temporal\ndynamics, fast-evolving trends, and long-range and short cyclic effects, which\ncan lead to poor downstream performance. While domain adaptation methods can\nmitigate these shifts, most methods need examples directly from the target\ndomain, making them suboptimal for pre-training. To address this challenge,\nmethods need to accommodate target domains with different temporal dynamics and\nbe capable of doing so without seeing any target examples during pre-training.\nRelative to other modalities, in time series, we expect that time-based and\nfrequency-based representations of the same example are located close together\nin the time-frequency space. To this end, we posit that time-frequency\nconsistency (TF-C) -- embedding a time-based neighborhood of a particular\nexample close to its frequency-based neighborhood and back -- is desirable for\npre-training. Motivated by TF-C, we define a decomposable pre-training model,\nwhere the self-supervised signal is provided by the distance between time and\nfrequency components, each individually trained by contrastive estimation. We\nevaluate the new method on eight datasets, including electrodiagnostic testing,\nhuman activity recognition, mechanical fault detection, and physical status\nmonitoring. Experiments against eight state-of-the-art methods show that TF-C\noutperforms baselines by 15.4% (F1 score) on average in one-to-one settings\n(e.g., fine-tuning an EEG-pretrained model on EMG data) and by up to 8.4% (F1\nscore) in challenging one-to-many settings, reflecting the breadth of scenarios\nthat arise in real-world applications. The source code and datasets are\navailable at https: //anonymous.4open.science/r/TFC-pretraining-6B07.",
    "descriptor": "\nComments: Under review; the anonymouse code repo link will be made non-anonymous after acceptance; 21 pages (13 pages main paper + 8 pages supplementary materials)\n",
    "authors": [
      "Xiang Zhang",
      "Ziyuan Zhao",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08496"
  },
  {
    "id": "arXiv:2206.08497",
    "title": "Unsupervised Kinematic Motion Detection for Part-segmented 3D Shape  Collections",
    "abstract": "3D models of manufactured objects are important for populating virtual worlds\nand for synthetic data generation for vision and robotics. To be most useful,\nsuch objects should be articulated: their parts should move when interacted\nwith. While articulated object datasets exist, creating them is\nlabor-intensive. Learning-based prediction of part motions can help, but all\nexisting methods require annotated training data. In this paper, we present an\nunsupervised approach for discovering articulated motions in a part-segmented\n3D shape collection. Our approach is based on a concept we call category\nclosure: any valid articulation of an object's parts should keep the object in\nthe same semantic category (e.g. a chair stays a chair). We operationalize this\nconcept with an algorithm that optimizes a shape's part motion parameters such\nthat it can transform into other shapes in the collection. We evaluate our\napproach by using it to re-discover part motions from the PartNet-Mobility\ndataset. For almost all shape categories, our method's predicted motion\nparameters have low error with respect to ground truth annotations,\noutperforming two supervised motion prediction methods.",
    "descriptor": "\nComments: SIGGRAPH 2022\n",
    "authors": [
      "Xianghao Xu",
      "Yifan Ruan",
      "Srinath Sridhar",
      "Daniel Ritchie"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08497"
  },
  {
    "id": "arXiv:2206.08499",
    "title": "A Parametric Class of Approximate Gradient Updates for Policy  Optimization",
    "abstract": "Approaches to policy optimization have been motivated from diverse\nprinciples, based on how the parametric model is interpreted (e.g. value versus\npolicy representation) or how the learning objective is formulated, yet they\nshare a common goal of maximizing expected return. To better capture the\ncommonalities and identify key differences between policy optimization methods,\nwe develop a unified perspective that re-expresses the underlying updates in\nterms of a limited choice of gradient form and scaling function. In particular,\nwe identify a parameterized space of approximate gradient updates for policy\noptimization that is highly structured, yet covers both classical and recent\nexamples, including PPO. As a result, we obtain novel yet well motivated\nupdates that generalize existing algorithms in a way that can deliver benefits\nboth in terms of convergence speed and final result quality. An experimental\ninvestigation demonstrates that the additional degrees of freedom provided in\nthe parameterized family of updates can be leveraged to obtain non-trivial\nimprovements both in synthetic domains and on popular deep RL benchmarks.",
    "descriptor": "",
    "authors": [
      "Ramki Gummadi",
      "Saurabh Kumar",
      "Junfeng Wen",
      "Dale Schuurmans"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08499"
  },
  {
    "id": "arXiv:2206.08500",
    "title": "What do navigation agents learn about their environment?",
    "abstract": "Today's state of the art visual navigation agents typically consist of large\ndeep learning models trained end to end. Such models offer little to no\ninterpretability about the learned skills or the actions of the agent taken in\nresponse to its environment. While past works have explored interpreting deep\nlearning models, little attention has been devoted to interpreting embodied AI\nsystems, which often involve reasoning about the structure of the environment,\ntarget characteristics and the outcome of one's actions. In this paper, we\nintroduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal\nand Object Goal navigation agents. We use iSEE to probe the dynamic\nrepresentations produced by these agents for the presence of information about\nthe agent as well as the environment. We demonstrate interesting insights about\nnavigation agents using iSEE, including the ability to encode reachable\nlocations (to avoid obstacles), visibility of the target, progress from the\ninitial spawn location as well as the dramatic effect on the behaviors of\nagents when we mask out critical individual neurons. The code is available at:\nhttps://github.com/allenai/iSEE",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Kshitij Dwivedi",
      "Gemma Roig",
      "Aniruddha Kembhavi",
      "Roozbeh Mottaghi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08500"
  },
  {
    "id": "arXiv:2206.08506",
    "title": "A Numerical Reasoning Question Answering System with Fine-grained  Retriever and the Ensemble of Multiple Generators for FinQA",
    "abstract": "The numerical reasoning in the financial domain -- performing quantitative\nanalysis and summarizing the information from financial reports -- can greatly\nincrease business efficiency and reduce costs of billions of dollars. Here, we\npropose a numerical reasoning question answering system to answer numerical\nreasoning questions among financial text and table data sources, consisting of\na retriever module, a generator module, and an ensemble module. Specifically,\nin the retriever module, in addition to retrieving the whole row data, we\ninnovatively design a cell retriever that retrieves the gold cells to avoid\nbringing unrelated and similar cells in the same row to the inputs of the\ngenerator module. In the generator module, we utilize multiple generators to\nproduce programs, which are operation steps to answer the question. Finally, in\nthe ensemble module, we integrate multiple programs to choose the best program\nas the output of our system. In the final private test set in FinQA\nCompetition, our system obtains 69.79 execution accuracy.",
    "descriptor": "",
    "authors": [
      "Bin Wang",
      "Jiangzhou Ju",
      "Yunlin Mao",
      "Xin-Yu Dai",
      "Shujian Huang",
      "Jiajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08506"
  },
  {
    "id": "arXiv:2206.08507",
    "title": "On energy-stable and high order finite element methods for the wave  equation in heterogeneous media with perfectly matched layers",
    "abstract": "This paper presents a stable finite element approximation for the acoustic\nwave equation on second-order form, with perfectly matched layers (PML) at the\nboundaries. Energy estimates are derived for varying PML damping for both the\ndiscrete and the continuous case. Moreover, a priori error estimates are\nderived for constant PML damping. Most of the analysis is performed in Laplace\nspace. Numerical experiments in physical space validate the theoretical\nresults.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Gustav Ludvigsson",
      "Kenneth Duru",
      "Gunilla Kreiss"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08507"
  },
  {
    "id": "arXiv:2206.08509",
    "title": "Neural Architecture Adaptation for Object Detection by Searching Channel  Dimensions and Mapping Pre-trained Parameters",
    "abstract": "Most object detection frameworks use backbone architectures originally\ndesigned for image classification, conventionally with pre-trained parameters\non ImageNet. However, image classification and object detection are essentially\ndifferent tasks and there is no guarantee that the optimal backbone for\nclassification is also optimal for object detection. Recent neural architecture\nsearch (NAS) research has demonstrated that automatically designing a backbone\nspecifically for object detection helps improve the overall accuracy. In this\npaper, we introduce a neural architecture adaptation method that can optimize\nthe given backbone for detection purposes, while still allowing the use of\npre-trained parameters. We propose to adapt both the micro- and\nmacro-architecture by searching for specific operations and the number of\nlayers, in addition to the output channel dimensions of each block. It is\nimportant to find the optimal channel depth, as it greatly affects the feature\nrepresentation capability and computation cost. We conduct experiments with our\nsearched backbone for object detection and demonstrate that our backbone\noutperforms both manually designed and searched state-of-the-art backbones on\nthe COCO dataset.",
    "descriptor": "\nComments: Accepted to ICPR 2022\n",
    "authors": [
      "Harim Jung",
      "Myeong-Seok Oh",
      "Cheoljong Yang",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08509"
  },
  {
    "id": "arXiv:2206.08513",
    "title": "TLETA: Deep Transfer Learning and Integrated Cellular Knowledge for  Estimated Time of Arrival Prediction",
    "abstract": "Vehicle arrival time prediction has been studied widely. With the emergence\nof IoT devices and deep learning techniques, estimated time of arrival (ETA)\nhas become a critical component in intelligent transportation systems. Though\nmany tools exist for ETA, ETA for special vehicles, such as ambulances, fire\nengines, etc., is still challenging due to the limited amount of traffic data\nfor special vehicles. Existing works use one model for all types of vehicles,\nwhich can lead to low accuracy. To tackle this, as the first in the field, we\npropose a deep transfer learning framework TLETA for the driving time\nprediction. TLETA constructs cellular spatial-temporal knowledge grids for\nextracting driving patterns, combined with the road network structure embedding\nto build a deep neural network for ETA. TLETA contains transferable layers to\nsupport knowledge transfer between different categories of vehicles.\nImportantly, our transfer models only train the last layers to map the\ntransferred knowledge, that reduces the training time significantly. The\nexperimental studies show that our model predicts travel time with high\naccuracy and outperforms many state-of-the-art approaches.",
    "descriptor": "\nComments: 8 pages, 3 figures, 3 tables. The 25th IEEE International Conference on Intelligent Transportation Systems (IEEE ITSC 2022)\n",
    "authors": [
      "Hieu Tran",
      "Son Nguyen",
      "I-Ling Yen",
      "Farokh Bastani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.08513"
  },
  {
    "id": "arXiv:2206.08514",
    "title": "A Unified Evaluation of Textual Backdoor Learning: Frameworks and  Benchmarks",
    "abstract": "Textual backdoor attacks are a kind of practical threat to NLP systems. By\ninjecting a backdoor in the training phase, the adversary could control model\npredictions via predefined triggers. As various attack and defense models have\nbeen proposed, it is of great significance to perform rigorous evaluations.\nHowever, we highlight two issues in previous backdoor learning evaluations: (1)\nThe differences between real-world scenarios (e.g. releasing poisoned datasets\nor models) are neglected, and we argue that each scenario has its own\nconstraints and concerns, thus requires specific evaluation protocols; (2) The\nevaluation metrics only consider whether the attacks could flip the models'\npredictions on poisoned samples and retain performances on benign samples, but\nignore that poisoned samples should also be stealthy and semantic-preserving.\nTo address these issues, we categorize existing works into three practical\nscenarios in which attackers release datasets, pre-trained models, and\nfine-tuned models respectively, then discuss their unique evaluation\nmethodologies. On metrics, to completely evaluate poisoned samples, we use\ngrammar error increase and perplexity difference for stealthiness, along with\ntext similarity for validity. After formalizing the frameworks, we develop an\nopen-source toolkit OpenBackdoor to foster the implementations and evaluations\nof textual backdoor learning. With this toolkit, we perform extensive\nexperiments to benchmark attack and defense models under the suggested\nparadigm. To facilitate the underexplored defenses against poisoned datasets,\nwe further propose CUBE, a simple yet strong clustering-based defense baseline.\nWe hope that our frameworks and benchmarks could serve as the cornerstones for\nfuture model development and evaluations.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Ganqu Cui",
      "Lifan Yuan",
      "Bingxiang He",
      "Yangyi Chen",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08514"
  },
  {
    "id": "arXiv:2206.08515",
    "title": "ComENet: Towards Complete and Efficient Message Passing for 3D Molecular  Graphs",
    "abstract": "Many real-world data can be modeled as 3D graphs, but learning\nrepresentations that incorporates 3D information completely and efficiently is\nchallenging. Existing methods either use partial 3D information, or suffer from\nexcessive computational cost. To incorporate 3D information completely and\nefficiently, we propose a novel message passing scheme that operates within\n1-hop neighborhood. Our method guarantees full completeness of 3D information\non 3D graphs by achieving global and local completeness. Notably, we propose\nthe important rotation angles to fulfill global completeness. Additionally, we\nshow that our method is orders of magnitude faster than prior methods. We\nprovide rigorous proof of completeness and analysis of time complexity for our\nmethods. As molecules are in essence quantum systems, we build the\n\\underline{com}plete and \\underline{e}fficient graph neural network (ComENet)\nby combing quantum inspired basis functions and the proposed message passing\nscheme. Experimental results demonstrate the capability and efficiency of\nComENet, especially on real-world datasets that are large in both numbers and\nsizes of graphs. Our code is publicly available as part of the DIG library\n(\\url{https://github.com/divelab/DIG}).",
    "descriptor": "",
    "authors": [
      "Limei Wang",
      "Yi Liu",
      "Yuchao Lin",
      "Haoran Liu",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08515"
  },
  {
    "id": "arXiv:2206.08516",
    "title": "MetaFed: Federated Learning among Federations with Cyclic Knowledge  Distillation for Personalized Healthcare",
    "abstract": "Federated learning has attracted increasing attention to building models\nwithout accessing the raw user data, especially in healthcare. In real\napplications, different federations can seldom work together due to possible\nreasons such as data heterogeneity and distrust/inexistence of the central\nserver. In this paper, we propose a novel framework called MetaFed to\nfacilitate trustworthy FL between different federations. MetaFed obtains a\npersonalized model for each federation without a central server via the\nproposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each\nfederation as a meta distribution and aggregates knowledge of each federation\nin a cyclic manner. The training is split into two parts: common knowledge\naccumulation and personalization. Comprehensive experiments on three benchmarks\ndemonstrate that MetaFed without a server achieves better accuracy compared to\nstate-of-the-art methods (e.g., 10%+ accuracy improvement compared to the\nbaseline for PAMAP2) with fewer communication costs.",
    "descriptor": "\nComments: Accepted by IJCAI'22 federated learning workshop; code at this https URL\n",
    "authors": [
      "Yiqiang Chen",
      "Wang Lu",
      "Xin Qin",
      "Jindong Wang",
      "Xing Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08516"
  },
  {
    "id": "arXiv:2206.08517",
    "title": "Effective Solid State LiDAR Odometry Using Continuous-time Filter  Registration",
    "abstract": "Solid-state LiDARs are more compact and cheaper than the conventional\nmechanical multi-line spinning LiDARs, which have become increasingly popular\nin autonomous driving recently. However, there are several challenges for these\nnew LiDAR sensors, including severe motion distortions, small field of view and\nsparse point cloud, which hinder them from being widely used in LiDAR odometry.\nTo tackle these problems, we present an effective continuous-time LiDAR\nodometry (ECTLO) method for the Risley prism-based LiDARs with non-repetitive\nscanning patterns. To account for the noisy data, a filter-based point-to-plane\nGaussian Mixture Model is used for robust registration. Moreover, a LiDAR-only\ncontinuous-time motion model is employed to relieve the inevitable distortions.\nTo facilitate the implicit data association in parallel, we maintain all map\npoints within a single range image. Extensive experiments have been conducted\non various testbeds using the solid-state LiDARs with different scanning\npatterns, whose promising results demonstrate the efficacy of our proposed\napproach.",
    "descriptor": "\nComments: 8 pages, 6 figures\n",
    "authors": [
      "Xin Zheng",
      "Jianke Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08517"
  },
  {
    "id": "arXiv:2206.08520",
    "title": "Thompson Sampling Achieves $\\tilde O(\\sqrt{T})$ Regret in Linear  Quadratic Control",
    "abstract": "Thompson Sampling (TS) is an efficient method for decision-making under\nuncertainty, where an action is sampled from a carefully prescribed\ndistribution which is updated based on the observed data. In this work, we\nstudy the problem of adaptive control of stabilizable linear-quadratic\nregulators (LQRs) using TS, where the system dynamics are unknown. Previous\nworks have established that $\\tilde O(\\sqrt{T})$ frequentist regret is optimal\nfor the adaptive control of LQRs. However, the existing methods either work\nonly in restrictive settings, require a priori known stabilizing controllers,\nor utilize computationally intractable approaches. We propose an efficient TS\nalgorithm for the adaptive control of LQRs, TS-based Adaptive Control, TSAC,\nthat attains $\\tilde O(\\sqrt{T})$ regret, even for multidimensional systems,\nthereby solving the open problem posed in Abeille and Lazaric (2018). TSAC does\nnot require a priori known stabilizing controller and achieves fast\nstabilization of the underlying system by effectively exploring the environment\nin the early stages. Our result hinges on developing a novel lower bound on the\nprobability that the TS provides an optimistic sample. By carefully prescribing\nan early exploration strategy and a policy update rule, we show that TS\nachieves order-optimal regret in adaptive control of multidimensional\nstabilizable LQRs. We empirically demonstrate the performance and the\nefficiency of TSAC in several adaptive control tasks.",
    "descriptor": "\nComments: Accepted for presentation at the Conference on Learning Theory (COLT) 2022\n",
    "authors": [
      "Taylan Kargin",
      "Sahin Lale",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar",
      "Babak Hassibi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08520"
  },
  {
    "id": "arXiv:2206.08521",
    "title": "CLEAR: A Fully User-side Image Search System",
    "abstract": "We use many search engines on the Internet in our daily lives. However, they\nare not perfect. Their scoring function may not model our intent or they may\naccept only text queries even though we want to carry out a similar image\nsearch. In such cases, we need to make a compromise: We continue to use the\nunsatisfactory service or leave the service. Recently, a new solution,\nuser-side search systems, has been proposed. In this framework, each user\nbuilds their own search system that meets their preference with a user-defined\nscoring function and user-defined interface. Although the concept is appealing,\nit is still not clear if this approach is feasible in practice. In this\ndemonstration, we show the first fully user-side image search system, CLEAR,\nwhich realizes a similar-image search engine for Flickr. The challenge is that\nFlickr does not provide an official similar image search engine or\ncorresponding API. Nevertheless, CLEAR realizes it fully on a user-side. CLEAR\ndoes not use a backend server at all nor store any images or build search\nindices. It is in contrast to traditional search algorithms that require\npreparing a backend server and building a search index. Therefore, each user\ncan easily deploy their own CLEAR engine, and the resulting service is\ncustom-made and privacy-preserving. The online demo is available at\nhttps://clear.joisino.net. The source code is available at\nhttps://github.com/joisino/clear.",
    "descriptor": "",
    "authors": [
      "Ryoma Sato"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.08521"
  },
  {
    "id": "arXiv:2206.08522",
    "title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation",
    "abstract": "Benefiting from language flexibility and compositionality, humans naturally\nintend to use language to command an embodied agent for complex tasks such as\nnavigation and object manipulation. In this work, we aim to fill the blank of\nthe last mile of embodied agents -- object manipulation by following human\nguidance, e.g., \"move the red mug next to the box while keeping it upright.\" To\nthis end, we introduce an Automatic Manipulation Solver (AMSolver) simulator\nand build a Vision-and-Language Manipulation benchmark (VLMbench) based on it,\ncontaining various language instructions on categorized robotic manipulation\ntasks. Specifically, modular rule-based task templates are created to\nautomatically generate robot demonstrations with language instructions,\nconsisting of diverse object shapes and appearances, action types, and motion\nconstraints. We also develop a keypoint-based model 6D-CLIPort to deal with\nmulti-view observations and language input and output a sequence of 6 degrees\nof freedom (DoF) actions. We hope the new simulator and benchmark will\nfacilitate future research on language-guided robotic manipulation.",
    "descriptor": "",
    "authors": [
      "Kaizhi Zheng",
      "Xiaotong Chen",
      "Odest Chadwicke Jenkins",
      "Xin Eric Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08522"
  },
  {
    "id": "arXiv:2206.08523",
    "title": "A Spatio-Temporal Neural Network Forecasting Approach for Emulation of  Firefront Models",
    "abstract": "Computational simulations of wildfire spread typically employ empirical\nrate-of-spread calculations under various conditions (such as terrain, fuel\ntype, weather). Small perturbations in conditions can often lead to significant\nchanges in fire spread (such as speed and direction), necessitating a\ncomputationally expensive large set of simulations to quantify uncertainty.\nModel emulation seeks alternative representations of physical models using\nmachine learning, aiming to provide more efficient and/or simplified surrogate\nmodels. We propose a dedicated spatio-temporal neural network based framework\nfor model emulation, able to capture the complex behaviour of fire spread\nmodels. The proposed approach can approximate forecasts at fine spatial and\ntemporal resolutions that are often challenging for neural network based\napproaches. Furthermore, the proposed approach is robust even with small\ntraining sets, due to novel data augmentation methods. Empirical experiments\nshow good agreement between simulated and emulated firefronts, with an average\nJaccard score of 0.76.",
    "descriptor": "",
    "authors": [
      "Andrew Bolt",
      "Carolyn Huston",
      "Petra Kuhnert",
      "Joel Janek Dabrowski",
      "James Hilton",
      "Conrad Sanderson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08523"
  },
  {
    "id": "arXiv:2206.08524",
    "title": "CDNet: Contrastive Disentangled Network for Fine-Grained Image  Categorization of Ocular B-Scan Ultrasound",
    "abstract": "Precise and rapid categorization of images in the B-scan ultrasound modality\nis vital for diagnosing ocular diseases. Nevertheless, distinguishing various\ndiseases in ultrasound still challenges experienced ophthalmologists. Thus a\nnovel contrastive disentangled network (CDNet) is developed in this work,\naiming to tackle the fine-grained image categorization (FGIC) challenges of\nocular abnormalities in ultrasound images, including intraocular tumor (IOT),\nretinal detachment (RD), posterior scleral staphyloma (PSS), and vitreous\nhemorrhage (VH). Three essential components of CDNet are the weakly-supervised\nlesion localization module (WSLL), contrastive multi-zoom (CMZ) strategy, and\nhyperspherical contrastive disentangled loss (HCD-Loss), respectively. These\ncomponents facilitate feature disentanglement for fine-grained recognition in\nboth the input and output aspects. The proposed CDNet is validated on our ZJU\nOcular Ultrasound Dataset (ZJUOUSD), consisting of 5213 samples. Furthermore,\nthe generalization ability of CDNet is validated on two public and widely-used\nchest X-ray FGIC benchmarks. Quantitative and qualitative results demonstrate\nthe efficacy of our proposed CDNet, which achieves state-of-the-art performance\nin the FGIC task. Code is available at:\nhttps://github.com/ZeroOneGame/CDNet-for-OUS-FGIC .",
    "descriptor": "",
    "authors": [
      "Ruilong Dan",
      "Yunxiang Li",
      "Yijie Wang",
      "Gangyong Jia",
      "Ruiquan Ge",
      "Juan Ye",
      "Qun Jin",
      "Yaqi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08524"
  },
  {
    "id": "arXiv:2206.08526",
    "title": "k-Sliced Mutual Information: A Quantitative Study of Scalability with  Dimension",
    "abstract": "Sliced mutual information (SMI) is defined as an average of mutual\ninformation (MI) terms between one-dimensional random projections of the random\nvariables. It serves as a surrogate measure of dependence to classic MI that\npreserves many of its properties but is more scalable to high dimensions.\nHowever, a quantitative characterization of how SMI itself and estimation rates\nthereof depend on the ambient dimension, which is crucial to the understanding\nof scalability, remain obscure. This works extends the original SMI definition\nto $k$-SMI, which considers projections to $k$-dimensional subspaces, and\nprovides a multifaceted account on its dependence on dimension. Using a new\nresult on the continuity of differential entropy in the 2-Wasserstein metric,\nwe derive sharp bounds on the error of Monte Carlo (MC)-based estimates of\n$k$-SMI, with explicit dependence on $k$ and the ambient dimension, revealing\ntheir interplay with the number of samples. We then combine the MC integrator\nwith the neural estimation framework to provide an end-to-end $k$-SMI\nestimator, for which optimal convergence rates are established. We also explore\nasymptotics of the population $k$-SMI as dimension grows, providing Gaussian\napproximation results with a residual that decays under appropriate moment\nbounds. Our theory is validated with numerical experiments and is applied to\nsliced InfoGAN, which altogether provide a comprehensive quantitative account\nof the scalability question of $k$-SMI, including SMI as a special case when\n$k=1$.",
    "descriptor": "",
    "authors": [
      "Ziv Goldfeld",
      "Kristjan Greenewald",
      "Theshani Nuradha",
      "Galen Reeves"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08526"
  },
  {
    "id": "arXiv:2206.08528",
    "title": "SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe  Autonomous Driving",
    "abstract": "Safe reinforcement learning (RL) has achieved significant success on\nrisk-sensitive tasks and shown promise in autonomous driving (AD) as well.\nConsidering the distinctiveness of this community, efficient and reproducible\nbaselines are still lacking for safe AD. In this paper, we release SafeRL-Kit\nto benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit\ncontains several latest algorithms specific to zero-constraint-violation tasks,\nincluding Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible\nActor-Critic. In addition to existing approaches, we propose a novel\nfirst-order method named Exact Penalty Optimization (EPO) and sufficiently\ndemonstrate its capability in safe AD. All algorithms in SafeRL-Kit are\nimplemented (i) under the off-policy setting, which improves sample efficiency\nand can better leverage past logs; (ii) with a unified learning framework,\nproviding off-the-shelf interfaces for researchers to incorporate their\ndomain-specific knowledge into fundamental safe RL methods. Conclusively, we\nconduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed\nlight on their efficacy for safe autonomous driving. The source code is\navailable at \\href{ https://github.com/zlr20/saferl_kit}{this https URL}.",
    "descriptor": "\nComments: The 1st Workshop on Safe Learning for Autonomous Driving (SL4AD) with ICML 2022\n",
    "authors": [
      "Linrui Zhang",
      "Qin Zhang",
      "Li Shen",
      "Bo Yuan",
      "Xueqian Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08528"
  },
  {
    "id": "arXiv:2206.08529",
    "title": "Accelerating Shapley Explanation via Contributive Cooperator Selection",
    "abstract": "Even though Shapley value provides an effective explanation for a DNN model\nprediction, the computation relies on the enumeration of all possible input\nfeature coalitions, which leads to the exponentially growing complexity. To\naddress this problem, we propose a novel method SHEAR to significantly\naccelerate the Shapley explanation for DNN models, where only a few coalitions\nof input features are involved in the computation. The selection of the feature\ncoalitions follows our proposed Shapley chain rule to minimize the absolute\nerror from the ground-truth Shapley values, such that the computation can be\nboth efficient and accurate. To demonstrate the effectiveness, we\ncomprehensively evaluate SHEAR across multiple metrics including the absolute\nerror from the ground-truth Shapley value, the faithfulness of the\nexplanations, and running speed. The experimental results indicate SHEAR\nconsistently outperforms state-of-the-art baseline methods across different\nevaluation metrics, which demonstrates its potentials in real-world\napplications where the computational resource is limited.",
    "descriptor": "",
    "authors": [
      "Guanchu Wang",
      "Yu-Neng Chuang",
      "Mengnan Du",
      "Fan Yang",
      "Quan Zhou",
      "Pushkar Tripathi",
      "Xuanting Cai",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.08529"
  },
  {
    "id": "arXiv:2206.08530",
    "title": "GDsmith: Detecting Bugs in Graph Database Engines",
    "abstract": "Graph database engines stand out in the era of big data for their efficiency\nof modeling and processing linked data. There is a strong need of testing graph\ndatabase engines. However, random testing, the most practical way of automated\ntest generation, faces the challenges of semantic validity, non-empty result,\nand behavior diversity to detect bugs in graph database engines. To address\nthese challenges, in this paper, we propose GDsmith, the first black-box\napproach for testing graph database engines. It ensures that each randomly\ngenerated Cypher query satisfies the semantic requirements via skeleton\ngeneration and completion. GDsmith includes our technique to increase the\nprobability of producing Cypher queries that return non-empty results by\nleveraging three types of structural mutation strategies. GDsmith also includes\nour technique to improve the behavior diversity of the generated Cypher queries\nby selecting property keys according to their previous frequencies when\ngenerating new queries. Our evaluation results demonstrate that GDsmith is\neffective and efficient for automated query generation and substantially\noutperforms the baseline. GDsmith successfully detects 27 previously unknown\nbugs on the released versions of three popular open-source graph database\nengines and receive positive feedback from their developers.",
    "descriptor": "",
    "authors": [
      "Wei Lin",
      "Ziyue Hua",
      "Luyao Ren",
      "Zongyang Li",
      "Lu Zhang",
      "Tao Xie"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.08530"
  },
  {
    "id": "arXiv:2206.08536",
    "title": "Low-latency Mini-batch GNN Inference on CPU-FPGA Heterogeneous Platform",
    "abstract": "Mini-batch inference of Graph Neural Networks (GNNs) is a key problem in many\nreal-world applications. Recently, a GNN design principle of model\ndepth-receptive field decoupling has been proposed to address the well-known\nissue of neighborhood explosion. Decoupled GNN models achieve higher accuracy\nthan original models and demonstrate excellent scalability for mini-batch\ninference.\nWe map Decoupled GNNs onto CPU-FPGA heterogeneous platforms to achieve\nlow-latency mini-batch inference. On the FPGA platform, we design a novel GNN\nhardware accelerator with an adaptive datapath denoted Adaptive Computation\nKernel (ACK) that can execute various computation kernels of GNNs with\nlow-latency: (1) for dense computation kernels expressed as matrix\nmultiplication, ACK works as a systolic array with fully localized connections,\n(2) for sparse computation kernels, ACK follows the scatter-gather paradigm and\nworks as multiple parallel pipelines to support the irregular connectivity of\ngraphs. The proposed task scheduling hides the CPU-FPGA data communication\noverhead to reduce the inference latency. We develop a fast design space\nexploration algorithm to generate a single accelerator for multiple target GNN\nmodels. We implement our accelerator on a state-of-the-art CPU-FPGA platform\nand evaluate the performance using three representative models (GCN, GraphSAGE,\nand GAT). Results show that our CPU-FPGA implementation achieves\n$21.4-50.8\\times$, $2.9-21.6\\times$, $4.7\\times$ latency reduction compared\nwith state-of-the-art implementations on CPU-only, CPU-GPU and CPU-FPGA\nplatforms.",
    "descriptor": "",
    "authors": [
      "Bingyi Zhang",
      "Hanqing Zeng",
      "Viktor Prasanna"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.08536"
  },
  {
    "id": "arXiv:2206.08537",
    "title": "Large-Margin Representation Learning for Texture Classification",
    "abstract": "This paper presents a novel approach combining convolutional layers (CLs) and\nlarge-margin metric learning for training supervised models on small datasets\nfor texture classification. The core of such an approach is a loss function\nthat computes the distances between instances of interest and support vectors.\nThe objective is to update the weights of CLs iteratively to learn a\nrepresentation with a large margin between classes. Each iteration results in a\nlarge-margin discriminant model represented by support vectors based on such a\nrepresentation. The advantage of the proposed approach w.r.t. convolutional\nneural networks (CNNs) is two-fold. First, it allows representation learning\nwith a small amount of data due to the reduced number of parameters compared to\nan equivalent CNN. Second, it has a low training cost since the backpropagation\nconsiders only support vectors. The experimental results on texture and\nhistopathologic image datasets have shown that the proposed approach achieves\ncompetitive accuracy with lower computational cost and faster convergence when\ncompared to equivalent CNNs.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Jonathan de Matos",
      "Luiz Eduardo Soares de Oliveira",
      "Alceu de Souza Britto Junior",
      "Alessandro Lameiras Koerich"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08537"
  },
  {
    "id": "arXiv:2206.08542",
    "title": "Strategic Representation",
    "abstract": "Humans have come to rely on machines for reducing excessive information to\nmanageable representations. But this reliance can be abused -- strategic\nmachines might craft representations that manipulate their users. How can a\nuser make good choices based on strategic representations? We formalize this as\na learning problem, and pursue algorithms for decision-making that are robust\nto manipulation. In our main setting of interest, the system represents\nattributes of an item to the user, who then decides whether or not to consume.\nWe model this interaction through the lens of strategic classification (Hardt\net al. 2016), reversed: the user, who learns, plays first; and the system,\nwhich responds, plays second. The system must respond with representations that\nreveal `nothing but the truth' but need not reveal the entire truth. Thus, the\nuser faces the problem of learning set functions under strategic subset\nselection, which presents distinct algorithmic and statistical challenges. Our\nmain result is a learning algorithm that minimizes error despite strategic\nrepresentations, and our theoretical analysis sheds light on the trade-off\nbetween learning effort and susceptibility to manipulation.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Vineet Nair",
      "Ganesh Ghalme",
      "Inbal Talgam-Cohen",
      "Nir Rosenfeld"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.08542"
  },
  {
    "id": "arXiv:2206.08544",
    "title": "Bio-inspired Intelligence with Applications to Robotics: A Survey",
    "abstract": "In the past decades, considerable attention has been paid to bio-inspired\nintelligence and its applications to robotics. This paper provides a\ncomprehensive survey of bio-inspired intelligence, with a focus on\nneurodynamics approaches, to various robotic applications, particularly to path\nplanning and control of autonomous robotic systems. Firstly, the bio-inspired\nshunting model and its variants (additive model and gated dipole model) are\nintroduced, and their main characteristics are given in detail. Then, two main\nneurodynamics applications to real-time path planning and control of various\nrobotic systems are reviewed. A bio-inspired neural network framework, in which\nneurons are characterized by the neurodynamics models, is discussed for mobile\nrobots, cleaning robots, and underwater robots. The bio-inspired neural network\nhas been widely used in real-time collision-free navigation and cooperation\nwithout any learning procedures, global cost functions, and prior knowledge of\nthe dynamic environment. In addition, bio-inspired backstepping controllers for\nvarious robotic systems, which are able to eliminate the speed jump when a\nlarge initial tracking error occurs, are further discussed. Finally, the\ncurrent challenges and future research directions are discussed in this paper.",
    "descriptor": "",
    "authors": [
      "Junfei Li",
      "Zhe Xu",
      "Danjie Zhu",
      "Kevin Dong",
      "Tao Yan",
      "Zhu Zeng",
      "Simon X. Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08544"
  },
  {
    "id": "arXiv:2206.08547",
    "title": "Texture Generation Using Graph Generative Adversarial Network And  Differentiable Rendering",
    "abstract": "Novel texture synthesis for existing 3D mesh models is an important step\ntowards photo realistic asset generation for existing simulators. But existing\nmethods inherently work in the 2D image space which is the projection of the 3D\nspace from a given camera perspective. These methods take camera angle, 3D\nmodel information, lighting information and generate photorealistic 2D image.\nTo generate a photorealistic image from another perspective or lighting, we\nneed to make a computationally expensive forward pass each time we change the\nparameters. Also, it is hard to generate such images for a simulator that can\nsatisfy the temporal constraints the sequences of images should be similar but\nonly need to change the viewpoint of lighting as desired. The solution can not\nbe directly integrated with existing tools like Blender and Unreal Engine.\nManual solution is expensive and time consuming. We thus present a new system\ncalled a graph generative adversarial network (GGAN) that can generate textures\nwhich can be directly integrated into a given 3D mesh models with tools like\nBlender and Unreal Engine and can be simulated from any perspective and\nlighting condition easily.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Dharma KC",
      "Clayton T. Morrison",
      "Bradley Walls"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08547"
  },
  {
    "id": "arXiv:2206.08549",
    "title": "Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized  Images",
    "abstract": "Evaluation metrics in image synthesis play a key role to measure performances\nof generative models. However, most metrics mainly focus on image fidelity.\nExisting diversity metrics are derived by comparing distributions, and thus\nthey cannot quantify the diversity or rarity degree of each generated image. In\nthis work, we propose a new evaluation metric, called `rarity score', to\nmeasure the individual rarity of each image synthesized by generative models.\nWe first show empirical observation that common samples are close to each other\nand rare samples are far from each other in nearest-neighbor distances of\nfeature space. We then use our metric to demonstrate that the extent to which\ndifferent generative models produce rare images can be effectively compared. We\nalso propose a method to compare rarities between datasets that share the same\nconcept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in\ndifferent designs of feature spaces to better understand the relationship\nbetween feature spaces and resulting sparse images. Code will be publicly\navailable online for the research community.",
    "descriptor": "",
    "authors": [
      "Jiyeon Han",
      "Hwanil Choi",
      "Yunjey Choi",
      "Junho Kim",
      "Jung-Woo Ha",
      "Jaesik Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08549"
  },
  {
    "id": "arXiv:2206.08555",
    "title": "SOS: Score-based Oversampling for Tabular Data",
    "abstract": "Score-based generative models (SGMs) are a recent breakthrough in generating\nfake images. SGMs are known to surpass other generative models, e.g.,\ngenerative adversarial networks (GANs) and variational autoencoders (VAEs).\nBeing inspired by their big success, in this work, we fully customize them for\ngenerating fake tabular data. In particular, we are interested in oversampling\nminor classes since imbalanced classes frequently lead to sub-optimal training\noutcomes. To our knowledge, we are the first presenting a score-based tabular\ndata oversampling method. Firstly, we re-design our own score network since we\nhave to process tabular data. Secondly, we propose two options for our\ngeneration method: the former is equivalent to a style transfer for tabular\ndata and the latter uses the standard generative policy of SGMs. Lastly, we\ndefine a fine-tuning method, which further enhances the oversampling quality.\nIn our experiments with 6 datasets and 10 baselines, our method outperforms\nother oversampling methods in all cases.",
    "descriptor": "\nComments: Accepted by KDD 2022\n",
    "authors": [
      "Jayoung Kim",
      "Chaejeong Lee",
      "Yehjin Shin",
      "Sewon Park",
      "Minjung Kim",
      "Noseong Park",
      "Jihoon Cho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08555"
  },
  {
    "id": "arXiv:2206.08556",
    "title": "Thompson Sampling for Robust Transfer in Multi-Task Bandits",
    "abstract": "We study the problem of online multi-task learning where the tasks are\nperformed within similar but not necessarily identical multi-armed bandit\nenvironments. In particular, we study how a learner can improve its overall\nperformance across multiple related tasks through robust transfer of knowledge.\nWhile an upper confidence bound (UCB)-based algorithm has recently been shown\nto achieve nearly-optimal performance guarantees in a setting where all tasks\nare solved concurrently, it remains unclear whether Thompson sampling (TS)\nalgorithms, which have superior empirical performance in general, share similar\ntheoretical properties. In this work, we present a TS-type algorithm for a more\ngeneral online multi-task learning protocol, which extends the concurrent\nsetting. We provide its frequentist analysis and prove that it is also\nnearly-optimal using a novel concentration inequality for multi-task data\naggregation at random stopping times. Finally, we evaluate the algorithm on\nsynthetic data and show that the TS-type algorithm enjoys superior empirical\nperformance in comparison with the UCB-based algorithm and a baseline algorithm\nthat performs TS for each individual task without transfer.",
    "descriptor": "\nComments: To appear in Proceedings of the 39th International Conference on Machine Learning (ICML-2022)\n",
    "authors": [
      "Zhi Wang",
      "Chicheng Zhang",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08556"
  },
  {
    "id": "arXiv:2206.08558",
    "title": "How You Start Matters for Generalization",
    "abstract": "Characterizing the remarkable generalization properties of over-parameterized\nneural networks remains an open problem. In this paper, we promote a shift of\nfocus towards initialization rather than neural architecture or (stochastic)\ngradient descent to explain this implicit regularization. Through a Fourier\nlens, we derive a general result for the spectral bias of neural networks and\nshow that the generalization of neural networks is heavily tied to their\ninitialization. Further, we empirically solidify the developed theoretical\ninsights using practical, deep networks. Finally, we make a case against the\ncontroversial flat-minima conjecture and show that Fourier analysis grants a\nmore reliable framework for understanding the generalization of neural\nnetworks.",
    "descriptor": "",
    "authors": [
      "Sameera Ramasinghe",
      "Lachlan MacDonald",
      "Moshiur Farazi",
      "Hemanth Sartachandran",
      "Simon Lucey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08558"
  },
  {
    "id": "arXiv:2206.08561",
    "title": "Boosting Graph Structure Learning with Dummy Nodes",
    "abstract": "With the development of graph kernels and graph representation learning, many\nsuperior methods have been proposed to handle scalability and oversmoothing\nissues on graph structure learning. However, most of those strategies are\ndesigned based on practical experience rather than theoretical analysis. In\nthis paper, we use a particular dummy node connecting to all existing vertices\nwithout affecting original vertex and edge properties. We further prove that\nsuch the dummy node can help build an efficient monomorphic edge-to-vertex\ntransform and an epimorphic inverse to recover the original graph back. It also\nindicates that adding dummy nodes can preserve local and global structures for\nbetter graph representation learning. We extend graph kernels and graph neural\nnetworks with dummy nodes and conduct experiments on graph classification and\nsubgraph isomorphism matching tasks. Empirical results demonstrate that taking\ngraphs with dummy nodes as input significantly boosts graph structure learning,\nand using their edge-to-vertex graphs can also achieve similar results. We also\ndiscuss the gain of expressive power from the dummy in neural networks.",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Xin Liu",
      "Jiayang Cheng",
      "Yangqiu Song",
      "Xin Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08561"
  },
  {
    "id": "arXiv:2206.08564",
    "title": "MET: Masked Encoding for Tabular Data",
    "abstract": "We consider the task of self-supervised representation learning (SSL) for\ntabular data: tabular-SSL. Typical contrastive learning based SSL methods\nrequire instance-wise data augmentations which are difficult to design for\nunstructured tabular data. Existing tabular-SSL methods design such\naugmentations in a relatively ad-hoc fashion and can fail to capture the\nunderlying data manifold. Instead of augmentations based approaches for\ntabular-SSL, we propose a new reconstruction based method, called Masked\nEncoding for Tabular Data (MET), that does not require augmentations. MET is\nbased on the popular MAE approach for vision-SSL [He et al., 2021] and uses two\nkey ideas: (i) since each coordinate in a tabular dataset has a distinct\nmeaning, we need to use separate representations for all coordinates, and (ii)\nusing an adversarial reconstruction loss in addition to the standard one.\nEmpirical results on five diverse tabular datasets show that MET achieves a new\nstate of the art (SOTA) on all of these datasets and improves up to 9% over\ncurrent SOTA methods. We shed more light on the working of MET via experiments\non carefully designed simple datasets.",
    "descriptor": "\nComments: Under Review, 18 pages, 6 Tables, 4 Figures\n",
    "authors": [
      "Kushal Majmundar",
      "Sachin Goyal",
      "Praneeth Netrapalli",
      "Prateek Jain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08564"
  },
  {
    "id": "arXiv:2206.08565",
    "title": "Identifying Counterfeit Products using Blockchain Technology in Supply  Chain System",
    "abstract": "With the advent of globalization and the evergrowing rate of technology, the\nvolume of production as well as ease of procuring counterfeit goods has become\nunprecedented. Be it food, drug or luxury items, all kinds of industrial\nmanufacturers and distributors are now seeking greater transparency in supply\nchain operations with a view to deter counterfeiting. This paper introduces a\ndecentralized Blockchain based application system (DApp) with a view to\nidentifying counterfeit products in the supply chain system. With the rapid\nrise of Blockchain technology, it has become known that data recorded within\nBlockchain is immutable and secure. Hence, the proposed project here uses this\nconcept to handle the transfer of ownership of products. A consumer can verify\nthe product distribution and ownership information scanning a Quick Response\n(QR) code generated by the DApp for each product linked to the Blockchain.",
    "descriptor": "\nComments: 5 pages, 4 figures, 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)\n",
    "authors": [
      "Nafisa Anjum",
      "Pramit Dutta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08565"
  },
  {
    "id": "arXiv:2206.08566",
    "title": "Active Data Discovery: Mining Unknown Data using Submodular Information  Measures",
    "abstract": "Active Learning is a very common yet powerful framework for iteratively and\nadaptively sampling subsets of the unlabeled sets with a human in the loop with\nthe goal of achieving labeling efficiency. Most real world datasets have\nimbalance either in classes and slices, and correspondingly, parts of the\ndataset are rare. As a result, there has been a lot of work in designing active\nlearning approaches for mining these rare data instances. Most approaches\nassume access to a seed set of instances which contain these rare data\ninstances. However, in the event of more extreme rareness, it is reasonable to\nassume that these rare data instances (either classes or slices) may not even\nbe present in the seed labeled set, and a critical need for the active learning\nparadigm is to efficiently discover these rare data instances. In this work, we\nprovide an active data discovery framework which can mine unknown data slices\nand classes efficiently using the submodular conditional gain and submodular\nconditional mutual information functions. We provide a general algorithmic\nframework which works in a number of scenarios including image classification\nand object detection and works with both rare classes and rare slices present\nin the unlabeled set. We show significant accuracy and labeling efficiency\ngains with our approach compared to existing state-of-the-art active learning\napproaches for actively discovering these rare classes and slices.",
    "descriptor": "",
    "authors": [
      "Suraj Kothawade",
      "Shivang Chopra",
      "Saikat Ghosh",
      "Rishabh Iyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08566"
  },
  {
    "id": "arXiv:2206.08567",
    "title": "Rectify ViT Shortcut Learning by Visual Saliency",
    "abstract": "Shortcut learning is common but harmful to deep learning models, leading to\ndegenerated feature representations and consequently jeopardizing the model's\ngeneralizability and interpretability. However, shortcut learning in the widely\nused Vision Transformer framework is largely unknown. Meanwhile, introducing\ndomain-specific knowledge is a major approach to rectifying the shortcuts,\nwhich are predominated by background related factors. For example, in the\nmedical imaging field, eye-gaze data from radiologists is an effective human\nvisual prior knowledge that has the great potential to guide the deep learning\nmodels to focus on meaningful foreground regions of interest. However,\nobtaining eye-gaze data is time-consuming, labor-intensive and sometimes even\nnot practical. In this work, we propose a novel and effective saliency-guided\nvision transformer (SGT) model to rectify shortcut learning in ViT with the\nabsence of eye-gaze data. Specifically, a computational visual saliency model\nis adopted to predict saliency maps for input image samples. Then, the saliency\nmaps are used to distil the most informative image patches. In the proposed\nSGT, the self-attention among image patches focus only on the distilled\ninformative ones. Considering this distill operation may lead to global\ninformation lost, we further introduce, in the last encoder layer, a residual\nconnection that captures the self-attention across all the image patches. The\nexperiment results on four independent public datasets show that our SGT\nframework can effectively learn and leverage human prior knowledge without eye\ngaze data and achieves much better performance than baselines. Meanwhile, it\nsuccessfully rectifies the harmful shortcut learning and significantly improves\nthe interpretability of the ViT model, demonstrating the promise of\ntransferring human prior knowledge derived visual saliency in rectifying\nshortcut learning",
    "descriptor": "\nComments: NeurIPS2022 Under Review\n",
    "authors": [
      "Chong Ma",
      "Lin Zhao",
      "Yuzhong Chen",
      "David Weizhong Liu",
      "Xi Jiang",
      "Tuo Zhang",
      "Xintao Hu",
      "Dinggang Shen",
      "Dajiang Zhu",
      "Tianming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08567"
  },
  {
    "id": "arXiv:2206.08568",
    "title": "Multi-Contextual Predictions with Vision Transformer for Video Anomaly  Detection",
    "abstract": "Video Anomaly Detection(VAD) has been traditionally tackled in two main\nmethodologies: the reconstruction-based approach and the prediction-based one.\nAs the reconstruction-based methods learn to generalize the input image, the\nmodel merely learns an identity function and strongly causes the problem called\ngeneralizing issue. On the other hand, since the prediction-based ones learn to\npredict a future frame given several previous frames, they are less sensitive\nto the generalizing issue. However, it is still uncertain if the model can\nlearn the spatio-temporal context of a video. Our intuition is that the\nunderstanding of the spatio-temporal context of a video plays a vital role in\nVAD as it provides precise information on how the appearance of an event in a\nvideo clip changes. Hence, to fully exploit the context information for anomaly\ndetection in video circumstances, we designed the transformer model with three\ndifferent contextual prediction streams: masked, whole and partial. By learning\nto predict the missing frames of consecutive normal frames, our model can\neffectively learn various normality patterns in the video, which leads to a\nhigh reconstruction error at the abnormal cases that are unsuitable to the\nlearned context. To verify the effectiveness of our approach, we assess our\nmodel on the public benchmark datasets: USCD Pedestrian 2, CUHK Avenue and\nShanghaiTech and evaluate the performance with the anomaly score metric of\nreconstruction error. The results demonstrate that our proposed approach\nachieves a competitive performance compared to the existing video anomaly\ndetection methods.",
    "descriptor": "",
    "authors": [
      "Joo-Yeon Lee",
      "Woo-Jeoung Nam",
      "Seong-Whan Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08568"
  },
  {
    "id": "arXiv:2206.08569",
    "title": "Bootstrapped Transformer for Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning (RL) aims at learning policies from previously\ncollected static trajectory data without interacting with the real environment.\nRecent works provide a novel perspective by viewing offline RL as a generic\nsequence generation problem, adopting sequence models such as Transformer\narchitecture to model distributions over trajectories, and repurposing beam\nsearch as a planning algorithm. However, the training datasets utilized in\ngeneral offline RL tasks are quite limited and often suffer from insufficient\ndistribution coverage, which could be harmful to training sequence generation\nmodels yet has not drawn enough attention in the previous works. In this paper,\nwe propose a novel algorithm named Bootstrapped Transformer, which incorporates\nthe idea of bootstrapping and leverages the learned model to self-generate more\noffline data to further boost the sequence model training. We conduct extensive\nexperiments on two offline RL benchmarks and demonstrate that our model can\nlargely remedy the existing offline RL training limitations and beat other\nstrong baseline methods. We also analyze the generated pseudo data and the\nrevealed characteristics may shed some light on offline RL training. The codes\nare available at https://seqml.github.io/bootorl.",
    "descriptor": "\nComments: A complete manuscript under review\n",
    "authors": [
      "Kerong Wang",
      "Hanye Zhao",
      "Xufang Luo",
      "Kan Ren",
      "Weinan Zhang",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08569"
  },
  {
    "id": "arXiv:2206.08570",
    "title": "Event-triggered Design for Optimal Output Consensus of High-order  Multi-agent Systems",
    "abstract": "This paper studies the optimal output consensus problem for a group of\nheterogeneous linear multi-agent systems. Different from existing results, we\naim at effective controllers for these high-order agents under both\nevent-triggered control and event-triggered communication settings. We conduct\nan embedded design for the problem and constructively propose a multi-rate\nevent-triggered controller with a set of applicable parameters. The proposed\nevent-triggered rules are shown to be free of Zeno behaviors and can achieve\nthe optimal output consensus goal for these high-order agents. A simulation\nexample is given to verify the efficacy of our designs.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Yutao Tang",
      "Huaihui Liu",
      "Ruonan Li",
      "Kui Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08570"
  },
  {
    "id": "arXiv:2206.08572",
    "title": "Enhanced Bi-directional Motion Estimation for Video Frame Interpolation",
    "abstract": "We present a novel simple yet effective algorithm for motion-based video\nframe interpolation. Existing motion-based interpolation methods typically rely\non a pre-trained optical flow model or a U-Net based pyramid network for motion\nestimation, which either suffer from large model size or limited capacity in\nhandling complex and large motion cases. In this work, by carefully integrating\nintermediateoriented forward-warping, lightweight feature encoder, and\ncorrelation volume into a pyramid recurrent framework, we derive a compact\nmodel to simultaneously estimate the bidirectional motion between input frames.\nIt is 15 times smaller in size than PWC-Net, yet enables more reliable and\nflexible handling of challenging motion cases. Based on estimated\nbi-directional motion, we forward-warp input frames and their context features\nto intermediate frame, and employ a synthesis network to estimate the\nintermediate frame from warped representations. Our method achieves excellent\nperformance on a broad range of video frame interpolation benchmarks. Code will\nbe available soon.",
    "descriptor": "",
    "authors": [
      "Jin Xin",
      "Wu Longhai",
      "Shen Guotao",
      "Chen Youxin",
      "Chen Jie",
      "Koo Jayoon",
      "Hahm Cheul-hee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08572"
  },
  {
    "id": "arXiv:2206.08574",
    "title": "Using Transfer Learning for Code-Related Tasks",
    "abstract": "Deep learning (DL) techniques have been used to support several code-related\ntasks such as code summarization and bug-fixing. In particular, pre-trained\ntransformer models are on the rise, also thanks to the excellent results they\nachieved in Natural Language Processing (NLP) tasks. The basic idea behind\nthese models is to first pre-train them on a generic dataset using a\nself-supervised task (e.g, filling masked words in sentences). Then, these\nmodels are fine-tuned to support specific tasks of interest (e.g, language\ntranslation). A single model can be fine-tuned to support multiple tasks,\npossibly exploiting the benefits of transfer learning. This means that\nknowledge acquired to solve a specific task (e.g, language translation) can be\nuseful to boost performance on another task (e.g, sentiment classification).\nWhile the benefits of transfer learning have been widely studied in NLP,\nlimited empirical evidence is available when it comes to code-related tasks. In\nthis paper, we assess the performance of the Text-To-Text Transfer Transformer\n(T5) model in supporting four different code-related tasks: (i) automatic\nbug-fixing, (ii) injection of code mutants, (iii) generation of assert\nstatements, and (iv) code summarization. We pay particular attention in\nstudying the role played by pre-training and multi-task fine-tuning on the\nmodel's performance. We show that (i) the T5 can achieve better performance as\ncompared to state-of-the-art baselines; and (ii) while pre-training helps the\nmodel, not all tasks benefit from a multi-task fine-tuning.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2102.02017\n",
    "authors": [
      "Antonio Mastropaolo",
      "Nathan Cooper",
      "David Nader Palacio",
      "Simone Scalabrino",
      "Denys Poshyvanyk",
      "Rocco Oliveto",
      "Gabriele Bavota"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.08574"
  },
  {
    "id": "arXiv:2206.08575",
    "title": "Query-Efficient and Scalable Black-Box Adversarial Attacks on Discrete  Sequential Data via Bayesian Optimization",
    "abstract": "We focus on the problem of adversarial attacks against models on discrete\nsequential data in the black-box setting where the attacker aims to craft\nadversarial examples with limited query access to the victim model. Existing\nblack-box attacks, mostly based on greedy algorithms, find adversarial examples\nusing pre-computed key positions to perturb, which severely limits the search\nspace and might result in suboptimal solutions. To this end, we propose a\nquery-efficient black-box attack using Bayesian optimization, which dynamically\ncomputes important positions using an automatic relevance determination (ARD)\ncategorical kernel. We introduce block decomposition and history subsampling\ntechniques to improve the scalability of Bayesian optimization when an input\nsequence becomes long. Moreover, we develop a post-optimization algorithm that\nfinds adversarial examples with smaller perturbation size. Experiments on\nnatural language and protein classification tasks demonstrate that our method\nconsistently achieves higher attack success rate with significant reduction in\nquery count and modification rate compared to the previous state-of-the-art\nmethods.",
    "descriptor": "\nComments: ICML 2022; Codes at this https URL\n",
    "authors": [
      "Deokjae Lee",
      "Seungyong Moon",
      "Junhyeok Lee",
      "Hyun Oh Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08575"
  },
  {
    "id": "arXiv:2206.08578",
    "title": "Search engine effects on news consumption: ranking and  representativeness outweigh familiarity on news selection",
    "abstract": "Online platforms have transformed the ways in which individual access and\ninteract with news. For example, individuals put a high degree of trust in\nsearch engines. We use web-tracked behavioral data across a 2-month period and\nanalyze three competing factors, two algorithmic (ranking and\nrepresentativeness) and one psychological (familiarity), that could influence\nthe selection of news articles that appear in search results. Using news\nengagement as a proxy for familiarity, and Google search pages (n=1221) that\nled participants (n=280) to news articles, our results demonstrate the steering\npower of the algorithmic factors on news consumption as compared to\nfamiliarity. Despite the strong effect of ranking, we find that it plays a\nlesser role for news articles compared to non-news. We confirm that Google\nSearch drives individuals to unfamiliar sources and find that it increases the\ndiversity of the political audience to news sources. With our methodology, we\ntake a step in tackling the challenges of testing social science theories in\ndigital contexts shaped by algorithms.",
    "descriptor": "",
    "authors": [
      "Roberto Ulloa",
      "Celina Sylwia Kacperski"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08578"
  },
  {
    "id": "arXiv:2206.08582",
    "title": "DFG-NAS: Deep and Flexible Graph Neural Architecture Search",
    "abstract": "Graph neural networks (GNNs) have been intensively applied to various\ngraph-based applications. Despite their success, manually designing the\nwell-behaved GNNs requires immense human expertise. And thus it is inefficient\nto discover the potentially optimal data-specific GNN architecture. This paper\nproposes DFG-NAS, a new neural architecture search (NAS) method that enables\nthe automatic search of very deep and flexible GNN architectures. Unlike most\nexisting methods that focus on micro-architectures, DFG-NAS highlights another\nlevel of design: the search for macro-architectures on how atomic propagation\n(\\textbf{\\texttt{P}}) and transformation (\\textbf{\\texttt{T}}) operations are\nintegrated and organized into a GNN. To this end, DFG-NAS proposes a novel\nsearch space for \\textbf{\\texttt{P-T}} permutations and combinations based on\nmessage-passing dis-aggregation, defines four custom-designed\nmacro-architecture mutations, and employs the evolutionary algorithm to conduct\nan efficient and effective search. Empirical studies on four node\nclassification tasks demonstrate that DFG-NAS outperforms state-of-the-art\nmanual designs and NAS methods of GNNs.",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "Wentao Zhang",
      "Zheyu Lin",
      "Yu Shen",
      "Yang Li",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08582"
  },
  {
    "id": "arXiv:2206.08583",
    "title": "NAFS: A Simple yet Tough-to-beat Baseline for Graph Representation  Learning",
    "abstract": "Recently, graph neural networks (GNNs) have shown prominent performance in\ngraph representation learning by leveraging knowledge from both graph structure\nand node features. However, most of them have two major limitations. First,\nGNNs can learn higher-order structural information by stacking more layers but\ncan not deal with large depth due to the over-smoothing issue. Second, it is\nnot easy to apply these methods on large graphs due to the expensive\ncomputation cost and high memory usage. In this paper, we present node-adaptive\nfeature smoothing (NAFS), a simple non-parametric method that constructs node\nrepresentations without parameter learning. NAFS first extracts the features of\neach node with its neighbors of different hops by feature smoothing, and then\nadaptively combines the smoothed features. Besides, the constructed node\nrepresentation can further be enhanced by the ensemble of smoothed features\nextracted via different smoothing strategies. We conduct experiments on four\nbenchmark datasets on two different application scenarios: node clustering and\nlink prediction. Remarkably, NAFS with feature ensemble outperforms the\nstate-of-the-art GNNs on these tasks and mitigates the aforementioned two\nlimitations of most learning-based GNN counterparts.",
    "descriptor": "\nComments: 17 pages, 8 figures\n",
    "authors": [
      "Wentao Zhang",
      "Zeang Sheng",
      "Mingyu Yang",
      "Yang Li",
      "Yu Shen",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08583"
  },
  {
    "id": "arXiv:2206.08585",
    "title": "HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment  and Semantic-Region-Aware Inpainting",
    "abstract": "Hairstyle transfer is the task of modifying a source hairstyle to a target\none. Although recent hairstyle transfer models can reflect the delicate\nfeatures of hairstyles, they still have two major limitations. First, the\nexisting methods fail to transfer hairstyles when a source and a target image\nhave different poses (e.g., viewing direction or face size), which is prevalent\nin the real world. Also, the previous models generate unrealistic images when\nthere is a non-trivial amount of regions in the source image occluded by its\noriginal hair. When modifying long hair to short hair, shoulders or backgrounds\noccluded by the long hair need to be inpainted. To address these issues, we\npropose a novel framework for pose-invariant hairstyle transfer, HairFIT. Our\nmodel consists of two stages: 1) flow-based hair alignment and 2) hair\nsynthesis. In the hair alignment stage, we leverage a keypoint-based optical\nflow estimator to align a target hairstyle with a source pose. Then, we\ngenerate a final hairstyle-transferred image in the hair synthesis stage based\non Semantic-region-aware Inpainting Mask (SIM) estimator. Our SIM estimator\ndivides the occluded regions in the source image into different semantic\nregions to reflect their distinct features during the inpainting. To\ndemonstrate the effectiveness of our model, we conduct quantitative and\nqualitative evaluations using multi-view datasets, K-hairstyle and VoxCeleb.\nThe results indicate that HairFIT achieves a state-of-the-art performance by\nsuccessfully transferring hairstyles between images of different poses, which\nhas never been achieved before.",
    "descriptor": "\nComments: BMVC 2021 Oral Presentation\n",
    "authors": [
      "Chaeyeon Chung",
      "Taewoo Kim",
      "Hyelin Nam",
      "Seunghwan Choi",
      "Gyojung Gu",
      "Sunghyun Park",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08585"
  },
  {
    "id": "arXiv:2206.08587",
    "title": "Prototyping fast and agile motions for legged robots with Horizon",
    "abstract": "For legged robots to perform agile, highly dynamic and contact-rich motions,\nwhole-body trajectories computation of under-actuated complex systems subject\nto non-linear dynamics is required. In this work, we present hands-on\napplications of Horizon, a novel open-source framework for trajectory\noptimization tailored to robotic systems, that provides a collection of tools\nto simplify dynamic motion generation. Horizon was tested on a broad range of\nbehaviours involving several robotic platforms: we introduce its building\nblocks and describe the complete procedure to generate three complex motions\nusing its intuitive and straightforward API.",
    "descriptor": "\nComments: 4 pages, 5 figures, workshop paper: 6th Workshop on Legged Robots\n",
    "authors": [
      "Francesco Ruscelli",
      "Arturo Laurenzi",
      "Nikos G. Tsagarakis",
      "Enrico Mingo Hoffman"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08587"
  },
  {
    "id": "arXiv:2206.08589",
    "title": "Business Process Model for Interoperability Improvement in the  Agricultural Domain Using Digital Twins",
    "abstract": "A farm generates a lot of data from various systems, which is then stored in\na distributed manner, usually in non-standardized formats, which bears the risk\nof data inconsistencies. This work addresses this issue by using business\nprocess management (BPM) to demonstrate that the use of digital twins (DTs) can\nimprove interoperability between services in the agriculture domain. Steps from\nthe BPM lifecycle were applied to a farming use case in Germany. First, the\nas-is business process model was discovered and modeled without DTs, analyzed\nand then redesigned into the to-be model according to the DT integration. The\nto-be model showed a reduction in the number of tasks needed to be performed by\nthe farmer as well as an improvement of process data quality, interoperability,\nand efficiency. Finally, a comparison of the' average processing times of both\nmodels with the help of process simulation revealed improvements in the to-be\nprocess.",
    "descriptor": "",
    "authors": [
      "Emily Calvet",
      "Rodrigo Falc\u00e3o",
      "Lucineia Thom"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.08589"
  },
  {
    "id": "arXiv:2206.08593",
    "title": "Automatic Correction of Human Translations",
    "abstract": "We introduce translation error correction (TEC), the task of automatically\ncorrecting human-generated translations. Imperfections in machine translations\n(MT) have long motivated systems for improving translations post-hoc with\nautomatic post-editing. In contrast, little attention has been devoted to the\nproblem of automatically correcting human translations, despite the intuition\nthat humans make distinct errors that machines would be well-suited to assist\nwith, from typos to inconsistencies in translation conventions. To investigate\nthis, we build and release the Aced corpus with three TEC datasets. We show\nthat human errors in TEC exhibit a more diverse range of errors and far fewer\ntranslation fluency errors than the MT errors in automatic post-editing\ndatasets, suggesting the need for dedicated TEC models that are specialized to\ncorrect human errors. We show that pre-training instead on synthetic errors\nbased on human errors improves TEC F-score by as much as 5.1 points. We\nconducted a human-in-the-loop user study with nine professional translation\neditors and found that the assistance of our TEC system led them to produce\nsignificantly higher quality revised translations.",
    "descriptor": "\nComments: NAACL 2022. Dataset available at: this https URL\n",
    "authors": [
      "Jessy Lin",
      "Geza Kovacs",
      "Aditya Shastry",
      "Joern Wuebker",
      "John DeNero"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08593"
  },
  {
    "id": "arXiv:2206.08594",
    "title": "Accelerating numerical methods by gradient-based meta-solving",
    "abstract": "In science and engineering applications, it is often required to solve\nsimilar computational problems repeatedly. In such cases, we can utilize the\ndata from previously solved problem instances to improve the efficiency of\nfinding subsequent solutions. This offers a unique opportunity to combine\nmachine learning (in particular, meta-learning) and scientific computing. To\ndate, a variety of such domain-specific methods have been proposed in the\nliterature, but a generic approach for designing these methods remains\nunder-explored. In this paper, we tackle this issue by formulating a general\nframework to describe these problems, and propose a gradient-based algorithm to\nsolve them in a unified way. As an illustration of this approach, we study the\nadaptive generation of parameters for iterative solvers to accelerate the\nsolution of differential equations. We demonstrate the performance and\nversatility of our method through theoretical analysis and numerical\nexperiments, including applications to incompressible flow simulations and an\ninverse problem of parameter estimation.",
    "descriptor": "",
    "authors": [
      "Sohei Arisaka",
      "Qianxiao Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08594"
  },
  {
    "id": "arXiv:2206.08598",
    "title": "On the Influence of Enforcing Model Identifiability on Learning dynamics  of Gaussian Mixture Models",
    "abstract": "A common way to learn and analyze statistical models is to consider\noperations in the model parameter space. But what happens if we optimize in the\nparameter space and there is no one-to-one mapping between the parameter space\nand the underlying statistical model space? Such cases frequently occur for\nhierarchical models which include statistical mixtures or stochastic neural\nnetworks, and these models are said to be singular. Singular models reveal\nseveral important and well-studied problems in machine learning like the\ndecrease in convergence speed of learning trajectories due to attractor\nbehaviors. In this work, we propose a relative reparameterization technique of\nthe parameter space, which yields a general method for extracting regular\nsubmodels from singular models. Our method enforces model identifiability\nduring training and we study the learning dynamics for gradient descent and\nexpectation maximization for Gaussian Mixture Models (GMMs) under relative\nparameterization, showing faster experimental convergence and a improved\nmanifold shape of the dynamics around the singularity. Extending the analysis\nbeyond GMMs, we furthermore analyze the Fisher information matrix under\nrelative reparameterization and its influence on the generalization error, and\nshow how the method can be applied to more complex models like deep neural\nnetworks.",
    "descriptor": "",
    "authors": [
      "Pascal Mattia Esser",
      "Frank Nielsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08598"
  },
  {
    "id": "arXiv:2206.08603",
    "title": "Air Gap Control of the Novel Cross (+) Type 4-Pole MAGLEV Carrier System",
    "abstract": "Mechanical non-contact carrier systems based on magnetic levitation (MAGLEV)\nare used in special transportation areas (clean rooms, chemical areas, etc.).\nAmong these types of carriers, 4-pole hybrid electromagnetic systems\n(containing permanent magnets and electromagnets) stand out with their low\nenergy consumption. The main problem of maglev carrier systems is their\nnon-linear characteristics and unstable open-loop response. In this study, PID\nand I-PD controllers are designed for the air gap control of the new cross-type\n4-pole mechanical contactless carrier system. Thus, the instability problem was\novercome and the desired reference tracking for each degree of freedom was\nsuccessfully carried out in simulation environments, and the results were\ncompared.",
    "descriptor": "\nComments: 4 pages, 9 figures, 3 tables\n",
    "authors": [
      "Enes Mahmut G\u00f6ker",
      "Ahmet Fevzi Bozkurt",
      "Bora Baykal",
      "Kadir Erkan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08603"
  },
  {
    "id": "arXiv:2206.08604",
    "title": "An F-shape Click Model for Information Retrieval on Multi-block Mobile  Pages",
    "abstract": "To provide click simulation or relevance estimation based on users' implicit\ninteraction feedback, click models have been much studied during recent years.\nMost click models focus on user behaviors towards a single list. However, with\nthe development of user interface (UI) design, the layout of displayed items on\na result page tends to be multi-block (i.e., multi-list) style instead of a\nsingle list, which requires different assumptions to model user behaviors more\naccurately. There exist click models for multi-block pages in desktop contexts,\nbut they cannot be directly applied to mobile scenarios due to different\ninteraction manners, result types and especially multi-block presentation\nstyles. In particular, multi-block mobile pages can normally be decomposed into\ninterleavings of basic vertical blocks and horizontal blocks, thus resulting in\ntypically F-shape forms. To mitigate gaps between desktop and mobile contexts\nfor multi-block pages, we conduct a user eye-tracking study, and identify\nusers' sequential browsing, block skip and comparison patterns on F-shape\npages. These findings lead to the design of a novel F-shape Click Model (FSCM),\nwhich serves as a general solution to multi-block mobile pages. Firstly, we\nconstruct a directed acyclic graph (DAG) for each page, where each item is\nregarded as a vertex and each edge indicates the user's possible examination\nflow. Secondly, we propose DAG-structured GRUs and a comparison module to model\nusers' sequential (sequential browsing, block skip) and non-sequential\n(comparison) behaviors respectively. Finally, we combine GRU states and\ncomparison patterns to perform user click predictions. Experiments on a\nlarge-scale real-world dataset validate the effectiveness of FSCM on user\nbehavior predictions compared with baseline models.",
    "descriptor": "\nComments: Paper under review\n",
    "authors": [
      "Lingyue Fu",
      "Jianghao Lin",
      "Weiwen Liu",
      "Ruiming Tang",
      "Weinan Zhang",
      "Rui Zhang",
      "Yong Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08604"
  },
  {
    "id": "arXiv:2206.08605",
    "title": "On Efficient Real-Time Semantic Segmentation: A Survey",
    "abstract": "Semantic segmentation is the problem of assigning a class label to every\npixel in an image, and is an important component of an autonomous vehicle\nvision stack for facilitating scene understanding and object detection.\nHowever, many of the top performing semantic segmentation models are extremely\ncomplex and cumbersome, and as such are not suited to deployment onboard\nautonomous vehicle platforms where computational resources are limited and\nlow-latency operation is a vital requirement. In this survey, we take a\nthorough look at the works that aim to address this misalignment with more\ncompact and efficient models capable of deployment on low-memory embedded\nsystems while meeting the constraint of real-time inference. We discuss several\nof the most prominent works in the field, placing them within a taxonomy based\non their major contributions, and finally we evaluate the inference speed of\nthe discussed models under consistent hardware and software setups that\nrepresent a typical research environment with high-end GPU and a realistic\ndeployed scenario using low-memory embedded GPU hardware. Our experimental\nresults demonstrate that many works are capable of real-time performance on\nresource-constrained hardware, while illustrating the consistent trade-off\nbetween latency and accuracy.",
    "descriptor": "\nComments: 18 pages, 13 figures, 4 tables This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Christopher J. Holder",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08605"
  },
  {
    "id": "arXiv:2206.08607",
    "title": "Optimal Shelf Arrangement to Minimize Robot Retrieval Time",
    "abstract": "Shelves are commonly used to store objects in homes, stores, and warehouses.\nWe formulate the problem of Optimal Shelf Arrangement (OSA), where the goal is\nto optimize the arrangement of objects on a shelf for access time given an\naccess frequency and movement cost for each object. We propose OSA-MIP, a\nmixed-integer program (MIP), show that it finds an optimal solution for OSA\nunder certain conditions, and provide bounds on its suboptimal solutions in\ngeneral cost settings. We analytically characterize a necessary and sufficient\nshelf density condition for which there exists an arrangement such that any\nobject can be retrieved without removing objects from the shelf. Experimental\ndata from 1,575 simulated shelf trials and 54 trials with a physical Fetch\nrobot equipped with a pushing blade and suction grasping tool suggest that\narranging the objects optimally reduces the expected retrieval cost by 60-80%\nin fully-observed configurations and reduces the expected search cost by 50-70%\nwhile increasing the search success rate by up to 2x in partially-observed\nconfigurations.",
    "descriptor": "\nComments: 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)\n",
    "authors": [
      "Lawrence Yunliang Chen",
      "Huang Huang",
      "Michael Danielczuk",
      "Jeffrey Ichnowski",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08607"
  },
  {
    "id": "arXiv:2206.08609",
    "title": "Locally Structure-Preserving div-curl operators for high order  Discontinuous Galerkin schemes",
    "abstract": "We propose a novel Structure-Preserving Discontinuous Galerkin (SPDG)\noperator that recovers at the discrete level the algebraic property related to\nthe divergence of the curl of a vector field, which is typically referred to as\ndiv-curl problem. A staggered Cartesian grid is adopted in 3D, where the vector\nfield is naturally defined at the corners of the control volume, while its curl\nis evaluated as a cell-centered quantity. Firstly, the curl operator is\nrewritten as the divergence of a tensor, hence allowing compatible finite\ndifference schemes to be devised and to be proven to mimic the algebraic\ndiv-curl property. Successively, a high order DG divergence operator is built\nupon integration by parts, so that the structure-preserving finite difference\ndiv-curl operator is exactly retrieved for first order discretizations. We\nfurther demonstrate that the novel SPDG schemes are capable of obtaining a zero\ndiv-curl identity with machine precision from second up to sixth order\naccuracy. In a second part, we show the applicability of these SPDG methods by\nsolving the incompressible Navier-Stokes equations written in vortex-stream\nformulation. This hyperbolic system deals with divergence-free involutions\nrelated to the velocity and vorticity field as well as to the stream function,\nthus it provides an ideal setting for the validation of the novel schemes. A\ncompatible discretization of the numerical viscosity is also proposed in order\nto maintain the structure-preserving property of the div-curl DG operators even\nin the presence of artificial or physical dissipative terms. Finally, to\novercome the time step restriction dictated by the viscous sub-system,\nImplicit-Explicit (IMEX) Runge-Kutta time stepping techniques are tailored to\nhandle the SPDG framework.",
    "descriptor": "",
    "authors": [
      "Walter Boscheri",
      "Giacomo Dimarco",
      "Lorenzo Pareschi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08609"
  },
  {
    "id": "arXiv:2206.08610",
    "title": "Masked Autoencoders for Generic Event Boundary Detection CVPR'2022  Kinetics-GEBD Challenge",
    "abstract": "Generic Event Boundary Detection (GEBD) tasks aim at detecting generic,\ntaxonomy-free event boundaries that segment a whole video into chunks. In this\npaper, we apply Masked Autoencoders to improve algorithm performance on the\nGEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders\nfine-tuned on the GEBD task as a self-supervised learner with other base\nmodels. Moreover, we also use a semi-supervised pseudo-label method to take\nfull advantage of the abundant unlabeled Kinetics-400 data while training. In\naddition, we propose a soft-label method to partially balance the positive and\nnegative samples and alleviate the problem of ambiguous labeling in this task.\nLastly, a tricky segmentation alignment policy is implemented to refine\nboundaries predicted by our models to more accurate locations. With our\napproach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set,\nwhich improved the F1-score by 2.31% compared to the winner of the 2021\nKinetics-GEBD Challenge. Our code is available at\nhttps://github.com/ContentAndMaterialPortrait/MAE-GEBD.",
    "descriptor": "",
    "authors": [
      "Rui He",
      "Yuanxi Sun",
      "Youzeng Li",
      "Zuwei Huang",
      "Feng Hu",
      "Xu Cheng",
      "Jie Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08610"
  },
  {
    "id": "arXiv:2206.08611",
    "title": "Medical Dialogue Response Generation with Pivotal Information Recalling",
    "abstract": "Medical dialogue generation is an important yet challenging task. Most\nprevious works rely on the attention mechanism and large-scale pretrained\nlanguage models. However, these methods often fail to acquire pivotal\ninformation from the long dialogue history to yield an accurate and informative\nresponse, due to the fact that the medical entities usually scatters throughout\nmultiple utterances along with the complex relationships between them. To\nmitigate this problem, we propose a medical response generation model with\nPivotal Information Recalling (MedPIR), which is built on two components, i.e.,\nknowledge-aware dialogue graph encoder and recall-enhanced generator. The\nknowledge-aware dialogue graph encoder constructs a dialogue graph by\nexploiting the knowledge relationships between entities in the utterances, and\nencodes it with a graph attention network. Then, the recall-enhanced generator\nstrengthens the usage of these pivotal information by generating a summary of\nthe dialogue before producing the actual response. Experimental results on two\nlarge-scale medical dialogue datasets show that MedPIR outperforms the strong\nbaselines in BLEU scores and medical entities F1 measure.",
    "descriptor": "",
    "authors": [
      "Yu Zhao",
      "Yunxin Li",
      "Yuxiang Wu",
      "Baotian Hu",
      "Qingcai Chen",
      "Xiaolong Wang",
      "Yuxin Ding",
      "Min Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08611"
  },
  {
    "id": "arXiv:2206.08614",
    "title": "Understanding Aesthetics with Language: A Photo Critique Dataset for  Aesthetic Assessment",
    "abstract": "Computational inference of aesthetics is an ill-defined task due to its\nsubjective nature. Many datasets have been proposed to tackle the problem by\nproviding pairs of images and aesthetic scores based on human ratings. However,\nhumans are better at expressing their opinion, taste, and emotions by means of\nlanguage rather than summarizing them in a single number. In fact, photo\ncritiques provide much richer information as they reveal how and why users rate\nthe aesthetics of visual stimuli. In this regard, we propose the Reddit Photo\nCritique Dataset (RPCD), which contains tuples of image and photo critiques.\nRPCD consists of 74K images and 220K comments and is collected from a Reddit\ncommunity used by hobbyists and professional photographers to improve their\nphotography skills by leveraging constructive community feedback. The proposed\ndataset differs from previous aesthetics datasets mainly in three aspects,\nnamely (i) the large scale of the dataset and the extension of the comments\ncriticizing different aspects of the image, (ii) it contains mostly UltraHD\nimages, and (iii) it can easily be extended to new data as it is collected\nthrough an automatic pipeline. To the best of our knowledge, in this work, we\npropose the first attempt to estimate the aesthetic quality of visual stimuli\nfrom the critiques. To this end, we exploit the polarity of the sentiment of\ncriticism as an indicator of aesthetic judgment. We demonstrate how sentiment\npolarity correlates positively with the aesthetic judgment available for two\naesthetic assessment benchmarks. Finally, we experiment with several models by\nusing the sentiment scores as a target for ranking images. Dataset and\nbaselines are available (https://github.com/mediatechnologycenter/aestheval).",
    "descriptor": "",
    "authors": [
      "Daniel Vera Nieto",
      "Luigi Celona",
      "Clara Fernandez-Labrador"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08614"
  },
  {
    "id": "arXiv:2206.08615",
    "title": "The Role of Depth, Width, and Activation Complexity in the Number of  Linear Regions of Neural Networks",
    "abstract": "Many feedforward neural networks generate continuous and piecewise-linear\n(CPWL) mappings. Specifically, they partition the input domain into regions on\nwhich the mapping is an affine function. The number of these so-called linear\nregions offers a natural metric to characterize the expressiveness of CPWL\nmappings. Although the precise determination of this quantity is often out of\nreach, bounds have been proposed for specific architectures, including the\nwell-known ReLU and Maxout networks. In this work, we propose a more general\nperspective and provide precise bounds on the maximal number of linear regions\nof CPWL networks based on three sources of expressiveness: depth, width, and\nactivation complexity. Our estimates rely on the combinatorial structure of\nconvex partitions and highlight the distinctive role of depth which, on its\nown, is able to exponentially increase the number of regions. We then introduce\na complementary stochastic framework to estimate the average number of linear\nregions produced by a CPWL network architecture. Under reasonable assumptions,\nthe expected density of linear regions along any 1D path is bounded by the\nproduct of depth, width, and a measure of activation complexity (up to a\nscaling factor). This yields an identical role to the three sources of\nexpressiveness: no exponential growth with depth is observed anymore.",
    "descriptor": "",
    "authors": [
      "Alexis Goujon",
      "Arian Etemadi",
      "Michael Unser"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08615"
  },
  {
    "id": "arXiv:2206.08617",
    "title": "Convex reformulations for a special class of nonlinear MPC problems",
    "abstract": "We show how the solution to NMPC problems for a special type of input-affine\ndiscrete-time systems can be obtained by reformulating the underlying\nnon-convex optimal control problem in terms of a finite number of convex\nsubproblems. The reformulation is facilitated by exact (input-state)\nlinearization, which is shown to provide beneficial properties for the treated\nclass of systems. We characterize possible types of the resulting convex\nsubproblems and illustrate our approach with three numerical examples.",
    "descriptor": "\nComments: 9 pages, 8 figures, submitted to 20th European Control Conference 2022\n",
    "authors": [
      "Manuel Kl\u00e4dtke",
      "Moritz Schulze Darup"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08617"
  },
  {
    "id": "arXiv:2206.08621",
    "title": "A Graph-Enhanced Click Model for Web Search",
    "abstract": "To better exploit search logs and model users' behavior patterns, numerous\nclick models are proposed to extract users' implicit interaction feedback. Most\ntraditional click models are based on the probabilistic graphical model (PGM)\nframework, which requires manually designed dependencies and may oversimplify\nuser behaviors. Recently, methods based on neural networks are proposed to\nimprove the prediction accuracy of user behaviors by enhancing the expressive\nability and allowing flexible dependencies. However, they still suffer from the\ndata sparsity and cold-start problems. In this paper, we propose a novel\ngraph-enhanced click model (GraphCM) for web search. Firstly, we regard each\nquery or document as a vertex, and propose novel homogeneous graph construction\nmethods for queries and documents respectively, to fully exploit both\nintra-session and inter-session information for the sparsity and cold-start\nproblems. Secondly, following the examination hypothesis, we separately model\nthe attractiveness estimator and examination predictor to output the\nattractiveness scores and examination probabilities, where graph neural\nnetworks and neighbor interaction techniques are applied to extract the\nauxiliary information encoded in the pre-constructed homogeneous graphs.\nFinally, we apply combination functions to integrate examination probabilities\nand attractiveness scores into click predictions. Extensive experiments\nconducted on three real-world session datasets show that GraphCM not only\noutperforms the state-of-art models, but also achieves superior performance in\naddressing the data sparsity and cold-start problems.",
    "descriptor": "\nComments: 10 pages; Accepted by SIGIR 2021\n",
    "authors": [
      "Jianghao Lin",
      "Weiwen Liu",
      "Xinyi Dai",
      "Weinan Zhang",
      "Shuai Li",
      "Ruiming Tang",
      "Xiuqiang He",
      "Jianye Hao",
      "Yong Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08621"
  },
  {
    "id": "arXiv:2206.08622",
    "title": "Accelerated Subdivision for Clustering Roots of Polynomials given by  Evaluation Oracles",
    "abstract": "In our quest for the design, the analysis and the implementation of a\nsubdivision algorithm for finding the complex roots of univariate polynomials\ngiven by oracles for their evaluation, we present sub-algorithms allowing\nsubstantial acceleration of subdivision for complex roots clustering for such\npolynomials. We rely on Cauchy sums which approximate power sums of the roots\nin a fixed complex disc and can be computed in a small number of evaluations\n--polylogarithmic in the degree. We describe root exclusion, root counting,\nroot radius approximation and a procedure for contracting a disc towards the\ncluster of root it contains, called $\\varepsilon$-compression. To demonstrate\nthe efficiency of our algorithms, we combine them in a prototype root\nclustering algorithm. For computing clusters of roots of polynomials that can\nbe evaluated fast, our implementation competes advantageously with user's\nchoice for root finding, MPsolve.",
    "descriptor": "",
    "authors": [
      "R\u00e9mi Imbach",
      "Victor Y. Pan"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.08622"
  },
  {
    "id": "arXiv:2206.08626",
    "title": "MSDF: A General Open-Domain Multi-Skill Dialog Framework",
    "abstract": "Dialog systems have achieved significant progress and have been widely used\nin various scenarios. The previous researches mainly focused on designing\ndialog generation models in a single scenario, while comprehensive abilities\nare required to handle tasks under various scenarios in the real world. In this\npaper, we propose a general Multi-Skill Dialog Framework, namely MSDF, which\ncan be applied in different dialog tasks (e.g. knowledge grounded dialog and\npersona based dialog). Specifically, we propose a transferable response\ngenerator pre-trained on diverse large-scale dialog corpora as the backbone of\nMSDF, consisting of BERT-based encoders and a GPT-based decoder. To select the\nresponse consistent with dialog history, we propose a consistency selector\ntrained through negative sampling. Moreover, the flexible copy mechanism of\nexternal knowledge is also employed to enhance the utilization of multiform\nknowledge in various scenarios. We conduct experiments on knowledge grounded\ndialog, recommendation dialog, and persona based dialog tasks. The experimental\nresults indicate that our MSDF outperforms the baseline models with a large\nmargin. In the Multi-skill Dialog of 2021 Language and Intelligence Challenge,\nour general MSDF won the 3rd prize, which proves our MSDF is effective and\ncompetitive.",
    "descriptor": "",
    "authors": [
      "Yu Zhao",
      "Xinshuo Hu",
      "Yunxin Li",
      "Baotian Hu",
      "Dongfang Li",
      "Sichao Chen",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08626"
  },
  {
    "id": "arXiv:2206.08631",
    "title": "On Computing Optimal Linear Diagrams",
    "abstract": "Linear diagrams are an effective way to visualize set-based data by\nrepresenting elements as columns and sets as rows with one or more horizontal\nline segments, whose vertical overlaps with other rows indicate set\nintersections and their contained elements. The efficacy of linear diagrams\nheavily depends on having few line segments. The underlying minimization\nproblem has already been explored heuristically, but its computational\ncomplexity has yet to be classified. In this paper, we show that minimizing\nline segments in linear diagrams is equivalent to a well-studied NP-hard\nproblem, and extend the NP-hardness to a restricted setting. We develop new\nalgorithms for computing linear diagrams with minimum number of line segments\nthat build on a traveling salesperson (TSP) formulation and allow constraints\non the element orders, namely, forcing two sets to be drawn as single line\nsegments, giving weights to sets, and allowing hierarchical constraints via\nPQ-trees. We conduct an experimental evaluation and compare previous algorithms\nfor minimizing line segments with our TSP formulation, showing that a\nstate-of-the art TSP-solver can solve all considered instances optimally, most\nof them within few milliseconds.",
    "descriptor": "\nComments: 17 pages, 4 figures. Extended version of Diagrams 2022 paper\n",
    "authors": [
      "Alexander Dobler",
      "Martin N\u00f6llenburg"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.08631"
  },
  {
    "id": "arXiv:2206.08632",
    "title": "Learning Using Privileged Information for Zero-Shot Action Recognition",
    "abstract": "Zero-Shot Action Recognition (ZSAR) aims to recognize video actions that have\nnever been seen during training. Most existing methods assume a shared semantic\nspace between seen and unseen actions and intend to directly learn a mapping\nfrom a visual space to the semantic space. This approach has been challenged by\nthe semantic gap between the visual space and semantic space. This paper\npresents a novel method that uses object semantics as privileged information to\nnarrow the semantic gap and, hence, effectively, assist the learning. In\nparticular, a simple hallucination network is proposed to implicitly extract\nobject semantics during testing without explicitly extracting objects and a\ncross-attention module is developed to augment visual feature with the object\nsemantics. Experiments on the Olympic Sports, HMDB51 and UCF101 datasets have\nshown that the proposed method outperforms the state-of-the-art methods by a\nlarge margin.",
    "descriptor": "",
    "authors": [
      "Zhiyi Gao",
      "Wanqing Li",
      "Zihui Guo",
      "Bin Yu",
      "Yonghong Hou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08632"
  },
  {
    "id": "arXiv:2206.08638",
    "title": "Minimum Noticeable Difference based Adversarial Privacy Preserving Image  Generation",
    "abstract": "Deep learning models are found to be vulnerable to adversarial examples, as\nwrong predictions can be caused by small perturbation in input for deep\nlearning models. Most of the existing works of adversarial image generation try\nto achieve attacks for most models, while few of them make efforts on\nguaranteeing the perceptual quality of the adversarial examples. High quality\nadversarial examples matter for many applications, especially for the privacy\npreserving. In this work, we develop a framework based on the Minimum\nNoticeable Difference (MND) concept to generate adversarial privacy preserving\nimages that have minimum perceptual difference from the clean ones but are able\nto attack deep learning models. To achieve this, an adversarial loss is firstly\nproposed to make the deep learning models attacked by the adversarial images\nsuccessfully. Then, a perceptual quality-preserving loss is developed by taking\nthe magnitude of perturbation and perturbation-caused structural and gradient\nchanges into account, which aims to preserve high perceptual quality for\nadversarial image generation. To the best of our knowledge, this is the first\nwork on exploring quality-preserving adversarial image generation based on the\nMND concept for privacy preserving. To evaluate its performance in terms of\nperceptual quality, the deep models on image classification and face\nrecognition are tested with the proposed method and several anchor methods in\nthis work. Extensive experimental results demonstrate that the proposed MND\nframework is capable of generating adversarial images with remarkably improved\nperformance metrics (e.g., PSNR, SSIM, and MOS) than that generated with the\nanchor methods.",
    "descriptor": "",
    "authors": [
      "Wen Sun",
      "Jian Jin",
      "Weisi Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08638"
  },
  {
    "id": "arXiv:2206.08639",
    "title": "Experimental evaluation of neutron-induced errors on a multicore RISC-V  platform",
    "abstract": "RISC-V architectures have gained importance in the last years due to their\nflexibility and open-source Instruction Set Architecture (ISA), allowing\ndevelopers to efficiently adopt RISC-V processors in several domains with a\nreduced cost. For application domains, such as safety-critical and\nmission-critical, the execution must be reliable as a fault can compromise the\nsystem's ability to operate correctly. However, the application's error rate on\nRISC-V processors is not significantly evaluated, as it has been done for\nstandard x86 processors. In this work, we investigate the error rate of a\ncommercial RISC-V ASIC platform, the GAP8, exposed to a neutron beam. We show\nthat for computing-intensive applications, such as classification Convolutional\nNeural Networks (CNN), the error rate can be 3.2x higher than the average error\nrate. Additionally, we find that the majority (96.12%) of the errors on the CNN\ndo not generate misclassifications. Finally, we also evaluate the events that\ncause application interruption on GAP8 and show that the major source of\nincorrect interruptions is application hangs (i.g., due to an infinite loop or\na racing condition).",
    "descriptor": "",
    "authors": [
      "Fernando Fernandes dos Santos",
      "Angeliki Kritikakou",
      "Olivier Sentieys"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.08639"
  },
  {
    "id": "arXiv:2206.08640",
    "title": "Uncertainty-aware Evaluation of Time-Series Classification for Online  Handwriting Recognition with Domain Shift",
    "abstract": "For many applications, analyzing the uncertainty of a machine learning model\nis indispensable. While research of uncertainty quantification (UQ) techniques\nis very advanced for computer vision applications, UQ methods for\nspatio-temporal data are less studied. In this paper, we focus on models for\nonline handwriting recognition, one particular type of spatio-temporal data.\nThe data is observed from a sensor-enhanced pen with the goal to classify\nwritten characters. We conduct a broad evaluation of aleatoric (data) and\nepistemic (model) UQ based on two prominent techniques for Bayesian inference,\nStochastic Weight Averaging-Gaussian (SWAG) and Deep Ensembles. Next to a\nbetter understanding of the model, UQ techniques can detect out-of-distribution\ndata and domain shifts when combining right-handed and left-handed writers (an\nunderrepresented group).",
    "descriptor": "",
    "authors": [
      "Andreas Kla\u00df",
      "Sven M. Lorenz",
      "Martin W. Lauer-Schmaltz",
      "David R\u00fcgamer",
      "Bernd Bischl",
      "Christopher Mutschler",
      "Felix Ott"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08640"
  },
  {
    "id": "arXiv:2206.08641",
    "title": "Improving Diversity of Multiple Trajectory Prediction based on  Map-adaptive Lane Loss",
    "abstract": "Prior arts in the field of motion predictions for autonomous driving tend to\nfocus on finding a trajectory that is close to the ground truth trajectory.\nSuch problem formulations and approaches, however, frequently lead to loss of\ndiversity and biased trajectory predictions. Therefore, they are unsuitable for\nreal-world autonomous driving where diverse and road-dependent multimodal\ntrajectory predictions are critical for safety. To this end, this study\nproposes a novel loss function, \\textit{Lane Loss}, that ensures map-adaptive\ndiversity and accommodates geometric constraints. A two-stage trajectory\nprediction architecture with a novel trajectory candidate proposal module,\n\\textit{Trajectory Prediction Attention (TPA)}, is trained with Lane Loss\nencourages multiple trajectories to be diversely distributed, covering feasible\nmaneuvers in a map-aware manner. Furthermore, considering that the existing\ntrajectory performance metrics are focusing on evaluating the accuracy based on\nthe ground truth future trajectory, a quantitative evaluation metric is also\nsuggested to evaluate the diversity of predicted multiple trajectories. The\nexperiments performed on the Argoverse dataset show that the proposed method\nsignificantly improves the diversity of the predicted trajectories without\nsacrificing the prediction accuracy.",
    "descriptor": "\nComments: 11pages 5figures\n",
    "authors": [
      "Sanmin Kim",
      "Hyeongseok Jeon",
      "Junwon Choi",
      "Dongsuk Kum"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08641"
  },
  {
    "id": "arXiv:2206.08642",
    "title": "Supercloseness of the local discontinuous Galerkin method for a  singularly perturbed convection-diffusion problem",
    "abstract": "A singularly perturbed convection-diffusion problem posed on the unit square\nin $\\mathbb{R}^2$, whose solution has exponential boundary layers, is solved\nnumerically using the local discontinuous Galerkin (LDG) method with piecewise\npolynomials of degree at most $k>0$ on three families of layer-adapted meshes:\nShishkin-type, Bakhvalov-Shishkin-type and Bakhvalov-type.On Shishkin-type\nmeshes this method is known to be no greater than $O(N^{-(k+1/2)})$ accurate in\nthe energy norm induced by the bilinear form of the weak formulation, where $N$\nmesh intervals are used in each coordinate direction. (Note: all bounds in this\nabstract are uniform in the singular perturbation parameter and neglect\nlogarithmic factors that will appear in our detailed analysis.) A delicate\nargument is used in this paper to establish $O(N^{-(k+1)})$ energy-norm\nsuperconvergence on all three types of mesh for the difference between the LDG\nsolution and a local Gauss-Radau projection of the exact solution into the\nfinite element space. This supercloseness property implies a new $N^{-(k+1)}$\nbound for the $L^2$ error between the LDG solution on each type of mesh and the\nexact solution of the problem; this bound is optimal (up to logarithmic\nfactors). Numerical experiments confirm our theoretical results.",
    "descriptor": "\nComments: 26 pages, 10 figures, 22 references\n",
    "authors": [
      "Yao Cheng",
      "Shan Jiang",
      "Martin Stynes"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08642"
  },
  {
    "id": "arXiv:2206.08644",
    "title": "A Hybrid Modelling Approach for Aerial Manipulators",
    "abstract": "Aerial manipulators (AM) exhibit particularly challenging, non-linear\ndynamics; the UAV and the manipulator it is carrying form a tightly coupled\ndynamic system, mutually impacting each other. The mathematical model\ndescribing these dynamics forms the core of many solutions in non-linear\ncontrol and deep reinforcement learning. Traditionally, the formulation of the\ndynamics involves Euler angle parametrization in the Lagrangian framework or\nquaternion parametrization in the Newton-Euler framework. The former has the\ndisadvantage of giving birth to singularities and the latter being\nalgorithmically complex. This work presents a hybrid solution, combining the\nbenefits of both, namely a quaternion approach leveraging the Lagrangian\nframework, connecting the singularity-free parameterization with the\nalgorithmic simplicity of the Lagrangian approach. We do so by offering\ndetailed insights into the kinematic modeling process and the formulation of\nthe dynamics of a general aerial manipulator. The obtained dynamics model is\nvalidated experimentally against a real-time physics engine. A practical\napplication of the obtained dynamics model is shown in the context of a\ncomputed torque feedback controller (feedback linearization), where we analyze\nits real-time capability with increasingly complex models.",
    "descriptor": "\nComments: 26 pages, 12 figures, to be published in the Journal of Intelligent and Robotic Systems (JINT/Springer)\n",
    "authors": [
      "Paul Kremer",
      "Jose Luis Sanchez-Lopez",
      "Holger Voos"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08644"
  },
  {
    "id": "arXiv:2206.08645",
    "title": "Local Slot Attention for Vision-and-Language Navigation",
    "abstract": "Vision-and-language navigation (VLN), a frontier study aiming to pave the way\nfor general-purpose robots, has been a hot topic in the computer vision and\nnatural language processing community. The VLN task requires an agent to\nnavigate to a goal location following natural language instructions in\nunfamiliar environments.\nRecently, transformer-based models have gained significant improvements on\nthe VLN task. Since the attention mechanism in the transformer architecture can\nbetter integrate inter- and intra-modal information of vision and language.\nHowever, there exist two problems in current transformer-based models.\n1) The models process each view independently without taking the integrity of\nthe objects into account.\n2) During the self-attention operation in the visual modality, the views that\nare spatially distant can be inter-weaved with each other without explicit\nrestriction. This kind of mixing may introduce extra noise instead of useful\ninformation.\nTo address these issues, we propose 1) A slot-attention based module to\nincorporate information from segmentation of the same object. 2) A local\nattention mask mechanism to limit the visual attention span. The proposed\nmodules can be easily plugged into any VLN architecture and we use the\nRecurrent VLN-Bert as our base model. Experiments on the R2R dataset show that\nour model has achieved the state-of-the-art results.",
    "descriptor": "\nComments: ICMR 2022\n",
    "authors": [
      "Yifeng Zhuang",
      "Qiang Sun",
      "Yanwei Fu",
      "Lifeng Chen",
      "Xiangyang Sue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08645"
  },
  {
    "id": "arXiv:2206.08646",
    "title": "Scalable Differentially Private Clustering via Hierarchically Separated  Trees",
    "abstract": "We study the private $k$-median and $k$-means clustering problem in $d$\ndimensional Euclidean space. By leveraging tree embeddings, we give an\nefficient and easy to implement algorithm, that is empirically competitive with\nstate of the art non private methods. We prove that our method computes a\nsolution with cost at most $O(d^{3/2}\\log n)\\cdot OPT + O(k d^2 \\log^2 n /\n\\epsilon^2)$, where $\\epsilon$ is the privacy guarantee. (The dimension term,\n$d$, can be replaced with $O(\\log k)$ using standard dimension reduction\ntechniques.) Although the worst-case guarantee is worse than that of state of\nthe art private clustering methods, the algorithm we propose is practical, runs\nin near-linear, $\\tilde{O}(nkd)$, time and scales to tens of millions of\npoints. We also show that our method is amenable to parallelization in\nlarge-scale distributed computing environments. In particular we show that our\nprivate algorithms can be implemented in logarithmic number of MPC rounds in\nthe sublinear memory regime. Finally, we complement our theoretical analysis\nwith an empirical evaluation demonstrating the algorithm's efficiency and\naccuracy in comparison to other privacy clustering baselines.",
    "descriptor": "\nComments: To appear at KDD'22\n",
    "authors": [
      "Vincent Cohen-Addad",
      "Alessandro Epasto",
      "Silvio Lattanzi",
      "Vahab Mirrokni",
      "Andres Munoz",
      "David Saulpic",
      "Chris Schwiegelshohn",
      "Sergei Vassilvitskii"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08646"
  },
  {
    "id": "arXiv:2206.08652",
    "title": "An efficient spectral method for the fractional Schr\u00f6dinger equation  on the real line",
    "abstract": "The fractional Schr\\\"{o}dinger equation (FSE) on the real line arises in a\nbroad range of physical settings and their numerical simulation is challenging\ndue to the nonlocal nature and the power law decay of the solution at infinity.\nIn this paper, we propose a new spectral discretization scheme for the FSE in\nspace based upon Malmquist-Takenaka functions. We show that this new\ndiscretization scheme achieves much better performance than existing\ndiscretization schemes in the case where the underlying FSE involves the square\nroot of the Laplacian, while in other cases it also exhibits comparable or even\nbetter performance. Numerical experiments are provided to illustrate the\neffectiveness of the proposed method.",
    "descriptor": "\nComments: 25 pages, 12 figures\n",
    "authors": [
      "Mengxia Shen",
      "Haiyong Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08652"
  },
  {
    "id": "arXiv:2206.08653",
    "title": "All Mistakes Are Not Equal: Comprehensive Hierarchy Aware Multi-label  Predictions (CHAMP)",
    "abstract": "This paper considers the problem of Hierarchical Multi-Label Classification\n(HMC), where (i) several labels can be present for each example, and (ii)\nlabels are related via a domain-specific hierarchy tree. Guided by the\nintuition that all mistakes are not equal, we present Comprehensive Hierarchy\nAware Multi-label Predictions (CHAMP), a framework that penalizes a\nmisprediction depending on its severity as per the hierarchy tree. While there\nhave been works that apply such an idea to single-label classification, to the\nbest of our knowledge, there are limited such works for multilabel\nclassification focusing on the severity of mistakes. The key reason is that\nthere is no clear way of quantifying the severity of a misprediction a priori\nin the multilabel setting. In this work, we propose a simple but effective\nmetric to quantify the severity of a mistake in HMC, naturally leading to\nCHAMP. Extensive experiments on six public HMC datasets across modalities\n(image, audio, and text) demonstrate that incorporating hierarchical\ninformation leads to substantial gains as CHAMP improves both AUPRC (2.6%\nmedian percentage improvement) and hierarchical metrics (2.85% median\npercentage improvement), over stand-alone hierarchical or multilabel\nclassification methods. Compared to standard multilabel baselines, CHAMP\nprovides improved AUPRC in both robustness (8.87% mean percentage improvement )\nand less data regimes. Further, our method provides a framework to enhance\nexisting multilabel classification algorithms with better mistakes (18.1% mean\npercentage increment).",
    "descriptor": "",
    "authors": [
      "Ashwin Vaswani",
      "Gaurav Aggarwal",
      "Praneeth Netrapalli",
      "Narayan G Hegde"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08653"
  },
  {
    "id": "arXiv:2206.08655",
    "title": "Learning Implicit Feature Alignment Function for Semantic Segmentation",
    "abstract": "Integrating high-level context information with low-level details is of\ncentral importance in semantic segmentation. Towards this end, most existing\nsegmentation models apply bilinear up-sampling and convolutions to feature maps\nof different scales, and then align them at the same resolution. However,\nbilinear up-sampling blurs the precise information learned in these feature\nmaps and convolutions incur extra computation costs. To address these issues,\nwe propose the Implicit Feature Alignment function (IFA). Our method is\ninspired by the rapidly expanding topic of implicit neural representations,\nwhere coordinate-based neural networks are used to designate fields of signals.\nIn IFA, feature vectors are viewed as representing a 2D field of information.\nGiven a query coordinate, nearby feature vectors with their relative\ncoordinates are taken from the multi-level feature maps and then fed into an\nMLP to generate the corresponding output. As such, IFA implicitly aligns the\nfeature maps at different levels and is capable of producing segmentation maps\nin arbitrary resolutions. We demonstrate the efficacy of IFA on multiple\ndatasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be\ncombined with improvement on various architectures, and it achieves\nstate-of-the-art computation-accuracy trade-off on common benchmarks. Code will\nbe made available at https://github.com/hzhupku/IFA.",
    "descriptor": "",
    "authors": [
      "Hanzhe Hu",
      "Yinbo Chen",
      "Jiarui Xu",
      "Shubhankar Borse",
      "Hong Cai",
      "Fatih Porikli",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08655"
  },
  {
    "id": "arXiv:2206.08656",
    "title": "tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks",
    "abstract": "Larger Spiking Neural Network (SNN) models are typically favorable as they\ncan offer higher accuracy. However, employing such models on the resource- and\nenergy-constrained embedded platforms is inefficient. Towards this, we present\na tinySNN framework that optimizes the memory and energy requirements of SNN\nprocessing in both the training and inference phases, while keeping the\naccuracy high. It is achieved by reducing the SNN operations, improving the\nlearning quality, quantizing the SNN parameters, and selecting the appropriate\nSNN model. Furthermore, our tinySNN quantizes different SNN parameters (i.e.,\nweights and neuron parameters) to maximize the compression while exploring\ndifferent combinations of quantization schemes, precision levels, and rounding\nschemes to find the model that provides acceptable accuracy. The experimental\nresults demonstrate that our tinySNN significantly reduces the memory footprint\nand the energy consumption of SNNs without accuracy loss as compared to the\nbaseline network. Therefore, our tinySNN effectively compresses the given SNN\nmodel to achieve high accuracy in a memory- and energy-efficient manner, hence\nenabling the employment of SNNs for the resource- and energy-constrained\nembedded applications.",
    "descriptor": "\nComments: 9 figures\n",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08656"
  },
  {
    "id": "arXiv:2206.08657",
    "title": "Bridge-Tower: Building Bridges Between Encoders in Vision-Language  Representation Learning",
    "abstract": "Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a cross-modal encoder, or feed the last-layer\nuni-modal features directly into the top cross-modal encoder, ignoring the\nsemantic information at the different levels in the deep uni-modal encoders.\nBoth approaches possibly restrict vision-language representation learning and\nlimit model performance. In this paper, we introduce multiple bridge layers\nthat build a connection between the top layers of uni-modal encoders and each\nlayer of the cross-modal encoder. This enables comprehensive bottom-up\ninteractions between visual and textual representations at different semantic\nlevels, resulting in more effective cross-modal alignment and fusion. Our\nproposed Bridge-Tower, pre-trained with only $4$M images, achieves\nstate-of-the-art performance on various downstream vision-language tasks. On\nthe VQAv2 test-std set, Bridge-Tower achieves an accuracy of $78.73\\%$,\noutperforming the previous state-of-the-art METER model by $1.09\\%$ with the\nsame pre-training data and almost no additional parameters and computational\ncost. Notably, when further scaling the model, Bridge-Tower achieves an\naccuracy of $81.15\\%$, surpassing models that are pre-trained on\norders-of-magnitude larger datasets. Code is available at\nhttps://github.com/microsoft/BridgeTower.",
    "descriptor": "",
    "authors": [
      "Xiao Xu",
      "Chenfei Wu",
      "Shachar Rosenman",
      "Vasudev Lal",
      "Nan Duan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08657"
  },
  {
    "id": "arXiv:2206.08659",
    "title": "Digital Twin Data Modelling by Randomized Orthogonal Decomposition and  Deep Learning",
    "abstract": "A digital twin is a surrogate model that has the main feature to mirror the\noriginal process behavior. Associating the dynamical process with a digital\ntwin model of reduced complexity has the significant advantage to map the\ndynamics with high accuracy and reduced costs in CPU time and hardware to\ntimescales over which that suffers significantly changes and so it is difficult\nto explore. This paper introduces a new framework for creating efficient\ndigital twin models of fluid flows. We introduce a novel algorithm that\ncombines the advantages of Krylov based dynamic mode decomposition with proper\northogonal decomposition and outperforms the selection of the most influential\nmodes. We prove that randomized orthogonal decomposition algorithm provides\nseveral advantages over SVD empirical orthogonal decomposition methods and\nmitigates the projection error formulating a multiobjective optimization\nproblem.We involve the state-of-the-art artificial intelligence Deep Learning\n(DL) to perform a real-time adaptive calibration of the digital twin model,\nwith increasing fidelity. The output is a high-fidelity DIGITAL TWIN DATA MODEL\nof the fluid flow dynamics, with the advantage of a reduced complexity. The new\nmodelling tools are investigated in the numerical simulation of three wave\nphenomena with increasing complexity. We show that the outputs are consistent\nwith the original source data.We perform a thorough assessment of the\nperformance of the new digital twin data models, in terms of numerical accuracy\nand computational efficiency, including a time simulation response feature\nstudy.",
    "descriptor": "",
    "authors": [
      "Diana Alina Bistrian",
      "Omer San",
      "Ionel Michael Navon"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08659"
  },
  {
    "id": "arXiv:2206.08660",
    "title": "Efficient Raycasting of View-Dependent Piecewise Constant  Representations of Volumetric Data",
    "abstract": "We present an efficient raycasting-based rendering algorithm for\nview-dependent piecewise constant representations of volumetric data. Our\nalgorithm leverages the properties of perspective projection to simplify\nintersections of rays with the view-dependent frustums that form part of these\nrepresentations. It also leverages spatial homogeneity in the underlying volume\ndata to minimize memory accesses. We further introduce techniques for skipping\nempty-space and for dynamic subsampling for accelerated approximate renderings\nat controlled frame rates. Benchmarks show that responsive frame rates can be\nachieved close to the viewpoint of generation for HD display resolutions, while\nproviding high-fidelity approximate renderings of Gigabyte-sized volumes.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Aryaman Gupta",
      "Ulrik G\u00fcnther",
      "Pietro Incardona",
      "Guido Reina",
      "Steffen Frey",
      "Stefan Gumhold",
      "Ivo F. Sbalzarini"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.08660"
  },
  {
    "id": "arXiv:2206.08661",
    "title": "Boosting Factorization Machines via Saliency-Guided Mixup",
    "abstract": "Factorization machines (FMs) are widely used in recommender systems due to\ntheir adaptability and ability to learn from sparse data. However, for the\nubiquitous non-interactive features in sparse data, existing FMs can only\nestimate the parameters corresponding to these features via the inner product\nof their embeddings. Undeniably, they cannot learn the direct interactions of\nthese features, which limits the model's expressive power. To this end, we\nfirst present MixFM, inspired by Mixup, to generate auxiliary training data to\nboost FMs. Unlike existing augmentation strategies that require labor costs and\nexpertise to collect additional information such as position and fields, these\nextra data generated by MixFM only by the convex combination of the raw ones\nwithout any professional knowledge support. More importantly, if the parent\nsamples to be mixed have non-interactive features, MixFM will establish their\ndirect interactions. Second, considering that MixFM may generate redundant or\neven detrimental instances, we further put forward a novel Factorization\nMachine powered by Saliency-guided Mixup (denoted as SMFM). Guided by the\ncustomized saliency, SMFM can generate more informative neighbor data. Through\ntheoretical analysis, we prove that the proposed methods minimize the upper\nbound of the generalization error, which hold a beneficial effect on enhancing\nFMs. Significantly, we give the first generalization bound of FM, implying the\ngeneralization requires more data and a smaller embedding size under the\nsufficient representation capability. Finally, extensive experiments on five\ndatasets confirm that our approaches are superior to baselines. Besides, the\nresults show that \"poisoning\" mixed data is likewise beneficial to the FM\nvariants.",
    "descriptor": "",
    "authors": [
      "Chenwang Wu",
      "Defu Lian",
      "Yong Ge",
      "Min Zhou",
      "Enhong Chen",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08661"
  },
  {
    "id": "arXiv:2206.08662",
    "title": "PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile  Devices",
    "abstract": "Recent researches in artificial intelligence have proposed versatile\nconvolutional neural networks (CNN) with different structures and substantially\nimproved the accuracy of various intelligent applications. Nevertheless, the\nCNN inference imposes heavy computation overhead on mobile devices, but\nuploading the large volume of raw data to the cloud causes significant network\nlatency. Motivated by the spatial independence of convolution operation, we\npropose pipeline cooperation (PICO) framework to accelerate CNN inference using\nmultiple diverse mobile devices in this paper. PICO divides the CNN and mobile\ndevices into several stages and combines them into an inference pipeline. PICO\nfaces three main challenges: (1) Parallelizing the convolution operation\nintroduces redundant calculation. (2) The partition is greatly complicated\nsince the structures of many CNNs are directed acyclic graphs (DAG). (3) The\nmobile devices own diverse computing resources. In response to these issues, a\ntwo-step optimization is proposed based on deep analysis. We first orchestrate\nthe DAG into sequential pieces, then divides these pieces and devices into\nstages. The optimization goal is to minimize the redundant calculation during\npartition and maximize the throughput. In our experiment with $2 \\sim 8$\nRaspberryPi devices, the throughput can be improved by $1.8 \\sim 6.8 \\times$\nunder different CPU frequencies.",
    "descriptor": "\nComments: Under Review of an IEEE journal\n",
    "authors": [
      "Xiang Yang",
      "Zikang Xu",
      "Qi Qi",
      "Jingyu Wang",
      "Haifeng Sun",
      "Jianxin Liao",
      "Song Guo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.08662"
  },
  {
    "id": "arXiv:2206.08669",
    "title": "VGSwarm: A Vision-based Gene Regulation Network for UAVs Swarm Behavior  Emergence",
    "abstract": "UAVs (Unmanned Aerial Vehicles) dynamic encirclement is an emerging field\nwith great potential. Researchers often get inspirations from biological\nsystems, either from macro-world like fish schools or bird flocks etc, or from\nmicro-world like gene regulatory networks. However, most swarm control\nalgorithms rely on centralized control, global information acquisition, or\ncommunication between neighboring agents. In this work, we propose a\ndistributed swarm control method based purely on vision without any direct\ncommunications, in which swarm agents of e.g. UAVs can generate an entrapping\npattern to encircle an escaping target of UAV based purly on their installed\nomnidirectional vision sensors. A finite-state-machine describing the behavior\nmodel of each individual drone is also designed so that a swarm of drones can\naccomplish searching and entrapping of the target collectively. We verify the\neffectiveness and efficiency of the proposed method in various simulation and\nreal-world experiments.",
    "descriptor": "",
    "authors": [
      "Yuwei Cai",
      "Huanlin Li",
      "Zhun Fan",
      "Juncao Hong",
      "Peng Xu",
      "Hui Cheng",
      "Xiaomi Zhu",
      "Bingliang Hu",
      "Zhifeng Hao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08669"
  },
  {
    "id": "arXiv:2206.08672",
    "title": "A Deep Learning Approach for the Segmentation of Electroencephalography  Data in Eye Tracking Applications",
    "abstract": "The collection of eye gaze information provides a window into many critical\naspects of human cognition, health and behaviour. Additionally, many\nneuroscientific studies complement the behavioural information gained from eye\ntracking with the high temporal resolution and neurophysiological markers\nprovided by electroencephalography (EEG). One of the essential eye-tracking\nsoftware processing steps is the segmentation of the continuous data stream\ninto events relevant to eye-tracking applications, such as saccades, fixations,\nand blinks.\nHere, we introduce DETRtime, a novel framework for time-series segmentation\nthat creates ocular event detectors that do not require additionally recorded\neye-tracking modality and rely solely on EEG data. Our end-to-end deep\nlearning-based framework brings recent advances in Computer Vision to the\nforefront of the times series segmentation of EEG data. DETRtime achieves\nstate-of-the-art performance in ocular event detection across diverse\neye-tracking experiment paradigms. In addition to that, we provide evidence\nthat our model generalizes well in the task of EEG sleep stage segmentation.",
    "descriptor": "\nComments: 21 pages, Published at the Proceedings of the 39th International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Lukas Wolf",
      "Ard Kastrati",
      "Martyna Beata P\u0142omecka",
      "Jie-Ming Li",
      "Dustin Klebe",
      "Alexander Veicht",
      "Roger Wattenhofer",
      "Nicolas Langer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08672"
  },
  {
    "id": "arXiv:2206.08673",
    "title": "A Quantitative and Qualitative Analysis of Suicide Ideation Detection  using Deep Learning",
    "abstract": "For preventing youth suicide, social media platforms have received much\nattention from researchers. A few researches apply machine learning, or deep\nlearning-based text classification approaches to classify social media posts\ncontaining suicidality risk. This paper replicated competitive social\nmedia-based suicidality detection/prediction models. We evaluated the\nfeasibility of detecting suicidal ideation using multiple datasets and\ndifferent state-of-the-art deep learning models, RNN-, CNN-, and\nAttention-based models. Using two suicidality evaluation datasets, we evaluated\n28 combinations of 7 input embeddings with 4 commonly used deep learning models\nand 5 pretrained language models in quantitative and qualitative ways. Our\nreplication study confirms that deep learning works well for social media-based\nsuicidality detection in general, but it highly depends on the dataset's\nquality.",
    "descriptor": "\nComments: Accepted in HealTAC 2022\n",
    "authors": [
      "Siqu Long",
      "Rina Cabral",
      "Josiah Poon",
      "Soyeon Caren Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08673"
  },
  {
    "id": "arXiv:2206.08675",
    "title": "Understanding Robust Overfitting of Adversarial Training and Beyond",
    "abstract": "Robust overfitting widely exists in adversarial training of deep networks.\nThe exact underlying reasons for this are still not completely understood.\nHere, we explore the causes of robust overfitting by comparing the data\ndistribution of \\emph{non-overfit} (weak adversary) and \\emph{overfitted}\n(strong adversary) adversarial training, and observe that the distribution of\nthe adversarial data generated by weak adversary mainly contain small-loss\ndata. However, the adversarial data generated by strong adversary is more\ndiversely distributed on the large-loss data and the small-loss data. Given\nthese observations, we further designed data ablation adversarial training and\nidentify that some small-loss data which are not worthy of the adversary\nstrength cause robust overfitting in the strong adversary mode. To relieve this\nissue, we propose \\emph{minimum loss constrained adversarial training} (MLCAT):\nin a minibatch, we learn large-loss data as usual, and adopt additional\nmeasures to increase the loss of the small-loss data. Technically, MLCAT\nhinders data fitting when they become easy to learn to prevent robust\noverfitting; philosophically, MLCAT reflects the spirit of turning waste into\ntreasure and making the best use of each adversarial data; algorithmically, we\ndesigned two realizations of MLCAT, and extensive experiments demonstrate that\nMLCAT can eliminate robust overfitting and further boost adversarial\nrobustness.",
    "descriptor": "\nComments: ICML2022\n",
    "authors": [
      "Chaojian Yu",
      "Bo Han",
      "Li Shen",
      "Jun Yu",
      "Chen Gong",
      "Mingming Gong",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08675"
  },
  {
    "id": "arXiv:2206.08680",
    "title": "BITS Pilani at HinglishEval: Quality Evaluation for Code-Mixed Hinglish  Text Using Transformers",
    "abstract": "Code-Mixed text data consists of sentences having words or phrases from more\nthan one language. Most multi-lingual communities worldwide communicate using\nmultiple languages, with English usually one of them. Hinglish is a Code-Mixed\ntext composed of Hindi and English but written in Roman script. This paper aims\nto determine the factors influencing the quality of Code-Mixed text data\ngenerated by the system. For the HinglishEval task, the proposed model uses\nmulti-lingual BERT to find the similarity between synthetically generated and\nhuman-generated sentences to predict the quality of synthetically generated\nHinglish sentences.",
    "descriptor": "",
    "authors": [
      "Shaz Furniturewala",
      "Vijay Kumari",
      "Amulya Ratna Dash",
      "Hriday Kedia",
      "Yashvardhan Sharma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08680"
  },
  {
    "id": "arXiv:2206.08683",
    "title": "AggNet: Learning to Aggregate Faces for Group Membership Verification",
    "abstract": "In some face recognition applications, we are interested to verify whether an\nindividual is a member of a group, without revealing their identity. Some\nexisting methods, propose a mechanism for quantizing precomputed face\ndescriptors into discrete embeddings and aggregating them into one group\nrepresentation. However, this mechanism is only optimized for a given closed\nset of individuals and needs to learn the group representations from scratch\nevery time the groups are changed. In this paper, we propose a deep\narchitecture that jointly learns face descriptors and the aggregation mechanism\nfor better end-to-end performances. The system can be applied to new groups\nwith individuals never seen before and the scheme easily manages new\nmemberships or membership endings. We show through experiments on multiple\nlarge-scale wild-face datasets, that the proposed method leads to higher\nverification performance compared to other baselines.",
    "descriptor": "",
    "authors": [
      "Marzieh Gheisari",
      "Javad Amirian",
      "Teddy Furon",
      "Laurent Amsaleg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08683"
  },
  {
    "id": "arXiv:2206.08684",
    "title": "Sparse Double Descent: Where Network Pruning Aggravates Overfitting",
    "abstract": "People usually believe that network pruning not only reduces the\ncomputational cost of deep networks, but also prevents overfitting by\ndecreasing model capacity. However, our work surprisingly discovers that\nnetwork pruning sometimes even aggravates overfitting. We report an unexpected\nsparse double descent phenomenon that, as we increase model sparsity via\nnetwork pruning, test performance first gets worse (due to overfitting), then\ngets better (due to relieved overfitting), and gets worse at last (due to\nforgetting useful information). While recent studies focused on the deep double\ndescent with respect to model overparameterization, they failed to recognize\nthat sparsity may also cause double descent. In this paper, we have three main\ncontributions. First, we report the novel sparse double descent phenomenon\nthrough extensive experiments. Second, for this phenomenon, we propose a novel\nlearning distance interpretation that the curve of $\\ell_{2}$ learning distance\nof sparse models (from initialized parameters to final parameters) may\ncorrelate with the sparse double descent curve well and reflect generalization\nbetter than minima flatness. Third, in the context of sparse double descent, a\nwinning ticket in the lottery ticket hypothesis surprisingly may not always\nwin.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Zheng He",
      "Zeke Xie",
      "Quanzhi Zhu",
      "Zengchang Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08684"
  },
  {
    "id": "arXiv:2206.08686",
    "title": "Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement  Learning",
    "abstract": "Achieving human-level dexterity is an important open problem in robotics.\nHowever, tasks of dexterous hand manipulation, even at the baby level, are\nchallenging to solve through reinforcement learning (RL). The difficulty lies\nin the high degrees of freedom and the required cooperation among heterogeneous\nagents (e.g., joints of fingers). In this study, we propose the Bimanual\nDexterous Hands Benchmark (Bi-DexHands), a simulator that involves two\ndexterous hands with tens of bimanual manipulation tasks and thousands of\ntarget objects. Specifically, tasks in Bi-DexHands are designed to match\ndifferent levels of human motor skills according to cognitive science\nliterature. We built Bi-DexHands in the Issac Gym; this enables highly\nefficient RL training, reaching 30,000+ FPS by only one single NVIDIA RTX 3090.\nWe provide a comprehensive benchmark for popular RL algorithms under different\nsettings; this includes Single-agent/Multi-agent RL, Offline RL, Multi-task RL,\nand Meta RL. Our results show that the PPO type of on-policy algorithms can\nmaster simple manipulation tasks that are equivalent up to 48-month human\nbabies (e.g., catching a flying object, opening a bottle), while multi-agent RL\ncan further help to master manipulations that require skilled bimanual\ncooperation (e.g., lifting a pot, stacking blocks). Despite the success on each\nsingle task, when it comes to acquiring multiple manipulation skills, existing\nRL algorithms fail to work in most of the multi-task and the few-shot learning\nsettings, which calls for more substantial development from the RL community.\nOur project is open sourced at https://github.com/PKU-MARL/DexterousHands.",
    "descriptor": "\nComments: 36 pages, 7 figures\n",
    "authors": [
      "Yuanpei Chen",
      "Yaodong Yang",
      "Tianhao Wu",
      "Shengjie Wang",
      "Xidong Feng",
      "Jiechuang Jiang",
      "Stephen Marcus McAleer",
      "Hao Dong",
      "Zongqing Lu",
      "Song-Chun Zhu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.08686"
  },
  {
    "id": "arXiv:2206.08687",
    "title": "You Only Derive Once (YODO): Automatic Differentiation for Efficient  Sensitivity Analysis in Bayesian Networks",
    "abstract": "Sensitivity analysis measures the influence of a Bayesian network's\nparameters on a quantity of interest defined by the network, such as the\nprobability of a variable taking a specific value. In particular, the so-called\nsensitivity value measures the quantity of interest's partial derivative with\nrespect to the network's conditional probabilities. However, finding such\nvalues in large networks with thousands of parameters can become\ncomputationally very expensive. We propose to use automatic differentiation\ncombined with exact inference to obtain all sensitivity values in a single\npass. Our method first marginalizes the whole network once using e.g. variable\nelimination and then backpropagates this operation to obtain the gradient with\nrespect to all input parameters. We demonstrate our routines by ranking all\nparameters by importance on a Bayesian network modeling humanitarian crises and\ndisasters, and then show the method's efficiency by scaling it to huge networks\nwith up to 100'000 parameters. An implementation of the methods using the\npopular machine learning library PyTorch is freely available.",
    "descriptor": "",
    "authors": [
      "Rafael Ballester-Ripoll",
      "Manuele Leonelli"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08687"
  },
  {
    "id": "arXiv:2206.08688",
    "title": "Detecting Connectivity Issues in Android Apps",
    "abstract": "Android is the most popular mobile operating system in the world, running on\nmore than 70% of mobile devices. This implies a gigantic and very competitive\nmarket for Android apps. Being successful in such a market is far from trivial\nand requires, besides the tackling of a problem or need felt by a vast\naudience, the development of high-quality apps. As recently showed in the\nliterature, connectivity issues (e.g., mishandling of zero/unreliable Internet\nconnection) can result in bugs and/or crashes, negatively affecting the app's\nuser experience. While these issues have been studied in the literature, there\nare no techniques able to automatically detect and report them to developers.\nWe present CONAN, a tool able to detect statically 16 types of connectivity\nissues affecting Android apps. We assessed the ability of CONAN to precisely\nidentify these issues in a set of 44 open source apps, observing an average\nprecision of 80%. Then, we studied the relevance of these issues for developers\nby (i) conducting interviews with six practitioners working with commercial\nAndroid apps, and (ii) submitting 84 issue reports for 27 open source apps. Our\nresults show that several of the identified connectivity issues are considered\nas relevant by practitioners in specific contexts, in which connectivity is\nconsidered a first-class feature.",
    "descriptor": "\nComments: Accepted for publication in SANER 2022\n",
    "authors": [
      "Alejandro Mazuera-Rozo",
      "Camilo Escobar-Vel\u00e1squez",
      "Juan Espitia-Acero",
      "Mario Linares-V\u00e1squez",
      "Gabriele Bavota"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.08688"
  },
  {
    "id": "arXiv:2206.08697",
    "title": "Reviewer Preferences and Gender Disparities in Aesthetic Judgments",
    "abstract": "Aesthetic preferences are considered highly subjective resulting in\ninherently noisy judgements of aesthetic objects, yet certain aspects of\naesthetic judgement display convergent trends over time. This paper present a\nstudy that uses literary reviews as a proxy for aesthetic judgement in order to\nidentify systematic components that can be attributed to bias. Specifically we\nfind that judgement of literary quality in newspapers displays a gender bias in\npreference of male writers. Male reviewers have a same gender preference while\nfemale reviewer show an opposite gender preference. While alternative accounts\nexist of this apparent gender disparity, we argue that it reflects a cultural\ngender antagonism.",
    "descriptor": "",
    "authors": [
      "Ida Marie Schytt Lassen",
      "Yuri Bizzoni",
      "Telam Peura",
      "Mads Rosendahl Thomsen",
      "Kristoffer Laigaard Nielbo"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08697"
  },
  {
    "id": "arXiv:2206.08698",
    "title": "Towards computing complete parameter ranges in parametric modeling",
    "abstract": "In parametric design, the geometric model is edited by changing relevant\nparameters in the parametric model, which is commonly done sequentially on\nmultiple parameters. Without guidance on allowable parameter ranges that can\nguarantee the solvability of the geometric constraint system, the user could\nassign improper parameter values to the model's parameters, which would further\nlead to a failure in model updating. However, current commercial CAD systems\nprovide little support for the proper parameter assignments. Although the\nexisting methods can compute allowable ranges for individual parameters, they\nface difficulties in handling multi-parameter situations. In particular, these\nmethods could miss some feasible parameter values and provide incomplete\nallowable parameter ranges. To solve this problem, an automatic approach is\nproposed in this paper to compute complete parameter ranges in multi-parameter\nediting. In the approach, a set of variable parameters are first selected to be\nsequentially edited by the user; before each editing operation, the\none-dimensional ranges of the variable parameters are presented as guidance. To\ncompute the one-dimensional ranges, each variable parameter is expressed as an\nequality-constrained function, and its one-dimensional allowable range is\nobtained by calculating the function range. To effectively obtain the function\nrange which can hardly be calculated in a normal way, the function range\nproblem is converted into a constrained optimization problem, and is then\nsolved by Lagrange multiplier method and the Niching particle swarm\noptimization algorithm (the NichePSO). The effectiveness and efficiency of the\nproposed approach is verified by several experimental results.",
    "descriptor": "",
    "authors": [
      "Zhihong Tang",
      "Qiang Zou",
      "Shuming Gao"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.08698"
  },
  {
    "id": "arXiv:2206.08701",
    "title": "Towards Real-Time Visual Tracking with Graded Color-names Features",
    "abstract": "MeanShift algorithm has been widely used in tracking tasks because of its\nsimplicity and efficiency. However, the traditional MeanShift algorithm needs\nto label the initial region of the target, which reduces the applicability of\nthe algorithm. Furthermore, it is only applicable to the scene with a large\noverlap rate between the target area and the candidate area. Therefore, when\nthe target speed is fast, the target scale change, shape deformation or the\ntarget occlusion occurs, the tracking performance will be deteriorated. In this\npaper, we address the challenges above-mentioned by developing a tracking\nmethod that combines the background models and the graded features of\ncolor-names under the MeanShift framework. This method significantly improve\nperformance in the above scenarios. In addition, it facilitates the balance\nbetween detection accuracy and detection speed. Experimental results\ndemonstrate the validation of the proposed method.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Lin Li",
      "Guoli Wang",
      "Xuemei Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08701"
  },
  {
    "id": "arXiv:2206.08702",
    "title": "Sheaf Neural Networks with Connection Laplacians",
    "abstract": "A Sheaf Neural Network (SNN) is a type of Graph Neural Network (GNN) that\noperates on a sheaf, an object that equips a graph with vector spaces over its\nnodes and edges and linear maps between these spaces. SNNs have been shown to\nhave useful theoretical properties that help tackle issues arising from\nheterophily and over-smoothing. One complication intrinsic to these models is\nfinding a good sheaf for the task to be solved. Previous works proposed two\ndiametrically opposed approaches: manually constructing the sheaf based on\ndomain knowledge and learning the sheaf end-to-end using gradient-based\nmethods. However, domain knowledge is often insufficient, while learning a\nsheaf could lead to overfitting and significant computational overhead. In this\nwork, we propose a novel way of computing sheaves drawing inspiration from\nRiemannian geometry: we leverage the manifold assumption to compute\nmanifold-and-graph-aware orthogonal maps, which optimally align the tangent\nspaces of neighbouring data points. We show that this approach achieves\npromising results with less computational overhead when compared to previous\nSNN models. Overall, this work provides an interesting connection between\nalgebraic topology and differential geometry, and we hope that it will spark\nfuture research in this direction.",
    "descriptor": "\nComments: Presented at the ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning\n",
    "authors": [
      "Federico Barbero",
      "Cristian Bodnar",
      "Haitz S\u00e1ez de Oc\u00e1riz Borde",
      "Michael Bronstein",
      "Petar Veli\u010dkovi\u0107",
      "Pietro Li\u00f2"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2206.08702"
  },
  {
    "id": "arXiv:2206.08703",
    "title": "Plotly-Resampler: Effective Visual Analytics for Large Time Series",
    "abstract": "Visual analytics is arguably the most important step in getting acquainted\nwith your data. This is especially the case for time series, as this data type\nis hard to describe and cannot be fully understood when using for example\nsummary statistics. To realize effective time series visualization, four\nrequirements have to be met; a tool should be (1) interactive, (2) scalable to\nmillions of data points, (3) integrable in conventional data science\nenvironments, and (4) highly configurable. We observe that open source Python\nvisualization toolkits empower data scientists in most visual analytics tasks,\nbut lack the combination of scalability and interactivity to realize effective\ntime series visualization. As a means to facilitate these requirements, we\ncreated Plotly-Resampler, an open source Python library. Plotly-Resampler is an\nadd-on for Plotly's Python bindings, enhancing line chart scalability on top of\nan interactive toolkit by aggregating the underlying data depending on the\ncurrent graph view. Plotly-Resampler is built to be snappy, as the reactivity\nof a tool qualitatively affects how analysts visually explore and analyze data.\nA benchmark task highlights how our toolkit scales better than alternatives in\nterms of number of samples and time series. Additionally, Plotly-Resampler's\nflexible data aggregation functionality paves the path towards researching\nnovel aggregation techniques. Plotly-Resampler's integrability, together with\nits configurability, convenience, and high scalability, allows to effectively\nanalyze high-frequency data in your day-to-day Python environment.",
    "descriptor": "\nComments: The first two authors contributed equally. Submitted to IEEE VIS 2022\n",
    "authors": [
      "Jonas Van Der Donckt",
      "Jeroen Van Der Donckt",
      "Emiel Deprost",
      "Sofie Van Hoecke"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08703"
  },
  {
    "id": "arXiv:2206.08704",
    "title": "Maximum Class Separation as Inductive Bias in One Matrix",
    "abstract": "Maximizing the separation between classes constitutes a well-known inductive\nbias in machine learning and a pillar of many traditional algorithms. By\ndefault, deep networks are not equipped with this inductive bias and therefore\nmany alternative solutions have been proposed through differential\noptimization. Current approaches tend to optimize classification and separation\njointly: aligning inputs with class vectors and separating class vectors\nangularly. This paper proposes a simple alternative: encoding maximum\nseparation as an inductive bias in the network by adding one fixed matrix\nmultiplication before computing the softmax activations. The main observation\nbehind our approach is that separation does not require optimization but can be\nsolved in closed-form prior to training and plugged into a network. We outline\na recursive approach to obtain the matrix consisting of maximally separable\nvectors for any number of classes, which can be added with negligible\nengineering effort and computational overhead. Despite its simple nature, this\none matrix multiplication provides real impact. We show that our proposal\ndirectly boosts classification, long-tailed recognition, out-of-distribution\ndetection, and open-set recognition, from CIFAR to ImageNet. We find\nempirically that maximum separation works best as a fixed bias; making the\nmatrix learnable adds nothing to the performance. The closed-form\nimplementation and code to reproduce the experiments are on github.",
    "descriptor": "",
    "authors": [
      "Tejaswi Kasarla",
      "Gertjan J. Burghouts",
      "Max van Spengler",
      "Elise van der Pol",
      "Rita Cucchiara",
      "Pascal Mettes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08704"
  },
  {
    "id": "arXiv:2206.08705",
    "title": "Explainability's Gain is Optimality's Loss? -- How Explanations Bias  Decision-making",
    "abstract": "Decisions in organizations are about evaluating alternatives and choosing the\none that would best serve organizational goals. To the extent that the\nevaluation of alternatives could be formulated as a predictive task with\nappropriate metrics, machine learning algorithms are increasingly being used to\nimprove the efficiency of the process. Explanations help to facilitate\ncommunication between the algorithm and the human decision-maker, making it\neasier for the latter to interpret and make decisions on the basis of\npredictions by the former. Feature-based explanations' semantics of causal\nmodels, however, induce leakage from the decision-maker's prior beliefs. Our\nfindings from a field experiment demonstrate empirically how this leads to\nconfirmation bias and disparate impact on the decision-maker's confidence in\nthe predictions. Such differences can lead to sub-optimal and biased decision\noutcomes.",
    "descriptor": "\nComments: To appear in the Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES '22)\n",
    "authors": [
      "Charles Wan",
      "Rodrigo Belo",
      "Leid Zejnilovi\u0107"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08705"
  },
  {
    "id": "arXiv:2206.08707",
    "title": "Environment-Aware Hybrid Beamforming by Leveraging Channel Knowledge Map",
    "abstract": "Hybrid analog/digital beamforming is a promising technique to realize\nmillimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems\ncost-effectively. However, existing hybrid beamforming designs mainly rely on\nreal-time channel training or beam sweeping to find the desired beams, which\nincurs prohibitive overhead due to a large number of antennas at both the\ntransmitter and receiver with only limited radio frequency (RF) chains. To\nresolve this challenging issue, in this paper, we propose a new\nenvironment-aware hybrid beamforming technique that requires only light\nreal-time training, by leveraging the useful tool of channel knowledge map\n(CKM) with the user's location information. CKM is a site-specific database,\nwhich offers location-specific channel-relevant information to facilitate or\neven obviate the acquisition of real-time channel state information (CSI). Two\nspecific types of CKM are proposed in this paper for hybrid beamforming design\nin mmWave massive MIMO systems, namely channel angle map (CAM) and beam index\nmap (BIM). It is shown that compared with existing environment-unaware schemes,\nthe proposed environment-aware hybrid beamforming scheme based on CKM can\ndrastically improve the effective communication rate, even under moderate user\nlocation errors, thanks to its great saving of the prohibitive real-time\ntraining overhead.",
    "descriptor": "",
    "authors": [
      "Di Wu",
      "Yong Zeng",
      "Shi Jin",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08707"
  },
  {
    "id": "arXiv:2206.08709",
    "title": "Statistical and Neural Methods for Cross-lingual Entity Label Mapping in  Knowledge Graphs",
    "abstract": "Knowledge bases such as Wikidata amass vast amounts of named entity\ninformation, such as multilingual labels, which can be extremely useful for\nvarious multilingual and cross-lingual applications. However, such labels are\nnot guaranteed to match across languages from an information consistency\nstandpoint, greatly compromising their usefulness for fields such as machine\ntranslation. In this work, we investigate the application of word and sentence\nalignment techniques coupled with a matching algorithm to align cross-lingual\nentity labels extracted from Wikidata in 10 languages. Our results indicate\nthat mapping between Wikidata's main labels stands to be considerably improved\n(up to $20$ points in F1-score) by any of the employed methods. We show how\nmethods relying on sentence embeddings outperform all others, even across\ndifferent scripts. We believe the application of such techniques to measure the\nsimilarity of label pairs, coupled with a knowledge base rich in high-quality\nentity labels, to be an excellent asset to machine translation.",
    "descriptor": "",
    "authors": [
      "Gabriel Amaral",
      "M\u0101rcis Pinnis",
      "Inguna Skadi\u0146a",
      "Odinaldo Rodrigues",
      "Elena Simperl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08709"
  },
  {
    "id": "arXiv:2206.08712",
    "title": "An Algorithm for the SE(3)-Transformation on Neural Implicit Maps for  Remapping Functions",
    "abstract": "Implicit representations are widely used for object reconstruction due to\ntheir efficiency and flexibility. In 2021, a novel structure named neural\nimplicit map has been invented for incremental reconstruction. A neural\nimplicit map alleviates the problem of inefficient memory cost of previous\nonline 3D dense reconstruction while producing better quality. % However, the\nneural implicit map suffers the limitation that it does not support remapping\nas the frames of scans are encoded into a deep prior after generating the\nneural implicit map. This means, that neither this generation process is\ninvertible, nor a deep prior is transformable. The non-remappable property\nmakes it not possible to apply loop-closure techniques. % We present a neural\nimplicit map based transformation algorithm to fill this gap. As our neural\nimplicit map is transformable, our model supports remapping for this special\nmap of latent features. % Experiments show that our remapping module is capable\nto well-transform neural implicit maps to new poses. Embedded into a SLAM\nframework, our mapping model is able to tackle the remapping of loop closures\nand demonstrates high-quality surface reconstruction. % Our implementation is\navailable at github\\footnote{\\url{https://github.com/Jarrome/IMT_Mapping}} for\nthe research community.",
    "descriptor": "\nComments: Accepted to RAL2022, code at this https URL\n",
    "authors": [
      "Yijun Yuan",
      "Andreas Nuechter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08712"
  },
  {
    "id": "arXiv:2206.08713",
    "title": "Evaluating the Impact of Source Code Parsers on ML4SE Models",
    "abstract": "As researchers and practitioners apply Machine Learning to increasingly more\nsoftware engineering problems, the approaches they use become more\nsophisticated. A lot of modern approaches utilize internal code structure in\nthe form of an abstract syntax tree (AST) or its extensions: path-based\nrepresentation, complex graph combining AST with additional edges. Even though\nthe process of extracting ASTs from code can be done with different parsers,\nthe impact of choosing a parser on the final model quality remains unstudied.\nMoreover, researchers often omit the exact details of extracting particular\ncode representations.\nIn this work, we evaluate two models, namely Code2Seq and TreeLSTM, in the\nmethod name prediction task backed by eight different parsers for the Java\nlanguage. To unify the process of data preparation with different parsers, we\ndevelop SuperParser, a multi-language parser-agnostic library based on\nPathMiner. SuperParser facilitates the end-to-end creation of datasets suitable\nfor training and evaluation of ML models that work with structural information\nfrom source code. Our results demonstrate that trees built by different parsers\nvary in their structure and content. We then analyze how this diversity affects\nthe models' quality and show that the quality gap between the most and least\nsuitable parsers for both models turns out to be significant. Finally, we\ndiscuss other features of the parsers that researchers and practitioners should\ntake into account when selecting a parser along with the impact on the models'\nquality.\nThe code of SuperParser is publicly available at\nhttps://doi.org/10.5281/zenodo.6366591. We also publish Java-norm, the dataset\nwe use to evaluate the models: https://doi.org/10.5281/zenodo.6366599.",
    "descriptor": "\nComments: 12 pages, 3 figures\n",
    "authors": [
      "Ilya Utkin",
      "Egor Spirin",
      "Egor Bogomolov",
      "Timofey Bryksin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08713"
  },
  {
    "id": "arXiv:2206.08714",
    "title": "Relaxing safety for metric first-order temporal logic via dynamic free  variables",
    "abstract": "We define a fragment of metric first-order temporal logic formulas that\nguarantees the finiteness of their table representations. We extend our\nfragment's definition to cover the temporal dual operators trigger and release\nand show that our fragment is strictly larger than those previously used in the\nliterature. We integrate these additions into an existing runtime verification\ntool and formally verify in Isabelle/HOL that the tool correctly outputs the\ntable of constants that satisfy the monitored formula. Finally, we provide some\nexample specifications that are now monitorable thanks to our contributions.",
    "descriptor": "\nComments: 12 pages, conference, appendix\n",
    "authors": [
      "Jonathan Julian Huerta y Munive"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08714"
  },
  {
    "id": "arXiv:2206.08718",
    "title": "CATTO: Just-in-time Test Case Selection and Execution",
    "abstract": "Regression testing ensures a System Under Test (SUT) still works as expected\nafter changes to it. The simplest approach for regression testing consists of\nre-running the entire test suite against the changed version of the SUT.\nHowever, this might result in a time- and resource-consuming process; \\eg when\ndealing with large and/or complex SUTs and test suits. To work around this\nproblem, test Case Selection (TCS) strategies can be used. Such strategies seek\nto build a temporary test suite comprising only those test cases that are\nrelevant to the changes made to the SUT, so avoiding executing those test cases\nthat do not exercise the changed parts. In this paper, we introduce CATTO\n(Commit Adaptive Tool for Test suite Optimization) and CATTO INTELLIJ PLUGIN.\nThe former is a tool implementing a TCS strategy for SUTs written in Java,\nwhile the latter is a wrapper to allow developers to use \\toolName directly in\nIntelliJ. We also conducted a preliminary evaluation of CATTO on seven\nopen-source Java SUTs in terms of reductions in test-suite size, fault-reveling\ntest cases, and fault-detection capability. The results are promising and\nsuggest that CATTO can be of help to developers when performing regression\ntesting. The video demo and the documentation of the tool is available at:\n\\url{https://catto-tool.github.io/}",
    "descriptor": "",
    "authors": [
      "Dario Amoroso d'Aragona",
      "Fabiano Pecorelli",
      "Simone Romano",
      "Giuseppe Scanniello",
      "Maria Teresa Baldassarre",
      "Andrea Janes",
      "Valentina Lenarduzzi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.08718"
  },
  {
    "id": "arXiv:2206.08720",
    "title": "Fast Finite Width Neural Tangent Kernel",
    "abstract": "The Neural Tangent Kernel (NTK), defined as $\\Theta_\\theta^f(x_1, x_2) =\n\\left[\\partial f(\\theta, x_1)\\big/\\partial \\theta\\right] \\left[\\partial\nf(\\theta, x_2)\\big/\\partial \\theta\\right]^T$ where $\\left[\\partial f(\\theta,\n\\cdot)\\big/\\partial \\theta\\right]$ is a neural network (NN) Jacobian, has\nemerged as a central object of study in deep learning. In the infinite width\nlimit, the NTK can sometimes be computed analytically and is useful for\nunderstanding training and generalization of NN architectures. At finite\nwidths, the NTK is also used to better initialize NNs, compare the conditioning\nacross models, perform architecture search, and do meta-learning.\nUnfortunately, the finite width NTK is notoriously expensive to compute, which\nseverely limits its practical utility. We perform the first in-depth analysis\nof the compute and memory requirements for NTK computation in finite width\nnetworks. Leveraging the structure of neural networks, we further propose two\nnovel algorithms that change the exponent of the compute and memory\nrequirements of the finite width NTK, dramatically improving efficiency. Our\nalgorithms can be applied in a black box fashion to any differentiable\nfunction, including those implementing neural networks. We open-source our\nimplementations within the Neural Tangents package (arXiv:1912.02803) at\nhttps://github.com/google/neural-tangents.",
    "descriptor": "\nComments: Published as a conference paper at ICML 2022\n",
    "authors": [
      "Roman Novak",
      "Jascha Sohl-Dickstein",
      "Samuel S. Schoenholz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08720"
  },
  {
    "id": "arXiv:2206.08722",
    "title": "WaTZ: A Trusted WebAssembly Runtime Environment with Remote Attestation  for TrustZone",
    "abstract": "WebAssembly (Wasm) is a novel low-level bytecode format that swiftly gained\npopularity for its efficiency, versatility and security, with near-native\nperformance. Besides, trusted execution environments (TEEs) shield critical\nsoftware assets against compromised infrastructures. However, TEEs do not\nguarantee the code to be trustworthy or that it was not tampered with. Instead,\none relies on remote attestation to assess the code before execution. This\npaper describes WaTZ, which is (i) an efficient and secure runtime for trusted\nexecution of Wasm code for Arm's TrustZone TEE, and (ii) a lightweight remote\nattestation system optimised for Wasm applications running in TrustZone, as it\nlacks built-in mechanisms for attestation. The remote attestation protocol is\nformally verified using a state-of-the-art analyser and model checker. Our\nextensive evaluation of Arm-based hardware uses synthetic and real-world\nbenchmarks, illustrating typical tasks IoT devices achieve. WaTZ's execution\nspeed is on par with Wasm runtimes in the normal world and reaches roughly half\nthe speed of native execution, which is compensated by the additional security\nguarantees and the interoperability offered by Wasm. WaTZ is open-source and\navailable on GitHub along with instructions to reproduce our experiments.",
    "descriptor": "\nComments: This publication incorporates results from the VEDLIoT project, which received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 957197\n",
    "authors": [
      "J\u00e4mes M\u00e9n\u00e9trey",
      "Marcelo Pasin",
      "Pascal Felber",
      "Valerio Schiavoni"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2206.08722"
  },
  {
    "id": "arXiv:2206.08723",
    "title": "CookDial: A dataset for task-oriented dialogs grounded in procedural  documents",
    "abstract": "This work presents a new dialog dataset, CookDial, that facilitates research\non task-oriented dialog systems with procedural knowledge understanding. The\ncorpus contains 260 human-to-human task-oriented dialogs in which an agent,\ngiven a recipe document, guides the user to cook a dish. Dialogs in CookDial\nexhibit two unique features: (i) procedural alignment between the dialog flow\nand supporting document; (ii) complex agent decision-making that involves\nsegmenting long sentences, paraphrasing hard instructions and resolving\ncoreference in the dialog context. In addition, we identify three challenging\n(sub)tasks in the assumed task-oriented dialog system: (1) User Question\nUnderstanding, (2) Agent Action Frame Prediction, and (3) Agent Response\nGeneration. For each of these tasks, we develop a neural baseline model, which\nwe evaluate on the CookDial dataset. We publicly release the CookDial dataset,\ncomprising rich annotations of both dialogs and recipe documents, to stimulate\nfurther research on domain-specific document-grounded dialog systems.",
    "descriptor": "\nComments: The dataset and codes are available at this https URL\n",
    "authors": [
      "Yiwei Jiang",
      "Klim Zaporojets",
      "Johannes Deleu",
      "Thomas Demeester",
      "Chris Develder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08723"
  },
  {
    "id": "arXiv:2206.08724",
    "title": "Crowdsourcing Relative Rankings of Multi-Word Expressions: Experts  versus Non-Experts",
    "abstract": "In this study we investigate to which degree experts and non-experts agree on\nquestions of difficulty in a crowdsourcing experiment. We ask non-experts\n(second language learners of Swedish) and two groups of experts (teachers of\nSwedish as a second/foreign language and CEFR experts) to rank multi-word\nexpressions in a crowdsourcing experiment. We find that the resulting rankings\nby all the three tested groups correlate to a very high degree, which suggests\nthat judgments produced in a comparative setting are not influenced by\nprofessional insights into Swedish as a second language.",
    "descriptor": "",
    "authors": [
      "David Alfter",
      "Therese Lindstr\u00f6m Tiedemann",
      "Elena Volodina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08724"
  },
  {
    "id": "arXiv:2206.08725",
    "title": "Galois LCD Codes Over Fq + uFq + vFq + uvFq",
    "abstract": "In \\cite{anote}, Wu and Shi studied $ l $-Galois LCD codes over finite chain\nring $\\mathcal{R}=\\mathbb{F}_q+u\\mathbb{F}_q$, where $u^2=0$ and $ q=p^e$ for\nsome prime $p$ and positive integer $e$. In this work, we extend the results to\nthe finite non chain ring $ \\mathcal{R}\n=\\mathbb{F}_q+u\\mathbb{F}_q+v\\mathbb{F}_q+uv\\mathbb{F}_q$, where $u^2=u,v^2=v $\nand $ uv=vu $. We define a correspondence between $ l $-Galois dual of linear\ncodes over $ \\mathcal{R} $ and $ l $-Galois dual of its component codes over $\n\\mathbb{F}_q .$ Further, we construct Euclidean LCD and $ l $-Galois LCD codes\nfrom linear code over $ \\mathcal{R} $. This consequently leads us to prove that\nany linear code over $ \\mathcal{R} $ is equivalent to Euclidean ($ q>3 $) and $\nl $-Galois LCD ($0<l<e$, and $p^{e-l}+1\\mid p^e-1$) code over $ \\mathcal{R} .$\nFinally, we investigate MDS codes over $ \\mathcal{R} .$",
    "descriptor": "",
    "authors": [
      "Astha Agrawal",
      "Gyanendra K. Verma",
      "R. K. Sharma"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.08725"
  },
  {
    "id": "arXiv:2206.08726",
    "title": "Evaluation of Contrastive Learning with Various Code Representations for  Code Clone Detection",
    "abstract": "Code clones are pairs of code snippets that implement similar functionality.\nClone detection is a fundamental branch of automatic source code comprehension,\nhaving many applications in refactoring recommendation, plagiarism detection,\nand code summarization. A particularly interesting case of clone detection is\nthe detection of semantic clones, i.e., code snippets that have the same\nfunctionality but significantly differ in implementation. A promising approach\nto detecting semantic clones is contrastive learning (CL), a machine learning\nparadigm popular in computer vision but not yet commonly adopted for code\nprocessing.\nOur work aims to evaluate the most popular CL algorithms combined with three\nsource code representations on two tasks. The first task is code clone\ndetection, which we evaluate on the POJ-104 dataset containing implementations\nof 104 algorithms. The second task is plagiarism detection. To evaluate the\nmodels on this task, we introduce CodeTransformator, a tool for transforming\nsource code. We use it to create a dataset that mimics plagiarised code based\non competitive programming solutions. We trained nine models for both tasks and\ncompared them with six existing approaches, including traditional tools and\nmodern pre-trained neural models. The results of our evaluation show that\nproposed models perform diversely in each task, however the performance of the\ngraph-based models is generally above the others. Among CL algorithms, SimCLR\nand SwAV lead to better results, while Moco is the most robust approach. Our\ncode and trained models are available at\nhttps://doi.org/10.5281/zenodo.6360627, https://doi.org/10.5281/zenodo.5596345.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "Maksim Zubkov",
      "Egor Spirin",
      "Egor Bogomolov",
      "Timofey Bryksin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08726"
  },
  {
    "id": "arXiv:2206.08727",
    "title": "The ITU Faroese Pairs Dataset",
    "abstract": "This article documents a dataset of sentence pairs between Faroese and\nDanish, produced at ITU Copenhagen. The data covers tranlsation from both\nsource languages, and is intended for use as training data for machine\ntranslation systems in this language pair.",
    "descriptor": "",
    "authors": [
      "Leon Derczynski",
      "Annika Solveig Hedegaard Isfeldt",
      "Signhild Djurhuus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08727"
  },
  {
    "id": "arXiv:2206.08733",
    "title": "Efficient WiFi LiDAR SLAM for Autonomous Robots in Large Environments",
    "abstract": "Autonomous robots operating in indoor and GPS denied environments can use\nLiDAR for SLAM instead. However, LiDARs do not perform well in\ngeometrically-degraded environments, due to the challenge of loop closure\ndetection and computational load to perform scan matching. Existing WiFi\ninfrastructure can be exploited for localization and mapping with low hardware\nand computational cost. Yet, accurate pose estimation using WiFi is challenging\nas different signal values can be measured at the same location due to the\nunpredictability of signal propagation. Therefore, we introduce the use of WiFi\nfingerprint sequence for pose estimation (i.e. loop closure) in SLAM. This\napproach exploits the spatial coherence of location fingerprints obtained while\na mobile robot is moving. This has better capability of correcting odometry\ndrift. The method also incorporates LiDAR scans and thus, improving\ncomputational efficiency for large and geometrically-degraded environments\nwhile maintaining the accuracy of LiDAR SLAM. We conducted experiments in an\nindoor environment to illustrate the effectiveness of the method. The results\nare evaluated based on Root Mean Square Error (RMSE) and it has achieved an\naccuracy of 0.88m for the test environment.",
    "descriptor": "\nComments: accepted by the 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)\n",
    "authors": [
      "Khairuldanial Ismail",
      "Ran Liu",
      "Zhenghong Qin",
      "Achala Athukorala",
      "Billy Pik Lik Lau",
      "Muhammad Shalihan",
      "Chau Yuen",
      "U-Xuan Tan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08733"
  },
  {
    "id": "arXiv:2206.08735",
    "title": "A Co-design view of Compute in-Memory with Non-Volatile Elements for  Neural Networks",
    "abstract": "Deep Learning neural networks are pervasive, but traditional computer\narchitectures are reaching the limits of being able to efficiently execute them\nfor the large workloads of today. They are limited by the von Neumann\nbottleneck: the high cost in energy and latency incurred in moving data between\nmemory and the compute engine. Today, special CMOS designs address this\nbottleneck. The next generation of computing hardware will need to eliminate or\ndramatically mitigate this bottleneck. We discuss how compute-in-memory can\nplay an important part in this development. Here, a non-volatile memory based\ncross-bar architecture forms the heart of an engine that uses an analog process\nto parallelize the matrix vector multiplication operation, repeatedly used in\nall neural network workloads. The cross-bar architecture, at times referred to\nas a neuromorphic approach, can be a key hardware element in future computing\nmachines. In the first part of this review we take a co-design view of the\ndesign constraints and the demands it places on the new materials and memory\ndevices that anchor the cross-bar architecture. In the second part, we review\nwhat is knows about the different new non-volatile memory materials and devices\nsuited for compute in-memory, and discuss the outlook and challenges.",
    "descriptor": "\nComments: 56 pages, 15 figures\n",
    "authors": [
      "Wilfried Haensch",
      "Anand Raghunathan",
      "Kaushik Roy",
      "Bhaswar Chakrabart",
      "Charudatta M. Phatak",
      "Cheng Wang",
      "Supratik Guha"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.08735"
  },
  {
    "id": "arXiv:2206.08737",
    "title": "N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation  Motions in Unseen and Dynamic Environments",
    "abstract": "Despite its importance in both industrial and service robotics, mobile\nmanipulation remains a significant challenge as it requires a seamless\nintegration of end-effector trajectory generation with navigation skills as\nwell as reasoning over long-horizons. Existing methods struggle to control the\nlarge configuration space, and to navigate dynamic and unknown environments. In\nprevious work, we proposed to decompose mobile manipulation tasks into a\nsimplified motion generator for the end-effector in task space and a trained\nreinforcement learning agent for the mobile base to account for kinematic\nfeasibility of the motion. In this work, we introduce Neural Navigation for\nMobile Manipulation (N$^2$M$^2$) which extends this decomposition to complex\nobstacle environments and enables it to tackle a broad range of tasks in real\nworld settings. The resulting approach can perform unseen, long-horizon tasks\nin unexplored environments while instantly reacting to dynamic obstacles and\nenvironmental changes. At the same time, it provides a simple way to define new\nmobile manipulation tasks. We demonstrate the capabilities of our proposed\napproach in extensive simulation and real-world experiments on multiple\nkinematically diverse mobile manipulators. Code and videos are publicly\navailable at this http URL",
    "descriptor": "\nComments: Project website: this http URL\n",
    "authors": [
      "Daniel Honerkamp",
      "Tim Welschehold",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08737"
  },
  {
    "id": "arXiv:2206.08738",
    "title": "Detecting Adversarial Examples in Batches -- a geometrical approach",
    "abstract": "Many deep learning methods have successfully solved complex tasks in computer\nvision and speech recognition applications. Nonetheless, the robustness of\nthese models has been found to be vulnerable to perturbed inputs or adversarial\nexamples, which are imperceptible to the human eye, but lead the model to\nerroneous output decisions. In this study, we adapt and introduce two geometric\nmetrics, density and coverage, and evaluate their use in detecting adversarial\nsamples in batches of unseen data. We empirically study these metrics using\nMNIST and two real-world biomedical datasets from MedMNIST, subjected to two\ndifferent adversarial attacks. Our experiments show promising results for both\nmetrics to detect adversarial examples. We believe that his work can lay the\nground for further study on these metrics' use in deployed machine learning\nsystems to monitor for possible attacks by adversarial examples or related\npathologies such as dataset shift.",
    "descriptor": "\nComments: Submitted to AdvML workshop at ICML2022\n",
    "authors": [
      "Danush Kumar Venkatesh",
      "Peter Steinbach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08738"
  },
  {
    "id": "arXiv:2206.08742",
    "title": "Near-Optimal No-Regret Learning for General Convex Games",
    "abstract": "A recent line of work has established uncoupled learning dynamics such that,\nwhen employed by all players in a game, each player's \\emph{regret} after $T$\nrepetitions grows polylogarithmically in $T$, an exponential improvement over\nthe traditional guarantees within the no-regret framework. However, so far\nthese results have only been limited to certain classes of games with\nstructured strategy spaces -- such as normal-form and extensive-form games. The\nquestion as to whether $O(\\text{polylog} T)$ regret bounds can be obtained for\ngeneral convex and compact strategy sets -- which occur in many fundamental\nmodels in economics and multiagent systems -- while retaining efficient\nstrategy updates is an important question. In this paper, we answer this in the\npositive by establishing the first uncoupled learning algorithm with $O(\\log\nT)$ per-player regret in general \\emph{convex games}, that is, games with\nconcave utility functions supported on arbitrary convex and compact strategy\nsets. Our learning dynamics are based on an instantiation of optimistic\nfollow-the-regularized-leader over an appropriately \\emph{lifted} space using a\n\\emph{self-concordant regularizer} that is, peculiarly, not a barrier for the\nfeasible region. Further, our learning dynamics are efficiently implementable\ngiven access to a proximal oracle for the convex strategy set, leading to\n$O(\\log\\log T)$ per-iteration complexity; we also give extensions when access\nto only a \\emph{linear} optimization oracle is assumed. Finally, we adapt our\ndynamics to guarantee $O(\\sqrt{T})$ regret in the adversarial regime. Even in\nthose special cases where prior results apply, our algorithm improves over the\nstate-of-the-art regret bounds either in terms of the dependence on the number\nof iterations or on the dimension of the strategy sets.",
    "descriptor": "",
    "authors": [
      "Gabriele Farina",
      "Ioannis Anagnostides",
      "Haipeng Luo",
      "Chung-Wei Lee",
      "Christian Kroer",
      "Tuomas Sandholm"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08742"
  },
  {
    "id": "arXiv:2206.08743",
    "title": "Learning Fair Representation via Distributional Contrastive  Disentanglement",
    "abstract": "Learning fair representation is crucial for achieving fairness or debiasing\nsensitive information. Most existing works rely on adversarial representation\nlearning to inject some invariance into representation. However, adversarial\nlearning methods are known to suffer from relatively unstable training, and\nthis might harm the balance between fairness and predictiveness of\nrepresentation. We propose a new approach, learning FAir Representation via\ndistributional CONtrastive Variational AutoEncoder (FarconVAE), which induces\nthe latent space to be disentangled into sensitive and nonsensitive parts. We\nfirst construct the pair of observations with different sensitive attributes\nbut with the same labels. Then, FarconVAE enforces each non-sensitive latent to\nbe closer, while sensitive latents to be far from each other and also far from\nthe non-sensitive latent by contrasting their distributions. We provide a new\ntype of contrastive loss motivated by Gaussian and Student-t kernels for\ndistributional contrastive learning with theoretical analysis. Besides, we\nadopt a new swap-reconstruction loss to boost the disentanglement further.\nFarconVAE shows superior performance on fairness, pretrained model debiasing,\nand domain generalization tasks from various modalities, including tabular,\nimage, and text.",
    "descriptor": "\nComments: Accepted by KDD 2022 (Research Track)\n",
    "authors": [
      "Changdae Oh",
      "Heeji Won",
      "Junhyuk So",
      "Taero Kim",
      "Yewon Kim",
      "Hosik Choi",
      "Kyungwoo Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08743"
  },
  {
    "id": "arXiv:2206.08747",
    "title": "Machine Learning-Driven Process of Alumina Ceramics Laser Machining",
    "abstract": "Laser machining is a highly flexible non-contact manufacturing technique that\nhas been employed widely across academia and industry. Due to nonlinear\ninteractions between light and matter, simulation methods are extremely\ncrucial, as they help enhance the machining quality by offering comprehension\nof the inter-relationships between the laser processing parameters. On the\nother hand, experimental processing parameter optimization recommends a\nsystematic, and consequently time-consuming, investigation over the available\nprocessing parameter space. An intelligent strategy is to employ machine\nlearning (ML) techniques to capture the relationship between picosecond laser\nmachining parameters for finding proper parameter combinations to create the\ndesired cuts on industrial-grade alumina ceramic with deep, smooth and\ndefect-free patterns. Laser parameters such as beam amplitude and frequency,\nscanner passing speed and the number of passes over the surface, as well as the\nvertical distance of the scanner from the sample surface, are used for\npredicting the depth, top width, and bottom width of the engraved channels\nusing ML models. Owing to the complex correlation between laser parameters, it\nis shown that Neural Networks (NN) are the most efficient in predicting the\noutputs. Equipped with an ML model that captures the interconnection between\nlaser parameters and the engraved channel dimensions, one can predict the\nrequired input parameters to achieve a target channel geometry. This strategy\nsignificantly reduces the cost and effort of experimental laser machining\nduring the development phase, without compromising accuracy or performance. The\ndeveloped techniques can be applied to a wide range of ceramic laser machining\nprocesses.",
    "descriptor": "",
    "authors": [
      "Razyeh Behbahani",
      "Hamidreza Yazdani Sarvestani",
      "Erfan Fatehi",
      "Elham Kiyani",
      "Behnam Ashrafi",
      "Mikko Karttunen",
      "Meysam Rahmat"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08747"
  },
  {
    "id": "arXiv:2206.08748",
    "title": "ReViSe: Remote Vital Signs Measurement Using Smartphone Camera",
    "abstract": "Remote Photoplethysmography (rPPG) is a fast, effective, inexpensive and\nconvenient method for collecting biometric data as it enables vital signs\nestimation using face videos. Remote contactless medical service provisioning\nhas proven to be a dire necessity during the COVID-19 pandemic. We propose an\nend-to-end framework to measure people's vital signs including Heart Rate (HR),\nHeart Rate Variability (HRV), Oxygen Saturation (SpO2) and Blood Pressure (BP)\nbased on the rPPG methodology from the video of a user's face captured with a\nsmartphone camera. We extract face landmarks with a deep learning-based neural\nnetwork model in real-time. Multiple face patches also called\nRegion-of-Interests (RoIs) are extracted by using the predicted face landmarks.\nSeveral filters are applied to reduce the noise from the RoIs in the extracted\ncardiac signals called Blood Volume Pulse (BVP) signal. We trained and\nvalidated machine learning models using two public rPPG datasets namely the\nTokyoTech rPPG and the Pulse Rate Detection (PURE) datasets, on which our\nmodels achieved the following Mean Absolute Errors (MAE): a) for HR, 1.73 and\n3.95 Beats-Per-Minute (bpm) respectively, b) for HRV, 18.55 and 25.03 ms\nrespectively, and c) for SpO2, a MAE of 1.64 on the PURE dataset. We validated\nour end-to-end rPPG framework, ReViSe, in real life environment, and thereby\ncreated the Video-HR dataset. Our HR estimation model achieved a MAE of 2.49\nbpm on this dataset. Since no publicly available rPPG datasets existed for BP\nmeasurement with face videos, we used a dataset with signals from fingertip\nsensor to train our model and also created our own video dataset, Video-BP. On\nour Video-BP dataset, our BP estimation model achieved a MAE of 6.7 mmHg for\nSystolic Blood Pressure (SBP), and a MAE of 9.6 mmHg for Diastolic Blood\nPressure (DBP).",
    "descriptor": "",
    "authors": [
      "Donghao Qiao",
      "Amtul Haq Ayesha",
      "Farhana Zulkernine",
      "Raihan Masroor",
      "Nauman Jaffar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08748"
  },
  {
    "id": "arXiv:2206.08749",
    "title": "From a few Accurate 2D Correspondences to 3D Point Clouds",
    "abstract": "Key points, correspondences, projection matrices, point clouds and dense\nclouds are the skeletons in image-based 3D reconstruction, of which point\nclouds have the important role in generating a realistic and natural model for\na 3D reconstructed object. To achieve a good 3D reconstruction, the point\nclouds must be almost everywhere in the surface of the object. In this article,\nwith a main purpose to build the point clouds covering the entire surface of\nthe object, we propose a new feature named a geodesic feature or geo-feature.\nBased on the new geo-feature, if there are several (given) initial world points\non the object's surface along with all accurately estimated projection\nmatrices, some new world points on the geodesics connecting any two of these\ngiven world points will be reconstructed. Then the regions on the surface\nbordering by these initial world points will be covered by the point clouds.\nThus, if the initial world points are around the surface, the point clouds will\ncover the entire surface.\nThis article proposes a new method to estimate the world points and\nprojection matrices from their correspondences. This method derives the\nclosed-form and iterative solutions for the world points and projection\nmatrices and proves that when the number of world points is less than seven and\nthe number of images is at least five, the proposed solutions are global\noptimal. We propose an algorithm named World points from their Correspondences\n(WPfC) to estimate the world points and projection matrices from their\ncorrespondences, and another algorithm named Creating Point Clouds (CrPC) to\ncreate the point clouds from the world points and projection matrices given by\nthe first algorithm.",
    "descriptor": "",
    "authors": [
      "Trung-Kien Le",
      "Ping Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08749"
  },
  {
    "id": "arXiv:2206.08750",
    "title": "Enriched physics-informed neural networks for in-plane crack problems:  Theory and MATLAB codes",
    "abstract": "In this paper, a method based on the physics-informed neural networks (PINNs)\nis presented to model in-plane crack problems in the linear elastic fracture\nmechanics. Instead of forming a mesh, the PINNs is meshless and can be trained\non batches of randomly sampled collocation points. In order to capture the\ntheoretical singular behavior of the near-tip stress and strain fields, the\nstandard PINNs formulation is enriched here by including the crack-tip\nasymptotic functions such that the singular solutions at the crack-tip region\ncan be modeled accurately without a high degree of nodal refinement. The\nlearnable parameters of the enriched PINNs are trained to satisfy the governing\nequations of the cracked body and the corresponding boundary conditions. It was\nfound that the incorporation of the crack-tip enrichment functions in PINNs is\nsubstantially simpler and more trouble-free than in the finite element (FEM) or\nboundary element (BEM) methods. The present algorithm is tested on a class of\nrepresentative benchmarks with different modes of loading types. Results show\nthat the present method allows the calculation of accurate stress intensity\nfactors (SIFs) with far fewer degrees of freedom. A self-contained MATLAB code\nand data-sets accompanying this manuscript are also provided.",
    "descriptor": "\nComments: 31 pages, 17 figures, 5 tables\n",
    "authors": [
      "Yan Gu",
      "Chuanzeng Zhang",
      "Peijun Zhang",
      "Mikhail V. Golub"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08750"
  },
  {
    "id": "arXiv:2206.08751",
    "title": "A Database for Perceived Quality Assessment of User-Generated VR Videos",
    "abstract": "Virtual reality (VR) videos (typically in the form of 360$^\\circ$ videos)\nhave gained increasing attention due to the fast development of VR technologies\nand the remarkable popularization of consumer-grade 360$^\\circ$ cameras and\ndisplays. Thus it is pivotal to understand how people perceive user-generated\nVR videos, which may suffer from commingled authentic distortions, often\nlocalized in space and time. In this paper, we establish one of the largest\n360$^\\circ$ video databases, containing 502 user-generated videos with rich\ncontent and distortion diversities. We capture viewing behaviors (i.e.,\nscanpaths) of 139 users, and collect their opinion scores of perceived quality\nunder four different viewing conditions (two starting points $\\times$ two\nexploration times). We provide a thorough statistical analysis of recorded\ndata, resulting in several interesting observations, such as the significant\nimpact of viewing conditions on viewing behaviors and perceived quality.\nBesides, we explore other usage of our data and analysis, including evaluation\nof computational models for quality assessment and saliency detection of\n360$^\\circ$ videos. We have made the dataset and code available at\nhttps://github.com/Yao-Yiru/VR-Video-Database.",
    "descriptor": "",
    "authors": [
      "Yuming Fang",
      "Yiru Yao",
      "Xiangjie Sui",
      "Kede Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.08751"
  },
  {
    "id": "arXiv:2206.08752",
    "title": "Federated learning with incremental clustering for heterogeneous data",
    "abstract": "Federated learning enables different parties to collaboratively build a\nglobal model under the orchestration of a server while keeping the training\ndata on clients' devices. However, performance is affected when clients have\nheterogeneous data. To cope with this problem, we assume that despite data\nheterogeneity, there are groups of clients who have similar data distributions\nthat can be clustered. In previous approaches, in order to cluster clients the\nserver requires clients to send their parameters simultaneously. However, this\ncan be problematic in a context where there is a significant number of\nparticipants that may have limited availability. To prevent such a bottleneck,\nwe propose FLIC (Federated Learning with Incremental Clustering), in which the\nserver exploits the updates sent by clients during federated training instead\nof asking them to send their parameters simultaneously. Hence no additional\ncommunications between the server and the clients are necessary other than what\nclassical federated learning requires. We empirically demonstrate for various\nnon-IID cases that our approach successfully splits clients into groups\nfollowing the same data distributions. We also identify the limitations of FLIC\nby studying its capability to partition clients at the early stages of the\nfederated learning process efficiently. We further address attacks on models as\na form of data heterogeneity and empirically show that FLIC is a robust defense\nagainst poisoning attacks even when the proportion of malicious clients is\nhigher than 50\\%.",
    "descriptor": "",
    "authors": [
      "Fabiola Espinoza Castellon",
      "Aurelien Mayoue",
      "Jacques-Henri Sublemontier",
      "Cedric Gouy-Pailler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08752"
  },
  {
    "id": "arXiv:2206.08755",
    "title": "Compositional Exploration of Combinatorial Scientific Models",
    "abstract": "We implement a novel representation of model search spaces as diagrams over a\ncategory of models, where we have restricted attention to a broad class of\nmodels whose structure is presented by \\C-sets. (Co)limits in these diagram\ncategories allow the creation of composite model spaces from more primitive\nspaces. We present a novel implementation of the computer algebra of finitely\npresented categories and diagram categories (including their limits and\ncolimits), which formalizes a notion of model space exploration. This is\ncoupled with strategies to facilitate the selection of desired models from\nthese model spaces. We demonstrate our framework by generating a tool which\nfits experimental data, searching an epidemiology-relevant subspace of\nmass-action kinetic models.",
    "descriptor": "",
    "authors": [
      "Kristopher Brown",
      "Tyler Hanks",
      "James Fairbanks"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2206.08755"
  },
  {
    "id": "arXiv:2206.08757",
    "title": "Beyond Ridge Regression for Distribution-Free Data",
    "abstract": "In supervised batch learning, the predictive normalized maximum likelihood\n(pNML) has been proposed as the min-max regret solution for the\ndistribution-free setting, where no distributional assumptions are made on the\ndata. However, the pNML is not defined for a large capacity hypothesis class as\nover-parameterized linear regression. For a large class, a common approach is\nto use regularization or a model prior. In the context of online prediction\nwhere the min-max solution is the Normalized Maximum Likelihood (NML), it has\nbeen suggested to use NML with ``luckiness'': A prior-like function is applied\nto the hypothesis class, which reduces its effective size. Motivated by the\nluckiness concept, for linear regression we incorporate a luckiness function\nthat penalizes the hypothesis proportionally to its l2 norm. This leads to the\nridge regression solution. The associated pNML with luckiness (LpNML)\nprediction deviates from the ridge regression empirical risk minimizer (Ridge\nERM): When the test data reside in the subspace corresponding to the small\neigenvalues of the empirical correlation matrix of the training data, the\nprediction is shifted toward 0. Our LpNML reduces the Ridge ERM error by up to\n20% for the PMLB sets, and is up to 4.9% more robust in the presence of\ndistribution shift compared to recent leading methods for UCI sets.",
    "descriptor": "",
    "authors": [
      "Koby Bibas",
      "Meir Feder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08757"
  },
  {
    "id": "arXiv:2206.08758",
    "title": "Rectifying Mono-Label Boolean Classifiers",
    "abstract": "We elaborate on the notion of rectification of a Boolean classifier $\\Sigma$.\nGiven $\\Sigma$ and some background knowledge $T$, postulates characterizing the\nway $\\Sigma$ must be changed into a new classifier $\\Sigma \\star T$ that\ncomplies with $T$ have already been presented. We focus here on the specific\ncase of mono-label Boolean classifiers, i.e., there is a single target concept\nand any instance is classified either as positive (an element of the concept),\nor as negative (an element of the complementary concept). In this specific\ncase, our main contribution is twofold: (1) we show that there is a unique\nrectification operator $\\star$ satisfying the postulates, and (2) when $\\Sigma$\nand $T$ are Boolean circuits, we show how a classification circuit equivalent\nto $\\Sigma \\star T$ can be computed in time linear in the size of $\\Sigma$ and\n$T$; when $\\Sigma$ and $T$ are decision trees, a decision tree equivalent to\n$\\Sigma \\star T$ can be computed in time polynomial in the size of $\\Sigma$ and\n$T$.",
    "descriptor": "",
    "authors": [
      "Sylvie Coste-Marquis",
      "Pierre Marquis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08758"
  },
  {
    "id": "arXiv:2206.08760",
    "title": "A Novel Loop Fission Technique Inspired by Implicit Computational  Complexity",
    "abstract": "This work explores an unexpected application of Implicit Computational\nComplexity (ICC) to parallelize loops in imperative programs. Thanks to a\nlightweight dependency analysis, our algorithm allows splitting a loop into\nmultiple loops that can be run in parallel, resulting in gains in terms of\nexecution time similar to state-of-the-art automatic parallelization tools when\nboth are applicable. Our graph-based algorithm is intuitive, language-agnostic,\nproven correct, and applicable to all types of loops, even if their loop\niteration space is unknown statically or at compile time, if they are not in\ncanonical form or if they contain loop-carried dependency. As contributions we\ndeliver the computational technique, proof of its preservation of semantic\ncorrectness, and experimental results to quantify the expected performance\ngains. Our benchmarks also show that the technique could be seamlessly\nintegrated into compiler passes or other automatic parallelization suites. We\nassert that this original and automatable loop transformation method was\ndiscovered thanks to the \"orthogonal\" approach offered by ICC.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Aubert",
      "Thomas Rubiano",
      "Neea Rusch",
      "Thomas Seiller"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08760"
  },
  {
    "id": "arXiv:2206.08768",
    "title": "C-Pack of IPAs: A C90 Program Benchmark of Introductory Programming  Assignments",
    "abstract": "Due to the vast number of students enrolled in Massive Open Online Courses\n(MOOCs), there has been an increasing number of automated program repair\ntechniques focused on introductory programming assignments (IPAs). Such\ntechniques take advantage of previous correct student implementations in order\nto provide automated, comprehensive, and personalized feedback to students.\nThis paper presents C-Pack-IPAs, a publicly available benchmark of students'\nprograms submitted for 25 different IPAs. C-Pack-IPAs contains semantically\ncorrect, semantically incorrect, and syntactically incorrect programs plus a\ntest suite for each IPA. Hence, C-Pack-IPAs can be used to help evaluate the\ndevelopment of novel semantic, as well as syntactic, automated program repair\nframeworks, focused on providing feedback to novice programmers.",
    "descriptor": "\nComments: 3 pages, 3 tables, 1 GitHub url: this https URL\n",
    "authors": [
      "Pedro Orvalho",
      "Mikol\u00e1\u0161 Janota",
      "Vasco Manquinho"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.08768"
  },
  {
    "id": "arXiv:2206.08771",
    "title": "Downlink Massive MU-MIMO with Successively-Regularized Zero Forcing  Precoding",
    "abstract": "In this letter, we consider linear precoding for downlink massive multi-user\n(MU) multiple-input multiple-output (MIMO) systems. We propose the novel\nsuccessively-regularized zero forcing (SRZF) precoding, which exploits\nsuccessive null spaces of the MIMO channels of the users, along with\nregularization, to control the inter-user interference and to enhance\nperformance and robustness to imperfect channel state information (CSI) at the\nbase station (BS). We compare the weighted sum rate of the proposed SRZF\nprecoding with those of block diagonalization and conventional and regularized\nzero forcing precoding for fixed and locally-optimal power allocation\nstrategies as well as for perfect and imperfect CSI via computer simulations.\nOur simulation results reveal that for both underloaded and critically-loaded\nsystems and perfect and imperfect CSI at the BS, the proposed SRZF precoding\nsignificantly outperforms the considered baseline schemes, making it an\nattractive option for downlink massive MU-MIMO systems.",
    "descriptor": "\nComments: 5 pages (main paper) + 2 pages (MATLAB test), 2 figures. This work has been submitted to the IEEE for possible publication\n",
    "authors": [
      "Aravindh Krishnamoorthy",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08771"
  },
  {
    "id": "arXiv:2206.08772",
    "title": "Parallel Multiphysics Simulation for the Stabilized Optimal  Transportation Meshfree (OTM)",
    "abstract": "This paper presents a parallel \\PG{implementation} for the Optimal\nTransportation Meshfree (OTM) method on large CPU clusters. Communications are\nhandled with the Message Passing Interface (MPI). The Recursive Coordinate\nBisection (RCB) algorithm is utilized for domain decomposition and for\nimplementing dynamic load-balancing strategy. This work involves three new\nconcepts to reduce the computational efforts: Dynamic halo regions, Efficient\ndata management strategies for ease of addition and deletion of nodes and\nmaterial points using advanced STL container, and nearest neighborhood\ncommunication for detection of neighbors and communication. Also, Linked Cell\napproach has been implemented to further reduce the computational efforts.\nParallel performance analysis is investigated for challenging multiphysics\napplications like Taylor rod impact and serrated chip formation process.\nAdequate scalability of parallel implementation for these applications is\nreported.",
    "descriptor": "\nComments: 21 page, 18 figures\n",
    "authors": [
      "Sandeep Kumar",
      "Pierre Gosselet",
      "Dengpeng Huang",
      "Christian Wei\u00dfenfels",
      "Peter Wriggers"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.08772"
  },
  {
    "id": "arXiv:2206.08774",
    "title": "Spectral-Efficiency of Cell-Free Massive MIMO with Multicarrier-Division  Duplex",
    "abstract": "A multicarrier-division duplex (MDD)-based cell-free (CF) scheme, namely\nMDD-CF, is proposed, which enables downlink (DL) data and uplink (UL) data or\npilots to be concurrently transmitted on mutually orthogonal subcarriers in\ndistributed CF massive MIMO (mMIMO) systems. To demonstrate the advantages of\nMDD-CF, we firstly study the spectral-efficiency (SE) performance in terms of\none coherence interval (CT) associated with access point (AP)-selection, power-\nand subcarrier-allocation. Since the formulated SE optimization is a\nmixed-integer non-convex problem that is NP-hard to solve, we leverage the\ninherent association between involved variables to transform it into a\ncontinuous-integer convex-concave problem. Then, a quadratic transform\n(QT)-assisted iterative algorithm is proposed to achieve SE maximization. Next,\nwe extend our study to the case of one radio frame consisting of several CT\nintervals. In this regard, a novel two-phase CT interval (TPCT) scheme is\ndesigned to not only improve the SE in radio frame but also provide consistent\ndata transmissions over fast time-varying channels. Correspondingly, to\nfacilitate the optimization, we propose a two-step iterative algorithm by\nbuilding the connections between two phases in TPCT through an iteration\nfactor. Simulation results show that, MDD-CF can significantly outperform\nin-band full duplex (IBFD)-CF due to the efficient interference management.\nFurthermore, compared with time-division duplex (TDD)-CF, MDD-CF is more robust\nto high-mobility scenarios and achieves better SE performance.",
    "descriptor": "",
    "authors": [
      "Bohan Li",
      "Lie-Liang Yang",
      "Robert G. Maunder",
      "Songlin Sun",
      "Pei Xiao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08774"
  },
  {
    "id": "arXiv:2206.08776",
    "title": "Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms",
    "abstract": "We generalize the multiple-play multi-armed bandits (MP-MAB) problem with a\nshareable arm setting, in which several plays can share the same arm.\nFurthermore, each shareable arm has a finite reward capacity and a ''per-load''\nreward distribution, both of which are unknown to the learner. The reward from\na shareable arm is load-dependent, which is the \"per-load\" reward multiplying\neither the number of plays pulling the arm, or its reward capacity when the\nnumber of plays exceeds the capacity limit. When the \"per-load\" reward follows\na Gaussian distribution, we prove a sample complexity lower bound of learning\nthe capacity from load-dependent rewards and also a regret lower bound of this\nnew MP-MAB problem. We devise a capacity estimator whose sample complexity\nupper bound matches the lower bound in terms of reward means and capacities. We\nalso propose an online learning algorithm to address the problem and prove its\nregret upper bound. This regret upper bound's first term is the same as regret\nlower bound's, and its second and third terms also evidently correspond to\nlower bound's. Extensive experiments validate our algorithm's performance and\nalso its gain in 5G & 4G base station selection.",
    "descriptor": "\nComments: to appear in ICML 2022\n",
    "authors": [
      "Xuchuang Wang",
      "Hong Xie",
      "John C.S. Lui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08776"
  },
  {
    "id": "arXiv:2206.08778",
    "title": "CTooth: A Fully Annotated 3D Dataset and Benchmark for Tooth Volume  Segmentation on Cone Beam Computed Tomography Images",
    "abstract": "3D tooth segmentation is a prerequisite for computer-aided dental diagnosis\nand treatment. However, segmenting all tooth regions manually is subjective and\ntime-consuming. Recently, deep learning-based segmentation methods produce\nconvincing results and reduce manual annotation efforts, but it requires a\nlarge quantity of ground truth for training. To our knowledge, there are few\ntooth data available for the 3D segmentation study. In this paper, we establish\na fully annotated cone beam computed tomography dataset CTooth with tooth gold\nstandard. This dataset contains 22 volumes (7363 slices) with fine tooth labels\nannotated by experienced radiographic interpreters. To ensure a relative even\ndata sampling distribution, data variance is included in the CTooth including\nmissing teeth and dental restoration. Several state-of-the-art segmentation\nmethods are evaluated on this dataset. Afterwards, we further summarise and\napply a series of 3D attention-based Unet variants for segmenting tooth\nvolumes. This work provides a new benchmark for the tooth volume segmentation\ntask. Experimental evidence proves that attention modules of the 3D UNet\nstructure boost responses in tooth areas and inhibit the influence of\nbackground and noise. The best performance is achieved by 3D Unet with SKNet\nattention module, of 88.04 \\% Dice and 78.71 \\% IOU, respectively. The\nattention-based Unet framework outperforms other state-of-the-art methods on\nthe CTooth dataset. The codebase and dataset are released.",
    "descriptor": "",
    "authors": [
      "Weiwei Cui",
      "Yaqi Wang",
      "Qianni Zhang",
      "Huiyu Zhou",
      "Dan Song",
      "Xingyong Zuo",
      "Gangyong Jia",
      "Liaoyuan Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08778"
  },
  {
    "id": "arXiv:2206.08781",
    "title": "Reinforcement Learning in Macroeconomic Policy Design: A New Frontier?",
    "abstract": "Agent-based computational macroeconomics is a field with a rich academic\nhistory, yet one which has struggled to enter mainstream policy design\ntoolboxes, plagued by the challenges associated with representing a complex and\ndynamic reality. The field of Reinforcement Learning (RL), too, has a rich\nhistory, and has recently been at the centre of several exponential\ndevelopments. Modern RL implementations have been able to achieve unprecedented\nlevels of sophistication, handling previously-unthinkable degrees of\ncomplexity. This review surveys the historical barriers of classical\nagent-based techniques in macroeconomic modelling, and contemplates whether\nrecent developments in RL can overcome any of them.",
    "descriptor": "",
    "authors": [
      "Callum Tilbury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2206.08781"
  },
  {
    "id": "arXiv:2206.08783",
    "title": "A Human-Centric Method for Generating Causal Explanations in Natural  Language for Autonomous Vehicle Motion Planning",
    "abstract": "Inscrutable AI systems are difficult to trust, especially if they operate in\nsafety-critical settings like autonomous driving. Therefore, there is a need to\nbuild transparent and queryable systems to increase trust levels. We propose a\ntransparent, human-centric explanation generation method for autonomous vehicle\nmotion planning and prediction based on an existing white-box system called\nIGP2. Our method integrates Bayesian networks with context-free generative\nrules and can give causal natural language explanations for the high-level\ndriving behaviour of autonomous vehicles. Preliminary testing on simulated\nscenarios shows that our method captures the causes behind the actions of\nautonomous vehicles and generates intelligible explanations with varying\ncomplexity.",
    "descriptor": "\nComments: IJCAI Workshop on Artificial Intelligence for Autonomous Driving (AI4AD), 2022\n",
    "authors": [
      "Balint Gyevnar",
      "Massimiliano Tamborski",
      "Cheng Wang",
      "Christopher G. Lucas",
      "Shay B. Cohen",
      "Stefano V. Albrecht"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08783"
  },
  {
    "id": "arXiv:2206.08786",
    "title": "Discovery of the Content and Engagement with the Content",
    "abstract": "In the second half of the 20th century, Parliament allowed broadcasters to\ntransmit radio and eventually television coverage of debates and meetings of\nselect committees. More recently, in an effort to further improve transparency\nand citizen engagement, the UK Parliament started publishing videos of these\ndebates and meetings itself, and tweeting details of debates as they happened.\nIn this paper, we attempt to characterise how people engage with video data of\nParliamentary debates by using more than two years of Google Analytics data\naround these videos. We analyse the patterns of engagement - how do they land\non a particular video? How do they hear about this video, i.e., what is the\n(HTTP) referrer website that led to the user clicking on the video? Once a user\nlands on a video, how do they engage with it? For how long is the video played?\nWhat is the next destination? etc. Answering these questions is an important\nfirst step towards understanding why and how people use Parliamentary videos,\nand therefore, how the video delivery platform should be adapted and\npersonalised for the needs of the citizens of the country. Taking inspiration\nfrom An, Kwak, and Jansen (2017), we employ Non-Negative Matrix Factorization\n(NMF) (Lee and Seung, 1999) on the video views matrix to identify different\narchetypes of users, and identify archetypes. A deeper examination of the\narchetypes we find reveals that they are primarily distinguished by how they\nland on the video page: Search (i.e., through a search engine), Referral (i.e.,\nfrom other Parliamentary websites), Direct (i.e., through a direct link, which\nis embedded on another website), Social (i.e., through a social platform such\nas Facebook or Twitter) and Others.",
    "descriptor": "\nComments: In APEN workshop, AAAI ICWSM 2018\n",
    "authors": [
      "Pushkal Agarwal",
      "Nishanth Sastry",
      "Edward Wood"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08786"
  },
  {
    "id": "arXiv:2206.08788",
    "title": "Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal  Fake News Detection",
    "abstract": "The proliferation of fake news and its serious negative social influence push\nfake news detection methods to become necessary tools for web managers.\nMeanwhile, the multi-media nature of social media makes multi-modal fake news\ndetection popular for its ability to capture more modal features than uni-modal\ndetection methods. However, current literature on multi-modal detection is more\nlikely to pursue the detection accuracy but ignore the robustness of the\ndetector. To address this problem, we propose a comprehensive robustness\nevaluation of multi-modal fake news detectors. In this work, we simulate the\nattack methods of malicious users and developers, i.e., posting fake news and\ninjecting backdoors. Specifically, we evaluate multi-modal detectors with five\nadversarial and two backdoor attack methods. Experiment results imply that: (1)\nThe detection performance of the state-of-the-art detectors degrades\nsignificantly under adversarial attacks, even worse than general detectors; (2)\nMost multi-modal detectors are more vulnerable when subjected to attacks on\nvisual modality than textual modality; (3) Popular events' images will cause\nsignificant degradation to the detectors when they are subjected to backdoor\nattacks; (4) The performance of these detectors under multi-modal attacks is\nworse than under uni-modal attacks; (5) Defensive methods will improve the\nrobustness of the multi-modal detectors.",
    "descriptor": "",
    "authors": [
      "Jinyin Chen",
      "Chengyu Jia",
      "Haibin Zheng",
      "Ruoxi Chen",
      "Chenbo Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.08788"
  },
  {
    "id": "arXiv:2206.08789",
    "title": "Reconstructing vehicles from orthographic drawings using deep neural  networks",
    "abstract": "This paper explores the current state-of-the-art of object reconstruction\nfrom multiple orthographic drawings using deep neural networks. It proposes two\nalgorithms to extract multiple views from a single image. The paper proposes a\nsystem based on pixel-aligned implicit functions (PIFu) and develops an\nadvanced sampling strategy to generate signed distance samples. It also\ncompares this approach to depth map regression from multiple views.\nAdditionally, the paper uses a novel dataset for vehicle reconstruction from\nthe racing game Assetto Corsa, which features higher quality models than the\ncommonly used ShapeNET dataset. The trained neural network generalizes well to\nreal-world inputs and creates plausible and detailed reconstructions.",
    "descriptor": "\nComments: 9 Pages\n",
    "authors": [
      "Robin Klippert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08789"
  },
  {
    "id": "arXiv:2206.08790",
    "title": "Self-supervised speech unit discovery from articulatory and acoustic  features using VQ-VAE",
    "abstract": "The human perception system is often assumed to recruit motor knowledge when\nprocessing auditory speech inputs. Using articulatory modeling and deep\nlearning, this study examines how this articulatory information can be used for\ndiscovering speech units in a self-supervised setting. We used vector-quantized\nvariational autoencoders (VQ-VAE) to learn discrete representations from\narticulatory and acoustic speech data. In line with the zero-resource paradigm,\nan ABX test was then used to investigate how the extracted representations\nencode phonetically relevant properties. Experiments were conducted on three\ndifferent corpora in English and French. We found that articulatory information\nrather organises the latent representations in terms of place of articulation\nwhereas the speech acoustics mainly structure the latent space in terms of\nmanner of articulation. We show that an optimal fusion of the two modalities\ncan lead to a joint representation of these phonetic dimensions more accurate\nthan each modality considered individually. Since articulatory information is\nusually not available in a practical situation, we finally investigate the\nbenefit it provides when inferred from the speech acoustics in a\nself-supervised manner.",
    "descriptor": "",
    "authors": [
      "Marc-Antoine Georges",
      "Jean-Luc Schwartz",
      "Thomas Hueber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08790"
  },
  {
    "id": "arXiv:2206.08791",
    "title": "DU-Net based Unsupervised Contrastive Learning for Cancer Segmentation  in Histology Images",
    "abstract": "In this paper, we introduce an unsupervised cancer segmentation framework for\nhistology images. The framework involves an effective contrastive learning\nscheme for extracting distinctive visual representations for segmentation. The\nencoder is a Deep U-Net (DU-Net) structure that contains an extra fully\nconvolution layer compared to the normal U-Net. A contrastive learning scheme\nis developed to solve the problem of lacking training sets with high-quality\nannotations on tumour boundaries. A specific set of data augmentation\ntechniques are employed to improve the discriminability of the learned colour\nfeatures from contrastive learning. Smoothing and noise elimination are\nconducted using convolutional Conditional Random Fields. The experiments\ndemonstrate competitive performance in segmentation even better than some\npopular supervised networks.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2002.05709 by other authors\n",
    "authors": [
      "Yilong Li",
      "Yaqi Wang",
      "Huiyu Zhou",
      "Huaqiong Wang",
      "Gangyong Jia",
      "Qianni Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08791"
  },
  {
    "id": "arXiv:2206.08792",
    "title": "FD-CAM: Improving Faithfulness and Discriminability of Visual  Explanation for CNNs",
    "abstract": "Class activation map (CAM) has been widely studied for visual explanation of\nthe internal working mechanism of convolutional neural networks. The key of\nexisting CAM-based methods is to compute effective weights to combine\nactivation maps in the target convolution layer. Existing gradient and score\nbased weighting schemes have shown superiority in ensuring either the\ndiscriminability or faithfulness of the CAM, but they normally cannot excel in\nboth properties. In this paper, we propose a novel CAM weighting scheme, named\nFD-CAM, to improve both the faithfulness and discriminability of the CAM-based\nCNN visual explanation. First, we improve the faithfulness and discriminability\nof the score-based weights by performing a grouped channel switching operation.\nSpecifically, for each channel, we compute its similarity group and switch the\ngroup of channels on or off simultaneously to compute changes in the class\nprediction score as the weights. Then, we combine the improved score-based\nweights with the conventional gradient-based weights so that the\ndiscriminability of the final CAM can be further improved. We perform extensive\ncomparisons with the state-of-the-art CAM algorithms. The quantitative and\nqualitative results show our FD-CAM can produce more faithful and more\ndiscriminative visual explanations of the CNNs. We also conduct experiments to\nverify the effectiveness of the proposed grouped channel switching and weight\ncombination scheme on improving the results. Our code is available at\nhttps://github.com/crishhh1998/FD-CAM.",
    "descriptor": "\nComments: Accepted by ICPR 2022 and also accepted by CVPR 2022 Explainable Artificial Intelligence for Computer Vision (XAI4CV) Workshop\n",
    "authors": [
      "Hui Li",
      "Zihao Li",
      "Rui Ma",
      "Tieru Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08792"
  },
  {
    "id": "arXiv:2206.08793",
    "title": "A general error analysis for randomized low-rank approximation methods",
    "abstract": "We propose a general error analysis related to the low-rank approximation of\na given real matrix in both the spectral and Frobenius norms. First, we derive\ndeterministic error bounds that hold with some minimal assumptions. Second, we\nderive error bounds in expectation in the non-standard Gaussian case, assuming\na non-trivial mean and a general covariance matrix for the random matrix\nvariable. The proposed analysis generalizes and improves the error bounds for\nspectral and Frobenius norms proposed by Halko, Martinsson and Tropp. Third, we\nconsider the Randomized Singular Value Decomposition and specialize our error\nbounds in expectation in this setting. Numerical experiments on an\ninstructional synthetic test case demonstrate the tightness of the new error\nbounds.",
    "descriptor": "",
    "authors": [
      "Youssef Diouane",
      "Selime G\u00fcrol",
      "Alexandre Scotto Di Perrotolo",
      "Xavier Vasseur"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08793"
  },
  {
    "id": "arXiv:2206.08794",
    "title": "The Importance of Background Information for Out of Distribution  Generalization",
    "abstract": "Domain generalization in medical image classification is an important problem\nfor trustworthy machine learning to be deployed in healthcare. We find that\nexisting approaches for domain generalization which utilize ground-truth\nabnormality segmentations to control feature attributions have poor\nout-of-distribution (OOD) performance relative to the standard baseline of\nempirical risk minimization (ERM). We investigate what regions of an image are\nimportant for medical image classification and show that parts of the\nbackground, that which is not contained in the abnormality segmentation,\nprovides helpful signal. We then develop a new task-specific mask which covers\nall relevant regions. Utilizing this new segmentation mask significantly\nimproves the performance of the existing methods on the OOD test sets. To\nobtain better generalization results than ERM, we find it necessary to scale up\nthe training data size in addition to the usage of these task-specific masks.",
    "descriptor": "\nComments: 6 pages, 2 figures\n",
    "authors": [
      "Jupinder Parmar",
      "Khaled Saab",
      "Brian Pogatchnik",
      "Daniel Rubin",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08794"
  },
  {
    "id": "arXiv:2206.08800",
    "title": "Self-supervised deep visual servoing for high precision peg-in-hole  insertion",
    "abstract": "Many industrial assembly tasks involve peg-in-hole like insertions with\nsub-millimeter tolerances which are challenging, even in highly calibrated\nrobot cells. Visual servoing can be employed to increase the robustness towards\nuncertainties in the system, however, state of the art methods either rely on\naccurate 3D models for synthetic renderings or manual involvement in\nacquisition of training data. We present a novel self-supervised visual\nservoing method for high precision peg-in-hole insertion, which is fully\nautomated and does not rely on synthetic data. We demonstrate its applicability\nfor insertion of electronic components into a printed circuit board with tight\ntolerances. We show that peg-in-hole insertion can be drastically sped up by\npreceding a robust but slow force-based insertion strategy with our proposed\nvisual servoing method, the configuration of which is fully autonomous.",
    "descriptor": "\nComments: Accepted at IEEE CASE 2022\n",
    "authors": [
      "Rasmus Laurvig Haugaard",
      "Anders Glent Buch",
      "Thorbj\u00f8rn Mosekj\u00e6r Iversen"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08800"
  },
  {
    "id": "arXiv:2206.08801",
    "title": "Video Shadow Detection via Spatio-Temporal Interpolation Consistency  Training",
    "abstract": "It is challenging to annotate large-scale datasets for supervised video\nshadow detection methods. Using a model trained on labeled images to the video\nframes directly may lead to high generalization error and temporal inconsistent\nresults. In this paper, we address these challenges by proposing a\nSpatio-Temporal Interpolation Consistency Training (STICT) framework to\nrationally feed the unlabeled video frames together with the labeled images\ninto an image shadow detection network training. Specifically, we propose the\nSpatial and Temporal ICT, in which we define two new interpolation schemes,\n\\textit{i.e.}, the spatial interpolation and the temporal interpolation. We\nthen derive the spatial and temporal interpolation consistency constraints\naccordingly for enhancing generalization in the pixel-wise classification task\nand for encouraging temporal consistent predictions, respectively. In addition,\nwe design a Scale-Aware Network for multi-scale shadow knowledge learning in\nimages, and propose a scale-consistency constraint to minimize the discrepancy\namong the predictions at different scales. Our proposed approach is extensively\nvalidated on the ViSha dataset and a self-annotated dataset. Experimental\nresults show that, even without video labels, our approach is better than most\nstate of the art supervised, semi-supervised or unsupervised image/video shadow\ndetection methods and other methods in related tasks. Code and dataset are\navailable at \\url{https://github.com/yihong-97/STICT}.",
    "descriptor": "\nComments: Accepted in CVPR2022\n",
    "authors": [
      "Xiao Lu",
      "Yihong Cao",
      "Sheng Liu",
      "Chengjiang Long",
      "Zipei Chen",
      "Xuanyu Zhou",
      "Yimin Yang",
      "Chunxia Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08801"
  },
  {
    "id": "arXiv:2206.08802",
    "title": "Open-Sampling: Exploring Out-of-Distribution data for Re-balancing  Long-tailed datasets",
    "abstract": "Deep neural networks usually perform poorly when the training dataset suffers\nfrom extreme class imbalance. Recent studies found that directly training with\nout-of-distribution data (i.e., open-set samples) in a semi-supervised manner\nwould harm the generalization performance. In this work, we theoretically show\nthat out-of-distribution data can still be leveraged to augment the minority\nclasses from a Bayesian perspective. Based on this motivation, we propose a\nnovel method called Open-sampling, which utilizes open-set noisy labels to\nre-balance the class priors of the training dataset. For each open-set\ninstance, the label is sampled from our pre-defined distribution that is\ncomplementary to the distribution of original class priors. We empirically show\nthat Open-sampling not only re-balances the class priors but also encourages\nthe neural network to learn separable representations. Extensive experiments\ndemonstrate that our proposed method significantly outperforms existing data\nre-balancing methods and can boost the performance of existing state-of-the-art\nmethods.",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Hongxin Wei",
      "Lue Tao",
      "Renchunzi Xie",
      "Lei Feng",
      "Bo An"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08802"
  },
  {
    "id": "arXiv:2206.08803",
    "title": "Local Characteristic Decomposition Based Central-Upwind Scheme",
    "abstract": "We propose novel less diffusive schemes for conservative one- and\ntwo-dimensional hyperbolic systems of nonlinear partial differential equations\n(PDEs). The main challenges in the development of accurate and robust numerical\nmethods for the studied systems come from the complicated wave structures, such\nas shocks, rarefactions and contact discontinuities, arising even for smooth\ninitial conditions. In order to reduce the diffusion in the original\ncentral-upwind schemes, we use a local characteristic decomposition procedure\nto develop a new class of central-upwind schemes. We apply the developed\nschemes to the one- and two-dimensional Euler equations of gas dynamics to\nillustrate the performance on a variety of examples. The obtained numerical\nresults clearly demonstrate that the proposed new schemes outperform the\noriginal central-upwind schemes.",
    "descriptor": "",
    "authors": [
      "Alina Chertock",
      "Shaoshuai Chu",
      "Michael Herty",
      "Alexander Kurganov",
      "Maria Lukavcova-Medvidova"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.08803"
  },
  {
    "id": "arXiv:2206.08804",
    "title": "Truly Unordered Probabilistic Rule Sets for Multi-class Classification",
    "abstract": "Rule set learning has long been studied and has recently been frequently\nrevisited due to the need for interpretable models. Still, existing methods\nhave several shortcomings: 1) most recent methods require a binary feature\nmatrix as input, learning rules directly from numeric variables is\nunderstudied; 2) existing methods impose orders among rules, either explicitly\nor implicitly, which harms interpretability; and 3) currently no method exists\nfor learning probabilistic rule sets for multi-class target variables (there is\nonly a method for probabilistic rule lists).\nWe propose TURS, for Truly Unordered Rule Sets, which addresses these\nshortcomings. We first formalise the problem of learning truly unordered rule\nsets. To resolve conflicts caused by overlapping rules, i.e., instances covered\nby multiple rules, we propose a novel approach that exploits the probabilistic\nproperties of our rule sets. We next develop a two-phase heuristic algorithm\nthat learns rule sets by carefully growing rules. An important innovation is\nthat we use a surrogate score to take the global potential of the rule set into\naccount when learning a local rule.\nFinally, we empirically demonstrate that, compared to non-probabilistic and\n(explicitly or implicitly) ordered state-of-the-art methods, our method learns\nrule sets that not only have better interpretability (i.e., they are smaller\nand truly unordered), but also better predictive performance.",
    "descriptor": "\nComments: Accepted for publication at ECMLPKDD 2022\n",
    "authors": [
      "Lincen Yang",
      "Matthijs van Leeuwen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08804"
  },
  {
    "id": "arXiv:2206.08805",
    "title": "Complexity of the Multiobjective Spanner Problem",
    "abstract": "In this paper, we take an in-depth look at the complexity of a hitherto\nunexplored Multiobjective Spanner (MSp) problem. The MSp is a multiobjective\ngeneralization of the well-studied Minimum t-Spanner problem. This\nmultiobjective approach allows us to find solutions that offer a viable\ncompromise between cost and utility. Thus, the MSp can be a powerful modeling\ntool when it comes to the planning of, e.g., infrastructure. We show that for\ndegree-3 bounded outerplanar instances the MSp is intractable and computing the\nnon-dominated set is BUCO-hard. Additionally, we prove that if P != NP, neither\nthe non-dominated set nor the set of extreme points can be computed in\noutput-polynomial time, for instances with unit costs and arbitrary graphs.\nFurthermore, we consider the directed versions of the cases above.",
    "descriptor": "",
    "authors": [
      "Fritz B\u00f6kler",
      "Henning Jasper"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.08805"
  },
  {
    "id": "arXiv:2206.08809",
    "title": "Holistic Transformer: A Joint Neural Network for Trajectory Prediction  and Decision-Making of Autonomous Vehicles",
    "abstract": "Trajectory prediction and behavioral decision-making are two important tasks\nfor autonomous vehicles that require good understanding of the environmental\ncontext; behavioral decisions are better made by referring to the outputs of\ntrajectory predictions. However, most current solutions perform these two tasks\nseparately. Therefore, a joint neural network that combines multiple cues is\nproposed and named as the holistic transformer to predict trajectories and make\nbehavioral decisions simultaneously. To better explore the intrinsic\nrelationships between cues, the network uses existing knowledge and adopts\nthree kinds of attention mechanisms: the sparse multi-head type for reducing\nnoise impact, feature selection sparse type for optimally using partial prior\nknowledge, and multi-head with sigmoid activation type for optimally using\nposteriori knowledge. Compared with other trajectory prediction models, the\nproposed model has better comprehensive performance and good interpretability.\nPerceptual noise robustness experiments demonstrate that the proposed model has\ngood noise robustness. Thus, simultaneous trajectory prediction and behavioral\ndecision-making combining multiple cues can reduce computational costs and\nenhance semantic relationships between scenes and agents.",
    "descriptor": "\nComments: 26 pages, 6 figures\n",
    "authors": [
      "Hongyu Hu",
      "Qi Wang",
      "Zhengguang Zhang",
      "Zhengyi Li",
      "Zhenhai Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08809"
  },
  {
    "id": "arXiv:2206.08821",
    "title": "Exploring Web3 From the View of Blockchain",
    "abstract": "Web3 is the most hyped concept from 2020 to date, greatly motivating the\nprosperity of the Internet of Value and Metaverse. However, no solid evidence\nstipulates the exact definition, criterion, or standard in the sense of such a\nbuzzword. To fill the gap, we aim to clarify the term in this work. We narrow\ndown the connotation of Web3 by separating it from high-level controversy\nargues and, instead, focusing on its protocol, architecture, and evaluation\nfrom the perspective of blockchain fields. Specifically, we have identified all\npotential architectural design types and evaluated each of them by employing\nthe scenario-based architecture evaluation method. The evaluation shows that\nexisting applications are neither secure nor adoptable as claimed. Meanwhile,\nwe also discuss opportunities and challenges surrounding the Web3 space and\nanswer several prevailing questions from communities. A primary result is that\nWeb3 still relies on traditional internet infrastructure, not as independent as\nadvocated. This report, as of June 2022, provides the first strict research on\nWeb3 in the view of blockchain. We hope that this work would provide a guide\nfor the development of future Web3 services.",
    "descriptor": "\nComments: Tech Report 2022\n",
    "authors": [
      "Qin Wang",
      "Rujia Li",
      "Qi Wang",
      "Shiping Chen",
      "Mark Ryan",
      "Thomas Hardjono"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08821"
  },
  {
    "id": "arXiv:2206.08823",
    "title": "Language with Vision: a Study on Grounded Word and Sentence Embeddings",
    "abstract": "Language grounding to vision is an active field of research aiming to enrich\ntext-based representations of word meanings by leveraging perceptual knowledge\nfrom vision. Despite many attempts at language grounding, it is still unclear\nhow to effectively inject visual knowledge into the word embeddings of a\nlanguage in such a way that a proper balance of textual and visual knowledge is\nmaintained. Some common concerns are the following. Is visual grounding\nbeneficial for abstract words or is its contribution only limited to concrete\nwords? What is the optimal way of bridging the gap between text and vision? How\nmuch do we gain by visually grounding textual embeddings? The present study\naddresses these questions by proposing a simple yet very effective grounding\napproach for pre-trained word embeddings. Our model aligns textual embeddings\nwith vision while largely preserving the distributional statistics that\ncharacterize word use in text corpora. By applying a learned alignment, we are\nable to generate visually grounded embeddings for unseen words, including\nabstract words. A series of evaluations on word similarity benchmarks shows\nthat visual grounding is beneficial not only for concrete words, but also for\nabstract words. We also show that our method for visual grounding offers\nadvantages for contextualized embeddings, but only when these are trained on\ncorpora of relatively modest size. Code and grounded embeddings for English are\navailable at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.",
    "descriptor": "",
    "authors": [
      "Hassan Shahmohammadi",
      "Maria Heitmeier",
      "Elnaz Shafaei-Bajestan",
      "Hendrik P. A. Lensch",
      "Harald Baayen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08823"
  },
  {
    "id": "arXiv:2206.08825",
    "title": "An integral equation method for the advection-diffusion equation on  time-dependent domains in the plane",
    "abstract": "Boundary integral methods are attractive for solving homogeneous linear\nconstant coefficient elliptic partial differential equations on complex\ngeometries, since they can offer accurate solutions with a computational cost\nthat is linear or close to linear in the number of discretization points on the\nboundary of the domain. However, these numerical methods are not\nstraightforward to apply to time-dependent equations, which often arise in\nscience and engineering. We address this problem with an integral\nequation-based solver for the advection-diffusion equation on moving and\ndeforming geometries in two space dimensions. In this method, an adaptive\nhigh-order accurate time-stepping scheme based on semi-implicit spectral\ndeferred correction is applied. One time-step then involves solving a sequence\nof non-homogeneous modified Helmholtz equations, a method known as elliptic\nmarching. Our solution methodology utilizes several recently developed methods,\nincluding special purpose quadrature, a function extension technique and a\nspectral Ewald method for the modified Helmholtz kernel. Special care is also\ntaken to handle the time-dependent geometries. The numerical method is tested\nthrough several numerical examples to demonstrate robustness, flexibility and\naccuracy",
    "descriptor": "",
    "authors": [
      "Fredrik Fryklund",
      "Sara P\u00e5lsson",
      "Anna-Karin Tornberg"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08825"
  },
  {
    "id": "arXiv:2206.08826",
    "title": "Multimodal Attention-based Deep Learning for Alzheimer's Disease  Diagnosis",
    "abstract": "Alzheimer's Disease (AD) is the most common neurodegenerative disorder with\none of the most complex pathogeneses, making effective and clinically\nactionable decision support difficult. The objective of this study was to\ndevelop a novel multimodal deep learning framework to aid medical professionals\nin AD diagnosis. We present a Multimodal Alzheimer's Disease Diagnosis\nframework (MADDi) to accurately detect the presence of AD and mild cognitive\nimpairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in\nthat we use cross-modal attention, which captures interactions between\nmodalities - a method not previously explored in this domain. We perform\nmulti-class classification, a challenging task considering the strong\nsimilarities between MCI and AD. We compare with previous state-of-the-art\nmodels, evaluate the importance of attention, and examine the contribution of\neach modality to the model's performance. MADDi classifies MCI, AD, and\ncontrols with 96.88% accuracy on a held-out test set. When examining the\ncontribution of different attention schemes, we found that the combination of\ncross-modal attention with self-attention performed the best, and no attention\nlayers in the model performed the worst, with a 7.9% difference in F1-Scores.\nOur experiments underlined the importance of structured clinical data to help\nmachine learning models contextualize and interpret the remaining modalities.\nExtensive ablation studies showed that any multimodal mixture of input features\nwithout access to structured clinical information suffered marked performance\nlosses. This study demonstrates the merit of combining multiple input\nmodalities via cross-modal attention to deliver highly accurate AD diagnostic\ndecision support.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Michal Golovanevsky",
      "Carsten Eickhoff",
      "Ritambhara Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.08826"
  },
  {
    "id": "arXiv:2206.08829",
    "title": "FedNew: A Communication-Efficient and Privacy-Preserving Newton-Type  Method for Federated Learning",
    "abstract": "Newton-type methods are popular in federated learning due to their fast\nconvergence. Still, they suffer from two main issues, namely: low communication\nefficiency and low privacy due to the requirement of sending Hessian\ninformation from clients to parameter server (PS). In this work, we introduced\na novel framework called FedNew in which there is no need to transmit Hessian\ninformation from clients to PS, hence resolving the bottleneck to improve\ncommunication efficiency. In addition, FedNew hides the gradient information\nand results in a privacy-preserving approach compared to the existing\nstate-of-the-art. The core novel idea in FedNew is to introduce a two level\nframework, and alternate between updating the inverse Hessian-gradient product\nusing only one alternating direction method of multipliers (ADMM) step and then\nperforming the global model update using Newton's method. Though only one ADMM\npass is used to approximate the inverse Hessian-gradient product at each\niteration, we develop a novel theoretical approach to show the converging\nbehavior of FedNew for convex problems. Additionally, a significant reduction\nin communication overhead is achieved by utilizing stochastic quantization.\nNumerical results using real datasets show the superiority of FedNew compared\nto existing methods in terms of communication costs.",
    "descriptor": "",
    "authors": [
      "Anis Elgabli",
      "Chaouki Ben Issaid",
      "Amrit S. Bedi",
      "Ketan Rajawat",
      "Mehdi Bennis",
      "Vaneet Aggarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08829"
  },
  {
    "id": "arXiv:2206.08832",
    "title": "Prediction of Solar Radiation Based on Spatial and Temporal Embeddings  for Solar Generation Forecast",
    "abstract": "A novel method for real-time solar generation forecast using weather data,\nwhile exploiting both spatial and temporal structural dependencies is proposed.\nThe network observed over time is projected to a lower-dimensional\nrepresentation where a variety of weather measurements are used to train a\nstructured regression model while weather forecast is used at the inference\nstage. Experiments were conducted at 288 locations in the San Antonio, TX area\non obtained from the National Solar Radiation Database. The model predicts\nsolar irradiance with a good accuracy (R2 0.91 for the summer, 0.85 for the\nwinter, and 0.89 for the global model). The best accuracy was obtained by the\nRandom Forest Regressor. Multiple experiments were conducted to characterize\ninfluence of missing data and different time horizons providing evidence that\nthe new algorithm is robust for data missing not only completely at random but\nalso when the mechanism is spatial, and temporal.",
    "descriptor": "\nComments: Proceedings of the 53rd IEEE Hawaii International Conference on System Sciences (HICSS 2020)\n",
    "authors": [
      "Mohammad Alqudah",
      "Tatjana Dokic",
      "Mladen Kezunovic",
      "Zoran Obradovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08832"
  },
  {
    "id": "arXiv:2206.08833",
    "title": "A Comparative Study of Confidence Calibration in Deep Learning: From  Computer Vision to Medical Imaging",
    "abstract": "Although deep learning prediction models have been successful in the\ndiscrimination of different classes, they can often suffer from poor\ncalibration across challenging domains including healthcare. Moreover, the\nlong-tail distribution poses great challenges in deep learning classification\nproblems including clinical disease prediction. There are approaches proposed\nrecently to calibrate deep prediction in computer vision, but there are no\nstudies found to demonstrate how the representative models work in different\nchallenging contexts. In this paper, we bridge the confidence calibration from\ncomputer vision to medical imaging with a comparative study of four high-impact\ncalibration models. Our studies are conducted in different contexts (natural\nimage classification and lung cancer risk estimation) including in balanced vs.\nimbalanced training sets and in computer vision vs. medical imaging. Our\nresults support key findings: (1) We achieve new conclusions which are not\nstudied under different learning contexts, e.g., combining two calibration\nmodels that both mitigate the overconfident prediction can lead to\nunder-confident prediction, and simpler calibration models from the computer\nvision domain tend to be more generalizable to medical imaging. (2) We\nhighlight the gap between general computer vision tasks and medical imaging\nprediction, e.g., calibration methods ideal for general computer vision tasks\nmay in fact damage the calibration of medical imaging prediction. (3) We also\nreinforce previous conclusions in natural image classification settings. We\nbelieve that this study has merits to guide readers to choose calibration\nmodels and understand gaps between general computer vision and medical imaging\ndomains.",
    "descriptor": "\nComments: 17 pages, 6 figures\n",
    "authors": [
      "Riqiang Gao",
      "Thomas Li",
      "Yucheng Tang",
      "Zhoubing Xu",
      "Michael Kammer",
      "Sanja L. Antic",
      "Kim Sandler",
      "Fabien Moldonado",
      "Thomas A. Lasko",
      "Bennett Landman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08833"
  },
  {
    "id": "arXiv:2206.08835",
    "title": "What can Speech and Language Tell us About the Working Alliance in  Psychotherapy",
    "abstract": "We are interested in the problem of conversational analysis and its\napplication to the health domain. Cognitive Behavioral Therapy is a structured\napproach in psychotherapy, allowing the therapist to help the patient to\nidentify and modify the malicious thoughts, behavior, or actions. This\ncooperative effort can be evaluated using the Working Alliance Inventory\nObserver-rated Shortened - a 12 items inventory covering task, goal, and\nrelationship - which has a relevant influence on therapeutic outcomes. In this\nwork, we investigate the relation between this alliance inventory and the\nspoken conversations (sessions) between the patient and the psychotherapist. We\nhave delivered eight weeks of e-therapy, collected their audio and video call\nsessions, and manually transcribed them. The spoken conversations have been\nannotated and evaluated with WAI ratings by professional therapists. We have\ninvestigated speech and language features and their association with WAI items.\nThe feature types include turn dynamics, lexical entrainment, and\nconversational descriptors extracted from the speech and language signals. Our\nfindings provide strong evidence that a subset of these features are strong\nindicators of working alliance. To the best of our knowledge, this is the first\nand a novel study to exploit speech and language for characterising working\nalliance.",
    "descriptor": "\nComments: Accepted at Interspeech 2022\n",
    "authors": [
      "Sebastian P. Bayerl",
      "Gabriel Roccabruna",
      "Shammur Absar Chowdhury",
      "Tommaso Ciulli",
      "Morena Danieli",
      "Korbinian Riedhammer",
      "Giuseppe Riccardi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08835"
  },
  {
    "id": "arXiv:2206.08839",
    "title": "Decentralized adaptive clustering of deep nets is beneficial for client  collaboration",
    "abstract": "We study the problem of training personalized deep learning models in a\ndecentralized peer-to-peer setting, focusing on the setting where data\ndistributions differ between the clients and where different clients have\ndifferent local learning tasks. We study both covariate and label shift, and\nour contribution is an algorithm which for each client finds beneficial\ncollaborations based on a similarity estimate for the local task. Our method\ndoes not rely on hyperparameters which are hard to estimate, such as the number\nof client clusters, but rather continuously adapts to the network topology\nusing soft cluster assignment based on a novel adaptive gossip algorithm. We\ntest the proposed method in various settings where data is not independent and\nidentically distributed among the clients. The experimental evaluation shows\nthat the proposed method performs better than previous state-of-the-art\nalgorithms for this problem setting, and handles situations well where previous\nmethods fail.",
    "descriptor": "",
    "authors": [
      "Edvin Listo Zec",
      "Ebba Ekblom",
      "Martin Willbo",
      "Olof Mogren",
      "Sarunas Girdzijauskas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08839"
  },
  {
    "id": "arXiv:2206.08841",
    "title": "Random projections and Kernelised Leave One Cluster Out  Cross-Validation: Universal baselines and evaluation tools for supervised  machine learning for materials properties",
    "abstract": "With machine learning being a popular topic in current computational\nmaterials science literature, creating representations for compounds has become\ncommon place. These representations are rarely compared, as evaluating their\nperformance - and the performance of the algorithms that they are used with -\nis non-trivial. With many materials datasets containing bias and skew caused by\nthe research process, leave one cluster out cross validation (LOCO-CV) has been\nintroduced as a way of measuring the performance of an algorithm in predicting\npreviously unseen groups of materials. This raises the question of the impact,\nand control, of the range of cluster sizes on the LOCO-CV measurement outcomes.\nWe present a thorough comparison between composition-based representations, and\ninvestigate how kernel approximation functions can be used to better separate\ndata to enhance LOCO-CV applications.\nWe find that domain knowledge does not improve machine learning performance\nin most tasks tested, with band gap prediction being the notable exception. We\nalso find that the radial basis function improves the linear separability of\nchemical datasets in all 10 datasets tested and provide a framework for the\napplication of this function in the LOCO-CV process to improve the outcome of\nLOCO-CV measurements regardless of machine learning algorithm, choice of\nmetric, and choice of compound representation. We recommend kernelised LOCO-CV\nas a training paradigm for those looking to measure the extrapolatory power of\nan algorithm on materials data.",
    "descriptor": "\nComments: 16 pages including references, 9 figures\n",
    "authors": [
      "Samantha Durdy",
      "Michael Gaultois",
      "Vladimir Gusev",
      "Danushka Bollegala",
      "Matthew J. Rosseinsky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ],
    "url": "https://arxiv.org/abs/2206.08841"
  },
  {
    "id": "arXiv:2206.08842",
    "title": "Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product  Retrieval",
    "abstract": "Our goal in this research is to study a more realistic environment in which\nwe can conduct weakly-supervised multi-modal instance-level product retrieval\nfor fine-grained product categories. We first contribute the Product1M\ndatasets, and define two real practical instance-level retrieval tasks to\nenable the evaluations on the price comparison and personalized\nrecommendations. For both instance-level tasks, how to accurately pinpoint the\nproduct target mentioned in the visual-linguistic data and effectively decrease\nthe influence of irrelevant contents is quite challenging. To address this, we\nexploit to train a more effective cross-modal pertaining model which is\nadaptively capable of incorporating key concept information from the\nmulti-modal data, by using an entity graph whose node and edge respectively\ndenote the entity and the similarity relation between entities. Specifically, a\nnovel Entity-Graph Enhanced Cross-Modal Pretraining (EGE-CMP) model is proposed\nfor instance-level commodity retrieval, that explicitly injects entity\nknowledge in both node-based and subgraph-based ways into the multi-modal\nnetworks via a self-supervised hybrid-stream transformer, which could reduce\nthe confusion between different object contents, thereby effectively guiding\nthe network to focus on entities with real semantic. Experimental results well\nverify the efficacy and generalizability of our EGE-CMP, outperforming several\nSOTA cross-modal baselines like CLIP, UNITER and CAPTURE.",
    "descriptor": "",
    "authors": [
      "Xiao Dong",
      "Xunlin Zhan",
      "Yunchao Wei",
      "Xiaoyong Wei",
      "Yaowei Wang",
      "Minlong Lu",
      "Xiaochun Cao",
      "Xiaodan Liang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.08842"
  },
  {
    "id": "arXiv:2206.08843",
    "title": "AutoML Two-Sample Test",
    "abstract": "Two-sample tests are important in statistics and machine learning, both as\ntools for scientific discovery as well as to detect distribution shifts. This\nled to the development of many sophisticated test procedures going beyond the\nstandard supervised learning frameworks, whose usage can require specialized\nknowledge about two-sample testing. We use a simple test that takes the mean\ndiscrepancy of a witness function as the test statistic and prove that\nminimizing a squared loss leads to a witness with optimal testing power. This\nallows us to leverage recent advancements in AutoML. Without any user input\nabout the problems at hand, and using the same method for all our experiments,\nour AutoML two-sample test achieves competitive performance on a diverse\ndistribution shift benchmark as well as on challenging two-sample testing\nproblems.\nWe provide an implementation of the AutoML two-sample test in the Python\npackage autotst.",
    "descriptor": "",
    "authors": [
      "Jonas M. K\u00fcbler",
      "Vincent Stimper",
      "Simon Buchholz",
      "Krikamol Muandet",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08843"
  },
  {
    "id": "arXiv:2206.08849",
    "title": "On the Bug-proneness of Structures Inspired by Functional Programming in  JavaScript Projects",
    "abstract": "Language constructs inspired by functional programming have made their way\ninto most mainstream programming languages. Many researchers and developers\nconsider that these constructs lead to programs that are more concise,\nreusable, and easier to understand. However, few studies investigate the\nimplications of using them in mainstream programming languages. This paper\nquantifies the prevalence of four concepts typically associated with functional\nprogramming in JavaScript: recursion, immutability, lazy evaluation, and\nfunctions as values. We focus on JavaScript programs due to the availability of\nsome of these concepts in the language since its inception, its inspiration\nfrom functional programming languages, and its popularity. We mine 91 GitHub\nrepositories (22+ million LOC) written mostly in JavaScript (over 50% of the\ncode), measuring the usage of these concepts from both static and temporal\nperspectives. We also measure the likelihood of bug-fixing commits removing\nuses of these concepts (which would hint at bug-proneness) and their\nassociation with the presence of code comments (which would hint at code that\nis hard to understand). We find that these concepts are in widespread use (1\nfor every 46.65 LOC, 43.59% of LOC). In addition, the usage of higher-order\nfunctions, immutability, and lazy evaluation-related structures has been\ngrowing throughout the years for the analyzed projects, while the usage of\nrecursion and callbacks & promises has decreased. We also find statistical\nevidence that removing these structures, with the exception of the ones\nassociated to immutability, is less common in bug-fixing commits than in other\ncommits. In addition, their presence is not correlated with comment size. Our\nfindings suggest that functional programming concepts are important for\ndevelopers using a multi-paradigm language, and their usage does not make\nprograms harder to understand.",
    "descriptor": "",
    "authors": [
      "Fernando Alves",
      "Delano Oliveira",
      "Fernanda Madeiral",
      "Fernando Castor"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.08849"
  },
  {
    "id": "arXiv:2206.08851",
    "title": "SMPL: Simulated Industrial Manufacturing and Process Control Learning  Environments",
    "abstract": "Traditional biological and pharmaceutical manufacturing plants are controlled\nby human workers or pre-defined thresholds. Modernized factories have advanced\nprocess control algorithms such as model predictive control (MPC). However,\nthere is little exploration of applying deep reinforcement learning to control\nmanufacturing plants. One of the reasons is the lack of high fidelity\nsimulations and standard APIs for benchmarking. To bridge this gap, we develop\nan easy-to-use library that includes five high-fidelity simulation\nenvironments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which\ncover a wide range of manufacturing processes. We build these environments on\npublished dynamics models. Furthermore, we benchmark online and offline,\nmodel-based and model-free reinforcement learning algorithms for comparisons of\nfollow-up research.",
    "descriptor": "\nComments: Neurips 2022 Preprint. Under review\n",
    "authors": [
      "Mohan Zhang",
      "Xiaozhou Wang",
      "Benjamin Decardi-Nelson",
      "Bo Song",
      "An Zhang",
      "Jinfeng Liu",
      "Sile Tao",
      "Jiayi Cheng",
      "Xiaohong Liu",
      "DengDeng Yu",
      "Matthew Poon",
      "Animesh Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08851"
  },
  {
    "id": "arXiv:2206.08852",
    "title": "Channel-wise Mixed-precision Assignment for DNN Inference on Constrained  Edge Nodes",
    "abstract": "Quantization is widely employed in both cloud and edge systems to reduce the\nmemory occupation, latency, and energy consumption of deep neural networks. In\nparticular, mixed-precision quantization, i.e., the use of different bit-widths\nfor different portions of the network, has been shown to provide excellent\nefficiency gains with limited accuracy drops, especially with optimized\nbit-width assignments determined by automated Neural Architecture Search (NAS)\ntools. State-of-the-art mixed-precision works layer-wise, i.e., it uses\ndifferent bit-widths for the weights and activations tensors of each network\nlayer. In this work, we widen the search space, proposing a novel NAS that\nselects the bit-width of each weight tensor channel independently. This gives\nthe tool the additional flexibility of assigning a higher precision only to the\nweights associated with the most informative features. Testing on the MLPerf\nTiny benchmark suite, we obtain a rich collection of Pareto-optimal models in\nthe accuracy vs model size and accuracy vs energy spaces. When deployed on the\nMPIC RISC-V edge processor, our networks reduce the memory and energy for\ninference by up to 63% and 27% respectively compared to a layer-wise approach,\nfor the same accuracy.",
    "descriptor": "",
    "authors": [
      "Matteo Risso",
      "Alessio Burrello",
      "Luca Benini",
      "Enrico Macii",
      "Massimo Poncino",
      "Daniele Jahier Pagliari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08852"
  },
  {
    "id": "arXiv:2206.08853",
    "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale  Knowledge",
    "abstract": "Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite and knowledge bases\n(https://minedojo.org) to promote research towards the goal of generally\ncapable embodied agents.",
    "descriptor": "",
    "authors": [
      "Linxi Fan",
      "Guanzhi Wang",
      "Yunfan Jiang",
      "Ajay Mandlekar",
      "Yuncong Yang",
      "Haoyi Zhu",
      "Andrew Tang",
      "De-An Huang",
      "Yuke Zhu",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08853"
  },
  {
    "id": "arXiv:2206.08855",
    "title": "Intelligent Trading System: Multidimensional financial time series  clustering based on self-organizing map",
    "abstract": "Multidimensional time series clustering is an important problem in time\nseries data analysis. This paper provides a new research idea for the\nbehavioral analysis of financial markets, using the intrinsic correlation\nexisting between transactions in the same segment of the financial market to\ncluster and analyze multidimensional time-series data, so as to obtain\ndifferent types of market characteristics. In this paper, we propose a\nmultidimensional time series clustering model based on graph attention\nautoencoder (GATE) and mask self-organizing map (Mask-SOM), based on which we\nrealize multi-step prediction of financial derivatives prices and intelligent\ntrading system construction. To obtain and fully utilize the correlation\nfeatures between multidimensional financial time series data containing high\nnoise for clustering analysis, constant curvature Riemannian manifolds are\nintroduced in the graph attention autoencoder, and the multidimensional\nfinancial time series features captured by the encoder are embedded into the\nmanifold. Following that, the multidimensional financial time series clustering\nanalysis is implemented using Mask-SOM analysis manifold encoding. Finally, the\nfeasibility and effectiveness of the model are verified using real financial\ndatasets.",
    "descriptor": "",
    "authors": [
      "Pei Dehao",
      "Luo Chao"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.08855"
  },
  {
    "id": "arXiv:2206.08856",
    "title": "SwarmHive: Heterogeneous Swarm of Drones for Robust Autonomous Landing  on Moving Robot",
    "abstract": "The paper focuses on a heterogeneous swarm of drones to achieve a dynamic\nlanding of formation on a moving robot. This challenging task was not yet\nachieved by scientists. The key technology is that instead of facilitating each\nagent of the swarm of drones with computer vision that considerably increases\nthe payload and shortens the flight time, we propose to install only one camera\non the leader drone. The follower drones receive the commands from the leader\nUAV and maintain a collision-free trajectory with the artificial potential\nfield. The experimental results revealed a high accuracy of the swarm landing\non a static mobile platform (RMSE of 4.48 cm). RMSE of swarm landing on the\nmobile platform moving with the maximum velocities of 1.0 m/s and 1.5 m/s\nequals 8.76 cm and 8.98 cm, respectively. The proposed SwarmHive technology\nwill allow the time-saving landing of the swarm for further drone recharging.\nThis will make it possible to achieve self-sustainable operation of a\nmulti-agent robotic system for such scenarios as rescue operations, inspection\nand maintenance, autonomous warehouse inventory, cargo delivery, and etc.",
    "descriptor": "\nComments: Accepted paper at IEEE Vehicular Technology Conference 2022 (IEEE VTC 2022), IEEE copyright\n",
    "authors": [
      "Ayush Gupta",
      "Ahmed Baza",
      "Ekaterina Dorzhieva",
      "Mert Alper",
      "Mariia Makarova",
      "Stepan Perminov",
      "Aleksey Fedoseev",
      "Dzmitry Tsetserukou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08856"
  },
  {
    "id": "arXiv:2206.08861",
    "title": "DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide  Image Classification",
    "abstract": "Multiple Instance Learning (MIL) is widely used in analyzing\nhistopathological Whole Slide Images (WSIs). However, existing MIL methods do\nnot explicitly model the data distribution, and instead they only learn a\nbag-level or instance-level decision boundary discriminatively by training a\nclassifier. In this paper, we propose DGMIL: a feature distribution guided deep\nMIL framework for WSI classification and positive patch localization. Instead\nof designing complex discriminative network architectures, we reveal that the\ninherent feature distribution of histopathological image data can serve as a\nvery effective guide for instance classification. We propose a\ncluster-conditioned feature distribution modeling method and a pseudo\nlabel-based iterative feature space refinement strategy so that in the final\nfeature space the positive and negative instances can be easily separated.\nExperiments on the CAMELYON16 dataset and the TCGA Lung Cancer dataset show\nthat our method achieves new SOTA for both global classification and positive\npatch localization tasks.",
    "descriptor": "\nComments: accepted by MICCAI 2022\n",
    "authors": [
      "Linhao Qu",
      "Xiaoyuan Luo",
      "Shaolei Liu",
      "Manning Wang",
      "Zhijian Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08861"
  },
  {
    "id": "arXiv:2206.08862",
    "title": "Analysis of Time- versus Event-Triggered Consensus for a  Single-Integrator Multi-Agent System",
    "abstract": "It is well known that the employed triggering scheme has great impact on the\ncontrol performance when control loops operate under scarce communication\nresources. Various practical and simulative works have demonstrated the\npotential of event-triggered control to reduce communication while providing a\nsimilar performance level when compared to time-triggered control. For\nnon-cooperative networked control systems, analytical performance comparisons\nof time- and event-triggered control support this finding under certain\nassumptions. While being well-studied in the non-cooperative setting, it\nremains unclear if and how the performance relationship of the triggering\nschemes is altered in a multi-agent system setup. To close this gap, in this\npaper, we consider a homogeneous single-integrator multi-agent consensus\nproblem for which we compare the performance of time- and event-triggered\ncontrol schemes analytically. Under the assumption of equal average triggering\nrates, we use the long-term average of the quadratic deviation from consensus\nas a performance measure to contrast the triggering schemes. Contrary to the\nnon-cooperative setting, we prove that event-triggered control performs worse\nthan time-triggered control beyond a certain number of agents in this setup. In\naddition, we derive the asymptotic order of the performance measure as a\nfunction of the number of agents under both triggering schemes.",
    "descriptor": "\nComments: 7 pages, 1 figure\n",
    "authors": [
      "David Meister",
      "Frank Aurzada",
      "Mikhail Lifshits",
      "Frank Allg\u00f6wer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08862"
  },
  {
    "id": "arXiv:2206.08864",
    "title": "Avoid Overfitting User Specific Information in Federated Keyword  Spotting",
    "abstract": "Keyword spotting (KWS) aims to discriminate a specific wake-up word from\nother signals precisely and efficiently for different users. Recent works\nutilize various deep networks to train KWS models with all users' speech data\ncentralized without considering data privacy. Federated KWS (FedKWS) could\nserve as a solution without directly sharing users' data. However, the small\namount of data, different user habits, and various accents could lead to fatal\nproblems, e.g., overfitting or weight divergence. Hence, we propose several\nstrategies to encourage the model not to overfit user-specific information in\nFedKWS. Specifically, we first propose an adversarial learning strategy, which\nupdates the downloaded global model against an overfitted local model and\nexplicitly encourages the global model to capture user-invariant information.\nFurthermore, we propose an adaptive local training strategy, letting clients\nwith more training data and more uniform class distributions undertake more\nlocal update steps. Equivalently, this strategy could weaken the negative\nimpacts of those users whose data is less qualified. Our proposed FedKWS-UI\ncould explicitly and implicitly learn user-invariant information in FedKWS.\nAbundant experimental results on federated Google Speech Commands verify the\neffectiveness of FedKWS-UI.",
    "descriptor": "\nComments: Accepted by Interspeech 2022\n",
    "authors": [
      "Xin-Chun Li",
      "Jin-Lin Tang",
      "Shaoming Song",
      "Bingshuai Li",
      "Yinchuan Li",
      "Yunfeng Shao",
      "Le Gan",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08864"
  },
  {
    "id": "arXiv:2206.08869",
    "title": "Fast Lossless Neural Compression with Integer-Only Discrete Flows",
    "abstract": "By applying entropy codecs with learned data distributions, neural\ncompressors have significantly outperformed traditional codecs in terms of\ncompression ratio. However, the high inference latency of neural networks\nhinders the deployment of neural compressors in practical applications. In this\nwork, we propose Integer-only Discrete Flows (IODF), an efficient neural\ncompressor with integer-only arithmetic. Our work is built upon integer\ndiscrete flows, which consists of invertible transformations between discrete\nrandom variables. We propose efficient invertible transformations with\ninteger-only arithmetic based on 8-bit quantization. Our invertible\ntransformation is equipped with learnable binary gates to remove redundant\nfilters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x\ninference speedup compared to the fastest existing neural compressors, while\nretaining the high compression rates on ImageNet32 and ImageNet64.",
    "descriptor": "\nComments: Accepted as a conference paper at International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Siyu Wang",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.08869"
  },
  {
    "id": "arXiv:2206.08871",
    "title": "How robust are pre-trained models to distribution shift?",
    "abstract": "The vulnerability of machine learning models to spurious correlations has\nmostly been discussed in the context of supervised learning (SL). However,\nthere is a lack of insight on how spurious correlations affect the performance\nof popular self-supervised learning (SSL) and auto-encoder based models (AE).\nIn this work, we shed light on this by evaluating the performance of these\nmodels on both real world and synthetic distribution shift datasets. Following\nobservations that the linear head itself can be susceptible to spurious\ncorrelations, we develop a novel evaluation scheme with the linear head trained\non out-of-distribution (OOD) data, to isolate the performance of the\npre-trained models from a potential bias of the linear head used for\nevaluation. With this new methodology, we show that SSL models are consistently\nmore robust to distribution shifts and thus better at OOD generalisation than\nAE and SL models.",
    "descriptor": "",
    "authors": [
      "Yuge Shi",
      "Imant Daunhawer",
      "Julia E. Vogt",
      "Philip H.S. Torr",
      "Amartya Sanyal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08871"
  },
  {
    "id": "arXiv:2206.08874",
    "title": "SwarmHawk: Self-Sustaining Multi-Agent System for Landing on a Moving  Platform through an Agent Supervision",
    "abstract": "Heterogeneous teams of mobile robots and UAVs are offering a substantial\nbenefit in an autonomous exploration of the environment. Nevertheless, although\njoint exploration scenarios for such systems are widely discussed, they are\nstill suffering from low adaptability to changes in external conditions and\nfaults of swarm agents during the UAV docking. We propose a novel vision-based\ndrone swarm docking system for robust landing on a moving platform when one of\nthe agents lost its position signal. The proposed SwarmHawk system relies on\nvision-based detection for the mobile platform tracking and navigation of its\nagents. Each drone of the swarm carries an RGB camera and AprilTag3 QR-code\nmarker on board. SwarmHawk can switch between two modes of operation, acting as\na homogeneous swarm in case of global UAV localization or assigning leader\ndrones to navigate its neighbors in case of a camera fault in one of the drones\nor global localization failure. Two experiments were performed to evaluate\nSwarmHawk's performance under the global and local localization with static and\nmoving platforms. The experimental results revealed a sufficient accuracy in\nthe swarm landing task on a static mobile platform (error of 4.2 cm in\nhomogeneous formation and 1.9 cm in leader-follower formation) and on moving\nplatform (error of 6.9 cm in homogeneous formation and 4.7 cm in\nleader-follower formation). Moreover, the drones showed a good landing on a\nplatform moving along a complex trajectory (average error of 19.4 cm) in\nleader-follower formation. The proposed SwarmHawk technology can be potentially\napplied in various swarm scenarios, including complex environment exploration,\ninspection, and drone delivery.",
    "descriptor": "\nComments: Accepted paper at IEEE International Conference on Unmanned Aircraft System (ICUAS 2022), IEEE copyright\n",
    "authors": [
      "Ayush Gupta",
      "Ekaterina Dorzhieva",
      "Ahmed Baza",
      "Mert Alper",
      "Aleksey Fedoseev",
      "Dzmitry Tsetserukou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08874"
  },
  {
    "id": "arXiv:2206.08880",
    "title": "Improving Generalization of Metric Learning via Listwise  Self-distillation",
    "abstract": "Most deep metric learning (DML) methods employ a strategy that forces all\npositive samples to be close in the embedding space while keeping them away\nfrom negative ones. However, such a strategy ignores the internal relationships\nof positive (negative) samples and often leads to overfitting, especially in\nthe presence of hard samples and mislabeled samples. In this work, we propose a\nsimple yet effective regularization, namely Listwise Self-Distillation (LSD),\nwhich progressively distills a model's own knowledge to adaptively assign a\nmore appropriate distance target to each sample pair in a batch. LSD encourages\nsmoother embeddings and information mining within positive (negative) samples\nas a way to mitigate overfitting and thus improve generalization. Our LSD can\nbe directly integrated into general DML frameworks. Extensive experiments show\nthat LSD consistently boosts the performance of various metric learning methods\non multiple datasets.",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Zelong Zeng",
      "Fan Yang",
      "Zheng Wang",
      "Shin'ichi Satoh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08880"
  },
  {
    "id": "arXiv:2206.08881",
    "title": "Logic-based Reward Shaping for Multi-Agent Reinforcement Learning",
    "abstract": "Reinforcement learning (RL) relies heavily on exploration to learn from its\nenvironment and maximize observed rewards. Therefore, it is essential to design\na reward function that guarantees optimal learning from the received\nexperience. Previous work has combined automata and logic based reward shaping\nwith environment assumptions to provide an automatic mechanism to synthesize\nthe reward function based on the task. However, there is limited work on how to\nexpand logic-based reward shaping to Multi-Agent Reinforcement Learning (MARL).\nThe environment will need to consider the joint state in order to keep track of\nother agents if the task requires cooperation, thus suffering from the curse of\ndimensionality with respect to the number of agents. This project explores how\nlogic-based reward shaping for MARL can be designed for different scenarios and\ntasks. We present a novel method for semi-centralized logic-based MARL reward\nshaping that is scalable in the number of agents and evaluate it in multiple\nscenarios.",
    "descriptor": "\nComments: 10 pages, technical report\n",
    "authors": [
      "Ingy ElSayed-Aly",
      "Lu Feng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.08881"
  },
  {
    "id": "arXiv:2206.08882",
    "title": "Edge-Aided Sensor Data Sharing in Vehicular Communication Networks",
    "abstract": "Sensor data sharing in vehicular networks can significantly improve the range\nand accuracy of environmental perception for connected automated vehicles.\nDifferent concepts and schemes for dissemination and fusion of sensor data have\nbeen developed. It is common to these schemes that measurement errors of the\nsensors impair the perception quality and can result in road traffic accidents.\nSpecifically, when the measurement error from the sensors (also referred as\nmeasurement noise) is unknown and time varying, the performance of the data\nfusion process is restricted, which represents a major challenge in the\ncalibration of sensors. In this paper, we consider sensor data sharing and\nfusion in a vehicular network with both, vehicle-to-infrastructure and\nvehicle-to-vehicle communication. We propose a method, named Bidirectional\nFeedback Noise Estimation (BiFNoE), in which an edge server collects and caches\nsensor measurement data from vehicles. The edge estimates the noise and the\ntargets alternately in double dynamic sliding time windows and enhances the\ndistributed cooperative environment sensing at each vehicle with low\ncommunication costs. We evaluate the proposed algorithm and data dissemination\nstrategy in an application scenario by simulation and show that the perception\naccuracy is on average improved by around 80 % with only 12 kbps uplink and 28\nkbps downlink bandwidth.",
    "descriptor": "\nComments: Accepted for IEEE 95th Vehicular Technology Conference (VTC2022-Spring)\n",
    "authors": [
      "Rui Song",
      "Anupama Hegde",
      "Numan Senel",
      "Alois Knoll",
      "Andreas Festag"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08882"
  },
  {
    "id": "arXiv:2206.08883",
    "title": "CtrlFormer: Learning Transferable State Representation for Visual  Control via Transformer",
    "abstract": "Transformer has achieved great successes in learning vision and language\nrepresentation, which is general across various downstream tasks. In visual\ncontrol, learning transferable state representation that can transfer between\ndifferent control tasks is important to reduce the training sample size.\nHowever, porting Transformer to sample-efficient visual control remains a\nchallenging and unsolved problem. To this end, we propose a novel Control\nTransformer (CtrlFormer), possessing many appealing benefits that prior arts do\nnot have. Firstly, CtrlFormer jointly learns self-attention mechanisms between\nvisual tokens and policy tokens among different control tasks, where multitask\nrepresentation can be learned and transferred without catastrophic forgetting.\nSecondly, we carefully design a contrastive reinforcement learning paradigm to\ntrain CtrlFormer, enabling it to achieve high sample efficiency, which is\nimportant in control problems. For example, in the DMControl benchmark, unlike\nrecent advanced methods that failed by producing a zero score in the \"Cartpole\"\ntask after transfer learning with 100k samples, CtrlFormer can achieve a\nstate-of-the-art score with only 100k samples while maintaining the performance\nof previous tasks. The code and models are released in our project homepage.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Yao Mu",
      "Shoufa Chen",
      "Mingyu Ding",
      "Jianyu Chen",
      "Runjian Chen",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08883"
  },
  {
    "id": "arXiv:2206.08884",
    "title": "Resolution Limits of Non-Adaptive 20 Questions Search for a Moving  Target",
    "abstract": "Using the 20 questions estimation framework with query-dependent noise, we\nstudy non-adaptive search strategies for a moving target over the unit cube\nwith unknown initial location and velocities under a piecewise constant\nvelocity model. In this search problem, there is an oracle who knows the\ninstantaneous location of the target at any time. Our task is to query the\noracle as few times as possible to accurately estimate the location of the\ntarget at any specified time. We first study the case where the oracle's answer\nto each query is corrupted by discrete noise and then generalize our results to\nthe case of additive white Gaussian noise. In our formulation, the performance\ncriterion is the resolution, which is defined as the maximal $L_\\infty$\ndistance between the true locations and estimated locations. We characterize\nthe minimal resolution of an optimal non-adaptive query procedure with a finite\nnumber of queries by deriving non-asymptotic and asymptotic bounds. Our bounds\nare tight in the first-order asymptotic sense when the number of queries\nsatisfies a certain condition and our bounds are tight in the stronger\nsecond-order asymptotic sense when the target moves with a constant velocity.\nTo prove our results, we relate the current problem to channel coding, borrow\nideas from finite blocklength information theory and construct bounds on the\nnumber of possible quantized target trajectories.",
    "descriptor": "\nComments: extended version of arXiv:2103.08097, under review in IEEE Transactions on Information Theory\n",
    "authors": [
      "Lin Zhou",
      "Alfred Hero"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08884"
  },
  {
    "id": "arXiv:2206.08888",
    "title": "Fast Population-Based Reinforcement Learning on a Single Machine",
    "abstract": "Training populations of agents has demonstrated great promise in\nReinforcement Learning for stabilizing training, improving exploration and\nasymptotic performance, and generating a diverse set of solutions. However,\npopulation-based training is often not considered by practitioners as it is\nperceived to be either prohibitively slow (when implemented sequentially), or\ncomputationally expensive (if agents are trained in parallel on independent\naccelerators). In this work, we compare implementations and revisit previous\nstudies to show that the judicious use of compilation and vectorization allows\npopulation-based training to be performed on a single machine with one\naccelerator with minimal overhead compared to training a single agent. We also\nshow that, when provided with a few accelerators, our protocols extend to large\npopulation sizes for applications such as hyperparameter tuning. We hope that\nthis work and the public release of our code will encourage practitioners to\nuse population-based learning more frequently for their research and\napplications.",
    "descriptor": "",
    "authors": [
      "Arthur Flajolet",
      "Claire Bizon Monroc",
      "Karim Beguir",
      "Thomas Pierrot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.08888"
  },
  {
    "id": "arXiv:2206.08890",
    "title": "Representational Multiplicity Should Be Exposed, Not Eliminated",
    "abstract": "It is prevalent and well-observed, but poorly understood, that two machine\nlearning models with similar performance during training can have very\ndifferent real-world performance characteristics. This implies elusive\ndifferences in the internals of the models, manifesting as representational\nmultiplicity (RM). We introduce a conceptual and experimental setup for\nanalyzing RM and show that certain training methods systematically result in\ngreater RM than others, measured by activation similarity via singular vector\ncanonical correlation analysis (SVCCA). We further correlate it with predictive\nmultiplicity measured by the variance in i.i.d. and out-of-distribution test\nset predictions, in four common image data sets. We call for systematic\nmeasurement and maximal exposure, not elimination, of RM in models. Qualitative\ntools such as our confabulator analysis can facilitate understanding and\ncommunication of RM effects to stakeholders.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Ari Heljakka",
      "Martin Trapp",
      "Juho Kannala",
      "Arno Solin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08890"
  },
  {
    "id": "arXiv:2206.08896",
    "title": "Evolution through Large Models",
    "abstract": "This paper pursues the insight that large language models (LLMs) trained to\ngenerate code can vastly improve the effectiveness of mutation operators\napplied to programs in genetic programming (GP). Because such LLMs benefit from\ntraining data that includes sequential changes and modifications, they can\napproximate likely changes that humans would make. To highlight the breadth of\nimplications of such evolution through large models (ELM), in the main\nexperiment ELM combined with MAP-Elites generates hundreds of thousands of\nfunctional examples of Python programs that output working ambulating robots in\nthe Sodarace domain, which the original LLM had never seen in pre-training.\nThese examples then help to bootstrap training a new conditional language model\nthat can output the right walker for a particular terrain. The ability to\nbootstrap new models that can output appropriate artifacts for a given context\nin a domain where zero training data was previously available carries\nimplications for open-endedness, deep learning, and reinforcement learning.\nThese implications are explored here in depth in the hope of inspiring new\ndirections of research now opened up by ELM.",
    "descriptor": "",
    "authors": [
      "Joel Lehman",
      "Jonathan Gordon",
      "Shawn Jain",
      "Kamal Ndousse",
      "Cathy Yeh",
      "Kenneth O. Stanley"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.08896"
  },
  {
    "id": "arXiv:2206.08898",
    "title": "SimA: Simple Softmax-free Attention for Vision Transformers",
    "abstract": "Recently, vision transformers have become very popular. However, deploying\nthem in many applications is computationally expensive partly due to the\nSoftmax layer in the attention block. We introduce a simple but effective,\nSoftmax-free attention block, SimA, which normalizes query and key matrices\nwith simple $\\ell_1$-norm instead of using Softmax layer. Then, the attention\nblock in SimA is a simple multiplication of three matrices, so SimA can\ndynamically change the ordering of the computation at the test time to achieve\nlinear computation on the number of tokens or the number of channels. We\nempirically show that SimA applied to three SOTA variations of transformers,\nDeiT, XCiT, and CvT, results in on-par accuracy compared to the SOTA models,\nwithout any need for Softmax layer. Interestingly, changing SimA from\nmulti-head to single-head has only a small effect on the accuracy, which\nsimplifies the attention block further. The code is available here:\n$\\href{https://github.com/UCDvision/sima}{\\text{This https URL}}$",
    "descriptor": "\nComments: Code is available here: $\\href{this https URL}{\\text{This https URL}}$\n",
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08898"
  },
  {
    "id": "arXiv:2206.08899",
    "title": "Popular decision tree algorithms are provably noise tolerant",
    "abstract": "Using the framework of boosting, we prove that all impurity-based decision\ntree learning algorithms, including the classic ID3, C4.5, and CART, are highly\nnoise tolerant. Our guarantees hold under the strongest noise model of nasty\nnoise, and we provide near-matching upper and lower bounds on the allowable\nnoise rate. We further show that these algorithms, which are simple and have\nlong been central to everyday machine learning, enjoy provable guarantees in\nthe noisy setting that are unmatched by existing algorithms in the theoretical\nliterature on decision tree learning. Taken together, our results add to an\nongoing line of research that seeks to place the empirical success of these\npractical decision tree algorithms on firm theoretical footing.",
    "descriptor": "\nComments: Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022\n",
    "authors": [
      "Guy Blanc",
      "Jane Lange",
      "Ali Malik",
      "Li-Yang Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08899"
  },
  {
    "id": "arXiv:2206.08903",
    "title": "Colonoscopy 3D Video Dataset with Paired Depth from 2D-3D Registration",
    "abstract": "Screening colonoscopy is an important clinical application for several 3D\ncomputer vision techniques, including depth estimation, surface reconstruction,\nand missing region detection. However, the development, evaluation, and\ncomparison of these techniques in real colonoscopy videos remain largely\nqualitative due to the difficulty of acquiring ground truth data. In this work,\nwe present a Colonoscopy 3D Video Dataset (C3VD) acquired with a high\ndefinition clinical colonoscope and high-fidelity colon models for benchmarking\ncomputer vision methods in colonoscopy. We introduce a novel multimodal 2D-3D\nregistration technique to register optical video sequences with ground truth\nrendered views of a known 3D model. The different modalities are registered by\ntransforming optical images to depth maps with a Generative Adversarial Network\nand aligning edge features with an evolutionary optimizer. This registration\nmethod achieves an average translation error of 0.321 millimeters and an\naverage rotation error of 0.159 degrees in simulation experiments where\nerror-free ground truth is available. The method also leverages video\ninformation, improving registration accuracy by 55.6% for translation and 60.4%\nfor rotation compared to single frame registration. 22 short video sequences\nwere registered to generate 10,015 total frames with paired ground truth depth,\nsurface normals, optical flow, occlusion, six degree-of-freedom pose, coverage\nmaps, and 3D models. The dataset also includes screening videos acquired by a\ngastroenterologist with paired ground truth pose and 3D surface models. The\ndataset and registration source code are available at durr.jhu.edu/C3VD.",
    "descriptor": "",
    "authors": [
      "Taylor L. Bobrow",
      "Mayank Golhar",
      "Rohan Vijayan",
      "Venkata S. Akshintala",
      "Juan R. Garcia",
      "Nicholas J. Durr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08903"
  },
  {
    "id": "arXiv:2206.08905",
    "title": "What makes Ethereum blockchain transactions be processed fast or slow?  An empirical study",
    "abstract": "The Ethereum platform allows developers to implement and deploy applications\ncalled Dapps onto the blockchain for public use through the use of smart\ncontracts. To execute code within a smart contract, a paid transaction must be\nissued towards one of the functions that are exposed in the interface of a\ncontract. However, such a transaction is only processed once one of the miners\nin the peer-to-peer network selects it, adds it to a block, and appends that\nblock to the blockchain This creates a delay between transaction submission and\ncode execution. It is crucial for Dapp developers to be able to precisely\nestimate when transactions will be processed, since this allows them to define\nand provide a certain Quality of Service (QoS) level (e.g., 95% of the\ntransactions processed within 1 minute). However, the impact that different\nfactors have on these times have not yet been studied. Processing time\nestimation services are used by Dapp developers to achieve predefined QoS. Yet,\nthese services offer minimal insights into what factors impact processing\ntimes. Considering the vast amount of data that surrounds the Ethereum\nblockchain, changes in processing times are hard for Dapp developers to\npredict, making it difficult to maintain said QoS. In our study, we build\nrandom forest models to understand the factors that are associated with\ntransaction processing times. We engineer several features that capture\nblockchain internal factors, as well as gas pricing behaviors of transaction\nissuers. By interpreting our models, we conclude that features surrounding gas\npricing behaviors are very strongly associated with transaction processing\ntimes. Based on our empirical results, we provide Dapp developers with concrete\ninsights that can help them provide and maintain high levels of QoS.",
    "descriptor": "\nComments: Under Peer review in Empirical Software Engineering Journal\n",
    "authors": [
      "Michael Pacheco",
      "Gustavo A. Oliva",
      "Gopi Krishnan Rajbahadur",
      "Ahmed E. Hassan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.08905"
  },
  {
    "id": "arXiv:2206.08910",
    "title": "niksss at HinglishEval: Language-agnostic BERT-based Contextual  Embeddings with Catboost for Quality Evaluation of the Low-Resource  Synthetically Generated Code-Mixed Hinglish Text",
    "abstract": "This paper describes the system description for the HinglishEval challenge at\nINLG 2022. The goal of this task was to investigate the factors influencing the\nquality of the code-mixed text generation system. The task was divided into two\nsubtasks, quality rating prediction and annotators disagreement prediction of\nthe synthetic Hinglish dataset. We attempted to solve these tasks using\nsentence-level embeddings, which are obtained from mean pooling the\ncontextualized word embeddings for all input tokens in our text. We\nexperimented with various classifiers on top of the embeddings produced for\nrespective tasks. Our best-performing system ranked 1st on subtask B and 3rd on\nsubtask A.",
    "descriptor": "",
    "authors": [
      "Nikhil Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08910"
  },
  {
    "id": "arXiv:2206.08916",
    "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
    "abstract": "We propose Unified-IO, a model that performs a large variety of AI tasks\nspanning classical computer vision tasks, including pose estimation, object\ndetection, depth estimation and image generation, vision-and-language tasks\nsuch as region captioning and referring expression comprehension, to natural\nlanguage processing tasks such as question answering and paraphrasing.\nDeveloping a single unified model for such a large variety of tasks poses\nunique challenges due to the heterogeneous inputs and outputs pertaining to\neach task, including RGB images, per-pixel maps, binary masks, bounding boxes,\nand language. We achieve this unification by homogenizing every supported input\nand output into a sequence of discrete vocabulary tokens. This common\nrepresentation across all tasks allows us to train a single transformer-based\narchitecture, jointly on over 80 diverse datasets in the vision and language\nfields. Unified-IO is the first model capable of performing all 7 tasks on the\nGRIT benchmark and produces strong results across 16 diverse benchmarks like\nNYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail,\nwith no task or benchmark specific fine-tuning. Demos for Unified-IO are\navailable at https://unified-io.allenai.org.",
    "descriptor": "",
    "authors": [
      "Jiasen Lu",
      "Christopher Clark",
      "Rowan Zellers",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08916"
  },
  {
    "id": "arXiv:2206.08918",
    "title": "Learning a Single Neuron with Adversarial Label Noise via Gradient  Descent",
    "abstract": "We study the fundamental problem of learning a single neuron, i.e., a\nfunction of the form $\\mathbf{x}\\mapsto\\sigma(\\mathbf{w}\\cdot\\mathbf{x})$ for\nmonotone activations $\\sigma:\\mathbb{R}\\mapsto\\mathbb{R}$, with respect to the\n$L_2^2$-loss in the presence of adversarial label noise. Specifically, we are\ngiven labeled examples from a distribution $D$ on $(\\mathbf{x},\ny)\\in\\mathbb{R}^d \\times \\mathbb{R}$ such that there exists\n$\\mathbf{w}^\\ast\\in\\mathbb{R}^d$ achieving $F(\\mathbf{w}^\\ast)=\\epsilon$, where\n$F(\\mathbf{w})=\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[(\\sigma(\\mathbf{w}\\cdot\n\\mathbf{x})-y)^2]$. The goal of the learner is to output a hypothesis vector\n$\\mathbf{w}$ such that $F(\\mathbb{w})=C\\, \\epsilon$ with high probability,\nwhere $C>1$ is a universal constant. As our main contribution, we give\nefficient constant-factor approximate learners for a broad class of\ndistributions (including log-concave distributions) and activation functions.\nConcretely, for the class of isotropic log-concave distributions, we obtain the\nfollowing important corollaries:\nFor the logistic activation, we obtain the first polynomial-time constant\nfactor approximation (even under the Gaussian distribution). Our algorithm has\nsample complexity $\\widetilde{O}(d/\\epsilon)$, which is tight within\npolylogarithmic factors.\nFor the ReLU activation, we give an efficient algorithm with sample\ncomplexity $\\tilde{O}(d\\, \\polylog(1/\\epsilon))$. Prior to our work, the best\nknown constant-factor approximate learner had sample complexity\n$\\tilde{\\Omega}(d/\\epsilon)$.\nIn both of these settings, our algorithms are simple, performing\ngradient-descent on the (regularized) $L_2^2$-loss. The correctness of our\nalgorithms relies on novel structural results that we establish, showing that\n(essentially all) stationary points of the underlying non-convex loss are\napproximately optimal.",
    "descriptor": "",
    "authors": [
      "Ilias Diakonikolas",
      "Vasilis Kontonis",
      "Christos Tzamos",
      "Nikos Zarifis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08918"
  },
  {
    "id": "arXiv:2206.08919",
    "title": "VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix",
    "abstract": "Existing vision-language pre-training (VLP) methods primarily rely on paired\nimage-text datasets, which are either annotated by enormous human labors, or\ncrawled from the internet followed by elaborate data cleaning techniques. To\nreduce the dependency on well-aligned image-text pairs, it is promising to\ndirectly leverage the large-scale text-only and image-only corpora. This paper\nproposes a data augmentation method, namely cross-modal CutMix (CMC), for\nimplicit cross-modal alignment learning in unpaired VLP. Specifically, CMC\ntransforms natural sentences from the textual view into a multi-modal view,\nwhere visually-grounded words in a sentence are randomly replaced by diverse\nimage patches with similar semantics. There are several appealing proprieties\nof the proposed CMC. First, it enhances the data diversity while keeping the\nsemantic meaning intact for tackling problems where the aligned data are\nscarce; Second, by attaching cross-modal noise on uni-modal data, it guides\nmodels to learn token-level interactions across modalities for better\ndenoising. Furthermore, we present a new unpaired VLP method, dubbed as\nVLMixer, that integrates CMC with contrastive learning to pull together the\nuni-modal and multi-modal views for better instance-level alignments among\ndifferent modalities. Extensive experiments on five downstream tasks show that\nVLMixer could surpass previous state-of-the-art unpaired VLP methods.",
    "descriptor": "",
    "authors": [
      "Teng Wang",
      "Wenhao Jiang",
      "Zhichao Lu",
      "Feng Zheng",
      "Ran Cheng",
      "Chengguo Yin",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08919"
  },
  {
    "id": "arXiv:2206.08920",
    "title": "VectorMapNet: End-to-end Vectorized HD Map Learning",
    "abstract": "Autonomous driving systems require a good understanding of surrounding\nenvironments, including moving obstacles and static High-Definition (HD)\nsemantic maps. Existing methods approach the semantic map problem by offline\nmanual annotations, which suffer from serious scalability issues. More recent\nlearning-based methods produce dense rasterized segmentation predictions which\ndo not include instance information of individual map elements and require\nheuristic post-processing that involves many hand-designed components, to\nobtain vectorized maps. To that end, we introduce an end-to-end vectorized HD\nmap learning pipeline, termed VectorMapNet. VectorMapNet takes onboard sensor\nobservations and predicts a sparse set of polylines primitives in the\nbird's-eye view to model the geometry of HD maps. Based on this pipeline, our\nmethod can explicitly model the spatial relation between map elements and\ngenerate vectorized maps that are friendly for downstream autonomous driving\ntasks without the need for post-processing. In our experiments, VectorMapNet\nachieves strong HD map learning performance on nuScenes dataset, surpassing\nprevious state-of-the-art methods by 14.2 mAP. Qualitatively, we also show that\nVectorMapNet is capable of generating comprehensive maps and capturing more\nfine-grained details of road geometry. To the best of our knowledge,\nVectorMapNet is the first work designed toward end-to-end vectorized HD map\nlearning problems.",
    "descriptor": "",
    "authors": [
      "Yicheng Liu",
      "Yue Wang",
      "Yilun Wang",
      "Hang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08920"
  },
  {
    "id": "arXiv:2206.08921",
    "title": "Efficiently Learning Single-Arm Fling Motions to Smooth Garments",
    "abstract": "Recent work has shown that 2-arm \"fling\" motions can be effective for garment\nsmoothing. We consider single-arm fling motions. Unlike 2-arm fling motions,\nwhich require little robot trajectory parameter tuning, single-arm fling\nmotions are sensitive to trajectory parameters. We consider a single 6-DOF\nrobot arm that learns fling trajectories to achieve high garment coverage.\nGiven a garment grasp point, the robot explores different parameterized fling\ntrajectories in physical experiments. To improve learning efficiency, we\npropose a coarse-to-fine learning method that first uses a multi-armed bandit\n(MAB) framework to efficiently find a candidate fling action, which it then\nrefines via a continuous optimization method. Further, we propose novel\ntraining and execution-time stopping criteria based on fling outcome\nuncertainty. Compared to baselines, we show that the proposed method\nsignificantly accelerates learning. Moreover, with prior experience on similar\ngarments collected through self-supervision, the MAB learning time for a new\ngarment is reduced by up to 87%. We evaluate on 6 garment types: towels,\nT-shirts, long-sleeve shirts, dresses, sweat pants, and jeans. Results suggest\nthat using prior experience, a robot requires under 30 minutes to learn a fling\naction for a novel garment that achieves 60-94% coverage.",
    "descriptor": "",
    "authors": [
      "Lawrence Yunliang Chen",
      "Huang Huang",
      "Ellen Novoseller",
      "Daniel Seita",
      "Jeffrey Ichnowski",
      "Michael Laskey",
      "Richard Cheng",
      "Thomas Kollar",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08921"
  },
  {
    "id": "arXiv:2206.08927",
    "title": "Cross-task Attention Mechanism for Dense Multi-task Learning",
    "abstract": "Multi-task learning has recently become a promising solution for a\ncomprehensive understanding of complex scenes. Not only being memory-efficient,\nmulti-task models with an appropriate design can favor exchange of\ncomplementary signals across tasks. In this work, we jointly address 2D\nsemantic segmentation, and two geometry-related tasks, namely dense depth,\nsurface normal estimation as well as edge estimation showing their benefit on\nindoor and outdoor datasets. We propose a novel multi-task learning\narchitecture that exploits pair-wise cross-task exchange through\ncorrelation-guided attention and self-attention to enhance the average\nrepresentation learning for all tasks. We conduct extensive experiments\nconsidering three multi-task setups, showing the benefit of our proposal in\ncomparison to competitive baselines in both synthetic and real benchmarks. We\nalso extend our method to the novel multi-task unsupervised domain adaptation\nsetting. Our code is available at https://github.com/cv-rits/DenseMTL.",
    "descriptor": "\nComments: 10 figures, 6 tables, 23 pages\n",
    "authors": [
      "Ivan Lopes",
      "Tuan-Hung Vu",
      "Raoul de Charette"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08927"
  },
  {
    "id": "arXiv:2206.08929",
    "title": "TAVA: Template-free Animatable Volumetric Actors",
    "abstract": "Coordinate-based volumetric representations have the potential to generate\nphoto-realistic virtual avatars from images. However, virtual avatars also need\nto be controllable even to a novel pose that may not have been observed.\nTraditional techniques, such as LBS, provide such a function; yet it usually\nrequires a hand-designed body template, 3D scan data, and limited appearance\nmodels. On the other hand, neural representation has been shown to be powerful\nin representing visual details, but are under explored on deforming dynamic\narticulated actors. In this paper, we propose TAVA, a method to create T\nemplate-free Animatable Volumetric Actors, based on neural representations. We\nrely solely on multi-view data and a tracked skeleton to create a volumetric\nmodel of an actor, which can be animated at the test time given novel pose.\nSince TAVA does not require a body template, it is applicable to humans as well\nas other creatures such as animals. Furthermore, TAVA is designed such that it\ncan recover accurate dense correspondences, making it amenable to\ncontent-creation and editing tasks. Through extensive experiments, we\ndemonstrate that the proposed method generalizes well to novel poses as well as\nunseen views and showcase basic editing capabilities.",
    "descriptor": "\nComments: Code: this https URL; Project Website: this https URL\n",
    "authors": [
      "Ruilong Li",
      "Julian Tanke",
      "Minh Vo",
      "Michael Zollhofer",
      "Jurgen Gall",
      "Angjoo Kanazawa",
      "Christoph Lassner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08929"
  },
  {
    "id": "arXiv:2206.08398",
    "title": "Learning Generic Lung Ultrasound Biomarkers for Decoupling Feature  Extraction from Downstream Tasks",
    "abstract": "Contemporary artificial neural networks (ANN) are trained end-to-end, jointly\nlearning both features and classifiers for the task of interest. Though\nenormously effective, this paradigm imposes significant costs in assembling\nannotated task-specific datasets and training large-scale networks. We propose\nto decouple feature learning from downstream lung ultrasound tasks by\nintroducing an auxiliary pre-task of visual biomarker classification. We\ndemonstrate that one can learn an informative, concise, and interpretable\nfeature space from ultrasound videos by training models for predicting\nbiomarker labels. Notably, biomarker feature extractors can be trained from\ndata annotated with weak video-scale supervision. These features can be used by\na variety of downstream Expert models targeted for diverse clinical tasks\n(Diagnosis, lung severity, S/F ratio). Crucially, task-specific expert models\nare comparable in accuracy to end-to-end models directly trained for such\ntarget tasks, while being significantly lower cost to train.",
    "descriptor": "",
    "authors": [
      "Gautam Rajendrakumar Gare",
      "Tom Fox",
      "Pete Lowery",
      "Kevin Zamora",
      "Hai V. Tran",
      "Laura Hutchins",
      "David Montgomery",
      "Amita Krishnan",
      "Deva Kannan Ramanan",
      "Ricardo Luis Rodriguez",
      "Bennett P deBoisblanc",
      "John Michael Galeotti"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08398"
  },
  {
    "id": "arXiv:2206.08401",
    "title": "Are decentralized finance really decentralized? A social network  analysis of the Aave protocol on the Ethereum blockchain",
    "abstract": "Decentralized finance (DeFi) has the potential to disrupt centralized finance\nby validating peer-to-peer transactions through tamper-proof smart contracts\nand thus significantly lower the transaction cost charged by financial\nintermediaries. However, the actual realization of peer-to-peer transactions\nand the levels and effect of decentralization are largely unknown. Our research\npioneers a blockchain network study that applies social network analysis to\nmeasure the level, dynamics, and impacts of decentralization in DeFi token\ntransactions on Ethereum blockchain. First, we find a significant\ncore-periphery structure in the AAVE token transaction network where the cores\ninclude the two largest centralized crypto exchanges. Second, we provide\nevidence that multiple network features consistently characterize\ndecentralization dynamics. Finally, we document that a more decentralized\nnetwork significantly predicts a higher return and lower volatilities of the\nDeFi tokens. We point out that our approach is seminal for inspiring future\nextensions related to the facets of application scenarios, research questions,\nand methodologies on the mechanics of blockchain decentralization.",
    "descriptor": "\nComments: Accepted at 29th Annual Global Finance Conference featuring Professor Robert Engle, The 2003 Nobel Laureate in Economic Sciences\n",
    "authors": [
      "Ziqiao Ao",
      "Gergely Horvath",
      "Luyao Zhang"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Cryptography and Security (cs.CR)",
      "Statistical Finance (q-fin.ST)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.08401"
  },
  {
    "id": "arXiv:2206.08439",
    "title": "OpenSRH: optimizing brain tumor surgery using intraoperative stimulated  Raman histology",
    "abstract": "Accurate intraoperative diagnosis is essential for providing safe and\neffective care during brain tumor surgery. Our standard-of-care diagnostic\nmethods are time, resource, and labor intensive, which restricts access to\noptimal surgical treatments. To address these limitations, we propose an\nalternative workflow that combines stimulated Raman histology (SRH), a rapid\noptical imaging method, with deep learning-based automated interpretation of\nSRH images for intraoperative brain tumor diagnosis and real-time surgical\ndecision support. Here, we present OpenSRH, the first public dataset of\nclinical SRH images from 300+ brain tumors patients and 1300+ unique whole\nslide optical images. OpenSRH contains data from the most common brain tumors\ndiagnoses, full pathologic annotations, whole slide tumor segmentations, raw\nand processed optical imaging data for end-to-end model development and\nvalidation. We provide a framework for patch-based whole slide SRH\nclassification and inference using weak (i.e. patient-level) diagnostic labels.\nFinally, we benchmark two computer vision tasks: multiclass histologic brain\ntumor classification and patch-based contrastive representation learning. We\nhope OpenSRH will facilitate the clinical translation of rapid optical imaging\nand real-time ML-based surgical decision support in order to improve the\naccess, safety, and efficacy of cancer surgery in the era of precision\nmedicine. Dataset access, code, and benchmarks are available at\nopensrh.mlins.org.",
    "descriptor": "",
    "authors": [
      "Cheng Jiang",
      "Asadur Chowdury",
      "Xinhai Hou",
      "Akhil Kondepudi",
      "Christian W. Freudiger",
      "Kyle Conway",
      "Sandra Camelo-Piragua",
      "Daniel A. Orringer",
      "Honglak Lee",
      "Todd C. Hollon"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08439"
  },
  {
    "id": "arXiv:2206.08457",
    "title": "Wireless Picosecond Time Synchronization for Distributed Antenna Arrays",
    "abstract": "Distributed antenna arrays have been proposed for many applications ranging\nfrom space-based observatories to automated vehicles. Achieving good\nperformance in distributed antenna systems requires stringent synchronization\nat the wavelength and information level to ensure that the transmitted signals\narrive coherently at the target, or that scattered and received signals can be\nappropriately processed via distributed algorithms. In this paper we address\nthe challenge of high precision time synchronization to align the operations of\nelements in a distributed antenna array and to overcome time-varying bias\nbetween platforms due to oscillator drift. We use a spectrally sparse two-tone\nwaveform, which obtains approximately optimal time estimation accuracy, in a\ntwo-way time transfer process. We also describe a technique for determining the\ntrue time delay using the ambiguous two-tone matched filter output, and we\ncompare the time synchronization precision of the two-tone waveform with the\nmore common linear frequency modulation (LFM) waveform. We experimentally\ndemonstrate wireless time synchronization using a single pulse 40$\\,$MHz\ntwo-tone waveform over a 90$\\,$cm 5.8$\\,$GHz wireless link in a laboratory\nsetting, obtaining a timing precision of 2.26$\\,$ps.",
    "descriptor": "",
    "authors": [
      "Jason M. Merlo",
      "Serge R. Mghabghab",
      "Jeffrey A. Nanzer"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08457"
  },
  {
    "id": "arXiv:2206.08465",
    "title": "Variational Estimators of the Degree-corrected Latent Block Model for  Bipartite Networks",
    "abstract": "Biclustering on bipartite graphs is an unsupervised learning task that\nsimultaneously clusters the two types of objects in the graph, for example,\nusers and movies in a movie review dataset. The latent block model (LBM) has\nbeen proposed as a model-based tool for biclustering. Biclustering results by\nthe LBM are, however, usually dominated by the row and column sums of the data\nmatrix, i.e., degrees. We propose a degree-corrected latent block model\n(DC-LBM) to accommodate degree heterogeneity in row and column clusters, which\ngreatly outperforms the classical LBM in the MovieLens dataset and simulated\ndata. We develop an efficient variational expectation-maximization algorithm by\nobserving that the row and column degrees maximize the objective function in\nthe M step given any probability assignment on the cluster labels. We prove the\nlabel consistency of the variational estimator under the DC-LBM, which allows\nthe expected graph density goes to zero as long as the average expected degrees\nof rows and columns go to infinity.",
    "descriptor": "",
    "authors": [
      "Yunpeng Zhao",
      "Ning Hao",
      "Ji Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08465"
  },
  {
    "id": "arXiv:2206.08481",
    "title": "Orientation-guided Graph Convolutional Network for Bone Surface  Segmentation",
    "abstract": "Due to imaging artifacts and low signal-to-noise ratio in ultrasound images,\nautomatic bone surface segmentation networks often produce fragmented\npredictions that can hinder the success of ultrasound-guided computer-assisted\nsurgical procedures. Existing pixel-wise predictions often fail to capture the\naccurate topology of bone tissues due to a lack of supervision to enforce\nconnectivity. In this work, we propose an orientation-guided graph\nconvolutional network to improve connectivity while segmenting the bone\nsurface. We also propose an additional supervision on the orientation of the\nbone surface to further impose connectivity. We validated our approach on 1042\nvivo US scans of femur, knee, spine, and distal radius. Our approach improves\nover the state-of-the-art methods by 5.01% in connectivity metric.",
    "descriptor": "\nComments: Accepted at MICCAI 2022\n",
    "authors": [
      "Aimon Rahman",
      "Wele Gedara Chaminda Bandara",
      "Jeya Maria Jose Valanarasu",
      "Ilker Hacihaliloglu",
      "Vishal M Patel"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08481"
  },
  {
    "id": "arXiv:2206.08525",
    "title": "Simultaneous Speech Extraction for Multiple Target Speakers under the  Meeting Scenarios(V1)",
    "abstract": "Recently, the target speech separation or extraction techniques under the\nmeeting scenario have become a hot research trend. We propose a speaker\ndiarization aware multiple target speech separation system (SD-MTSS) to\nsimultaneously extract the voice of each speaker from the mixed speech, rather\nthan requiring a succession of independent processes as presented in previous\nsolutions. SD-MTSS consists of a speaker diarization (SD) module and a multiple\ntarget speech separation (MTSS) module. The former one infers the target\nspeaker voice activity detection (TSVAD) states of the mixture, as well as gets\ndifferent speakers' single-talker audio segments as the reference speech. The\nlatter one employs both the mixed audio and reference speech as inputs, and\nthen it generates an estimated mask. By exploiting the TSVAD decision and the\nestimated mask, our SD-MTSS model can extract the speech of each speaker\nconcurrently in a conversion recording without additional enrollment audio in\nadvance.Experimental results show that our MTSS model outperforms our baselines\nwith a large margin, achieving 1.38dB SDR, 1.34dB SI-SNR, and 0.13 PESQ\nimprovements over the state-of-the-art SpEx+ baseline on the WSJ0-2mix-extr\ndataset, respectively. The SD-MTSS system makes a significant improvement than\nthe baseline on the Alimeeting dataset as well.",
    "descriptor": "\nComments: 4pages, 3 figures\n",
    "authors": [
      "Bang Zeng",
      "Weiqing Wang",
      "Yuanyuan Bao",
      "Ming Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.08525"
  },
  {
    "id": "arXiv:2206.08531",
    "title": "Reframed GES with a Neural Conditional Dependence Measure",
    "abstract": "In a nonparametric setting, the causal structure is often identifiable only\nup to Markov equivalence, and for the purpose of causal inference, it is useful\nto learn a graphical representation of the Markov equivalence class (MEC). In\nthis paper, we revisit the Greedy Equivalence Search (GES) algorithm, which is\nwidely cited as a score-based algorithm for learning the MEC of the underlying\ncausal structure. We observe that in order to make the GES algorithm consistent\nin a nonparametric setting, it is not necessary to design a scoring metric that\nevaluates graphs. Instead, it suffices to plug in a consistent estimator of a\nmeasure of conditional dependence to guide the search. We therefore present a\nreframing of the GES algorithm, which is more flexible than the standard\nscore-based version and readily lends itself to the nonparametric setting with\na general measure of conditional dependence. In addition, we propose a neural\nconditional dependence (NCD) measure, which utilizes the expressive power of\ndeep neural networks to characterize conditional independence in a\nnonparametric manner. We establish the optimality of the reframed GES algorithm\nunder standard assumptions and the consistency of using our NCD estimator to\ndecide conditional independence. Together these results justify the proposed\napproach. Experimental results demonstrate the effectiveness of our method in\ncausal discovery, as well as the advantages of using our NCD measure over\nkernel-based measures.",
    "descriptor": "\nComments: Accepted to UAI 2022\n",
    "authors": [
      "Xinwei Shen",
      "Shengyu Zhu",
      "Jiji Zhang",
      "Shoubo Hu",
      "Zhitang Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08531"
  },
  {
    "id": "arXiv:2206.08543",
    "title": "Multi-Classification of Brain Tumor Images Using Transfer Learning Based  Deep Neural Network",
    "abstract": "In recent advancement towards computer based diagnostics system, the\nclassification of brain tumor images is a challenging task. This paper mainly\nfocuses on elevating the classification accuracy of brain tumor images with\ntransfer learning based deep neural network. The classification approach is\nstarted with the image augmentation operation including rotation, zoom,\nhori-zontal flip, width shift, height shift, and shear to increase the\ndiversity in image datasets. Then the general features of the input brain tumor\nimages are extracted based on a pre-trained transfer learning method comprised\nof Inception-v3. Fi-nally, the deep neural network with 4 customized layers is\nemployed for classi-fying the brain tumors in most frequent brain tumor types\nas meningioma, glioma, and pituitary. The proposed model acquires an effective\nperformance with an overall accuracy of 96.25% which is much improved than some\nexisting multi-classification methods. Whereas, the fine-tuning of\nhyper-parameters and inclusion of customized DNN with the Inception-v3 model\nresults in an im-provement of the classification accuracy.",
    "descriptor": "\nComments: 7 pages, 4 figures, 2 tables, International Virtual Conference on ARTIFICIAL INTELLIGENCE FOR SMART COMMUNITY, Malaysia\n",
    "authors": [
      "Pramit Dutta",
      "Khaleda Akhter Sathi",
      "Md. Saiful Islam"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08543"
  },
  {
    "id": "arXiv:2206.08545",
    "title": "NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling  Rates",
    "abstract": "Conventionally, audio super-resolution models fixed the initial and the\ntarget sampling rates, which necessitate the model to be trained for each pair\nof sampling rates. We introduce NU-Wave 2, a diffusion model for neural audio\nupsampling that enables the generation of 48 kHz audio signals from inputs of\nvarious sampling rates with a single model. Based on the architecture of\nNU-Wave, NU-Wave 2 uses short-time Fourier convolution (STFC) to generate\nharmonics to resolve the main failure modes of NU-Wave, and incorporates\nbandwidth spectral feature transform (BSFT) to condition the bandwidths of\ninputs in the frequency domain. We experimentally demonstrate that NU-Wave 2\nproduces high-resolution audio regardless of the sampling rate of input while\nrequiring fewer parameters than other models. The official code and the audio\nsamples are available at https://mindslab-ai.github.io/nuwave2.",
    "descriptor": "\nComments: Accepted to Interspeech 2022\n",
    "authors": [
      "Seungu Han",
      "Junhyeok Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08545"
  },
  {
    "id": "arXiv:2206.08557",
    "title": "COVID-19 Detection using Transfer Learning with Convolutional Neural  Network",
    "abstract": "The Novel Coronavirus disease 2019 (COVID-19) is a fatal infectious disease,\nfirst recognized in December 2019 in Wuhan, Hubei, China, and has gone on an\nepidemic situation. Under these circumstances, it became more important to\ndetect COVID-19 in infected people. Nowadays, the testing kits are gradually\nlessening in number compared to the number of infected population. Under recent\nprevailing conditions, the diagnosis of lung disease by analyzing chest CT\n(Computed Tomography) images has become an important tool for both diagnosis\nand prophecy of COVID-19 patients. In this study, a Transfer learning strategy\n(CNN) for detecting COVID-19 infection from CT images has been proposed. In the\nproposed model, a multilayer Convolutional neural network (CNN) with Transfer\nlearning model Inception V3 has been designed. Similar to CNN, it uses\nconvolution and pooling to extract features, but this transfer learning model\ncontains weights of dataset Imagenet. Thus it can detect features very\neffectively which gives it an upper hand for achieving better accuracy.",
    "descriptor": "\nComments: 4 pages, 4 figures, 2nd International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST), DHAKA, Bangladesh\n",
    "authors": [
      "Pramit Dutta",
      "Tanny Roy",
      "Nafisa Anjum"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08557"
  },
  {
    "id": "arXiv:2206.08573",
    "title": "Optimal Extragradient-Based Bilinearly-Coupled Saddle-Point Optimization",
    "abstract": "We consider the smooth convex-concave bilinearly-coupled saddle-point\nproblem, $\\min_{\\mathbf{x}}\\max_{\\mathbf{y}}~F(\\mathbf{x}) +\nH(\\mathbf{x},\\mathbf{y}) - G(\\mathbf{y})$, where one has access to stochastic\nfirst-order oracles for $F$, $G$ as well as the bilinear coupling function $H$.\nBuilding upon standard stochastic extragradient analysis for variational\ninequalities, we present a stochastic \\emph{accelerated gradient-extragradient\n(AG-EG)} descent-ascent algorithm that combines extragradient and Nesterov's\nacceleration in general stochastic settings. This algorithm leverages scheduled\nrestarting to admit a fine-grained nonasymptotic convergence rate that matches\nknown lower bounds by both \\citet{ibrahim2020linear} and \\citet{zhang2021lower}\nin their corresponding settings, plus an additional statistical error term for\nbounded stochastic noise that is optimal up to a constant prefactor. This is\nthe first result that achieves such a relatively mature characterization of\noptimality in saddle-point optimization.",
    "descriptor": "",
    "authors": [
      "Simon S. Du",
      "Gauthier Gidel",
      "Michael I. Jordan",
      "Chris Junchi Li"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08573"
  },
  {
    "id": "arXiv:2206.08588",
    "title": "Efficient Deterministic Preparation of Quantum States Using Decision  Diagrams",
    "abstract": "Loading classical data into quantum registers is one of the most important\nprimitives of quantum computing. While the complexity of preparing a generic\nquantum state is exponential in the number of qubits, in many practical tasks\nthe state to prepare has a certain structure that allows for faster\npreparation. In this paper, we consider quantum states that can be efficiently\nrepresented by (reduced) decision diagrams, a versatile data structure for the\nrepresentation and analysis of Boolean functions. We design an algorithm that\nutilises the structure of decision diagrams to prepare their associated quantum\nstates. Our algorithm has a circuit complexity that is linear in the number of\npaths in the decision diagram. Numerical experiments show that our algorithm\nreduces the circuit complexity by up to 31.85\\% compared to the\nstate-of-the-art algorithm, when preparing generic $n$-qubit states with\ndifferent degrees of non-zero amplitudes. Additionally, for states with sparse\ndecision diagrams, including the initial state of the quantum Byzantine\nagreement protocol, our algorithm reduces the number of CNOTs by 86.61\\% $\\sim$\n99.9\\%.",
    "descriptor": "",
    "authors": [
      "Fereshte Mozafari",
      "Giovanni De Micheli",
      "Yuxiang Yang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08588"
  },
  {
    "id": "arXiv:2206.08600",
    "title": "On Integrating Prior Knowledge into Gaussian Processes for Prognostic  Health Monitoring",
    "abstract": "Gaussian process regression is a powerful method for predicting states based\non given data. It has been successfully applied for probabilistic predictions\nof structural systems to quantify, for example, the crack growth in mechanical\nstructures. Typically, predefined mean and covariance functions are employed to\nconstruct the Gaussian process model. Then, the model is updated using current\ndata during operation while prior information based on previous data is\nignored. However, predefined mean and covariance functions without prior\ninformation reduce the potential of Gaussian processes. This paper proposes a\nmethod to improve the predictive capabilities of Gaussian processes. We\nintegrate prior knowledge by deriving the mean and covariance functions from\nprevious data. More specifically, we first approximate previous data by a\nweighted sum of basis functions and then derive the mean and covariance\nfunctions directly from the estimated weight coefficients. Basis functions may\nbe either estimated or derived from problem-specific governing equations to\nincorporate physical information. The applicability and effectiveness of this\napproach are demonstrated for fatigue crack growth, laser degradation, and\nmilling machine wear data. We show that well-chosen mean and covariance\nfunctions, like those based on previous data, significantly increase look-ahead\ntime and accuracy. Using physical basis functions further improves accuracy. In\naddition, computation effort for training is significantly reduced.",
    "descriptor": "",
    "authors": [
      "Simon Pfingstl",
      "Markus Zimmermann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08600"
  },
  {
    "id": "arXiv:2206.08612",
    "title": "OADAT: Experimental and Synthetic Clinical Optoacoustic Data for  Standardized Image Processing",
    "abstract": "Optoacoustic (OA) imaging is based on excitation of biological tissues with\nnanosecond-duration laser pulses followed by subsequent detection of ultrasound\nwaves generated via light-absorption-mediated thermoelastic expansion. OA\nimaging features a powerful combination between rich optical contrast and high\nresolution in deep tissues. This enabled the exploration of a number of\nattractive new applications both in clinical and laboratory settings. However,\nno standardized datasets generated with different types of experimental set-up\nand associated processing methods are available to facilitate advances in\nbroader applications of OA in clinical settings. This complicates an objective\ncomparison between new and established data processing methods, often leading\nto qualitative results and arbitrary interpretations of the data. In this\npaper, we provide both experimental and synthetic OA raw signals and\nreconstructed image domain datasets rendered with different experimental\nparameters and tomographic acquisition geometries. We further provide trained\nneural networks to tackle three important challenges related to OA image\nprocessing, namely accurate reconstruction under limited view tomographic\nconditions, removal of spatial undersampling artifacts and anatomical\nsegmentation for improved image reconstruction. Specifically, we define 18\nexperiments corresponding to the aforementioned challenges as benchmarks to be\nused as a reference for the development of more advanced processing methods.",
    "descriptor": "\nComments: 25 pages, 18 figures, 8 tables\n",
    "authors": [
      "Berkan Lafci",
      "Firat Ozdemir",
      "Xos\u00e9 Lu\u00eds De\u00e1n-Ben",
      "Daniel Razansky",
      "Fernando Perez-Cruz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08612"
  },
  {
    "id": "arXiv:2206.08627",
    "title": "RECAPP: Crafting a More Efficient Catalyst for Convex Optimization",
    "abstract": "The accelerated proximal point algorithm (APPA), also known as \"Catalyst\", is\na well-established reduction from convex optimization to approximate proximal\npoint computation (i.e., regularized minimization). This reduction is\nconceptually elegant and yields strong convergence rate guarantees. However,\nthese rates feature an extraneous logarithmic term arising from the need to\ncompute each proximal point to high accuracy. In this work, we propose a novel\nRelaxed Error Criterion for Accelerated Proximal Point (RECAPP) that eliminates\nthe need for high accuracy subproblem solutions. We apply RECAPP to two\ncanonical problems: finite-sum and max-structured minimization. For finite-sum\nproblems, we match the best known complexity, previously obtained by\ncarefully-designed problem-specific algorithms. For minimizing $\\max_y f(x,y)$\nwhere $f$ is convex in $x$ and strongly-concave in $y$, we improve on the best\nknown (Catalyst-based) bound by a logarithmic factor.",
    "descriptor": "\nComments: Accepted at ICML'22\n",
    "authors": [
      "Yair Carmon",
      "Arun Jambulapati",
      "Yujia Jin",
      "Aaron Sidford"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08627"
  },
  {
    "id": "arXiv:2206.08629",
    "title": "A two-stage approach for a mixed-integer economic dispatch game in  integrated electrical and gas distribution systems",
    "abstract": "In this paper, we formulate for the first time the economic dispatch problem\nof an integrated electrical and gas distribution system as a game equilibrium\nproblem. Specifically, by assuming that electricity and gas prices depend\nlinearly on the total consumption and by approximating the non-linear gas-flow\nequations with a piece-wise affine model, we obtain a potential mixed-integer\ngame. To compute an approximate generalized Nash equilibrium, we propose an\niterative two-stage method that exploits a problem convexification and the gas\nflow model. We quantify the quality of the computed solution and perform a\nnumerical study to evaluate the performance of our method.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Wicak Ananduta",
      "Sergio Grammatico"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08629"
  },
  {
    "id": "arXiv:2206.08648",
    "title": "Orthonormal Expansions for Translation-Invariant Kernels",
    "abstract": "We present a general Fourier analytic technique for constructing orthonormal\nbasis expansions of translation-invariant kernels from orthonormal bases of\n$\\mathscr{L}_2(\\mathbb{R})$. This allows us to derive explicit expansions on\nthe real line for (i) Mat\\'ern kernels of all half-integer orders in terms of\nassociated Laguerre functions, (ii) the Cauchy kernel in terms of rational\nfunctions, and (iii) the Gaussian kernel in terms of Hermite functions.",
    "descriptor": "\nComments: 23 pages, 8 figures\n",
    "authors": [
      "Filip Tronarp",
      "Toni Karvonen"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08648"
  },
  {
    "id": "arXiv:2206.08666",
    "title": "The Sensorium competition on predicting large-scale mouse primary visual  cortex activity",
    "abstract": "The neural underpinning of the biological visual system is challenging to\nstudy experimentally, in particular as the neuronal activity becomes\nincreasingly nonlinear with respect to visual input. Artificial neural networks\n(ANNs) can serve a variety of goals for improving our understanding of this\ncomplex system, not only serving as predictive digital twins of sensory cortex\nfor novel hypothesis generation in silico, but also incorporating bio-inspired\narchitectural motifs to progressively bridge the gap between biological and\nmachine vision. The mouse has recently emerged as a popular model system to\nstudy visual information processing, but no standardized large-scale benchmark\nto identify state-of-the-art models of the mouse visual system has been\nestablished. To fill this gap, we propose the Sensorium benchmark competition.\nWe collected a large-scale dataset from mouse primary visual cortex containing\nthe responses of more than 28,000 neurons across seven mice stimulated with\nthousands of natural images, together with simultaneous behavioral measurements\nthat include running speed, pupil dilation, and eye movements. The benchmark\nchallenge will rank models based on predictive performance for neuronal\nresponses on a held-out test set, and includes two tracks for model input\nlimited to either stimulus only (Sensorium) or stimulus plus behavior\n(Sensorium+). We provide a starting kit to lower the barrier for entry,\nincluding tutorials, pre-trained baseline models, and APIs with one line\ncommands for data loading and submission. We would like to see this as a\nstarting point for regular challenges and data releases, and as a standard tool\nfor measuring progress in large-scale neural system identification models of\nthe mouse visual system and beyond.",
    "descriptor": "\nComments: NeurIPS 2022 Competition Track\n",
    "authors": [
      "Konstantin F. Willeke",
      "Paul G. Fahey",
      "Mohammad Bashiri",
      "Laura Pede",
      "Max F. Burg",
      "Christoph Blessing",
      "Santiago A. Cadena",
      "Zhiwei Ding",
      "Konstantin-Klemens Lurz",
      "Kayla Ponder",
      "Taliah Muhammad",
      "Saumil S. Patel",
      "Alexander S. Ecker",
      "Andreas S. Tolias",
      "Fabian H. Sinz"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08666"
  },
  {
    "id": "arXiv:2206.08671",
    "title": "FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and  Federated Image Classification",
    "abstract": "Modern deep learning systems are increasingly deployed in situations such as\npersonalization and federated learning where it is necessary to support i)\nlearning on small amounts of data, and ii) communication efficient distributed\ntraining protocols. In this work we develop FiLM Transfer (FiT) which fulfills\nthese requirements in the image classification setting. FiT uses an\nautomatically configured Naive Bayes classifier on top of a fixed backbone that\nhas been pretrained on large image datasets. Parameter efficient FiLM layers\nare used to modulate the backbone, shaping the representation for the\ndownstream task. The network is trained via an episodic fine-tuning protocol.\nThe approach is parameter efficient which is key for enabling few-shot\nlearning, inexpensive model updates for personalization, and communication\nefficient federated learning. We experiment with FiT on a wide range of\ndownstream datasets and show that it achieves better classification accuracy\nthan the state-of-the-art Big Transfer (BiT) algorithm at low-shot and on the\nchallenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters.\nFinally, we demonstrate the parameter efficiency of FiT in distributed low-shot\napplications including model personalization and federated learning where model\nupdate size is an important performance metric.",
    "descriptor": "",
    "authors": [
      "Aliaksandra Shysheya",
      "John Bronskill",
      "Massimiliano Patacchiola",
      "Sebastian Nowozin",
      "Richard E Turner"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08671"
  },
  {
    "id": "arXiv:2206.08734",
    "title": "A remark on Kashin's discrepancy argument and partial coloring in the  Koml\u00f3s conjecture",
    "abstract": "In this expository note, we discuss an early partial coloring result of B.\nKashin [C. R. Acad. Bulgare Sci., 1985]. Although this result only implies\nSpencer's six standard deviations [Trans. Amer. Math. Soc., 1985] up to a\n$\\log\\log n$ factor, Kashin's argument gives a simple proof of the existence of\na constant discrepancy partial coloring in the setup of Koml\\'{o}s conjecture.",
    "descriptor": "\nComments: 3 pages, an expository note\n",
    "authors": [
      "Afonso S. Bandeira",
      "Antoine Maillard",
      "Nikita Zhivotovskiy"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2206.08734"
  },
  {
    "id": "arXiv:2206.08736",
    "title": "Generalised Policy Improvement with Geometric Policy Composition",
    "abstract": "We introduce a method for policy improvement that interpolates between the\ngreedy approach of value-based reinforcement learning (RL) and the full\nplanning approach typical of model-based RL. The new method builds on the\nconcept of a geometric horizon model (GHM, also known as a gamma-model), which\nmodels the discounted state-visitation distribution of a given policy. We show\nthat we can evaluate any non-Markov policy that switches between a set of base\nMarkov policies with fixed probability by a careful composition of the base\npolicy GHMs, without any additional learning. We can then apply generalised\npolicy improvement (GPI) to collections of such non-Markov policies to obtain a\nnew Markov policy that will in general outperform its precursors. We provide a\nthorough theoretical analysis of this approach, develop applications to\ntransfer and standard RL, and empirically demonstrate its effectiveness over\nstandard GPI on a challenging deep RL continuous control task. We also provide\nan analysis of GHM training methods, proving a novel convergence result\nregarding previously proposed methods and showing how to train these models\nstably in deep RL settings.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Shantanu Thakoor",
      "Mark Rowland",
      "Diana Borsa",
      "Will Dabney",
      "R\u00e9mi Munos",
      "Andr\u00e9 Barreto"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08736"
  },
  {
    "id": "arXiv:2206.08753",
    "title": "Information Geometry of Risks and Returns",
    "abstract": "We often think of hedging and investments as having different, even competing\ngoals. In reality optimal hedging and optimal investments are intimately\nconnected. One person's optimal investment is another's optimal hedge. This\nfollows from a geometric structure formed by probabilistic representations of\nmarket views and risk scenarios. Understanding this geometric structure is\nfundamental to risk recycling (and to product design in general).",
    "descriptor": "\nComments: 21 pages, 2 figures\n",
    "authors": [
      "Andrei N. Soklakov"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Information Theory (cs.IT)",
      "General Finance (q-fin.GN)"
    ],
    "url": "https://arxiv.org/abs/2206.08753"
  },
  {
    "id": "arXiv:2206.08756",
    "title": "Tensor-on-Tensor Regression: Riemannian Optimization,  Over-parameterization, Statistical-computational Gap, and Their Interplay",
    "abstract": "We study the tensor-on-tensor regression, where the goal is to connect tensor\nresponses to tensor covariates with a low Tucker rank parameter tensor/matrix\nwithout the prior knowledge of its intrinsic rank. We propose the Riemannian\ngradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with\nthe challenge of unknown rank by studying the effect of rank\nover-parameterization. We provide the first convergence guarantee for the\ngeneral tensor-on-tensor regression by showing that RGD and RGN respectively\nconverge linearly and quadratically to a statistically optimal estimate in both\nrank correctly-parameterized and over-parameterized settings. Our theory\nreveals an intriguing phenomenon: Riemannian optimization methods naturally\nadapt to over-parameterization without modifications to their implementation.\nWe also give the first rigorous evidence for the statistical-computational gap\nin scalar-on-tensor regression under the low-degree polynomials framework. Our\ntheory demonstrates a ``blessing of statistical-computational gap\" phenomenon:\nin a wide range of scenarios in tensor-on-tensor regression for tensors of\norder three or higher, the computationally required sample size matches what is\nneeded by moderate rank over-parameterization when considering computationally\nfeasible estimators, while there are no such benefits in the matrix settings.\nThis shows moderate rank over-parameterization is essentially ``cost-free\" in\nterms of sample size in tensor-on-tensor regression of order three or higher.\nFinally, we conduct simulation studies to show the advantages of our proposed\nmethods and to corroborate our theoretical findings.",
    "descriptor": "",
    "authors": [
      "Yuetian Luo",
      "Anru R. Zhang"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08756"
  },
  {
    "id": "arXiv:2206.08780",
    "title": "Spherical Sliced-Wasserstein",
    "abstract": "Many variants of the Wasserstein distance have been introduced to reduce its\noriginal computational burden. In particular the Sliced-Wasserstein distance\n(SW), which leverages one-dimensional projections for which a closed-form\nsolution of the Wasserstein distance is available, has received a lot of\ninterest. Yet, it is restricted to data living in Euclidean spaces, while the\nWasserstein distance has been studied and used recently on manifolds. We focus\nmore specifically on the sphere, for which we define a novel SW discrepancy,\nwhich we call spherical Sliced-Wasserstein, making a first step towards\ndefining SW discrepancies on manifolds. Our construction is notably based on\nclosed-form solutions of the Wasserstein distance on the circle, together with\na new spherical Radon transform. Along with efficient algorithms and the\ncorresponding implementations, we illustrate its properties in several machine\nlearning use cases where spherical representations of data are at stake:\ndensity estimation on the sphere, variational inference or hyperspherical\nauto-encoders.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Bonet",
      "Paul Berg",
      "Nicolas Courty",
      "Fran\u00e7ois Septier",
      "Lucas Drumetz",
      "Minh-Tan Pham"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08780"
  },
  {
    "id": "arXiv:2206.08787",
    "title": "Leveraging Uncertainty in Deep Learning for Pancreatic Adenocarcinoma  Grading",
    "abstract": "Pancreatic cancers have one of the worst prognoses compared to other cancers,\nas they are diagnosed when cancer has progressed towards its latter stages. The\ncurrent manual histological grading for diagnosing pancreatic adenocarcinomas\nis time-consuming and often results in misdiagnosis. In digital pathology,\nAI-based cancer grading must be extremely accurate in prediction and\nuncertainty quantification to improve reliability and explainability and are\nessential for gaining clinicians trust in the technology. We present Bayesian\nConvolutional Neural Networks for automated pancreatic cancer grading from MGG\nand HE stained images to estimate uncertainty in model prediction. We show that\nthe estimated uncertainty correlates with prediction error. Specifically, it is\nuseful in setting the acceptance threshold using a metric that weighs\nclassification accuracy-reject trade-off and misclassification cost controlled\nby hyperparameters and can be employed in clinical settings.",
    "descriptor": "\nComments: 26th UK Conference on Medical Image Understanding and Analysis; 27 - 29 July 2022; University of Cambridge, UK. arXiv admin note: text overlap with arXiv:2003.10769\n",
    "authors": [
      "Biraja Ghoshal",
      "Bhargab Ghoshal",
      "Allan Tucker"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08787"
  },
  {
    "id": "arXiv:2206.08797",
    "title": "Weighted Sum Secrecy Rate Maximization for RIS-Assisted Full Duplex  systems",
    "abstract": "This letter considers the secure communication in a reconfigurable\nintelligent surface (RIS) aided full duplex (FD) system. A FD base station (BS)\nserves an uplink (UL) user and a downlink (DL) user simultaneously over the\nsame timefrequency dimension assisted by a RIS in the presence of an\neavesdropper. In addition, the BS transmits artificial noise (AN) to interfere\nthe eavesdropper's channel. We aim to maximize the weighted sum secrecy rate of\nUL and DL users by jointly optimizing the transmit beamforming, receive\nbeamforming and AN covariance matrix at the BS, and passive beamforming at the\nRIS. To handle the non-convex problem, we decompose it into tractable\nsubproblems and propose an efficient algorithm based on alternating\noptimization framework. Specifically, the receive beamforming is derived as a\nclosed-form solution while other variables are obtained by using semidefinite\nrelaxation (SDR) method and successive convex approximation (SCA) algorithm.\nSimulation results demonstrate the superior performance of our proposed scheme\ncompared to other baseline schemes.",
    "descriptor": "\nComments: This is our preliminary research paper. More detailed papers will be updated as soon as possible\n",
    "authors": [
      "Pengxin Guan",
      "Yiru Wang",
      "Yuping Zhao"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.08797"
  },
  {
    "id": "arXiv:2206.08810",
    "title": "Interior point methods are not worse than Simplex",
    "abstract": "Whereas interior point methods provide polynomial-time linear programming\nalgorithms, the running time bounds depend on bit-complexity or condition\nmeasures that can be unbounded in the problem dimension. This is in contrast\nwith the simplex method that always admits an exponential bound. We introduce a\nnew polynomial-time path-following interior point method where the number of\niterations also admits a combinatorial upper bound $O(2^{n} n^{1.5}\\log n)$ for\nan $n$-variable linear program in standard form. This complements previous work\nby Allamigeon, Benchimol, Gaubert, and Joswig (SIAGA 2018) that exhibited a\nfamily of instances where any path-following method must take exponentially\nmany iterations.\nThe number of iterations of our algorithm is at most $O(n^{1.5}\\log n)$ times\nthe number of segments of any piecewise linear curve in the wide neighborhood\nof the central path. In particular, it matches the number of iterations of any\npath following interior point method up to this polynomial factor. The overall\nexponential upper bound derives from studying the `max central path', a\npiecewise-linear curve with the number of pieces bounded by the total length of\n$2n$ shadow vertex simplex paths.\nOur algorithm falls into the family of layered least squares interior point\nmethods introduced by Vavasis and Ye (Math. Prog. 1996). In contrast to\nprevious layered least squares methods that partition the kernel of the\nconstraint matrix into coordinate subspaces, our method creates layers based on\na general subspace providing more flexibility. Our result also implies the same\nbound on the number of iterations of the trust region interior point method by\nLan, Monteiro, and Tsuchiya (SIOPT 2009).",
    "descriptor": "",
    "authors": [
      "Xavier Allamigeon",
      "Daniel Dadush",
      "Georg Loho",
      "Bento Natura",
      "L\u00e1szl\u00f3 A. V\u00e9gh"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08810"
  },
  {
    "id": "arXiv:2206.08818",
    "title": "Projected distances for multi-parameter persistence modules",
    "abstract": "We introduce the new notions of projected distances and projected barcodes\nfor multi-parameter persistence modules. Projected barcodes are defined as\nderived pushforward of persistence modules onto $\\mathbb{R}$. Projected\ndistances come in two flavors: the integral sheaf metrics (ISM) and the sliced\nconvolution distances (SCD). We conduct a systematic study of the stability of\nprojected barcodes and show that the fibered barcode is a particular instance\nof projected barcodes. We prove that the ISM and the SCD provide lower bounds\nfor the convolution distance. Furthermore, we show that the $\\gamma$-linear ISM\nand the $\\gamma$-linear SCD which are projected distances tailored for\n$\\gamma$-sheaves can be computed using TDA software dedicated to one-parameter\npersistence modules. Moreover, the time and memory complexity required to\ncompute these two metrics are advantageous, since our approach does not require\ncomputing nor storing an entire $n$-persistence module.",
    "descriptor": "\nComments: 52 pages\n",
    "authors": [
      "Nicolas Berkouk",
      "Francois Petit"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.08818"
  },
  {
    "id": "arXiv:2206.08847",
    "title": "Asymmetric transport computations in Dirac models of topological  insulators",
    "abstract": "This paper presents a fast algorithm for computing transport properties of\ntwo-dimensional Dirac operators with linear domain walls, which model the\nmacroscopic behavior of the robust and asymmetric transport observed at an\ninterface separating two two-dimensional topological insulators. Our method is\nbased on reformulating the partial differential equation as a corresponding\nvolume integral equation, which we solve via a spectral discretization scheme.\nWe demonstrate the accuracy of our method by confirming the quantization of\nan appropriate interface conductivity modeling transport asymmetry along the\ninterface, and moreover confirm that this quantity is immune to local\nperturbations. We also compute the far-field scattering matrix generated by\nsuch perturbations and verify that while asymmetric transport is topologically\nprotected the absence of back-scattering is not.",
    "descriptor": "",
    "authors": [
      "Guillaume Bal",
      "Jeremy G Hoskins",
      "Zhongjian Wang"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.08847"
  },
  {
    "id": "arXiv:2206.08868",
    "title": "Generalized Frank-Wolfe Algorithm for Bilevel Optimization",
    "abstract": "In this paper, we study a class of bilevel optimization problems, also known\nas simple bilevel optimization, where we minimize a smooth objective function\nover the optimal solution set of another convex constrained optimization\nproblem. Several iterative methods have been developed for tackling this class\nof problems. Alas, their convergence guarantees are not satisfactory as they\nare either asymptotic for the upper-level objective, or the convergence rates\nare slow and sub-optimal. To address this issue, in this paper, we introduce a\ngeneralization of the Frank-Wolfe (FW) method to solve the considered problem.\nThe main idea of our method is to locally approximate the solution set of the\nlower-level problem via a cutting plane, and then run a FW-type update to\ndecrease the upper-level objective. When the upper-level objective is convex,\nwe show that our method requires\n${\\mathcal{O}}(\\max\\{1/\\epsilon_f,1/\\epsilon_g\\})$ iterations to find a\nsolution that is $\\epsilon_f$-optimal for the upper-level objective and\n$\\epsilon_g$-optimal for the lower-level objective. Moreover, when the\nupper-level objective is non-convex, our method requires\n${\\mathcal{O}}(\\max\\{1/\\epsilon_f^2,1/(\\epsilon_f\\epsilon_g)\\})$ iterations to\nfind an $(\\epsilon_f,\\epsilon_g)$-optimal solution. We further prove stronger\nconvergence guarantees under the H\\\"olderian error bound assumption on the\nlower-level problem. To the best of our knowledge, our method achieves the\nbest-known iteration complexity for the considered bilevel problem. We also\npresent numerical experiments to showcase the superior performance of our\nmethod compared with state-of-the-art methods.",
    "descriptor": "\nComments: 24 pages, 4 figures\n",
    "authors": [
      "Ruichen Jiang",
      "Nazanin Abolfazli",
      "Aryan Mokhtari",
      "Erfan Yazdandoost Hamedani"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08868"
  },
  {
    "id": "arXiv:2206.08873",
    "title": "Mirror Descent with Relative Smoothness in Measure Spaces, with  application to Sinkhorn and EM",
    "abstract": "Many problems in machine learning can be formulated as optimizing a convex\nfunctional over a space of measures. This paper studies the convergence of the\nmirror descent algorithm in this infinite-dimensional setting. Defining Bregman\ndivergences through directional derivatives, we derive the convergence of the\nscheme for relatively smooth and strongly convex pairs of functionals. Applying\nour result to joint distributions and the Kullback--Leibler (KL) divergence, we\nshow that Sinkhorn's primal iterations for entropic optimal transport in the\ncontinuous setting correspond to a mirror descent, and we obtain a new proof of\nits (sub)linear convergence. We also show that Expectation Maximization (EM)\ncan always formally be written as a mirror descent, and, when optimizing on the\nlatent distribution while fixing the mixtures, we derive sublinear rates of\nconvergence.",
    "descriptor": "",
    "authors": [
      "Pierre-Cyril Aubin-Frankowski",
      "Anna Korba",
      "Flavien L\u00e9ger"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08873"
  },
  {
    "id": "arXiv:2206.08885",
    "title": "Incorporating intratumoral heterogeneity into weakly-supervised deep  learning models via variance pooling",
    "abstract": "Supervised learning tasks such as cancer survival prediction from gigapixel\nwhole slide images (WSIs) are a critical challenge in computational pathology\nthat requires modeling complex features of the tumor microenvironment. These\nlearning tasks are often solved with deep multi-instance learning (MIL) models\nthat do not explicitly capture intratumoral heterogeneity. We develop a novel\nvariance pooling architecture that enables a MIL model to incorporate\nintratumoral heterogeneity into its predictions. Two interpretability tools\nbased on representative patches are illustrated to probe the biological signals\ncaptured by these models. An empirical study with 4,479 gigapixel WSIs from the\nCancer Genome Atlas shows that adding variance pooling onto MIL frameworks\nimproves survival prediction performance for five cancer types.",
    "descriptor": "\nComments: To appear in MICCAI 2022\n",
    "authors": [
      "Iain Carmichael",
      "Andrew H. Song",
      "Richard J. Chen",
      "Drew F.K. Williamson",
      "Tiffany Y. Chen",
      "Faisal Mahmood"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08885"
  },
  {
    "id": "arXiv:2206.08889",
    "title": "Lossy Compression with Gaussian Diffusion",
    "abstract": "We describe a novel lossy compression approach called DiffC which is based on\nunconditional diffusion generative models. Unlike modern compression schemes\nwhich rely on transform coding and quantization to restrict the transmitted\ninformation, DiffC relies on the efficient communication of pixels corrupted by\nGaussian noise. We implement a proof of concept and find that it works\nsurprisingly well despite the lack of an encoder transform, outperforming the\nstate-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC\nonly uses a single model to encode and denoise corrupted pixels at arbitrary\nbitrates. The approach further provides support for progressive coding, that\nis, decoding from partial bit streams. We perform a rate-distortion analysis to\ngain a deeper understanding of its performance, providing analytical results\nfor multivariate Gaussian data as well as initial results for general\ndistributions. Furthermore, we show that a flow-based reconstruction achieves a\n3 dB gain over ancestral sampling at high bitrates.",
    "descriptor": "",
    "authors": [
      "Lucas Theis",
      "Tim Salimans",
      "Matthew D. Hoffman",
      "Fabian Mentzer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08889"
  },
  {
    "id": "arXiv:2206.08894",
    "title": "Scaling multi-species occupancy models to large citizen science datasets",
    "abstract": "Citizen science datasets can be very large and promise to improve species\ndistribution modelling, but detection is imperfect, risking bias when fitting\nmodels. In particular, observers may not detect species that are actually\npresent. Occupancy models can estimate and correct for this observation\nprocess, and multi-species occupancy models exploit similarities in the\nobservation process, which can improve estimates for rare species. However, the\ncomputational methods currently used to fit these models do not scale to large\ndatasets. We develop approximate Bayesian inference methods and use graphics\nprocessing units (GPUs) to scale multi-species occupancy models to very large\ncitizen science data. We fit multi-species occupancy models to one month of\ndata from the eBird project consisting of 186,811 checklist records comprising\n430 bird species. We evaluate the predictions on a spatially separated test set\nof 59,338 records, comparing two different inference methods -- Markov chain\nMonte Carlo (MCMC) and variational inference (VI) -- to occupancy models fitted\nto each species separately using maximum likelihood. We fitted models to the\nentire dataset using VI, and up to 32,000 records with MCMC. VI fitted to the\nentire dataset performed best, outperforming single-species models on both AUC\n(90.4% compared to 88.7%) and on log likelihood (-0.080 compared to -0.085). We\nalso evaluate how well range maps predicted by the model agree with expert\nmaps. We find that modelling the detection process greatly improves agreement\nand that the resulting maps agree as closely with expert maps as ones estimated\nusing high quality survey data. Our results demonstrate that multi-species\noccupancy models are a compelling approach to model large citizen science\ndatasets, and that, once the observation process is taken into account, they\ncan model species distributions accurately.",
    "descriptor": "\nComments: 39 pages, 6 figures (+ supplementary material)\n",
    "authors": [
      "Martin Ingram",
      "Damjan Vukcevic",
      "Nick Golding"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.08894"
  },
  {
    "id": "arXiv:2206.08900",
    "title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning",
    "abstract": "The linearised Laplace method for estimating model uncertainty has received\nrenewed attention in the Bayesian deep learning community. The method provides\nreliable error bars and admits a closed-form expression for the model evidence,\nallowing for scalable selection of model hyperparameters. In this work, we\nexamine the assumptions behind this method, particularly in conjunction with\nmodel selection. We show that these interact poorly with some now-standard\ntools of deep learning--stochastic approximation methods and normalisation\nlayers--and make recommendations for how to better adapt this classic method to\nthe modern setting. We provide theoretical support for our recommendations and\nvalidate them empirically on MLPs, classic CNNs, residual networks with and\nwithout normalisation layers, generative autoencoders and transformers.",
    "descriptor": "\nComments: Paper appearing at ICML 2022\n",
    "authors": [
      "Javier Antor\u00e1n",
      "David Janz",
      "James Urquhart Allingham",
      "Erik Daxberger",
      "Riccardo Barbano",
      "Eric Nalisnick",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08900"
  },
  {
    "id": "arXiv:2206.08901",
    "title": "SYMBA: Symbolic Computation of Squared Amplitudes in High Energy Physics  with Machine ALearning",
    "abstract": "The cross section is one of the most important physical quantities in\nhigh-energy physics and the most time consuming to compute. While machine\nlearning has proven to be highly successful in numerical calculations in\nhigh-energy physics, analytical calculations using machine learning are still\nin their infancy. In this work, we use a sequence-to-sequence transformer model\nto compute a key element of the cross section calculation, namely, the squared\namplitude of an interaction. We show that a transformer model is able to\npredict correctly 89.0% and 99.4% of squared amplitudes of QCD and QED\nprocesses, respectively. We discuss the performance of the current model, its\nlimitations and possible future directions for this work.",
    "descriptor": "",
    "authors": [
      "Abdulhakim Alnuqaydan",
      "Sergei Gleyzer",
      "Harrison Prosper"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08901"
  },
  {
    "id": "arXiv:2206.08917",
    "title": "The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide  Electrocatalysis",
    "abstract": "Computational catalysis and machine learning communities have made\nconsiderable progress in developing machine learning models for catalyst\ndiscovery and design. Yet, a general machine learning potential that spans the\nchemical space of catalysis is still out of reach. A significant hurdle is\nobtaining access to training data across a wide range of materials. One\nimportant class of materials where data is lacking are oxides, which inhibits\nmodels from studying the Oxygen Evolution Reaction and oxide electrocatalysis\nmore generally. To address this we developed the Open Catalyst 2022(OC22)\ndataset, consisting of 62,521 Density Functional Theory (DFT) relaxations\n(~9,884,504 single point calculations) across a range of oxide materials,\ncoverages, and adsorbates (*H, *O, *N, *C, *OOH, *OH, *OH2, *O2, *CO). We\ndefine generalized tasks to predict the total system energy that are applicable\nacross catalysis, develop baseline performance of several graph neural networks\n(SchNet, DimeNet++, ForceNet, SpinConv, PaiNN, GemNet-dT, GemNet-OC), and\nprovide pre-defined dataset splits to establish clear benchmarks for future\nefforts. For all tasks, we study whether combining datasets leads to better\nresults, even if they contain different materials or adsorbates. Specifically,\nwe jointly train models on Open Catalyst 2020 (OC20) Dataset and OC22, or\nfine-tune pretrained OC20 models on OC22. In the most general task, GemNet-OC\nsees a ~32% improvement in energy predictions through fine-tuning and a ~9%\nimprovement in force predictions via joint training. Surprisingly, joint\ntraining on both the OC20 and much smaller OC22 datasets also improves total\nenergy predictions on OC20 by ~19%. The dataset and baseline models are open\nsourced, and a public leaderboard will follow to encourage continued community\ndevelopments on the total energy tasks and data.",
    "descriptor": "\nComments: 37 pages, 11 figures\n",
    "authors": [
      "Richard Tran",
      "Janice Lan",
      "Muhammed Shuaibi",
      "Siddharth Goyal",
      "Brandon M. Wood",
      "Abhishek Das",
      "Javier Heras-Domingo",
      "Adeesh Kolluru",
      "Ammar Rizvi",
      "Nima Shoghi",
      "Anuroop Sriram",
      "Zachary Ulissi",
      "C. Lawrence Zitnick"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.08917"
  },
  {
    "id": "arXiv:1308.1351",
    "title": "An $O^*(1.0821^n)$-Time Algorithm for Computing Maximum Independent Set  in Graphs with Bounded Degree 3",
    "abstract": "Comments: While working on an updated version, we observed a bug in one of the cases of our extensive case analysis. We are withdrawing this paper while we work to fix the bug. We will add an updated version once we manage to fix the bug",
    "descriptor": "\nComments: While working on an updated version, we observed a bug in one of the cases of our extensive case analysis. We are withdrawing this paper while we work to fix the bug. We will add an updated version once we manage to fix the bug\n",
    "authors": [
      "Davis Issac",
      "Ragesh Jaiswal"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1308.1351"
  },
  {
    "id": "arXiv:1810.00226",
    "title": "Toward single particle reconstruction without particle picking: Breaking  the detection limit",
    "abstract": "Comments: Older citations to this paper refer to version arXiv:1810.00226v1, parts of which now appear in: Tamir Bendory, Nicolas Boumal, William Leeb, Eitan Levin, and Amit Singer. \"Multi-target detection with application to cryo-electron microscopy.\" Inverse Problems 35, no. 10 (2019): 104003",
    "descriptor": "\nComments: Older citations to this paper refer to version arXiv:1810.00226v1, parts of which now appear in: Tamir Bendory, Nicolas Boumal, William Leeb, Eitan Levin, and Amit Singer. \"Multi-target detection with application to cryo-electron microscopy.\" Inverse Problems 35, no. 10 (2019): 104003\n",
    "authors": [
      "Tamir Bendory",
      "Nicolas Boumal",
      "William Leeb",
      "Eitan Levin",
      "Amit Singer"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1810.00226"
  },
  {
    "id": "arXiv:1905.03231",
    "title": "Smoothing Policies and Safe Policy Gradients",
    "abstract": "Smoothing Policies and Safe Policy Gradients",
    "descriptor": "",
    "authors": [
      "Matteo Papini",
      "Matteo Pirotta",
      "Marcello Restelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1905.03231"
  },
  {
    "id": "arXiv:1910.09083",
    "title": "Spectral CUSUM for Online Network Structure Change Detection",
    "abstract": "Spectral CUSUM for Online Network Structure Change Detection",
    "descriptor": "",
    "authors": [
      "Minghe Zhang",
      "Liyan Xie",
      "Yao Xie"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/1910.09083"
  },
  {
    "id": "arXiv:2002.07563",
    "title": "A Model to Measure the Spread Power of Rumors",
    "abstract": "Comments: This is a preprint of an article published in \"Journal of Ambient Intelligence and Humanized Computing\"",
    "descriptor": "\nComments: This is a preprint of an article published in \"Journal of Ambient Intelligence and Humanized Computing\"\n",
    "authors": [
      "Zoleikha Jahanbakhsh-Nagadeh",
      "Mohammad-Reza Feizi-Derakhshi",
      "Majid Ramezani",
      "Taymaz Akan",
      "Meysam Asgari-Chenaghlu",
      "Narjes Nikzad-Khasmakhi",
      "Ali-Reza Feizi-Derakhshi",
      "Mehrdad Ranjbar-Khadivi",
      "Elnaz Zafarani-Moattar",
      "Mohammad-Ali Balafar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2002.07563"
  },
  {
    "id": "arXiv:2002.10061",
    "title": "Omni-Scale CNNs: a simple and effective kernel size configuration for  time series classification",
    "abstract": "Comments: Accepted by ICLR 2022",
    "descriptor": "\nComments: Accepted by ICLR 2022\n",
    "authors": [
      "Wensi Tang",
      "Guodong Long",
      "Lu Liu",
      "Tianyi Zhou",
      "Michael Blumenstein",
      "Jing Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.10061"
  },
  {
    "id": "arXiv:2004.13472",
    "title": "Linear Dependent Type Theory for Quantum Programming Languages",
    "abstract": "Linear Dependent Type Theory for Quantum Programming Languages",
    "descriptor": "",
    "authors": [
      "Peng Fu",
      "Kohei Kishida",
      "Peter Selinger"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2004.13472"
  },
  {
    "id": "arXiv:2005.01223",
    "title": "Complexity of Sparse Polynomial Solving 2: Renormalization",
    "abstract": "Comments: 102 pages. Minor revision from the previous version, essentially a clarification of Algorithm 1",
    "descriptor": "\nComments: 102 pages. Minor revision from the previous version, essentially a clarification of Algorithm 1\n",
    "authors": [
      "Gregorio Malajovich"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2005.01223"
  },
  {
    "id": "arXiv:2006.04429",
    "title": "Beyond Worst-Case Analysis in Stochastic Approximation: Moment  Estimation Improves Instance Complexity",
    "abstract": "Beyond Worst-Case Analysis in Stochastic Approximation: Moment  Estimation Improves Instance Complexity",
    "descriptor": "",
    "authors": [
      "Jingzhao Zhang",
      "Hongzhou Lin",
      "Subhro Das",
      "Suvrit Sra",
      "Ali Jadbabaie"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.04429"
  },
  {
    "id": "arXiv:2006.04487",
    "title": "Blockchain Consensus and Integrity: Similarities and Learnings from  Ancient Literature",
    "abstract": "Comments: Updated title on Arxiv",
    "descriptor": "\nComments: Updated title on Arxiv\n",
    "authors": [
      "Ashish Kundu",
      "Arun Ayachitula",
      "Nagamani Sistla"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2006.04487"
  },
  {
    "id": "arXiv:2006.06879",
    "title": "Active Sampling for Min-Max Fairness",
    "abstract": "Active Sampling for Min-Max Fairness",
    "descriptor": "",
    "authors": [
      "Jacob Abernethy",
      "Pranjal Awasthi",
      "Matth\u00e4us Kleindessner",
      "Jamie Morgenstern",
      "Chris Russell",
      "Jie Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.06879"
  },
  {
    "id": "arXiv:2008.04104",
    "title": "Asymptotically Stable Optimal Multi-rate Rigid Body Attitude Estimation  based on Lagrange-d'Alembert Principle",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2007.08185",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2007.08185\n",
    "authors": [
      "Maulik Bhatt",
      "Amit K. Sanyal",
      "Srikant Sukumar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2008.04104"
  },
  {
    "id": "arXiv:2011.12468",
    "title": "Nudge: Accelerating Overdue Pull Requests Towards Completion",
    "abstract": "Nudge: Accelerating Overdue Pull Requests Towards Completion",
    "descriptor": "",
    "authors": [
      "Chandra Maddila",
      "Sai Surya Upadrasta",
      "Chetan Bansal",
      "Nachiappan Nagappan",
      "Georgios Gousios",
      "Arie van Deursen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2011.12468"
  },
  {
    "id": "arXiv:2101.03961",
    "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity",
    "abstract": "Comments: JMLR",
    "descriptor": "\nComments: JMLR\n",
    "authors": [
      "William Fedus",
      "Barret Zoph",
      "Noam Shazeer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.03961"
  },
  {
    "id": "arXiv:2101.11353",
    "title": "Variational Nested Dropout",
    "abstract": "Comments: 20 pages, 17 figures",
    "descriptor": "\nComments: 20 pages, 17 figures\n",
    "authors": [
      "Yufei Cui",
      "Yu Mao",
      "Ziquan Liu",
      "Qiao Li",
      "Antoni B. Chan",
      "Xue Liu",
      "Tei-Wei Kuo",
      "Chun Jason Xue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.11353"
  },
  {
    "id": "arXiv:2102.12035",
    "title": "Multiple Access Channel Simulation",
    "abstract": "Comments: 30 pages, 3 figures",
    "descriptor": "\nComments: 30 pages, 3 figures\n",
    "authors": [
      "Gowtham R. Kurri",
      "Viswanathan Ramachandran",
      "Sibi Raj B. Pillai",
      "Vinod M. Prabhakaran"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.12035"
  },
  {
    "id": "arXiv:2103.01907",
    "title": "Fairness in Credit Scoring: Assessment, Implementation and Profit  Implications",
    "abstract": "Comments: Accepted to European Journal of Operational Research",
    "descriptor": "\nComments: Accepted to European Journal of Operational Research\n",
    "authors": [
      "Nikita Kozodoi",
      "Johannes Jacob",
      "Stefan Lessmann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Risk Management (q-fin.RM)"
    ],
    "url": "https://arxiv.org/abs/2103.01907"
  },
  {
    "id": "arXiv:2103.01910",
    "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset",
    "abstract": "Comments: Added an n-gram with back-off baseline model to the lexical translation task (Section 7.2.4). Also synchronised the paper structure to the LREC2022 version of this work. This arxiv version is a longer version of the LREC2022 version including more experiments and an additional lexical translation task",
    "descriptor": "\nComments: Added an n-gram with back-off baseline model to the lexical translation task (Section 7.2.4). Also synchronised the paper structure to the LREC2022 version of this work. This arxiv version is a longer version of the LREC2022 version including more experiments and an additional lexical translation task\n",
    "authors": [
      "Josiah Wang",
      "Pranava Madhyastha",
      "Josiel Figueiredo",
      "Chiraag Lala",
      "Lucia Specia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2103.01910"
  },
  {
    "id": "arXiv:2103.04053",
    "title": "NVUM: Non-Volatile Unbiased Memory for Robust Medical Image  Classification",
    "abstract": "Comments: MICCAI 2022 Early Accept",
    "descriptor": "\nComments: MICCAI 2022 Early Accept\n",
    "authors": [
      "Fengbei Liu",
      "Yuanhong Chen",
      "Yu Tian",
      "Yuyuan Liu",
      "Chong Wang",
      "Vasileios Belagiannis",
      "Gustavo Carneiro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.04053"
  },
  {
    "id": "arXiv:2103.16446",
    "title": "CovidTracker: A comprehensive Covid-related social media dataset for NLP  tasks",
    "abstract": "CovidTracker: A comprehensive Covid-related social media dataset for NLP  tasks",
    "descriptor": "",
    "authors": [
      "Richard Plant",
      "Amir Hussain"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2103.16446"
  },
  {
    "id": "arXiv:2104.03958",
    "title": "GrASP: A Library for Extracting and Exploring Human-Interpretable  Textual Patterns",
    "abstract": "Comments: Proceedings of Language Resources and Evaluation (LREC), Marseille, France pp 6093-6103 (2022)",
    "descriptor": "\nComments: Proceedings of Language Resources and Evaluation (LREC), Marseille, France pp 6093-6103 (2022)\n",
    "authors": [
      "Piyawat Lertvittayakumjorn",
      "Leshem Choshen",
      "Eyal Shnarch",
      "Francesca Toni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.03958"
  },
  {
    "id": "arXiv:2104.08820",
    "title": "Fair Coin Flipping: Tighter Analysis and the Many-Party Case",
    "abstract": "Comments: Published in SODA 2017",
    "descriptor": "\nComments: Published in SODA 2017\n",
    "authors": [
      "Niv Buchbinder",
      "Iftach Haitner",
      "Nissan Levi",
      "Eliad Tsfadia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.08820"
  },
  {
    "id": "arXiv:2104.12958",
    "title": "On the Evaluation of the Eigendecomposition of the Airy Integral  Operator",
    "abstract": "Comments: 48 pages, 3 tables, 8 figures",
    "descriptor": "\nComments: 48 pages, 3 tables, 8 figures\n",
    "authors": [
      "Zewen Shen",
      "Kirill Serkh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2104.12958"
  },
  {
    "id": "arXiv:2105.06506",
    "title": "Sanity Simulations for Saliency Methods",
    "abstract": "Comments: Accepted to International Conference on Machine Learning (ICML 2022)",
    "descriptor": "\nComments: Accepted to International Conference on Machine Learning (ICML 2022)\n",
    "authors": [
      "Joon Sik Kim",
      "Gregory Plumb",
      "Ameet Talwalkar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.06506"
  },
  {
    "id": "arXiv:2106.03076",
    "title": "A Convergence Theory for SVGD in the Population Limit under Talagrand's  Inequality T1",
    "abstract": "A Convergence Theory for SVGD in the Population Limit under Talagrand's  Inequality T1",
    "descriptor": "",
    "authors": [
      "Adil Salim",
      "Lukang Sun",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03076"
  },
  {
    "id": "arXiv:2106.03305",
    "title": "Faster Cut-Equivalent Trees in Simple Graphs",
    "abstract": "Comments: Change license",
    "descriptor": "\nComments: Change license\n",
    "authors": [
      "Tianyi Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.03305"
  },
  {
    "id": "arXiv:2106.05378",
    "title": "Feature and Parameter Selection in Stochastic Linear Bandits",
    "abstract": "Feature and Parameter Selection in Stochastic Linear Bandits",
    "descriptor": "",
    "authors": [
      "Ahmadreza Moradipari",
      "Berkay Turan",
      "Yasin Abbasi-Yadkori",
      "Mahnoosh Alizadeh",
      "Mohammad Ghavamzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05378"
  },
  {
    "id": "arXiv:2106.06788",
    "title": "Learngene: From Open-World to Your Learning Task",
    "abstract": "Comments: To be appeared in AAAI-22",
    "descriptor": "\nComments: To be appeared in AAAI-22\n",
    "authors": [
      "Qiufeng Wang",
      "Xin Geng",
      "Shuxia Lin",
      "Shiyu Xia",
      "Lei Qi",
      "Ning Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06788"
  },
  {
    "id": "arXiv:2106.09533",
    "title": "Author Clustering and Topic Estimation for Short Texts",
    "abstract": "Author Clustering and Topic Estimation for Short Texts",
    "descriptor": "",
    "authors": [
      "Graham Tierney",
      "Christopher Bail",
      "Alexander Volfovsky"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.09533"
  },
  {
    "id": "arXiv:2107.00472",
    "title": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets",
    "abstract": "Comments: 35 pages, 11 figures",
    "descriptor": "\nComments: 35 pages, 11 figures\n",
    "authors": [
      "Baojian Zhou",
      "Yifan Sun"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.00472"
  },
  {
    "id": "arXiv:2107.04123",
    "title": "Efficient topology optimization using compatibility projection in  micromechanical homogenization",
    "abstract": "Comments: 17 pages, 5 figures",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Indre J\u00f6dicke",
      "Richard J. Leute",
      "Till Junge",
      "Lars Pastewka"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2107.04123"
  },
  {
    "id": "arXiv:2107.10060",
    "title": "Conditional GANs with Auxiliary Discriminative Classifier",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Liang Hou",
      "Qi Cao",
      "Huawei Shen",
      "Siyuan Pan",
      "Xiaoshuang Li",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.10060"
  },
  {
    "id": "arXiv:2108.05433",
    "title": "Learning to Hash Robustly, Guaranteed",
    "abstract": "Learning to Hash Robustly, Guaranteed",
    "descriptor": "",
    "authors": [
      "Alexandr Andoni",
      "Daniel Beaglehole"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.05433"
  },
  {
    "id": "arXiv:2108.07664",
    "title": "On the Complexity of Two-Party Differential Privacy",
    "abstract": "Comments: Accepted to STOC 2022",
    "descriptor": "\nComments: Accepted to STOC 2022\n",
    "authors": [
      "Iftach Haitner",
      "Noam Mazor",
      "Jad Silbak",
      "Eliad Tsfadia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2108.07664"
  },
  {
    "id": "arXiv:2108.10752",
    "title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse  Self-Attention Layers",
    "abstract": "Comments: To be published in INTERSPEECH 2022",
    "descriptor": "\nComments: To be published in INTERSPEECH 2022\n",
    "authors": [
      "Juntae Kim",
      "Jeehye Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2108.10752"
  },
  {
    "id": "arXiv:2109.01365",
    "title": "A Comparative Study of Nonlinear MPC and Differential-Flatness-Based  Control for Quadrotor Agile Flight",
    "abstract": "Comments: The paper has been accepted for publication in the IEEE Transactions on Robotics (T-RO), 2022",
    "descriptor": "\nComments: The paper has been accepted for publication in the IEEE Transactions on Robotics (T-RO), 2022\n",
    "authors": [
      "Sihao Sun",
      "Angel Romero",
      "Philipp Foehn",
      "Elia Kaufmann",
      "Davide Scaramuzza"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.01365"
  },
  {
    "id": "arXiv:2109.01408",
    "title": "Automatic Foot Ulcer Segmentation Using an Ensemble of Convolutional  Neural Networks",
    "abstract": "Comments: Accepted for the 26th International Conference on Pattern Recognition (ICPR 2022)",
    "descriptor": "\nComments: Accepted for the 26th International Conference on Pattern Recognition (ICPR 2022)\n",
    "authors": [
      "Amirreza Mahbod",
      "Gerald Schaefer",
      "Rupert Ecker",
      "Isabella Ellinger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.01408"
  },
  {
    "id": "arXiv:2109.02533",
    "title": "Neural Ensemble Search via Bayesian Sampling",
    "abstract": "Comments: Published as a conference paper at UAI 2022",
    "descriptor": "\nComments: Published as a conference paper at UAI 2022\n",
    "authors": [
      "Yao Shu",
      "Yizhou Chen",
      "Zhongxiang Dai",
      "Bryan Kian Hsiang Low"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.02533"
  },
  {
    "id": "arXiv:2109.03465",
    "title": "A Survey of Sound Source Localization with Deep Learning Methods",
    "abstract": "Comments: Accepted for publication in The Journal of the Acoustical Society of America",
    "descriptor": "\nComments: Accepted for publication in The Journal of the Acoustical Society of America\n",
    "authors": [
      "Pierre-Amaury Grumiaux",
      "Sr\u0111an Kiti\u0107",
      "Laurent Girin",
      "Alexandre Gu\u00e9rin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2109.03465"
  },
  {
    "id": "arXiv:2109.03856",
    "title": "Local Augmentation for Graph Neural Networks",
    "abstract": "Comments: Accepted by ICML'22",
    "descriptor": "\nComments: Accepted by ICML'22\n",
    "authors": [
      "Songtao Liu",
      "Rex Ying",
      "Hanze Dong",
      "Lanqing Li",
      "Tingyang Xu",
      "Yu Rong",
      "Peilin Zhao",
      "Junzhou Huang",
      "Dinghao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.03856"
  },
  {
    "id": "arXiv:2109.04033",
    "title": "New Versions of Gradient Temporal Difference Learning",
    "abstract": "New Versions of Gradient Temporal Difference Learning",
    "descriptor": "",
    "authors": [
      "Donghwan Lee",
      "Han-Dong Lim",
      "Jihoon Park",
      "Okyong Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04033"
  },
  {
    "id": "arXiv:2109.05075",
    "title": "On the Compression of Neural Networks Using $\\ell_0$-Norm Regularization  and Weight Pruning",
    "abstract": "Comments: 23 pages, 6 figures, 2 tables",
    "descriptor": "\nComments: 23 pages, 6 figures, 2 tables\n",
    "authors": [
      "Felipe Dennis de Resende Oliveira",
      "Eduardo Luiz Ortiz Batista",
      "Rui Seara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2109.05075"
  },
  {
    "id": "arXiv:2109.11938",
    "title": "Meta-brain Models: biologically-inspired cognitive agents",
    "abstract": "Comments: 20 pages, 3 figures",
    "descriptor": "\nComments: 20 pages, 3 figures\n",
    "authors": [
      "Bradly Alicea",
      "Jesse Parent"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.11938"
  },
  {
    "id": "arXiv:2109.13376",
    "title": "Counting colorings of triangle-free graphs",
    "abstract": "Comments: 16 pp",
    "descriptor": "\nComments: 16 pp\n",
    "authors": [
      "Anton Bernshteyn",
      "Tyler Brazelton",
      "Ruijia Cao",
      "Akum Kang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2109.13376"
  },
  {
    "id": "arXiv:2109.13916",
    "title": "Unsolved Problems in ML Safety",
    "abstract": "Comments: Position Paper",
    "descriptor": "\nComments: Position Paper\n",
    "authors": [
      "Dan Hendrycks",
      "Nicholas Carlini",
      "John Schulman",
      "Jacob Steinhardt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.13916"
  },
  {
    "id": "arXiv:2110.01229",
    "title": "3LegRace: Privacy-Preserving DNN Training over TEEs and GPUs",
    "abstract": "Comments: Accepted to Privacy Enhancing Technologies Symposium (PETS) 2022",
    "descriptor": "\nComments: Accepted to Privacy Enhancing Technologies Symposium (PETS) 2022\n",
    "authors": [
      "Yue Niu",
      "Ramy E. Ali",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.01229"
  },
  {
    "id": "arXiv:2110.04328",
    "title": "Distinguishing rule- and exemplar-based generalization in learning  systems",
    "abstract": "Comments: To appear at the 39th International Conference on Machine Learning (ICML 2022)",
    "descriptor": "\nComments: To appear at the 39th International Conference on Machine Learning (ICML 2022)\n",
    "authors": [
      "Ishita Dasgupta",
      "Erin Grant",
      "Thomas L. Griffiths"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04328"
  },
  {
    "id": "arXiv:2110.04484",
    "title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR",
    "abstract": "Comments: Accepted by Interspeech 2022",
    "descriptor": "\nComments: Accepted by Interspeech 2022\n",
    "authors": [
      "Han Zhu",
      "Li Wang",
      "Jindong Wang",
      "Gaofeng Cheng",
      "Pengyuan Zhang",
      "Yonghong Yan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.04484"
  },
  {
    "id": "arXiv:2110.05007",
    "title": "Boosting Fast Adversarial Training with Learnable Adversarial  Initialization",
    "abstract": "Comments: Accepted by TIP",
    "descriptor": "\nComments: Accepted by TIP\n",
    "authors": [
      "Xiaojun Jia",
      "Yong Zhang",
      "Baoyuan Wu",
      "Jue Wang",
      "Xiaochun Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05007"
  },
  {
    "id": "arXiv:2110.06256",
    "title": "Neural Network Weights Do Not Converge to Stationary Points: An  Invariant Measure Perspective",
    "abstract": "Neural Network Weights Do Not Converge to Stationary Points: An  Invariant Measure Perspective",
    "descriptor": "",
    "authors": [
      "Jingzhao Zhang",
      "Haochuan Li",
      "Suvrit Sra",
      "Ali Jadbabaie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.06256"
  },
  {
    "id": "arXiv:2110.06435",
    "title": "Dropout Prediction Uncertainty Estimation Using Neuron Activation  Strength",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Haichao Yu",
      "Zhe Chen",
      "Dong Lin",
      "Gil Shamir",
      "Jie Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06435"
  },
  {
    "id": "arXiv:2110.09599",
    "title": "Label-Descriptive Patterns and Their Application to Characterizing  Classification Errors",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Michael Hedderich",
      "Jonas Fischer",
      "Dietrich Klakow",
      "Jilles Vreeken"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.09599"
  },
  {
    "id": "arXiv:2110.09697",
    "title": "abess: A Fast Best Subset Selection Library in Python and R",
    "abstract": "abess: A Fast Best Subset Selection Library in Python and R",
    "descriptor": "",
    "authors": [
      "Jin Zhu",
      "Xueqin Wang",
      "Liyuan Hu",
      "Junhao Huang",
      "Kangkang Jiang",
      "Yanhang Zhang",
      "Shiyun Lin",
      "Junxian Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.09697"
  },
  {
    "id": "arXiv:2110.12933",
    "title": "Computing elements of certain form in ideals to prove properties of  operators",
    "abstract": "Comments: 26 pages",
    "descriptor": "\nComments: 26 pages\n",
    "authors": [
      "Clemens Hofstadler",
      "Clemens G. Raab",
      "Georg Regensburger"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2110.12933"
  },
  {
    "id": "arXiv:2110.13900",
    "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech  Processing",
    "abstract": "Comments: Submitted to the Journal of Selected Topics in Signal Processing (JSTSP)",
    "descriptor": "\nComments: Submitted to the Journal of Selected Topics in Signal Processing (JSTSP)\n",
    "authors": [
      "Sanyuan Chen",
      "Chengyi Wang",
      "Zhengyang Chen",
      "Yu Wu",
      "Shujie Liu",
      "Zhuo Chen",
      "Jinyu Li",
      "Naoyuki Kanda",
      "Takuya Yoshioka",
      "Xiong Xiao",
      "Jian Wu",
      "Long Zhou",
      "Shuo Ren",
      "Yanmin Qian",
      "Yao Qian",
      "Jian Wu",
      "Michael Zeng",
      "Xiangzhan Yu",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.13900"
  },
  {
    "id": "arXiv:2110.14802",
    "title": "You Are the Best Reviewer of Your Own Papers: An Owner-Assisted Scoring  Mechanism",
    "abstract": "Comments: Corrected typos and added a reference",
    "descriptor": "\nComments: Corrected typos and added a reference\n",
    "authors": [
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14802"
  },
  {
    "id": "arXiv:2110.15729",
    "title": "Decision Attentive Regularization to Improve Simultaneous Speech  Translation Systems",
    "abstract": "Comments: 5 pages, 3 figures, 1 table",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table\n",
    "authors": [
      "Mohd Abbas Zaidi",
      "Beomseok Lee",
      "Sangha Kim",
      "Chanwoo Kim"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.15729"
  },
  {
    "id": "arXiv:2111.00228",
    "title": "whu-nercms at trecvid2021:instance search task",
    "abstract": "Comments: 9 pages, 4 figures",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Yanrui Niu",
      "Jingyao Yang",
      "Ankang Lu",
      "Baojin Huang",
      "Yue Zhang",
      "Ji Huang",
      "Shishi Wen",
      "Dongshu Xu",
      "Chao Liang",
      "Zhongyuan Wang",
      "Jun Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2111.00228"
  },
  {
    "id": "arXiv:2111.00261",
    "title": "Explicit and Efficient Construction of (nearly) Optimal Rate Codes for  Binary Deletion Channel and the Poisson Repeat Channel",
    "abstract": "Explicit and Efficient Construction of (nearly) Optimal Rate Codes for  Binary Deletion Channel and the Poisson Repeat Channel",
    "descriptor": "",
    "authors": [
      "Ittai Rubinstein"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2111.00261"
  },
  {
    "id": "arXiv:2111.00998",
    "title": "PDE-READ: Human-readable Partial Differential Equation Discovery using  Deep Learning",
    "abstract": "Comments: 41 pages, 18 figures",
    "descriptor": "\nComments: 41 pages, 18 figures\n",
    "authors": [
      "Robert Stephany",
      "Christopher Earls"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.00998"
  },
  {
    "id": "arXiv:2111.01701",
    "title": "Improve Single-Point Zeroth-Order Optimization Using High-Pass and  Low-Pass Filters",
    "abstract": "Improve Single-Point Zeroth-Order Optimization Using High-Pass and  Low-Pass Filters",
    "descriptor": "",
    "authors": [
      "Xin Chen",
      "Yujie Tang",
      "Na Li"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.01701"
  },
  {
    "id": "arXiv:2111.02355",
    "title": "A Theoretical Analysis on Independence-driven Importance Weighting for  Covariate-shift Generalization",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Renzhe Xu",
      "Xingxuan Zhang",
      "Zheyan Shen",
      "Tong Zhang",
      "Peng Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.02355"
  },
  {
    "id": "arXiv:2111.03740",
    "title": "Toward Learning Human-aligned Cross-domain Robust Models by Countering  Misaligned Features",
    "abstract": "Comments: to appear at UAI 2022",
    "descriptor": "\nComments: to appear at UAI 2022\n",
    "authors": [
      "Haohan Wang",
      "Zeyi Huang",
      "Hanlin Zhang",
      "Yong Jae Lee",
      "Eric Xing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.03740"
  },
  {
    "id": "arXiv:2111.05457",
    "title": "Optimizing Number, Placement, and Backhaul Connectivity of Multi-UAV  Networks",
    "abstract": "Comments: To appear in IEEE Internet of Things Journal",
    "descriptor": "\nComments: To appear in IEEE Internet of Things Journal\n",
    "authors": [
      "Javad Sabzehali",
      "Vijay K. Shah",
      "Qiang Fan",
      "Biplav Choudhury",
      "Lingjia Liu",
      "Jeffrey H. Reed"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2111.05457"
  },
  {
    "id": "arXiv:2111.06310",
    "title": "Self-Normalized Importance Sampling for Neural Language Modeling",
    "abstract": "Comments: Accepted at INTERSPEECH 2022",
    "descriptor": "\nComments: Accepted at INTERSPEECH 2022\n",
    "authors": [
      "Zijian Yang",
      "Yingbo Gao",
      "Alexander Gerstenberger",
      "Jintao Jiang",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2111.06310"
  },
  {
    "id": "arXiv:2111.09360",
    "title": "Personalized Federated Learning through Local Memorization",
    "abstract": "Comments: 23 pages, ICML 2022",
    "descriptor": "\nComments: 23 pages, ICML 2022\n",
    "authors": [
      "Othmane Marfoq",
      "Giovanni Neglia",
      "Laetitia Kameni",
      "Richard Vidal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.09360"
  },
  {
    "id": "arXiv:2111.11707",
    "title": "Boosting Neural Machine Translation with Dependency-Scaled  Self-Attention Network",
    "abstract": "Boosting Neural Machine Translation with Dependency-Scaled  Self-Attention Network",
    "descriptor": "",
    "authors": [
      "Ru Peng",
      "Nankai Lin",
      "Yi Fang",
      "Shengyi Jiang",
      "Tianyong Hao",
      "Boyu Chen",
      "Junbo Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2111.11707"
  },
  {
    "id": "arXiv:2111.12503",
    "title": "Extracting Triangular 3D Models, Materials, and Lighting From Images",
    "abstract": "Comments: Project website: this https URL",
    "descriptor": "\nComments: Project website: this https URL\n",
    "authors": [
      "Jacob Munkberg",
      "Jon Hasselgren",
      "Tianchang Shen",
      "Jun Gao",
      "Wenzheng Chen",
      "Alex Evans",
      "Thomas M\u00fcller",
      "Sanja Fidler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2111.12503"
  },
  {
    "id": "arXiv:2111.14813",
    "title": "TransWeather: Transformer-based Restoration of Images Degraded by  Adverse Weather Conditions",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Rajeev Yasarla",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.14813"
  },
  {
    "id": "arXiv:2112.00552",
    "title": "SaDe: Learning Models that Provably Satisfy Domain Constraints",
    "abstract": "Comments: 16 pages",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Kshitij Goyal",
      "Sebastijan Dumancic",
      "Hendrik Blockeel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2112.00552"
  },
  {
    "id": "arXiv:2112.03609",
    "title": "Decision-Focused Learning: Through the Lens of Learning to Rank",
    "abstract": "Comments: Accepted for presentation at ICML, 2022",
    "descriptor": "\nComments: Accepted for presentation at ICML, 2022\n",
    "authors": [
      "Jayanta Mandi",
      "V\u00edctor Bucarey",
      "Maxime Mulamba",
      "Tias Guns"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.03609"
  },
  {
    "id": "arXiv:2112.03857",
    "title": "Grounded Language-Image Pre-training",
    "abstract": "Comments: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1",
    "descriptor": "\nComments: CVPR 2022; updated visualizations; fixed hyper-parameters in Appendix C.1\n",
    "authors": [
      "Liunian Harold Li",
      "Pengchuan Zhang",
      "Haotian Zhang",
      "Jianwei Yang",
      "Chunyuan Li",
      "Yiwu Zhong",
      "Lijuan Wang",
      "Lu Yuan",
      "Lei Zhang",
      "Jenq-Neng Hwang",
      "Kai-Wei Chang",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2112.03857"
  },
  {
    "id": "arXiv:2112.04219",
    "title": "Learning over All Stabilizing Nonlinear Controllers for a  Partially-Observed Linear System",
    "abstract": "Learning over All Stabilizing Nonlinear Controllers for a  Partially-Observed Linear System",
    "descriptor": "",
    "authors": [
      "Ruigang Wang",
      "Nicholas H. Barbara",
      "Max Revay",
      "Ian R. Manchester"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2112.04219"
  },
  {
    "id": "arXiv:2112.07508",
    "title": "Anti-Money Laundering Alert Optimization Using Machine Learning with  Graphs",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Ahmad Naser Eddin",
      "Jacopo Bono",
      "David Apar\u00edcio",
      "David Polido",
      "Jo\u00e3o Tiago Ascens\u00e3o",
      "Pedro Bizarro",
      "Pedro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.07508"
  },
  {
    "id": "arXiv:2112.07897",
    "title": "Tight query complexity bounds for learning graph partitions",
    "abstract": "Comments: Accepted for presentation at the 35th Annual Conference of Learning Theory, 2022",
    "descriptor": "\nComments: Accepted for presentation at the 35th Annual Conference of Learning Theory, 2022\n",
    "authors": [
      "Xizhi Liu",
      "Sayan Mukherjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.07897"
  },
  {
    "id": "arXiv:2201.06463",
    "title": "Bayesian Calibration of imperfect computer models using Physics-informed  priors",
    "abstract": "Comments: 26 pages, 15 figures",
    "descriptor": "\nComments: 26 pages, 15 figures\n",
    "authors": [
      "Michail Spitieris",
      "Ingelin Steinsland"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2201.06463"
  },
  {
    "id": "arXiv:2201.10029",
    "title": "PONI: Potential Functions for ObjectGoal Navigation with  Interaction-free Learning",
    "abstract": "Comments: 8 pages + supplementary. Accepted in CVPR 2022",
    "descriptor": "\nComments: 8 pages + supplementary. Accepted in CVPR 2022\n",
    "authors": [
      "Santhosh Kumar Ramakrishnan",
      "Devendra Singh Chaplot",
      "Ziad Al-Halah",
      "Jitendra Malik",
      "Kristen Grauman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.10029"
  },
  {
    "id": "arXiv:2201.10990",
    "title": "Learning To Recognize Procedural Activities with Distant Supervision",
    "abstract": "Comments: CVPR 2022. Code will be released here this https URL",
    "descriptor": "\nComments: CVPR 2022. Code will be released here this https URL\n",
    "authors": [
      "Xudong Lin",
      "Fabio Petroni",
      "Gedas Bertasius",
      "Marcus Rohrbach",
      "Shih-Fu Chang",
      "Lorenzo Torresani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10990"
  },
  {
    "id": "arXiv:2201.11569",
    "title": "Human Interpretation of Saliency-based Explanation Over Text",
    "abstract": "Comments: FAccT 2022",
    "descriptor": "\nComments: FAccT 2022\n",
    "authors": [
      "Hendrik Schuff",
      "Alon Jacovi",
      "Heike Adel",
      "Yoav Goldberg",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11569"
  },
  {
    "id": "arXiv:2201.11729",
    "title": "Implicit Regularization in Hierarchical Tensor Factorization and Deep  Convolutional Neural Networks",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Noam Razin",
      "Asaf Maman",
      "Nadav Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.11729"
  },
  {
    "id": "arXiv:2201.11927",
    "title": "Constrained Variational Policy Optimization for Safe Reinforcement  Learning",
    "abstract": "Comments: ICML 2022. 25 pages",
    "descriptor": "\nComments: ICML 2022. 25 pages\n",
    "authors": [
      "Zuxin Liu",
      "Zhepeng Cen",
      "Vladislav Isenbaev",
      "Wei Liu",
      "Zhiwei Steven Wu",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.11927"
  },
  {
    "id": "arXiv:2201.12176",
    "title": "Generative Coarse-Graining of Molecular Conformations",
    "abstract": "Comments: 23 pages, 11 figures",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Wujie Wang",
      "Minkai Xu",
      "Chen Cai",
      "Benjamin Kurt Miller",
      "Tess Smidt",
      "Yusu Wang",
      "Jian Tang",
      "Rafael G\u00f3mez-Bombarelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.12176"
  },
  {
    "id": "arXiv:2201.12987",
    "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention  Mechanism",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Siqi Miao",
      "Miaoyuan Liu",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12987"
  },
  {
    "id": "arXiv:2202.00436",
    "title": "ROCK: Causal Inference Principles for Reasoning about Commonsense  Causality",
    "abstract": "Comments: To appear, ICML 2022",
    "descriptor": "\nComments: To appear, ICML 2022\n",
    "authors": [
      "Jiayao Zhang",
      "Hongming Zhang",
      "Weijie J. Su",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2202.00436"
  },
  {
    "id": "arXiv:2202.00602",
    "title": "Meta-Learning Hypothesis Spaces for Sequential Decision-making",
    "abstract": "Comments: 23 pages, 11 figures",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Parnian Kassraie",
      "Jonas Rothfuss",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.00602"
  },
  {
    "id": "arXiv:2202.00787",
    "title": "Achieving Fairness at No Utility Cost via Data Reweighing with Influence",
    "abstract": "Comments: To appear in ICML 2022",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "Peizhao Li",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2202.00787"
  },
  {
    "id": "arXiv:2202.00821",
    "title": "Optimizing Sequential Experimental Design with Deep Reinforcement  Learning",
    "abstract": "Optimizing Sequential Experimental Design with Deep Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Tom Blau",
      "Edwin V. Bonilla",
      "Iadine Chades",
      "Amir Dezfouli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00821"
  },
  {
    "id": "arXiv:2202.01030",
    "title": "Too much information: why CDCL solvers need to forget learned clauses",
    "abstract": "Too much information: why CDCL solvers need to forget learned clauses",
    "descriptor": "",
    "authors": [
      "Tom Kr\u00fcger",
      "Jan-Hendrik Lorenz",
      "Florian W\u00f6rz"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.01030"
  },
  {
    "id": "arXiv:2202.01129",
    "title": "Structure-preserving GANs",
    "abstract": "Comments: 39 pages, 16 figures",
    "descriptor": "\nComments: 39 pages, 16 figures\n",
    "authors": [
      "Jeremiah Birrell",
      "Markos A. Katsoulakis",
      "Luc Rey-Bellet",
      "Wei Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.01129"
  },
  {
    "id": "arXiv:2202.03038",
    "title": "Deep Networks on Toroids: Removing Symmetries Reveals the Structure of  Flat Regions in the Landscape Geometry",
    "abstract": "Deep Networks on Toroids: Removing Symmetries Reveals the Structure of  Flat Regions in the Landscape Geometry",
    "descriptor": "",
    "authors": [
      "Fabrizio Pittorino",
      "Antonio Ferraro",
      "Gabriele Perugini",
      "Christoph Feinauer",
      "Carlo Baldassi",
      "Riccardo Zecchina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)"
    ],
    "url": "https://arxiv.org/abs/2202.03038"
  },
  {
    "id": "arXiv:2202.03077",
    "title": "Adversarial Attack and Defense for Non-Parametric Two-Sample Tests",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Xilie Xu",
      "Jingfeng Zhang",
      "Feng Liu",
      "Masashi Sugiyama",
      "Mohan Kankanhalli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2202.03077"
  },
  {
    "id": "arXiv:2202.03760",
    "title": "Modeling Structure with Undirected Neural Networks",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Tsvetomila Mihaylova",
      "Vlad Niculae",
      "Andr\u00e9 F. T. Martins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.03760"
  },
  {
    "id": "arXiv:2202.03926",
    "title": "Distribution Regression with Sliced Wasserstein Kernels",
    "abstract": "Distribution Regression with Sliced Wasserstein Kernels",
    "descriptor": "",
    "authors": [
      "Dimitri Meunier",
      "Massimiliano Pontil",
      "Carlo Ciliberto"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03926"
  },
  {
    "id": "arXiv:2202.04557",
    "title": "Universal Hopfield Networks: A General Framework for Single-Shot  Associative Memory Models",
    "abstract": "Comments: 09/02/22 initial upload; 17/06/222 camera ready version upload",
    "descriptor": "\nComments: 09/02/22 initial upload; 17/06/222 camera ready version upload\n",
    "authors": [
      "Beren Millidge",
      "Tommaso Salvatori",
      "Yuhang Song",
      "Thomas Lukasiewicz",
      "Rafal Bogacz"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04557"
  },
  {
    "id": "arXiv:2202.04713",
    "title": "PINs: Progressive Implicit Networks for Multi-Scale Neural  Representations",
    "abstract": "Comments: ICML 2022 (spotlight)",
    "descriptor": "\nComments: ICML 2022 (spotlight)\n",
    "authors": [
      "Zoe Landgraf",
      "Alexander Sorkine Hornung",
      "Ricardo Silveira Cabral"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.04713"
  },
  {
    "id": "arXiv:2202.05120",
    "title": "Low-Rank Approximation with $1/\u03b5^{1/3}$ Matrix-Vector Products",
    "abstract": "Comments: STOC '22",
    "descriptor": "\nComments: STOC '22\n",
    "authors": [
      "Ainesh Bakshi",
      "Kenneth L. Clarkson",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.05120"
  },
  {
    "id": "arXiv:2202.05628",
    "title": "Artemis: Articulated Neural Pets with Appearance and Motion synthesis",
    "abstract": "Comments: Accepted to ACM SIGGRAPH 2022 (Journal track)",
    "descriptor": "\nComments: Accepted to ACM SIGGRAPH 2022 (Journal track)\n",
    "authors": [
      "Haimin Luo",
      "Teng Xu",
      "Yuheng Jiang",
      "Chenglin Zhou",
      "Qiwei Qiu",
      "Yingliang Zhang",
      "Wei Yang",
      "Lan Xu",
      "Jingyi Yu"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.05628"
  },
  {
    "id": "arXiv:2202.05748",
    "title": "Borrowing from yourself: Faster future video segmentation with partial  channel update",
    "abstract": "Borrowing from yourself: Faster future video segmentation with partial  channel update",
    "descriptor": "",
    "authors": [
      "Evann Courdier",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.05748"
  },
  {
    "id": "arXiv:2202.05780",
    "title": "A Modern Self-Referential Weight Matrix That Learns to Modify Itself",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Kazuki Irie",
      "Imanol Schlag",
      "R\u00f3bert Csord\u00e1s",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05780"
  },
  {
    "id": "arXiv:2202.05798",
    "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time  Predictions to Training Patterns via Spotlights of Attention",
    "abstract": "Comments: Two first authors. Accepted to ICML 2022",
    "descriptor": "\nComments: Two first authors. Accepted to ICML 2022\n",
    "authors": [
      "Kazuki Irie",
      "R\u00f3bert Csord\u00e1s",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05798"
  },
  {
    "id": "arXiv:2202.06767",
    "title": "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training  Benchmark",
    "abstract": "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training  Benchmark",
    "descriptor": "",
    "authors": [
      "Jiaxi Gu",
      "Xiaojun Meng",
      "Guansong Lu",
      "Lu Hou",
      "Minzhe Niu",
      "Xiaodan Liang",
      "Lewei Yao",
      "Runhui Huang",
      "Wei Zhang",
      "Xin Jiang",
      "Chunjing Xu",
      "Hang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06767"
  },
  {
    "id": "arXiv:2202.07503",
    "title": "BED: A Real-Time Object Detection System for Edge Devices",
    "abstract": "BED: A Real-Time Object Detection System for Edge Devices",
    "descriptor": "",
    "authors": [
      "Guanchu Wang",
      "Zaid Pervaiz Bhat",
      "Zhimeng Jiang",
      "Yi-Wei Chen",
      "Daochen Zha",
      "Alfredo Costilla Reyes",
      "Afshin Niktash",
      "Gorkem Ulkar",
      "Erman Okman",
      "Xia Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.07503"
  },
  {
    "id": "arXiv:2202.09248",
    "title": "Stochastic Perturbations of Tabular Features for Non-Deterministic  Inference with Automunge",
    "abstract": "Comments: 42 pages, 23 figures",
    "descriptor": "\nComments: 42 pages, 23 figures\n",
    "authors": [
      "Nicholas J. Teague"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.09248"
  },
  {
    "id": "arXiv:2202.10969",
    "title": "A Framework for Distributed Quantum Queries in the CONGEST Model",
    "abstract": "A Framework for Distributed Quantum Queries in the CONGEST Model",
    "descriptor": "",
    "authors": [
      "Joran van Apeldoorn",
      "Tijn de Vos"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2202.10969"
  },
  {
    "id": "arXiv:2202.11474",
    "title": "Residual Bootstrap Exploration for Stochastic Linear Bandit",
    "abstract": "Comments: Accepted by UAI 2022",
    "descriptor": "\nComments: Accepted by UAI 2022\n",
    "authors": [
      "Shuang Wu",
      "Chi-Hua Wang",
      "Yuantong Li",
      "Guang Cheng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.11474"
  },
  {
    "id": "arXiv:2202.12932",
    "title": "Capturing Actionable Dynamics with Structured Latent Ordinary  Differential Equations",
    "abstract": "Comments: Accepted for the 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022). Github code can be found at this https URL",
    "descriptor": "\nComments: Accepted for the 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022). Github code can be found at this https URL\n",
    "authors": [
      "Paidamoyo Chapfuwa",
      "Sherri Rose",
      "Lawrence Carin",
      "Edward Meeds",
      "Ricardo Henao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12932"
  },
  {
    "id": "arXiv:2203.00161",
    "title": "On Testability of the Front-Door Model via Verma Constraints",
    "abstract": "Comments: 17 pages. In proceedings of the 38th Conference on Uncertainty in Artificial Intelligence",
    "descriptor": "\nComments: 17 pages. In proceedings of the 38th Conference on Uncertainty in Artificial Intelligence\n",
    "authors": [
      "Rohit Bhattacharya",
      "Razieh Nabi"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.00161"
  },
  {
    "id": "arXiv:2203.00756",
    "title": "Real time spectrogram inversion on mobile phone",
    "abstract": "Real time spectrogram inversion on mobile phone",
    "descriptor": "",
    "authors": [
      "Oleg Rybakov",
      "Marco Tagliasacchi",
      "Yunpeng Li",
      "Liyang Jiang",
      "Xia Zhang",
      "Fadi Biadsy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2203.00756"
  },
  {
    "id": "arXiv:2203.01884",
    "title": "Graph Neural Networks for Multimodal Single-Cell Data Integration",
    "abstract": "Comments: Accepted by KDD 2022 Applied Data Science Track",
    "descriptor": "\nComments: Accepted by KDD 2022 Applied Data Science Track\n",
    "authors": [
      "Hongzhi Wen",
      "Jiayuan Ding",
      "Wei Jin",
      "Yiqi Wang",
      "Yuying Xie",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.01884"
  },
  {
    "id": "arXiv:2203.01912",
    "title": "Bayesian Spillover Graphs for Dynamic Networks",
    "abstract": "Bayesian Spillover Graphs for Dynamic Networks",
    "descriptor": "",
    "authors": [
      "Grace Deng",
      "David S. Matteson"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.01912"
  },
  {
    "id": "arXiv:2203.03821",
    "title": "Coarse-to-Fine Vision Transformer",
    "abstract": "Coarse-to-Fine Vision Transformer",
    "descriptor": "",
    "authors": [
      "Mengzhao Chen",
      "Mingbao Lin",
      "Ke Li",
      "Yunhang Shen",
      "Yongjian Wu",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.03821"
  },
  {
    "id": "arXiv:2203.04923",
    "title": "On-Robot Learning With Equivariant Models",
    "abstract": "On-Robot Learning With Equivariant Models",
    "descriptor": "",
    "authors": [
      "Dian Wang",
      "Mingxi Jia",
      "Xupeng Zhu",
      "Robin Walters",
      "Robert Platt"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.04923"
  },
  {
    "id": "arXiv:2203.04928",
    "title": "DISCO: Comprehensive and Explainable Disinformation Detection",
    "abstract": "DISCO: Comprehensive and Explainable Disinformation Detection",
    "descriptor": "",
    "authors": [
      "Dongqi Fu",
      "Yikun Ban",
      "Hanghang Tong",
      "Ross Maciejewski",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2203.04928"
  },
  {
    "id": "arXiv:2203.05119",
    "title": "MetAug: Contrastive Learning via Meta Feature Augmentation",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Jiangmeng Li",
      "Wenwen Qiang",
      "Changwen Zheng",
      "Bing Su",
      "Hui Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05119"
  },
  {
    "id": "arXiv:2203.09251",
    "title": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs",
    "abstract": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs",
    "descriptor": "",
    "authors": [
      "Andrea Tirinzoni",
      "Aymen Al-Marjani",
      "Emilie Kaufmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.09251"
  },
  {
    "id": "arXiv:2203.10172",
    "title": "Importance Sampling Placement in Off-Policy Temporal-Difference Methods",
    "abstract": "Comments: 5 pages, 2 figures",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Eric Graves",
      "Sina Ghiassian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.10172"
  },
  {
    "id": "arXiv:2203.10726",
    "title": "TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation  with Transformers",
    "abstract": "TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation  with Transformers",
    "descriptor": "",
    "authors": [
      "Di Liu",
      "Yunhe Gao",
      "Qilong Zhangli",
      "Ligong Han",
      "Xiaoxiao He",
      "Zhaoyang Xia",
      "Song Wen",
      "Qi Chang",
      "Zhennan Yan",
      "Mu Zhou",
      "Dimitris Metaxas"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.10726"
  },
  {
    "id": "arXiv:2203.11973",
    "title": "Scalable Deep Reinforcement Learning Algorithms for Mean Field Games",
    "abstract": "Scalable Deep Reinforcement Learning Algorithms for Mean Field Games",
    "descriptor": "",
    "authors": [
      "Mathieu Lauri\u00e8re",
      "Sarah Perrin",
      "Sertan Girgin",
      "Paul Muller",
      "Ayush Jain",
      "Theophile Cabannes",
      "Georgios Piliouras",
      "Julien P\u00e9rolat",
      "Romuald \u00c9lie",
      "Olivier Pietquin",
      "Matthieu Geist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.11973"
  },
  {
    "id": "arXiv:2203.13880",
    "title": "Reinforcement Learning with Action-Free Pre-Training from Videos",
    "abstract": "Comments: International Conference on Machine Learning (ICML 2022). Project page: this https URL",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML 2022). Project page: this https URL\n",
    "authors": [
      "Younggyo Seo",
      "Kimin Lee",
      "Stephen James",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.13880"
  },
  {
    "id": "arXiv:2203.14402",
    "title": "UV Volumes for Real-time Rendering of Editable Free-view Human  Performance",
    "abstract": "Comments: Project Page: this https URL",
    "descriptor": "\nComments: Project Page: this https URL\n",
    "authors": [
      "Yue Chen",
      "Xuan Wang",
      "Xingyu Chen",
      "Qi Zhang",
      "Xiaoyu Li",
      "Yu Guo",
      "Jue Wang",
      "Fei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.14402"
  },
  {
    "id": "arXiv:2203.15163",
    "title": "CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal  Segmentation in MRI",
    "abstract": "CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal  Segmentation in MRI",
    "descriptor": "",
    "authors": [
      "Alex Ling Yu Hung",
      "Haoxin Zheng",
      "Qi Miao",
      "Steven S. Raman",
      "Demetri Terzopoulos",
      "Kyunghyun Sung"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.15163"
  },
  {
    "id": "arXiv:2203.15758",
    "title": "A Sparsity-promoting Dictionary Model for Variational Autoencoders",
    "abstract": "Comments: Proc. of Interspeech 2022",
    "descriptor": "\nComments: Proc. of Interspeech 2022\n",
    "authors": [
      "Mostafa Sadeghi",
      "Paul Magron"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.15758"
  },
  {
    "id": "arXiv:2203.16085",
    "title": "Combination of Time-domain, Frequency-domain, and Cepstral-domain  Acoustic Features for Speech Commands Classification",
    "abstract": "Comments: 5 pages, 4 figures",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Yikang Wang",
      "Hiromitsu Nishizaki"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16085"
  },
  {
    "id": "arXiv:2204.00147",
    "title": "Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth  Boxes",
    "abstract": "Comments: Accepted at IJCNN 2022",
    "descriptor": "\nComments: Accepted at IJCNN 2022\n",
    "authors": [
      "Akhil Meethal",
      "Marco Pedersoli",
      "Zhongwen Zhu",
      "Francisco Perdigon Romero",
      "Eric Granger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.00147"
  },
  {
    "id": "arXiv:2204.01365",
    "title": "Deep learning, stochastic gradient descent and diffusion maps",
    "abstract": "Deep learning, stochastic gradient descent and diffusion maps",
    "descriptor": "",
    "authors": [
      "Carmina Fjellstr\u00f6m",
      "Kaj Nystr\u00f6m"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.01365"
  },
  {
    "id": "arXiv:2204.02063",
    "title": "Verifiable Quantum Advantage without Structure",
    "abstract": "Comments: 50 pages, added a variant with worst-case completeness at the end of Section 6",
    "descriptor": "\nComments: 50 pages, added a variant with worst-case completeness at the end of Section 6\n",
    "authors": [
      "Takashi Yamakawa",
      "Mark Zhandry"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.02063"
  },
  {
    "id": "arXiv:2204.04500",
    "title": "Faster Min-Plus Product for Monotone Instances",
    "abstract": "Comments: Add acknowledgment",
    "descriptor": "\nComments: Add acknowledgment\n",
    "authors": [
      "Shucheng Chi",
      "Ran Duan",
      "Tianle Xie",
      "Tianyi Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2204.04500"
  },
  {
    "id": "arXiv:2204.05490",
    "title": "Modelling Evolutionary and Stationary User Preferences for Temporal Sets  Prediction",
    "abstract": "Comments: This paper is still in progress",
    "descriptor": "\nComments: This paper is still in progress\n",
    "authors": [
      "Le Yu",
      "Zihang Liu",
      "Tongyu Zhu",
      "Leilei Sun",
      "Bowen Du",
      "Weifeng Lv"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.05490"
  },
  {
    "id": "arXiv:2204.06507",
    "title": "Out-of-Distribution Detection with Deep Nearest Neighbors",
    "abstract": "Comments: 15 pages, 4 figures, accepted in ICML 2022",
    "descriptor": "\nComments: 15 pages, 4 figures, accepted in ICML 2022\n",
    "authors": [
      "Yiyou Sun",
      "Yifei Ming",
      "Xiaojin Zhu",
      "Yixuan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.06507"
  },
  {
    "id": "arXiv:2204.06653",
    "title": "Sketching Algorithms and Lower Bounds for Ridge Regression",
    "abstract": "Comments: To appear at ICML 2022",
    "descriptor": "\nComments: To appear at ICML 2022\n",
    "authors": [
      "Praneeth Kacham",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.06653"
  },
  {
    "id": "arXiv:2204.07566",
    "title": "Improving Frame-Online Neural Speech Enhancement with Overlapped-Frame  Prediction",
    "abstract": "Comments: in IEEE Signal Processing Letters",
    "descriptor": "\nComments: in IEEE Signal Processing Letters\n",
    "authors": [
      "Zhong-Qiu Wang",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.07566"
  },
  {
    "id": "arXiv:2204.08582",
    "title": "MASSIVE: A 1M-Example Multilingual Natural Language Understanding  Dataset with 51 Typologically-Diverse Languages",
    "abstract": "Comments: Preprint; 8 pages",
    "descriptor": "\nComments: Preprint; 8 pages\n",
    "authors": [
      "Jack FitzGerald",
      "Christopher Hench",
      "Charith Peris",
      "Scott Mackie",
      "Kay Rottmann",
      "Ana Sanchez",
      "Aaron Nash",
      "Liam Urbach",
      "Vishesh Kakarala",
      "Richa Singh",
      "Swetha Ranganath",
      "Laurie Crist",
      "Misha Britan",
      "Wouter Leeuwis",
      "Gokhan Tur",
      "Prem Natarajan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.08582"
  },
  {
    "id": "arXiv:2204.08775",
    "title": "Plots.jl -- a user extendable plotting API for the julia programming  language",
    "abstract": "Comments: 22 pages, 6 figures, 6 code listings",
    "descriptor": "\nComments: 22 pages, 6 figures, 6 code listings\n",
    "authors": [
      "Simon Christ",
      "Daniel Schwabeneder",
      "Christopher Rackauckas",
      "Michael Krabbe Borregaard",
      "Thomas Breloff"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2204.08775"
  },
  {
    "id": "arXiv:2204.09634",
    "title": "Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering",
    "abstract": "Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering",
    "descriptor": "",
    "authors": [
      "Samuel Lipping",
      "Parthasaarathy Sudarsanam",
      "Konstantinos Drossos",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.09634"
  },
  {
    "id": "arXiv:2204.10495",
    "title": "Adversarial Estimators",
    "abstract": "Adversarial Estimators",
    "descriptor": "",
    "authors": [
      "Jonas Metzger"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.10495"
  },
  {
    "id": "arXiv:2204.13661",
    "title": "Toward Compositional Generalization in Object-Oriented World Modeling",
    "abstract": "Comments: ICML 2022 Long Presentation. Website: this http URL",
    "descriptor": "\nComments: ICML 2022 Long Presentation. Website: this http URL\n",
    "authors": [
      "Linfeng Zhao",
      "Lingzhi Kong",
      "Robin Walters",
      "Lawson L.S. Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.13661"
  },
  {
    "id": "arXiv:2205.00165",
    "title": "NeuralEF: Deconstructing Kernels by Deep Neural Networks",
    "abstract": "Comments: International Conference on Machine Learning (ICML), 2022",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML), 2022\n",
    "authors": [
      "Zhijie Deng",
      "Jiaxin Shi",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.00165"
  },
  {
    "id": "arXiv:2205.00225",
    "title": "Recognising Known Configurations of Garments For Dual-Arm Robotic  Flattening",
    "abstract": "Comments: ICRCV 2022",
    "descriptor": "\nComments: ICRCV 2022\n",
    "authors": [
      "Li Duan",
      "Gerardo Argon-Camarasa"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00225"
  },
  {
    "id": "arXiv:2205.01573",
    "title": "StreamingHub: Interactive Stream Analysis Workflows",
    "abstract": "Comments: Code Repository at this https URL",
    "descriptor": "\nComments: Code Repository at this https URL\n",
    "authors": [
      "Yasith Jayawardana",
      "Vikas G. Ashok",
      "Sampath Jayarathna"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Digital Libraries (cs.DL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.01573"
  },
  {
    "id": "arXiv:2205.01647",
    "title": "Intelligent Trajectory Design for RIS-NOMA aided Multi-robot  Communications",
    "abstract": "Intelligent Trajectory Design for RIS-NOMA aided Multi-robot  Communications",
    "descriptor": "",
    "authors": [
      "Xinyu Gao",
      "Xidong Mu",
      "Wenqiang Yi",
      "Yuanwei Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.01647"
  },
  {
    "id": "arXiv:2205.01833",
    "title": "OpenAlex: A fully-open index of scholarly works, authors, venues,  institutions, and concepts",
    "abstract": "Comments: Submitted to the 26th International Conference on Science, Technology and Innovation Indicators (STI 2022)",
    "descriptor": "\nComments: Submitted to the 26th International Conference on Science, Technology and Innovation Indicators (STI 2022)\n",
    "authors": [
      "Jason Priem",
      "Heather Piwowar",
      "Richard Orr"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2205.01833"
  },
  {
    "id": "arXiv:2205.02719",
    "title": "Communication-Efficient Adaptive Federated Learning",
    "abstract": "Comments: Accepted by ICML 2022 (37 pages, 7 figures, 3 tables)",
    "descriptor": "\nComments: Accepted by ICML 2022 (37 pages, 7 figures, 3 tables)\n",
    "authors": [
      "Yujia Wang",
      "Lu Lin",
      "Jinghui Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.02719"
  },
  {
    "id": "arXiv:2205.03921",
    "title": "Online Algorithms with Multiple Predictions",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Keerti Anand",
      "Rong Ge",
      "Amit Kumar",
      "Debmalya Panigrahi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.03921"
  },
  {
    "id": "arXiv:2205.04401",
    "title": "Accelerating potential evaluation over unstructured meshes in two  dimensions",
    "abstract": "Comments: 47 pages, 9 tables, 13 figures",
    "descriptor": "\nComments: 47 pages, 9 tables, 13 figures\n",
    "authors": [
      "Zewen Shen",
      "Kirill Serkh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.04401"
  },
  {
    "id": "arXiv:2205.04525",
    "title": "Towards a multi-stakeholder value-based assessment framework for  algorithmic systems",
    "abstract": "Comments: Accepted at the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT'22)",
    "descriptor": "\nComments: Accepted at the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT'22)\n",
    "authors": [
      "Mireia Yurrita",
      "Dave Murray-Rust",
      "Agathe Balayn",
      "Alessandro Bozzon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.04525"
  },
  {
    "id": "arXiv:2205.06672",
    "title": "Local Attention Graph-based Transformer for Multi-target Genetic  Alteration Prediction",
    "abstract": "Local Attention Graph-based Transformer for Multi-target Genetic  Alteration Prediction",
    "descriptor": "",
    "authors": [
      "Daniel Reisenb\u00fcchler",
      "Sophia J. Wagner",
      "Melanie Boxberg",
      "Tingying Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06672"
  },
  {
    "id": "arXiv:2205.09930",
    "title": "BayesPCN: A Continually Learnable Predictive Coding Associative Memory",
    "abstract": "BayesPCN: A Continually Learnable Predictive Coding Associative Memory",
    "descriptor": "",
    "authors": [
      "Jason Yoo",
      "Frank Wood"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09930"
  },
  {
    "id": "arXiv:2205.11172",
    "title": "How Powerful are Spectral Graph Neural Networks",
    "abstract": "Comments: To be published in ICML2022",
    "descriptor": "\nComments: To be published in ICML2022\n",
    "authors": [
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11172"
  },
  {
    "id": "arXiv:2205.11269",
    "title": "Dynamic Split Computing for Efficient Deep Edge Intelligence",
    "abstract": "Comments: Accepted by the 2022 International Conference on Machine Learning (ICML 2022) DyNN Workshop",
    "descriptor": "\nComments: Accepted by the 2022 International Conference on Machine Learning (ICML 2022) DyNN Workshop\n",
    "authors": [
      "Arian Bakhtiarnia",
      "Nemanja Milo\u0161evi\u0107",
      "Qi Zhang",
      "Dragana Bajovi\u0107",
      "Alexandros Iosifidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11269"
  },
  {
    "id": "arXiv:2205.11814",
    "title": "Penalized Proximal Policy Optimization for Safe Reinforcement Learning",
    "abstract": "Comments: IJCAI2022",
    "descriptor": "\nComments: IJCAI2022\n",
    "authors": [
      "Linrui Zhang",
      "Li Shen",
      "Long Yang",
      "Shixiang Chen",
      "Bo Yuan",
      "Xueqian Wang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11814"
  },
  {
    "id": "arXiv:2205.12184",
    "title": "Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time  Reinforcement Learning",
    "abstract": "Comments: Proceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022",
    "descriptor": "\nComments: Proceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022\n",
    "authors": [
      "Harley Wiltzer",
      "David Meger",
      "Marc G. Bellemare"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12184"
  },
  {
    "id": "arXiv:2205.13344",
    "title": "A neural network based controller for underwater robotic vehicles",
    "abstract": "Comments: References added. This is a slightly updated version of the work presented at the COBEM 2011 - 21st Congress of Mechanical Engineering, 2011, Natal Brazil",
    "descriptor": "\nComments: References added. This is a slightly updated version of the work presented at the COBEM 2011 - 21st Congress of Mechanical Engineering, 2011, Natal Brazil\n",
    "authors": [
      "Josiane Maria Macedo Fernandes",
      "Marcelo Costa Tanaka",
      "Raimundo Carlos Silv\u00e9rio Freire J\u00fanior",
      "Wallace Moreira Bessa"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.13344"
  },
  {
    "id": "arXiv:2205.14108",
    "title": "Scalable Interpretability via Polynomials",
    "abstract": "Comments: 26 pages including appendix. v2 includes source code link at this https URL, and v3 fixes to baseline results in Table 1",
    "descriptor": "\nComments: 26 pages including appendix. v2 includes source code link at this https URL, and v3 fixes to baseline results in Table 1\n",
    "authors": [
      "Abhimanyu Dubey",
      "Filip Radenovic",
      "Dhruv Mahajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14108"
  },
  {
    "id": "arXiv:2205.14120",
    "title": "Neural Basis Models for Interpretability",
    "abstract": "Comments: 17 pages including appendix. v2 includes link to source code available at this https URL v3 includes updates to baseline results",
    "descriptor": "\nComments: 17 pages including appendix. v2 includes link to source code available at this https URL v3 includes updates to baseline results\n",
    "authors": [
      "Filip Radenovic",
      "Abhimanyu Dubey",
      "Dhruv Mahajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14120"
  },
  {
    "id": "arXiv:2206.00843",
    "title": "DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware  Efficiency of Compact Neural Networks",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Yonggan Fu",
      "Haichuan Yang",
      "Jiayi Yuan",
      "Meng Li",
      "Cheng Wan",
      "Raghuraman Krishnamoorthi",
      "Vikas Chandra",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00843"
  },
  {
    "id": "arXiv:2206.01266",
    "title": "Exponential Separations in Symmetric Neural Networks",
    "abstract": "Exponential Separations in Symmetric Neural Networks",
    "descriptor": "",
    "authors": [
      "Aaron Zweig",
      "Joan Bruna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01266"
  },
  {
    "id": "arXiv:2206.01366",
    "title": "Supernet Training for Federated Image Classification under System  Heterogeneity",
    "abstract": "Comments: Spotlight paper on ICML 22 Workshop: \"Dynamic Neural Networks\"; Under review for NeurIPS 22 Proceedings",
    "descriptor": "\nComments: Spotlight paper on ICML 22 Workshop: \"Dynamic Neural Networks\"; Under review for NeurIPS 22 Proceedings\n",
    "authors": [
      "Taehyeon Kim",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01366"
  },
  {
    "id": "arXiv:2206.02146",
    "title": "Recurrent Video Restoration Transformer with Guided Deformable Attention",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Jingyun Liang",
      "Yuchen Fan",
      "Xiaoyu Xiang",
      "Rakesh Ranjan",
      "Eddy Ilg",
      "Simon Green",
      "Jiezhang Cao",
      "Kai Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.02146"
  },
  {
    "id": "arXiv:2206.02241",
    "title": "Conceptual Design of the Memory System of the Robot Cognitive  Architecture ArmarX",
    "abstract": "Conceptual Design of the Memory System of the Robot Cognitive  Architecture ArmarX",
    "descriptor": "",
    "authors": [
      "Fabian Peller-Konrad",
      "Rainer Kartmann",
      "Christian R. G. Dreher",
      "Andre Meixner",
      "Fabian Reister",
      "Markus Grotz",
      "Tamim Asfour"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.02241"
  },
  {
    "id": "arXiv:2206.02262",
    "title": "Diffusion-GAN: Training GANs with Diffusion",
    "abstract": "Comments: Project homepage: this https URL",
    "descriptor": "\nComments: Project homepage: this https URL\n",
    "authors": [
      "Zhendong Wang",
      "Huangjie Zheng",
      "Pengcheng He",
      "Weizhu Chen",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.02262"
  },
  {
    "id": "arXiv:2206.03192",
    "title": "Generalized Data Distribution Iteration",
    "abstract": "Comments: This article is an extended work of arXiv:2112.04145 and arXiv:2106.06232. This article draws heavily from arXiv:2112.04145 and arXiv:2106.06232. We hope that adding some of the views from arXiv:2112.04145 into the appendix could greatly improve the readability",
    "descriptor": "\nComments: This article is an extended work of arXiv:2112.04145 and arXiv:2106.06232. This article draws heavily from arXiv:2112.04145 and arXiv:2106.06232. We hope that adding some of the views from arXiv:2112.04145 into the appendix could greatly improve the readability\n",
    "authors": [
      "Jiajun Fan",
      "Changnan Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.03192"
  },
  {
    "id": "arXiv:2206.03655",
    "title": "pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning",
    "abstract": "pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning",
    "descriptor": "",
    "authors": [
      "Daoyuan Chen",
      "Dawei Gao",
      "Weirui Kuang",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03655"
  },
  {
    "id": "arXiv:2206.03966",
    "title": "FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization",
    "abstract": "FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization",
    "descriptor": "",
    "authors": [
      "Zhen Wang",
      "Weirui Kuang",
      "Ce Zhang",
      "Bolin Ding",
      "Yaliang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03966"
  },
  {
    "id": "arXiv:2206.04459",
    "title": "SDQ: Stochastic Differentiable Quantization with Mixed Precision",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Xijie Huang",
      "Zhiqiang Shen",
      "Shichao Li",
      "Zechun Liu",
      "Xianghong Hu",
      "Jeffry Wicaksana",
      "Eric Xing",
      "Kwang-Ting Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04459"
  },
  {
    "id": "arXiv:2206.05085",
    "title": "Improved Direct Voxel Grid Optimization for Radiance Fields  Reconstruction",
    "abstract": "Comments: Project page this https URL ; Code this https URL ; Results updated",
    "descriptor": "\nComments: Project page this https URL ; Code this https URL ; Results updated\n",
    "authors": [
      "Cheng Sun",
      "Min Sun",
      "Hwann-Tzong Chen"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.05085"
  },
  {
    "id": "arXiv:2206.05330",
    "title": "The Gender Gap in Scholarly Self-Promotion on Social Media",
    "abstract": "The Gender Gap in Scholarly Self-Promotion on Social Media",
    "descriptor": "",
    "authors": [
      "Hao Peng",
      "Misha Teplitskiy",
      "Daniel M. Romero",
      "Em\u0151ke-\u00c1gnes Horv\u00e1t"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.05330"
  },
  {
    "id": "arXiv:2206.05706",
    "title": "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer",
    "abstract": "Comments: Accepted at NAACL 2022 (main conference)",
    "descriptor": "\nComments: Accepted at NAACL 2022 (main conference)\n",
    "authors": [
      "Rachit Bansal",
      "Milan Aggarwal",
      "Sumit Bhatia",
      "Jivat Neet Kaur",
      "Balaji Krishnamurthy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05706"
  },
  {
    "id": "arXiv:2206.05903",
    "title": "Geometrically Guided Integrated Gradients",
    "abstract": "Comments: 19 pages, 23 figures, funding sources added",
    "descriptor": "\nComments: 19 pages, 23 figures, funding sources added\n",
    "authors": [
      "Md Mahfuzur Rahman",
      "Noah Lewis",
      "Sergey Plis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05903"
  },
  {
    "id": "arXiv:2206.06531",
    "title": "A Stochastic Proximal Method for Nonsmooth Regularized Finite Sum  Optimization",
    "abstract": "A Stochastic Proximal Method for Nonsmooth Regularized Finite Sum  Optimization",
    "descriptor": "",
    "authors": [
      "Dounia Lakhmiri",
      "Dominique Orban",
      "Andrea Lodi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.06531"
  },
  {
    "id": "arXiv:2206.06563",
    "title": "Zeroth-Order Topological Insights into Iterative Magnitude Pruning",
    "abstract": "Comments: Presented (with proceedings) at ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning. 16 pages, 5 figures",
    "descriptor": "\nComments: Presented (with proceedings) at ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning. 16 pages, 5 figures\n",
    "authors": [
      "Aishwarya Balwani",
      "Jakob Krzyston"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2206.06563"
  },
  {
    "id": "arXiv:2206.06829",
    "title": "Efficient Decoder-free Object Detection with Transformers",
    "abstract": "Comments: Update metadata, 10 pages",
    "descriptor": "\nComments: Update metadata, 10 pages\n",
    "authors": [
      "Peixian Chen",
      "Mengdan Zhang",
      "Yunhang Shen",
      "Kekai Sheng",
      "Yuting Gao",
      "Xing Sun",
      "Ke Li",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06829"
  },
  {
    "id": "arXiv:2206.06847",
    "title": "On the Finite-Time Performance of the Knowledge Gradient Algorithm",
    "abstract": "Comments: require a further check of the proofs (Sections A.7, A.8) to ensure correctness",
    "descriptor": "\nComments: require a further check of the proofs (Sections A.7, A.8) to ensure correctness\n",
    "authors": [
      "Yanwen Li",
      "Siyang Gao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06847"
  },
  {
    "id": "arXiv:2206.07091",
    "title": "Monitoring Fog Computing: a Review, Taxonomy and Open Challenges",
    "abstract": "Monitoring Fog Computing: a Review, Taxonomy and Open Challenges",
    "descriptor": "",
    "authors": [
      "Breno Costa",
      "Joao Bachiega Jr",
      "Leonardo Reboucas de Carvalho",
      "Michel Rosa",
      "Aleteia Araujo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07091"
  },
  {
    "id": "arXiv:2206.07311",
    "title": "Can pruning improve certified robustness of neural networks?",
    "abstract": "Can pruning improve certified robustness of neural networks?",
    "descriptor": "",
    "authors": [
      "Zhangheng Li",
      "Tianlong Chen",
      "Linyi Li",
      "Bo Li",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07311"
  },
  {
    "id": "arXiv:2206.07527",
    "title": "QONNX: Representing Arbitrary-Precision Quantized Neural Networks",
    "abstract": "Comments: 9 pages, 5 figures, Contribution to 4th Workshop on Accelerated Machine Learning (AccML) at HiPEAC 2022 Conference",
    "descriptor": "\nComments: 9 pages, 5 figures, Contribution to 4th Workshop on Accelerated Machine Learning (AccML) at HiPEAC 2022 Conference\n",
    "authors": [
      "Alessandro Pappalardo",
      "Yaman Umuroglu",
      "Michaela Blott",
      "Jovan Mitrevski",
      "Ben Hawks",
      "Nhan Tran",
      "Vladimir Loncar",
      "Sioni Summers",
      "Hendrik Borras",
      "Jules Muhizi",
      "Matthew Trahms",
      "Shih-Chieh Hsu",
      "Scott Hauck",
      "Javier Duarte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Programming Languages (cs.PL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07527"
  },
  {
    "id": "arXiv:2206.07690",
    "title": "ELUDE: Generating interpretable explanations via a decomposition into  labelled and unlabelled features",
    "abstract": "ELUDE: Generating interpretable explanations via a decomposition into  labelled and unlabelled features",
    "descriptor": "",
    "authors": [
      "Vikram V. Ramaswamy",
      "Sunnie S. Y. Kim",
      "Nicole Meister",
      "Ruth Fong",
      "Olga Russakovsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07690"
  },
  {
    "id": "arXiv:2206.07695",
    "title": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
    "abstract": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
    "descriptor": "",
    "authors": [
      "Katja Schwarz",
      "Axel Sauer",
      "Michael Niemeyer",
      "Yiyi Liao",
      "Andreas Geiger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07695"
  },
  {
    "id": "arXiv:2206.07785",
    "title": "Participation and Data Valuation in IoT Data Markets through Distributed  Coalitions",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Shashi Raj Pandey",
      "Pierre Pinson",
      "Petar Popovski"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07785"
  },
  {
    "id": "arXiv:2206.07823",
    "title": "An Investigation of Kripke-style Modal Type Theories",
    "abstract": "An Investigation of Kripke-style Modal Type Theories",
    "descriptor": "",
    "authors": [
      "Jason Z. S. Hu",
      "Brigitte Pientka"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.07823"
  },
  {
    "id": "arXiv:2206.07826",
    "title": "Metric-Fair Classifier Derandomization",
    "abstract": "Comments: Proceedings of ICML 2022",
    "descriptor": "\nComments: Proceedings of ICML 2022\n",
    "authors": [
      "Jimmy Wu",
      "Yatong Chen",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07826"
  },
  {
    "id": "arXiv:2206.07842",
    "title": "Queried Unlabeled Data Improves and Robustifies Class-Incremental  Learning",
    "abstract": "Comments: Accepted by TMLR 2022",
    "descriptor": "\nComments: Accepted by TMLR 2022\n",
    "authors": [
      "Tianlong Chen",
      "Sijia Liu",
      "Shiyu Chang",
      "Lisa Amini",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07842"
  },
  {
    "id": "arXiv:2206.07981",
    "title": "Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment  Analysis in Videos",
    "abstract": "Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment  Analysis in Videos",
    "descriptor": "",
    "authors": [
      "Lianyang Ma",
      "Yu Yao",
      "Tao Liang",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07981"
  },
  {
    "id": "arXiv:2206.07990",
    "title": "Patch-level Representation Learning for Self-supervised Vision  Transformers",
    "abstract": "Comments: Accepted to CVPR 2022. Code is available at this https URL",
    "descriptor": "\nComments: Accepted to CVPR 2022. Code is available at this https URL\n",
    "authors": [
      "Sukmin Yun",
      "Hankook Lee",
      "Jaehyung Kim",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07990"
  },
  {
    "id": "arXiv:2206.08058",
    "title": "Nonwords Pronunciation Classification in Language Development Tests for  Preschool Children",
    "abstract": "Comments: Accepted at Interspeech 2022",
    "descriptor": "\nComments: Accepted at Interspeech 2022\n",
    "authors": [
      "Ilja Baumann",
      "Dominik Wagner",
      "Sebastian Bayerl",
      "Tobias Bocklet"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.08058"
  },
  {
    "id": "arXiv:2206.08075",
    "title": "Exploring Collaborative Game Play with Robots to Encourage Good Hand  Hygiene Practises among Children",
    "abstract": "Comments: 8 pages, 9 figures, 31st IEEE International Conference on Robot & Human Interactive Communication (RO-MAN 2022)",
    "descriptor": "\nComments: 8 pages, 9 figures, 31st IEEE International Conference on Robot & Human Interactive Communication (RO-MAN 2022)\n",
    "authors": [
      "Devasena Pasupuleti",
      "Sreejith Sasidharan",
      "Rajesh Sharma",
      "Gayathri Manikutty"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08075"
  },
  {
    "id": "arXiv:2206.08225",
    "title": "All the World's a (Hyper)Graph: A Data Drama",
    "abstract": "Comments: Landing page for code and data: this https URL",
    "descriptor": "\nComments: Landing page for code and data: this https URL\n",
    "authors": [
      "Corinna Coupette",
      "Jilles Vreeken",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.08225"
  }
]