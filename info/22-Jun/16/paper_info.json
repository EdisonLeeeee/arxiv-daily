[
  {
    "id": "arXiv:2206.07052",
    "title": "Sequential Optimization Numbers and Conjecture about Edge-Symmetry and  Weight-Symmetry Shortest Weight-Constrained Path",
    "abstract": "This paper defines multidimensional sequential optimization numbers and prove\nthat the unsigned Stirling numbers of first kind are 1-dimensional sequential\noptimization numbers. This paper gives a recurrence formula and an upper bound\nof multidimensional sequential optimization numbers. We proof that the\nk-dimensional sequential optimization numbers, denoted by O_k (n,m), are almost\nin {O_k (n,a)}, where a belong to[1,eklog(n-1)+(epi)^2/6(2^k-1)+M_1], n is the\nsize of k-dimensional sequential optimization numbers and M_1 is large positive\ninteger. Many achievements of the Stirling numbers of first kind can be\ntransformed into the properties of k-dimensional sequential optimization\nnumbers by k-dimensional extension and we give some examples. Shortest\nweight-constrained path is NP-complete problem [1]. In the case of edge\nsymmetry and weight symmetry, we use the definition of the optimization set to\ndesign 2-dimensional Bellman-Ford algorithm to solve it. According to the fact\nthat P_1 (n,m>M) less than or equal to e^(-M_1 ), where M=elog(n-1)+e+M_1, M_1\nis a positive integer and P_1 (n,m) is the probability of 1-dimensional\nsequential optimization numbers, this paper conjecture that the probability of\nsolving edge-symmetry and weight-symmetry shortest weight-constrained path\nproblem in polynomial time approaches 1 exponentially with the increase of\nconstant term in algorithm complexity. The results of a large number of\nsimulation experiments agree with this conjecture.",
    "descriptor": "",
    "authors": [
      "Zile Hui"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07052"
  },
  {
    "id": "arXiv:2206.07074",
    "title": "$C^0$-hybrid high-order methods for biharmonic problems",
    "abstract": "We devise and analyze $C^0$-conforming hybrid high-order (HHO) methods to\napproximate biharmonic problems with either clamped or simply supported\nboundary conditions. $C^0$-conforming HHO methods hinge on cell unknowns which\nare $C^0$-conforming polynomials of order $(k+2)$ approximating the solution in\nthe mesh cells and on face unknowns which are polynomials of order $k\\ge0$\napproximating the normal derivative of the solution on the mesh skeleton. Such\nmethods deliver $O(h^{k+1})$ $H^2$-error estimates for smooth solutions. An\nimportant novelty in the error analysis is to lower the minimal regularity\nrequirement on the exact solution. The technique to achieve this has broader\napplicability than just $C^0$-conforming HHO methods, and to illustrate this\npoint, we outline the error analysis for the well-known $C^0$-conforming\ninterior penalty discontinuous Galerkin (IPDG) methods as well. The present\ntechnique does not require bubble functions or a $C^1$-smoother to evaluate the\nright-hand side in case of rough loads. Finally, numerical results including\ncomparisons to various existing methods showcase the efficiency of the proposed\n$C^0$-conforming HHO methods.",
    "descriptor": "",
    "authors": [
      "Zhaonan Dong",
      "Alexandre Ern"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07074"
  },
  {
    "id": "arXiv:2206.07077",
    "title": "Comparison of Different Configurations of Saturated Core Fault Current  Limiters in a Power Grid by Numerical Method",
    "abstract": "Short circuit fault currents are increasing due to growing demand for\nelectricity and high complexity in power systems. Because the fault currents\nreach the highest value which the breakers are unable to restrict, the\nelectrical grid security is under jeopardy. By entering a limiting impedance\ninto a transmission line in series, these impedances restrict the rising\namounts of fault currents to levels that are acceptable. Saturated core fault\ncurrent limiters (SCFCLs) are a pivotal tool for limiting fault currents rise\nin power networks that have good performance characteristics. In a normal\ncondition, these limiters have slight effects on the system and can effectively\nlimit short circuit currents when occur. In this chapter, various structures of\nSCFCLs with different arrangements of ac windings & dc windings are presented\nand the currents passing through the FCLs under the normal and faulty system\nconditions are assessed and compared. The flux density in various regions of\nthe core in different arrangements has been investigated as well and the\ndesired analyzes have been performed. Simulation will be presented based on\nCOMSOL Multiphysics 5.4, a finite element software package which can provide a\nprecious assessment to compare these protective devices with different\nconfigurations.",
    "descriptor": "\nComments: 31 pages, 37 figures, This manuscript will appear as a book chapter in a new book: Modernization of Electric Power Systems, Springer\n",
    "authors": [
      "Aydin Zaboli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07077"
  },
  {
    "id": "arXiv:2206.07080",
    "title": "Measuring Inconsistency in Declarative Process Specifications",
    "abstract": "We address the problem of measuring inconsistency in declarative process\nspecifications, with an emphasis on linear temporal logic on fixed traces\n(LTLff). As we will show, existing inconsistency measures for classical logic\ncannot provide a meaningful assessment of inconsistency in LTL in general, as\nthey cannot adequately handle the temporal operators. We therefore propose a\nnovel paraconsistent semantics as a framework for inconsistency measurement. We\nthen present two new inconsistency measures based on these semantics and show\nthat they satisfy important desirable properties. We show how these measures\ncan be applied to declarative process models and investigate the computational\ncomplexity of the introduced approach.",
    "descriptor": "",
    "authors": [
      "Carl Corea",
      "John Grant",
      "Matthias Thimm"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07080"
  },
  {
    "id": "arXiv:2206.07081",
    "title": "Applications of Generative Adversarial Networks in Neuroimaging and  Clinical Neuroscience",
    "abstract": "Generative adversarial networks (GANs) are one powerful type of deep learning\nmodels that have been successfully utilized in numerous fields. They belong to\na broader family called generative methods, which generate new data with a\nprobabilistic model by learning sample distribution from real examples. In the\nclinical context, GANs have shown enhanced capabilities in capturing spatially\ncomplex, nonlinear, and potentially subtle disease effects compared to\ntraditional generative methods. This review appraises the existing literature\non the applications of GANs in imaging studies of various neurological\nconditions, including Alzheimer's disease, brain tumors, brain aging, and\nmultiple sclerosis. We provide an intuitive explanation of various GAN methods\nfor each application and further discuss the main challenges, open questions,\nand promising future directions of leveraging GANs in neuroimaging. We aim to\nbridge the gap between advanced deep learning methods and neurology research by\nhighlighting how GANs can be leveraged to support clinical decision making and\ncontribute to a better understanding of the structural and functional patterns\nof brain diseases.",
    "descriptor": "",
    "authors": [
      "Rongguang Wang",
      "Vishnu Bashyam",
      "Zhijian Yang",
      "Fanyang Yu",
      "Vasiliki Tassopoulou",
      "Lasya P. Sreepada",
      "Sai Spandana Chintapalli",
      "Dushyant Sahoo",
      "Ioanna Skampardoni",
      "Konstantina Nikita",
      "Ahmed Abdulkadir",
      "Junhao Wen",
      "Christos Davatzikos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07081"
  },
  {
    "id": "arXiv:2206.07082",
    "title": "Stability and Generalization of Stochastic Optimization with Nonconvex  and Nonsmooth Problems",
    "abstract": "Stochastic optimization has found wide applications in minimizing objective\nfunctions in machine learning, which motivates a lot of theoretical studies to\nunderstand its practical success. Most of existing studies focus on the\nconvergence of optimization errors, while the generalization analysis of\nstochastic optimization is much lagging behind. This is especially the case for\nnonconvex and nonsmooth problems often encountered in practice. In this paper,\nwe initialize a systematic stability and generalization analysis of stochastic\noptimization on nonconvex and nonsmooth problems. We introduce novel\nalgorithmic stability measures and establish their quantitative connection on\nthe gap between population gradients and empirical gradients, which is then\nfurther extended to study the gap between the Moreau envelope of the empirical\nrisk and that of the population risk. To our knowledge, these quantitative\nconnection between stability and generalization in terms of either gradients or\nMoreau envelopes have not been studied in the literature. We introduce a class\nof sampling-determined algorithms, for which we develop bounds for three\nstability measures. Finally, we apply these discussions to derive error bounds\nfor stochastic gradient descent and its adaptive variant, where we show how to\nachieve an implicit regularization by tuning the step sizes and the number of\niterations.",
    "descriptor": "",
    "authors": [
      "Yunwen Lei"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07082"
  },
  {
    "id": "arXiv:2206.07084",
    "title": "An Efficient HTN to STRIPS Encoding for Concurrent Plans",
    "abstract": "The Hierarchical Task Network (HTN) formalism is used to express a wide\nvariety of planning problems in terms of decompositions of tasks into subtaks.\nMany techniques have been proposed to solve such hierarchical planning\nproblems. A particular technique is to encode hierarchical planning problems as\nclassical STRIPS planning problems. One advantage of this technique is to\nbenefit directly from the constant improvements made by STRIPS planners.\nHowever, there are still few effective and expressive encodings. In this paper,\nwe present a new HTN to STRIPS encoding allowing to generate concurrent plans.\nWe show experimentally that this encoding outperforms previous approaches on\nhierarchical IPC benchmarks.",
    "descriptor": "",
    "authors": [
      "N. Cavrel",
      "D. Pellier",
      "H. Fiorino"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07084"
  },
  {
    "id": "arXiv:2206.07085",
    "title": "Understanding the Generalization Benefit of Normalization Layers:  Sharpness Reduction",
    "abstract": "Normalization layers (e.g., Batch Normalization, Layer Normalization) were\nintroduced to help with optimization difficulties in very deep nets, but they\nclearly also help generalization, even in not-so-deep nets. Motivated by the\nlong-held belief that flatter minima lead to better generalization, this paper\ngives mathematical analysis and supporting experiments suggesting that\nnormalization (together with accompanying weight-decay) encourages GD to reduce\nthe sharpness of loss surface. Here \"sharpness\" is carefully defined given that\nthe loss is scale-invariant, a known consequence of normalization.\nSpecifically, for a fairly broad class of neural nets with normalization, our\ntheory explains how GD with a finite learning rate enters the so-called Edge of\nStability (EoS) regime, and characterizes the trajectory of GD in this regime\nvia a continuous sharpness-reduction flow.",
    "descriptor": "\nComments: 68 pages, many figures\n",
    "authors": [
      "Kaifeng Lyu",
      "Zhiyuan Li",
      "Sanjeev Arora"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07085"
  },
  {
    "id": "arXiv:2206.07086",
    "title": "Synthesizing Mathematical Identities with E-Graphs",
    "abstract": "Identities compactly describe properties of a mathematical expression and can\nbe leveraged into faster and more accurate function implementations. However,\nidentities must currently be discovered manually, which requires a lot of\nexpertise. We propose a two-phase synthesis and deduplication pipeline that\ndiscovers these identities automatically. In the synthesis step, a set of\nrewrite rules is composed, using an e-graph, to discover candidate identities.\nHowever, most of these candidates are duplicates, which a secondary\ndeduplication step discards using integer linear programming and another\ne-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215\ncandidate identities which the deduplication phase then reduces down to 125\ncore identities.",
    "descriptor": "",
    "authors": [
      "Ian Briggs",
      "Pavel Panchekha"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.07086"
  },
  {
    "id": "arXiv:2206.07087",
    "title": "Combining Counterfactuals With Shapley Values To Explain Image Models",
    "abstract": "With the widespread use of sophisticated machine learning models in sensitive\napplications, understanding their decision-making has become an essential task.\nModels trained on tabular data have witnessed significant progress in\nexplanations of their underlying decision making processes by virtue of having\na small number of discrete features. However, applying these methods to\nhigh-dimensional inputs such as images is not a trivial task. Images are\ncomposed of pixels at an atomic level and do not carry any interpretability by\nthemselves. In this work, we seek to use annotated high-level interpretable\nfeatures of images to provide explanations. We leverage the Shapley value\nframework from Game Theory, which has garnered wide acceptance in general XAI\nproblems. By developing a pipeline to generate counterfactuals and subsequently\nusing it to estimate Shapley values, we obtain contrastive and interpretable\nexplanations with strong axiomatic guarantees.",
    "descriptor": "",
    "authors": [
      "Aditya Lahiri",
      "Kamran Alipour",
      "Ehsan Adeli",
      "Babak Salimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07087"
  },
  {
    "id": "arXiv:2206.07088",
    "title": "About MathPartner web service",
    "abstract": "The report is devoted to the current state of the MathPartner computer\nalgebra web project. We discuss the main directions of development of the\nproject and give several examples of using it to solve selected problems.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Gennadi Malaschonok",
      "Ivan Borisov"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.07088"
  },
  {
    "id": "arXiv:2206.07089",
    "title": "A Collaboration Strategy in the Mining Pool for  Proof-of-Neural-Architecture Consensus",
    "abstract": "In most popular public accessible cryptocurrency systems, the mining pool\nplays a key role because mining cryptocurrency with the mining pool turns the\nnon-profitable situation into profitable for individual miners. In many recent\nnovel blockchain consensuses, the deep learning training procedure becomes the\ntask for miners to prove their workload, thus the computation power of miners\nwill not purely be spent on the hash puzzle. In this way, the hardware and\nenergy will support the blockchain service and deep learning training\nsimultaneously. While the incentive of miners is to earn tokens, individual\nminers are motivated to join mining pools to become more competitive. In this\npaper, we are the first to demonstrate a mining pool solution for novel\nconsensuses based on deep learning.\nThe mining pool manager partitions the full searching space into subspaces\nand all miners are scheduled to collaborate on the Neural Architecture Search\n(NAS) tasks in the assigned subspace. Experiments demonstrate that the\nperformance of this type of mining pool is more competitive than an individual\nminer. Due to the uncertainty of miners' behaviors, the mining pool manager\nchecks the standard deviation of the performance of high reward miners and\nprepares backup miners to ensure the completion of the tasks of high reward\nminers.",
    "descriptor": "",
    "authors": [
      "Boyang Li",
      "Qing Lu",
      "Weiwen Jiang",
      "Taeho Jung",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07089"
  },
  {
    "id": "arXiv:2206.07090",
    "title": "Parallelization of Machine Learning Algorithms Respectively on Single  Machine and Spark",
    "abstract": "With the rapid development of big data technologies, how to dig out useful\ninformation from massive data becomes an essential problem. However, using\nmachine learning algorithms to analyze large data may be time-consuming and\ninefficient on the traditional single machine. To solve these problems, this\npaper has made some research on the parallelization of several classic machine\nlearning algorithms respectively on the single machine and the big data\nplatform Spark. We compare the runtime and efficiency of traditional machine\nlearning algorithms with parallelized machine learning algorithms respectively\non the single machine and Spark platform. The research results have shown\nsignificant improvement in runtime and efficiency of parallelized machine\nlearning algorithms.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Jiajun Shen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07090"
  },
  {
    "id": "arXiv:2206.07091",
    "title": "Monitoring Fog Computing: a Review, Taxonomy and Open Challenges",
    "abstract": "Fog computing is a distributed paradigm that provides computational resources\nin the users' vicinity. Fog orchestration is a set of functionalities that\ncoordinate the dynamic infrastructure and manage the services to guarantee the\nService Level Agreements. Monitoring is an orchestration functionality of prime\nimportance. It is the basis for resource management actions, collecting status\nof resource and service and delivering updated data to the orchestrator. There\nare several cloud monitoring solutions and tools, but none of them comply with\nfog characteristics and challenges. Fog specific monitoring solutions are rare,\nand they may not be prepared to compose an orchestration service. This paper\ndescribes fog orchestration challenges and analyses fog monitoring requirements\nin this context, creating a taxonomy of fog monitoring solutions by surveying\nthe literature. Finally, fog monitoring proposals are detailed and categorized\nby this new taxonomy.",
    "descriptor": "",
    "authors": [
      "Breno Costa",
      "Joao Bachiega Jr",
      "Leonardo Reboucas de Carvalho",
      "Michel Rosa",
      "Aleteia P. F. Araujo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07091"
  },
  {
    "id": "arXiv:2206.07092",
    "title": "Optimization Heuristics for Cost-Efficient Long-Term Cloud Portfolio  Allocations Under Uncertainty",
    "abstract": "Today's cloud infrastructure landscape offers a broad range of services to\nbuild and operate software applications. The myriad of options, however, has\nalso brought along a new layer of complexity. When it comes to procuring cloud\ncomputing resources, consumers can purchase their virtual machines from\ndifferent providers on different marketspaces to form so called cloud\nportfolios: a bundle of virtual machines whereby the virtual machines have\ndifferent technical characteristics and pricing mechanisms. Thus, selecting the\nright server instances for a given set of applications such that the\nallocations are cost efficient is a non-trivial task. In this paper we propose\na formal specification of the cloud portfolio management problem that takes an\napplication-driven approach and incorporates the nuances of the commonly\nencountered reserved, on-demand and spot market types. We present two distinct\ncost optimization heuristics for this stochastic temporal bin packing problem,\none taking a naive first fit strategy, while the other is built on the concepts\nof genetic algorithms. The results of the evaluation show that the former\noptimization approach significantly outperforms the latter, both in terms of\nexecution speeds and solution quality.",
    "descriptor": "\nComments: 10 pages, 11 figures, conference submission\n",
    "authors": [
      "Maximilian Kiessler",
      "Valentin Haag",
      "Benedikt Pittl",
      "Erich Schikuta"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.07092"
  },
  {
    "id": "arXiv:2206.07093",
    "title": "Helm -- What It Can Do and Where Is It Going?",
    "abstract": "Deploying an application into a Kubernetes cluster requires sending a\nmanifest file to the cluster's control plane interface. This action is\ntypically performed through a kubectl client which is configured and authorized\nto communicate with the control plane's Uniform Resource Locator (URL). An\napplication typically requires many Kubernetes resources such as pods,\ndeployments, secrets, service and volumes. Configuring each of these through\nmanifest files requires complex scripting, especially when there are numerous\nresources needed.\nA solution to the complex management tasks is Helm. Helm provides both a tool\nand underlying framework that packages the necessary manifest files. These\npackages are deployed through a single step install command which abstracts all\nthe underlying control plane interaction from the user. Similar to application\ninstalls through Debian's package manager dpkg, packages are shared through\nlocal and remote repositories and allow the user to easily install, update,\ndelete or handle concurrent versions.",
    "descriptor": "",
    "authors": [
      "Michael Howard"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07093"
  },
  {
    "id": "arXiv:2206.07094",
    "title": "Cloud Computing -- Everything As A Service",
    "abstract": "Compute infrastructure hosted by a cloud provider allows an application to\nscale without limit. The application developer no longer needs to worry about\nthe up-front investment in a server farm provisioned for a worst-case load\nscenario. However, managing cloud deployments requires a sophisticated\nframework that can autoscale the infrastructure and guarantee the up-time of\nrunning container images. This paper surveys existing research addressing the\nmanagement and orchestration of cloud deployments as well as the modelling\nframework to abstract away the low-level details of the host infrastructure. We\ninvestigate blockchain distributed ledgers, quantum computing and Internet of\nThings application stacks to show how they can utilize cloud deployments.",
    "descriptor": "",
    "authors": [
      "Michael Howard"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.07094"
  },
  {
    "id": "arXiv:2206.07095",
    "title": "Elementary remarks about Pisano periods",
    "abstract": "In this short note, we reprove in a very elementary way some known facts\nabout Pisano periods as well as some considerations about the link between\nPisano periods and the order of roots of the characteristic equation. The\ntechnics only requires a small background in ring theory (merely the definition\nof a commutative ring). The tools set here can be reused for all linear\nrecurrences with quadratic non-constant characteristic equation.",
    "descriptor": "",
    "authors": [
      "G\u00e9rard Duchamp",
      "Pierre Simonnet"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.07095"
  },
  {
    "id": "arXiv:2206.07098",
    "title": "Plurality Veto: A Simple Voting Rule Achieving Optimal Metric Distortion",
    "abstract": "The metric distortion framework posits that n voters and m candidates are\njointly embedded in a metric space such that voters rank candidates that are\ncloser to them higher. A voting rule's purpose is to pick a candidate with\nminimum total distance to the voters, given only the rankings, but not the\nactual distances. As a result, in the worst case, each deterministic rule picks\na candidate whose total distance is at least three times larger than that of an\noptimal one, i.e., has distortion at least 3. A recent breakthrough result\nshowed that achieving this bound of 3 is possible; however, the proof is\nnon-constructive, and the voting rule itself is a complicated exhaustive\nsearch.\nOur main result is an extremely simple voting rule, called Plurality Veto,\nwhich achieves the same optimal distortion of 3. Each candidate starts with a\nscore equal to his number of first-place votes. These scores are then gradually\ndecreased via an n-round veto process in which a candidate drops out when his\nscore reaches zero. One after the other, voters decrement the score of their\nbottom choice among the standing candidates, and the last standing candidate\nwins. We give a one-paragraph proof that this voting rule achieves distortion\n3. This rule is also immensely practical, and it only makes two queries to each\nvoter, so it has low communication overhead.\nWe also generalize Plurality Veto into a class of randomized voting rules in\nthe following way: Plurality veto is run only for k < n rounds; then, a\ncandidate is chosen with probability proportional to his residual score. This\ngeneral rule interpolates between Random Dictatorship (for k=0) and Plurality\nVeto (for k=n-1), and k controls the variance of the output. We show that for\nall k, this rule has distortion at most 3.",
    "descriptor": "",
    "authors": [
      "Fatih Erdem Kizilkaya",
      "David Kempe"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07098"
  },
  {
    "id": "arXiv:2206.07099",
    "title": "Resource-Mediated Consensus Formation",
    "abstract": "In social sciences, simulating opinion dynamics to study the interplay\nbetween homophily and influence, and the subsequent formation of echo chambers,\nis of great importance. As such, in this paper we investigate echo chambers by\nimplementing a unique social game in which we spawn in a large number of\nagents, each assigned one of the two opinions on an issue and a finite amount\nof influence in the form of a game currency. Agents attempt to have an opinion\nthat is a majority at the end of the game, to obtain a reward also paid in the\ngame currency. At the beginning of each round, a randomly selected agent is\nselected, referred to as a speaker. The second agent is selected in the radius\nof speaker influence (which is a set subset of the speaker's neighbors) to\ninteract with the speaker as a listener. In this interaction, the speaker\nproposes a payoff in the game currency from their personal influence budget to\npersuade the listener to hold the speaker's opinion in future rounds until\nchosen listener again. The listener can either choose to accept or reject this\npayoff to hold the speaker's opinion for future rounds. The listener's choice\nis informed only by their estimate of global majority opinion through a limited\nview of the opinions of their neighboring agents. We show that the influence\ngame leads to the formation of \"echo chambers,\" or homogeneous clusters of\nopinions. We also investigate various scenarios to disrupt the creation of such\necho chambers, including the introduction of resource disparity between agents\nwith different opinions, initially preferentially assigning opinions to agents,\nand the introduction of committed agents, who never change their initial\nopinion.",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Omar Malik",
      "James Flamino",
      "Boleslaw K. Szymanski"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07099"
  },
  {
    "id": "arXiv:2206.07102",
    "title": "Generalized Nash Equilibrium Models for Asymmetric, Non-cooperative  Games on Line Graphs: Application to Water Resource Systems",
    "abstract": "This paper investigates the game theory of resource-allocation situations\nwhere the \"first come, first serve\" heuristic creates inequitable, asymmetric\nbenefits to the players. Specifically, this problem is formulated as a\nGeneralized Nash Equilibrium Model where the players are arranged sequentially\nalong a directed line graph. The goal of the model is to reduce the asymmetric\nbenefits among the players using a policy instrument. It serves as a more\nrealistic, alternative approach to the line-graph models considered in the\ncooperative game-theoretic literature. An application-oriented formulation is\nalso developed for water resource systems. The players in this model are\nutilities who withdraw water and are arranged along a river basin from upstream\nto downstream. This model is applied to a stylized, three-node model as well as\na test bed in the Duck River Basin in Tennessee, USA. Based on the results, a\nnon-cooperative, water-release market can be an acceptable policy instrument\naccording to metrics traditionally used in cooperative game theory",
    "descriptor": "",
    "authors": [
      "Nathan Boyd",
      "Steven Gabriel",
      "George Rest",
      "Tom Dumm"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07102"
  },
  {
    "id": "arXiv:2206.07104",
    "title": "Most, And Least, Compact Spanning Trees of a Graph",
    "abstract": "We introduce the concept of Most, and Least, Compact Spanning Trees --\ndenoted respectively by $T^*(G)$ and $T^\\#(G)$ -- of a simple, connected,\nundirected and unweighted graph $G(V, E, W)$. For a spanning tree $T(G) \\in\n\\mathcal{T}(G)$ to be considered $T^*(G)$, where $\\mathcal{T}(G)$ represents\nthe set of all the spanning trees of the graph $G$, it must have the least sum\nof inter-vertex pair shortest path distances from amongst the members of the\nset $\\mathcal{T}(G)$. Similarly, for it to be considered $T^\\#(G)$, it must\nhave the highest sum of inter-vertex pair shortest path distances. In this\nwork, we present an iteratively greedy rank-and-regress method that produces at\nleast one $T^*(G)$ or $T^\\#(G)$ by eliminating one extremal edge per\niteration.The rank function for performing the elimination is based on the\nelements of the matrix of relative forest accessibilities of a graph and the\nrelated forest distance. We provide empirical evidence in support of our\nmethodology using some standard graph families; and discuss potentials for\ncomputational efficiencies, along with relevant trade-offs, to enable the\nextraction of $T^*(G)$ and $T^\\#(G)$ within reasonable time limits on standard\nplatforms.",
    "descriptor": "",
    "authors": [
      "Gyan Ranjan",
      "Nishant Saurabh",
      "Amit Ashutosh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.07104"
  },
  {
    "id": "arXiv:2206.07106",
    "title": "NewsEdits: A News Article Revision Dataset and a Document-Level  Reasoning Challenge",
    "abstract": "News article revision histories provide clues to narrative and factual\nevolution in news articles. To facilitate analysis of this evolution, we\npresent the first publicly available dataset of news revision histories,\nNewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million\narticles with 4.6 million versions from over 22 English- and French-language\nnewspaper sources based in three countries, spanning 15 years of coverage\n(2006-2021).\nWe define article-level edit actions: Addition, Deletion, Edit and Refactor,\nand develop a high-accuracy extraction algorithm to identify these actions. To\nunderscore the factual nature of many edit actions, we conduct analyses showing\nthat added and deleted sentences are more likely to contain updating events,\nmain content and quotes than unchanged sentences.\nFinally, to explore whether edit actions are predictable, we introduce three\nnovel tasks aimed at predicting actions performed during version updates. We\nshow that these tasks are possible for expert humans but are challenging for\nlarge NLP models. We hope this can spur research in narrative framing and help\nprovide predictive tools for journalists chasing breaking news.",
    "descriptor": "",
    "authors": [
      "Alexander Spangher",
      "Xiang Ren",
      "Jonathan May",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07106"
  },
  {
    "id": "arXiv:2206.07115",
    "title": "If it Bleeds, it Leads: A Computational Approach to Covering Crime in  Los Angeles",
    "abstract": "Developing and improving computational approaches to covering news can\nincrease journalistic output and improve the way stories are covered. In this\nwork we approach the problem of covering crime stories in Los Angeles. We\npresent a machine-in-the-loop system that covers individual crimes by (1)\nlearning the prototypical coverage archetypes from classical news articles on\ncrime to learn their structure and (2) using output from the Los Angeles Police\ndepartment to generate \"lede paragraphs\", first structural unit of\ncrime-articles. We introduce a probabilistic graphical model for learning\narticle structure and a rule-based system for generating ledes. We hope our\nwork can lead to systems that use these components together to form the\nskeletons of news articles covering crime.\nThis work was done for a class project in Jonathan May's Advanced Natural\nLanguage Processing Course, Fall, 2019.",
    "descriptor": "",
    "authors": [
      "Alexander Spangher",
      "Divya Choudhary"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07115"
  },
  {
    "id": "arXiv:2206.07116",
    "title": "A Partially Synchronizing Coloring",
    "abstract": "Given a finite directed graph, a coloring of its edges turns the graph into a\nfinite-state automaton. A k-synchronizing word of a deterministic automaton is\na word in the alphabet of colors at its edges that maps the state set of the\nautomaton at least on k-element subset. A coloring of edges of a directed\nstrongly connected finite graph of a uniform outdegree (constant outdegree of\nany vertex) is k-synchronizing if the coloring turns the graph into a\ndeterministic finite automaton possessing a k-synchronizing word.\nFor k=1 one has the well known road coloring problem. The recent positive\nsolution of the road coloring problem implies an elegant generalization\nconsidered first by Beal and Perrin: a directed finite strongly connected graph\nof uniform outdegree is k-synchronizing iff the greatest common divisor of\nlengths of all its cycles is k.\nSome consequences for coloring of an arbitrary finite digraph are presented.\nWe describe a subquadratic algorithm of the road coloring for the\nk-synchronization implemented in the package TESTAS. A new linear visualization\nprogram demonstrates the obtained coloring. Some consequences for coloring of\nan arbitrary finite digraph and of such a graph of uniform outdegree are\npresented.",
    "descriptor": "\nComments: 9 pages, 2 figures Lecture Notes in Computer Science, 6072(2010), 363-370. arXiv admin note: text overlap with arXiv:0801.2838, arXiv:0709.0099\n",
    "authors": [
      "A.N. Trahtman"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.07116"
  },
  {
    "id": "arXiv:2206.07117",
    "title": "TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation",
    "abstract": "3D hand pose estimation methods have made significant progress recently.\nHowever, estimation accuracy is often far from sufficient for specific\nreal-world applications, and thus there is significant room for improvement.\nThis paper proposes TriHorn-Net, a novel model that uses specific innovations\nto improve hand pose estimation accuracy on depth images. The first innovation\nis the decomposition of the 3D hand pose estimation into the estimation of 2D\njoint locations in the depth image space (UV), and the estimation of their\ncorresponding depths aided by two complementary attention maps. This\ndecomposition prevents depth estimation, which is a more difficult task, from\ninterfering with the UV estimations at both the prediction and feature levels.\nThe second innovation is PixDropout, which is, to the best of our knowledge,\nthe first appearance-based data augmentation method for hand depth images.\nExperimental results demonstrate that the proposed model outperforms the\nstate-of-the-art methods on three public benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Mohammad Rezaei",
      "Razieh Rastgoo",
      "Vassilis Athitsos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07117"
  },
  {
    "id": "arXiv:2206.07125",
    "title": "Self-Supervised Pretraining for Differentially Private Learning",
    "abstract": "We demonstrate self-supervised pretraining (SSP) is a scalable solution to\ndeep learning with differential privacy (DP) regardless of the size of\navailable public datasets in image classification. When facing the lack of\npublic datasets, we show the features generated by SSP on only one single image\nenable a private classifier to obtain much better utility than the non-learned\nhandcrafted features under the same privacy budget. When a moderate or large\nsize public dataset is available, the features produced by SSP greatly\noutperform the features trained with labels on various complex private datasets\nunder the same private budget. We also compared multiple DP-enabled training\nframeworks to train a private classifier on the features generated by SSP.\nFinally, we report a non-trivial utility 25.3\\% of a private ImageNet-1K\ndataset when $\\epsilon=3$.",
    "descriptor": "",
    "authors": [
      "Arash Asadian",
      "Evan Weidner",
      "Lei Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07125"
  },
  {
    "id": "arXiv:2206.07126",
    "title": "Lazy Queries Can Reduce Variance in Zeroth-order Optimization",
    "abstract": "A major challenge of applying zeroth-order (ZO) methods is the high query\ncomplexity, especially when queries are costly. We propose a novel gradient\nestimation technique for ZO methods based on adaptive lazy queries that we term\nas LAZO. Different from the classic one-point or two-point gradient estimation\nmethods, LAZO develops two alternative ways to check the usefulness of old\nqueries from previous iterations, and then adaptively reuses them to construct\nthe low-variance gradient estimates. We rigorously establish that through\njudiciously reusing the old queries, LAZO can reduce the variance of stochastic\ngradient estimates so that it not only saves queries per iteration but also\nachieves the regret bound for the symmetric two-point method. We evaluate the\nnumerical performance of LAZO, and demonstrate the low-variance property and\nthe performance gain of LAZO in both regret and query complexity relative to\nseveral existing ZO methods. The idea of LAZO is general, and can be applied to\nother variants of ZO methods.",
    "descriptor": "",
    "authors": [
      "Quan Xiao",
      "Qing Ling",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07126"
  },
  {
    "id": "arXiv:2206.07129",
    "title": "Potentials and Limits of Using Preconfigured Spatial Beams as Bandwidth  Resources: Beam Selection vs Beam Aggregation",
    "abstract": "This letter studies how to use spatial beams pre-configured in a legacy\nspatial division multiple access (SDMA) network as bandwidth resources via the\nimplementation of non-orthogonal multiple access (NOMA). Two different beam\nmanagement schemes, namely beam selection and beam aggregation, are developed\nto improve the overall system throughput without consuming extra spectrum or\nchanging the performance of the legacy network. Analytical and simulation\nresults are presented to show that the two schemes realize different tradeoffs\nbetween system performance and complexity.",
    "descriptor": "",
    "authors": [
      "Zhiguo Ding"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07129"
  },
  {
    "id": "arXiv:2206.07136",
    "title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and  Stronger",
    "abstract": "Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping norm $R$, however, is shown to be vital for achieving high\naccuracy under DP. We propose an easy-to-use replacement, called AutoClipping,\nthat eliminates the need to tune $R$ for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, which shows that it enjoys an asymptotic\nconvergence rate that matches the standard SGD. We also demonstrate on various\nlanguage and vision tasks that automatic clipping outperforms or matches the\nstate-of-the-art, and can be easily employed with minimal changes to existing\ncodebases.",
    "descriptor": "",
    "authors": [
      "Zhiqi Bu",
      "Yu-Xiang Wang",
      "Sheng Zha",
      "George Karypis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07136"
  },
  {
    "id": "arXiv:2206.07137",
    "title": "Prioritized Training on Points that are Learnable, Worth Learning, and  Not Yet Learnt",
    "abstract": "Training on web-scale data can take months. But much computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.",
    "descriptor": "\nComments: ICML 2022. follow up to workshop version arXiv:2107.02565\n",
    "authors": [
      "S\u00f6ren Mindermann",
      "Jan Brauner",
      "Muhammed Razzak",
      "Mrinank Sharma",
      "Andreas Kirsch",
      "Winnie Xu",
      "Benedikt H\u00f6ltgen",
      "Aidan N. Gomez",
      "Adrien Morisot",
      "Sebastian Farquhar",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07137"
  },
  {
    "id": "arXiv:2206.07139",
    "title": "MBGDT:Robust Mini-Batch Gradient Descent",
    "abstract": "In high dimensions, most machine learning method perform fragile even there\nare a little outliers. To address this, we hope to introduce a new method with\nthe base learner, such as Bayesian regression or stochastic gradient descent to\nsolve the problem of the vulnerability in the model. Because the mini-batch\ngradient descent allows for a more robust convergence than the batch gradient\ndescent, we work a method with the mini-batch gradient descent, called\nMini-Batch Gradient Descent with Trimming (MBGDT). Our method show state-of-art\nperformance and have greater robustness than several baselines when we apply\nour method in designed dataset.",
    "descriptor": "",
    "authors": [
      "Hanming Wang",
      "Haozheng Luo",
      "Yue Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07139"
  },
  {
    "id": "arXiv:2206.07144",
    "title": "Flatten the Curve: Efficiently Training Low-Curvature Neural Networks",
    "abstract": "The highly non-linear nature of deep neural networks causes them to be\nsusceptible to adversarial examples and have unstable gradients which hinders\ninterpretability. However, existing methods to solve these issues, such as\nadversarial training, are expensive and often sacrifice predictive accuracy.\nIn this work, we consider curvature, which is a mathematical quantity which\nencodes the degree of non-linearity. Using this, we demonstrate low-curvature\nneural networks (LCNNs) that obtain drastically lower curvature than standard\nmodels while exhibiting similar predictive performance, which leads to improved\nrobustness and stable gradients, with only a marginally increased training\ntime. To achieve this, we minimize a data-independent upper bound on the\ncurvature of a neural network, which decomposes overall curvature in terms of\ncurvatures and slopes of its constituent layers. To efficiently minimize this\nbound, we introduce two novel architectural components: first, a non-linearity\ncalled centered-softplus that is a stable variant of the softplus\nnon-linearity, and second, a Lipschitz-constrained batch normalization layer.\nOur experiments show that LCNNs have lower curvature, more stable gradients\nand increased off-the-shelf adversarial robustness when compared to their\nstandard high-curvature counterparts, all without affecting predictive\nperformance. Our approach is easy to use and can be readily incorporated into\nexisting neural network models.",
    "descriptor": "",
    "authors": [
      "Suraj Srinivas",
      "Kyle Matoba",
      "Himabindu Lakkaraju",
      "Francois Fleuret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07144"
  },
  {
    "id": "arXiv:2206.07146",
    "title": "Learning Hands-On Electronics from Home: A Simulator for Fritzing",
    "abstract": "The recent pandemic has forced us to teach online, which is especially\ndifficult for hand-on courses in robotics, like basic electronics. In this\npaper, I present a simulator which tries to replicate the same experience\nstudents will encounter during the exercises in the laboratory. The simulator\nhas been developed in Fritzing, uses realistic multimeters for measuring and\nchecks for common mistakes. Results show not only that the simulator was\nextremely useful during the pandemic, but also that it can supplement\nlaboratory exercises when teaching in-classroom.",
    "descriptor": "\nComments: 12 pages, 4 figures, to be published in 25th International Conference Series on Climbing and Walking Robots (CLAWAR 2022)\n",
    "authors": [
      "Andres Fai\u00f1a"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07146"
  },
  {
    "id": "arXiv:2206.07148",
    "title": "It's Time for Artistic Correspondence in Music and Video",
    "abstract": "We present an approach for recommending a music track for a given video, and\nvice versa, based on both their temporal alignment and their correspondence at\nan artistic level. We propose a self-supervised approach that learns this\ncorrespondence directly from data, without any need of human annotations. In\norder to capture the high-level concepts that are required to solve the task,\nwe propose modeling the long-term temporal context of both the video and the\nmusic signals, using Transformer networks for each modality. Experiments show\nthat this approach strongly outperforms alternatives that do not exploit the\ntemporal context. The combination of our contributions improve retrieval\naccuracy up to 10x over prior state of the art. This strong improvement allows\nus to introduce a wide range of analyses and applications. For instance, we can\ncondition music retrieval based on visually defined attributes.",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Didac Suris",
      "Carl Vondrick",
      "Bryan Russell",
      "Justin Salamon"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07148"
  },
  {
    "id": "arXiv:2206.07150",
    "title": "Attacks on Perception-Based Control Systems: Modeling and Fundamental  Limits",
    "abstract": "In this work, we study performance of perception-based control systems in the\npresence of attacks. We focus on a wide class of stochastic nonlinear control\nsystems, and provide methods for modeling and analysis of their resiliency to\nstealthy attacks on both physical and perception-based sensing. Specifically,\nwe consider a general setup with a nonlinear affine physical plant controlled\nwith a perception-based controller that maps both the physical sensor (e.g.,\nIMUs) and perceptual (e.g., camera) measurements to the control input; in\naddition, the system is equipped with a statistical or learning-based anomaly\ndetector (AD) to detect the presence of abnormal behaviours in the system. To\nenable general performance analysis, we model the attacks on perception and\nphysical sensing in the most general form. Further, we introduce the notions of\nattack effectiveness and stealthiness that are independent of the employed AD;\ni.e., the attack remaining stealthy even from the best existing detectors. In\nsuch setting, we consider attacks with different levels of runtime knowledge\nabout the plant and its states. We find sufficient conditions for existence of\nstealthy effective attacks that force the plant state into an unsafe region\nwithout being detected by any employed AD. We show that as the open-loop\nunstable plant dynamics diverges faster and the closed-loop system converges\nfaster to an equilibrium point, the system will be more vulnerable to effective\nstealthy attacks. Specifically, we show that depending on runtime information\navailable to the attacker, the probability of attack remaining stealthy\n(against any AD) can be arbitrarily close to one, if the attackers estimate of\nthe plant state is arbitrarily close to the true plant state.",
    "descriptor": "",
    "authors": [
      "Amir Khazraei",
      "Henry Pfister",
      "Miroslav Pajic"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07150"
  },
  {
    "id": "arXiv:2206.07152",
    "title": "An Intelligent Assistant for Converting City Requirements to Formal  Specification",
    "abstract": "As more and more monitoring systems have been deployed to smart cities, there\ncomes a higher demand for converting new human-specified requirements to\nmachine-understandable formal specifications automatically. However, these\nhuman-specific requirements are often written in English and bring missing,\ninaccurate, or ambiguous information. In this paper, we present CitySpec, an\nintelligent assistant system for requirement specification in smart cities.\nCitySpec not only helps overcome the language differences brought by English\nrequirements and formal specifications, but also offers solutions to those\nmissing, inaccurate, or ambiguous information. The goal of this paper is to\ndemonstrate how CitySpec works. Specifically, we present three demos: (1)\ninteractive completion of requirements in CitySpec; (2) human-in-the-loop\ncorrection while CitySepc encounters exceptions; (3) online learning in\nCitySpec.",
    "descriptor": "\nComments: This demo paper is accepted by SMARTCOMP 2022\n",
    "authors": [
      "Zirong Chen",
      "Isaac Li",
      "Haoxiang Zhang",
      "Sarah Preum",
      "John Stankovic",
      "Meiyi Ma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07152"
  },
  {
    "id": "arXiv:2206.07155",
    "title": "Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut  Features",
    "abstract": "Deep learning models trained in a fully supervised manner have been shown to\nrely on so-called \"shortcut\" features. Shortcut features are inputs that are\nassociated with the outcome of interest in the training data, but are either no\nlonger associated or not present in testing or deployment settings. Here we\nprovide experiments that show recent self-supervised models trained on images\nand text provide more robust image representations and reduce the model's\nreliance on visual shortcut features on a realistic medical imaging example.\nAdditionally, we find that these self-supervised models \"forget\" shortcut\nfeatures more quickly than fully supervised ones when fine-tuned on labeled\ndata. Though not a complete solution, our experiments provide compelling\nevidence that self-supervised models trained on images and text provide some\nresilience to visual shortcut features.",
    "descriptor": "\nComments: 4 pages, 2 figures, accepted at SCIS workshop, ICML 2022\n",
    "authors": [
      "Anil Palepu",
      "Andrew L Beam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07155"
  },
  {
    "id": "arXiv:2206.07160",
    "title": "LAVENDER: Unifying Video-Language Understanding as Masked Language  Modeling",
    "abstract": "Unified vision-language frameworks have greatly advanced in recent years,\nmost of which adopt an encoder-decoder architecture to unify image-text tasks\nas sequence-to-sequence generation. However, existing video-language (VidL)\nmodels still require task-specific designs in model architecture and training\nobjectives for each task. In this work, we explore a unified VidL framework\nLAVENDER, where Masked Language Modeling (MLM) is used as the common interface\nfor all pre-training and downstream tasks. Such unification leads to a\nsimplified model architecture, where only a lightweight MLM head, instead of a\ndecoder with much more parameters, is needed on top of the multimodal encoder.\nSurprisingly, experimental results show that this unified framework achieves\ncompetitive performance on 14 VidL benchmarks, covering video question\nanswering, text-to-video retrieval and video captioning. Extensive analyses\nfurther demonstrate the advantage of LAVENDER over existing VidL methods in:\n(i) supporting all downstream tasks with just a single set of parameter values\nwhen multi-task finetuned; (ii) few-shot generalization on various downstream\ntasks; and (iii) enabling zero-shot evaluation on video question answering\ntasks. Code is available at https://github.com/microsoft/LAVENDER.",
    "descriptor": "",
    "authors": [
      "Linjie Li",
      "Zhe Gan",
      "Kevin Lin",
      "Chung-Ching Lin",
      "Zicheng Liu",
      "Ce Liu",
      "Lijuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07160"
  },
  {
    "id": "arXiv:2206.07161",
    "title": "GraphFM: Improving Large-Scale GNN Training via Feature Momentum",
    "abstract": "Training of graph neural networks (GNNs) for large-scale node classification\nis challenging. A key difficulty lies in obtaining accurate hidden node\nrepresentations while avoiding the neighborhood explosion problem. Here, we\npropose a new technique, named as feature momentum (FM), that uses a momentum\nstep to incorporate historical embeddings when updating feature\nrepresentations. We develop two specific algorithms, known as GraphFM-IB and\nGraphFM-OB, that consider in-batch and out-of-batch data, respectively.\nGraphFM-IB applies FM to in-batch sampled data, while GraphFM-OB applies FM to\nout-of-batch data that are 1-hop neighborhood of in-batch data. We provide a\nrigorous convergence analysis for GraphFM-IB and theoretical insight of\nGraphFM-OB for the estimation error of feature embeddings. Empirically, we\nobserve that GraphFM-IB can effectively alleviate the neighborhood explosion\nproblem of existing methods. In addition, GraphFM-OB achieves promising\nperformance on multiple large-scale graph datasets.",
    "descriptor": "",
    "authors": [
      "Haiyang Yu",
      "Limei Wang",
      "Bokun Wang",
      "Meng Liu",
      "Tianbao Yang",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07161"
  },
  {
    "id": "arXiv:2206.07162",
    "title": "Category-Agnostic 6D Pose Estimation with Conditional Neural Processes",
    "abstract": "We present a novel meta-learning approach for 6D pose estimation on unknown\nobjects. In contrast to \"instance-level\" pose estimation methods, our algorithm\nlearns object representation in a category-agnostic way, which endows it with\nstrong generalization capabilities within and across object categories.\nSpecifically, we employ a conditional neural process-based meta-learning\napproach to train an encoder to capture texture and geometry of an object in a\nlatent representation, based on very few RGB-D images and ground-truth\nkeypoints. The latent representation is then used by a simultaneously\nmeta-trained decoder to predict the 6D pose of the object in new images. To\nevaluate our algorithm, experiments are conducted on our new fully-annotated\nsynthetic datasets generated from Multiple Categories in Multiple Scenes\n(MCMS). Experimental results demonstrate that our model performs well on unseen\nobjects with various shapes and appearances.",
    "descriptor": "\nComments: Accepted at CVPR2022 workshop: Women in Computer Vision (WiCV)\n",
    "authors": [
      "Yumeng Li",
      "Ning Gao",
      "Hanna Ziesche",
      "Gerhard Neumann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07162"
  },
  {
    "id": "arXiv:2206.07163",
    "title": "DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction  via A Structure-Specific Generative Method",
    "abstract": "Joint 2D cardiac segmentation and 3D volume reconstruction are fundamental to\nbuilding statistical cardiac anatomy models and understanding functional\nmechanisms from motion patterns. However, due to the low through-plane\nresolution of cine MR and high inter-subject variance, accurately segmenting\ncardiac images and reconstructing the 3D volume are challenging. In this study,\nwe propose an end-to-end latent-space-based framework, DeepRecon, that\ngenerates multiple clinically essential outcomes, including accurate image\nsegmentation, synthetic high-resolution 3D image, and 3D reconstructed volume.\nOur method identifies the optimal latent representation of the cine image that\ncontains accurate semantic information for cardiac structures. In particular,\nour model jointly generates synthetic images with accurate semantic information\nand segmentation of the cardiac structures using the optimal latent\nrepresentation. We further explore downstream applications of 3D shape\nreconstruction and 4D motion pattern adaptation by the different latent-space\nmanipulation strategies.The simultaneously generated high-resolution images\npresent a high interpretable value to assess the cardiac shape and\nmotion.Experimental results demonstrate the effectiveness of our approach on\nmultiple fronts including 2D segmentation, 3D reconstruction, downstream 4D\nmotion pattern adaption performance.",
    "descriptor": "\nComments: MICCAI2022\n",
    "authors": [
      "Qi Chang",
      "Zhennan Yan",
      "Mu Zhou",
      "Di Liu",
      "Khalid Sawalha",
      "Meng Ye",
      "Qilong Zhangli",
      "Mikael Kanski",
      "Subhi Al Aref",
      "Leon Axel",
      "Dimitris Metaxas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07163"
  },
  {
    "id": "arXiv:2206.07164",
    "title": "Edge Security: Challenges and Issues",
    "abstract": "Edge computing is a paradigm that shifts data processing services to the\nnetwork edge, where data are generated. While such an architecture provides\nfaster processing and response, among other benefits, it also raises critical\nsecurity issues and challenges that must be addressed. This paper discusses the\nsecurity threats and vulnerabilities emerging from the edge network\narchitecture spanning from the hardware layer to the system layer. We further\ndiscuss privacy and regulatory compliance challenges in such networks. Finally,\nwe argue the need for a holistic approach to analyze edge network security\nposture, which must consider knowledge from each layer.",
    "descriptor": "\nComments: 21 pages. Survey paper\n",
    "authors": [
      "Xin Jin",
      "Charalampos Katsis",
      "Fan Sang",
      "Jiahao Sun",
      "Ashish Kundu",
      "Ramana Kompella"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07164"
  },
  {
    "id": "arXiv:2206.07166",
    "title": "Regularizing a Model-based Policy Stationary Distribution to Stabilize  Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning (RL) extends the paradigm of classical RL\nalgorithms to purely learning from static datasets, without interacting with\nthe underlying environment during the learning process. A key challenge of\noffline RL is the instability of policy training, caused by the mismatch\nbetween the distribution of the offline data and the undiscounted stationary\nstate-action distribution of the learned policy. To avoid the detrimental\nimpact of distribution mismatch, we regularize the undiscounted stationary\ndistribution of the current policy towards the offline data during the policy\noptimization process. Further, we train a dynamics model to both implement this\nregularization and better estimate the stationary distribution of the current\npolicy, reducing the error induced by distribution mismatch. On a wide range of\ncontinuous-control offline RL datasets, our method indicates competitive\nperformance, which validates our algorithm. The code is publicly available.",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Shentao Yang",
      "Yihao Feng",
      "Shujian Zhang",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07166"
  },
  {
    "id": "arXiv:2206.07167",
    "title": "Understanding Narratives through Dimensions of Analogy",
    "abstract": "Analogical reasoning is a powerful qualitative reasoning tool that enables\nhumans to connect two situations, and to generalize their knowledge from\nfamiliar to novel situations. Cognitive Science research provides valuable\ninsights into the richness and complexity of analogical reasoning, together\nwith implementations of expressive analogical reasoners with limited\nscalability. Modern scalable AI techniques with the potential to reason by\nanalogy have been only applied to the special case of proportional analogy, and\nnot to understanding higher-order analogies. In this paper, we aim to bridge\nthe gap by: 1) formalizing six dimensions of analogy based on mature insights\nfrom Cognitive Science research, 2) annotating a corpus of fables with each of\nthese dimensions, and 3) defining four tasks with increasing complexity that\nenable scalable evaluation of AI techniques. Experiments with language models\nand neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art\nmethods can be applied to reason by analogy with a limited success, motivating\nthe need for further research towards comprehensive and scalable analogical\nreasoning by AI. We make all our code and data available.",
    "descriptor": "",
    "authors": [
      "Thiloshon Nagarajah",
      "Filip Ilievski",
      "Jay Pujara"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07167"
  },
  {
    "id": "arXiv:2206.07170",
    "title": "Towards Goal, Feasibility, and Diversity-Oriented Deep Generative Models  in Design",
    "abstract": "Deep Generative Machine Learning Models (DGMs) have been growing in\npopularity across the design community thanks to their ability to learn and\nmimic complex data distributions. DGMs are conventionally trained to minimize\nstatistical divergence between the distribution over generated data and\ndistribution over the dataset on which they are trained. While sufficient for\nthe task of generating \"realistic\" fake data, this objective is typically\ninsufficient for design synthesis tasks. Instead, design problems typically\ncall for adherence to design requirements, such as performance targets and\nconstraints. Advancing DGMs in engineering design requires new training\nobjectives which promote engineering design objectives. In this paper, we\npresent the first Deep Generative Model that simultaneously optimizes for\nperformance, feasibility, diversity, and target achievement. We benchmark\nperformance of the proposed method against several Deep Generative Models over\neight evaluation metrics that focus on feasibility, diversity, and satisfaction\nof design performance targets. Methods are tested on a challenging\nmulti-objective bicycle frame design problem with skewed, multimodal data of\ndifferent datatypes. The proposed framework was found to outperform all Deep\nGenerative Models in six of eight metrics.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.03005\n",
    "authors": [
      "Lyle Regenwetter",
      "Faez Ahmed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07170"
  },
  {
    "id": "arXiv:2206.07171",
    "title": "Automated image analysis in large-scale cellular electron microscopy: A  literature survey",
    "abstract": "Large-scale electron microscopy (EM) datasets generated using (semi-)\nautomated microscopes are becoming the standard in EM. Given the vast amounts\nof data, manual analysis of all data is not feasible, thus automated analysis\nis crucial. The main challenges in automated analysis include the annotation\nthat is needed to analyse and interpret biomedical images, coupled with\nachieving high-throughput. Here, we review the current state-of-the-art of\nautomated computer techniques and major challenges for the analysis of\nstructures in cellular EM. The advanced computer vision, deep learning and\nsoftware tools that have been developed in the last five years for automatic\nbiomedical image analysis are discussed with respect to annotation,\nsegmentation and scalability for EM data. Integration of automatic image\nacquisition and analysis will allow for high-throughput analysis of\nmillimeter-range datasets with nanometer resolution.",
    "descriptor": "",
    "authors": [
      "Anusha Aswatha",
      "Ahmad Alsahaf",
      "Ben N. G. Giepmans",
      "George Azzopardi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07171"
  },
  {
    "id": "arXiv:2206.07172",
    "title": "Parameterized Complexity Results for Bayesian Inference",
    "abstract": "We present completeness results for inference in Bayesian networks with\nrespect to two different parameterizations, namely the number of variables and\nthe topological vertex separation number. For this we introduce the\nparameterized complexity classes $\\mathsf{W[1]PP}$ and $\\mathsf{XLPP}$, which\nrelate to $\\mathsf{W[1]}$ and $\\mathsf{XNLP}$ respectively as $\\mathsf{PP}$\ndoes to $\\mathsf{NP}$. The second parameter is intended as a natural\ntranslation of the notion of pathwidth to the case of directed acyclic graphs,\nand as such it is a stronger parameter than the more commonly considered\ntreewidth. Based on a recent conjecture, the completeness results for this\nparameter suggest that deterministic algorithms for inference require\nexponential space in terms of pathwidth and by extension treewidth. These\nresults are intended to contribute towards a more precise understanding of the\nparameterized complexity of Bayesian inference and thus of its required\ncomputational resources in terms of both time and space.",
    "descriptor": "\nComments: 12 pages; submitted to PGM2022 (this https URL)\n",
    "authors": [
      "Hans Bodlaender",
      "Nils Donselaar",
      "Johan Kwisthout"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.07172"
  },
  {
    "id": "arXiv:2206.07173",
    "title": "Measuring Representational Harms in Image Captioning",
    "abstract": "Previous work has largely considered the fairness of image captioning systems\nthrough the underspecified lens of \"bias.\" In contrast, we present a set of\ntechniques for measuring five types of representational harms, as well as the\nresulting measurements obtained for two of the most popular image captioning\ndatasets using a state-of-the-art image captioning system. Our goal was not to\naudit this image captioning system, but rather to develop normatively grounded\nmeasurement techniques, in turn providing an opportunity to reflect on the many\nchallenges involved. We propose multiple measurement techniques for each type\nof harm. We argue that by doing so, we are better able to capture the\nmulti-faceted nature of each type of harm, in turn improving the (collective)\nvalidity of the resulting measurements. Throughout, we discuss the assumptions\nunderlying our measurement approach and point out when they do not hold.",
    "descriptor": "\nComments: ACM Conference on Fairness, Accountability, and Transparency (FAccT) 2022\n",
    "authors": [
      "Angelina Wang",
      "Solon Barocas",
      "Kristen Laird",
      "Hanna Wallach"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07173"
  },
  {
    "id": "arXiv:2206.07176",
    "title": "Frequency-centroid features for word recognition of non-native English  speakers",
    "abstract": "The objective of this work is to investigate complementary features which can\naid the quintessential Mel frequency cepstral coefficients (MFCCs) in the task\nof closed, limited set word recognition for non-native English speakers of\ndifferent mother-tongues. Unlike the MFCCs, which are derived from the spectral\nenergy of the speech signal, the proposed frequency-centroids (FCs) encapsulate\nthe spectral centres of the different bands of the speech spectrum, with the\nbands defined by the Mel filterbank. These features, in combination with the\nMFCCs, are observed to provide relative performance improvement in English word\nrecognition, particularly under varied noisy conditions. A two-stage\nConvolution Neural Network (CNN) is used to model the features of the English\nwords uttered with Arabic, French and Spanish accents.",
    "descriptor": "\nComments: Published in IEEE Irish Signals & Systems Conference (ISSC), 2022\n",
    "authors": [
      "Pierre Berjon",
      "Rajib Sharma",
      "Avishek Nag",
      "Soumyabrata Dev"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07176"
  },
  {
    "id": "arXiv:2206.07179",
    "title": "Proximal Splitting Adversarial Attacks for Semantic Segmentation",
    "abstract": "Classification has been the focal point of research on adversarial attacks,\nbut only a few works investigate methods suited to denser prediction tasks,\nsuch as semantic segmentation. The methods proposed in these works do not\naccurately solve the adversarial segmentation problem and, therefore, are\noveroptimistic in terms of size of the perturbations required to fool models.\nHere, we propose a white-box attack for these models based on a proximal\nsplitting to produce adversarial perturbations with much smaller $\\ell_1$,\n$\\ell_2$, or $\\ell_\\infty$ norms. Our attack can handle large numbers of\nconstraints within a nonconvex minimization framework via an Augmented\nLagrangian approach, coupled with adaptive constraint scaling and masking\nstrategies. We demonstrate that our attack significantly outperforms previously\nproposed ones, as well as classification attacks that we adapted for\nsegmentation, providing a first comprehensive benchmark for this dense task.\nOur results push current limits concerning robustness evaluations in\nsegmentation tasks.",
    "descriptor": "\nComments: Code available at: this https URL\n",
    "authors": [
      "J\u00e9r\u00f4me Rony",
      "Jean-Christophe Pesquet",
      "Ismail Ben Ayed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07179"
  },
  {
    "id": "arXiv:2206.07180",
    "title": "Integrating deduction and model finding in a language independent  setting",
    "abstract": "Software artifacts are ubiquitous in our lives being an essential part of\nhome appliances, cars, cel phones, and even in more critical activities like\naeronautics and health sciences. In this context software failures may produce\nenormous losses, either economical or, in the extreme, in human lives. Software\nanalysis is an area in software engineering concerned on the application of\ndifferent techniques in order to prove the (relative) absence of errors in\nsoftware artifacts. In many cases these methods of analysis are applied by\nfollowing certain methodological directives that ensure better results. In a\nprevious work we presented the notion of satisfiability calculus as a model\ntheoretical counterpart of Meseguer's proof calculus, providing a formal\nfoundation for a variety of tools that are based on model construction. The\npresent work shows how effective satisfiability sub-calculi, a special type of\nsatisfiability calculi, can be combined with proof calculi, in order to provide\nfoundations to certain methodological approaches to software analysis by\nrelating the construction of finite counterexamples and the absence of proofs,\nin an abstract categorical setting.",
    "descriptor": "",
    "authors": [
      "Carlos Gustavo Lopez Pombo",
      "Agust\u00edn Eloy Martinez Su\u00f1\u00e9"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.07180"
  },
  {
    "id": "arXiv:2206.07181",
    "title": "To Aggregate or Not? Learning with Separate Noisy Labels",
    "abstract": "The rawly collected training data often comes with separate noisy labels\ncollected from multiple imperfect annotators (e.g., via crowdsourcing).\nTypically one would first aggregate the separate noisy labels into one and\napply standard training methods. The literature has also studied extensively on\neffective aggregation approaches. This paper revisits this choice and aims to\nprovide an answer to the question of whether one should aggregate separate\nnoisy labels into single ones or use them separately as given. We theoretically\nanalyze the performance of both approaches under the empirical risk\nminimization framework for a number of popular loss functions, including the\nones designed specifically for the problem of learning with noisy labels. Our\ntheorems conclude that label separation is preferred over label aggregation\nwhen the noise rates are high, or the number of labelers/annotations is\ninsufficient. Extensive empirical results validate our conclusion.",
    "descriptor": "\nComments: Paper under Review\n",
    "authors": [
      "Jiaheng Wei",
      "Zhaowei Zhu",
      "Tianyi Luo",
      "Ehsan Amid",
      "Abhishek Kumar",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07181"
  },
  {
    "id": "arXiv:2206.07182",
    "title": "Automated Detection of Typed Links in Issue Trackers",
    "abstract": "Stakeholders in software projects use issue trackers like JIRA to capture and\nmanage issues, including requirements and bugs. To ease issue navigation and\nstructure project knowledge, stakeholders manually connect issues via links of\ncertain types that reflect different dependencies, such as Epic-, Block-,\nDuplicate-, or Relate- links. Based on a large dataset of 15 JIRA repositories,\nwe study how well state-of-the-art machine learning models can automatically\ndetect common link types. We found that a pure BERT model trained on titles and\ndescriptions of linked issues significantly outperforms other optimized deep\nlearning models, achieving an encouraging average macro F1-score of 0.64 for\ndetecting 9 popular link types across all repositories (weighted F1-score of\n0.73). For the specific Subtask- and Epic- links, the model achieved top\nF1-scores of 0.89 and 0.97, respectively. Our model does not simply learn the\ntextual similarity of the issues. In general, shorter issue text seems to\nimprove the prediction accuracy with a strong negative correlation of -0.70. We\nfound that Relate-links often get confused with the other links, which suggests\nthat they are likely used as default links in unclear cases. We also observed\nsignificant differences across the repositories, depending on how they are used\nand by whom.",
    "descriptor": "\nComments: Accepted at RE2022, eCF Paper Id: 1655146264348\n",
    "authors": [
      "Clara Marie L\u00fcders",
      "Tim Pietz",
      "Walid Maalej"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.07182"
  },
  {
    "id": "arXiv:2206.07185",
    "title": "Aeneas: Rust Verification by Functional Translation",
    "abstract": "We present Aeneas, a new verification toolchain for Rust programs based on a\nlightweight functional translation. We leverage Rust's rich region-based type\nsystem to eliminate memory reasoning for many Rust programs, as long as they do\nnot rely on interior mutability or unsafe code. Doing so, we relieve the proof\nengineer of the burden of memory-based reasoning, allowing them to instead\nfocus on functional properties of their code.\nOur first contribution is a new approach to borrows and controlled aliasing.\nWe propose a pure, functional semantics for LLBC, a Low-Level Borrow Calculus\nthat captures a large subset of Rust programs. Our semantics is value-based,\nmeaning there is no notion of memory, addresses or pointer arithmetic. Our\nsemantics is also ownership-centric, meaning that we enforce soundness of\nborrows via a semantic criterion based on loans rather than through a syntactic\ntype-based lifetime discipline. We claim that our semantics captures the\nessence of the borrow mechanism rather than its current implementation in the\nRust compiler.\nOur second contribution is a translation from LLBC to a pure lambda-calculus.\nThis allows the user to reason about the original Rust program through the\ntheorem prover of their choice. To deal with the well-known technical\ndifficulty of terminating a borrow, we rely on a novel approach, in which we\napproximate the borrow graph in the presence of function calls. This in turn\nallows us to perform the translation using a new technical device called\nbackward functions.\nWe implement our toolchain in a mixture of Rust and OCaml. Our evaluation\nshows significant gains of verification productivity for the programmer.\nRust goes to great lengths to enforce static control of aliasing; the proof\nengineer should not waste any time on memory reasoning when so much already\ncomes \"for free\"!",
    "descriptor": "",
    "authors": [
      "Son Ho",
      "Jonathan Protzenko"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.07185"
  },
  {
    "id": "arXiv:2206.07188",
    "title": "Defending Observation Attacks in Deep Reinforcement Learning via  Detection and Denoising",
    "abstract": "Neural network policies trained using Deep Reinforcement Learning (DRL) are\nwell-known to be susceptible to adversarial attacks. In this paper, we consider\nattacks manifesting as perturbations in the observation space managed by the\nexternal environment. These attacks have been shown to downgrade policy\nperformance significantly. We focus our attention on well-trained deterministic\nand stochastic neural network policies in the context of continuous control\nbenchmarks subject to four well-studied observation space adversarial attacks.\nTo defend against these attacks, we propose a novel defense strategy using a\ndetect-and-denoise schema. Unlike previous adversarial training approaches that\nsample data in adversarial scenarios, our solution does not require sampling\ndata in an environment under attack, thereby greatly reducing risk during\ntraining. Detailed experimental results show that our technique is comparable\nwith state-of-the-art adversarial training approaches.",
    "descriptor": "",
    "authors": [
      "Zikang Xiong",
      "Joe Eappen",
      "He Zhu",
      "Suresh Jagannathan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07188"
  },
  {
    "id": "arXiv:2206.07190",
    "title": "Codec at SemEval-2022 Task 5: Multi-Modal Multi-Transformer Misogynous  Meme Classification Framework",
    "abstract": "In this paper we describe our work towards building a generic framework for\nboth multi-modal embedding and multi-label binary classification tasks, while\nparticipating in task 5 (Multimedia Automatic Misogyny Identification) of\nSemEval 2022 competition.\nSince pretraining deep models from scratch is a resource and data hungry\ntask, our approach is based on three main strategies. We combine different\nstate-of-the-art architectures to capture a wide spectrum of semantic signals\nfrom the multi-modal input. We employ a multi-task learning scheme to be able\nto use multiple datasets from the same knowledge domain to help increase the\nmodel's performance. We also use multiple objectives to regularize and fine\ntune different system components.",
    "descriptor": "\nComments: Accepted for publication at the 16th International Workshop on Semantic Evaluation, Task 5: MAMI - Multimedia Automatic Misogyny Identification co-located with NAACL 2022\n",
    "authors": [
      "Ahmed Mahran",
      "Carlo Alessandro Borella",
      "Konstantinos Perifanos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07190"
  },
  {
    "id": "arXiv:2206.07194",
    "title": "Machines Explaining Linear Programs",
    "abstract": "There has been a recent push in making machine learning models more\ninterpretable so that their performance can be trusted. Although successful,\nthese methods have mostly focused on the deep learning methods while the\nfundamental optimization methods in machine learning such as linear programs\n(LP) have been left out. Even if LPs can be considered as whitebox or clearbox\nmodels, they are not easy to understand in terms of relationships between\ninputs and outputs. As a linear program only provides the optimal solution to\nan optimization problem, further explanations are often helpful. In this work,\nwe extend the attribution methods for explaining neural networks to linear\nprograms. These methods explain the model by providing relevance scores for the\nmodel inputs, to show the influence of each input on the output. Alongside\nusing classical gradient-based attribution methods we also propose a way to\nadapt perturbation-based attribution methods to LPs. Our evaluations of several\ndifferent linear and integer problems showed that attribution methods can\ngenerate useful explanations for linear programs. However, we also demonstrate\nthat using a neural attribution method directly might come with some drawbacks,\nas the properties of these methods on neural networks do not necessarily\ntransfer to linear programs. The methods can also struggle if a linear program\nhas more than one optimal solution, as a solver just returns one possible\nsolution. Our results can hopefully be used as a good starting point for\nfurther research in this direction.",
    "descriptor": "\nComments: Main paper: 9.5 pages, References: 2.5 pages, Supplement: 6 pages. Main paper: 5 figures, 4 tables, Supplement: 3 figures, 6 tables\n",
    "authors": [
      "David Steinmann",
      "Matej Ze\u010devi\u0107",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07194"
  },
  {
    "id": "arXiv:2206.07195",
    "title": "Tearing Apart NOTEARS: Controlling the Graph Prediction via Variance  Manipulation",
    "abstract": "Simulations are ubiquitous in machine learning. Especially in graph learning,\nsimulations of Directed Acyclic Graphs (DAG) are being deployed for evaluating\nnew algorithms. In the literature, it was recently argued that\ncontinuous-optimization approaches to structure discovery such as NOTEARS might\nbe exploiting the sortability of the variable's variances in the available data\ndue to their use of least square losses. Specifically, since structure\ndiscovery is a key problem in science and beyond, we want to be invariant to\nthe scale being used for measuring our data (e.g. meter versus centimeter\nshould not affect the causal direction inferred by the algorithm). In this\nwork, we further strengthen this initial, negative empirical suggestion by both\nproving key results in the multivariate case and corroborating with further\nempirical evidence. In particular, we show that we can control the resulting\ngraph with our targeted variance attacks, even in the case where we can only\npartially manipulate the variances of the data.",
    "descriptor": "\nComments: Main paper: 5.5 pages, References: 1 page, Supplement: 2 pages. Main paper: 3 figures, Supplement: 1 figure, 1 table\n",
    "authors": [
      "Jonas Seng",
      "Matej Ze\u010devi\u0107",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07195"
  },
  {
    "id": "arXiv:2206.07196",
    "title": "Towards a Solution to Bongard Problems: A Causal Approach",
    "abstract": "To date, Bongard Problems (BP) remain one of the few fortresses of AI history\nyet to be raided by the powerful models of the current era. We present a\nsystematic analysis using modern techniques from the intersection of causality\nand AI/ML in a humble effort of reviving research around BPs. Specifically, we\nfirst compile the BPs into a Markov decision process, then secondly pose causal\nassumptions on the data generating process arguing for their applicability to\nBPs, and finally apply reinforcement learning techniques for solving the BPs\nsubject to the causal assumptions.",
    "descriptor": "\nComments: Main paper: 5.5 pages, References: 1 page, Supplement: 1 page. Main paper: 5 figures, Supplement: 3 figures\n",
    "authors": [
      "Salahedine Youssef",
      "Matej Ze\u010devi\u0107",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07196"
  },
  {
    "id": "arXiv:2206.07197",
    "title": "Improving Solar Flare Prediction by Time Series Outlier Detection",
    "abstract": "Solar flares not only pose risks to outer space technologies and astronauts'\nwell being, but also cause disruptions on earth to our hight-tech,\ninterconnected infrastructure our lives highly depend on. While a number of\nmachine-learning methods have been proposed to improve flare prediction, none\nof them, to the best of our knowledge, have investigated the impact of outliers\non the reliability and those models' performance. In this study, we investigate\nthe impact of outliers in a multivariate time series benchmark dataset, namely\nSWAN-SF, on flare prediction models, and test our hypothesis. That is, there\nexist outliers in SWAN-SF, removal of which enhances the performance of the\nprediction models on unseen datasets. We employ Isolation Forest to detect the\noutliers among the weaker flare instances. Several experiments are carried out\nusing a large range of contamination rates which determine the percentage of\npresent outliers. We asses the quality of each dataset in terms of its actual\ncontamination using TimeSeriesSVC. In our best finding, we achieve a 279%\nincrease in True Skill Statistic and 68% increase in Heidke Skill Score. The\nresults show that overall a significant improvement can be achieved to flare\nprediction if outliers are detected and removed properly.",
    "descriptor": "",
    "authors": [
      "Junzhi Wen",
      "Md Reazul Islam",
      "Azim Ahmadzadeh",
      "Rafal A. Angryk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Solar and Stellar Astrophysics (astro-ph.SR)"
    ],
    "url": "https://arxiv.org/abs/2206.07197"
  },
  {
    "id": "arXiv:2206.07198",
    "title": "Surgical Phase Recognition in Laparoscopic Cholecystectomy",
    "abstract": "Automatic recognition of surgical phases in surgical videos is a fundamental\ntask in surgical workflow analysis. In this report, we propose a\nTransformer-based method that utilizes calibrated confidence scores for a\n2-stage inference pipeline, which dynamically switches between a baseline model\nand a separately trained transition model depending on the calibrated\nconfidence level. Our method outperforms the baseline model on the Cholec80\ndataset, and can be applied to a variety of action segmentation methods.",
    "descriptor": "",
    "authors": [
      "Yunfan Li",
      "Vinayak Shenoy",
      "Prateek Prasanna",
      "I.V. Ramakrishnan",
      "Haibin Ling",
      "Himanshu Gupta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07198"
  },
  {
    "id": "arXiv:2206.07200",
    "title": "Using Machine Learning to Augment Dynamic Time Warping Based Signal  Classification",
    "abstract": "Modern applications such as voice recognition rely on the ability to compare\nsignals to pre-recorded ones to classify them. However, this comparison\ntypically needs to ignore differences due to signal noise, temporal offset,\nsignal magnitude, and other external factors. The Dynamic Time Warping (DTW)\nalgorithm quantifies this similarity by finding corresponding regions between\nthe signals and non-linearly warping one signal by stretching and shrinking it.\nUnfortunately, searching through all \"warps\" of a signal to find the best\ncorresponding regions is computationally expensive. The FastDTW algorithm\nimproves performance, but sacrifices accuracy by only considering small signal\nwarps.\nMy goal is to improve the speed of DTW while maintaining high accuracy. My\nkey insight is that in any particular application domain, signals exhibit\nspecific types of variation. For example, the accelerometer signal measured for\ntwo different people would differ based on their stride length and weight. My\nsystem, called Machine Learning DTW (MLDTW), uses machine learning to learn the\ntypes of warps that are common in a particular domain. It then uses the learned\nmodel to improve DTW performance by limiting the search of potential warps\nappropriately. My results show that compared to FastDTW, MLDTW is at least as\nfast and reduces errors by 60% on average across four different data sets.\nThese improvements will significantly impact a wide variety of applications\n(e.g. health monitoring) and enable more scalable processing of multivariate,\nhigher frequency, and longer signal recordings.",
    "descriptor": "\nComments: Presented at Regeneron International Science and Engineering Fair (ISEF) 2022\n",
    "authors": [
      "Arvind Seshan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07200"
  },
  {
    "id": "arXiv:2206.07201",
    "title": "An autonomous robot for pruning modern, planar fruit trees",
    "abstract": "Dormant pruning of fruit trees is an important task for maintaining tree\nhealth and ensuring high-quality fruit. Due to decreasing labor availability,\npruning is a prime candidate for robotic automation. However, pruning also\nrepresents a uniquely difficult problem for robots, requiring robust systems\nfor perception, pruning point determination, and manipulation that must operate\nunder variable lighting conditions and in complex, highly unstructured\nenvironments. In this paper, we introduce a system for pruning sweet cherry\ntrees (in a planar tree architecture called an upright fruiting offshoot\nconfiguration) that integrates various subsystems from our previous work on\nperception and manipulation. The resulting system is capable of operating\ncompletely autonomously and requires minimal control of the environment. We\nvalidate the performance of our system through field trials in a sweet cherry\norchard, ultimately achieving a cutting success rate of 58%. Though not fully\nrobust and requiring improvements in throughput, our system is the first to\noperate on fruit trees and represents a useful base platform to be improved in\nthe future.",
    "descriptor": "",
    "authors": [
      "Alexander You",
      "Nidhi Parayil",
      "Josyula Gopala Krishna",
      "Uddhav Bhattarai",
      "Ranjan Sapkota",
      "Dawood Ahmed",
      "Matthew Whiting",
      "Manoj Karkee",
      "Cindy M. Grimm",
      "Joseph R. Davidson"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07201"
  },
  {
    "id": "arXiv:2206.07203",
    "title": "Attributions Beyond Neural Networks: The Linear Program Case",
    "abstract": "Linear Programs (LPs) have been one of the building blocks in machine\nlearning and have championed recent strides in differentiable optimizers for\nlearning systems. While there exist solvers for even high-dimensional LPs,\nunderstanding said high-dimensional solutions poses an orthogonal and\nunresolved problem. We introduce an approach where we consider neural encodings\nfor LPs that justify the application of attribution methods from explainable\nartificial intelligence (XAI) designed for neural learning systems. The several\nencoding functions we propose take into account aspects such as feasibility of\nthe decision space, the cost attached to each input, or the distance to special\npoints of interest. We investigate the mathematical consequences of several XAI\nmethods on said neural LP encodings. We empirically show that the attribution\nmethods Saliency and LIME reveal indistinguishable results up to perturbation\nlevels, and we propose the property of Directedness as the main discriminative\ncriterion between Saliency and LIME on one hand, and a perturbation-based\nFeature Permutation approach on the other hand. Directedness indicates whether\nan attribution method gives feature attributions with respect to an increase of\nthat feature. We further notice the baseline selection problem beyond the\nclassical computer vision setting for Integrated Gradients.",
    "descriptor": "\nComments: Main paper: 9.5 pages, References: 2 pages, Supplement: 2.5 pages. Main paper: 5 figures, 2 tables, Supplement: 1 figure\n",
    "authors": [
      "Florian Peter Busch",
      "Matej Ze\u010devi\u0107",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07203"
  },
  {
    "id": "arXiv:2206.07207",
    "title": "Multimodal Event Graphs: Towards Event Centric Understanding of  Multimodal World",
    "abstract": "Understanding how events described or shown in multimedia content relate to\none another is a critical component to developing robust artificially\nintelligent systems which can reason about real-world media. While much\nresearch has been devoted to event understanding in the text, image, and video\ndomains, none have explored the complex relations that events experience across\ndomains. For example, a news article may describe a `protest' event while a\nvideo shows an `arrest' event. Recognizing that the visual `arrest' event is a\nsubevent of the broader `protest' event is a challenging, yet important problem\nthat prior work has not explored. In this paper, we propose the novel task of\nMultiModal Event Event Relations to recognize such cross-modal event relations.\nWe contribute a large-scale dataset consisting of 100k video-news article\npairs, as well as a benchmark of densely annotated data. We also propose a\nweakly supervised multimodal method which integrates commonsense knowledge from\nan external knowledge base (KB) to predict rich multimodal event hierarchies.\nExperiments show that our model outperforms a number of competitive baselines\non our proposed benchmark. We also perform a detailed analysis of our model's\nperformance and suggest directions for future research.",
    "descriptor": "",
    "authors": [
      "Hammad A. Ayyubi",
      "Christopher Thomas",
      "Lovish Chum",
      "Rahul Lokesh",
      "Yulei Niu",
      "Xudong Lin",
      "Long Chen",
      "Jaywon Koo",
      "Sounak Ray",
      "Shih-Fu Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07207"
  },
  {
    "id": "arXiv:2206.07209",
    "title": "On Approximating Total Variation Distance",
    "abstract": "Total variation distance (TV distance) is a fundamental notion of distance\nbetween probability distributions. In this work, we introduce and study the\ncomputational problem of determining the TV distance between two product\ndistributions over the domain $\\{0,1\\}^n$. We establish the following results.\n1. Exact computation of TV distance between two product distributions is\n$\\#\\mathsf{P}$-complete. This is in stark contrast with other distance measures\nsuch as KL, Chi-square, and Hellinger which tensorize over the marginals.\n2. Given two product distributions $P$ and $Q$ with marginals of $P$ being at\nleast $1/2$ and marginals of $Q$ being at most the respective marginals of $P$,\nthere exists a fully polynomial-time randomized approximation scheme (FPRAS)\nfor computing the TV distance between $P$ and $Q$. In particular, this leads to\nan efficient approximation scheme for the interesting case when $P$ is an\narbitrary product distribution and $Q$ is the uniform distribution.\nWe pose the question of characterizing the complexity of approximating the TV\ndistance between two arbitrary product distributions as a basic open problem in\ncomputational statistics.",
    "descriptor": "\nComments: 22 pages, 1 figure\n",
    "authors": [
      "Arnab Bhattacharyya",
      "Sutanu Gayen",
      "Kuldeep S. Meel",
      "Dimitrios Myrisiotis",
      "A. Pavan",
      "N. V. Vinodchandran"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.07209"
  },
  {
    "id": "arXiv:2206.07210",
    "title": "Hidden Influences of Crowd Behavior in Crowdfunding: An Experimental  Study",
    "abstract": "Crowdfunding continues to transform financing opportunities for many across\nthe globe. While extensive research has explored factors related to fundraising\nsuccess, less is known about the social signaling mechanisms that lead\npotential contributors to fund a project. Existing large-scale observational\nstudies point to non-straightforward characteristics of prior contributions\n(aka \"crowd signals\") that forecast further contributions to a project, albeit\nwithout theoretical support for their effectiveness in predicting fundraising\nsuccess. We translate empirical crowd signals based on variations in the\namounts and timings of contributions into mock contribution scenarios to\nscrutinize the influence of essential signals on contributors' decisions to\nfund. We conduct two experiments with 1,250 online participants. The first\nexperiment investigates whether high crowd signals, i.e., contributions of\nvarying amounts arriving at unequally spaced time intervals, are making people\nmore likely to contribute to a crowdfunding project. The second experiment\nfurther examines the effect of basic competition on the role of the crowd\nsignals. Across both, we observe that high crowd signals attract 19.2% more\ncontributors than low signals. These findings are robust to different project\ntypes, fundraising goals, participants' interest level in the projects, their\naltruistic attitudes, and susceptibility to social influence. Participants'\nunguided, post-hoc reflections about the reasons behind their choice to fund\nrevealed that most were unaware of their reliance on any crowd signals and\ninstead attributed their decision to nonexistent differences in project\ndescriptions. These results point to the power of crowd signals unbeknownst to\nthose affected by them and lay the groundwork for theory-building, specifically\nin relation to the essential signaling that is happening on online platforms.",
    "descriptor": "",
    "authors": [
      "Henry K. Dambanemuya",
      "Eunseo Choi",
      "Darren Gergle",
      "Em\u0151ke-\u00c1gnes Horv\u00e1t"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.07210"
  },
  {
    "id": "arXiv:2206.07212",
    "title": "Explainable expected goal models for performance analysis in football  analytics",
    "abstract": "The expected goal provides a more representative measure of the team and\nplayer performance which also suit the low-scoring nature of football instead\nof score in modern football. The score of a match involves randomness and often\nmay not represent the performance of the teams and players, therefore it has\nbeen popular to use the alternative statistics in recent years such as shots on\ntarget, ball possessions, and drills. To measure the probability of a shot\nbeing a goal by the expected goal, several features are used to train an\nexpected goal model which is based on the event and tracking football data. The\nselection of these features, the size and date of the data, and the model which\nare used as the parameters that may affect the performance of the model. Using\nblack-box machine learning models for increasing the predictive performance of\nthe model decreases its interpretability that causes the loss of information\nthat can be gathered from the model. This paper proposes an accurate expected\ngoal model trained consisting of 315,430 shots from seven seasons between\n2014-15 and 2020-21 of the top-five European football leagues. Moreover, this\nmodel is explained by using explainable artificial intelligence tool to obtain\nan explainable expected goal model for evaluating a team or player performance.\nTo best of our knowledge, this is the first paper that demonstrates a practical\napplication of an explainable artificial intelligence tool aggregated profiles\nto explain a group of observations on an accurate expected goal model for\nmonitoring the team and player performance. Moreover, these methods can be\ngeneralized to other sports branches.",
    "descriptor": "",
    "authors": [
      "Mustafa Cavus",
      "Przemys\u0142aw Biecek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07212"
  },
  {
    "id": "arXiv:2206.07215",
    "title": "SPENDER: A Platform for Secure and Privacy-Preserving Decentralized P2P  E-Commerce",
    "abstract": "The blockchain technology empowers secure, trustless, and privacy-preserving\ntrading with cryptocurrencies. However, existing blockchain-based trading\nplatforms only support trading cryptocurrencies with digital assets (e.g.,\nNFTs). Although several payment service providers have started to accept\ncryptocurrency as a payment method for tangible goods (e.g., Visa, PayPal),\ncustomers still need to trust and hand over their private information to\ncentralized E-commerce platforms (e.g., Amazon, eBay). To enable trustless and\nprivacy-preserving trading between cryptocurrencies and real goods, we propose\nSPENDER, a smart-contract-based platform for Secure and Privacy-PresErviNg\nDecentralized P2P E-commeRce. The design of our platform enables various\nadvantageous features and brings unlimited future potential. Moreover, our\nplatform provides a complete paradigm for designing real-world Web3\ninfrastructures on the blockchain, which broadens the application scope and\nexploits the intrinsic values of cryptocurrencies. The platform has been built\nand tested on the Terra ecosystem, and we plan to open-source the code later.",
    "descriptor": "\nComments: 9 pages, 2 figures, preprint\n",
    "authors": [
      "Shuhao Zheng",
      "Junliang Luo",
      "Erqun Dong",
      "Can Chen",
      "Xue Liu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2206.07215"
  },
  {
    "id": "arXiv:2206.07220",
    "title": "Towards Verifiable Differentially-Private Polling",
    "abstract": "Analyses that fulfill differential privacy provide plausible deniability to\nindividuals while allowing analysts to extract insights from data. However,\nbeyond an often acceptable accuracy tradeoff, these statistical disclosure\ntechniques generally inhibit the verifiability of the provided information, as\none cannot check the correctness of the participants' truthful information, the\ndifferentially private mechanism, or the unbiased random number generation.\nWhile related work has already discussed this opportunity, an efficient\nimplementation with a precise bound on errors and corresponding proofs of the\ndifferential privacy property is so far missing. In this paper, we follow an\napproach based on zero-knowledge proofs~(ZKPs), in specific succinct\nnon-interactive arguments of knowledge, as a verifiable computation technique\nto prove the correctness of a differentially private query output. In\nparticular, we ensure the guarantees of differential privacy hold despite the\nlimitations of ZKPs that operate on finite fields and have limited branching\ncapabilities. We demonstrate that our approach has practical performance and\ndiscuss how practitioners could employ our primitives to verifiably query\nindividuals' age from their digitally signed ID card in a differentially\nprivate manner.",
    "descriptor": "",
    "authors": [
      "Gonzalo Munilla Garrido",
      "Matthias Babel",
      "Johannes Sedlmeir"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07220"
  },
  {
    "id": "arXiv:2206.07227",
    "title": "State Supervised Steering Function for Sampling-based Kinodynamic  Planning",
    "abstract": "Sampling-based motion planners such as RRT* and BIT*, when applied to\nkinodynamic motion planning, rely on steering functions to generate\ntime-optimal solutions connecting sampled states. Implementing exact steering\nfunctions requires either analytical solutions to the time-optimal control\nproblem, or nonlinear programming (NLP) solvers to solve the boundary value\nproblem given the system's kinodynamic equations. Unfortunately, analytical\nsolutions are unavailable for many real-world domains, and NLP solvers are\nprohibitively computationally expensive, hence fast and optimal kinodynamic\nmotion planning remains an open problem. We provide a solution to this problem\nby introducing State Supervised Steering Function (S3F), a novel approach to\nlearn time-optimal steering functions. S3F is able to produce near-optimal\nsolutions to the steering function orders of magnitude faster than its NLP\ncounterpart. Experiments conducted on three challenging robot domains show that\nRRT* using S3F significantly outperforms state-of-the-art planning approaches\non both solution cost and runtime. We further provide a proof of probabilistic\ncompleteness of RRT* modified to use S3F.",
    "descriptor": "\nComments: 11 pages, 7 figures. In Proceedings of AAMAS 2022\n",
    "authors": [
      "Pranav Atreya",
      "Joydeep Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07227"
  },
  {
    "id": "arXiv:2206.07229",
    "title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on  Data-Driven Deep Learning",
    "abstract": "Emotion classification of speech and assessment of the emotion strength are\nrequired in applications such as emotional text-to-speech and voice conversion.\nThe emotion attribute ranking function based on Support Vector Machine (SVM)\nwas proposed to predict emotion strength for emotional speech corpus. However,\nthe trained ranking function doesn't generalize to new domains, which limits\nthe scope of applications, especially for out-of-domain or unseen speech. In\nthis paper, we propose a data-driven deep learning model, i.e. StrengthNet, to\nimprove the generalization of emotion strength assessment for seen and unseen\nspeech. This is achieved by the fusion of emotional data from various domains.\nWe follow a multi-task learning network architecture that includes an acoustic\nencoder, a strength predictor, and an auxiliary emotion predictor. Experiments\nshow that the predicted emotion strength of the proposed StrengthNet is highly\ncorrelated with ground truth scores for both seen and unseen speech. We release\nthe source codes at: https://github.com/ttslr/StrengthNet.",
    "descriptor": "\nComments: To appear in INTERSPEECH 2022. 5 pages, 4 figures. Substantial text overlap with arXiv:2110.03156\n",
    "authors": [
      "Rui Liu",
      "Berrak Sisman",
      "Bj\u00f6rn Schuller",
      "Guanglai Gao",
      "Haizhou Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07229"
  },
  {
    "id": "arXiv:2206.07230",
    "title": "Automating Dependency Updates in Practice: An Exploratory Study on  GitHub Dependabot",
    "abstract": "Dependency update bots automatically open pull requests to update software\ndependencies on behalf of developers. Early research shows that developers are\nsuspicious of updates performed by bots and feel tired of overwhelming\nnotifications from these bots. Despite this, dependency update bots are\nbecoming increasingly popular. Such contrast motivates us to investigate\nDependabot, currently the most visible bot in GitHub, to reveal the\neffectiveness and limitations of the state-of-art dependency update bots. We\nuse exploratory data analysis and developer survey to evaluate the\neffectiveness of Dependabot in keeping dependencies up-to-date, reducing update\nsuspicion, and reducing notification fatigue. We obtain mixed findings. On the\npositive side, Dependabot is effective in reducing technical lag and developers\nare highly receptive to its pull requests. On the negative side, its\ncompatibility scores are too scarce to be effective in reducing update\nsuspicion; developers tend to configure Dependabot toward reducing the number\nof notifications; and 11.3\\% of projects have deprecated Dependabot in favor of\nother alternatives. Our findings reveal a large room for improvement in\ndependency update bots which calls for effort from both bot designers and\nsoftware engineering researchers.",
    "descriptor": "",
    "authors": [
      "Runzhi He",
      "Hao He",
      "Yuxia Zhang",
      "Minghui Zhou"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.07230"
  },
  {
    "id": "arXiv:2206.07231",
    "title": "Resilience and Energy-Awareness in Constraint-Driven-Controlled  Multi-Robot Systems",
    "abstract": "In the context of constraint-driven control of multi-robot systems, in this\npaper, we propose an optimization-based framework that is able to ensure\nresilience and energy-awareness of teams of robots. The approach is based on a\nnovel, frame-theoretic, measure of resilience which allows us to analyze and\nenforce resilient behaviors of multi-robot systems. The properties of\nresilience and energy-awareness are encoded as constraints of a convex\noptimization program which is used to synthesize the robot control inputs. This\nallows for the combination of such properties with the execution of coordinated\ntasks to achieve resilient and energy-aware robot operations. The effectiveness\nof the proposed method is illustrated in a simulated scenario where a team of\nrobots is deployed to execute two tasks subject to energy and resilience\nconstraints.",
    "descriptor": "",
    "authors": [
      "Gennaro Notomista"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07231"
  },
  {
    "id": "arXiv:2206.07234",
    "title": "Brownian Noise Reduction: Maximizing Privacy Subject to Accuracy  Constraints",
    "abstract": "There is a disconnect between how researchers and practitioners handle\nprivacy-utility tradeoffs. Researchers primarily operate from a privacy first\nperspective, setting strict privacy requirements and minimizing risk subject to\nthese constraints. Practitioners often desire an accuracy first perspective,\npossibly satisfied with the greatest privacy they can get subject to obtaining\nsufficiently small error. Ligett et al. have introduced a \"noise reduction\"\nalgorithm to address the latter perspective. The authors show that by adding\ncorrelated Laplace noise and progressively reducing it on demand, it is\npossible to produce a sequence of increasingly accurate estimates of a private\nparameter while only paying a privacy cost for the least noisy iterate\nreleased. In this work, we generalize noise reduction to the setting of\nGaussian noise, introducing the Brownian mechanism. The Brownian mechanism\nworks by first adding Gaussian noise of high variance corresponding to the\nfinal point of a simulated Brownian motion. Then, at the practitioner's\ndiscretion, noise is gradually decreased by tracing back along the Brownian\npath to an earlier time. Our mechanism is more naturally applicable to the\ncommon setting of bounded $\\ell_2$-sensitivity, empirically outperforms\nexisting work on common statistical tasks, and provides customizable control of\nprivacy loss over the entire interaction with the practitioner. We complement\nour Brownian mechanism with ReducedAboveThreshold, a generalization of the\nclassical AboveThreshold algorithm that provides adaptive privacy guarantees.\nOverall, our results demonstrate that one can meet utility constraints while\nstill maintaining strong levels of privacy.",
    "descriptor": "\nComments: 18 pages, 4 figures\n",
    "authors": [
      "Justin Whitehouse",
      "Zhiwei Steven Wu",
      "Aaditya Ramdas",
      "Ryan Rogers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07234"
  },
  {
    "id": "arXiv:2206.07235",
    "title": "Training Discrete Deep Generative Models via Gapped Straight-Through  Estimator",
    "abstract": "While deep generative models have succeeded in image processing, natural\nlanguage processing, and reinforcement learning, training that involves\ndiscrete random variables remains challenging due to the high variance of its\ngradient estimation process. Monte Carlo is a common solution used in most\nvariance reduction approaches. However, this involves time-consuming resampling\nand multiple function evaluations. We propose a Gapped Straight-Through (GST)\nestimator to reduce the variance without incurring resampling overhead. This\nestimator is inspired by the essential properties of Straight-Through\nGumbel-Softmax. We determine these properties and show via an ablation study\nthat they are essential. Experiments demonstrate that the proposed GST\nestimator enjoys better performance compared to strong baselines on two\ndiscrete deep generative modeling tasks, MNIST-VAE and ListOps.",
    "descriptor": "\nComments: Accepted at the International Conference on Machine Learning (ICML) 2022. The first two authors contributed equally\n",
    "authors": [
      "Ting-Han Fan",
      "Ta-Chung Chi",
      "Alexander I. Rudnicky",
      "Peter J. Ramadge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07235"
  },
  {
    "id": "arXiv:2206.07238",
    "title": "Location-based Twitter Filtering for the Creation of Low-Resource  Language Datasets in Indonesian Local Languages",
    "abstract": "Twitter contains an abundance of linguistic data from the real world. We\nexamine Twitter for user-generated content in low-resource languages such as\nlocal Indonesian. For NLP to work in Indonesian, it must consider local\ndialects, geographic context, and regional culture influence Indonesian\nlanguages. This paper identifies the problems we faced when constructing a\nLocal Indonesian NLP dataset. Furthermore, we are developing a framework for\ncreating, collecting, and classifying Local Indonesian datasets for NLP. Using\ntwitter's geolocation tool for automatic annotating.",
    "descriptor": "",
    "authors": [
      "Mukhlis Amien",
      "Chong Feng",
      "Heyan Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07238"
  },
  {
    "id": "arXiv:2206.07240",
    "title": "Test-Time Adaptation for Visual Document Understanding",
    "abstract": "Self-supervised pretraining has been able to produce transferable\nrepresentations for various visual document understanding (VDU) tasks. However,\nthe ability of such representations to adapt to new distribution shifts at\ntest-time has not been studied yet. We propose DocTTA, a novel test-time\nadaptation approach for documents that leverages cross-modality self-supervised\nlearning via masked visual language modeling as well as pseudo labeling to\nadapt models learned on a \\textit{source} domain to an unlabeled\n\\textit{target} domain at test time. We also introduce new benchmarks using\nexisting public datasets for various VDU tasks including entity recognition,\nkey-value extraction, and document visual question answering tasks where DocTTA\nimproves the source model performance up to 1.79\\% in (F1 score), 3.43\\% (F1\nscore), and 17.68\\% (ANLS score), respectively while drastically reducing\ncalibration error on target data.",
    "descriptor": "",
    "authors": [
      "Sayna Ebrahimi",
      "Sercan O. Arik",
      "Tomas Pfister"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07240"
  },
  {
    "id": "arXiv:2206.07242",
    "title": "Coevolutionary Dynamics of Actions and Opinions in Social Networks",
    "abstract": "Modeling opinion formation and decision-making processes, important in their\nown rights, have been treated as separate problems in the study of dynamical\nmodels for social networks. Empirical studies suggest a deep intertwining\nbetween these two processes, and in this paper, we bridge the gap in the\nexisting research by proposing a novel coevolutionary model. In the model, each\nindividual can select an action from a binary set, and also holds an opinion on\nwhich action they prefer. Actions and opinions coevolve on a two-layer network\nstructure. Under some reasonable assumptions on the network structure and\nasynchronous updating mechanics, we use rigorous analysis to establish that for\nall initial conditions, the actions converge in a finite number of time steps\nwhile opinions converge asymptotically. Next, we provide sufficient conditions\nfor the emergence and the stability of polarized equilibria, whereby the\npopulation splits into two communities, each selecting and supporting one of\nthe actions. Finally, numerical simulations are used to examine \"pluralistic\nignorance\", whereby a social group incorrectly assumes the opinions of others\ndue to the actions observed.",
    "descriptor": "\nComments: Manuscript submitted to IEEE Transactions on Automatic Control 2021 August 9th\n",
    "authors": [
      "Hassan Dehghani Aghbolagh",
      "Mengbin Ye",
      "Lorenzo Zino",
      "Ming Cao",
      "Zhiyong Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07242"
  },
  {
    "id": "arXiv:2206.07243",
    "title": "Closed-form Approximation for Performance Bound of Finite Blocklength  Massive MIMO Transmission",
    "abstract": "Ultra-reliable low latency communications (uRLLC) is adopted in the fifth\ngeneration (5G) mobile networks to better support mission-critical applications\nthat demand high level of reliability and low latency. With the aid of\nwell-established multiple-input multiple-output (MIMO) information theory,\nuRLLC in the future 6G is expected to provide enhanced capability towards\nextreme connectivity. Since the latency constraint can be represented\nequivalently by blocklength, channel coding theory at finite block-length plays\nan important role in the theoretic analysis of uRLLC. On the basis of\nPolyanskiy's and Yang's asymptotic results, we first derive the exact\nclose-form expressions for the expectation and variance of channel dispersion.\nThen, the bound of average maximal achievable rate is given for massive MIMO\nsystems in ideal independent and identically distributed fading channels. This\nis the study to reveal the underlying connections among the fundamental\nparameters in MIMO transmissions in a concise and complete close-form formula.\nMost importantly, the inversely proportional law observed therein implies that\nthe latency can be further reduced at expense of spatial degrees of freedom.",
    "descriptor": "",
    "authors": [
      "X. You",
      "B. Sheng",
      "Y. Huang",
      "W. Xu",
      "C. Zhang",
      "D. Wang",
      "P. Zhu",
      "C. Ji"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07243"
  },
  {
    "id": "arXiv:2206.07244",
    "title": "OpSparse: a Highly Optimized Framework for Sparse General Matrix  Multiplication on GPUs",
    "abstract": "Sparse general matrix multiplication (SpGEMM) is an important and expensive\ncomputation primitive in many real-world applications. Due to SpGEMM's inherent\nirregularity and the vast diversity of its input matrices, developing\nhigh-performance SpGEMM implementation on modern processors such as GPUs is\nchallenging. The state-of-the-art SpGEMM libraries (i.e., $nsparse$ and\n$spECK$) adopt several algorithms to tackle the challenges of global load\nbalance, local load balance, and allocation of the result matrix. While these\nlibraries focus on the high-level algorithm design for SpGEMM, they neglect\nseveral low-level architecture-specific optimizations, which causes inefficient\nimplementations in their libraries. In this paper, we classify their\ninefficient implementations into seven categories. Based on our observations,\nwe propose a highly optimized SpGEMM library called $OpSparse$. The\noptimizations in $OpSparse$ include 1) optimizing the binning method by\nimproving the utilization of the shared memory, 2) optimizing the hashing\nmethod by reducing the access to the hash table, 3) improving the trade-off\nbetween hash collision rate and hardware utilization in the hashing method by\nsetting appropriate binning ranges, 4) reducing the overheads of global memory\nutilization by minimizing the global memory usage of the metadata, and 5)\nimproving the execution parallelism by overlapping global memory allocation\nwith kernel execution. Performance evaluations with 26 commonly used matrices\non an Nvidia Tesla V100 GPU show that $OpSparse$ achieves up to $27.8\\times$,\n$1.81\\times$, and $2.04\\times$ performance speedup over three state-of-the-art\nlibraries: $cuSPARSE$, $nsparse$, and $spECK$, respectively.",
    "descriptor": "\nComments: This paper has been submitted to the IEEE Access since May 7, 2022, and is currently under review by IEEE Access. 20 pages, 11 fgures, 5 tables\n",
    "authors": [
      "Zhaoyang Du",
      "Yijin Guan",
      "Tianchan Guan",
      "Dimin Niu",
      "Linyong Huang",
      "Hongzhong Zheng",
      "Yuan Xie"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07244"
  },
  {
    "id": "arXiv:2206.07245",
    "title": "An Extractive-and-Abstractive Framework for Source Code Summarization",
    "abstract": "(Source) Code summarization aims to automatically generate summaries/comments\nfor a given code snippet in the form of natural language. Such summaries play a\nkey role in helping developers understand and maintain source code. Existing\ncode summarization techniques can be categorized into extractive methods and\nabstractive methods. The extractive methods extract a subset of important\nstatements and keywords from the code snippet using retrieval techniques, and\ngenerate a summary that preserves factual details in important statements and\nkeywords. However, such a subset may miss identifier or entity naming, and\nconsequently, the naturalness of generated summary is usually poor. The\nabstractive methods can generate human-written-like summaries leveraging\nencoder-decoder models from the neural machine translation domain. The\ngenerated summaries however often miss important factual details.\nTo generate human-written-like summaries with preserved factual details, we\npropose a novel extractive-and-abstractive framework. The extractive module in\nthe framework performs a task of extractive code summarization, which takes in\nthe code snippet and predicts important statements containing key factual\ndetails. The abstractive module in the framework performs a task of abstractive\ncode summarization, which takes in the entire code snippet and important\nstatements in parallel and generates a succinct and human-written-like natural\nlanguage summary. We evaluate the effectiveness of our technique, called EACS,\nby conducting extensive experiments on three datasets involving six programming\nlanguages. Experimental results show that EACS significantly outperforms\nstate-of-the-art techniques in terms of all three widely used metrics,\nincluding BLEU, METEOR, and ROUGH-L.",
    "descriptor": "",
    "authors": [
      "Weisong Sun",
      "Chunrong Fang",
      "Yuchen Chen",
      "Quanjun Zhang",
      "Guanhong Tao",
      "Tingxu Han",
      "Yifei Ge",
      "Yudu You",
      "Bin Luo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07245"
  },
  {
    "id": "arXiv:2206.07247",
    "title": "Fair Ranking as Fair Division: Impact-Based Individual Fairness in  Ranking",
    "abstract": "Rankings have become the primary interface in two-sided online markets. Many\nhave noted that the rankings not only affect the satisfaction of the users\n(e.g., customers, listeners, employers, travelers), but that the position in\nthe ranking allocates exposure -- and thus economic opportunity -- to the\nranked items (e.g., articles, products, songs, job seekers, restaurants,\nhotels). This has raised questions of fairness to the items, and most existing\nworks have addressed fairness by explicitly linking item exposure to item\nrelevance. However, we argue that any particular choice of such a link function\nmay be difficult to defend, and we show that the resulting rankings can still\nbe unfair. To avoid these shortcomings, we develop a new axiomatic approach\nthat is rooted in principles of fair division. This not only avoids the need to\nchoose a link function, but also more meaningfully quantifies the impact on the\nitems beyond exposure. Our axioms of envy-freeness and dominance over uniform\nranking postulate that for a fair ranking policy every item should prefer their\nown rank allocation over that of any other item, and that no item should be\nactively disadvantaged by the rankings. To compute ranking policies that are\nfair according to these axioms, we propose a new ranking objective related to\nthe Nash Social Welfare. We show that the solution has guarantees regarding its\nenvy-freeness, its dominance over uniform rankings for every item, and its\nPareto optimality. In contrast, we show that conventional exposure-based\nfairness can produce large amounts of envy and have a highly disparate impact\non the items. Beyond these theoretical results, we illustrate empirically how\nour framework controls the trade-off between impact-based individual item\nfairness and user utility.",
    "descriptor": "\nComments: accepted at KDD2022\n",
    "authors": [
      "Yuta Saito",
      "Thorsten Joachims"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07247"
  },
  {
    "id": "arXiv:2206.07248",
    "title": "Blockchain based Secure Energy Marketplace Scheme to Motivate P2P  Microgrids",
    "abstract": "In the past years trend of microgrids is increasing very fast to reduce\npeak-hour costs. However, in these systems, third parties are still involved in\nselling surplus energy. This results in increased cost of energy and there are\nmany operational and security barriers in such systems. These issues can be\nsolved by the decentralized distributed system of microgrids where a consumer\ncan locally sell their surplus energy to another consumer. To deploy such a\nsystem, one must consider security barriers for the transaction of energy. This\npaper proposes a solution to these problems by devising a scheme as a\nmarketplace where users interact with each other to buy and sell energy at\nbetter rates and get energy-generating resources on lease so that users do not\nhave to worry about capital investment. Agreement between owner of resources\nand consumer is recorded on blockchain based smart contracts. In this paper, a\nsurvey is performed for existing well known, decentralized energy solutions.\nThis paper also proposes an extra layer of security to leverage a shielded\nexecution environment so that information of energy generated, utilized, and\nshared cannot be changed by consumers and third parties even if the system is\ncompromised.",
    "descriptor": "\nComments: Accepted to be published in the International Journal of Informatics and Communication Technology (IJ-ICT)\n",
    "authors": [
      "Muhammad Awais",
      "Qamar Abbas",
      "Shehbaz Tariq",
      "Sayyaf Haider Warraich"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07248"
  },
  {
    "id": "arXiv:2206.07250",
    "title": "Streaming Algorithms for Ellipsoidal Approximation of Convex Polytopes",
    "abstract": "We give efficient deterministic one-pass streaming algorithms for finding an\nellipsoidal approximation of a symmetric convex polytope. The algorithms are\nnear-optimal in that their approximation factors differ from that of the\noptimal offline solution only by a factor sub-logarithmic in the aspect ratio\nof the polytope.",
    "descriptor": "\nComments: Accepted to COLT 2022\n",
    "authors": [
      "Yury Makarychev",
      "Naren Sarayu Manoj",
      "Max Ovsiankin"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.07250"
  },
  {
    "id": "arXiv:2206.07253",
    "title": "TeKo: Text-Rich Graph Neural Networks with External Knowledge",
    "abstract": "Graph Neural Networks (GNNs) have gained great popularity in tackling various\nanalytical tasks on graph-structured data (i.e., networks). Typical GNNs and\ntheir variants follow a message-passing manner that obtains network\nrepresentations by the feature propagation process along network topology,\nwhich however ignore the rich textual semantics (e.g., local word-sequence)\nthat exist in many real-world networks. Existing methods for text-rich networks\nintegrate textual semantics by mainly utilizing internal information such as\ntopics or phrases/words, which often suffer from an inability to\ncomprehensively mine the text semantics, limiting the reciprocal guidance\nbetween network structure and text semantics. To address these problems, we\npropose a novel text-rich graph neural network with external knowledge (TeKo),\nin order to take full advantage of both structural and textual information\nwithin text-rich networks. Specifically, we first present a flexible\nheterogeneous semantic network that incorporates high-quality entities and\ninteractions among documents and entities. We then introduce two types of\nexternal knowledge, that is, structured triplets and unstructured entity\ndescription, to gain a deeper insight into textual semantics. We further design\na reciprocal convolutional mechanism for the constructed heterogeneous semantic\nnetwork, enabling network structure and textual semantics to collaboratively\nenhance each other and learn high-level network representations. Extensive\nexperimental results on four public text-rich networks as well as a large-scale\ne-commerce searching dataset illustrate the superior performance of TeKo over\nstate-of-the-art baselines.",
    "descriptor": "",
    "authors": [
      "Zhizhi Yu",
      "Di Jin",
      "Jianguo Wei",
      "Ziyang Liu",
      "Yue Shang",
      "Yun Xiao",
      "Jiawei Han",
      "Lingfei Wu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07253"
  },
  {
    "id": "arXiv:2206.07255",
    "title": "GRAM-HD: 3D-Consistent Image Generation at High Resolution with  Generative Radiance Manifolds",
    "abstract": "Recent works have shown that 3D-aware GANs trained on unstructured single\nimage collections can generate multiview images of novel instances. The key\nunderpinnings to achieve this are a 3D radiance field generator and a volume\nrendering process. However, existing methods either cannot generate\nhigh-resolution images (e.g., up to 256X256) due to the high computation cost\nof neural volume rendering, or rely on 2D CNNs for image-space upsampling which\njeopardizes the 3D consistency across different views. This paper proposes a\nnovel 3D-aware GAN that can generate high resolution images (up to 1024X1024)\nwhile keeping strict 3D consistency as in volume rendering. Our motivation is\nto achieve super-resolution directly in the 3D space to preserve 3D\nconsistency. We avoid the otherwise prohibitively-expensive computation cost by\napplying 2D convolutions on a set of 2D radiance manifolds defined in the\nrecent generative radiance manifold (GRAM) approach, and apply dedicated loss\nfunctions for effective GAN training at high resolution. Experiments on FFHQ\nand AFHQv2 datasets show that our method can produce high-quality 3D-consistent\nresults that significantly outperform existing methods.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Jianfeng Xiang",
      "Jiaolong Yang",
      "Yu Deng",
      "Xin Tong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07255"
  },
  {
    "id": "arXiv:2206.07258",
    "title": "CLNode: Curriculum Learning for Node Classification",
    "abstract": "Node classification is a fundamental graph-based task that aims to predict\nthe classes of unlabeled nodes, for which Graph Neural Networks (GNNs) are the\nstate-of-the-art methods. In current GNNs, training nodes (or training samples)\nare treated equally throughout training. The quality of the samples, however,\nvaries greatly according to the graph structure. Consequently, the performance\nof GNNs could be harmed by two types of low-quality samples: (1) Inter-class\nnodes situated near class boundaries that connect neighboring classes. These\nnodes' representations lack the typical characteristics of their corresponding\nclasses. Because GNNs are data-driven approaches, training on these nodes could\ndegrade the accuracy. (2) Mislabeled nodes. In real-world graphs, nodes are\noften mislabeled, which can significantly degrade the robustness of GNNs. To\nmitigate the detrimental effect of the low-quality samples, we present CLNode\n(Curriculum Learning for Node Classification), which automatically adjusts the\nweights of samples during training based on their quality. Specifically, we\nfirst design a neighborhood-based difficulty measurer to accurately measure the\nquality of samples. Subsequently, based on these measurements, we employ a\ntraining scheduler to adjust the sample weights in each training epoch. To\nevaluate the effectiveness of CLNode, we conduct extensive experiments by\napplying it to four representative backbone GNNs. Experimental results on six\nreal-world networks demonstrate that CLNode is a general framework that can be\ncombined with various GNNs to improve their accuracy and robustness.",
    "descriptor": "",
    "authors": [
      "Xiaowen Wei",
      "Weiwei Liu",
      "Yibing Zhan",
      "Du Bo",
      "Wenbin Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07258"
  },
  {
    "id": "arXiv:2206.07259",
    "title": "Self-Supervised Learning of Image Scale and Orientation",
    "abstract": "We study the problem of learning to assign a characteristic pose, i.e., scale\nand orientation, for an image region of interest. Despite its apparent\nsimplicity, the problem is non-trivial; it is hard to obtain a large-scale set\nof image regions with explicit pose annotations that a model directly learns\nfrom. To tackle the issue, we propose a self-supervised learning framework with\na histogram alignment technique. It generates pairs of image patches by random\nrescaling/rotating and then train an estimator to predict their\nscale/orientation values so that their relative difference is consistent with\nthe rescaling/rotating used. The estimator learns to predict a non-parametric\nhistogram distribution of scale/orientation without any supervision.\nExperiments show that it significantly outperforms previous methods in\nscale/orientation estimation and also improves image matching and 6 DoF camera\npose estimation by incorporating our patch poses into a matching process.",
    "descriptor": "\nComments: Presented in BMVC 2021, code is available on this https URL\n",
    "authors": [
      "Jongmin Lee",
      "Yoonwoo Jeong",
      "Minsu Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07259"
  },
  {
    "id": "arXiv:2206.07260",
    "title": "On Enforcing Better Conditioned Meta-Learning for Rapid Few-Shot  Adaptation",
    "abstract": "Inspired by the concept of preconditioning, we propose a novel method to\nincrease adaptation speed for gradient-based meta-learning methods without\nincurring extra parameters. We demonstrate that recasting the optimization\nproblem to a non-linear least-squares formulation provides a principled way to\nactively enforce a $\\textit{well-conditioned}$ parameter space for\nmeta-learning models based on the concepts of the condition number and local\ncurvature. Our comprehensive evaluations show that the proposed method\nsignificantly outperforms its unconstrained counterpart especially during\ninitial adaptation steps, while achieving comparable or better overall results\non several few-shot classification tasks -- creating the possibility of\ndynamically choosing the number of adaptation steps at inference time.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Markus Hiller",
      "Mehrtash Harandi",
      "Tom Drummond"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07260"
  },
  {
    "id": "arXiv:2206.07264",
    "title": "Born for Auto-Tagging: Faster and better with new objective functions",
    "abstract": "Keyword extraction is a task of text mining. It is applied to increase search\nvolume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass\nscale of online articles and photos efficiently and accurately. BAT is invented\nfor auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP\nnot only provides service as a customized recommender system but also increases\nthe converting rate in E-commerce. The strength of BAT converges faster and\nbetter than other SOTA models, as its 4-layer structure achieves the best F\nscores at 50 epochs. In other words, it performs better than other models which\nrequire deeper layers at 100 epochs. To generate rich and clean tags, awoo\ncreates new objective functions to maintain similar ${\\rm F_1}$ scores with\ncross-entropy while enhancing ${\\rm F_2}$ scores simultaneously. To assure the\neven better performance of F scores awoo revamps the learning rate strategy\nproposed by Transformer \\cite{Transformer} to increase ${\\rm F_1}$ and ${\\rm\nF_2}$ scores at the same time.",
    "descriptor": "",
    "authors": [
      "Chiung-ju Liu",
      "Huang-Ting Shieh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07264"
  },
  {
    "id": "arXiv:2206.07266",
    "title": "Deployment of AGRI-BOT in Greenhouse Administration",
    "abstract": "Modern agriculture is constantly evolving to increase production despite\nunfavorable environmental conditions. A promising approach is 'greenhouse\ncultivation' providing a microclimate to the cultivated plants to overcome\nunfavorable climate. However, massive-sized greenhouses develop non-uniform\nmicro-climate throughout the complex requiring high degree of human\nsupervision. We propose deploying an Agri-Bot to create and maintain positive\necological conditions in the greenhouse, reducing labor costs and increasing\nproduction. The prototype will contain two primary systems, the navigation\nsystem and the data analytics system. The navigation system will be controlled\nby an Arduino, and data analytics will be handled using an ESP8266 microchip.\nNumerous sensors for measuring the greenhouse parameters will be mounted on the\nrobot. It will follow a predefined path, while taking readings at checkpoints.\nThe microchip will collect and process data from sensors, transmit to the\ncloud, and give commands to the actuators. The soil and climate parameters like\ntemperature, humidity, light intensity, soil moisture, pH will be measured\nperiodically. When the parameters are not within a specified range, the\nAgri-Bot will take corrective actions like switching on blowers/heaters,\nstarting irrigation etc. If external intervention is required, eg., fertilizer,\nit will indicate accordingly. Deploying such an Agri-Bot for monitoring and\ncontrolling microclimate in large-scale greenhouses can mitigate labor costs\nwhile increasing productivity. In spite of an initial cost, it can provide a\nhigh return on investment by providing flexibility, low power consumption and\neasy management to help greenhouse be water efficient, provide evenly dispersed\nand controlled sunlight intensity, temperature and humidity.",
    "descriptor": "\nComments: Presented at Eureka Hackathon, India\n",
    "authors": [
      "Ruchita Bhadre",
      "Prathamesh Yeole"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.07266"
  },
  {
    "id": "arXiv:2206.07267",
    "title": "Rethinking Generalization in Few-Shot Classification",
    "abstract": "Single image-level annotations only correctly describe an often small subset\nof an image's content, particularly when complex real-world scenes are\ndepicted. While this might be acceptable in many classification scenarios, it\nposes a significant challenge for applications where the set of classes differs\nsignificantly between training and test time. In this paper, we take a closer\nlook at the implications in the context of $\\textit{few-shot learning}$.\nSplitting the input samples into patches and encoding these via the help of\nVision Transformers allows us to establish semantic correspondences between\nlocal regions across images and independent of their respective class. The most\ninformative patch embeddings for the task at hand are then determined as a\nfunction of the support set via online optimization at inference time,\nadditionally providing visual interpretability of `$\\textit{what matters\nmost}$' in the image. We build on recent advances in unsupervised training of\nnetworks via masked image modelling to overcome the lack of fine-grained labels\nand learn the more general statistical structure of the data while avoiding\nnegative image-level annotation influence, $\\textit{aka}$ supervision collapse.\nExperimental results show the competitiveness of our approach, achieving new\nstate-of-the-art results on four popular few-shot classification benchmarks for\n$5$-shot and $1$-shot scenarios.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Markus Hiller",
      "Rongkai Ma",
      "Mehrtash Harandi",
      "Tom Drummond"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07267"
  },
  {
    "id": "arXiv:2206.07269",
    "title": "Resource-Constrained Edge AI with Early Exit Prediction",
    "abstract": "By leveraging the data sample diversity, the early-exit network recently\nemerges as a prominent neural network architecture to accelerate the deep\nlearning inference process. However, intermediate classifiers of the early\nexits introduce additional computation overhead, which is unfavorable for\nresource-constrained edge artificial intelligence (AI). In this paper, we\npropose an early exit prediction mechanism to reduce the on-device computation\noverhead in a device-edge co-inference system supported by early-exit networks.\nSpecifically, we design a low-complexity module, namely the Exit Predictor, to\nguide some distinctly \"hard\" samples to bypass the computation of the early\nexits. Besides, considering the varying communication bandwidth, we extend the\nearly exit prediction mechanism for latency-aware edge inference, which adapts\nthe prediction thresholds of the Exit Predictor and the confidence thresholds\nof the early-exit network via a few simple regression models. Extensive\nexperiment results demonstrate the effectiveness of the Exit Predictor in\nachieving a better tradeoff between accuracy and on-device computation overhead\nfor early-exit networks. Besides, compared with the baseline methods, the\nproposed method for latency-aware edge inference attains higher inference\naccuracy under different bandwidth conditions.",
    "descriptor": "\nComments: 25 pages, 16 figures, 6 tables. This paper is accepted by Journal of Communications and Information Networks\n",
    "authors": [
      "Rongkang Dong",
      "Yuyi Mao",
      "Jun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07269"
  },
  {
    "id": "arXiv:2206.07271",
    "title": "Human Heuristics for AI-Generated Language Are Flawed",
    "abstract": "Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems produce smart replies,\nautocompletes, and translations. AI-generated language is often not identified\nas such but poses as human language, raising concerns about novel forms of\ndeception and manipulation. Here, we study how humans discern whether one of\nthe most personal and consequential forms of language - a self-presentation -\nwas generated by AI. Across six experiments, participants (N = 4,650) tried to\nidentify self-presentations generated by state-of-the-art language models.\nAcross professional, hospitality, and romantic settings, we find that humans\nare unable to identify AI-generated self-presentations. Combining qualitative\nanalyses with language feature engineering, we find that human judgments of\nAI-generated language are handicapped by intuitive but flawed heuristics such\nas associating first-person pronouns, authentic words, or family topics with\nhumanity. We show that these heuristics make human judgment of generated\nlanguage predictable and manipulable, allowing AI systems to produce language\nperceived as more human than human. We conclude by discussing solutions - such\nas AI accents or fair use policies - to reduce the deceptive potential of\ngenerated language, limiting the subversion of human intuition.",
    "descriptor": "",
    "authors": [
      "Maurice Jakesch",
      "Jeffrey Hancock",
      "Mor Naaman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.07271"
  },
  {
    "id": "arXiv:2206.07272",
    "title": "Machine vision for vial positioning detection toward the safe automation  of material synthesis",
    "abstract": "Although robot-based automation in chemistry laboratories can accelerate the\nmaterial development process, surveillance-free environments may lead to\ndangerous accidents primarily due to machine control errors. Object detection\ntechniques can play vital roles in addressing these safety issues; however,\nstate-of-the-art detectors, including single-shot detector (SSD) models, suffer\nfrom insufficient accuracy in environments involving complex and noisy scenes.\nWith the aim of improving safety in a surveillance-free laboratory, we report a\nnovel deep learning (DL)-based object detector, namely, DenseSSD. For the\nforemost and frequent problem of detecting vial positions, DenseSSD achieved a\nmean average precision (mAP) over 95% based on a complex dataset involving both\nempty and solution-filled vials, greatly exceeding those of conventional\ndetectors; such high precision is critical to minimizing failure-induced\naccidents. Additionally, DenseSSD was observed to be highly insensitive to the\nenvironmental changes, maintaining its high precision under the variations of\nsolution colors or testing view angles. The robustness of DenseSSD would allow\nthe utilized equipment settings to be more flexible. This work demonstrates\nthat DenseSSD is useful for enhancing safety in an automated material synthesis\nenvironment, and it can be extended to various applications where high\ndetection accuracy and speed are both needed.",
    "descriptor": "",
    "authors": [
      "Leslie Ching Ow Tiong",
      "Hyuk Jun Yoo",
      "Na Yeon Kim",
      "Kwan-Young Lee",
      "Sang Soo Han",
      "Donghun Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.07272"
  },
  {
    "id": "arXiv:2206.07276",
    "title": "Two-Timescale Optimization for Intelligent Reflecting Surface-Assisted  MIMO Transmission in Fast-Changing Channels",
    "abstract": "The application of intelligent reflecting surface (IRS) depends on the\nknowledge of channel state information (CSI), and has been hindered by the\nheavy overhead of channel training, estimation, and feedback in fast-changing\nchannels. This paper presents a new two-timescale beamforming approach to\nmaximizing the average achievable rate (AAR) of IRS-assisted MIMO systems,\nwhere the IRS is configured relatively infrequently based on statistical CSI\n(S-CSI) and the base station precoder and power allocation are updated\nfrequently based on quickly outdated instantaneous CSI (I-CSI). The key idea is\nthat we first reveal the optimal small-timescale power allocation based on\noutdated I-CSI yields a water-filling structure. Given the optimal power\nallocation, a new mini-batch sampling (mbs)- based particle swarm optimization\n(PSO) algorithm is developed to optimize the large-timescale IRS configuration\nwith reduced channel samples. Another important aspect is that we develop a\nmodel-driven PSO algorithm to optimize the IRS configuration, which maximizes a\nlower bound of the AAR by only using the S-CSI and eliminates the need of\nchannel samples. The modeldriven PSO serves as a dependable lower bound for the\nmbs-PSO. Simulations corroborate the superiority of the new two-timescale\nbeamforming strategy to its alternatives in terms of the AAR and efficiency,\nwith the benefits of the IRS demonstrated.",
    "descriptor": "\nComments: 15 pages, 11 figures, Accepted by IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Yashuai Cao",
      "Tiejun Lv",
      "Wei Ni"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.07276"
  },
  {
    "id": "arXiv:2206.07277",
    "title": "ALASCA: Rethinking Label Smoothing for Deep Learning Under Label Noise",
    "abstract": "As label noise, one of the most popular distribution shifts, severely\ndegrades deep neural networks' generalization performance, robust training with\nnoisy labels is becoming an important task in modern deep learning. In this\npaper, we propose our framework, coined as Adaptive LAbel smoothing on\nSub-ClAssifier (ALASCA), that provides a robust feature extractor with\ntheoretical guarantee and negligible additional computation. First, we derive\nthat the label smoothing (LS) incurs implicit Lipschitz regularization (LR).\nFurthermore, based on these derivations, we apply the adaptive LS (ALS) on\nsub-classifiers architectures for the practical application of adaptive LR on\nintermediate layers. We conduct extensive experiments for ALASCA and combine it\nwith previous noise-robust methods on several datasets and show our framework\nconsistently outperforms corresponding baselines.",
    "descriptor": "\nComments: ICML Workshop on Principles of Distribution Shift 2022\n",
    "authors": [
      "Jongwoo Ko",
      "Bongsoo Yi",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07277"
  },
  {
    "id": "arXiv:2206.07278",
    "title": "Nebula Graph: An open source distributed graph database",
    "abstract": "This paper introduces the recent work of Nebula Graph, an open-source,\ndistributed, scalable, and native graph database. We present a system design\ntrade-off and a comprehensive overview of Nebula Graph internals, including\ngraph data models, partitioning strategies, secondary indexes, optimizer rules,\nstorage-side transactions, graph query languages, observability, graph\nprocessing frameworks, and visualization tool-kits. In addition, three sets of\nlarge-scale graph b",
    "descriptor": "",
    "authors": [
      "Min Wu",
      "Xinglu Yi",
      "Hui Yu",
      "Yu Liu",
      "Yujue Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.07278"
  },
  {
    "id": "arXiv:2206.07279",
    "title": "Global Convergence of Federated Learning for Mixed Regression",
    "abstract": "This paper studies the problem of model training under Federated Learning\nwhen clients exhibit cluster structure. We contextualize this problem in mixed\nregression, where each client has limited local data generated from one of $k$\nunknown regression models. We design an algorithm that achieves global\nconvergence from any initialization, and works even when local data volume is\nhighly unbalanced -- there could exist clients that contain $O(1)$ data points\nonly. Our algorithm first runs moment descent on a few anchor clients (each\nwith $\\tilde{\\Omega}(k)$ data points) to obtain coarse model estimates. Then\neach client alternately estimates its cluster labels and refines the model\nestimates based on FedAvg or FedProx. A key innovation in our analysis is a\nuniform estimate on the clustering errors, which we prove by bounding the VC\ndimension of general polynomial concept classes based on the theory of\nalgebraic geometry.",
    "descriptor": "",
    "authors": [
      "Lili Su",
      "Jiaming Xu",
      "Pengkun Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07279"
  },
  {
    "id": "arXiv:2206.07282",
    "title": "Human Eyes Inspired Recurrent Neural Networks are More Robust Against  Adversarial Noises",
    "abstract": "Compared to human vision, computer vision based on convolutional neural\nnetworks (CNN) are more vulnerable to adversarial noises. This difference is\nlikely attributable to how the eyes sample visual input and how the brain\nprocesses retinal samples through its dorsal and ventral visual pathways, which\nare under-explored for computer vision. Inspired by the brain, we design\nrecurrent neural networks, including an input sampler that mimics the human\nretina, a dorsal network that guides where to look next, and a ventral network\nthat represents the retinal samples. Taking these modules together, the models\nlearn to take multiple glances at an image, attend to a salient part at each\nglance, and accumulate the representation over time to recognize the image. We\ntest such models for their robustness against a varying level of adversarial\nnoises with a special focus on the effect of different input sampling\nstrategies. Our findings suggest that retinal foveation and sampling renders a\nmodel more robust against adversarial noises, and the model may correct itself\nfrom an attack when it is given a longer time to take more glances at an image.\nIn conclusion, robust visual recognition can benefit from the combined use of\nthree brain-inspired mechanisms: retinal transformation, attention guided eye\nmovement, and recurrent processing, as opposed to feedforward-only CNNs.",
    "descriptor": "",
    "authors": [
      "Minkyu Choi",
      "Yizhen Zhang",
      "Kuan Han",
      "Xiaokai Wang",
      "Zhongming Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07282"
  },
  {
    "id": "arXiv:2206.07284",
    "title": "A Survey on Gradient Inversion: Attacks, Defenses and Future Directions",
    "abstract": "Recent studies have shown that the training samples can be recovered from\ngradients, which are called Gradient Inversion (GradInv) attacks. However,\nthere remains a lack of extensive surveys covering recent advances and thorough\nanalysis of this issue. In this paper, we present a comprehensive survey on\nGradInv, aiming to summarize the cutting-edge research and broaden the horizons\nfor different domains. Firstly, we propose a taxonomy of GradInv attacks by\ncharacterizing existing attacks into two paradigms: iteration- and\nrecursion-based attacks. In particular, we dig out some critical ingredients\nfrom the iteration-based attacks, including data initialization, model training\nand gradient matching. Second, we summarize emerging defense strategies against\nGradInv attacks. We find these approaches focus on three perspectives covering\ndata obscuration, model improvement and gradient protection. Finally, we\ndiscuss some promising directions and open problems for further research.",
    "descriptor": "\nComments: Accepted by IJCAI-ECAI 2022\n",
    "authors": [
      "Rui Zhang",
      "Song Guo",
      "Junxiao Wang",
      "Xin Xie",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07284"
  },
  {
    "id": "arXiv:2206.07286",
    "title": "Faster Decomposition of Weighted Graphs into Cliques using Fisher's  Inequality",
    "abstract": "Mining groups of genes that consistently co-express is an important problem\nin biomedical research, where it is critical for applications such as\ndrug-repositioning and designing new disease treatments. Recently, Cooley et\nal. modeled this problem as Exact Weighted Clique Decomposition (EWCD) in\nwhich, given an edge-weighted graph $G$ and a positive integer $k$, the goal is\nto decompose $G$ into at most $k$ (overlapping) weighted cliques so that an\nedge's weight is exactly equal to the sum of weights for cliques it\nparticipates in. They show EWCD is fixed-parameter-tractable, giving a\n$4^k$-kernel alongside a backtracking algorithm (together called cricca) to\niteratively build a decomposition. Unfortunately, because of inherent\nexponential growth in the space of potential solutions, cricca is typically\nable to decompose graphs only when $k \\leq 11$.\nIn this work, we establish reduction rules that exponentially decrease the\nsize of the kernel (from $4^k$ to $k2^k$) for EWCD. In addition, we use\ninsights about the structure of potential solutions to give new search rules\nthat speed up the decomposition algorithm. At the core of our techniques is a\nresult from combinatorial design theory called Fisher's inequality\ncharacterizing set systems with restricted intersections. We deploy our\nkernelization and decomposition algorithms (together called DeCAF) on a corpus\nof biologically-inspired data and obtain over two orders of magnitude speed-up\nover cricca. As a result, DeCAF scales to instances with $k \\geq 17$.",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Shweta Jain",
      "Yo Mizutani",
      "Blair Sullivan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.07286"
  },
  {
    "id": "arXiv:2206.07288",
    "title": "Streaming non-autoregressive model for any-to-many voice conversion",
    "abstract": "Voice conversion models have developed for decades, and current mainstream\nresearch focuses on non-streaming voice conversion. However, streaming voice\nconversion is more suitable for practical application scenarios than\nnon-streaming voice conversion. In this paper, we propose a streaming\nany-to-many voice conversion based on fully non-autoregressive model, which\nincludes a streaming transformer based acoustic model and a streaming vocoder.\nStreaming transformer based acoustic model is composed of a pre-trained encoder\nfrom streaming end-to-end based automatic speech recognition model and a\ndecoder modified on FastSpeech blocks. Streaming vocoder is designed for\nstreaming task with pseudo quadrature mirror filter bank and causal\nconvolution. Experimental results show that the proposed method achieves\nsignificant performance both in latency and conversion quality and can be\nreal-time on CPU and GPU.",
    "descriptor": "",
    "authors": [
      "Ziyi Chen",
      "Haoran Miao",
      "Pengyuan Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07288"
  },
  {
    "id": "arXiv:2206.07289",
    "title": "Text-Aware End-to-end Mispronunciation Detection and Diagnosis",
    "abstract": "Mispronunciation detection and diagnosis (MDD) technology is a key component\nof computer-assisted pronunciation training system (CAPT). In the field of\nassessing the pronunciation quality of constrained speech, the given\ntranscriptions can play the role of a teacher. Conventional methods have fully\nutilized the prior texts for the model construction or improving the system\nperformance, e.g. forced-alignment and extended recognition networks. Recently,\nsome end-to-end based methods attempt to incorporate the prior texts into model\ntraining and preliminarily show the effectiveness. However, previous studies\nmostly consider applying raw attention mechanism to fuse audio representations\nwith text representations, without taking possible text-pronunciation mismatch\ninto account. In this paper, we present a gating strategy that assigns more\nimportance to the relevant audio features while suppressing irrelevant text\ninformation. Moreover, given the transcriptions, we design an extra contrastive\nloss to reduce the gap between the learning objective of phoneme recognition\nand MDD. We conducted experiments using two publicly available datasets (TIMIT\nand L2-Arctic) and our best model improved the F1 score from $57.51\\%$ to\n$61.75\\%$ compared to the baselines. Besides, we provide a detailed analysis to\nshed light on the effectiveness of gating mechanism and contrastive learning on\nMDD.",
    "descriptor": "\nComments: Rejected by Interspeech2022\n",
    "authors": [
      "Linkai Peng",
      "Yingming Gao",
      "Binghuai Lin",
      "Dengfeng Ke",
      "Yanlu Xie",
      "Jinsong Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07289"
  },
  {
    "id": "arXiv:2206.07290",
    "title": "Differentiable Top-k Classification Learning",
    "abstract": "The top-k classification accuracy is one of the core metrics in machine\nlearning. Here, k is conventionally a positive integer, such as 1 or 5, leading\nto top-1 or top-5 training objectives. In this work, we relax this assumption\nand optimize the model for multiple k simultaneously instead of using a single\nk. Leveraging recent advances in differentiable sorting and ranking, we propose\na differentiable top-k cross-entropy classification loss. This allows training\nthe network while not only considering the top-1 prediction, but also, e.g.,\nthe top-2 and top-5 predictions. We evaluate the proposed loss function for\nfine-tuning on state-of-the-art architectures, as well as for training from\nscratch. We find that relaxing k does not only produce better top-5 accuracies,\nbut also leads to top-1 accuracy improvements. When fine-tuning publicly\navailable ImageNet models, we achieve a new state-of-the-art for these models.",
    "descriptor": "\nComments: Published at ICML 2022, Code @ this https URL\n",
    "authors": [
      "Felix Petersen",
      "Hilde Kuehne",
      "Christian Borgelt",
      "Oliver Deussen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07290"
  },
  {
    "id": "arXiv:2206.07293",
    "title": "FRCRN: Boosting Feature Representation using Frequency Recurrence for  Monaural Speech Enhancement",
    "abstract": "Convolutional recurrent networks (CRN) integrating a convolutional\nencoder-decoder (CED) structure and a recurrent structure have achieved\npromising performance for monaural speech enhancement. However, feature\nrepresentation across frequency context is highly constrained due to limited\nreceptive fields in the convolutions of CED. In this paper, we propose a\nconvolutional recurrent encoder-decoder (CRED) structure to boost feature\nrepresentation along the frequency axis. The CRED applies frequency recurrence\non 3D convolutional feature maps along the frequency axis following each\nconvolution, therefore, it is capable of catching long-range frequency\ncorrelations and enhancing feature representations of speech inputs. The\nproposed frequency recurrence is realized efficiently using a feedforward\nsequential memory network (FSMN). Besides the CRED, we insert two stacked FSMN\nlayers between the encoder and the decoder to model further temporal dynamics.\nWe name the proposed framework as Frequency Recurrent CRN (FRCRN). We design\nFRCRN to predict complex Ideal Ratio Mask (cIRM) in complex-valued domain and\noptimize FRCRN using both time-frequency-domain and time-domain losses. Our\nproposed approach achieved state-of-the-art performance on wideband benchmark\ndatasets and achieved 2nd place for the real-time fullband track in terms of\nMean Opinion Score (MOS) and Word Accuracy (WAcc) in the ICASSP 2022 Deep Noise\nSuppression (DNS) challenge.",
    "descriptor": "\nComments: The paper has been accepted by ICASSP 2022. 5 pages, 2 figures, 5 tables\n",
    "authors": [
      "Shengkui Zhao",
      "Bin Ma",
      "Karn N. Watcharasupat",
      "Woon-Seng Gan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07293"
  },
  {
    "id": "arXiv:2206.07295",
    "title": "FOLD-TR: A Scalable and Efficient Inductive Learning Algorithm for  Learning To Rank",
    "abstract": "FOLD-R++ is a new inductive learning algorithm for binary classification\ntasks. It generates an (explainable) normal logic program for mixed type\n(numerical and categorical) data. We present a customized FOLD-R++ algorithm\nwith the ranking framework, called FOLD-TR, that aims to rank new items\nfollowing the ranking pattern in the training data. Like FOLD-R++, the FOLD-TR\nalgorithm is able to handle mixed-type data directly and provide native\njustification to explain the comparison between a pair of items.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2202.06913. text overlap with arXiv:2110.07843\n",
    "authors": [
      "Huaduo Wang",
      "Gopal Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07295"
  },
  {
    "id": "arXiv:2206.07296",
    "title": "Enhanced Knowledge Selection for Grounded Dialogues via Document  Semantic Graphs",
    "abstract": "Providing conversation models with background knowledge has been shown to\nmake open-domain dialogues more informative and engaging. Existing models treat\nknowledge selection as a sentence ranking or classification problem where each\nsentence is handled individually, ignoring the internal semantic connection\namong sentences in the background document. In this work, we propose to\nautomatically convert the background knowledge documents into document semantic\ngraphs and then perform knowledge selection over such graphs. Our document\nsemantic graphs preserve sentence-level information through the use of sentence\nnodes and provide concept connections between sentences. We jointly apply\nmulti-task learning for sentence-level and concept-level knowledge selection\nand show that it improves sentence-level selection. Our experiments show that\nour semantic graph-based knowledge selection improves over sentence selection\nbaselines for both the knowledge selection task and the end-to-end response\ngeneration task on HollE and improves generalization on unseen topics in WoW.",
    "descriptor": "\nComments: NAACL 2022. Please refer to this https URL for code and resources\n",
    "authors": [
      "Sha Li",
      "Madhi Namazifar",
      "Di Jin",
      "Mohit Bansal",
      "Heng Ji",
      "Yang Liu",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07296"
  },
  {
    "id": "arXiv:2206.07298",
    "title": "S\\textsuperscript{2}-FPN: Scale-ware Strip Attention Guided Feature  Pyramid Network for Real-time Semantic Segmentation",
    "abstract": "Modern high-performance semantic segmentation methods employ a heavy backbone\nand dilated convolution to extract the relevant feature. Although extracting\nfeatures with both contextual and semantic information is critical for the\nsegmentation tasks, it brings a memory footprint and high computation cost for\nreal-time applications. This paper presents a new model to achieve a trade-off\nbetween accuracy/speed for real-time road scene semantic segmentation.\nSpecifically, we proposed a lightweight model named Scale-aware Strip Attention\nGuided Feature Pyramid Network (S\\textsuperscript{2}-FPN). Our network consists\nof three main modules: Attention Pyramid Fusion (APF) module, Scale-aware Strip\nAttention Module (SSAM), and Global Feature Upsample (GFU) module. APF adopts\nan attention mechanisms to learn discriminative multi-scale features and help\nclose the semantic gap between different levels. APF uses the scale-aware\nattention to encode global context with vertical stripping operation and models\nthe long-range dependencies, which helps relate pixels with similar semantic\nlabel. In addition, APF employs channel-wise reweighting block (CRB) to\nemphasize the channel features. Finally, the decoder of\nS\\textsuperscript{2}-FPN then adopts GFU, which is used to fuse features from\nAPF and the encoder. Extensive experiments have been conducted on two\nchallenging semantic segmentation benchmarks, which demonstrate that our\napproach achieves better accuracy/speed trade-off with different model\nsettings. The proposed models have achieved a results of 76.2\\%mIoU/87.3FPS,\n77.4\\%mIoU/67FPS, and 77.8\\%mIoU/30.5FPS on Cityscapes dataset, and\n69.6\\%mIoU,71.0\\% mIoU, and 74.2\\% mIoU on Camvid dataset. The code for this\nwork will be made available at \\url{https://github.com/mohamedac29/S2-FPN",
    "descriptor": "",
    "authors": [
      "Mohammed A. M. Elhassan",
      "Chenhui Yang",
      "Chenxi Huang",
      "Tewodros Legesse Munea",
      "Xin Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07298"
  },
  {
    "id": "arXiv:2206.07300",
    "title": "From Outcome-Based to Language-Based Preferences",
    "abstract": "We review the literature on models that try to explain human behavior in\nsocial interactions described by normal-form games with monetary payoffs. We\nstart by covering social and moral preferences. We then focus on the growing\nbody of research showing that people react to the language in which actions are\ndescribed, especially when it activates moral concerns. We conclude by arguing\nthat behavioral economics is in the midst of a paradigm shift towards\nlanguage-based preferences, which will require an exploration of new models and\nexperimental setups.",
    "descriptor": "\nComments: Forthcoming in the Journal of Economic Literature\n",
    "authors": [
      "Valerio Capraro",
      "Joseph Y. Halpern",
      "Matjaz Perc"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07300"
  },
  {
    "id": "arXiv:2206.07303",
    "title": "Energetic Variational Neural Network Discretizations to Gradient Flows",
    "abstract": "We propose structure-preserving neural-network-based numerical schemes to\nsolve both $L^2$-gradient flows and generalized diffusions. In more detail, by\nusing neural networks as tools for spatial discretizations, we introduce a\nstructure-preserving Eulerian algorithm to solve $L^2$-gradient flows and a\nstructure-preserving Lagrangian algorithm to solve generalized diffusions. The\nLagrangian algorithm for the generalized diffusion evolves the \"flow map\" which\ndetermines the dynamics of the generalized diffusion. This avoids computing the\nWasserstein distance between two probability functions, which is non-trivial.\nThe key ideas behind these schemes are to construct numerical discretizations\nbased on the variational formulations of the gradient flows, i.e., the\nenergy-dissipation laws, directly. More precisely, we construct minimizing\nmovement schemes for these two types of gradient flow by introducing temporal\ndiscretization first, which is more efficient and convenient in\nneural-network-based implementations. The variational discretizations ensure\nthe proper energy dissipation in numerical solutions and are crucial for the\nlong-term stability of numerical computation. The neural-network-based spatial\ndiscretization enables us to solve these gradient flows in high dimensions.\nVarious numerical experiments are presented to demonstrate the accuracy and\nenergy stability of the proposed numerical approaches.",
    "descriptor": "\nComments: 24 pages, 7 figures\n",
    "authors": [
      "Ziqing Hu",
      "Chun Liu",
      "Yiwei Wang",
      "Zhiliang Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07303"
  },
  {
    "id": "arXiv:2206.07304",
    "title": "Knowledge Management System with NLP-Assisted Annotations: A Brief  Survey and Outlook",
    "abstract": "Knowledge management systems are in high demand for industrial researchers,\nchemical or research enterprises, or evidence-based decision making. However,\nexisting systems have limitations in categorizing and organizing paper insights\nor relationships. Traditional databases are usually disjoint with logging\nsystems, which limit its utility in generating concise, collated overviews. In\nthis work, we briefly survey existing approaches of this problem space and\npropose a unified framework that utilizes relational databases to log\nhierarchical information to facilitate the research and writing process, or\ngenerate useful knowledge from references or insights from connected concepts.\nThis framework of knowledge management system enables novel functionalities\nencompassing improved hierarchical notetaking, AI-assisted brainstorming, and\nmulti-directional relationships. Potential applications include managing\ninventories and changes for manufacture or research enterprises, or generating\nanalytic reports with evidence-based decision making.",
    "descriptor": "",
    "authors": [
      "Baihan Lin"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07304"
  },
  {
    "id": "arXiv:2206.07307",
    "title": "VCT: A Video Compression Transformer",
    "abstract": "We show how transformers can be used to vastly simplify neural video\ncompression. Previous methods have been relying on an increasing number of\narchitectural biases and priors, including motion prediction and warping\noperations, resulting in complex models. Instead, we independently map input\nframes to representations and use a transformer to model their dependencies,\nletting it predict the distribution of future representations given the past.\nThe resulting video compression transformer outperforms previous methods on\nstandard video compression data sets. Experiments on synthetic data show that\nour model learns to handle complex motion patterns such as panning, blurring\nand fading purely from data. Our approach is easy to implement, and we release\ncode to facilitate future research.",
    "descriptor": "",
    "authors": [
      "Fabian Mentzer",
      "George Toderici",
      "David Minnen",
      "Sung-Jin Hwang",
      "Sergi Caelles",
      "Mario Lucic",
      "Eirikur Agustsson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07307"
  },
  {
    "id": "arXiv:2206.07308",
    "title": "Cost-Aware Exploration for Chiplet-Based Architecture with Advanced  Packaging Technologies",
    "abstract": "The chiplet-based System-in-Package~(SiP) technology enables more design\nflexibility via various inter-chiplet connection and heterogeneous integration.\nHowever, it is not known how to convert such flexibility into cost efficiency,\nwhich is critical when making a design decision. In this paper, we develop an\nanalytical cost model that can estimate the cost of the 2.5D chiplet-based SiP\nsystems under various interconnection options and technology nodes. We\nconducted two case studies using our cost model to explore the cost\ncharacteristics of the 2.5D chiplet-based SiP system. Based on the case\nstudies, we made several observations on the interposer selection, design\npartition granularity, and technology node adoption for cost-efficient\nchiplet-based SiP design.",
    "descriptor": "",
    "authors": [
      "Tianqi Tang",
      "Yuan Xie"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.07308"
  },
  {
    "id": "arXiv:2206.07309",
    "title": "Estimating the Optimal Covariance with Imperfect Mean in Diffusion  Probabilistic Models",
    "abstract": "Diffusion probabilistic models (DPMs) are a class of powerful deep generative\nmodels (DGMs). Despite their success, the iterative generation process over the\nfull timesteps is much less efficient than other DGMs such as GANs. Thus, the\ngeneration performance on a subset of timesteps is crucial, which is greatly\ninfluenced by the covariance design in DPMs. In this work, we consider diagonal\nand full covariances to improve the expressive power of DPMs. We derive the\noptimal result for such covariances, and then correct it when the mean of DPMs\nis imperfect. Both the optimal and the corrected ones can be decomposed into\nterms of conditional expectations over functions of noise. Building upon it, we\npropose to estimate the optimal covariance and its correction given imperfect\nmean by learning these conditional expectations. Our method can be applied to\nDPMs with both discrete and continuous timesteps. We consider the diagonal\ncovariance in our implementation for computational efficiency. For an efficient\npractical implementation, we adopt a parameter sharing scheme and a two-stage\ntraining process. Empirically, our method outperforms a wide variety of\ncovariance design on likelihood results, and improves the sample quality\nespecially on a small number of timesteps.",
    "descriptor": "\nComments: Accepted in ICML 2022\n",
    "authors": [
      "Fan Bao",
      "Chongxuan Li",
      "Jiacheng Sun",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07309"
  },
  {
    "id": "arXiv:2206.07311",
    "title": "Can pruning improve certified robustness of neural networks?",
    "abstract": "With the rapid development of deep learning, the sizes of neural networks\nbecome larger and larger so that the training and inference often overwhelm the\nhardware resources. Given the fact that neural networks are often\nover-parameterized, one effective way to reduce such computational overhead is\nneural network pruning, by removing redundant parameters from trained neural\nnetworks. It has been recently observed that pruning can not only reduce\ncomputational overhead but also can improve empirical robustness of deep neural\nnetworks (NNs), potentially owing to removing spurious correlations while\npreserving the predictive accuracies. This paper for the first time\ndemonstrates that pruning can generally improve certified robustness for\nReLU-based NNs under the complete verification setting. Using the popular\nBranch-and-Bound (BaB) framework, we find that pruning can enhance the\nestimated bound tightness of certified robustness verification, by alleviating\nlinear relaxation and sub-domain split problems. We empirically verify our\nfindings with off-the-shelf pruning methods and further present a new\nstability-based pruning method tailored for reducing neuron instability, that\noutperforms existing pruning methods in enhancing certified robustness. Our\nexperiments show that by appropriately pruning an NN, its certified accuracy\ncan be boosted up to 8.2% under standard training, and up to 24.5% under\nadversarial training on the CIFAR10 dataset. We additionally observe the\nexistence of certified lottery tickets that can match both standard and\ncertified robust accuracies of the original dense models across different\ndatasets. Our findings offer a new angle to study the intriguing interaction\nbetween sparsity and robustness, i.e. interpreting the interaction of sparsity\nand certified robustness via neuron stability. Codes are available at:\nhttps://github.com/VITA-Group/CertifiedPruning.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2011.13824 by other authors\n",
    "authors": [
      "Zhangheng Li",
      "Tianlong Chen",
      "Linyi Li",
      "Bo Li",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07311"
  },
  {
    "id": "arXiv:2206.07314",
    "title": "Fast and Reliable Evaluation of Adversarial Robustness with  Minimum-Margin Attack",
    "abstract": "The AutoAttack (AA) has been the most reliable method to evaluate adversarial\nrobustness when considerable computational resources are available. However,\nthe high computational cost (e.g., 100 times more than that of the project\ngradient descent attack) makes AA infeasible for practitioners with limited\ncomputational resources, and also hinders applications of AA in the adversarial\ntraining (AT). In this paper, we propose a novel method, minimum-margin (MM)\nattack, to fast and reliably evaluate adversarial robustness. Compared with AA,\nour method achieves comparable performance but only costs 3% of the\ncomputational time in extensive experiments. The reliability of our method lies\nin that we evaluate the quality of adversarial examples using the margin\nbetween two targets that can precisely identify the most adversarial example.\nThe computational efficiency of our method lies in an effective Sequential\nTArget Ranking Selection (STARS) method, ensuring that the cost of the MM\nattack is independent of the number of classes. The MM attack opens a new way\nfor evaluating adversarial robustness and provides a feasible and reliable way\nto generate high-quality adversarial examples in AT.",
    "descriptor": "",
    "authors": [
      "Ruize Gao",
      "Jiongxiao Wang",
      "Kaiwen Zhou",
      "Feng Liu",
      "Binghui Xie",
      "Gang Niu",
      "Bo Han",
      "James Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07314"
  },
  {
    "id": "arXiv:2206.07316",
    "title": "Online Contextual Decision-Making with a Smart Predict-then-Optimize  Method",
    "abstract": "We study an online contextual decision-making problem with resource\nconstraints. At each time period, the decision-maker first predicts a reward\nvector and resource consumption matrix based on a given context vector and then\nsolves a downstream optimization problem to make a decision. The final goal of\nthe decision-maker is to maximize the summation of the reward and the utility\nfrom resource consumption, while satisfying the resource constraints. We\npropose an algorithm that mixes a prediction step based on the \"Smart\nPredict-then-Optimize (SPO)\" method with a dual update step based on mirror\ndescent. We prove regret bounds and demonstrate that the overall convergence\nrate of our method depends on the $\\mathcal{O}(T^{-1/2})$ convergence of online\nmirror descent as well as risk bounds of the surrogate loss function used to\nlearn the prediction model. Our algorithm and regret bounds apply to a general\nconvex feasible region for the resource constraints, including both hard and\nsoft resource constraint cases, and they apply to a wide class of prediction\nmodels in contrast to the traditional settings of linear contextual models or\nfinite policy spaces. We also conduct numerical experiments to empirically\ndemonstrate the strength of our proposed SPO-type methods, as compared to\ntraditional prediction-error-only methods, on multi-dimensional knapsack and\nlongest path instances.",
    "descriptor": "",
    "authors": [
      "Heyuan Liu",
      "Paul Grigas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07316"
  },
  {
    "id": "arXiv:2206.07318",
    "title": "CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by  leveraging multilingual data",
    "abstract": "Identifying named entities is, in general, a practical and challenging task\nin the field of Natural Language Processing. Named Entity Recognition on the\ncode-mixed text is further challenging due to the linguistic complexity\nresulting from the nature of the mixing. This paper addresses the submission of\nteam CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER\ntask aimed to identify named entities on the code-mixed dataset. Our work\nconsists of Named Entity Recognition (NER) on the code-mixed dataset by\nleveraging the multilingual data. We achieved a weighted average F1 score of\n0.7044, i.e., 6% greater than the baseline.",
    "descriptor": "\nComments: SemEval 2022 Task 11: MultiCoNER Multilingual Complex Named Entity Recognition, NAACL, 2022\n",
    "authors": [
      "Suman Dowlagar",
      "Radhika Mamidi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07318"
  },
  {
    "id": "arXiv:2206.07319",
    "title": "Toward the smooth mesh climbing of a miniature robot using bioinspired  soft and expandable claws",
    "abstract": "While most micro-robots face difficulty traveling on rugged and uneven\nterrain, beetles can walk smoothly on the complex substrate without slipping or\ngetting stuck on the surface due to their stiffness-variable tarsi and\nexpandable hooks on the tip of tarsi. In this study, we found that beetles\nactively bent and expanded their claws regularly to crawl freely on mesh\nsurfaces. Inspired by the crawling mechanism of the beetles, we designed an\n8-cm miniature climbing robot equipping artificial claws to open and bend in\nthe same cyclic manner as natural beetles. The robot can climb freely with a\ncontrollable gait on the mesh surface, steep incline of the angle of 60{\\deg},\nand even transition surface. To our best knowledge, this is the first\nmicro-scale robot that can climb both the mesh surface and cliffy incline.",
    "descriptor": "",
    "authors": [
      "Hong Wang",
      "Peng Liu",
      "Phuoc Thanh Tran Ngoc",
      "Bing Li",
      "Yao Li",
      "Hirotaka Sato"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07319"
  },
  {
    "id": "arXiv:2206.07321",
    "title": "Morphence-2.0: Evasion-Resilient Moving Target Defense Powered by  Out-of-Distribution Detection",
    "abstract": "Evasion attacks against machine learning models often succeed via iterative\nprobing of a fixed target model, whereby an attack that succeeds once will\nsucceed repeatedly. One promising approach to counter this threat is making a\nmodel a moving target against adversarial inputs. To this end, we introduce\nMorphence-2.0, a scalable moving target defense (MTD) powered by\nout-of-distribution (OOD) detection to defend against adversarial examples. By\nregularly moving the decision function of a model, Morphence-2.0 makes it\nsignificantly challenging for repeated or correlated attacks to succeed.\nMorphence-2.0 deploys a pool of models generated from a base model in a manner\nthat introduces sufficient randomness when it responds to prediction queries.\nVia OOD detection, Morphence-2.0 is equipped with a scheduling approach that\nassigns adversarial examples to robust decision functions and benign samples to\nan undefended accurate models. To ensure repeated or correlated attacks fail,\nthe deployed pool of models automatically expires after a query budget is\nreached and the model pool is seamlessly replaced by a new model pool generated\nin advance. We evaluate Morphence-2.0 on two benchmark image classification\ndatasets (MNIST and CIFAR10) against 4 reference attacks (3 white-box and 1\nblack-box). Morphence-2.0 consistently outperforms prior defenses while\npreserving accuracy on clean data and reducing attack transferability. We also\nshow that, when powered by OOD detection, Morphence-2.0 is able to precisely\nmake an input-based movement of the model's decision function that leads to\nhigher prediction accuracy on both adversarial and benign queries.",
    "descriptor": "\nComments: 13 pages, 6 figures, 2 tables. arXiv admin note: substantial text overlap with arXiv:2108.13952\n",
    "authors": [
      "Abderrahmen Amich",
      "Ata Kaboudi",
      "Birhanu Eshete"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07321"
  },
  {
    "id": "arXiv:2206.07323",
    "title": "A Continuous Risk Assessment Methodology for Cloud Infrastructures",
    "abstract": "Cloud systems are dynamic environments which make it difficult to keep track\nof security risks that resources are exposed to. Traditionally, risk assessment\nis conducted for individual assets to evaluate existing threats; their results,\nhowever, are quickly outdated in such a dynamic environment. In this paper, we\npropose an adaptation of the traditional risk assessment methodology for cloud\ninfrastructures which loosely couples manual, in-depth analyses with\ncontinuous, automatic application of their results. These two parts are linked\nby a novel threat profile definition that allows to reusably describe\nconfiguration weaknesses based on properties that are common across assets and\ncloud providers. This way, threats can be identified automatically for all\nresources that exhibit the same properties, including new and modified ones. We\nalso present a prototype implementation which automatically evaluates an\ninfrastructure as code template of a cloud system against a set of threat\nprofiles, and we evaluate its performance. Our methodology not only enables\norganizations to reuse their threat analysis results, but also to collaborate\non their development, e.g. with the public community. To that end, we propose\nan initial open-source repository of threat profiles.",
    "descriptor": "",
    "authors": [
      "Immanuel Kunz",
      "Angelika Schneider",
      "Christian Banse"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07323"
  },
  {
    "id": "arXiv:2206.07325",
    "title": "Second order stabilized semi-implicit scheme for the Cahn-Hilliard model  with dynamic boundary conditions",
    "abstract": "We study the numerical algorithm and error analysis for the Cahn-Hilliard\nequation with dynamic boundary conditions. A second-order in time, linear and\nenergy stable scheme is proposed, which is an extension of the first-order\nstabilized approach. The corresponding energy stability and convergence\nanalysis of the scheme are derived theoretically. Some numerical experiments\nare performed to verify the effectiveness and accuracy of the second-order\nnumerical scheme, including numerical simulations under various initial\nconditions and energy potential functions, and comparisons with the literature\nworks.",
    "descriptor": "",
    "authors": [
      "Xiangjun Meng",
      "Xuelian Bao",
      "Zhengru Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07325"
  },
  {
    "id": "arXiv:2206.07326",
    "title": "Recent Advances in Scene Image Representation and Classification",
    "abstract": "With the rise of deep learning algorithms nowadays, scene image\nrepresentation methods on big data (e.g., SUN-397) have achieved a significant\nperformance boost in classification. However, the performance is still limited\nbecause the scene images are mostly complex in nature having higher intra-class\ndissimilarity and inter-class similarity problems. To deal with such problems,\nthere are several methods proposed in the literature with their own advantages\nand limitations. A detailed study of previous works is necessary to understand\ntheir pros and cons in image representation and classification. In this paper,\nwe review the existing scene image representation methods that are being used\nwidely for image classification. For this, we, first, devise the taxonomy using\nthe seminal existing methods proposed in the literature to this date. Next, we\ncompare their performance both qualitatively (e.g., quality of outputs,\npros/cons, etc.) and quantitatively (e.g., accuracy). Last, we speculate the\nprominent research directions in scene image representation tasks. Overall,\nthis survey provides in-depth insights and applications of recent scene image\nrepresentation methods for traditional Computer Vision (CV)-based methods, Deep\nLearning (DL)-based methods, and Search Engine (SE)-based methods.",
    "descriptor": "\nComments: This paper is under review in Computer Science Review (Elsevier) journal. This article may be deleted or updated based on the polices of the journal\n",
    "authors": [
      "Chiranjibi Sitaula",
      "Tej Bahadur Shahi",
      "Faezeh Marzbanrad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07326"
  },
  {
    "id": "arXiv:2206.07328",
    "title": "A Survey : Neural Networks for AMR-to-Text",
    "abstract": "AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.",
    "descriptor": "",
    "authors": [
      "Hongyu Hao",
      "Guangtong Li",
      "Zhiming Hu",
      "Huafeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07328"
  },
  {
    "id": "arXiv:2206.07329",
    "title": "Application-Oriented Selection of Privacy Enhancing Technologies",
    "abstract": "To create privacy-friendly software designs, architects need comprehensive\nknowledge of existing privacy-enhancing technologies (PETs) and their\nproperties. Existing works that systemize PETs, however, are outdated or focus\non comparison criteria rather than providing guidance for their practical\nselection. In this short paper we present an enhanced classification of PETs\nthat is more application-oriented than previous proposals. It integrates\nexisting criteria like the privacy protection goal, and also considers\npractical criteria like the functional context, a technology's maturity, and\nits impact on various non-functional requirements. We expect that our\nclassification simplifies the selection of PETs for experts and non-experts.",
    "descriptor": "",
    "authors": [
      "Immanuel Kunz",
      "Andreas Binder"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07329"
  },
  {
    "id": "arXiv:2206.07331",
    "title": "ETMA: Efficient Transformer Based Multilevel Attention framework for  Multimodal Fake News Detection",
    "abstract": "In this new digital era, social media has created a severe impact on the\nlives of people. In recent times, fake news content on social media has become\none of the major challenging problems for society. The dissemination of\nfabricated and false news articles includes multimodal data in the form of text\nand images. The previous methods have mainly focused on unimodal analysis.\nMoreover, for multimodal analysis, researchers fail to keep the unique\ncharacteristics corresponding to each modality. This paper aims to overcome\nthese limitations by proposing an Efficient Transformer based Multilevel\nAttention (ETMA) framework for multimodal fake news detection, which comprises\nthe following components: visual attention-based encoder, textual\nattention-based encoder, and joint attention-based learning. Each component\nutilizes the different forms of attention mechanism and uniquely deals with\nmultimodal data to detect fraudulent content. The efficacy of the proposed\nnetwork is validated by conducting several experiments on four real-world fake\nnews datasets: Twitter, Jruvika Fake News Dataset, Pontes Fake News Dataset,\nand Risdal Fake News Dataset using multiple evaluation metrics. The results\nshow that the proposed method outperforms the baseline methods on all four\ndatasets. Further, the computation time of the model is also lower than the\nstate-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Ashima Yadav",
      "Shivani Gaba",
      "Ishan Budhiraja",
      "Neeraj Kumar"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.07331"
  },
  {
    "id": "arXiv:2206.07332",
    "title": "Modelling of AC/DC Interactions of Converter-Interfaced Resources for  Harmonic Power-Flow Studies in Microgrids",
    "abstract": "Modern power distribution systems experience a large-scale integration of\nConverter-Interfaced Distributed Energy Resources (CIDERs). As acknowledged by\nrecent literature, the interaction of individual CIDER components and different\nCIDERs through the grid can lead to undesirable amplifications of harmonic\nfrequencies. In order to analyze and mitigate such phenomenon the authors of\nthis paper recently proposed a Harmonic Power-Flow (HPF) computation framework\nof polyphase grids with a high share of CIDERs. This paper extends this HPF\nframework to include the DC-side dynamics of CIDERs. The DC side is modelled by\na controlled current source and the DC-link capacitor, and the AC/DC converter\nis represented by an average model. Including the latter into the CIDER model\nintroduces a nonlinearity that needs to be approximated for the numerical\nsolution of the HPF. The CIDER model and HPF framework are extended to include\nthis linearization. The accuracy of the linearized CIDER model and the extended\nHPF framework is assessed for individual resources and a modified version of\nthe CIGRE low-voltage benchmark microgrid. Namely, the spectra obtained from\nthe HPF method are compared against time-domain simulations with Simulink. The\nobserved maximum errors are 7.86E-5p.u. w.r.t. voltage magnitude, 3.1E-4p.u.\nw.r.t. current magnitude, and 22mrad w.r.t. phase.",
    "descriptor": "",
    "authors": [
      "Johanna Kristin Maria Becker",
      "Andreas Martin Kettner",
      "Yihui Zuo",
      "Federico Cecati",
      "Sante Pugliese",
      "Marco Liserre",
      "Mario Paolone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07332"
  },
  {
    "id": "arXiv:2206.07335",
    "title": "On Numerical Integration in Neural Ordinary Differential Equations",
    "abstract": "The combination of ordinary differential equations and neural networks, i.e.,\nneural ordinary differential equations (Neural ODE), has been widely studied\nfrom various angles. However, deciphering the numerical integration in Neural\nODE is still an open challenge, as many researches demonstrated that numerical\nintegration significantly affects the performance of the model. In this paper,\nwe propose the inverse modified differential equations (IMDE) to clarify the\ninfluence of numerical integration on training Neural ODE models. IMDE is\ndetermined by the learning task and the employed ODE solver. It is shown that\ntraining a Neural ODE model actually returns a close approximation of the IMDE,\nrather than the true ODE. With the help of IMDE, we deduce that (i) the\ndiscrepancy between the learned model and the true ODE is bounded by the sum of\ndiscretization error and learning loss; (ii) Neural ODE using non-symplectic\nnumerical integration fail to learn conservation laws theoretically. Several\nexperiments are performed to numerically verify our theoretical analysis.",
    "descriptor": "",
    "authors": [
      "Aiqing Zhu",
      "Pengzhan Jin",
      "Beibei Zhu",
      "Yifa Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07335"
  },
  {
    "id": "arXiv:2206.07340",
    "title": "On the Design and Training Strategies for RNN-based Online Neural Speech  Separation Systems",
    "abstract": "While the performance of offline neural speech separation systems has been\ngreatly advanced by the recent development of novel neural network\narchitectures, there is typically an inevitable performance gap between the\nsystems and their online variants. In this paper, we investigate how RNN-based\noffline neural speech separation systems can be changed into their online\ncounterparts while mitigating the performance degradation. We decompose or\nreorganize the forward and backward RNN layers in a bidirectional RNN layer to\nform an online path and an offline path, which enables the model to perform\nboth online and offline processing with a same set of model parameters. We\nfurther introduce two training strategies for improving the online model via\neither a pretrained offline model or a multitask training objective. Experiment\nresults show that compared to the online models that are trained from scratch,\nthe proposed layer decomposition and reorganization schemes and training\nstrategies can effectively mitigate the performance gap between two RNN-based\noffline separation models and their online variants.",
    "descriptor": "\nComments: 5 pages, 1 figure\n",
    "authors": [
      "Kai Li",
      "Yi Luo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07340"
  },
  {
    "id": "arXiv:2206.07341",
    "title": "Cautious Learning of Multiattribute Preferences",
    "abstract": "This paper is dedicated to a cautious learning methodology for predicting\npreferences between alternatives characterized by binary attributes (formally,\neach alternative is seen as a subset of attributes). By \"cautious\", we mean\nthat the model learned to represent the multi-attribute preferences is general\nenough to be compatible with any strict weak order on the alternatives, and\nthat we allow ourselves not to predict some preferences if the data collected\nare not compatible with a reliable prediction. A predicted preference will be\nconsidered reliable if all the simplest models (following Occam's razor\nprinciple) explaining the training data agree on it. Predictions are based on\nan ordinal dominance relation between alternatives [Fishburn and LaValle,\n1996]. The dominance relation relies on an uncertainty set encompassing the\npossible values of the parameters of the multi-attribute utility function.\nNumerical tests are provided to evaluate the richness and the reliability of\nthe predictions made.",
    "descriptor": "",
    "authors": [
      "Hugo Gilbert",
      "Mohamed Ouaguenouni",
      "Meltem Ozturk",
      "Olivier Spanjaard"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07341"
  },
  {
    "id": "arXiv:2206.07344",
    "title": "Automatic Detection of Rice Disease in Images of Various Leaf Sizes",
    "abstract": "Fast, accurate and affordable rice disease detection method is required to\nassist rice farmers tackling equipment and expertise shortages problems. In\nthis paper, we focused on the solution using computer vision technique to\ndetect rice diseases from rice field photograph images. Dealing with images\ntook in real-usage situation by general farmers is quite challenging due to\nvarious environmental factors, and rice leaf object size variation is one major\nfactor caused performance gradation. To solve this problem, we presented a\ntechnique combining a CNN object detection with image tiling technique, based\non automatically estimated width size of rice leaves in the images as a size\nreference for dividing the original input image. A model to estimate leaf width\nwas created by small size CNN such as 18 layer ResNet architecture model. A new\ndivided tiled sub-image set with uniformly sized object was generated and used\nas input for training a rice disease prediction model. Our technique was\nevaluated on 4,960 images of eight different types of rice leaf diseases,\nincluding blast, blight, brown spot, narrow brown spot, orange, red stripe,\nrice grassy stunt virus, and streak disease. The mean absolute percentage error\n(MAPE) for leaf width prediction task evaluated on all eight classes was 11.18%\nin the experiment, indicating that the leaf width prediction model performed\nwell. The mean average precision (mAP) of the prediction performance on YOLOv4\narchitecture was enhanced from 87.56% to 91.14% when trained and tested with\nthe tiled dataset. According to our study, the proposed image tiling technique\nimproved rice disease detection efficiency.",
    "descriptor": "\nComments: 28 pages, 13 figures\n",
    "authors": [
      "Kantip Kiratiratanapruk",
      "Pitchayagan Temniranrat",
      "Wasin Sinthupinyo",
      "Sanparith Marukatat",
      "Sujin Patarapuwadol"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07344"
  },
  {
    "id": "arXiv:2206.07347",
    "title": "On the Use of Deep Mask Estimation Module for Neural Source Separation  Systems",
    "abstract": "Most of the recent neural source separation systems rely on a masking-based\npipeline where a set of multiplicative masks are estimated from and applied to\na signal representation of the input mixture. The estimation of such masks, in\nalmost all network architectures, is done by a single layer followed by an\noptional nonlinear activation function. However, recent literatures have\ninvestigated the use of a deep mask estimation module and observed performance\nimprovement compared to a shallow mask estimation module. In this paper, we\nanalyze the role of such deeper mask estimation module by connecting it to a\nrecently proposed unsupervised source separation method, and empirically show\nthat the deep mask estimation module is an efficient approximation of the\nso-called overseparation-grouping paradigm with the conventional shallow mask\nestimation layers.",
    "descriptor": "\nComments: Accepted by Interspeech 2022\n",
    "authors": [
      "Kai Li",
      "Xiaolin Hu",
      "Yi Luo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07347"
  },
  {
    "id": "arXiv:2206.07348",
    "title": "Unsupervised Capsule Networks of High-Dimension Point Clouds  classification",
    "abstract": "Three-dimensional point clouds learning is widely applied, but the point\nclouds are still unable to deal with classification and recognition tasks\nsatisfactorily in the cases of irregular geometric structures and\nhigh-dimensional space. In 3D space, point clouds tend to have regular\nEuclidean structure because of their density. On the contrary, due to the high\ndimensionality, the spatial structure of high-dimensional space is more\ncomplex, and point clouds are mostly presented in non-European structure.\nFurthermore, among current 3D point clouds classification algorithms, Canonical\nCapsules algorithm based on Euclidean distance is difficult to decompose and\nidentify non-Euclidean structures effectively. Thus, aiming at the point clouds\nclassification task of non-Euclidean structure in 3D and high-dimensional\nspace, this paper refers to the LLE algorithm based on geodesic distance for\noptimizing and proposes the unsupervised algorithm of high-dimensional point\nclouds capsule. In this paper, the geometric features of point clouds are\nconsidered in the extraction process, so as to transform the high-dimensional\nnon-Euclidean structure into a lower-dimensional Euclidean structure with\nretaining spatial geometric features. To verify the feasibility of the\nunsupervised algorithm of high-dimensional point clouds capsule, experiments\nare conducted in Swiss Roll dataset, point clouds MNIST dataset and point\nclouds LFW dataset. The results show that (1) non-Euclidean structures can be\ncan effectively identified by this model in Swiss Roll dataset; (2) a\nsignificant unsupervised learning effect is realized in point clouds MNIST\ndataset. In conclusion, the high-dimensional point clouds capsule unsupervised\nalgorithm proposed in this paper is conducive to expand the application\nscenarios of current point clouds classification and recognition tasks.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Quanfeng Xu",
      "Yi Tang",
      "Yan Yang",
      "Yumei She",
      "Zuo Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07348"
  },
  {
    "id": "arXiv:2206.07349",
    "title": "XMorpher: Full Transformer for Deformable Medical Image Registration via  Cross Attention",
    "abstract": "An effective backbone network is important to deep learning-based Deformable\nMedical Image Registration (DMIR), because it extracts and matches the features\nbetween two images to discover the mutual correspondence for fine registration.\nHowever, the existing deep networks focus on single image situation and are\nlimited in registration task which is performed on paired images. Therefore, we\nadvance a novel backbone network, XMorpher, for the effective corresponding\nfeature representation in DMIR. 1) It proposes a novel full transformer\narchitecture including dual parallel feature extraction networks which exchange\ninformation through cross attention, thus discovering multi-level semantic\ncorrespondence while extracting respective features gradually for final\neffective registration. 2) It advances the Cross Attention Transformer (CAT)\nblocks to establish the attention mechanism between images which is able to\nfind the correspondence automatically and prompts the features to fuse\nefficiently in the network. 3) It constrains the attention computation between\nbase windows and searching windows with different sizes, and thus focuses on\nthe local transformation of deformable registration and enhances the computing\nefficiency at the same time. Without any bells and whistles, our XMorpher gives\nVoxelmorph 2.8% improvement on DSC , demonstrating its effective representation\nof the features from the paired images in DMIR. We believe that our XMorpher\nhas great application potential in more paired medical images. Our XMorpher is\nopen on https://github.com/Solemoon/XMorpher",
    "descriptor": "\nComments: accepted by MICCAI 2022\n",
    "authors": [
      "Jiacheng Shi",
      "Yuting He",
      "Youyong Kong",
      "Jean-Louis Coatrieux",
      "Huazhong Shu",
      "Guanyu Yang",
      "Shuo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07349"
  },
  {
    "id": "arXiv:2206.07350",
    "title": "A Fast Heuristic for Computing Geodesic Cores in Large Networks",
    "abstract": "Motivated by the increasing interest in applications of graph geodesic\nconvexity in machine learning and data mining, we present a heuristic for\ncomputing the geodesic convex hull of node sets in networks. It generates a set\nof almost maximal outerplanar spanning subgraphs for the input graph, computes\nthe geodesic closure in each of these graphs, and regards a node as an element\nof the convex hull if it belongs to the closed sets for at least a user\nspecified number of outerplanar graphs. Our heuristic algorithm runs in time\nlinear in the number of edges of the input graph, i.e., it is faster with one\norder of magnitude than the standard algorithm computing the closure exactly.\nIts performance is evaluated empirically by approximating convexity based\ncore-periphery decomposition of networks. Our experimental results with large\nreal-world networks show that for most networks, the proposed heuristic was\nable to produce close approximations significantly faster than the standard\nalgorithm computing the exact convex hulls. For example, while our algorithm\ncalculated an approximate core-periphery decomposition in 5 hours or less for\nnetworks with more than 20 million edges, the standard algorithm did not\nterminate within 50 days.",
    "descriptor": "",
    "authors": [
      "Florian Seiffarth",
      "Tam\u00e1s Horv\u00e1th",
      "Stefan Wrobel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07350"
  },
  {
    "id": "arXiv:2206.07351",
    "title": "RecBole 2.0: Towards a More Up-to-Date Recommendation Library",
    "abstract": "In order to support the study of recent advances in recommender systems, this\npaper presents an extended recommendation library consisting of eight packages\nfor up-to-date topics and architectures. First of all, from a data perspective,\nwe consider three important topics related to data issues (i.e., sparsity, bias\nand distribution shift), and develop five packages accordingly: meta-learning,\ndata augmentation, debiasing, fairness and cross-domain recommendation.\nFurthermore, from a model perspective, we develop two benchmarking packages for\nTransformer-based and graph neural network (GNN)-based models, respectively.\nAll the packages (consisting of 65 new models) are developed based on a popular\nrecommendation framework RecBole, ensuring that both the implementation and\ninterface are unified. For each package, we provide complete implementations\nfrom data loading, experimental setup, evaluation and algorithm implementation.\nThis library provides a valuable resource to facilitate the up-to-date research\nin recommender systems. The project is released at the link:\nhttps://github.com/RUCAIBox/RecBole2.0.",
    "descriptor": "\nComments: A new version of recommendation toolkit -- RecBole\n",
    "authors": [
      "Wayne Xin Zhao",
      "Yupeng Hou",
      "Xingyu Pan",
      "Chen Yang",
      "Zeyu Zhang",
      "Zihan Lin",
      "Jingsen Zhang",
      "Shuqing Bian",
      "Jiakai Tang",
      "Wenqi Sun",
      "Yushuo Chen",
      "Lanling Xu",
      "Gaowei Zhang",
      "Zhen Tian",
      "Changxin Tian",
      "Shanlei Mu",
      "Xinyan Fan",
      "Xu Chen",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.07351"
  },
  {
    "id": "arXiv:2206.07352",
    "title": "Robust SAR ATR on MSTAR with Deep Learning Models trained on Full  Synthetic MOCEM data",
    "abstract": "The promising potential of Deep Learning for Automatic Target Recognition\n(ATR) on Synthetic Aperture Radar (SAR) images vanishes when considering the\ncomplexity of collecting training datasets measurements. Simulation can\novercome this issue by producing synthetic training datasets. However, because\nof the limited representativeness of simulation, models trained in a classical\nway with synthetic images have limited generalization abilities when dealing\nwith real measurement at test time. Previous works identified a set of equally\npromising deep-learning algorithms to tackle this issue. However, these\napproaches have been evaluated in a very favorable scenario with a synthetic\ntraining dataset that overfits the ground truth of the measured test data. In\nthis work, we study the ATR problem outside of this ideal condition, which is\nunlikely to occur in real operational contexts. Our contribution is threefold.\n(1) Using the MOCEM simulator (developed by SCALIAN DS for the French MoD/DGA),\nwe produce a synthetic MSTAR training dataset that differs significantly from\nthe real measurements. (2) We experimentally demonstrate the limits of the\nstate-of-the-art. (3) We show that domain randomization techniques and\nadversarial training can be combined to overcome this issue. We demonstrate\nthat this approach is more robust than the state-of-the-art, with an accuracy\nof 75 %, while having a limited impact on computing performance during\ntraining.",
    "descriptor": "",
    "authors": [
      "Benjamin Camus",
      "Corentin Le Barbu",
      "Eric Monteux"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07352"
  },
  {
    "id": "arXiv:2206.07353",
    "title": "Rethinking Reinforcement Learning for Recommendation: A Prompt  Perspective",
    "abstract": "Modern recommender systems aim to improve user experience. As reinforcement\nlearning (RL) naturally fits this objective -- maximizing an user's reward per\nsession -- it has become an emerging topic in recommender systems. Developing\nRL-based recommendation methods, however, is not trivial due to the\n\\emph{offline training challenge}. Specifically, the keystone of traditional RL\nis to train an agent with large amounts of online exploration making lots of\n`errors' in the process. In the recommendation setting, though, we cannot\nafford the price of making `errors' online. As a result, the agent needs to be\ntrained through offline historical implicit feedback, collected under different\nrecommendation policies; traditional RL algorithms may lead to sub-optimal\npolicies under these offline training settings.\nHere we propose a new learning paradigm -- namely Prompt-Based Reinforcement\nLearning (PRL) -- for the offline training of RL-based recommendation agents.\nWhile traditional RL algorithms attempt to map state-action input pairs to\ntheir expected rewards (e.g., Q-values), PRL directly infers actions (i.e.,\nrecommended items) from state-reward inputs. In short, the agents are trained\nto predict a recommended item given the prior interactions and an observed\nreward value -- with simple supervised learning. At deployment time, this\nhistorical (training) data acts as a knowledge base, while the state-reward\npairs are used as a prompt. The agents are thus used to answer the question:\n\\emph{ Which item should be recommended given the prior interactions \\& the\nprompted reward value}? We implement PRL with four notable recommendation\nmodels and conduct experiments on two real-world e-commerce datasets.\nExperimental results demonstrate the superior performance of our proposed\nmethods.",
    "descriptor": "",
    "authors": [
      "Xin Xin",
      "Tiago Pimentel",
      "Alexandros Karatzoglou",
      "Pengjie Ren",
      "Konstantina Christakopoulou",
      "Zhaochun Ren"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.07353"
  },
  {
    "id": "arXiv:2206.07358",
    "title": "The Complexity of Contracting Bipartite Graphs into Small Cycles",
    "abstract": "For a positive integer $\\ell \\geq 3$, the $C_\\ell$-Contractibility problem\ntakes as input an undirected simple graph $G$ and determines whether $G$ can be\ntransformed into a graph isomorphic to $C_\\ell$ (the induced cycle on $\\ell$\nvertices) using only edge contractions. Brouwer and Veldman [JGT 1987] showed\nthat $C_4$-Contractibility is NP-complete in general graphs. It is easy to\nverify that $C_3$-Contractibility is polynomial-time solvable. Dabrowski and\nPaulusma [IPL 2017] showed that $C_{\\ell}$-Contractibility is \\NP-complete\\ on\nbipartite graphs for $\\ell = 6$ and posed as open problems the status of the\nproblem when $\\ell$ is 4 or 5. In this paper, we show that both\n$C_5$-Contractibility and $C_4$-Contractibility are NP-complete on bipartite\ngraphs.",
    "descriptor": "",
    "authors": [
      "R. Krithika",
      "Roohani Sharma",
      "Prafullkumar Tale"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.07358"
  },
  {
    "id": "arXiv:2206.07359",
    "title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for  Emotion Recognition in Conversation",
    "abstract": "In emotion recognition in conversation (ERC), the emotion of the current\nutterance is predicted by considering the previous context, which can be\nutilized in many natural language processing tasks. Although multiple emotions\ncan coexist in a given sentence, most previous approaches take the perspective\nof a classification task to predict only a given label. However, it is\nexpensive and difficult to label the emotion of a sentence with confidence or\nmulti-label. In this paper, we automatically construct a grayscale label\nconsidering the correlation between emotions and use it for learning. That is,\ninstead of using a given label as a one-hot encoding, we construct a grayscale\nlabel by measuring scores for different emotions. We introduce several methods\nfor constructing grayscale labels and confirm that each method improves the\nemotion recognition performance. Our method is simple, effective, and\nuniversally applicable to previous systems. The experiments show a significant\nimprovement in the performance of baselines.",
    "descriptor": "\nComments: Accepted by INTERSPEECH 2022\n",
    "authors": [
      "Joosung Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07359"
  },
  {
    "id": "arXiv:2206.07360",
    "title": "SciTweets -- A Dataset and Annotation Framework for Detecting Scientific  Online Discourse",
    "abstract": "Scientific topics, claims and resources are increasingly debated as part of\nonline discourse, where prominent examples include discourse related to\nCOVID-19 or climate change. This has led to both significant societal impact\nand increased interest in scientific online discourse from various disciplines.\nFor instance, communication studies aim at a deeper understanding of biases,\nquality or spreading pattern of scientific information whereas computational\nmethods have been proposed to extract, classify or verify scientific claims\nusing NLP and IR techniques. However, research across disciplines currently\nsuffers from both a lack of robust definitions of the various forms of\nscience-relatedness as well as appropriate ground truth data for distinguishing\nthem. In this work, we contribute (a) an annotation framework and corresponding\ndefinitions for different forms of scientific relatedness of online discourse\nin Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our\nlabeling framework reaching an average Fleiss Kappa $\\kappa$ of 0.63, (c) a\nmulti-label classifier trained on our data able to detect science-relatedness\nwith 89% F1 and also able to detect distinct forms of scientific knowledge\n(claims, references). With this work we aim to lay the foundation for\ndeveloping and evaluating robust methods for analysing science as part of\nlarge-scale online discourse.",
    "descriptor": "\nComments: submitted to CIKM 2022\n",
    "authors": [
      "Salim Hafid",
      "Sebastian Schellhammer",
      "Sandra Bringay",
      "Konstantin Todorov",
      "Stefan Dietze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07360"
  },
  {
    "id": "arXiv:2206.07365",
    "title": "Modern Machine-Learning Predictive Models for Diagnosing Infectious  Diseases",
    "abstract": "Controlling infectious diseases is a major health priority because they can\nspread and infect humans, thus evolving into epidemics or pandemics. Therefore,\nearly detection of infectious diseases is a significant need, and many\nresearchers have developed models to diagnose them in the early stages. This\npaper reviewed research articles for recent machine-learning (ML) algorithms\napplied to infectious disease diagnosis. We searched the Web of Science,\nScienceDirect, PubMed, Springer, and IEEE databases from 2015 to 2022,\nidentified the pros and cons of the reviewed ML models, and discussed the\npossible recommendations to advance the studies in this field. We found that\nmost of the articles used small datasets, and few of them used real-time data.\nOur results demonstrated that a suitable ML technique depends on the nature of\nthe dataset and the desired goal.",
    "descriptor": "\nComments: 13 pages, 4 figures, 6 tables\n",
    "authors": [
      "Eman Yahia Alqaissi",
      "Fahd Saleh Alotaibi",
      "Muhammad Sher Ramzan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07365"
  },
  {
    "id": "arXiv:2206.07368",
    "title": "PCRAFT: Capacity Planning for Dependable Stateless Services",
    "abstract": "Fault-tolerance techniques depend on replication to enhance availability,\nalbeit at the cost of increased infrastructure costs. This results in a\nfundamental trade-off: Fault-tolerant services must satisfy given availability\nand performance constraints while minimising the number of replicated\nresources. These constraints pose capacity planning challenges for the service\noperators to minimise replication costs without negatively impacting\navailability.\nTo this end, we present PCRAFT, a system to enable capacity planning of\ndependable services. PCRAFT's capacity planning is based on a hybrid approach\nthat combines empirical performance measurements with probabilistic modelling\nof availability based on fault injection. In particular, we integrate\ntraditional service-level availability mechanisms (active route anywhere and\npassive failover) and deployment schemes (cloud and on-premises) to quantify\nthe number of nodes needed to satisfy the given availability and performance\nconstraints. Our evaluation based on real-world applications shows that cloud\ndeployment requires fewer nodes than on-premises deployments. Additionally,\nwhen considering on-premises deployments, we show how passive failover requires\nfewer nodes than active route anywhere. Furthermore, our evaluation quantify\nthe quality enhancement given by additional integrity mechanisms and how this\naffects the number of nodes needed.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Rasha Faqeh",
      "Andr\u00e8 Martin",
      "Valerio Schiavoni",
      "Pramod Bhatotia",
      "Pascal Felber",
      "Christof Fetzer"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07368"
  },
  {
    "id": "arXiv:2206.07369",
    "title": "DiffWire: Inductive Graph Rewiring via the Lov\u00e1sz Bound",
    "abstract": "Graph Neural Networks (GNNs) have been shown to achieve competitive results\nto tackle graph-related tasks, such as node and graph classification, link\nprediction and node and graph clustering in a variety of domains. Most GNNs use\na message passing framework and hence are called MPNNs. Despite their promising\nresults, MPNNs have been reported to suffer from over-smoothing, over-squashing\nand under-reaching. Graph rewiring and graph pooling have been proposed in the\nliterature as solutions to address these limitations. However, most\nstate-of-the-art graph rewiring methods fail to preserve the global topology of\nthe graph, are not differentiable (inductive) and require the tuning of\nhyper-parameters. In this paper, we propose DiffWire, a novel framework for\ngraph rewiring in MPNNs that is principled, fully differentiable and\nparameter-free by leveraging the Lov\\'asz bound. Our approach provides a\nunified theory for graph rewiring by proposing two new, complementary layers in\nMPNNs: first, CTLayer, a layer that learns the commute times and uses them as a\nrelevance function for edge re-weighting; second, GAPLayer, a layer to optimize\nthe spectral gap, depending on the nature of the network and the task at hand.\nWe empirically validate the value of our proposed approach and each of these\nlayers separately with benchmark datasets for graph classification. DiffWire\nbrings together the learnability of commute times to related definitions of\ncurvature, opening the door to the development of more expressive MPNNs.",
    "descriptor": "\nComments: 22 pages, 6 figures and 5 tables. Preprint under review\n",
    "authors": [
      "Adri\u00e1n Arnaiz-Rodr\u00edguez",
      "Ahmed Begga",
      "Francisco Escolano",
      "Nuria Oliver"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07369"
  },
  {
    "id": "arXiv:2206.07371",
    "title": "On the Stability of Modified Patankar Methods",
    "abstract": "Patankar schemes have attracted more and more interests as a time-integration\nmethod in the last years due to their unconditionally positivity preserving\nproperty. Even though they have been become of major interest, it was long time\nnot clear what the stability properties of such schemes are and how they really\nperform in practice. Recently, a new stability approach has been proposed,\nbased on Lyapnuov stability with an extension of the central manifold theorem,\nto investigate the stability properties of positive preserving\ntime-integrators. In this paper, we investigate the stability properties of the\nclassical modified Patankar--Runge--Kutta schemes (MPRK) and the modified\nPatankar Deferred Correction (MPDeC) approaches. We prove that most of the\nPatankar schemes are stable and we verify our theoretical results with\nnumerical simulations.",
    "descriptor": "\nComments: 29 pages, 13 Figures\n",
    "authors": [
      "Thomas Izgin",
      "Philipp \u00d6ffner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07371"
  },
  {
    "id": "arXiv:2206.07372",
    "title": "MonoGround: Detecting Monocular 3D Objects from the Ground",
    "abstract": "Monocular 3D object detection has attracted great attention for its\nadvantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping\nessence from the monocular imaging process, monocular 3D object detection\nsuffers from inaccurate depth estimation and thus has poor 3D detection\nresults. To alleviate this problem, we propose to introduce the ground plane as\na prior in the monocular 3d object detection. The ground plane prior serves as\nan additional geometric condition to the ill-posed mapping and an extra source\nin depth estimation. In this way, we can get a more accurate depth estimation\nfrom the ground. Meanwhile, to take full advantage of the ground plane prior,\nwe propose a depth-align training strategy and a precise two-stage depth\ninference method tailored for the ground plane prior. It is worth noting that\nthe introduced ground plane prior requires no extra data sources like LiDAR,\nstereo images, and depth information. Extensive experiments on the KITTI\nbenchmark show that our method could achieve state-of-the-art results compared\nwith other methods while maintaining a very fast speed. Our code and models are\navailable at https://github.com/cfzd/MonoGround.",
    "descriptor": "\nComments: CVPR22\n",
    "authors": [
      "Zequn Qin",
      "Xi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07372"
  },
  {
    "id": "arXiv:2206.07373",
    "title": "NatiQ: An End-to-end Text-to-Speech System for Arabic",
    "abstract": "NatiQ is end-to-end text-to-speech system for Arabic. Our speech synthesizer\nuses an encoder-decoder architecture with attention. We used both\ntacotron-based models (tacotron-1 and tacotron-2) and the faster transformer\nmodel for generating mel-spectrograms from characters. We concatenated\nTacotron1 with the WaveRNN vocoder, Tacotron2 with the WaveGlow vocoder and\nESPnet transformer with the parallel wavegan vocoder to synthesize waveforms\nfrom the spectrograms. We used in-house speech data for two voices: 1) neutral\nmale \"Hamza\"- narrating general content and news, and 2) expressive female\n\"Amina\"- narrating children story books to train our models. Our best systems\nachieve an average Mean Opinion Score (MOS) of 4.21 and 4.40 for Amina and\nHamza respectively. The objective evaluation of the systems using word and\ncharacter error rate (WER and CER) as well as the response time measured by\nreal-time factor favored the end-to-end architecture ESPnet. NatiQ demo is\navailable on-line at https://tts.qcri.org",
    "descriptor": "",
    "authors": [
      "Ahmed Abdelali",
      "Nadir Durrani",
      "Cenk Demiroglu",
      "Fahim Dalvi",
      "Hamdy Mubarak",
      "Kareem Darwish"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07373"
  },
  {
    "id": "arXiv:2206.07375",
    "title": "Knowledge4COVID-19: A Semantic-based Approach for Constructing a  COVID-19 related Knowledge Graph from Various Sources and Analysing  Treatments' Toxicities",
    "abstract": "In this paper, we present Knowledge4COVID-19, a framework that aims to\nshowcase the power of integrating disparate sources of knowledge to discover\nadverse drug effects caused by drug-drug interactions among COVID-19 treatments\nand pre-existing condition drugs. Initially, we focus on constructing the\nKnowledge4COVID-19 knowledge graph (KG) from the declarative definition of\nmapping rules using the RDF Mapping Language. Since valuable information about\ndrug treatments, drug-drug interactions, and side effects is present in textual\ndescriptions in scientific databases (e.g., DrugBank) or in scientific\nliterature (e.g., the CORD-19, the Covid-19 Open Research Dataset), the\nKnowledge4COVID-19 framework implements Natural Language Processing. The\nKnowledge4COVID-19 framework extracts relevant entities and predicates that\nenable the fine-grained description of COVID-19 treatments and the potential\nadverse events that may occur when these treatments are combined with\ntreatments of common comorbidities, e.g., hypertension, diabetes, or asthma.\nMoreover, on top of the KG, several techniques for the discovery and prediction\nof interactions and potential adverse effects of drugs have been developed with\nthe aim of suggesting more accurate treatments for treating the virus. We\nprovide services to traverse the KG and visualize the effects that a group of\ndrugs may have on a treatment outcome. Knowledge4COVID-19 was part of the\nPan-European hackathon#EUvsVirus in April 2020 and is publicly available as a\nresource through a GitHub repository\n(https://github.com/SDM-TIB/Knowledge4COVID-19) and a DOI\n(https://zenodo.org/record/4701817#.YH336-8zbol).",
    "descriptor": "",
    "authors": [
      "Ahmad Sakor",
      "Samaneh Jozashoori",
      "Emetis Niazmand",
      "Ariam Rivas",
      "Kostantinos Bougiatiotis",
      "Fotis Aisopos",
      "Enrique Iglesias",
      "Philipp D. Rohde",
      "Trupti Padiya",
      "Anastasia Krithara",
      "Georgios Paliouras",
      "Maria-Esther Vidal"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.07375"
  },
  {
    "id": "arXiv:2206.07376",
    "title": "Mean-Semivariance Policy Optimization via Risk-Averse Reinforcement  Learning",
    "abstract": "Keeping risk under control is often more crucial than maximizing expected\nreward in real-world decision-making situations, such as finance, robotics,\nautonomous driving, etc. The most natural choice of risk measures is variance,\nwhile it penalizes the upside volatility as much as the downside part. Instead,\nthe (downside) semivariance, which captures negative deviation of a random\nvariable under its mean, is more suitable for risk-averse proposes. This paper\naims at optimizing the mean-semivariance (MSV) criterion in reinforcement\nlearning w.r.t. steady rewards. Since semivariance is time-inconsistent and\ndoes not satisfy the standard Bellman equation, the traditional dynamic\nprogramming methods are inapplicable to MSV problems directly. To tackle this\nchallenge, we resort to the Perturbation Analysis (PA) theory and establish the\nperformance difference formula for MSV. We reveal that the MSV problem can be\nsolved by iteratively solving a sequence of RL problems with a policy-dependent\nreward function. Further, we propose two on-policy algorithms based on the\npolicy gradient theory and the trust region method. Finally, we conduct diverse\nexperiments from simple bandit problems to continuous control tasks in MuJoCo,\nwhich demonstrate the effectiveness of our proposed methods.",
    "descriptor": "",
    "authors": [
      "Xiaoteng Ma",
      "Shuai Ma",
      "Li Xia",
      "Qianchuan Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07376"
  },
  {
    "id": "arXiv:2206.07379",
    "title": "Convergence rates of a dual gradient method for constrained linear  ill-posed problems",
    "abstract": "In this paper we consider a dual gradient method for solving linear ill-posed\nproblems $Ax = y$, where $A : X \\to Y$ is a bounded linear operator from a\nBanach space $X$ to a Hilbert space $Y$. A strongly convex penalty function is\nused in the method to select a solution with desired feature. Under variational\nsource conditions on the sought solution, convergence rates are derived when\nthe method is terminated by either an {\\it a priori} stopping rule or the\ndiscrepancy principle. We also consider an acceleration of the method as well\nas its various applications.",
    "descriptor": "\nComments: Accepted\n",
    "authors": [
      "Qinian Jin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07379"
  },
  {
    "id": "arXiv:2206.07382",
    "title": "Sparse Structure Search for Parameter-Efficient Tuning",
    "abstract": "Adapting large pre-trained models (PTMs) through fine-tuning imposes\nprohibitive computational and storage burdens. Recent studies of\nparameter-efficient tuning (PET) find that only optimizing a small portion of\nparameters conditioned on PTMs could yield on-par performance compared to\nconventional fine-tuning. Generally, PET methods exquisitely design\nparameter-efficient modules (PET modules) which could be applied to arbitrary\nfine-grained positions inside PTMs. However, the effectiveness of these\nfine-grained positions largely relies on sophisticated manual designation,\nthereby usually producing sub-optimal results. In contrast to the manual\ndesignation, we explore constructing PET modules in an automatic manner. We\nautomatically \\textbf{S}earch for the \\textbf{S}parse \\textbf{S}tructure of\n\\textbf{P}arameter-\\textbf{E}fficient \\textbf{T}uning (S$^3$PET). Based on a\nunified framework of various PET methods, S$^3$PET conducts the differentiable\nPET structure search through bi-level optimization and proposes shifted global\nsigmoid method to explicitly control the number of trainable parameters.\nExtensive experiments show that S$^3$PET surpasses manual and random structures\nwith less trainable parameters. The searched structures preserve more than 99\\%\nfine-tuning performance with 0.01\\% trainable parameters. Moreover, the\nadvantage of S$^3$PET is amplified with extremely low trainable parameters\nbudgets (0.0009\\%$\\sim$0.01\\%). The searched structures are transferable and\nexplainable, providing suggestions and guidance for the future design of PET\nmethods.",
    "descriptor": "",
    "authors": [
      "Shengding Hu",
      "Zhen Zhang",
      "Ning Ding",
      "Yadao Wang",
      "Yasheng Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07382"
  },
  {
    "id": "arXiv:2206.07384",
    "title": "Blockchain-based Federated Learning for Industrial Metaverses: Incentive  Scheme with Optimal AoI",
    "abstract": "The emerging industrial metaverses realize the mapping and expanding\noperations of physical industry into virtual space for significantly upgrading\nintelligent manufacturing. The industrial metaverses obtain data from various\nproduction and operation lines by Industrial Internet of Things (IIoT), and\nthus conduct effective data analysis and decision-making, thereby enhancing the\nproduction efficiency of the physical space, reducing operating costs, and\nmaximizing commercial value. However, there still exist bottlenecks when\nintegrating metaverses into IIoT, such as the privacy leakage of sensitive data\nwith commercial secrets, IIoT sensing data freshness, and incentives for\nsharing these data. In this paper, we design a user-defined privacy-preserving\nframework with decentralized federated learning for the industrial metaverses.\nTo further improve privacy protection of industrial metaverse, a cross-chain\nempowered federated learning framework is further utilized to perform\ndecentralized, secure, and privacy-preserving data training on both physical\nand virtual spaces through a hierarchical blockchain architecture with a main\nchain and multiple subchains. Moreover, we introduce the age of information as\nthe data freshness metric and thus design an age-based contract model to\nmotivate data sensing among IIoT nodes. Numerical results indicate the\nefficiency of the proposed framework and incentive mechanism in the industrial\nmetaverses",
    "descriptor": "",
    "authors": [
      "Jiawen Kang",
      "Dongdong Ye",
      "Jiangtian Nie",
      "Jiang Xiao",
      "Xianjun Deng",
      "Siming Wang",
      "Zehui Xiong",
      "Rong Yu",
      "Dusit Niyato"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.07384"
  },
  {
    "id": "arXiv:2206.07387",
    "title": "The Manifold Hypothesis for Gradient-Based Explanations",
    "abstract": "When do gradient-based explanation algorithms provide meaningful\nexplanations? We propose a necessary criterion: their feature attributions need\nto be aligned with the tangent space of the data manifold. To provide evidence\nfor this hypothesis, we introduce a framework based on variational autoencoders\nthat allows to estimate and generate image manifolds. Through experiments\nacross a range of different datasets -- MNIST, EMNIST, CIFAR10, X-ray pneumonia\nand Diabetic Retinopathy detection -- we demonstrate that the more a feature\nattribution is aligned with the tangent space of the data, the more structured\nand explanatory it tends to be. In particular, the attributions provided by\npopular post-hoc methods such as Integrated Gradients, SmoothGrad and Input\n$\\times$ Gradient tend to be more strongly aligned with the data manifold than\nthe raw gradient. As a consequence, we suggest that explanation algorithms\nshould actively strive to align their explanations with the data manifold. In\npart, this can be achieved by adversarial training, which leads to better\nalignment across all datasets. Some form of adjustment to the model\narchitecture or training algorithm is necessary, since we show that\ngeneralization of neural networks alone does not imply the alignment of model\ngradients with the data manifold.",
    "descriptor": "",
    "authors": [
      "Sebastian Bordt",
      "Uddeshya Upadhyay",
      "Zeynep Akata",
      "Ulrike von Luxburg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07387"
  },
  {
    "id": "arXiv:2206.07389",
    "title": "Ultra Fast Deep Lane Detection with Hybrid Anchor Driven Ordinal  Classification",
    "abstract": "Modern methods mainly regard lane detection as a problem of pixel-wise\nsegmentation, which is struggling to address the problems of efficiency and\nchallenging scenarios like severe occlusions and extreme lighting conditions.\nInspired by human perception, the recognition of lanes under severe occlusions\nand extreme lighting conditions is mainly based on contextual and global\ninformation. Motivated by this observation, we propose a novel, simple, yet\neffective formulation aiming at ultra fast speed and the problem of challenging\nscenarios. Specifically, we treat the process of lane detection as an\nanchor-driven ordinal classification problem using global features. First, we\nrepresent lanes with sparse coordinates on a series of hybrid (row and column)\nanchors. With the help of the anchor-driven representation, we then reformulate\nthe lane detection task as an ordinal classification problem to get the\ncoordinates of lanes. Our method could significantly reduce the computational\ncost with the anchor-driven representation. Using the large receptive field\nproperty of the ordinal classification formulation, we could also handle\nchallenging scenarios. Extensive experiments on four lane detection datasets\nshow that our method could achieve state-of-the-art performance in terms of\nboth speed and accuracy. A lightweight version could even achieve 300+ frames\nper second(FPS). Our code is at\nhttps://github.com/cfzd/Ultra-Fast-Lane-Detection-v2.",
    "descriptor": "\nComments: TPAMI 2022\n",
    "authors": [
      "Zequn Qin",
      "Pengyi Zhang",
      "Xi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07389"
  },
  {
    "id": "arXiv:2206.07391",
    "title": "\"Why Here and Not There?\" -- Diverse Contrasting Explanations of  Dimensionality Reduction",
    "abstract": "Dimensionality reduction is a popular preprocessing and a widely used tool in\ndata mining. Transparency, which is usually achieved by means of explanations,\nis nowadays a widely accepted and crucial requirement of machine learning based\nsystems like classifiers and recommender systems. However, transparency of\ndimensionality reduction and other data mining tools have not been considered\nmuch yet, still it is crucial to understand their behavior -- in particular\npractitioners might want to understand why a specific sample got mapped to a\nspecific location. In order to (locally) understand the behavior of a given\ndimensionality reduction method, we introduce the abstract concept of\ncontrasting explanations for dimensionality reduction, and apply a realization\nof this concept to the specific application of explaining two dimensional data\nvisualization.",
    "descriptor": "",
    "authors": [
      "Andr\u00e9 Artelt",
      "Alexander Schulz",
      "Barbara Hammer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07391"
  },
  {
    "id": "arXiv:2206.07392",
    "title": "Volume Conductor: Interactive Visibility Management for Crowded Volumes",
    "abstract": "We present a novel smart visibility system for visualizing crowded volumetric\ndata containing many object instances. The presented approach allows users to\nform groups of objects through membership predicates and to individually\ncontrol the visibility of the instances in each group. Unlike previous smart\nvisibility approaches, our approach controls the visibility on a per-instance\nbasis and decides which instances are displayed or hidden based on the\nmembership predicates and the current view. Thus, cluttered and dense volumes\nthat are notoriously difficult to explore effectively are automatically\nsparsified so that the essential information is extracted and presented to the\nuser. The proposed system is generic and can be easily integrated into existing\nvolume rendering applications and applied to many different domains. We\ndemonstrate the use of the volume conductor for visualizing fiber-reinforced\npolymers and intracellular organelle structures.",
    "descriptor": "\nComments: 12 pages, 9 figures\n",
    "authors": [
      "\u017diga Lesar",
      "Ruwayda Alharbi",
      "Ciril Bohak",
      "Ond\u0159ej Strnad",
      "Christoph Heinzl",
      "Matija Marolt",
      "Ivan Viola"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.07392"
  },
  {
    "id": "arXiv:2206.07393",
    "title": "Structure and Power: an emerging landscape",
    "abstract": "In this paper, we give an overview of some recent work on applying tools from\ncategory theory in finite model theory, descriptive complexity, constraint\nsatisfaction, and combinatorics. The motivations for this work come from\nComputer Science, but there may also be something of interest for model\ntheorists and other logicians.\nThe basic setting involves studying the category of relational structures via\na resource-indexed family of adjunctions with some process category - which\nunfolds relational structures into treelike forms, allowing natural resource\nparameters to be assigned to these unfoldings. One basic instance of this\nscheme allows us to recover, in a purely structural, syntax-free way: the\nEhrenfeucht-Fraisse~game; the quantifier rank fragments of first-order logic;\nthe equivalences on structures induced by (i) the quantifier rank fragments,\n(ii) the restriction of this fragment to the existential positive part, and\n(iii) the extension with counting quantifiers; and the combinatorial parameter\nof tree-depth (Nesetril and Ossona de Mendez). Another instance recovers the\nk-pebble game, the finite-variable fragments, the corresponding equivalences,\nand the combinatorial parameter of treewidth. Other instances cover modal,\nguarded and hybrid fragments, generalized quantifiers, and a wide range of\ncombinatorial parameters. This whole scheme has been axiomatized in a very\ngeneral setting, of arboreal categories and arboreal covers.\nBeyond this basic level, a landscape is beginning to emerge, in which\nstructural features of the resource categories, adjunctions and comonads are\nreflected in degrees of logical and computational tractability of the\ncorresponding languages. Examples include semantic characterisation and\npreservation theorems, and Lovasz-type results on counting homomorphisms.",
    "descriptor": "\nComments: To appear in special issue for Trakhtenbrot centenary\n",
    "authors": [
      "Samson Abramsky"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.07393"
  },
  {
    "id": "arXiv:2206.07394",
    "title": "Efficient Adaptive Ensembling for Image Classification",
    "abstract": "In recent times, except for sporadic cases, the trend in Computer Vision is\nto achieve minor improvements over considerable increases in complexity.\nTo reverse this tendency, we propose a novel method to boost image\nclassification performances without an increase in complexity.\nTo this end, we revisited ensembling, a powerful approach, not often\nadequately used due to its nature of increased complexity and training time,\nmaking it viable by specific design choices. First, we trained end-to-end two\nEfficientNet-b0 models (known to be the architecture with the best overall\naccuracy/complexity trade-off in image classification) on disjoint subsets of\ndata (i.e. bagging). Then, we made an efficient adaptive ensemble by performing\nfine-tuning of a trainable combination layer. In this way, we were able to\noutperform the state-of-the-art by an average of 0.5\\% on the accuracy with\nrestrained complexity both in terms of number of parameters (by 5-60 times),\nand FLoating point Operations Per Second (by 10-100 times) on several major\nbenchmark datasets, fully embracing the green AI.",
    "descriptor": "\nComments: Submitted for consideration in Pattern Recognition Letters\n",
    "authors": [
      "Antonio Bruno",
      "Davide Moroni",
      "Massimo Martinelli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07394"
  },
  {
    "id": "arXiv:2206.07396",
    "title": "Selectivity Estimation of Inequality Joins In Databases",
    "abstract": "Selectivity estimation refers to the ability of the SQL query optimizer to\nestimate the size of the results of a predicate in the query. It is the main\ncalculation, based on which the optimizer can select the cheapest plan to\nexecute. While the problem is known since the mid 70s, we were surprised that\nthere are no solutions in the literature for the selectivity estimation of\ninequality joins. By testing four common database systems: Oracle, SQL-Server,\nPostgreSQL, and MySQL, we found that the open-source systems PostgreSQL and\nMySQL lack this estimation. Oracle and SQL-Server make fairly accurate\nestimations, yet their algorithms are secret. This paper thus proposes an\nalgorithm for inequality join selectivity estimation. The proposed algorithm\nhas been implemented in PostgreSQL and sent as a patch to be included in the\nnext releases.",
    "descriptor": "",
    "authors": [
      "Diogo Repas",
      "Zhicheng Luo",
      "Maxime Schoemans",
      "Mahmoud Sakr"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.07396"
  },
  {
    "id": "arXiv:2206.07403",
    "title": "Automating the resolution of flight conflicts: Deep reinforcement  learning in service of air traffic controllers",
    "abstract": "Dense and complex air traffic scenarios require higher levels of automation\nthan those exhibited by tactical conflict detection and resolution (CD\\&R)\ntools that air traffic controllers (ATCO) use today. However, the air traffic\ncontrol (ATC) domain, being safety critical, requires AI systems to which\noperators are comfortable to relinquishing control, guaranteeing operational\nintegrity and automation adoption. Two major factors towards this goal are\nquality of solutions, and transparency in decision making. This paper proposes\nusing a graph convolutional reinforcement learning method operating in a\nmultiagent setting where each agent (flight) performs a CD\\&R task, jointly\nwith other agents. We show that this method can provide high-quality solutions\nwith respect to stakeholders interests (air traffic controllers and airspace\nusers), addressing operational transparency issues.",
    "descriptor": "\nComments: 20 pages, 5 figures, 3 tables\n",
    "authors": [
      "George Vouros",
      "George Papadopoulos",
      "Alevizos Bastas",
      "Jose Manuel Cordero",
      "Ruben Rodrigez Rodrigez"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07403"
  },
  {
    "id": "arXiv:2206.07406",
    "title": "Hardening DNNs against Transfer Attacks during Network Compression using  Greedy Adversarial Pruning",
    "abstract": "The prevalence and success of Deep Neural Network (DNN) applications in\nrecent years have motivated research on DNN compression, such as pruning and\nquantization. These techniques accelerate model inference, reduce power\nconsumption, and reduce the size and complexity of the hardware necessary to\nrun DNNs, all with little to no loss in accuracy. However, since DNNs are\nvulnerable to adversarial inputs, it is important to consider the relationship\nbetween compression and adversarial robustness. In this work, we investigate\nthe adversarial robustness of models produced by several irregular pruning\nschemes and by 8-bit quantization. Additionally, while conventional pruning\nremoves the least important parameters in a DNN, we investigate the effect of\nan unconventional pruning method: removing the most important model parameters\nbased on the gradient on adversarial inputs. We call this method Greedy\nAdversarial Pruning (GAP) and we find that this pruning method results in\nmodels that are resistant to transfer attacks from their uncompressed\ncounterparts.",
    "descriptor": "\nComments: 4 pages, 2 figures\n",
    "authors": [
      "Jonah O'Brien Weiss",
      "Tiago Alves",
      "Sandip Kundu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07406"
  },
  {
    "id": "arXiv:2206.07410",
    "title": "A hybrid meshing framework adapted to the topography to simulate  Atmospheric Boundary Layer flows",
    "abstract": "A new topography adapted mesh generation process tailored to simulate\nAtmospheric Boundary Layer (ABL) flows on complex terrains is presented. The\nmesher is fully automatic given: the maximum and minimum surface mesh size, the\nsize of the first element of the boundary layer, the maximum size in the\nboundary layer and the size at the top of the domain. The following\ncontributions to the meshing workflow for ABL flow simulation are performed.\nFirst, we present a smooth topography modeling to query first and second order\ngeometry derivatives. Second, we propose a new adaptive meshing procedure to\ndiscretize the topography based on two different metrics. Third, the ABL mesher\nis presented, featuring both prisms and tetrahedra. We extrude the triangles of\nthe adapted surface mesh, generating prisms that reproduce the Surface Boundary\nLayer. Then, the rest of the domain is meshed with an unstructured tetrahedral\nmesh. In addition, for both the surface and volume meshers we detail a hybrid\nquality optimization approach, analyzing its impact on the solver for\nhigh-complexity terrains. We analyze the convergence of the triangle adaptive\napproach, obtaining quadratic convergence to the geometry and reducing to one\nhalf the error for the same amount of degrees of freedom than without\nadaptivity and optimization. We also study the mesh convergence of our RANS\nsolver, obtaining quadratic mesh convergence to the solution, and using a 30%\nof the degrees of freedom while reducing a 20% of the error of standard\nsemi-structured approaches. Finally, we present the generated meshes and the\nsimulation results for a complete complex topographic scenario.",
    "descriptor": "\nComments: 32 pages, 14 figures\n",
    "authors": [
      "Abel Gargallo-Peir\u00f3",
      "Matias Avila",
      "Arnau Folch"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07410"
  },
  {
    "id": "arXiv:2206.07416",
    "title": "Mutual Visibility by Fat Robots with Slim Omnidirectional Camera",
    "abstract": "In the existing literature of the Mutual Visibility problem for autonomous\nrobot swarms, the adopted visibility models have some idealistic assumptions\nthat are not consistent with practical sensing device implementations. This\npaper investigates the problem in the more realistic visibility model called\nopaque fat robots with slim omnidirectional camera. The robots are modeled as\nunit disks, each having an omnidirectional camera represented as a disk of\nsmaller size. We assume that the robots have compasses that allow agreement in\nthe direction and orientation of both axes of their local coordinate systems.\nThe robots are equipped with visible lights which serve as a medium of\ncommunication and also as a form of memory. We present a distributed algorithm\nfor the Mutual Visibility problem which is provably correct in the\nsemi-synchronous setting. Our algorithm also provides a solution for Leader\nElection which we use as a subroutine in our main algorithm. Although Leader\nElection is trivial with two axis agreement in the full visibility model, it is\nchallenging in our case and is of independent interest.",
    "descriptor": "",
    "authors": [
      "Kaustav Bose",
      "Abhinav Chakraborty",
      "Krishnendu Mukhopadhyaya"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computational Geometry (cs.CG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07416"
  },
  {
    "id": "arXiv:2206.07418",
    "title": "Designing a Provenance Analysis for SGX Enclaves",
    "abstract": "Intel SGX enables memory isolation and static integrity verification of code\nand data stored in user-space memory regions called enclaves. SGX effectively\nshields the execution of enclaves from the underlying untrusted OS. Attackers\ncannot tamper nor examine enclaves' content. However, these properties equally\nchallenge defenders as they are precluded from any provenance analysis to infer\nintrusions inside SGX enclaves. In this work, we propose SgxMonitor, a novel\nprovenance analysis to monitor and identify anomalous executions of enclave\ncode. To this end, we design a technique to extract contextual runtime\ninformation from an enclave and propose a novel model to represent enclaves'\nintrusions. Our experiments show that not only SgxMonitor incurs an overhead\ncomparable to traditional provenance tools, but it also exhibits\nmacro-benchmarks' overheads and slowdowns that marginally affect real use cases\ndeployment. Our evaluation shows SgxMonitor successfully identifies enclave\nintrusions carried out by the state of the art attacks while reporting no false\npositives and negatives during normal enclaves executions, thus supporting the\nuse of SgxMonitor in realistic scenarios.",
    "descriptor": "\nComments: 16 pages, 8 figures\n",
    "authors": [
      "Flavio Toffalini",
      "Mathias Payer",
      "Jianying Zhou",
      "Lorenzo Cavallaro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07418"
  },
  {
    "id": "arXiv:2206.07423",
    "title": "Zero-shot object goal visual navigation",
    "abstract": "Object goal visual navigation is a challenging task that aims to guide a\nrobot to find the target object only based on its visual observation, and the\ntarget is limited to the classes specified in the training stage. However, in\nreal households, there may exist numerous object classes that the robot needs\nto deal with, and it is hard for all of these classes to be contained in the\ntraining stage. To address this challenge, we propose a zero-shot object\nnavigation task by combining zero-shot learning with object goal visual\nnavigation, which aims at guiding robots to find objects belonging to novel\nclasses without any training samples. This task gives rise to the need to\ngeneralize the learned policy to novel classes, which is a less addressed issue\nof object navigation using deep reinforcement learning. To address this issue,\nwe utilize \"class-unrelated\" data as input to alleviate the overfitting of the\nclasses specified in the training stage. The class-unrelated input consists of\ndetection results and cosine similarity of word embeddings, and does not\ncontain any class-related visual features or knowledge graphs. Extensive\nexperiments on the AI2-THOR platform show that our model outperforms the\nbaseline models in both seen and unseen classes, which proves that our model is\nless class-sensitive and generalizes better. Our code is available at\nhttps://github.com/pioneer-innovation/Zero-Shot-Object-Navigation",
    "descriptor": "",
    "authors": [
      "Qianfan Zhao",
      "Lu Zhang",
      "Bin He",
      "Hong Qiao",
      "Zhiyong Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07423"
  },
  {
    "id": "arXiv:2206.07425",
    "title": "Discrete-time Layered-network Epidemics Model with Time-varying  Transition Rates and Multiple Resources",
    "abstract": "This paper studies a discrete-time time-varying multi-layer networked SIWS\n(susceptible-infected-water-susceptible) model with multiple resources under\nboth single-virus and competing multi-virus settings. Besides the\nhuman-to-human interaction, we also consider that the disease can diffuse on\ndifferent types of medium. We use \\emph{resources} to refer to any media that\nthe pathogen of a virus can spread through, and do not restrict the resource\nonly to be water. In the single-virus case, we give a full analysis of the\nsystem's behaviour related to its healthy state and endemic equilibrium. In the\nmulti-virus case, we show analytically that different equilibria appear driven\nby the competition among all viruses. We also show that some analytical results\nof the time-invariant system can be expanded into time-varying cases. Finally,\nwe illustrate the results through some simulations.",
    "descriptor": "",
    "authors": [
      "Shaoxuan Cui",
      "Fangzhou Liu",
      "Hildeberto Jard\u00f3n-Kojakhmetov",
      "Ming Cao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07425"
  },
  {
    "id": "arXiv:2206.07427",
    "title": "Estimating Confidence of Predictions of Individual Classifiers and Their  Ensembles for the Genre Classification Task",
    "abstract": "Genre identification is a subclass of non-topical text classification. The\nmain difference between this task and topical classification is that genres,\nunlike topics, usually do not correspond to simple keywords, and thus they need\nto be defined in terms of their functions in communication. Neural models based\non pre-trained transformers, such as BERT or XLM-RoBERTa, demonstrate SOTA\nresults in many NLP tasks, including non-topical classification. However, in\nmany cases, their downstream application to very large corpora, such as those\nextracted from social media, can lead to unreliable results because of dataset\nshifts, when some raw texts do not match the profile of the training set. To\nmitigate this problem, we experiment with individual models as well as with\ntheir ensembles. To evaluate the robustness of all models we use a prediction\nconfidence metric, which estimates the reliability of a prediction in the\nabsence of a gold standard label. We can evaluate robustness via the confidence\ngap between the correctly classified texts and the misclassified ones on a\nlabeled test corpus, higher gaps make it easier to improve our confidence that\nour classifier made the right decision. Our results show that for all of the\nclassifiers tested in this study, there is a confidence gap, but for the\nensembles, the gap is bigger, meaning that ensembles are more robust than their\nindividual models.",
    "descriptor": "",
    "authors": [
      "Mikhail Lepekhin",
      "Serge Sharoff"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07427"
  },
  {
    "id": "arXiv:2206.07428",
    "title": "Indexing Temporal Relations for Range-Duration Queries",
    "abstract": "Temporal information plays a crucial role in many database applications,\nhowever support for queries on such data is limited. We present an index\nstructure, termed RD-index, to support range-duration queries over interval\ntimestamped relations, which constrain both the range of the tuples' positions\non the timeline and their duration. RD-index is a grid structure in the\ntwo-dimensional space, representing the position on the timeline and the\nduration of timestamps, respectively. Instead of using a regular grid, we\nconsider the data distribution for the construction of the grid in order to\nensure that each grid cell contains approximately the same number of intervals.\nRD-index features provable bounds on the running time of all the operations,\nallow for a simple implementation, and supports very predictable query\nperformance. We benchmark our solution on a variety of datasets and query\nworkloads, investigating both the query rate and the behavior of the individual\nqueries. The results show that RD-index performs better than the baselines on\nrange-duration queries, for which it is explicitly designed. Furthermore, it\noutperforms specialized indexes also on workloads containing queries\nconstraining either only the duration or the range.",
    "descriptor": "",
    "authors": [
      "Matteo Ceccarello",
      "Anton Dign\u00f6s",
      "Johann Gamper",
      "Christina Khnaisser"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.07428"
  },
  {
    "id": "arXiv:2206.07429",
    "title": "Collusion-Resistant Worker Set Selection for Transparent and Verifiable  Voting",
    "abstract": "Collusion occurs when multiple malicious participants of a distributed\nprotocol work together to sabotage or spy on honest participants. Decentralized\nprotocols often rely on a subset of participants called workers for critical\noperations. Collusion between workers can be particularly harmful to the\nsecurity of the protocol. We propose two protocols that select a subset of\nworkers from the set of participants such that the probability of the workers\ncolluding together is minimized. Our first solution is a decentralized protocol\nthat randomly selects workers in a verifiable manner without any trusted\nentities. The second solution is an algorithm that uses a social graph of\nparticipants and community detection to select workers that are socially\ndistant in order to reduce the probability of collusion. We present our\nsolutions in the context of a decentralized voting protocol proposed by\nSchiedermeier et al. [24] that guarantees transparency and verifiability.\nEnabling collusion-resistance in order to ensure democratic voting is clearly\nof paramount importance thus the voting protocol provides a suitable use case\nfor our solutions.",
    "descriptor": "\nComments: 34 pages, 5 figures\n",
    "authors": [
      "Matthieu Bettinger",
      "Lucas Barbero",
      "Omar Hasan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07429"
  },
  {
    "id": "arXiv:2206.07431",
    "title": "Physically-admissible polarimetric data augmentation for road-scene  analysis",
    "abstract": "Polarimetric imaging, along with deep learning, has shown improved\nperformances on different tasks including scene analysis. However, its\nrobustness may be questioned because of the small size of the training\ndatasets. Though the issue could be solved by data augmentation, polarization\nmodalities are subject to physical feasibility constraints unaddressed by\nclassical data augmentation techniques. To address this issue, we propose to\nuse CycleGAN, an image translation technique based on deep generative models\nthat solely relies on unpaired data, to transfer large labeled road scene\ndatasets to the polarimetric domain. We design several auxiliary loss terms\nthat, alongside the CycleGAN losses, deal with the physical constraints of\npolarimetric images. The efficiency of this solution is demonstrated on road\nscene object detection tasks where generated realistic polarimetric images\nallow to improve performances on cars and pedestrian detection up to 9%. The\nresulting constrained CycleGAN is publicly released, allowing anyone to\ngenerate their own polarimetric images.",
    "descriptor": "",
    "authors": [
      "Cyprien Ruffino",
      "Rachel Blin",
      "Samia Ainouz",
      "Gilles Gasso",
      "Romain H\u00e9rault",
      "Fabrice Meriaudeau",
      "St\u00e9phane Canu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07431"
  },
  {
    "id": "arXiv:2206.07434",
    "title": "Self-Supervised Implicit Attention: Guided Attention by The Model Itself",
    "abstract": "We propose Self-Supervised Implicit Attention (SSIA), a new approach that\nadaptively guides deep neural network models to gain attention by exploiting\nthe properties of the models themselves. SSIA is a novel attention mechanism\nthat does not require any extra parameters, computation, or memory access costs\nduring inference, which is in contrast to existing attention mechanism. In\nshort, by considering attention weights as higher-level semantic information,\nwe reconsidered the implementation of existing attention mechanisms and further\npropose generating supervisory signals from higher network layers to guide\nlower network layers for parameter updates. We achieved this by building a\nself-supervised learning task using the hierarchical features of the network\nitself, which only works at the training stage. To verify the effectiveness of\nSSIA, we performed a particular implementation (called an SSIA block) in\nconvolutional neural network models and validated it on several image\nclassification datasets. The experimental results show that an SSIA block can\nsignificantly improve the model performance, even outperforms many popular\nattention methods that require additional parameters and computation costs,\nsuch as Squeeze-and-Excitation and Convolutional Block Attention Module. Our\nimplementation will be available on GitHub.",
    "descriptor": "",
    "authors": [
      "Jinyi Wu",
      "Xun Gong",
      "Zhemin Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07434"
  },
  {
    "id": "arXiv:2206.07435",
    "title": "Forecasting of depth and ego-motion with transformers and  self-supervision",
    "abstract": "This paper addresses the problem of end-to-end self-supervised forecasting of\ndepth and ego motion. Given a sequence of raw images, the aim is to forecast\nboth the geometry and ego-motion using a self supervised photometric loss. The\narchitecture is designed using both convolution and transformer modules. This\nleverages the benefits of both modules: Inductive bias of CNN, and the\nmulti-head attention of transformers, thus enabling a rich spatio-temporal\nrepresentation that enables accurate depth forecasting. Prior work attempts to\nsolve this problem using multi-modal input/output with supervised ground-truth\ndata which is not practical since a large annotated dataset is required.\nAlternatively to prior methods, this paper forecasts depth and ego motion using\nonly self-supervised raw images as input. The approach performs significantly\nwell on the KITTI dataset benchmark with several performance criteria being\neven comparable to prior non-forecasting self-supervised monocular depth\ninference methods.",
    "descriptor": "\nComments: Accepted in ICPR 2022\n",
    "authors": [
      "Houssem Boulahbal",
      "Adrian Voicila",
      "Andrew Comport"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07435"
  },
  {
    "id": "arXiv:2206.07438",
    "title": "Multi-Objective Hyperparameter Optimization -- An Overview",
    "abstract": "Hyperparameter optimization constitutes a large part of typical modern\nmachine learning workflows. This arises from the fact that machine learning\nmethods and corresponding preprocessing steps often only yield optimal\nperformance when hyperparameters are properly tuned. But in many applications,\nwe are not only interested in optimizing ML pipelines solely for predictive\naccuracy; additional metrics or constraints must be considered when determining\nan optimal configuration, resulting in a multi-objective optimization problem.\nThis is often neglected in practice, due to a lack of knowledge and readily\navailable software implementations for multi-objective hyperparameter\noptimization. In this work, we introduce the reader to the basics of multi-\nobjective hyperparameter optimization and motivate its usefulness in applied\nML. Furthermore, we provide an extensive survey of existing optimization\nstrategies, both from the domain of evolutionary algorithms and Bayesian\noptimization. We illustrate the utility of MOO in several specific ML\napplications, considering objectives such as operating conditions, prediction\ntime, sparseness, fairness, interpretability and robustness.",
    "descriptor": "\nComments: 56 pages, 11 figures, submitted to ACM TELO\n",
    "authors": [
      "Florian Karl",
      "Tobias Pielok",
      "Julia Moosbauer",
      "Florian Pfisterer",
      "Stefan Coors",
      "Martin Binder",
      "Lennart Schneider",
      "Janek Thomas",
      "Jakob Richter",
      "Michel Lang",
      "Eduardo C. Garrido-Merch\u00e1n",
      "Juergen Branke",
      "Bernd Bischl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07438"
  },
  {
    "id": "arXiv:2206.07441",
    "title": "Conformance Testing of Mealy Machines Under Input Restrictions",
    "abstract": "We introduce a grey-box conformance testing method for networks of\ninterconnected Mealy Machines. This approach addresses the scenario where all\ninterfaces of the component under test are observable, but its inputs are under\nthe control of other white-box components. We prove new conditions for full\nfault detection that exploit repetitions across branching executions of the\ncomposite machine in a novel way.Finally, we provide experimental evaluation of\nour approach on cascade compositions of up to a thousand states, and show that\nit notably out-performs existing black-box testing techniques.",
    "descriptor": "",
    "authors": [
      "Alberto Larrauri",
      "Roderick Bloem"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.07441"
  },
  {
    "id": "arXiv:2206.07442",
    "title": "Predicting Gender via Eye Movements",
    "abstract": "In this paper, we report the first stable results on gender prediction via\neye movements. We use a dataset with images of faces as stimuli and with a\nlarge number of 370 participants. Stability has two meanings for us: first that\nwe are able to estimate the standard deviation (SD) of a single prediction\nexperiment (it is around 4.1 %); this is achieved by varying the number of\nparticipants. And second, we are able to provide a mean accuracy with a very\nlow standard error (SEM): our accuracy is 65.2 %, and the SEM is 0.80 %; this\nis achieved through many runs of randomly selecting training and test sets for\nthe prediction. Our study shows that two particular classifiers achieve the\nbest accuracies: Random Forests and Logistic Regression. Our results reconfirm\nprevious findings that females are more biased towards the left eyes of the\nstimuli.",
    "descriptor": "",
    "authors": [
      "Rishabh Vallabh Varsha Haria",
      "Sahar Mahdie Klim Al Zaidawi",
      "Sebastian Maneth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07442"
  },
  {
    "id": "arXiv:2206.07446",
    "title": "Understanding and Optimizing Deep Learning Cold-Start Latency on Edge  Devices",
    "abstract": "DNNs are ubiquitous on edge devices nowadays. With its increasing importance\nand use cases, it's not likely to pack all DNNs into device memory and expect\nthat each inference has been warmed up. Therefore, cold inference, the process\nto read, initialize, and execute a DNN model, is becoming commonplace and its\nperformance is urgently demanded to be optimized. To this end, we present\nNNV12, the first on-device inference engine that optimizes for cold inference\nNNV12 is built atop 3 novel optimization knobs: selecting a proper kernel\n(implementation) for each DNN operator, bypassing the weights transformation\nprocess by caching the post-transformed weights on disk, and pipelined\nexecution of many kernels on asymmetric processors. To tackle with the huge\nsearch space, NNV12 employs a heuristic-based scheme to obtain a near-optimal\nkernel scheduling plan. We fully implement a prototype of NNV12 and evaluate\nits performance across extensive experiments. It shows that NNV12 achieves up\nto 15.2x and 401.5x compared to the state-of-the-art DNN engines on edge CPUs\nand GPUs, respectively.",
    "descriptor": "",
    "authors": [
      "Rongjie Yi",
      "Ting Cao",
      "Ao Zhou",
      "Xiao Ma",
      "Shangguang Wang",
      "Mengwei Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07446"
  },
  {
    "id": "arXiv:2206.07454",
    "title": "Collaboration Effect by Co-Authorship on Academic Citation and Social  Attention of Research",
    "abstract": "Academic citation and social attention measure different dimensions of the\nimpact of research results. Both measures do not correlate with each other, and\nthey are influenced by many factors. Among these factors are the field of\nresearch, the type of access, and co-authorship. In this study, the increase in\nthe impact due to co-authorship in scientific articles disaggregated by field\nof research and access type, was quantified. For this, the citations and social\nattention accumulated until the year 2021 by a total of 244,880 research\narticles published in the year 2018, were analyzed. The data source was\nDimensions.ai, and the units of study were research articles in Economics,\nHistory and Archaeology, and Mathematics. As the main results, a small\nproportion of the articles received a large part of the citations and most of\nthe social attention. Both citations and social attention in-creased, in\ngeneral, with the number of co-authors. Thus, the greater the number of\nco-authors, the greater the probability of being cited in academic articles and\nmentioned on social media. The advantage in citation and social attention due\nto collaboration is independent of the access type for the publication.\nFurthermore, although collaboration with an additional co-author is in general\npositive in terms of citation and social attention, these positive effects\nreduce as the number of co-authors increases.",
    "descriptor": "\nComments: 17 pages, 2 figures, 7 tables\n",
    "authors": [
      "Pablo Dorta-Gonz\u00e1lez",
      "Mar\u00eda Isabel Dorta-Gonz\u00e1lez"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.07454"
  },
  {
    "id": "arXiv:2206.07458",
    "title": "VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via  Speech-Visage Feature Selection",
    "abstract": "The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Distinct from the previous methods, our approach is to\nseparate the speech content and the visage-style from a given silent talking\nface video. By guiding the model to independently focus on modeling the two\nrepresentations, we can obtain the speech of high intelligibility from the\nmodel even when the input video of an unseen subject is given. To this end, we\nintroduce speech-visage selection module that separates the speech content and\nthe speaker identity from the visual features of the input video. The\ndisentangled representations are jointly incorporated to synthesize speech\nthrough visage-style based synthesizer which generates speech by coating the\nvisage-styles while maintaining the speech content. Thus, the proposed\nframework brings the advantage of synthesizing the speech containing the right\ncontent even when the silent talking face video of an unseen subject is given.\nWe validate the effectiveness of the proposed framework on the GRID, TCD-TIMIT\nvolunteer, and LRW datasets. The synthesized speech can be heard in\nsupplementary materials.",
    "descriptor": "\nComments: Submitted to ECCV 2022\n",
    "authors": [
      "Joanna Hong",
      "Minsu Kim",
      "Yong Man Ro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07458"
  },
  {
    "id": "arXiv:2206.07459",
    "title": "READ: Aggregating Reconstruction Error into Out-of-distribution  Detection",
    "abstract": "Detecting out-of-distribution (OOD) samples is crucial to the safe deployment\nof a classifier in the real world. However, deep neural networks are known to\nbe overconfident for abnormal data. Existing works directly design score\nfunction by mining the inconsistency from classifier for in-distribution (ID)\nand OOD. In this paper, we further complement this inconsistency with\nreconstruction error, based on the assumption that an autoencoder trained on ID\ndata can not reconstruct OOD as well as ID. We propose a novel method, READ\n(Reconstruction Error Aggregated Detector), to unify inconsistencies from\nclassifier and autoencoder. Specifically, the reconstruction error of raw\npixels is transformed to latent space of classifier. We show that the\ntransformed reconstruction error bridges the semantic gap and inherits\ndetection performance from the original. Moreover, we propose an adjustment\nstrategy to alleviate the overconfidence problem of autoencoder according to a\nfine-grained characterization of OOD data. Under two scenarios of pre-training\nand retraining, we respectively present two variants of our method, namely\nREAD-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED\n(Euclidean Distance) which retrains the classifier. Our methods do not require\naccess to test time OOD data for fine-tuning hyperparameters. Finally, we\ndemonstrate the effectiveness of the proposed methods through extensive\ncomparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10\npre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8%\ncompared with previous state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Wenyu Jiang",
      "Hao Cheng",
      "Mingcai Chen",
      "Shuai Feng",
      "Yuxin Ge",
      "Chongjun Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07459"
  },
  {
    "id": "arXiv:2206.07460",
    "title": "Coarse-to-fine Deep Video Coding with Hyperprior-guided Mode Prediction",
    "abstract": "The previous deep video compression approaches only use the single scale\nmotion compensation strategy and rarely adopt the mode prediction technique\nfrom the traditional standards like H.264/H.265 for both motion and residual\ncompression. In this work, we first propose a coarse-to-fine (C2F) deep video\ncompression framework for better motion compensation, in which we perform\nmotion estimation, compression and compensation twice in a coarse to fine\nmanner. Our C2F framework can achieve better motion compensation results\nwithout significantly increasing bit costs. Observing hyperprior information\n(i.e., the mean and variance values) from the hyperprior networks contains\ndiscriminant statistical information of different patches, we also propose two\nefficient hyperprior-guided mode prediction methods. Specifically, using\nhyperprior information as the input, we propose two mode prediction networks to\nrespectively predict the optimal block resolutions for better motion coding and\ndecide whether to skip residual information from each block for better residual\ncoding without introducing additional bit cost while bringing negligible extra\ncomputation cost. Comprehensive experimental results demonstrate our proposed\nC2F video compression framework equipped with the new hyperprior-guided mode\nprediction methods achieves the state-of-the-art performance on HEVC, UVG and\nMCL-JCV datasets.",
    "descriptor": "\nComments: CVPR2022\n",
    "authors": [
      "Zhihao Hu",
      "Guo Lu",
      "Jinyang Guo",
      "Shan Liu",
      "Wei Jiang",
      "Dong Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07460"
  },
  {
    "id": "arXiv:2206.07461",
    "title": "Conformance Checking with Uncertainty via SMT (Extended Version)",
    "abstract": "Logs of real-life processes often feature uncertainty pertaining the recorded\ntimestamps, data values, and/or events. We consider the problem of checking\nconformance of uncertain logs against data-aware reference processes.\nSpecifically, we show how to solve it via SMT encodings, lifting previous work\non data-aware SMT-based conformance checking to this more sophisticated\nsetting. Our approach is modular, in that it homogeneously accommodates for\ndifferent types of uncertainty. Moreover, using appropriate cost functions,\ndifferent conformance checking tasks can be addressed. We show the correctness\nof our approach and witness feasibility through a proof-of-concept\nimplementation.",
    "descriptor": "",
    "authors": [
      "Paolo Felli",
      "Alessandro Gianola",
      "Marco Montali",
      "Andrey Rivkin",
      "Sarah Winkler"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07461"
  },
  {
    "id": "arXiv:2206.07468",
    "title": "PolyU-BPCoMa: A Dataset and Benchmark Towards Mobile Colorized Mapping  Using a Backpack Multisensorial System",
    "abstract": "Constructing colorized point clouds from mobile laser scanning and images is\na fundamental work in surveying and mapping. It is also an essential\nprerequisite for building digital twins for smart cities. However, existing\npublic datasets are either in relatively small scales or lack accurate\ngeometrical and color ground truth. This paper documents a multisensorial\ndataset named PolyU-BPCoMA which is distinctively positioned towards mobile\ncolorized mapping. The dataset incorporates resources of 3D LiDAR, spherical\nimaging, GNSS and IMU on a backpack platform. Color checker boards are pasted\nin each surveyed area as targets and ground truth data are collected by an\nadvanced terrestrial laser scanner (TLS). 3D geometrical and color information\ncan be recovered in the colorized point clouds produced by the backpack system\nand the TLS, respectively. Accordingly, we provide an opportunity to benchmark\nthe mapping and colorization accuracy simultaneously for a mobile\nmultisensorial system. The dataset is approximately 800 GB in size covering\nboth indoor and outdoor environments. The dataset and development kits are\navailable at https://github.com/chenpengxin/PolyU-BPCoMa.git.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Wenzhong Shi",
      "Pengxin Chen",
      "Muyang Wang",
      "Sheng Bao",
      "Haodong Xiang",
      "Yue Yu",
      "Daping Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07468"
  },
  {
    "id": "arXiv:2206.07472",
    "title": "Collaborative Knowledge Graph Fusion by Exploiting the Open Corpus",
    "abstract": "To alleviate the challenges of building Knowledge Graphs (KG) from scratch, a\nmore general task is to enrich a KG using triples from an open corpus, where\nthe obtained triples contain noisy entities and relations. It is challenging to\nenrich a KG with newly harvested triples while maintaining the quality of the\nknowledge representation. This paper proposes a system to refine a KG using\ninformation harvested from an additional corpus. To this end, we formulate our\ntask as two coupled sub-tasks, namely join event extraction (JEE) and knowledge\ngraph fusion (KGF). We then propose a Collaborative Knowledge Graph Fusion\nFramework to allow our sub-tasks to mutually assist one another in an\nalternating manner. More concretely, the explorer carries out the JEE\nsupervised by both the ground-truth annotation and an existing KG provided by\nthe supervisor. The supervisor then evaluates the triples extracted by the\nexplorer and enriches the KG with those that are highly ranked. To implement\nthis evaluation, we further propose a Translated Relation Alignment Scoring\nMechanism to align and translate the extracted triples to the prior KG.\nExperiments verify that this collaboration can both improve the performance of\nthe JEE and the KGF.",
    "descriptor": "\nComments: Under review by IEEE Transactions on Knowledge and Data Engineering (TKDE)\n",
    "authors": [
      "Yue Wang",
      "Yao Wan",
      "Lu Bai",
      "Lixin Cui",
      "Zhuo Xu",
      "Ming Li",
      "Philip S. Yu",
      "Edwin R Hancock"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07472"
  },
  {
    "id": "arXiv:2206.07474",
    "title": "Priori Error Estimate of Deep Mixed Residual Method for Elliptic PDEs",
    "abstract": "In this work, we derive a priori error estimate of the mixed residual method\nwhen solving some elliptic PDEs. Our work is the first theoretical study of\nthis method. We prove that the neural network solutions will converge if we\nincrease the training samples and network size without any constraint on the\nratio of training samples to the network size. Besides, our results suggest\nthat the mixed residual method can recover high order derivatives better than\nthe deep Ritz method, which has also been verified by our numerical\nexperiments.",
    "descriptor": "",
    "authors": [
      "Lingfeng Li",
      "Xue-cheng Tai",
      "Jiang Yang",
      "Quanhui Zhu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07474"
  },
  {
    "id": "arXiv:2206.07475",
    "title": "Neural Control of Discrete Weak Formulations: Galerkin, Least-Squares  and Minimal-Residual Methods with Quasi-Optimal Weights",
    "abstract": "There is tremendous potential in using neural networks to optimize numerical\nmethods. In this paper, we introduce and analyse a framework for the neural\noptimization of discrete weak formulations, suitable for finite element\nmethods. The main idea of the framework is to include a neural-network function\nacting as a control variable in the weak form. Finding the neural control that\n(quasi-) minimizes a suitable cost (or loss) functional, then yields a\nnumerical approximation with desirable attributes. In particular, the framework\nallows in a natural way the incorporation of known data of the exact solution,\nor the incorporation of stabilization mechanisms (e.g., to remove spurious\noscillations).\nThe main result of our analysis pertains to the well-posedness and\nconvergence of the associated constrained-optimization problem. In particular,\nwe prove under certain conditions, that the discrete weak forms are stable, and\nthat quasi-minimizing neural controls exist, which converge quasi-optimally. We\nspecialize the analysis results to Galerkin, least-squares and minimal-residual\nformulations, where the neural-network dependence appears in the form of\nsuitable weights. Elementary numerical experiments support our findings and\ndemonstrate the potential of the framework.",
    "descriptor": "\nComments: 35 pages, 9 figures\n",
    "authors": [
      "Ignacio Brevis",
      "Ignacio Muga",
      "Kristoffer G. van der Zee"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07475"
  },
  {
    "id": "arXiv:2206.07476",
    "title": "OpenCitations, an open e-infrastructure to foster maximum reuse of  citation data",
    "abstract": "OpenCitations is an independent not-for-profit infrastructure organization\nfor open scholarship dedicated to the publication of open bibliographic and\ncitation data by the use of Semantic Web (Linked Data) technologies.\nOpenCitations collaborates with projects that are part of the Open Science\necosystem and complies with the UNESCO founding principles of Open Science, the\nI4OC recommendations, and the FAIR data principles that data should be\nFindable, Accessible, Interoperable and Reusable. Since its data satisfies all\nthe Reuse guidelines provided by FAIR in terms of richness, provenance, usage\nlicenses and domain-relevant community standards, OpenCitations provides an\nexample of a successful open e-infrastructure in which the reusability of data\nis integral to its mission.",
    "descriptor": "",
    "authors": [
      "Chiara Di Giambattista",
      "Ivan Heibi",
      "Silvio Peroni",
      "David Shotton"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2206.07476"
  },
  {
    "id": "arXiv:2206.07477",
    "title": "What Can Robots Teach Us About The COVID-19 Pandemic? Interactive  Demonstrations of Epidemiological Models Using a Swarm of Brushbots",
    "abstract": "This paper describes the methodology and outcomes of a series of educational\nevents conducted in 2021 which leveraged robot swarms to educate high-school\nand university students about epidemiological models and how they can inform\nsocietal and governmental policies. With a specific focus on the COVID-19\npandemic, the events consisted of 4 online and 3 in-person workshops where\nstudents had the chance to interact with a swarm of 20 custom-built brushbots\n-- small-scale vibration-driven robots optimized for portability and\nrobustness. Through the analysis of data collected during a post-event survey,\nthis paper shows how the events positively impacted the students' views on the\nscientific method to guide real-world decision making, as well as their\ninterest in robotics.",
    "descriptor": "",
    "authors": [
      "Gennaro Notomista",
      "Siddharth Mayya"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07477"
  },
  {
    "id": "arXiv:2206.07496",
    "title": "Normalization, Square Roots, and the Exponential and Logarithmic Maps in  Geometric Algebras of Less than 6D",
    "abstract": "Geometric algebras of dimension $n < 6$ are becoming increasingly popular for\nthe modeling of 3D and 3+1D geometry. With this increased popularity comes the\nneed for efficient algorithms for common operations such as normalization,\nsquare roots, and exponential and logarithmic maps. The current work presents a\nsignature agnostic analysis of these common operations in all geometric\nalgebras of dimension $n < 6$, and gives efficient numerical implementations in\nthe most popular algebras $\\mathbb{R}_{4}$, $\\mathbb{R}_{3,1}$,\n$\\mathbb{R}_{3,0,1}$ and $\\mathbb{R}_{4,1}$, in the hopes of lowering the\nthreshold for adoption of geometric algebra solutions by code maintainers.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Steven De Keninck",
      "Martin Roelfs"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.07496"
  },
  {
    "id": "arXiv:2206.07497",
    "title": "Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and  Evaluations of XAI Methods for ML-Assisted Rare Species Annotations",
    "abstract": "Insects are a crucial part of our ecosystem. Sadly, in the past few decades,\ntheir numbers have worryingly decreased. In an attempt to gain a better\nunderstanding of this process and monitor the insects populations, Deep\nLearning may offer viable solutions. However, given the breadth of their\ntaxonomy and the typical hurdles of fine grained analysis, such as high\nintraclass variability compared to low interclass variability, insect\nclassification remains a challenging task. There are few benchmark datasets,\nwhich impedes rapid development of better AI models. The annotation of rare\nspecies training data, however, requires expert knowledge. Explainable\nArtificial Intelligence (XAI) could assist biologists in these annotation\ntasks, but choosing the optimal XAI method is difficult. Our contribution to\nthese research challenges is threefold: 1) a dataset of thoroughly annotated\nimages of wild bees sampled from the iNaturalist database, 2) a ResNet model\ntrained on the wild bee dataset achieving classification scores comparable to\nsimilar state-of-the-art models trained on other fine-grained datasets and 3)\nan investigation of XAI methods to support biologists in annotation tasks.",
    "descriptor": "\nComments: 6 pages, 7 figures, 1 table submitted to CVPR 2022 All the code and the link to the dataset can be found at this https URL\n",
    "authors": [
      "Teodor Chiaburu",
      "Felix Biessmann",
      "Frank Hausser"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07497"
  },
  {
    "id": "arXiv:2206.07498",
    "title": "Safe Motion Planning for a Mobile Robot Navigating in Environments  Shared with Humans",
    "abstract": "In this paper, a robot navigating an environment shared with humans is\nconsidered, and a cost function that can be exploited in $\\text{RRT}^\\text{X}$,\na randomized sampling-based replanning algorithm that guarantees asymptotic\noptimality, to allow for a safe motion is proposed. The cost function is a path\nlength weighted by a danger index based on a prediction of human motion\nperformed using either a linear stochastic model, assuming constant\nlongitudinal velocity and varying lateral velocity, and a GMM/GMR-based model,\ncomputed on an experimental dataset of human trajectories. The proposed\napproach is validated using a dataset of human trajectories collected in a real\nworld setting.",
    "descriptor": "",
    "authors": [
      "Basak Sakcak",
      "Luca Bascetta"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07498"
  },
  {
    "id": "arXiv:2206.07499",
    "title": "Mitigating Intra-Cell Pilot Contamination in Massive MIMO: A Rate  Splitting Approach",
    "abstract": "Massive MIMO (MaMIMO) has become an integral part of the 5G standard, and is\nenvisioned to be further developed in beyond 5G networks. With a massive number\nof antennas at the base station (BS), MaMIMO is best equipped to cater\nprominent use cases of B5G networks such as enhanced mobile broadband (eMBB),\nultra-reliable low-latency communications (URLLC) and massive machine-type\ncommunications (mMTC) or combinations thereof. However, one of the critical\nchallenges to this pursuit is the sporadic access behaviour of the massive\nnumber of devices in practical networks that inevitably leads to the\nconspicuous pilot contamination problem. Conventional linearly precoded\nphysical layer strategies employed for downlink transmission in time division\nduplex (TDD) MaMIMO would incur a noticeable spectral efficiency (SE) loss in\nthe presence of this pilot contamination. In this paper, we aim to integrate a\nrobust multiple access and interference management strategy named\nrate-splitting multiple access (RSMA) with TDD MaMIMO for downlink transmission\nand investigate its SE performance. We propose a novel downlink transmission\nframework of RSMA in TDD MaMIMO, devise a precoder design strategy and power\nallocation schemes to maximize different network utility functions. Numerical\nresults reveal that RSMA is significantly more robust to pilot contamination\nand always achieves a SE performance that is equal to or better than the\nconventional linearly precoded MaMIMO transmission strategy.",
    "descriptor": "",
    "authors": [
      "Anup Mishra",
      "Yijie Mao",
      "Christo Kurisummoottil Thomas",
      "Luca Sanguinetti",
      "Bruno Clerckx"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.07499"
  },
  {
    "id": "arXiv:2206.07503",
    "title": "Balanced Allocations with the Choice of Noise",
    "abstract": "We consider the allocation of $m$ balls (jobs) into $n$ bins (servers). In\nthe standard Two-Choice process, at each step $t=1,2,\\ldots,m$ we first sample\ntwo randomly chosen bins, compare their two loads and then place a ball in the\nleast loaded bin. It is well-known that for any $m \\geq n$, this results in a\ngap (difference between the maximum and average load) of $\\log_2 \\log n +\n\\Theta(1)$ (with high probability).\nIn this work, we consider Two-Choice in different models with noisy load\ncomparisons. One key model involves an adaptive adversary whose power is\nlimited by some threshold $g \\in \\mathbb{N}$. In each round, such adversary can\ndetermine the result of any load comparison between two bins whose loads differ\nby at most $g$, while if the load difference is greater than $g$, the\ncomparison is correct.\nFor this adversarial model, we first prove that for any $m \\geq n$ the gap is\n$O(g+\\log n)$ with high probability. Then through a refined analysis we prove\nthat if $g \\leq \\log n$, then for any $m \\geq n$ the gap is $O(\\frac{g}{\\log g}\n\\cdot \\log \\log n)$. For constant values of $g$, this generalizes the heavily\nloaded analysis of [BCSV06, TW14] for the Two-Choice process, and establishes\nthat asymptotically the same gap bound holds even if many (or possibly all)\nload comparisons among \"similarly loaded\" bins are wrong. Finally, we\ncomplement these upper bounds with tight lower bounds, which establishes an\ninteresting phase transition on how the parameter $g$ impacts the gap.\nWe also apply a similar analysis to other noise models, including ones where\nbins only update their load information with delay. For example, for the model\nof [BCEFN12] where balls are allocated in consecutive batches of size $n$, we\npresent an improved and tight gap bound of $\\Theta(\\log n/ \\log \\log n )$.",
    "descriptor": "\nComments: Full version of PODC 2022 paper, 77 pages, 11 figures, 6 tables\n",
    "authors": [
      "Dimitrios Los",
      "Thomas Sauerwald"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2206.07503"
  },
  {
    "id": "arXiv:2206.07504",
    "title": "On the complexity of finding set repairs for data-graphs",
    "abstract": "In the deeply interconnected world we live in, pieces of information link\ndomains all around us. As graph databases embrace effectively relationships\namong data and allow processing and querying these connections efficiently,\nthey are rapidly becoming a popular platform for storage that supports a wide\nrange of domains and applications. As in the relational case, it is expected\nthat data preserves a set of integrity constraints that define the semantic\nstructure of the world it represents. When a database does not satisfy its\nintegrity constraints, a possible approach is to search for a 'similar'\ndatabase that does satisfy the constraints, also known as a repair. In this\nwork, we study the problem of computing subset and superset repairs for graph\ndatabases with data values using a notion of consistency based on a set of\nReg-GXPath expressions as integrity constraints. We show that for positive\nfragments of Reg-GXPath these problems admit a polynomial-time algorithm, while\nthe full expressive power of the language renders them intractable.",
    "descriptor": "\nComments: 35 pages , including Appendix\n",
    "authors": [
      "Sergio Abriola",
      "Santiago Cifuentes",
      "Mar\u00eda Vanina Mart\u00ednez",
      "Nina Pardal",
      "Edwin Pin"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.07504"
  },
  {
    "id": "arXiv:2206.07505",
    "title": "Revisiting Some Common Practices in Cooperative Multi-Agent  Reinforcement Learning",
    "abstract": "Many advances in cooperative multi-agent reinforcement learning (MARL) are\nbased on two common design principles: value decomposition and parameter\nsharing. A typical MARL algorithm of this fashion decomposes a centralized\nQ-function into local Q-networks with parameters shared across agents. Such an\nalgorithmic paradigm enables centralized training and decentralized execution\n(CTDE) and leads to efficient learning in practice. Despite all the advantages,\nwe revisit these two principles and show that in certain scenarios, e.g.,\nenvironments with a highly multi-modal reward landscape, value decomposition,\nand parameter sharing can be problematic and lead to undesired outcomes. In\ncontrast, policy gradient (PG) methods with individual policies provably\nconverge to an optimal solution in these cases, which partially supports some\nrecent empirical observations that PG can be effective in many MARL testbeds.\nInspired by our theoretical analysis, we present practical suggestions on\nimplementing multi-agent PG algorithms for either high rewards or diverse\nemergent behaviors and empirically validate our findings on a variety of\ndomains, ranging from the simplified matrix and grid-world games to complex\nbenchmarks such as StarCraft Multi-Agent Challenge and Google Research\nFootball. We hope our insights could benefit the community towards developing\nmore general and more powerful MARL algorithms. Check our project website at\nhttps://sites.google.com/view/revisiting-marl.",
    "descriptor": "\nComments: 15 pages, published as a conference paper in ICML 2022\n",
    "authors": [
      "Wei Fu",
      "Chao Yu",
      "Zelai Xu",
      "Jiaqi Yang",
      "Yi Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07505"
  },
  {
    "id": "arXiv:2206.07506",
    "title": "Legal Provocations for HCI in the Design and Development of Trustworthy  Autonomous Systems",
    "abstract": "We consider a series of legal provocations emerging from the proposed\nEuropean Union AI Act 2021 (AIA) and how they open up new possibilities for HCI\nin the design and development of trustworthy autonomous systems. The AIA\ncontinues the by design trend seen in recent EU regulation of emerging\ntechnologies. The AIA targets AI developments that pose risks to society and\ncitizens fundamental rights, introducing mandatory design and development\nrequirements for high-risk AI systems (HRAIS). These requirements regulate\ndifferent stages of the AI development cycle including ensuring data quality\nand governance strategies, mandating testing of systems, ensuring appropriate\nrisk management, designing for human oversight, and creating technical\ndocumentation. These requirements open up new opportunities for HCI that reach\nbeyond established concerns with the ethics and explainability of AI and\nsituate AI development in human-centered processes and methods of design to\nenable compliance with regulation and foster societal trust in AI.",
    "descriptor": "",
    "authors": [
      "Lachlan D. Urquhart",
      "Glenn McGarry",
      "Andy Crabtree"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.07506"
  },
  {
    "id": "arXiv:2206.07507",
    "title": "YOU SHALL NOT COMPUTE on my Data: Access Policies for Privacy-Preserving  Data Marketplaces and an Implementation for a Distributed Market using MPC",
    "abstract": "Personal data is an attractive source of insights for a diverse field of\nresearch and business. While our data is highly valuable, it is often\nprivacy-sensitive. Thus, regulations like the GDPR restrict what data can be\nlegally published, and what a buyer may do with this sensitive data. While\npersonal data must be protected, we can still sell some insights gathered from\nour data that do not hurt our privacy. A data marketplace is a platform that\nhelps users to sell their data while assisting buyers in discovering relevant\ndatasets. The major challenge such a marketplace faces is balancing between\noffering valuable insights into data while preserving privacy requirements.\nPrivate data marketplaces try to solve this challenge by offering\nprivacy-preserving computations on personal data. Such computations allow for\ncalculating statistics or training machine learning models on personal data\nwithout accessing the data in plain. However, the user selling the data cannot\nrestrict who can buy or what type of computation the data is allowed.\nWe close the latter gap by proposing a flexible access control architecture\nfor private data marketplaces, which can be applied to existing data markets.\nOur architecture enables data sellers to define detailed policies restricting\nwho can buy their data. Furthermore, a seller can control what computation a\nspecific buyer can purchase on the data, and make constraints on its parameters\nto mitigate privacy breaches. The data market's computation system then\nenforces the policies before initiating a computation.\nTo demonstrate the feasibility of our approach, we provide an implementation\nfor the KRAKEN marketplace, a distributed data market using MPC. We show that\nour approach is practical since it introduces a negligible performance overhead\nand is secure against several adversaries.",
    "descriptor": "\nComments: This paper was published in the 17th International Conference on Availability, Reliability and Security (ARES 2022), August 23--26, 2022, Vienna, Austria, ACM\n",
    "authors": [
      "Stefan More",
      "Lukas Alber"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07507"
  },
  {
    "id": "arXiv:2206.07509",
    "title": "Mandheling: Mixed-Precision On-Device DNN Training with DSP Offloading",
    "abstract": "This paper proposes Mandheling, the first system that enables highly\nresource-efficient on-device training by orchestrating the mixed-precision\ntraining with on-chip Digital Signal Processing (DSP) offloading. Mandheling\nfully explores the advantages of DSP in integer-based numerical calculation by\nfour novel techniques: (1) a CPU-DSP co-scheduling scheme to mitigate the\noverhead from DSP-unfriendly operators; (2) a self-adaptive rescaling algorithm\nto reduce the overhead of dynamic rescaling in backward propagation; (3) a\nbatch-splitting algorithm to improve the DSP cache efficiency; (4) a\nDSP-compute subgraph reusing mechanism to eliminate the preparation overhead on\nDSP. We have fully implemented Mandheling and demonstrate its effectiveness\nthrough extensive experiments. The results show that, compared to the\nstate-of-the-art DNN engines from TFLite and MNN, Mandheling reduces the\nper-batch training time by 5.5$\\times$ and the energy consumption by\n8.9$\\times$ on average. In end-to-end training tasks, Mandheling reduces up to\n10.7$\\times$ convergence time and 13.1$\\times$ energy consumption, with only\n1.9%-2.7% accuracy loss compared to the FP32 precision setting.",
    "descriptor": "",
    "authors": [
      "Daliang Xu",
      "Mengwei Xu",
      "Qipeng Wang",
      "Shangguang Wang",
      "Yun Ma",
      "Kang Huang",
      "Guang Huang",
      "Xin Jin",
      "Xuanzhe Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.07509"
  },
  {
    "id": "arXiv:2206.07510",
    "title": "Deep Multi-Task Networks For Occluded Pedestrian Pose Estimation",
    "abstract": "Most of the existing works on pedestrian pose estimation do not consider\nestimating the pose of an occluded pedestrians, as the annotations of the\noccluded parts are not available in relevant automotive datasets. For example,\nCityPersons, a well-known dataset for pedestrian detection in automotive scenes\ndoes not provide pose annotations, whereas MS-COCO, a non-automotive dataset,\ncontains human pose estimation. In this work, we propose a multi-task framework\nto extract pedestrian features through detection and instance segmentation\ntasks performed separately on these two distributions. Thereafter, an encoder\nlearns pose specific features using an unsupervised instance-level domain\nadaptation method for the pedestrian instances from both distributions. The\nproposed framework has improved state-of-the-art performances of pose\nestimation, pedestrian detection, and instance segmentation.",
    "descriptor": "\nComments: 4 pages, 5 tables, 2 figures\n",
    "authors": [
      "Arindam Das",
      "Sudip Das",
      "Ganesh Sistu",
      "Jonathan Horgan",
      "Ujjwal Bhattacharya",
      "Edward Jones",
      "Martin Glavin",
      "Ciar\u00e1n Eising"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07510"
  },
  {
    "id": "arXiv:2206.07511",
    "title": "Investigating Multi-Feature Selection and Ensembling for Audio  Classification",
    "abstract": "Deep Learning (DL) algorithms have shown impressive performance in diverse\ndomains. Among them, audio has attracted many researchers over the last couple\nof decades due to some interesting patterns--particularly in classification of\naudio data. For better performance of audio classification, feature selection\nand combination play a key role as they have the potential to make or break the\nperformance of any DL model. To investigate this role, we conduct an extensive\nevaluation of the performance of several cutting-edge DL models (i.e.,\nConvolutional Neural Network, EfficientNet, MobileNet, Supper Vector Machine\nand Multi-Perceptron) with various state-of-the-art audio features (i.e., Mel\nSpectrogram, Mel Frequency Cepstral Coefficients, and Zero Crossing Rate)\neither independently or as a combination (i.e., through ensembling) on three\ndifferent datasets (i.e., Free Spoken Digits Dataset, Audio Urdu Digits\nDataset, and Audio Gujarati Digits Dataset). Overall, results suggest feature\nselection depends on both the dataset and the model. However, feature\ncombinations should be restricted to the only features that already achieve\ngood performances when used individually (i.e., mostly Mel Spectrogram, Mel\nFrequency Cepstral Coefficients). Such feature combination/ensembling enabled\nus to outperform the previous state-of-the-art results irrespective of our\nchoice of DL model.",
    "descriptor": "",
    "authors": [
      "Muhammad Turab",
      "Teerath Kumar",
      "Malika Bendechache",
      "Takfarinas Saber"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07511"
  },
  {
    "id": "arXiv:2206.07520",
    "title": "Principal Trade-off Analysis",
    "abstract": "This paper develops Principal Trade-off Analysis (PTA), a decomposition\nmethod, analogous to Principal Component Analysis (PCA), which permits the\nrepresentation of any game as the weighted sum of disc games (continuous R-P-S\ngames). Applying PTA to empirically generated tournament graphs produces a\nsequence of embeddings into orthogonal 2D feature planes representing\nindependent strategic trade-offs. Each trade-off generates a mode of cyclic\ncompetition. Like PCA, PTA provides optimal low rank estimates of the\ntournament graphs that can be truncated for approximation. The complexity of\ncyclic competition can be quantified by computing the number of significant\ncyclic modes. We illustrate the PTA via application to a pair of games (Blotto,\nPokemon). The resulting 2D disc game representations are shown to be well\nsuited for visualization and are easily interpretable. In Blotto, PTA\nidentifies game symmetries, and specifies strategic trade-offs associated with\ndistinct win conditions. For Pokemon, PTA embeddings produce clusters in the\nembedding space that naturally correspond to Pokemon types, a design in the\ngame that produces cyclic trade offs.",
    "descriptor": "\nComments: 17 pages, 8 figures\n",
    "authors": [
      "Alexander Strang",
      "David SeWell",
      "Alexander Kim",
      "Kevin Alcedo",
      "David Rosenbluth"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07520"
  },
  {
    "id": "arXiv:2206.07527",
    "title": "QONNX: Representing Arbitrary-Precision Quantized Neural Networks",
    "abstract": "We present extensions to the Open Neural Network Exchange (ONNX) intermediate\nrepresentation format to represent arbitrary-precision quantized neural\nnetworks. We first introduce support for low precision quantization in existing\nONNX-based quantization formats by leveraging integer clipping, resulting in\ntwo new backward-compatible variants: the quantized operator format with\nclipping and quantize-clip-dequantize (QCDQ) format. We then introduce a novel\nhigher-level ONNX format called quantized ONNX (QONNX) that introduces three\nnew operators -- Quant, BipolarQuant, and Trunc -- in order to represent\nuniform quantization. By keeping the QONNX IR high-level and flexible, we\nenable targeting a wider variety of platforms. We also present utilities for\nworking with QONNX, as well as examples of its usage in the FINN and hls4ml\ntoolchains. Finally, we introduce the QONNX model zoo to share low-precision\nquantized neural networks.",
    "descriptor": "\nComments: 9 pages, 5 figures, Contribution to 4th Workshop on Accelerated Machine Learning (AccML) at HiPEAC 2022 Conference\n",
    "authors": [
      "Alessandro Pappalardo",
      "Yaman Umuroglu",
      "Michaela Blott",
      "Jovan Mitrevski",
      "Ben Hawks",
      "Nhan Tran",
      "Vladimir Loncar",
      "Sioni Summers",
      "Hendrik Borras",
      "Jules Muhizi",
      "Matthew Trahms",
      "Shih-Chieh Hsu",
      "Javier Duarte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Programming Languages (cs.PL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07527"
  },
  {
    "id": "arXiv:2206.07528",
    "title": "Corruption-Robust Contextual Search through Density Updates",
    "abstract": "We study the problem of contextual search in the adversarial noise model. Let\n$d$ be the dimension of the problem, $T$ be the time horizon and $C$ be the\ntotal amount of noise in the system. For the $\\eps$-ball loss, we give a tight\nregret bound of $O(C + d \\log(1/\\eps))$ improving over the $O(d^3 \\log(1/\\eps))\n\\log^2(T) + C \\log(T) \\log(1/\\eps))$ bound of Krishnamurthy et al (STOC21). For\nthe symmetric loss, we give an efficient algorithm with regret $O(C+d \\log T)$.\nOur techniques are a significant departure from prior approaches.\nSpecifically, we keep track of density functions over the candidate vectors\ninstead of a knowledge set consisting of the candidate vectors consistent with\nthe feedback obtained.",
    "descriptor": "\nComments: Extended abstract accepted at COLT22\n",
    "authors": [
      "Renato Paes Leme",
      "Chara Podimata",
      "Jon Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.07528"
  },
  {
    "id": "arXiv:2206.07534",
    "title": "Optimal Synthesis of LTI Koopman Models for Nonlinear Systems with  Inputs",
    "abstract": "A popular technique used to obtain linear representations of nonlinear\nsystems is the so-called Koopman approach, where the nonlinear dynamics are\nlifted to a (possibly infinite dimensional) linear space through nonlinear\nfunctions called observables. In the lifted space, the dynamics are linear and\nrepresented by a so-called Koopman operator. While the Koopman theory was\noriginally introduced for autonomous systems, it has been widely used to derive\nlinear time-invariant (LTI) models for nonlinear systems with inputs through\nvarious approximation schemes such as the extended dynamics mode decomposition\n(EDMD). However, recent extensions of the Koopman theory show that the lifting\nprocess for such systems results in a linear parameter-varying (LPV) model\ninstead of an LTI form. As LTI Koopman model based control has been\nsuccessfully used in practice and it is generally temping to use such LTI\ndescriptions of nonlinear systems, due to the simplicity of the associated\ncontrol tool chain, a systematic approach is needed to synthesise optimal LTI\napproximations of LPV Koopman models compared to the ad-hoc schemes such as\nEDMD, which is based on least-squares regression. In this work, we introduce\noptimal LTI Koopman approximations of exact Koopman models of nonlinear systems\nwith inputs by using l2-gain and generalized H2 norm performance measures. We\ndemonstrate the advantages of the proposed Koopman modelling procedure compared\nto EDMD.",
    "descriptor": "\nComments: Preprint submitted to the joint IFAC conference: SSSC-TDS-LPVS, 2022\n",
    "authors": [
      "Lucian Cristian Iacob",
      "Roland T\u00f3th",
      "Maarten Schoukens"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07534"
  },
  {
    "id": "arXiv:2206.07535",
    "title": "BaIT: Barometer for Information Trustworthiness",
    "abstract": "This paper presents a new approach to the FNC-1 fake news classification task\nwhich involves employing pre-trained encoder models from similar NLP tasks,\nnamely sentence similarity and natural language inference, and two neural\nnetwork architectures using this approach are proposed. Methods in data\naugmentation are explored as a means of tackling class imbalance in the\ndataset, employing common pre-existing methods and proposing a method for\nsample generation in the under-represented class using a novel sentence\nnegation algorithm. Comparable overall performance with existing baselines is\nachieved, while significantly increasing accuracy on an under-represented but\nnonetheless important class for FNC-1.",
    "descriptor": "",
    "authors": [
      "Ois\u00edn Nolan",
      "Jeroen van Mourik",
      "Callum Tilbury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07535"
  },
  {
    "id": "arXiv:2206.07536",
    "title": "Autonomous Platoon Control with Integrated Deep Reinforcement Learning  and Dynamic Programming",
    "abstract": "Deep Reinforcement Learning (DRL) is regarded as a potential method for\ncar-following control and has been mostly studied to support a single following\nvehicle. However, it is more challenging to learn a stable and efficient\ncar-following policy when there are multiple following vehicles in a platoon,\nespecially with unpredictable leading vehicle behavior. In this context, we\nadopt an integrated DRL and Dynamic Programming (DP) approach to learn\nautonomous platoon control policies, which embeds the Deep Deterministic Policy\nGradient (DDPG) algorithm into a finite-horizon value iteration framework.\nAlthough the DP framework can improve the stability and performance of DDPG, it\nhas the limitations of lower sampling and training efficiency. In this paper,\nwe propose an algorithm, namely Finite-Horizon-DDPG with Sweeping through\nreduced state space using Stationary approximation (FH-DDPG-SS), which uses\nthree key ideas to overcome the above limitations, i.e., transferring network\nweights backward in time, stationary policy approximation for earlier time\nsteps, and sweeping through reduced state space. In order to verify the\neffectiveness of FH-DDPG-SS, simulation using real driving data is performed,\nwhere the performance of FH-DDPG-SS is compared with those of the benchmark\nalgorithms. Finally, platoon safety and string stability for FH-DDPG-SS are\ndemonstrated.",
    "descriptor": "",
    "authors": [
      "Tong Liu",
      "Lei Lei",
      "Kan Zheng",
      "Kuan Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07536"
  },
  {
    "id": "arXiv:2206.07538",
    "title": "Body Gesture Recognition to Control a Social Robot",
    "abstract": "In this work, we propose a gesture based language to allow humans to interact\nwith robots using their body in a natural way. We have created a new gesture\ndetection model using neural networks and a custom dataset of humans performing\na set of body gestures to train our network. Furthermore, we compare body\ngesture communication with other communication channels to acknowledge the\nimportance of adding this knowledge to robots. The presented approach is\nextensively validated in diverse simulations and real-life experiments with\nnon-trained volunteers. This attains remarkable results and shows that it is a\nvaluable framework for social robotics applications, such as human robot\ncollaboration or human-robot interaction.",
    "descriptor": "",
    "authors": [
      "Javier Laplaza",
      "Joan Jaume Oliver",
      "Ram\u00f3n Romero",
      "Alberto Sanfeliu",
      "Ana\u00eds Garrell"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Databases (cs.DB)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07538"
  },
  {
    "id": "arXiv:2206.07543",
    "title": "P-Index",
    "abstract": "I propose the P-Index that genuinely constitutes a well-defined, compact\nauthor citation metric.",
    "descriptor": "",
    "authors": [
      "Shiva P. Pudasaini"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07543"
  },
  {
    "id": "arXiv:2206.07550",
    "title": "MPI: Evaluating and Inducing Personality in Pre-trained Language Models",
    "abstract": "Originated as a philosophical quest, personality discerns how individuals\ndiffer from each other in terms of thinking, feeling, and behaving. Towards\nbuilding social machines that work with humans on a daily basis, we are\nmotivated to ask: (1) Do existing pre-trained language models possess\npersonality, akin to their human counterpart? If so, (2) how can we evaluate\nthem? Further, given this evaluation framework, (3) how can we induce a certain\npersonality in a fully controllable fashion? To tackle these three questions,\nwe propose the Machine Personality Inventory (MPI) dataset for evaluating the\nmachine personality; MPI follows standardized personality tests, built upon the\nBig Five Personality Factors (Big Five) theory and personality assessment\ninventories. By evaluating models with MPI, we provide the first piece of\nevidence showing the existence of personality in pre-trained language models.\nWe further devise a Chain Prompting method to induce the language model with a\nspecific personality in a controllable manner, capable of producing diversified\nbehaviors. We hope to shed light on future studies by adopting personality as\nthe essential psychological guidance for various downstream tasks, building\nmore human-like and in situ dialogue agents.",
    "descriptor": "",
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07550"
  },
  {
    "id": "arXiv:2206.07551",
    "title": "Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation",
    "abstract": "Open-Set Domain Adaptation (OSDA) assumes that a target domain contains\nunknown classes, which are not discovered in a source domain. Existing domain\nadversarial learning methods are not suitable for OSDA because distribution\nmatching with \\textit{unknown} classes leads to the negative transfer. Previous\nOSDA methods have focused on matching the source and the target distribution by\nonly utilizing \\textit{known} classes. However, this \\textit{known}-only\nmatching may fail to learn the target-\\textit{unknown} feature space.\nTherefore, we propose Unknown-Aware Domain Adversarial Learning (UADAL), which\n\\textit{aligns} the source and the targe-\\textit{known} distribution while\nsimultaneously \\textit{segregating} the target-\\textit{unknown} distribution in\nthe feature alignment procedure. We provide theoretical analyses on the\noptimized state of the proposed \\textit{unknown-aware} feature alignment, so we\ncan guarantee both \\textit{alignment} and \\textit{segregation} theoretically.\nEmpirically, we evaluate UADAL on the benchmark datasets, which shows that\nUADAL outperforms other methods with better feature alignments by reporting the\nstate-of-the-art performances.",
    "descriptor": "\nComments: 9 pages, preprint\n",
    "authors": [
      "JoonHo Jang",
      "Byeonghu Na",
      "DongHyeok Shin",
      "Mingi Ji",
      "Kyungwoo Song",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07551"
  },
  {
    "id": "arXiv:2206.07553",
    "title": "On the fast convergence of minibatch heavy ball momentum",
    "abstract": "Simple stochastic momentum methods are widely used in machine learning\noptimization, but their good practical performance is at odds with an absence\nof theoretical guarantees of acceleration in the literature. In this work, we\naim to close the gap between theory and practice by showing that stochastic\nheavy ball momentum, which can be interpreted as a randomized Kaczmarz\nalgorithm with momentum, retains the fast linear rate of (deterministic) heavy\nball momentum on quadratic optimization problems, at least when minibatching\nwith a sufficiently large batch size is used. The analysis relies on carefully\ndecomposing the momentum transition matrix, and using new spectral norm\nconcentration bounds for products of independent random matrices. We provide\nnumerical experiments to demonstrate that our bounds are reasonably sharp.",
    "descriptor": "",
    "authors": [
      "Raghu Bollapragada",
      "Tyler Chen",
      "Rachel Ward"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07553"
  },
  {
    "id": "arXiv:2206.07554",
    "title": "Hierarchical Clustering in Graph Streams: Single-Pass Algorithms and  Space Lower Bounds",
    "abstract": "The Hierarchical Clustering (HC) problem consists of building a hierarchy of\nclusters to represent a given dataset. Motivated by the modern large-scale\napplications, we study the problem in the \\streaming model, in which the memory\nis heavily limited and only a single or very few passes over the input are\nallowed. Specifically, we investigate whether a good hierarchical clustering\ncan be obtained, or at least whether we can approximately estimate the value of\nthe optimal hierarchy. To measure the quality of a hierarchy, we use the HC\nminimization objective introduced by Dasgupta. Assuming that the input is an\n$n$-vertex weighted graph whose edges arrive in a stream, we derive the\nfollowing results on space-vs-accuracy tradeoffs:\n* With $O(n\\cdot \\text{polylog}\\,{n})$ space, we develop a single-pass\nalgorithm, whose approximation ratio matches the currently best offline\nalgorithm.\n* When the space is more limited, namely, $n^{1-o(1)}$, we prove that no\nalgorithm can even estimate the value of optimum HC tree to within an\n$o(\\frac{\\log{n}}{\\log\\log{n}})$ factor, even when allowed\n$\\text{polylog}{\\,{n}}$ passes over the input.\n* In the most stringent setting of $\\text{polylog}\\,{n}$ space, we rule out\nalgorithms that can even distinguish between \"highly\"-vs-\"poorly\" clusterable\ngraphs, namely, graphs that have an $n^{1/2-o(1)}$ factor gap between their HC\nobjective value.\n* Finally, we prove that any single-pass streaming algorithm that computes an\noptimal HC tree requires to store almost the entire input even if allowed\nexponential time.\nOur algorithmic results establish a general structural result that proves\nthat cut sparsifiers of input graph can preserve cost of \"balanced\" HC trees to\nwithin a constant factor. Our lower bound results include a new streaming lower\nbound for a novel problem \"One-vs-Many-Expanders\", which can be of independent\ninterest.",
    "descriptor": "\nComments: Full version of the paper accepted to COLT 2022. 55 pages, 3 figures\n",
    "authors": [
      "Sepehr Assadi",
      "Vaggos Chatziafratis",
      "Jakub \u0141\u0105cki",
      "Vahab Mirrokni",
      "Chen Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07554"
  },
  {
    "id": "arXiv:2206.07555",
    "title": "Respect as a Lens for the Design of AI Systems",
    "abstract": "Critical examinations of AI systems often apply principles such as fairness,\njustice, accountability, and safety, which is reflected in AI regulations such\nas the EU AI Act. Are such principles sufficient to promote the design of\nsystems that support human flourishing? Even if a system is in some sense fair,\njust, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or\notherwise conflict with cultural, individual, or social values. This paper\nproposes a dimension of interactional ethics thus far overlooked: the ways AI\nsystems should treat human beings. For this purpose, we explore the\nphilosophical concept of respect: if respect is something everyone needs and\ndeserves, shouldn't technology aim to be respectful? Despite its intuitive\nsimplicity, respect in philosophy is a complex concept with many disparate\nsenses. Like fairness or justice, respect can characterise how people deserve\nto be treated; but rather than relating primarily to the distribution of\nbenefits or punishments, respect relates to how people regard one another, and\nhow this translates to perception, treatment, and behaviour. We explore respect\nbroadly across several literatures, synthesising perspectives on respect from\nKantian, post-Kantian, dramaturgical, and agential realist design perspectives\nwith a goal of drawing together a view of what respect could mean for AI. In so\ndoing, we identify ways that respect may guide us towards more sociable\nartefacts that ethically and inclusively honour and recognise humans using the\nrich social language that we have evolved to interact with one another every\nday.",
    "descriptor": "\nComments: To appear in the Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES '22)\n",
    "authors": [
      "William Seymour",
      "Max Van Kleek",
      "Reuben Binns",
      "Dave Murray-Rust"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.07555"
  },
  {
    "id": "arXiv:2206.07556",
    "title": "KGEA: A Knowledge Graph Enhanced Article Quality Identification Dataset",
    "abstract": "With so many articles of varying quality being produced at every moment, it\nis a very urgent task to screen this data for quality articles and commit them\nout to social media. It is worth noting that high quality articles have many\ncharacteristics, such as relevance, text quality, straightforward, multi-sided,\nbackground, novelty and sentiment. Thus, it would be inadequate to purely use\nthe content of an article to identify its quality. Therefore, we plan to use\nthe external knowledge interaction to refine the performance and propose a\nknowledge graph enhanced article quality identification dataset (KGEA) based on\nBaidu Encyclopedia. We quantified the articles through 7 dimensions and use\nco-occurrence of the entities between the articles and the Baidu encyclopedia\nto construct the knowledge graph for every article. We also compared some text\nclassification baselines and found that external knowledge can guide the\narticles to a more competitive classification with the graph neural networks.",
    "descriptor": "",
    "authors": [
      "Chunhui Ai",
      "Derui Wang",
      "Yang Xu",
      "Wenrui Xie",
      "Ziqiang Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07556"
  },
  {
    "id": "arXiv:2206.07557",
    "title": "How to Reduce Change Detection to Semantic Segmentation",
    "abstract": "Change detection (CD) aims to identify changes that occur in an image pair\ntaken different times. Prior methods devise specific networks from scratch to\npredict change masks in pixel-level, and struggle with general segmentation\nproblems. In this paper, we propose a new paradigm that reduces CD to semantic\nsegmentation which means tailoring an existing and powerful semantic\nsegmentation network to solve CD. This new paradigm conveniently enjoys the\nmainstream semantic segmentation techniques to deal with general segmentation\nproblems in CD. Hence we can concentrate on studying how to detect changes. We\npropose a novel and importance insight that different change types exist in CD\nand they should be learned separately. Based on it, we devise a module named\nMTF to extract the change information and fuse temporal features. MTF enjoys\nhigh interpretability and reveals the essential characteristic of CD. And most\nsegmentation networks can be adapted to solve the CD problems with our MTF\nmodule. Finally, we propose C-3PO, a network to detect changes at pixel-level.\nC-3PO achieves state-of-the-art performance without bells and whistles. It is\nsimple but effective and can be considered as a new baseline in this field. Our\ncode will be available.",
    "descriptor": "",
    "authors": [
      "Guo-Hua Wang",
      "Bin-Bin Gao",
      "Chengjie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07557"
  },
  {
    "id": "arXiv:2206.07558",
    "title": "Contextualization and Generalization in Entity and Relation Extraction",
    "abstract": "During the past decade, neural networks have become prominent in Natural\nLanguage Processing (NLP), notably for their capacity to learn relevant word\nrepresentations from large unlabeled corpora. These word embeddings can then be\ntransferred and finetuned for diverse end applications during a supervised\ntraining phase. More recently, in 2018, the transfer of entire pretrained\nLanguage Models and the preservation of their contextualization capacities\nenabled to reach unprecedented performance on virtually every NLP benchmark,\nsometimes even outperforming human baselines. However, as models reach such\nimpressive scores, their comprehension abilities still appear as shallow, which\nreveal limitations of benchmarks to provide useful insights on their factors of\nperformance and to accurately measure understanding capabilities.\nIn this thesis, we study the behaviour of state-of-the-art models regarding\ngeneralization to facts unseen during training in two important Information\nExtraction tasks: Named Entity Recognition (NER) and Relation Extraction (RE).\nIndeed, traditional benchmarks present important lexical overlap between\nmentions and relations used for training and evaluating models, whereas the\nmain interest of Information Extraction is to extract previously unknown\ninformation. We propose empirical studies to separate performance based on\nmention and relation overlap with the training set and find that pretrained\nLanguage Models are mainly beneficial to detect unseen mentions, in particular\nout-of-domain. While this makes them suited for real use cases, there is still\na gap in performance between seen and unseen mentions that hurts generalization\nto new facts. In particular, even state-of-the-art ERE models rely on a shallow\nretention heuristic, basing their prediction more on arguments surface forms\nthan context.",
    "descriptor": "\nComments: PhD Thesis, 122 pages\n",
    "authors": [
      "Bruno Taill\u00e9"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07558"
  },
  {
    "id": "arXiv:2206.07562",
    "title": "Bayesian Federated Learning via Predictive Distribution Distillation",
    "abstract": "For most existing federated learning algorithms, each round consists of\nminimizing a loss function at each client to learn an optimal model at the\nclient, followed by aggregating these client models at the server. Point\nestimation of the model parameters at the clients does not take into account\nthe uncertainty in the models estimated at each client. In many situations,\nhowever, especially in limited data settings, it is beneficial to take into\naccount the uncertainty in the client models for more accurate and robust\npredictions. Uncertainty also provides useful information for other important\ntasks, such as active learning and out-of-distribution (OOD) detection. We\npresent a framework for Bayesian federated learning where each client infers\nthe posterior predictive distribution using its training data and present\nvarious ways to aggregate these client-specific predictive distributions at the\nserver. Since communicating and aggregating predictive distributions can be\nchallenging and expensive, our approach is based on distilling each client's\npredictive distribution into a single deep neural network. This enables us to\nleverage advances in standard federated learning to Bayesian federated learning\nas well. Unlike some recent works that have tried to estimate model uncertainty\nof each client, our work also does not make any restrictive assumptions, such\nas the form of the client's posterior distribution. We evaluate our approach on\nclassification in federated setting, as well as active learning and OOD\ndetection in federated settings, on which our approach outperforms various\nexisting federated learning baselines.",
    "descriptor": "\nComments: 15 pages(9 pages of main content, 2 pages of references, and 4 pages of supplementary content)\n",
    "authors": [
      "Shrey Bhatt",
      "Aishwarya Gupta",
      "Piyush Rai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07562"
  },
  {
    "id": "arXiv:2206.07565",
    "title": "A Meta-Analysis of Distributionally-Robust Models",
    "abstract": "State-of-the-art image classifiers trained on massive datasets (such as\nImageNet) have been shown to be vulnerable to a range of both intentional and\nincidental distribution shifts. On the other hand, several recent classifiers\nwith favorable out-of-distribution (OOD) robustness properties have emerged,\nachieving high accuracy on their target tasks while maintaining their\nin-distribution accuracy on challenging benchmarks. We present a meta-analysis\non a wide range of publicly released models, most of which have been published\nover the last twelve months. Through this meta-analysis, we empirically\nidentify four main commonalities for all the best-performing OOD-robust models,\nall of which illuminate the considerable promise of vision-language\npre-training.",
    "descriptor": "\nComments: To be presented at ICML Workshop on Principles of Distribution Shift 2022. Copyright 2022 by the author(s)\n",
    "authors": [
      "Benjamin Feuer",
      "Ameya Joshi",
      "Chinmay Hegde"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07565"
  },
  {
    "id": "arXiv:2206.07567",
    "title": "A Unifying Approach to Efficient (Near)-Gathering of Disoriented Robots  with Limited Visibility",
    "abstract": "We consider a swarm of $n$ robots in a $d$-dimensional Euclidean space. The\nrobots are oblivious (no persistent memory), disoriented (no common coordinate\nsystem/compass), and have limited visibility (observe other robots up to a\nconstant distance). The basic formation task gathering requires that all robots\nreach the same, not predefined position. In the related near-gathering task,\nthey must reach distinct positions such that every robot sees the entire swarm.\nIn the considered setting, gathering can be solved in $\\mathcal{O}(n +\n\\Delta^2)$ synchronous rounds both in two and three dimensions, where $\\Delta$\ndenotes the initial maximal distance of two robots.\nIn this work, we formalize a key property of efficient gathering protocols\nand use it to define $\\lambda$-contracting protocols. Any such protocol gathers\n$n$ robots in the $d$-dimensional space in $\\Theta(\\Delta^2)$ synchronous\nrounds. We prove that, among others, the $d$-dimensional generalization of the\nGtC-protocol is $\\lambda$-contracting. Remarkably, our improved and generalized\nruntime bound is independent of $n$ and $d$. The independence of $d$ answers an\nopen research question.\nWe also introduce an approach to make any $\\lambda$-contracting protocol\ncollisionfree (robots never occupy the same position) to solve near-gathering.\nThe resulting protocols maintain the runtime of $\\Theta (\\Delta^2)$ and work\neven in the semi-synchronous model. This yields the first near-gathering\nprotocols for disoriented robots and the first proven runtime bound. In\nparticular, we obtain the first protocol to solve Uniform Circle Formation\n(arrange the robots on the vertices of a regular $n$-gon) for oblivious,\ndisoriented robots with limited visibility.",
    "descriptor": "",
    "authors": [
      "Jannik Castenow",
      "Jonas Harbig",
      "Daniel Jung",
      "Peter Kling",
      "Till Knollmann",
      "Friedhelm Meyer auf der Heide"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07567"
  },
  {
    "id": "arXiv:2206.07568",
    "title": "Contrastive Learning as Goal-Conditioned Reinforcement Learning",
    "abstract": "In reinforcement learning (RL), it is easier to solve a task if given a good\nrepresentation. While deep RL should automatically acquire such good\nrepresentations, prior work often finds that learning representations in an\nend-to-end fashion is unstable and instead equip RL algorithms with additional\nrepresentation learning parts (e.g., auxiliary losses, data augmentation). How\ncan we design RL algorithms that directly acquire good representations? In this\npaper, instead of adding representation learning parts to an existing RL\nalgorithm, we show (contrastive) representation learning methods can be cast as\nRL algorithms in their own right. To do this, we build upon prior work and\napply contrastive representation learning to action-labeled trajectories, in\nsuch a way that the (inner product of) learned representations exactly\ncorresponds to a goal-conditioned value function. We use this idea to\nreinterpret a prior RL method as performing contrastive learning, and then use\nthe idea to propose a much simpler method that achieves similar performance.\nAcross a range of goal-conditioned RL tasks, we demonstrate that contrastive RL\nmethods achieve higher success rates than prior non-contrastive methods,\nincluding in the offline RL setting. We also show that contrastive RL\noutperforms prior methods on image-based tasks, without using data augmentation\nor auxiliary objectives.",
    "descriptor": "\nComments: Code is available on the website: this https URL\n",
    "authors": [
      "Benjamin Eysenbach",
      "Tianjun Zhang",
      "Ruslan Salakhutdinov",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07568"
  },
  {
    "id": "arXiv:2206.07570",
    "title": "Calibrating Agent-based Models to Microdata with Graph Neural Networks",
    "abstract": "Calibrating agent-based models (ABMs) to data is among the most fundamental\nrequirements to ensure the model fulfils its desired purpose. In recent years,\nsimulation-based inference methods have emerged as powerful tools for\nperforming this task when the model likelihood function is intractable, as is\noften the case for ABMs. In some real-world use cases of ABMs, both the\nobserved data and the ABM output consist of the agents' states and their\ninteractions over time. In such cases, there is a tension between the desire to\nmake full use of the rich information content of such granular data on the one\nhand, and the need to reduce the dimensionality of the data to prevent\ndifficulties associated with high-dimensional learning tasks on the other. A\npossible resolution is to construct lower-dimensional time-series through the\nuse of summary statistics describing the macrostate of the system at each time\npoint. However, a poor choice of summary statistics can result in an\nunacceptable loss of information from the original dataset, dramatically\nreducing the quality of the resulting calibration. In this work, we instead\npropose to learn parameter posteriors associated with granular microdata\ndirectly using temporal graph neural networks. We will demonstrate that such an\napproach offers highly compelling inductive biases for Bayesian inference using\nthe raw ABM microstates as output.",
    "descriptor": "\nComments: Accepted for a Spotlight presentation at the ICML 2022 Artificial Intelligence for Agent-based Modelling (AI4ABM) Workshop\n",
    "authors": [
      "Joel Dyer",
      "Patrick Cannon",
      "J. Doyne Farmer",
      "Sebastian M. Schmon"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07570"
  },
  {
    "id": "arXiv:2206.07572",
    "title": "A Multifidelity Monte Carlo Method for Realistic Computational Budgets",
    "abstract": "A method for the multifidelity Monte Carlo (MFMC) estimation of statistical\nquantities is proposed which is applicable to computational budgets of any\nsize. Based on a sequence of optimization problems each with a globally\nminimizing closed-form solution, this method extends the usability of a well\nknown MFMC algorithm, recovering it when the computational budget is large\nenough. Theoretical results verify that the proposed approach is at least as\noptimal as its namesake and retains the benefits of multifidelity estimation\nwith minimal assumptions on the budget or amount of available data, providing a\nnotable reduction in variance over simple Monte Carlo estimation.",
    "descriptor": "",
    "authors": [
      "Anthony Gruber",
      "Max Gunzburger",
      "Lili Ju",
      "Zhu Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07572"
  },
  {
    "id": "arXiv:2206.07573",
    "title": "AI and Pathology: Steering Treatment and Predicting Outcomes",
    "abstract": "The combination of data analysis methods, increasing computing capacity, and\nimproved sensors enable quantitative granular, multi-scale, cell-based\nanalyses. We describe the rich set of application challenges related to tissue\ninterpretation and survey AI methods currently used to address these\nchallenges. We focus on a particular class of targeted human tissue analysis -\nhistopathology - aimed at quantitative characterization of disease state,\npatient outcome prediction and treatment steering.",
    "descriptor": "",
    "authors": [
      "Rajarsi Gupta",
      "Jakub Kaczmarzyk",
      "Soma Kobayashi",
      "Tahsin Kurc",
      "Joel Saltz"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2206.07573"
  },
  {
    "id": "arXiv:2206.07578",
    "title": "E2V-SDE: From Asynchronous Events to Fast and Continuous Video  Reconstruction via Neural Stochastic Differential Equations",
    "abstract": "Event cameras respond to brightness changes in the scene asynchronously and\nindependently for every pixel. Due to the properties, these cameras have\ndistinct features: high dynamic range (HDR), high temporal resolution, and low\npower consumption. However, the results of event cameras should be processed\ninto an alternative representation for computer vision tasks. Also, they are\nusually noisy and cause poor performance in areas with few events. In recent\nyears, numerous researchers have attempted to reconstruct videos from events.\nHowever, they do not provide good quality videos due to a lack of temporal\ninformation from irregular and discontinuous data. To overcome these\ndifficulties, we introduce an E2V-SDE whose dynamics are governed in a latent\nspace by Stochastic differential equations (SDE). Therefore, E2V-SDE can\nrapidly reconstruct images at arbitrary time steps and make realistic\npredictions on unseen data. In addition, we successfully adopted a variety of\nimage composition techniques for improving image clarity and temporal\nconsistency. By conducting extensive experiments on simulated and real-scene\ndatasets, we verify that our model outperforms state-of-the-art approaches\nunder various video reconstruction settings. In terms of image quality, the\nLPIPS score improves by up to 12% and the reconstruction speed is 87% higher\nthan that of ET-Net.",
    "descriptor": "\nComments: 2022 CVPR oral\n",
    "authors": [
      "Jongwan Kim",
      "DongJin Lee",
      "Byunggook Na",
      "Seongsik Park",
      "Jeonghee Jo",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07578"
  },
  {
    "id": "arXiv:2206.07579",
    "title": "A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and  Future Directions",
    "abstract": "Clustering is a fundamental machine learning task which has been widely\nstudied in the literature. Classic clustering methods follow the assumption\nthat data are represented as features in a vectorized form through various\nrepresentation learning techniques. As the data become increasingly complicated\nand complex, the shallow (traditional) clustering methods can no longer handle\nthe high-dimensional data type. With the huge success of deep learning,\nespecially the deep unsupervised learning, many representation learning\ntechniques with deep architectures have been proposed in the past decade.\nRecently, the concept of Deep Clustering, i.e., jointly optimizing the\nrepresentation learning and clustering, has been proposed and hence attracted\ngrowing attention in the community. Motivated by the tremendous success of deep\nlearning in clustering, one of the most fundamental machine learning tasks, and\nthe large number of recent advances in this direction, in this paper we conduct\na comprehensive survey on deep clustering by proposing a new taxonomy of\ndifferent state-of-the-art approaches. We summarize the essential components of\ndeep clustering and categorize existing methods by the ways they design\ninteractions between deep representation learning and clustering. Moreover,\nthis survey also provides the popular benchmark datasets, evaluation metrics\nand open-source implementations to clearly illustrate various experimental\nsettings. Last but not least, we discuss the practical applications of deep\nclustering and suggest challenging topics deserving further investigations as\nfuture directions.",
    "descriptor": "\nComments: Github Repo: this https URL\n",
    "authors": [
      "Sheng Zhou",
      "Hongjia Xu",
      "Zhuonan Zheng",
      "Jiawei Chen",
      "Zhao li",
      "Jiajun Bu",
      "Jia Wu",
      "Xin Wang",
      "Wenwu Zhu",
      "Martin Ester"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07579"
  },
  {
    "id": "arXiv:2206.07580",
    "title": "Evaluating object detector ensembles for improving the robustness of  artifact detection in endoscopic video streams",
    "abstract": "In this contribution we use an ensemble deep-learning method for combining\nthe prediction of two individual one-stage detectors (i.e., YOLOv4 and Yolact)\nwith the aim to detect artefacts in endoscopic images. This ensemble strategy\nenabled us to improve the robustness of the individual models without harming\ntheir real-time computation capabilities. We demonstrated the effectiveness of\nour approach by training and testing the two individual models and various\nensemble configurations on the \"Endoscopic Artifact Detection Challenge\"\ndataset. Extensive experiments show the superiority, in terms of mean average\nprecision, of the ensemble approach over the individual models and previous\nworks in the state of the art.",
    "descriptor": "",
    "authors": [
      "Pedro Esteban Chavarrias-Solano",
      "Carlos Axel Garcia-Vega",
      "Francisco Javier Lopez-Tiro",
      "Gilberto Ochoa-Ruiz",
      "Thomas Bazin",
      "Dominique Lamarque",
      "Christian Daul"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07580"
  },
  {
    "id": "arXiv:2206.07581",
    "title": "Learn to Adapt: Robust Drift Detection in Security Domain",
    "abstract": "Deploying robust machine learning models has to account for concept drifts\narising due to the dynamically changing and non-stationary nature of data.\nAddressing drifts is particularly imperative in the security domain due to the\never-evolving threat landscape and lack of sufficiently labeled training data\nat the deployment time leading to performance degradation. Recently proposed\nconcept drift detection methods in literature tackle this problem by\nidentifying the changes in feature/data distributions and periodically\nretraining the models to learn new concepts. While these types of strategies\nshould absolutely be conducted when possible, they are not robust towards\nattacker-induced drifts and suffer from a delay in detecting new attacks. We\naim to address these shortcomings in this work. we propose a robust drift\ndetector that not only identifies drifted samples but also discovers new\nclasses as they arrive in an on-line fashion. We evaluate the proposed method\nwith two security-relevant data sets -- network intrusion data set released in\n2018 and APT Command and Control dataset combined with web categorization data.\nOur evaluation shows that our drifting detection method is not only highly\naccurate but also robust towards adversarial drifts and discovers new classes\nfrom drifted samples.",
    "descriptor": "",
    "authors": [
      "Aditya Kuppa",
      "Nhien-An Le-Khac"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07581"
  },
  {
    "id": "arXiv:2206.07585",
    "title": "NatGen: Generative pre-training by \"Naturalizing\" source code",
    "abstract": "Pre-trained Generative Language models (e.g. PLBART, CodeT5, SPT-Code) for\nsource code yielded strong results on several tasks in the past few years,\nincluding code generation and translation. These models have adopted varying\npre-training objectives to learn statistics of code construction from very\nlarge-scale corpora in a self-supervised fashion; the success of pre-trained\nmodels largely hinges on these pre-training objectives. This paper proposes a\nnew pre-training objective, \"Naturalizing\" of source code, exploiting code's\nbimodal, dual-channel (formal & natural channels) nature. Unlike natural\nlanguage, code's bimodal, dual-channel nature allows us to generate\nsemantically equivalent code at scale. We introduce six classes of semantic\npreserving transformations to introduce un-natural forms of code, and then\nforce our model to produce more natural original programs written by\ndevelopers. Learning to generate equivalent, but more natural code, at scale,\nover large corpora of open-source code, without explicit manual supervision,\nhelps the model learn to both ingest & generate code. We fine-tune our model in\nthree generative Software Engineering tasks: code generation, code translation,\nand code refinement with limited human-curated labeled data and achieve\nstate-of-the-art performance rivaling CodeT5. We show that our pre-trained\nmodel is especially competitive at zero-shot and few-shot learning, and better\nat learning code properties (e.g., syntax, data flow).",
    "descriptor": "\nComments: Accepted to be published in ESEC/FSE 2022\n",
    "authors": [
      "Saikat Chakraborty",
      "Toufique Ahmed",
      "Yangruibo Ding",
      "Premkumar Devanbu",
      "Baishakhi Ray"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.07585"
  },
  {
    "id": "arXiv:2206.07586",
    "title": "Machine Learning is Abduction Inference",
    "abstract": "Concept of Abduction with Gradated Contradictions is introduced here as a\nform of Peirce's abduction inference. The general form of abduction criterion\nis formalized in the proposed Logic of Gradated Contradictions and Logic of\nRecursive Aggregation. Common steps of an abduction procedure as minimization\nof such a criterion are specified as well. It is demonstrated on examples of 14\npopular textbook learners (from hierarchical clustering to k-NN and SVR) that\neach of them performs AGC. The proposed theory explains real life learners, yet\nit avoids any mention of statistics, so it can be considered as a logical\nalternative to the statistical learning theory.",
    "descriptor": "",
    "authors": [
      "Marina Sapir"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.07586"
  },
  {
    "id": "arXiv:2206.07587",
    "title": "AMR Alignment: Paying Attention to Cross-Attention",
    "abstract": "With the surge of Transformer models, many have investigated how attention\nacts on the learned representations. However, attention is still overlooked for\nspecific tasks, such as Semantic Parsing. A popular approach to the formal\nrepresentation of a sentence's meaning is Abstract Meaning Representation\n(AMR). Until now, the alignment between a sentence and its AMR representation\nhas been explored in different ways, such as through rules or via the\nExpectation Maximization (EM) algorithm. In this paper, we investigate the\nability of Transformer-based parsing models to yield effective alignments\nwithout ad-hoc strategies. We present the first in-depth exploration of\ncross-attention for AMR by proxy of alignment between the sentence spans and\nthe semantic units in the graph. We show how current Transformer-based parsers\nimplicitly encode the alignment information in the cross-attention weights and\nhow to leverage it to extract such alignment. Furthermore, we supervise and\nguide cross-attention using alignment, dropping the need for English- and\nAMR-specific rules.",
    "descriptor": "",
    "authors": [
      "Pere-Llu\u00eds Huguet Cabot",
      "Abelardo Carlos Mart\u00ednez Lorenzo",
      "Roberto Navigli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07587"
  },
  {
    "id": "arXiv:2206.07592",
    "title": "In-Range Farthest Point Queries and Related Problem in High Dimensions",
    "abstract": "Range-aggregate query is an important type of queries with numerous\napplications. It aims to obtain some structural information (defined by an\naggregate function $F(\\cdot)$) of the points (from a point set $P$) inside a\ngiven query range $B$. In this paper, we study the range-aggregate query\nproblem in high dimensional space for two aggregate functions: (1) $F(P \\cap\nB)$ is the farthest point in $P \\cap B$ to a query point $q$ in $\\mathbb{R}^d$\nand (2) $F(P \\cap B)$ is the minimum enclosing ball (MEB) of $P \\cap B$. For\nproblem (1), called In-Range Farthest Point (IFP) Query, we develop a\nbi-criteria approximation scheme: For any $\\epsilon>0$ that specifies the\napproximation ratio of the farthest distance and any $\\gamma>0$ that measures\nthe \"fuzziness\" of the query range, we show that it is possible to pre-process\n$P$ into a data structure of size $\\tilde{O}_{\\epsilon,\\gamma}(dn^{1+\\rho})$ in\n$\\tilde{O}_{\\epsilon,\\gamma}(dn^{1+\\rho})$ time such that given any\n$\\mathbb{R}^d$ query ball $B$ and query point $q$, it outputs in\n$\\tilde{O}_{\\epsilon,\\gamma}(dn^{\\rho})$ time a point $p$ that is a\n$(1-\\epsilon)$-approximation of the farthest point to $q$ among all points\nlying in a $(1+\\gamma)$-expansion $B(1+\\gamma)$ of $B$, where $0<\\rho<1$ is a\nconstant depending on $\\epsilon$ and $\\gamma$ and the hidden constants in big-O\nnotations depend only on $\\epsilon$, $\\gamma$ and $\\text{Polylog}(nd)$. For\nproblem (2), we show that the IFP result can be applied to develop query scheme\nwith similar time and space complexities to achieve a\n$(1+\\epsilon)$-approximation for MEB.",
    "descriptor": "",
    "authors": [
      "Ziyun Huang",
      "Jinhui Xu"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.07592"
  },
  {
    "id": "arXiv:2206.07593",
    "title": "HICEM: A High-Coverage Emotion Model for Artificial Emotional  Intelligence",
    "abstract": "As social robots and other intelligent machines enter the home, artificial\nemotional intelligence (AEI) is taking center stage to address users' desire\nfor deeper, more meaningful human-machine interaction. To accomplish such\nefficacious interaction, the next-generation AEI need comprehensive human\nemotion models for training. Unlike theory of emotion, which has been the\nhistorical focus in psychology, emotion models are a descriptive tools. In\npractice, the strongest models need robust coverage, which means defining the\nsmallest core set of emotions from which all others can be derived. To achieve\nthe desired coverage, we turn to word embeddings from natural language\nprocessing. Using unsupervised clustering techniques, our experiments show that\nwith as few as 15 discrete emotion categories, we can provide maximum coverage\nacross six major languages--Arabic, Chinese, English, French, Spanish, and\nRussian. In support of our findings, we also examine annotations from two\nlarge-scale emotion recognition datasets to assess the validity of existing\nemotion models compared to human perception at scale. Because robust,\ncomprehensive emotion models are foundational for developing real-world\naffective computing applications, this work has broad implications in social\nrobotics, human-machine interaction, mental healthcare, and computational\npsychology.",
    "descriptor": "",
    "authors": [
      "Benjamin Wortman",
      "James Z. Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07593"
  },
  {
    "id": "arXiv:2206.07604",
    "title": "ARES: Locally Adaptive Reconstruction-based Anomaly Scoring",
    "abstract": "How can we detect anomalies: that is, samples that significantly differ from\na given set of high-dimensional data, such as images or sensor data? This is a\npractical problem with numerous applications and is also relevant to the goal\nof making learning algorithms more robust to unexpected inputs. Autoencoders\nare a popular approach, partly due to their simplicity and their ability to\nperform dimension reduction. However, the anomaly scoring function is not\nadaptive to the natural variation in reconstruction error across the range of\nnormal samples, which hinders their ability to detect real anomalies. In this\npaper, we empirically demonstrate the importance of local adaptivity for\nanomaly scoring in experiments with real data. We then propose our novel\nAdaptive Reconstruction Error-based Scoring approach, which adapts its scoring\nbased on the local behaviour of reconstruction error over the latent space. We\nshow that this improves anomaly detection performance over relevant baselines\nin a wide variety of benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Adam Goodge",
      "Bryan Hooi",
      "See Kiong Ng",
      "Wee Siong Ng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07604"
  },
  {
    "id": "arXiv:2206.07609",
    "title": "Epistemic Deep Learning",
    "abstract": "The belief function approach to uncertainty quantification as proposed in the\nDemspter-Shafer theory of evidence is established upon the general mathematical\nmodels for set-valued observations, called random sets. Set-valued predictions\nare the most natural representations of uncertainty in machine learning. In\nthis paper, we introduce a concept called epistemic deep learning based on the\nrandom-set interpretation of belief functions to model epistemic learning in\ndeep neural networks. We propose a novel random-set convolutional neural\nnetwork for classification that produces scores for sets of classes by learning\nset-valued ground truth representations. We evaluate different formulations of\nentropy and distance measures for belief functions as viable loss functions for\nthese random-set networks. We also discuss methods for evaluating the quality\nof epistemic predictions and the performance of epistemic random-set neural\nnetworks. We demonstrate through experiments that the epistemic approach\nproduces better performance results when compared to traditional approaches of\nestimating uncertainty.",
    "descriptor": "\nComments: Accepted at ICML 2022 Workshop on Distribution-Free Uncertainty Quantification\n",
    "authors": [
      "Shireen Kudukkil Manchingal",
      "Fabio Cuzzolin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07609"
  },
  {
    "id": "arXiv:2206.07615",
    "title": "The SIGMORPHON 2022 Shared Task on Morpheme Segmentation",
    "abstract": "The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems\nto decompose a word into a sequence of morphemes and covered most types of\nmorphology: compounds, derivations, and inflections. Subtask 1, word-level\nmorpheme segmentation, covered 5 million words in 9 languages (Czech, English,\nSpanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13\nsystem submissions from 7 teams and the best system averaged 97.29% F1 score\nacross all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2,\nsentence-level morpheme segmentation, covered 18,735 sentences in 3 languages\n(Czech, English, Mongolian), received 10 system submissions from 3 teams, and\nthe best systems outperformed all three state-of-the-art subword tokenization\nmethods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis\nand support any type of future studies, we released all system predictions, the\nevaluation script, and all gold standard datasets.",
    "descriptor": "\nComments: The 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology\n",
    "authors": [
      "Khuyagbaatar Batsuren",
      "G\u00e1bor Bella",
      "Aryaman Arora",
      "Viktor Martinovi\u0107",
      "Kyle Gorman",
      "Zden\u011bk \u017dabokrtsk\u00fd",
      "Amarsanaa Ganbold",
      "\u0160\u00e1rka Dohnalov\u00e1",
      "Magda \u0160ev\u010d\u00edkov\u00e1",
      "Kate\u0159ina Pelegrinov\u00e1",
      "Fausto Giunchiglia",
      "Ryan Cotterell",
      "Ekaterina Vylomova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07615"
  },
  {
    "id": "arXiv:2206.07623",
    "title": "Dynamic State Estimation of Nonlinear Differential Algebraic Equation  Models of Power Networks",
    "abstract": "This paper investigates the joint problems of dynamic state estimation of\nalgebraic variables (voltage and phase angle) and generator states (rotor angle\nand frequency) of nonlinear differential algebraic equation (NDAE) power\nnetwork models, under uncertainty. Traditionally, these two problems have been\ndecoupled due to complexity of handling NDAE models. In particular, this paper\noffers the first attempt to solve the aforementioned problem in a coupled\napproach where the algebraic and generator states estimates are simultaneously\ncomputed. The proposed estimation algorithm herein is endowed with the\nfollowing properties: (i) it is fairly simple to implement and based on\nwell-understood Lyapunov theory; (ii) considers various sources of uncertainty\nfrom generator control inputs, loads, renewables, process and measurement\nnoise; (iii) models phasor measurement unit installations at arbitrary buses;\nand (iv) is computationally less intensive than the decoupled approach in the\nliterature.",
    "descriptor": "\nComments: IEEE Transactions on Power Systems, In Press, June 2022\n",
    "authors": [
      "Muhammad Nadeem",
      "Sebastian A. Nugroho",
      "Ahmad F. Taha"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07623"
  },
  {
    "id": "arXiv:2206.07625",
    "title": "A linear adaptive BDF2 scheme for phase field crystal equation",
    "abstract": "In this paper, we present and analyze a linear fully discrete second order\nscheme with variable time steps for the phase field crystal equation. More\nprecisely, we construct a linear adaptive time stepping scheme based on the\nsecond order backward differentiation formulation (BDF2) and use the Fourier\nspectral method for the spatial discretization. The scalar auxiliary variable\napproach is employed to deal with the nonlinear term, in which we only adopt a\nfirst order method to approximate the auxiliary variable. This treatment is\nextremely important in the derivation of the unconditional energy stability of\nthe proposed adaptive BDF2 scheme. However, we find for the first time that\nthis strategy will not affect the second order accuracy of the unknown phase\nfunction $\\phi^{n}$ by setting the positive constant $C_{0}$ large enough such\nthat $C_{0}\\geq 1/\\Dt.$ The energy stability of the adaptive BDF2 scheme is\nestablished with a mild constraint on the adjacent time step radio\n$\\gamma_{n+1}:=\\Dt_{n+1}/\\Dt_{n}\\leq 4.8645$. Furthermore, a rigorous error\nestimate of the second order accuracy of $\\phi^{n}$ is derived for the proposed\nscheme on the nonuniform mesh by using the uniform $H^{2}$ bound of the\nnumerical solutions. Finally, some numerical experiments are carried out to\nvalidate the theoretical results and demonstrate the efficiency of the fully\ndiscrete adaptive BDF2 scheme.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Dianming Hou",
      "Zhonghua Qiao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07625"
  },
  {
    "id": "arXiv:2206.07627",
    "title": "Exploring Capabilities of Monolingual Audio Transformers using Large  Datasets in Automatic Speech Recognition of Czech",
    "abstract": "In this paper, we present our progress in pretraining Czech monolingual audio\ntransformers from a large dataset containing more than 80 thousand hours of\nunlabeled speech, and subsequently fine-tuning the model on automatic speech\nrecognition tasks using a combination of in-domain data and almost 6 thousand\nhours of out-of-domain transcribed speech. We are presenting a large palette of\nexperiments with various fine-tuning setups evaluated on two public datasets\n(CommonVoice and VoxPopuli) and one extremely challenging dataset from the\nMALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust\nASR systems, which can take advantage of large labeled and unlabeled datasets\nand successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec\nmodels proved to be good zero-shot learners when no training data are available\nfor the target ASR task.",
    "descriptor": "\nComments: to be published in Proceedings of INTERSPEECH 2022\n",
    "authors": [
      "Jan Lehe\u010dka",
      "Jan \u0160vec",
      "Ale\u0161 Pra\u017e\u00e1k",
      "Josef V. Psutka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07627"
  },
  {
    "id": "arXiv:2206.07631",
    "title": "Clustered Scheduling and Communication Pipelining For Efficient Resource  Management Of Wireless Federated Learning",
    "abstract": "This paper proposes using communication pipelining to enhance the wireless\nspectrum utilization efficiency and convergence speed of federated learning in\nmobile edge computing applications. Due to limited wireless sub-channels, a\nsubset of the total clients is scheduled in each iteration of federated\nlearning algorithms. On the other hand, the scheduled clients wait for the\nslowest client to finish its computation. We propose to first cluster the\nclients based on the time they need per iteration to compute the local\ngradients of the federated learning model. Then, we schedule a mixture of\nclients from all clusters to send their local updates in a pipelined manner. In\nthis way, instead of just waiting for the slower clients to finish their\ncomputation, more clients can participate in each iteration. While the time\nduration of a single iteration does not change, the proposed method can\nsignificantly reduce the number of required iterations to achieve a target\naccuracy. We provide a generic formulation for optimal client clustering under\ndifferent settings, and we analytically derive an efficient algorithm for\nobtaining the optimal solution. We also provide numerical results to\ndemonstrate the gains of the proposed method for different datasets and deep\nlearning architectures.",
    "descriptor": "\nComments: 31 pages, 4 figures\n",
    "authors": [
      "Cihat Ke\u00e7eci",
      "Mohammad Shaqfeh",
      "Fawaz Al-Qahtani",
      "Muhammad Ismail",
      "Erchin Serpedin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07631"
  },
  {
    "id": "arXiv:2206.07633",
    "title": "Sublinear Algorithms for Hierarchical Clustering",
    "abstract": "Hierarchical clustering over graphs is a fundamental task in data mining and\nmachine learning with applications in domains such as phylogenetics, social\nnetwork analysis, and information retrieval. Specifically, we consider the\nrecently popularized objective function for hierarchical clustering due to\nDasgupta. Previous algorithms for (approximately) minimizing this objective\nfunction require linear time/space complexity. In many applications the\nunderlying graph can be massive in size making it computationally challenging\nto process the graph even using a linear time/space algorithm. As a result,\nthere is a strong interest in designing algorithms that can perform global\ncomputation using only sublinear resources. The focus of this work is to study\nhierarchical clustering for massive graphs under three well-studied models of\nsublinear computation which focus on space, time, and communication,\nrespectively, as the primary resources to optimize: (1) (dynamic) streaming\nmodel where edges are presented as a stream, (2) query model where the graph is\nqueried using neighbor and degree queries, (3) MPC model where the graph edges\nare partitioned over several machines connected via a communication channel.\nWe design sublinear algorithms for hierarchical clustering in all three\nmodels above. At the heart of our algorithmic results is a view of the\nobjective in terms of cuts in the graph, which allows us to use a relaxed\nnotion of cut sparsifiers to do hierarchical clustering while introducing only\na small distortion in the objective function. Our main algorithmic\ncontributions are then to show how cut sparsifiers of the desired form can be\nefficiently constructed in the query model and the MPC model. We complement our\nalgorithmic results by establishing nearly matching lower bounds that rule out\nthe possibility of designing better algorithms in each of these models.",
    "descriptor": "",
    "authors": [
      "Arpit Agarwal",
      "Sanjeev Khanna",
      "Huan Li",
      "Prathamesh Patil"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07633"
  },
  {
    "id": "arXiv:2206.07634",
    "title": "Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with  Occlusion Handling for 3D Detection and Segmentation",
    "abstract": "Object detection and semantic segmentation with the 3D lidar point cloud data\nrequire expensive annotation. We propose a data augmentation method that takes\nadvantage of already annotated data multiple times. We propose an augmentation\nframework that reuses real data, automatically finds suitable placements in the\nscene to be augmented, and handles occlusions explicitly. Due to the usage of\nthe real data, the scan points of newly inserted objects in augmentation\nsustain the physical characteristics of the lidar, such as intensity and\nraydrop. The pipeline proves competitive in training top-performing models for\n3D object detection and semantic segmentation. The new augmentation provides a\nsignificant performance gain in rare and essential classes, notably 6.65%\naverage precision gain for \"Hard\" pedestrian class in KITTI object detection or\n2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state\nof the art.",
    "descriptor": "\nComments: Submitted on 15th June 2022 to IEEE RA-L journal\n",
    "authors": [
      "Petr \u0160ebek",
      "\u0160imon Pokorn\u00fd",
      "Patrik Vacek",
      "Tom\u00e1\u0161 Svoboda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07634"
  },
  {
    "id": "arXiv:2206.07635",
    "title": "AI Ethics Issues in Real World: Evidence from AI Incident Database",
    "abstract": "With the powerful performance of Artificial Intelligence (AI) also comes\nprevalent ethical issues. Though governments and corporations have curated\nmultiple AI ethics guidelines to curb unethical behavior of AI, the effect has\nbeen limited, probably due to the vagueness of the guidelines. In this paper,\nwe take a closer look at how AI ethics issues take place in real world, in\norder to have a more in-depth and nuanced understanding of different ethical\nissues as well as their social impact. With a content analysis of AI Incident\nDatabase, which is an effort to prevent repeated real world AI failures by\ncataloging incidents, we identified 13 application areas which often see\nunethical use of AI, with intelligent service robots, language/vision models\nand autonomous driving taking the lead. Ethical issues appear in 8 different\nforms, from inappropriate use and racial discrimination, to physical safety and\nunfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI\npractitioners with a practical guideline when trying to deploy AI applications\nethically.",
    "descriptor": "",
    "authors": [
      "Mengyi Wei",
      "Zhixuan Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.07635"
  },
  {
    "id": "arXiv:2206.07636",
    "title": "SHREC 2022: Fitting and recognition of simple geometric primitives on  point clouds",
    "abstract": "This paper presents the methods that have participated in the SHREC 2022\ntrack on the fitting and recognition of simple geometric primitives on point\nclouds. As simple primitives we mean the classical surface primitives derived\nfrom constructive solid geometry, i.e., planes, spheres, cylinders, cones and\ntori. The aim of the track is to evaluate the quality of automatic algorithms\nfor fitting and recognising geometric primitives on point clouds. Specifically,\nthe goal is to identify, for each point cloud, its primitive type and some\ngeometric descriptors. For this purpose, we created a synthetic dataset,\ndivided into a training set and a test set, containing segments perturbed with\ndifferent kinds of point cloud artifacts. Among the six participants to this\ntrack, two are based on direct methods, while four are either fully based on\ndeep learning or combine direct and neural approaches. The performance of the\nmethods is evaluated using various classification and approximation measures.",
    "descriptor": "",
    "authors": [
      "Chiara Romanengo",
      "Andrea Raffo",
      "Silvia Biasotti",
      "Bianca Falcidieno",
      "Vlassis Fotis",
      "Ioannis Romanelis",
      "Eleftheria Psatha",
      "Konstantinos Moustakas",
      "Ivan Sipiran",
      "Quang-Thuc Nguyen",
      "Chi-Bien Chu",
      "Khoi-Nguyen Nguyen-Ngoc",
      "Dinh-Khoi Vo",
      "Tuan-An To",
      "Nham-Tan Nguyen",
      "Nhat-Quynh Le-Pham",
      "Hai-Dang Nguyen",
      "Minh-Triet Tran",
      "Yifan Qie",
      "Nabil Anwer"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07636"
  },
  {
    "id": "arXiv:2206.07639",
    "title": "Energy Saving Techniques for Energy Constrained CMOS Circuits and  Systems",
    "abstract": "Portable devices like smartphones, tablets, wearable electronic devices,\nmedical implants, wireless sensor nodes, and Internet-of-Things (IoT) devices\nhave tremendous constraints on their energy consumption. Adding more\nfunctionalities onto the portable devices increases its energy consumption\nsignificantly. However, the energy capacity of the battery does not increase\nproportionally. Hence, to overcome the constraints on energy consumption, two\nmain approaches are being undertaken by the designers to integrate more\nfunctionalities onto the energy-constrained systems. One approach involves\nreducing the inherent energy consumption of circuits, and the other involves\nharvesting energy from the ambient sources and utilizing it to power the\ncircuits. In this thesis, both the approaches mentioned above are followed in\ndeveloping energy-saving techniques for energy-constrained CMOS circuits and\nsystems. Switched-capacitor-assisted power gating technique is proposed to\nreduce the leakage current of large digital circuits. Benefits in overall\nenergy saving using nano-electromechanical switches (NEMS) for power gating is\nquantified on a system-on chip for a mobile platform made using a 14 nm gate\nlength FinFET. For reducing the quiescent current in the analog front-end\ncircuits, this thesis proposes a novel technique of realizing discrete-time\n(D-T) signal amplification using nano-electromechanical switches (NEMS). Apart\nfrom the three energy reduction techniques mentioned above, harvesting energy\nfrom mechanical vibrations using an efficient technique of power extraction\nfrom the piezoelectric transducer is proposed.",
    "descriptor": "",
    "authors": [
      "Sivaneswaran Sankar"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.07639"
  },
  {
    "id": "arXiv:2206.07642",
    "title": "Convergence and Price of Anarchy Guarantees of the Softmax Policy  Gradient in Markov Potential Games",
    "abstract": "We study the performance of policy gradient methods for the subclass of\nMarkov games known as Markov potential games (MPGs), which extends the notion\nof normal-form potential games to the stateful setting and includes the\nimportant special case of the fully cooperative setting where the agents share\nan identical reward function. Our focus in this paper is to study the\nconvergence of the policy gradient method for solving MPGs under softmax policy\nparameterization, both tabular and parameterized with general function\napproximators such as neural networks. We first show the asymptotic convergence\nof this method to a Nash equilibrium of MPGs for tabular softmax policies.\nSecond, we derive the finite-time performance of the policy gradient in two\nsettings: 1) using the log-barrier regularization, and 2) using the natural\npolicy gradient under the best-response dynamics (NPG-BR). Finally, extending\nthe notion of price of anarchy (POA) and smoothness in normal-form games, we\nintroduce the POA for MPGs and provide a POA bound for NPG-BR. To our\nknowledge, this is the first POA bound for solving MPGs. To support our\ntheoretical results, we empirically compare the convergence rates and POA of\npolicy gradient variants for both tabular and neural softmax policies.",
    "descriptor": "",
    "authors": [
      "Dingyang Chen",
      "Qi Zhang",
      "Thinh T. Doan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07642"
  },
  {
    "id": "arXiv:2206.07643",
    "title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone",
    "abstract": "Vision-language (VL) pre-training has recently received considerable\nattention. However, most existing end-to-end pre-training approaches either\nonly aim to tackle VL tasks such as image-text retrieval, visual question\nanswering (VQA) and image captioning that test high-level understanding of\nimages, or only target region-level understanding for tasks such as phrase\ngrounding and object detection. We present FIBER (Fusion-In-the-Backbone-based\ntransformER), a new VL model architecture that can seamlessly handle both these\ntypes of tasks. Instead of having dedicated transformer layers for fusion after\nthe uni-modal backbones, FIBER pushes multimodal fusion deep into the model by\ninserting cross-attention into the image and text backbones, bringing gains in\nterms of memory and performance. In addition, unlike previous work that is\neither only pre-trained on image-text data or on fine-grained data with\nbox-level annotations, we present a two-stage pre-training strategy that uses\nboth these kinds of data efficiently: (i) coarse-grained pre-training based on\nimage-text data; followed by (ii) fine-grained pre-training based on\nimage-text-box data. We conduct comprehensive experiments on a wide range of VL\ntasks, ranging from VQA, image captioning, and retrieval, to phrase grounding,\nreferring expression comprehension, and object detection. Using deep multimodal\nfusion coupled with the two-stage pre-training, FIBER provides consistent\nperformance improvements over strong baselines across all tasks, often\noutperforming methods using magnitudes more data. Code is available at\nhttps://github.com/microsoft/FIBER.",
    "descriptor": "\nComments: Project Website: this https URL\n",
    "authors": [
      "Zi-Yi Dou",
      "Aishwarya Kamath",
      "Zhe Gan",
      "Pengchuan Zhang",
      "Jianfeng Wang",
      "Linjie Li",
      "Zicheng Liu",
      "Ce Liu",
      "Yann LeCun",
      "Nanyun Peng",
      "Jianfeng Gao",
      "Lijuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07643"
  },
  {
    "id": "arXiv:2206.07647",
    "title": "Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a  Scalable Hyper-Ensemble Solution",
    "abstract": "Outlier detection (OD) literature exhibits numerous algorithms as it applies\nto diverse domains. However, given a new detection task, it is unclear how to\nchoose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in\nunsupervised settings. HP tuning is an ever-growing problem with the arrival of\nmany new detectors based on deep learning. While they have appealing properties\nsuch as task- driven representation learning and end-to-end optimization, deep\nmodels come with a long list of HPs. Surprisingly, the issue of model selection\nin the outlier mining literature has been \"the elephant in the room\"; a\nsignificant factor in unlocking the utmost potential of deep methods, yet\nlittle said or done to systematically tackle the issue. In the first part of\nthis paper, we conduct the first large-scale analysis on the HP sensitivity of\ndeep OD methods, and through more than 35,000 trained models, quantitatively\ndemonstrate that model selection is inevitable. Next, we design a HP-robust and\nscalable deep hyper-ensemble model called ROBOD that assembles models with\nvarying HP configurations, bypassing the choice paralysis. Importantly, we\nintroduce novel strategies to speed up ensemble training, such as parameter\nsharing, batch/simultaneous training, and data subsampling, that allow us to\ntrain fewer models with fewer parameters. Extensive experiments on both image\nand tabular datasets show that ROBOD achieves and retains robust,\nstate-of-the-art detection performance as compared to its modern counterparts,\nwhile taking only 2-10% of the time by the naive hyper-ensemble with\nindependent training.",
    "descriptor": "\nComments: 19 pages, 11 figures, 9 tables\n",
    "authors": [
      "Xueying Ding",
      "Lingxiao Zhao",
      "Leman Akoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.07647"
  },
  {
    "id": "arXiv:2206.07658",
    "title": "Experimental Validation of Spectral-Spatial Power Evolution Design Using  Raman Amplifiers",
    "abstract": "We experimentally validate a machine learning-enabled Raman amplification\nframework, capable of jointly shaping the signal power evolution in two\ndomains: frequency and fiber distance. The proposed experiment addresses the\namplification in the whole C-band, by optimizing four first-order\ncounter-propagating Raman pumps.",
    "descriptor": "\nComments: 4 pages, 5 figures\n",
    "authors": [
      "Mehran Soltani",
      "Francesco Da Ros",
      "Andrea Carena",
      "Darko Zibar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07658"
  },
  {
    "id": "arXiv:2206.07659",
    "title": "Model-based RL with Optimistic Posterior Sampling: Structural Conditions  and Sample Complexity",
    "abstract": "We propose a general framework to design posterior sampling methods for\nmodel-based RL. We show that the proposed algorithms can be analyzed by\nreducing regret to Hellinger distance based conditional probability estimation.\nWe further show that optimistic posterior sampling can control this Hellinger\ndistance, when we measure model error via data likelihood. This technique\nallows us to design and analyze unified posterior sampling algorithms with\nstate-of-the-art sample complexity guarantees for many model-based RL settings.\nWe illustrate our general result in many special cases, demonstrating the\nversatility of our framework.",
    "descriptor": "",
    "authors": [
      "Alekh Agarwal",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07659"
  },
  {
    "id": "arXiv:2206.07662",
    "title": "SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
    "abstract": "Recently, transformers have shown great potential in image classification and\nestablished state-of-the-art results on the ImageNet benchmark. However,\ncompared to CNNs, transformers converge slowly and are prone to overfitting in\nlow-data regimes due to the lack of spatial inductive biases. Such spatial\ninductive biases can be especially beneficial since the 2D structure of an\ninput image is not well preserved in transformers. In this work, we present\nSpatial Prior-enhanced Self-Attention (SP-SA), a novel variant of vanilla\nSelf-Attention (SA) tailored for vision transformers. Spatial Priors (SPs) are\nour proposed family of inductive biases that highlight certain groups of\nspatial relations. Unlike convolutional inductive biases, which are forced to\nfocus exclusively on hard-coded local regions, our proposed SPs are learned by\nthe model itself and take a variety of spatial relations into account.\nSpecifically, the attention score is calculated with emphasis on certain kinds\nof spatial relations at each head, and such learned spatial foci can be\ncomplementary to each other. Based on SP-SA we propose the SP-ViT family, which\nconsistently outperforms other ViT models with similar GFlops or parameters.\nOur largest model SP-ViT-L achieves a record-breaking 86.3% Top-1 accuracy with\na reduction in the number of parameters by almost 50% compared to previous\nstate-of-the-art model (150M for SP-ViT-L vs 271M for CaiT-M-36) among all\nImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o\nextra data.",
    "descriptor": "",
    "authors": [
      "Yuxuan Zhou",
      "Wangmeng Xiang",
      "Chao Li",
      "Biao Wang",
      "Xihan Wei",
      "Lei Zhang",
      "Margret Keuper",
      "Xiansheng Hua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07662"
  },
  {
    "id": "arXiv:2206.07665",
    "title": "Region-enhanced Deep Graph Convolutional Networks for Rumor Detection",
    "abstract": "Social media has been rapidly developing in the public sphere due to its ease\nof spreading new information, which leads to the circulation of rumors.\nHowever, detecting rumors from such a massive amount of information is becoming\nan increasingly arduous challenge. Previous work generally obtained valuable\nfeatures from propagation information. It should be noted that most methods\nonly target the propagation structure while ignoring the rumor transmission\npattern. This limited focus severely restricts the collection of spread data.\nTo solve this problem, the authors of the present study are motivated to\nexplore the regionalized propagation patterns of rumors. Specifically, a novel\nregion-enhanced deep graph convolutional network (RDGCN) that enhances the\npropagation features of rumors by learning regionalized propagation patterns\nand trains to learn the propagation patterns by unsupervised learning is\nproposed. In addition, a source-enhanced residual graph convolution layer\n(SRGCL) is designed to improve the graph neural network (GNN) oversmoothness\nand increase the depth limit of the rumor detection methods-based GNN.\nExperiments on Twitter15 and Twitter16 show that the proposed model performs\nbetter than the baseline approach on rumor detection and early rumor detection.",
    "descriptor": "\nComments: submitted to Neural Computing and Applications\n",
    "authors": [
      "Ge Wang",
      "Li Tan",
      "Tianbao Song",
      "Wei Wang",
      "Ziliang Shang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07665"
  },
  {
    "id": "arXiv:2206.07666",
    "title": "Transformer-based Automatic Speech Recognition of Formal and Colloquial  Czech in MALACH Project",
    "abstract": "Czech is a very specific language due to its large differences between the\nformal and the colloquial form of speech. While the formal (written) form is\nused mainly in official documents, literature, and public speeches, the\ncolloquial (spoken) form is used widely among people in casual speeches. This\ngap introduces serious problems for ASR systems, especially when training or\nevaluating ASR models on datasets containing a lot of colloquial speech, such\nas the MALACH project. In this paper, we are addressing this problem in the\nlight of a new paradigm in end-to-end ASR systems -- recently introduced\nself-supervised audio Transformers. Specifically, we are investigating the\ninfluence of colloquial speech on the performance of Wav2Vec 2.0 models and\ntheir ability to transcribe colloquial speech directly into formal transcripts.\nWe are presenting results with both formal and colloquial forms in the training\ntranscripts, language models, and evaluation transcripts.",
    "descriptor": "\nComments: to be published in Proceedings of TSD 2022\n",
    "authors": [
      "Jan Lehe\u010dka",
      "Josef V. Psutka",
      "Josef Psutka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07666"
  },
  {
    "id": "arXiv:2206.07667",
    "title": "A fitted second--order difference scheme on a modified Shishkin mesh for  a semilinear singularly-perturbed boundary-value problem",
    "abstract": "In the present paper we consider the numerical solving of a semilinear\nsingular--perturbation reaction--diffusion boundary--value problem having\nboundary layers. A new difference scheme is constructed, the second order of\nconvergence on a modified Shishkin mesh is shown. The numerical experiments are\nincluded in the paper, which confirm the theoretical results.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2009.06523\n",
    "authors": [
      "Samir Karasulji\u0107",
      "Irma Zenunovi\u0107"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07667"
  },
  {
    "id": "arXiv:2206.07669",
    "title": "A Unified Sequence Interface for Vision Tasks",
    "abstract": "While language tasks are naturally expressed in a single, unified, modeling\nframework, i.e., generating sequences of tokens, this has not been the case in\ncomputer vision. As a result, there is a proliferation of distinct\narchitectures and loss functions for different vision tasks. In this work we\nshow that a diverse set of \"core\" computer vision tasks can also be unified if\nformulated in terms of a shared pixel-to-sequence interface. We focus on four\ntasks, namely, object detection, instance segmentation, keypoint detection, and\nimage captioning, all with diverse types of outputs, e.g., bounding boxes or\ndense masks. Despite that, by formulating the output of each task as a sequence\nof discrete tokens with a unified interface, we show that one can train a\nneural network with a single model architecture and loss function on all these\ntasks, with no task-specific customization. To solve a specific task, we use a\nshort prompt as task description, and the sequence output adapts to the prompt\nso it can produce task-specific output. We show that such a model can achieve\ncompetitive performance compared to well-established task-specific models.",
    "descriptor": "\nComments: The first three authors contributed equally\n",
    "authors": [
      "Ting Chen",
      "Saurabh Saxena",
      "Lala Li",
      "Tsung-Yi Lin",
      "David J. Fleet",
      "Geoffrey Hinton"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07669"
  },
  {
    "id": "arXiv:2206.07672",
    "title": "Reconstructing Ultrametric Trees from Noisy Experiments",
    "abstract": "The problem of reconstructing evolutionary trees or phylogenies is of great\ninterest in computational biology. A popular model for this problem assumes\nthat we are given the set of leaves (current species) of an unknown binary tree\nand the results of `experiments' on triples of leaves (a,b,c), which return the\npair with the deepest least common ancestor. If the tree is assumed to be an\nultrametric (i.e., all root-leaf paths have the same length), the experiment\ncan be equivalently seen to return the closest pair of leaves. In this model,\nefficient algorithms are known for tree reconstruction.\nIn reality, since the data on which these `experiments' are run is itself\ngenerated by the stochastic process of evolution, these experiments are noisy.\nIn all reasonable models of evolution, if the branches leading to the leaves in\na triple separate from each other at common ancestors that are very close to\neach other in the tree, the result of the experiment should be close to\nuniformly random. Motivated by this, we consider a model where the noise on any\ntriple is just dependent on the three pairwise distances (referred to as\ndistance based noise).\nOur results are the following: 1. Suppose the length of every edge in the\nunknown tree is at least $\\tilde{O}(\\frac{1}{\\sqrt n})$ fraction of the length\nof a root-leaf path. Then, we give an efficient algorithm to reconstruct the\ntopology of the tree for a broad family of distance-based noise models.\nFurther, we show that if the edges are asymptotically shorter, then topology\nreconstruction is information-theoretically impossible.\n2. Further, for a specific distance-based noise model--which we refer to as\nthe homogeneous noise model--we show that the edge weights can also be\napproximately reconstructed under the same quantitative lower bound on the edge\nlengths.",
    "descriptor": "",
    "authors": [
      "Eshwar Ram Arunachaleswaran",
      "Anindya De",
      "Sampath Kannan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07672"
  },
  {
    "id": "arXiv:2206.07674",
    "title": "Summarizing Labeled Multi-Graphs",
    "abstract": "Real-world graphs can be difficult to interpret and visualize beyond a\ncertain size. To address this issue, graph summarization aims to simplify and\nshrink a graph, while maintaining its high-level structure and characteristics.\nMost summarization methods are designed for homogeneous, undirected, simple\ngraphs; however, many real-world graphs are ornate; with characteristics\nincluding node labels, directed edges, edge multiplicities, and self-loops. In\nthis paper we propose LM-Gsum, a versatile yet rigorous graph summarization\nmodel that (to the best of our knowledge, for the first time) can handle graphs\nwith all the aforementioned characteristics (and any combination thereof).\nMoreover, our proposed model captures basic sub-structures that are prevalent\nin real-world graphs, such as cliques, stars, etc. LM-Gsum compactly quantifies\nthe information content of a complex graph using a novel encoding scheme, where\nit seeks to minimize the total number of bits required to encode (i) the\nsummary graph, as well as (ii) the corrections required for reconstructing the\ninput graph losslessly. To accelerate the summary construction, it creates\nsuper-nodes efficiently by merging nodes in groups. Experiments demonstrate\nthat LM-Gsum facilitates the visualization of real-world complex graphs,\nrevealing interpretable structures and high- level relationships. Furthermore,\nLM-Gsum achieves better trade-off between compression rate and running time,\nrelative to existing methods (only) on comparable settings.",
    "descriptor": "\nComments: 17 pages, 8 figures, 4 tables\n",
    "authors": [
      "Dimitris Berberidis",
      "Pierre J. Liang",
      "Leman Akoglu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.07674"
  },
  {
    "id": "arXiv:2206.07680",
    "title": "Learning Large-scale Subsurface Simulations with a Hybrid Graph Network  Simulator",
    "abstract": "Subsurface simulations use computational models to predict the flow of fluids\n(e.g., oil, water, gas) through porous media. These simulations are pivotal in\nindustrial applications such as petroleum production, where fast and accurate\nmodels are needed for high-stake decision making, for example, for well\nplacement optimization and field development planning. Classical finite\ndifference numerical simulators require massive computational resources to\nmodel large-scale real-world reservoirs. Alternatively, streamline simulators\nand data-driven surrogate models are computationally more efficient by relying\non approximate physics models, however they are insufficient to model complex\nreservoir dynamics at scale. Here we introduce Hybrid Graph Network Simulator\n(HGNS), which is a data-driven surrogate model for learning reservoir\nsimulations of 3D subsurface fluid flows. To model complex reservoir dynamics\nat both local and global scale, HGNS consists of a subsurface graph neural\nnetwork (SGNN) to model the evolution of fluid flows, and a 3D-U-Net to model\nthe evolution of pressure. HGNS is able to scale to grids with millions of\ncells per time step, two orders of magnitude higher than previous surrogate\nmodels, and can accurately predict the fluid flow for tens of time steps (years\ninto the future). Using an industry-standard subsurface flow dataset (SPE-10)\nwith 1.1 million cells, we demonstrate that HGNS is able to reduce the\ninference time up to 18 times compared to standard subsurface simulators, and\nthat it outperforms other learning-based models by reducing long-term\nprediction errors by up to 21%.",
    "descriptor": "\nComments: SIGKDD 2022; 11 pages, 6 figures\n",
    "authors": [
      "Tailin Wu",
      "Qinchen Wang",
      "Yinan Zhang",
      "Rex Ying",
      "Kaidi Cao",
      "Rok Sosi\u010d",
      "Ridwan Jalali",
      "Hassan Hamam",
      "Marko Maucec",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07680"
  },
  {
    "id": "arXiv:2206.07681",
    "title": "Learning to Accelerate Partial Differential Equations via Latent Global  Evolution",
    "abstract": "Simulating the time evolution of Partial Differential Equations (PDEs) of\nlarge-scale systems is crucial in many scientific and engineering domains such\nas fluid dynamics, weather forecasting and their inverse optimization problems.\nHowever, both classical solvers and recent deep learning-based surrogate models\nare typically extremely computationally intensive, because of their local\nevolution: they need to update the state of each discretized cell at each time\nstep during inference. Here we develop Latent Evolution of PDEs (LE-PDE), a\nsimple, fast and scalable method to accelerate the simulation and inverse\noptimization of PDEs. LE-PDE learns a compact, global representation of the\nsystem and efficiently evolves it fully in the latent space with learned latent\nevolution models. LE-PDE achieves speed-up by having a much smaller latent\ndimension to update during long rollout as compared to updating in the input\nspace. We introduce new learning objectives to effectively learn such latent\ndynamics to ensure long-term stability. We further introduce techniques for\nspeeding-up inverse optimization of boundary conditions for PDEs via\nbackpropagation through time in latent space, and an annealing technique to\naddress the non-differentiability and sparse interaction of boundary\nconditions. We test our method in a 1D benchmark of nonlinear PDEs, 2D\nNavier-Stokes flows into turbulent phase and an inverse optimization of\nboundary conditions in 2D Navier-Stokes flow. Compared to state-of-the-art deep\nlearning-based surrogate models and other strong baselines, we demonstrate up\nto 128x reduction in the dimensions to update, and up to 15x improvement in\nspeed, while achieving competitive accuracy.",
    "descriptor": "\nComments: 25 pages, 13 figures\n",
    "authors": [
      "Tailin Wu",
      "Takashi Maruyama",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07681"
  },
  {
    "id": "arXiv:2206.07682",
    "title": "Emergent Abilities of Large Language Models",
    "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.",
    "descriptor": "",
    "authors": [
      "Jason Wei",
      "Yi Tay",
      "Rishi Bommasani",
      "Colin Raffel",
      "Barret Zoph",
      "Sebastian Borgeaud",
      "Dani Yogatama",
      "Maarten Bosma",
      "Denny Zhou",
      "Donald Metzler",
      "Ed H. Chi",
      "Tatsunori Hashimoto",
      "Oriol Vinyals",
      "Percy Liang",
      "Jeff Dean",
      "William Fedus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07682"
  },
  {
    "id": "arXiv:2206.07684",
    "title": "AVATAR: Unconstrained Audiovisual Speech Recognition",
    "abstract": "Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR\nthat incorporates visual cues, often from the movements of a speaker's mouth.\nUnlike works that simply focus on the lip motion, we investigate the\ncontribution of entire visual frames (visual actions, objects, background\netc.). This is particularly useful for unconstrained videos, where the speaker\nis not necessarily visible. To solve this task, we propose a new\nsequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained\nend-to-end from spectrograms and full-frame RGB. To prevent the audio stream\nfrom dominating training, we propose different word-masking strategies, thereby\nencouraging our model to pay attention to the visual stream. We demonstrate the\ncontribution of the visual modality on the How2 AV-ASR benchmark, especially in\nthe presence of simulated noise, and show that our model outperforms all other\nprior work by a large margin. Finally, we also create a new, real-world test\nbed for AV-ASR called VisSpeech, which demonstrates the contribution of the\nvisual modality under challenging audio conditions.",
    "descriptor": "",
    "authors": [
      "Valentin Gabeur",
      "Paul Hongsuck Seo",
      "Arsha Nagrani",
      "Chen Sun",
      "Karteek Alahari",
      "Cordelia Schmid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07684"
  },
  {
    "id": "arXiv:2206.07685",
    "title": "Decentralized WebRCT P2P network using Kademlia",
    "abstract": "Web Real-Time Communication (WebRTC) is a new standard and industry effort\nthat extends the web browsing model. For the first time, browsers are able to\ndirectly exchange real-time media with other browsers in a peer-to-peer\nfashion. Before WebRTC was introduced, it was cumbersome to build smooth chat\nand video applications, users often experience unstable connections, blurry\nvideos, and unclear sounds. WebRTC's peer-to-peer communication paradigm\nestablishes the real-time connection between browsers using the SIP(Session\nInitiation Protocol) Trapezoid. A wide set of protocols are bundled in WebRTC\nAPI, such as connection management, encoding/decoding negotiation, media\ncontrol, selection and control, firewall and NAT element traversal, etc.\nHowever, almost all current WebRTC applications are using centralized signaling\ninfrastructure which brings the problems of scalability, stability, and\nfault-tolerance. In this paper, I am presenting a decentralized architecture by\nintroducing the Kademlia network into WebRTC to reduce the need for a\ncentralized signaling service for WebRTC.",
    "descriptor": "",
    "authors": [
      "Ryle Zhou"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.07685"
  },
  {
    "id": "arXiv:2206.07687",
    "title": "Residual Sparsity Connection Learning for Efficient Video  Super-Resolution",
    "abstract": "Lighter and faster models are crucial for the deployment of video\nsuper-resolution (VSR) on resource-limited devices, e.g., smartphones and\nwearable devices. In this paper, we develop Residual Sparsity Connection\nLearning (RSCL), a structured pruning scheme, to reduce the redundancy of\nconvolution kernels and obtain a compact VSR network with a minor performance\ndrop. However, residual blocks require the pruned filter indices of skip and\nresidual connections to be the same, which is tricky for pruning. Thus, to\nmitigate the pruning restrictions of residual blocks, we design a Residual\nSparsity Connection (RSC) scheme by preserving the feature channels and only\noperating on the important channels. Moreover, for the pixel-shuffle operation,\nwe design a special pruning scheme by grouping several filters as pruning units\nto guarantee the accuracy of feature channel-space conversion after pruning. In\naddition, we introduce Temporal Finetuning (TF) to reduce the pruning error\namplification of hidden states with temporal propagation. Extensive experiments\nshow that the proposed RSCL significantly outperforms recent methods\nquantitatively and qualitatively. Codes and models will be released.",
    "descriptor": "",
    "authors": [
      "Bin Xia",
      "Jingwen He",
      "Yulun Zhang",
      "Yucheng Hang",
      "Wenming Yang",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07687"
  },
  {
    "id": "arXiv:2206.07689",
    "title": "Structured Video Tokens @ Ego4D PNR Temporal Localization Challenge 2022",
    "abstract": "This technical report describes the SViT approach for the Ego4D Point of No\nReturn (PNR) Temporal Localization Challenge. We propose a learning framework\nStructureViT (SViT for short), which demonstrates how utilizing the structure\nof a small number of images only available during training can improve a video\nmodel. SViT relies on two key insights. First, as both images and videos\ncontain structured information, we enrich a transformer model with a set of\n\\emph{object tokens} that can be used across images and videos. Second, the\nscene representations of individual frames in video should \"align\" with those\nof still images. This is achieved via a \"Frame-Clip Consistency\" loss, which\nensures the flow of structured information between images and videos. SViT\nobtains strong performance on the challenge test set with 0.656 absolute\ntemporal localization error.",
    "descriptor": "\nComments: Ego4D CVPR22 Object State Localization challenge. arXiv admin note: substantial text overlap with arXiv:2206.06346\n",
    "authors": [
      "Elad Ben-Avraham",
      "Roei Herzig",
      "Karttikeya Mangalam",
      "Amir Bar",
      "Anna Rohrbach",
      "Leonid Karlinsky",
      "Trevor Darrell",
      "Amir Globerson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07689"
  },
  {
    "id": "arXiv:2206.07690",
    "title": "ELUDE: Generating interpretable explanations via a decomposition into  labelled and unlabelled features",
    "abstract": "Deep learning models have achieved remarkable success in different areas of\nmachine learning over the past decade; however, the size and complexity of\nthese models make them difficult to understand. In an effort to make them more\ninterpretable, several recent works focus on explaining parts of a deep neural\nnetwork through human-interpretable, semantic attributes. However, it may be\nimpossible to completely explain complex models using only semantic attributes.\nIn this work, we propose to augment these attributes with a small set of\nuninterpretable features. Specifically, we develop a novel explanation\nframework ELUDE (Explanation via Labelled and Unlabelled DEcomposition) that\ndecomposes a model's prediction into two parts: one that is explainable through\na linear combination of the semantic attributes, and another that is dependent\non the set of uninterpretable features. By identifying the latter, we are able\nto analyze the \"unexplained\" portion of the model, obtaining insights into the\ninformation used by the model. We show that the set of unlabelled features can\ngeneralize to multiple models trained with the same feature space and compare\nour work to two popular attribute-oriented methods, Interpretable Basis\nDecomposition and Concept Bottleneck, and discuss the additional insights ELUDE\nprovides.",
    "descriptor": "",
    "authors": [
      "Vikram V. Ramaswamy",
      "Sunnie S. Y. Kim",
      "Nicole Meister",
      "Ruth Fong",
      "Olga Russakovsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07690"
  },
  {
    "id": "arXiv:2206.07692",
    "title": "A Simple Data Mixing Prior for Improving Self-Supervised Learning",
    "abstract": "Data mixing (e.g., Mixup, Cutmix, ResizeMix) is an essential component for\nadvancing recognition models. In this paper, we focus on studying its\neffectiveness in the self-supervised setting. By noticing the mixed images that\nshare the same source images are intrinsically related to each other, we hereby\npropose SDMP, short for $\\textbf{S}$imple $\\textbf{D}$ata $\\textbf{M}$ixing\n$\\textbf{P}$rior, to capture this straightforward yet essential prior, and\nposition such mixed images as additional $\\textbf{positive pairs}$ to\nfacilitate self-supervised representation learning. Our experiments verify that\nthe proposed SDMP enables data mixing to help a set of self-supervised learning\nframeworks (e.g., MoCo) achieve better accuracy and out-of-distribution\nrobustness. More notably, our SDMP is the first method that successfully\nleverages data mixing to improve (rather than hurt) the performance of Vision\nTransformers in the self-supervised setting. Code is publicly available at\nhttps://github.com/OliverRensu/SDMP",
    "descriptor": "\nComments: CVPR2022\n",
    "authors": [
      "Sucheng Ren",
      "Huiyu Wang",
      "Zhengqi Gao",
      "Shengfeng He",
      "Alan Yuille",
      "Yuyin Zhou",
      "Cihang Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07692"
  },
  {
    "id": "arXiv:2206.07694",
    "title": "DIRECTOR: Generator-Classifiers For Supervised Language Modeling",
    "abstract": "Current language models achieve low perplexity but their resulting\ngenerations still suffer from toxic responses, repetitiveness and\ncontradictions. The standard language modeling setup fails to address these\nissues. In this paper, we introduce a new architecture, {\\sc Director}, that\nconsists of a unified generator-classifier with both a language modeling and a\nclassification head for each output token. Training is conducted jointly using\nboth standard language modeling data, and data labeled with desirable and\nundesirable sequences. Experiments in several settings show that the model has\ncompetitive training and decoding speed compared to standard language models\nwhile yielding superior results, alleviating known issues while maintaining\ngeneration quality. It also outperforms existing model guiding approaches in\nterms of both accuracy and efficiency.",
    "descriptor": "",
    "authors": [
      "Kushal Arora",
      "Kurt Shuster",
      "Sainbayar Sukhbaatar",
      "Jason Weston"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07694"
  },
  {
    "id": "arXiv:2206.07695",
    "title": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
    "abstract": "State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to\nparameterize 3D radiance fields. While demonstrating impressive results,\nquerying an MLP for every sample along each ray leads to slow rendering.\nTherefore, existing approaches often render low-resolution feature maps and\nprocess them with an upsampling network to obtain the final image. Albeit\nefficient, neural rendering often entangles viewpoint and content such that\nchanging the camera pose results in unwanted changes of geometry or appearance.\nMotivated by recent results in voxel-based novel view synthesis, we investigate\nthe utility of sparse voxel grid representations for fast and 3D-consistent\ngenerative modeling in this paper. Our results demonstrate that monolithic MLPs\ncan indeed be replaced by 3D convolutions when combining sparse voxel grids\nwith progressive growing, free space pruning and appropriate regularization. To\nobtain a compact representation of the scene and allow for scaling to higher\nvoxel resolutions, our model disentangles the foreground object (modeled in 3D)\nfrom the background (modeled in 2D). In contrast to existing approaches, our\nmethod requires only a single forward pass to generate a full 3D scene. It\nhence allows for efficient rendering from arbitrary viewpoints while yielding\n3D consistent results with high visual fidelity.",
    "descriptor": "",
    "authors": [
      "Katja Schwarz",
      "Axel Sauer",
      "Michael Niemeyer",
      "Yiyi Liao",
      "Andreas Geiger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07695"
  },
  {
    "id": "arXiv:2206.07696",
    "title": "Diffusion Models for Video Prediction and Infilling",
    "abstract": "To predict and anticipate future outcomes or reason about missing information\nin a sequence is a key ability for agents to be able to make intelligent\ndecisions. This requires strong temporally coherent generative capabilities.\nDiffusion models have shown huge success in several generative tasks lately,\nbut have not been extensively explored in the video domain. We present\nRandom-Mask Video Diffusion (RaMViD), which extends image diffusion models to\nvideos using 3D convolutions, and introduces a new conditioning technique\nduring training. By varying the mask we condition on, the model is able to\nperform video prediction, infilling and upsampling. Since we do not use\nconcatenation to condition on a mask, as done in most conditionally trained\ndiffusion models, we are able to decrease the memory footprint. We evaluated\nthe model on two benchmark datasets for video prediction and one for video\ngeneration on which we achieved competitive results. On Kinetics-600 we\nachieved state-of-the-art for video prediction.",
    "descriptor": "",
    "authors": [
      "Tobias H\u00f6ppe",
      "Arash Mehrjou",
      "Stefan Bauer",
      "Didrik Nielsen",
      "Andrea Dittadi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07696"
  },
  {
    "id": "arXiv:2206.07698",
    "title": "Neural Deformable Voxel Grid for Fast Optimization of Dynamic View  Synthesis",
    "abstract": "Recently, Neural Radiance Fields (NeRF) is revolutionizing the task of novel\nview synthesis (NVS) for its superior performance. However, NeRF and its\nvariants generally require a lengthy per-scene training procedure, where a\nmulti-layer perceptron (MLP) is fitted to the captured images. To remedy the\nchallenge, the voxel-grid representation has been proposed to significantly\nspeed up the training. However, these existing methods can only deal with\nstatic scenes. How to develop an efficient and accurate dynamic view synthesis\nmethod remains an open problem. Extending the methods for static scenes to\ndynamic scenes is not straightforward as both the scene geometry and appearance\nchange over time. In this paper, built on top of the recent advances in\nvoxel-grid optimization, we propose a fast deformable radiance field method to\nhandle dynamic scenes. Our method consists of two modules. The first module\nadopts a deformation grid to store 3D dynamic features, and a light-weight MLP\nfor decoding the deformation that maps a 3D point in observation space to the\ncanonical space using the interpolated features. The second module contains a\ndensity and a color grid to model the geometry and density of the scene. The\nocclusion is explicitly modeled to further improve the rendering quality.\nExperimental results show that our method achieves comparable performance to\nD-NeRF using only 20 minutes for training, which is more than 70x faster than\nD-NeRF, clearly demonstrating the efficiency of our proposed method.",
    "descriptor": "\nComments: Technical Report: 29 pages; project page: this https URL\n",
    "authors": [
      "Xiang Guo",
      "Guanying Chen",
      "Yuchao Dai",
      "Xiaoqing Ye",
      "Jiadai Sun",
      "Xiao Tan",
      "Errui Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07698"
  },
  {
    "id": "arXiv:2206.07699",
    "title": "Prefix Language Models are Unified Modal Learners",
    "abstract": "With the success of vision-language pre-training, we have witnessed the\nstate-of-the-art has been pushed on multi-modal understanding and generation.\nHowever, the current pre-training paradigm is either incapable of targeting all\nmodalities at once (e.g., text generation and image generation), or requires\nmulti-fold well-designed tasks which significantly limits the scalability. We\ndemonstrate that a unified modal model could be learned with a prefix language\nmodeling objective upon text and image sequences. Thanks to the simple but\npowerful pre-training paradigm, our proposed model, DaVinci, is simple to\ntrain, scalable to huge data, and adaptable to a variety of downstream tasks\nacross modalities (language / vision / vision+language), types (understanding /\ngeneration) and settings (e.g., zero-shot, fine-tuning, linear evaluation) with\na single unified architecture. DaVinci achieves the competitive performance on\na wide range of 26 understanding / generation tasks, and outperforms previous\nunified vision-language models on most tasks, including ImageNet classification\n(+1.6%), VQAv2 (+1.4%), COCO caption generation (BLEU@4 +1.1%, CIDEr +1.5%) and\nCOCO image generation (IS +0.9%, FID -1.0%), at the comparable model and data\nscale. Furthermore, we offer a well-defined benchmark for future research by\nreporting the performance on different scales of the pre-training dataset on a\nheterogeneous and wide distribution coverage. Our results establish new,\nstronger baselines for future comparisons at different data scales and shed\nlight on the difficulties of comparing VLP models more generally.",
    "descriptor": "\nComments: 22 pages, 3 figures\n",
    "authors": [
      "Shizhe Diao",
      "Wangchunshu Zhou",
      "Xinsong Zhang",
      "Jiawei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07699"
  },
  {
    "id": "arXiv:2206.07700",
    "title": "Masked Siamese ConvNets",
    "abstract": "Self-supervised learning has shown superior performances over supervised\nmethods on various vision benchmarks. The siamese network, which encourages\nembeddings to be invariant to distortions, is one of the most successful\nself-supervised visual representation learning approaches. Among all the\naugmentation methods, masking is the most general and straightforward method\nthat has the potential to be applied to all kinds of input and requires the\nleast amount of domain knowledge. However, masked siamese networks require\nparticular inductive bias and practically only work well with Vision\nTransformers. This work empirically studies the problems behind masked siamese\nnetworks with ConvNets. We propose several empirical designs to overcome these\nproblems gradually. Our method performs competitively on low-shot image\nclassification and outperforms previous methods on object detection benchmarks.\nWe discuss several remaining issues and hope this work can provide useful data\npoints for future general-purpose self-supervised learning.",
    "descriptor": "",
    "authors": [
      "Li Jing",
      "Jiachen Zhu",
      "Yann LeCun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07700"
  },
  {
    "id": "arXiv:2206.07704",
    "title": "Waymo Open Dataset: Panoramic Video Panoptic Segmentation",
    "abstract": "Panoptic image segmentation is the computer vision task of finding groups of\npixels in an image and assigning semantic classes and object instance\nidentifiers to them. Research in image segmentation has become increasingly\npopular due to its critical applications in robotics and autonomous driving.\nThe research community thereby relies on publicly available benchmark dataset\nto advance the state-of-the-art in computer vision. Due to the high costs of\ndensely labeling the images, however, there is a shortage of publicly available\nground truth labels that are suitable for panoptic segmentation. The high\nlabeling costs also make it challenging to extend existing datasets to the\nvideo domain and to multi-camera setups. We therefore present the Waymo Open\nDataset: Panoramic Video Panoptic Segmentation Dataset, a large-scale dataset\nthat offers high-quality panoptic segmentation labels for autonomous driving.\nWe generate our dataset using the publicly available Waymo Open Dataset,\nleveraging the diverse set of camera images. Our labels are consistent over\ntime for video processing and consistent across multiple cameras mounted on the\nvehicles for full panoramic scene understanding. Specifically, we offer labels\nfor 28 semantic categories and 2,860 temporal sequences that were captured by\nfive cameras mounted on autonomous vehicles driving in three different\ngeographical locations, leading to a total of 100k labeled camera images. To\nthe best of our knowledge, this makes our dataset an order of magnitude larger\nthan existing datasets that offer video panoptic segmentation labels. We\nfurther propose a new benchmark for Panoramic Video Panoptic Segmentation and\nestablish a number of strong baselines based on the DeepLab family of models.\nWe will make the benchmark and the code publicly available. Find the dataset at\nhttps://waymo.com/open.",
    "descriptor": "\nComments: Our dataset can be found at this https URL\n",
    "authors": [
      "Jieru Mei",
      "Alex Zihao Zhu",
      "Xinchen Yan",
      "Hang Yan",
      "Siyuan Qiao",
      "Yukun Zhu",
      "Liang-Chieh Chen",
      "Henrik Kretzschmar",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07704"
  },
  {
    "id": "arXiv:2206.07705",
    "title": "LET-3D-AP: Longitudinal Error Tolerant 3D Average Precision for  Camera-Only 3D Detection",
    "abstract": "The popular object detection metric 3D Average Precision (3D AP) relies on\nthe intersection over union between predicted bounding boxes and ground truth\nbounding boxes. However, depth estimation based on cameras has limited\naccuracy, which may cause otherwise reasonable predictions that suffer from\nsuch longitudinal localization errors to be treated as false positives and\nfalse negatives. We therefore propose variants of the popular 3D AP metric that\nare designed to be more permissive with respect to depth estimation errors.\nSpecifically, our novel longitudinal error tolerant metrics, LET-3D-AP and\nLET-3D-APL, allow longitudinal localization errors of the predicted bounding\nboxes up to a given tolerance. The proposed metrics have been used in the Waymo\nOpen Dataset 3D Camera-Only Detection Challenge. We believe that they will\nfacilitate advances in the field of camera-only 3D detection by providing more\ninformative performance signals.",
    "descriptor": "\nComments: Find the primary metrics for the 2022 Waymo Open Dataset 3D Camera-Only Detection Challenge at this https URL . Find the code at this https URL\n",
    "authors": [
      "Wei-Chih Hung",
      "Henrik Kretzschmar",
      "Vincent Casser",
      "Jyh-Jing Hwang",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07705"
  },
  {
    "id": "arXiv:2206.07706",
    "title": "Masked Frequency Modeling for Self-Supervised Visual Pre-Training",
    "abstract": "We present Masked Frequency Modeling (MFM), a unified frequency-domain-based\napproach for self-supervised pre-training of visual models. Instead of randomly\ninserting mask tokens to the input embeddings in the spatial domain, in this\npaper, we shift the perspective to the frequency domain. Specifically, MFM\nfirst masks out a portion of frequency components of the input image and then\npredicts the missing frequencies on the frequency spectrum. Our key insight is\nthat predicting masked components in the frequency domain is more ideal to\nreveal underlying image patterns rather than predicting masked patches in the\nspatial domain, due to the heavy spatial redundancy. Our findings suggest that\nwith the right configuration of mask-and-predict strategy, both the structural\ninformation within high-frequency components and the low-level statistics among\nlow-frequency counterparts are useful in learning good representations. For the\nfirst time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese\nframework can learn meaningful representations even using none of the\nfollowing: (i) extra data, (ii) extra model, (iii) mask token. Experimental\nresults on ImageNet and several robustness benchmarks show the competitive\nperformance and advanced robustness of MFM compared with recent masked image\nmodeling approaches. Furthermore, we also comprehensively investigate the\neffectiveness of classical image restoration tasks for representation learning\nfrom a unified frequency perspective and reveal their intriguing relations with\nour MFM approach. Project page:\nhttps://www.mmlab-ntu.com/project/mfm/index.html.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Jiahao Xie",
      "Wei Li",
      "Xiaohang Zhan",
      "Ziwei Liu",
      "Yew Soon Ong",
      "Chen Change Loy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07706"
  },
  {
    "id": "arXiv:2206.07707",
    "title": "Variable Bitrate Neural Fields",
    "abstract": "Neural approximations of scalar and vector fields, such as signed distance\nfunctions and radiance fields, have emerged as accurate, high-quality\nrepresentations. State-of-the-art results are obtained by conditioning a neural\napproximation with a lookup from trainable feature grids that take on part of\nthe learning task and allow for smaller, more efficient neural networks.\nUnfortunately, these feature grids usually come at the cost of significantly\nincreased memory consumption compared to stand-alone neural network models. We\npresent a dictionary method for compressing such feature grids, reducing their\nmemory consumption by up to 100x and permitting a multiresolution\nrepresentation which can be useful for out-of-core streaming. We formulate the\ndictionary optimization as a vector-quantized auto-decoder problem which lets\nus learn end-to-end discrete neural representations in a space where no direct\nsupervision is available and with dynamic topology and structure. Our source\ncode will be available at https://github.com/nv-tlabs/vqad.",
    "descriptor": "\nComments: SIGGRAPH 2022. Project Page: this https URL\n",
    "authors": [
      "Towaki Takikawa",
      "Alex Evans",
      "Jonathan Tremblay",
      "Thomas M\u00fcller",
      "Morgan McGuire",
      "Alec Jacobson",
      "Sanja Fidler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.07707"
  },
  {
    "id": "arXiv:2206.07708",
    "title": "Finding Control Synthesis for Kinematic Shortest Paths",
    "abstract": "This work presents the analysis of the properties of the shortest path\ncontrol synthesis for the rigid body system. The systems we focus on in this\nwork have only kinematic constraints. However, even for seemingly simple\nsystems and constraints, the shortest paths for generic rigid body systems were\nonly found recently, especially for 3D systems. Based on the Pontraygon's\nMaximum Principle (MPM) and Lagrange equations, we present the necessary\nconditions for optimal switches, which form the control synthesis boundaries.\nWe formally show that the shortest path for nearby configurations will have\nsimilar adjoint functions and parameters, i.e., Lagrange multipliers. We\nfurther show that the gradients of the necessary condition equation can be used\nto verify whether a configuration is inside a control synthesis region or on\nthe boundary. We present a procedure to find the shortest kinematic paths and\ncontrol synthesis, using the gradients of the control constraints. Given the\nshortest path and the corresponding control sequences, the optimal control\nsequence for nearby configurations can be derived if and only if they belong to\nthe same control synthesis region. The proposed procedure can work for both 2D\nand 3D rigid body systems. We use a 2D Dubins vehicle system to verify the\ncorrectness of the proposed approach. More verifications and experiments will\nbe presented in the extensions of this work.",
    "descriptor": "",
    "authors": [
      "Weifu Wang",
      "Ping Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07708"
  },
  {
    "id": "arXiv:2206.07710",
    "title": "PlanarRecon: Real-time 3D Plane Detection and Reconstruction from Posed  Monocular Videos",
    "abstract": "We present PlanarRecon -- a novel framework for globally coherent detection\nand reconstruction of 3D planes from a posed monocular video. Unlike previous\nworks that detect planes in 2D from a single image, PlanarRecon incrementally\ndetects planes in 3D for each video fragment, which consists of a set of key\nframes, from a volumetric representation of the scene using neural networks. A\nlearning-based tracking and fusion module is designed to merge planes from\nprevious fragments to form a coherent global plane reconstruction. Such design\nallows PlanarRecon to integrate observations from multiple views within each\nfragment and temporal information across different ones, resulting in an\naccurate and coherent reconstruction of the scene abstraction with\nlow-polygonal geometry. Experiments show that the proposed approach achieves\nstate-of-the-art performances on the ScanNet dataset while being real-time.",
    "descriptor": "\nComments: CVPR 2022. Project page: this https URL\n",
    "authors": [
      "Yiming Xie",
      "Matheus Gadelha",
      "Fengting Yang",
      "Xiaowei Zhou",
      "Huaizu Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07710"
  },
  {
    "id": "arXiv:2206.07711",
    "title": "On the Eve of True Explainability for OWL Ontologies: Description Logic  Proofs with Evee and Evonne (Extended Version)",
    "abstract": "When working with description logic ontologies, understanding entailments\nderived by a description logic reasoner is not always straightforward. So far,\nthe standard ontology editor Prot\\'eg\\'e offers two services to help:\n(black-box) justifications for OWL 2 DL ontologies, and (glass-box) proofs for\nlightweight OWL EL ontologies, where the latter exploits the proof facilities\nof reasoner ELK. Since justifications are often insufficient in explaining\ninferences, there is thus only little tool support for explaining inferences in\nmore expressive DLs. In this paper, we introduce EVEE-LIBS, a Java library for\ncomputing proofs for DLs up to ALCH, and EVEE-PROTEGE, a collection of\nProt\\'eg\\'e plugins for displaying those proofs in Prot\\'eg\\'e. We also give a\nshort glimpse of the latest version of EVONNE, a more advanced standalone\napplication for displaying and interacting with proofs computed with EVEE-LIBS.",
    "descriptor": "",
    "authors": [
      "Christian Alrabbaa",
      "Stefan Borgwardt",
      "Tom Friese",
      "Patrick Koopmann",
      "Juli\u00e1n M\u00e9ndez",
      "Alexej Popovi\u010d"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07711"
  },
  {
    "id": "arXiv:1909.03601",
    "title": "Unbiased Recommender Learning from Missing-Not-At-Random Implicit  Feedback",
    "abstract": "Recommender systems widely use implicit feedback such as click data because\nof its general availability. Although the presence of clicks signals the users'\npreference to some extent, the lack of such clicks does not necessarily\nindicate a negative response from the users, as it is possible that the users\nwere not exposed to the items (positive-unlabeled problem). This leads to a\ndifficulty in predicting the users' preferences from implicit feedback.\nPrevious studies addressed the positive-unlabeled problem by uniformly\nupweighting the loss for the positive feedback data or estimating the\nconfidence of each data having relevance information via the EM-algorithm.\nHowever, these methods failed to address the missing-not-at-random problem in\nwhich popular or frequently recommended items are more likely to be clicked\nthan other items even if a user does not have a considerable interest in them.\nTo overcome these limitations, we first define an ideal loss function to be\noptimized to realize recommendations that maximize the relevance and propose an\nunbiased estimator for the ideal loss. Subsequently, we analyze the variance of\nthe proposed unbiased estimator and further propose a clipped estimator that\nincludes the unbiased estimator as a special case. We demonstrate that the\nclipped estimator is expected to improve the performance of the recommender\nsystem, by considering the bias-variance trade-off. We conduct semi-synthetic\nand real-world experiments and demonstrate that the proposed method largely\noutperforms the baselines. In particular, the proposed method works better for\nrare items that are less frequently observed in the training data. The findings\nindicate that the proposed method can better achieve the objective of\nrecommending items with the highest relevance.",
    "descriptor": "\nComments: accepted at WSDM'20\n",
    "authors": [
      "Yuta Saito",
      "Suguru Yaginuma",
      "Yuta Nishino",
      "Hayato Sakata",
      "Kazuhide Nakata"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1909.03601"
  },
  {
    "id": "arXiv:2206.07048",
    "title": "A smile is all you need: Predicting limiting activity coefficients from  SMILES with natural language processing",
    "abstract": "Knowledge of mixtures' phase equilibria is crucial in nature and technical\nchemistry. Phase equilibria calculations of mixtures require activity\ncoefficients. However, experimental data on activity coefficients is often\nlimited due to high cost of experiments. For an accurate and efficient\nprediction of activity coefficients, machine learning approaches have been\nrecently developed. However, current machine learning approaches still\nextrapolate poorly for activity coefficients of unknown molecules. In this\nwork, we introduce the SMILES-to-Properties-Transformer (SPT), a natural\nlanguage processing network to predict binary limiting activity coefficients\nfrom SMILES codes. To overcome the limitations of available experimental data,\nwe initially train our network on a large dataset of synthetic data sampled\nfrom COSMO-RS (10 Million data points) and then fine-tune the model on\nexperimental data (20 870 data points). This training strategy enables SPT to\naccurately predict limiting activity coefficients even for unknown molecules,\ncutting the mean prediction error in half compared to state-of-the-art models\nfor activity coefficient predictions such as COSMO-RS, UNIFAC, and improving on\nrecent machine learning approaches.",
    "descriptor": "\nComments: Code available at: this https URL; Data available at: this https URL\n",
    "authors": [
      "Benedikt Winter",
      "Clemens Winter",
      "Johannes Schilling",
      "Andr\u00e9 Bardow"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2206.07048"
  },
  {
    "id": "arXiv:2206.07050",
    "title": "Near-Exact Recovery for Tomographic Inverse Problems via Deep Learning",
    "abstract": "This work is concerned with the following fundamental question in scientific\nmachine learning: Can deep-learning-based methods solve noise-free inverse\nproblems to near-perfect accuracy? Positive evidence is provided for the first\ntime, focusing on a prototypical computed tomography (CT) setup. We demonstrate\nthat an iterative end-to-end network scheme enables reconstructions close to\nnumerical precision, comparable to classical compressed sensing strategies. Our\nresults build on our winning submission to the recent AAPM DL-Sparse-View CT\nChallenge. Its goal was to identify the state-of-the-art in solving the\nsparse-view CT inverse problem with data-driven techniques. A specific\ndifficulty of the challenge setup was that the precise forward model remained\nunknown to the participants. Therefore, a key feature of our approach was to\ninitially estimate the unknown fanbeam geometry in a data-driven calibration\nstep. Apart from an in-depth analysis of our methodology, we also demonstrate\nits state-of-the-art performance on the open-access real-world dataset LoDoPaB\nCT.",
    "descriptor": "\nComments: ICML 2022 (long talk). Code available at this https URL arXiv admin note: text overlap with arXiv:2106.00280\n",
    "authors": [
      "Martin Genzel",
      "Ingo G\u00fchring",
      "Jan Macdonald",
      "Maximilian M\u00e4rz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07050"
  },
  {
    "id": "arXiv:2206.07051",
    "title": "A Novel RIS-Aided EMF-Aware Beamforming Using Directional Spreading,  Truncation and Boosting",
    "abstract": "This paper addresses a drawback of massive multiple-input multiple-output\nMaximum Ratio Transmission beamforming. In some propagation conditions, when\nthe base station serves the same target user equipment for a long period, it\nreduces the transmit power (and degrades the received power) to avoid creating\nhigh exposure regions located in the vicinity of the antenna and concentrated\nin few directions (corresponding to the best propagation paths between the\nantenna and the receiver). In this paper, we propose a novel electromagnetic\nfield aware beamforming scheme, which (i) spreads the beamforming radiation\npattern in the angular domain by adding artificial propagation paths thanks to\nreconfigurable intelligent surfaces, (ii) truncates the pattern in strong\ndirections, and (iii) boosts it in weak directions. Compared to existing\nsolutions, it maximizes the received power. However, it also consumes more\npower. Finally, truncation alone is the best trade-off between received power\nand energy efficiency, under exposure constrain.",
    "descriptor": "\nComments: Accepted to 2022 European Conference on Networks and Communications (EuCNC) and the 6G Summit, 7-10 June 2022, Grenoble, France. arXiv admin note: substantial text overlap with arXiv:2206.06870\n",
    "authors": [
      "Nour Awarkeh",
      "Dinh-Thuy Phan-Huy",
      "Raphael Visoz",
      "Marco Di Renzo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07051"
  },
  {
    "id": "arXiv:2206.07083",
    "title": "Learning the Structure of Large Networked Systems Obeying Conservation  Laws",
    "abstract": "Many networked systems such as electric networks, the brain, and social\nnetworks of opinion dynamics are known to obey conservation laws. Examples of\nthis phenomenon include the Kirchoff laws in electric networks and opinion\nconsensus in social networks. Conservation laws in networked systems may be\nmodeled as balance equations of the form $X = B^{*} Y$, where the sparsity\npattern of $B^{*}$ captures the connectivity of the network, and $Y, X \\in\n\\mathbb{R}^p$ are vectors of \"potentials\" and \"injected flows\" at the nodes\nrespectively. The node potentials $Y$ cause flows across edges and the flows\n$X$ injected at the nodes are extraneous to the network dynamics. In several\npractical systems, the network structure is often unknown and needs to be\nestimated from data. Towards this, one has access to samples of the node\npotentials $Y$, but only the statistics of the node injections $X$. Motivated\nby this important problem, we study the estimation of the sparsity structure of\nthe matrix $B^{*}$ from $n$ samples of $Y$ under the assumption that the node\ninjections $X$ follow a Gaussian distribution with a known covariance\n$\\Sigma_X$. We propose a new $\\ell_{1}$-regularized maximum likelihood\nestimator for this problem in the high-dimensional regime where the size of the\nnetwork $p$ is larger than sample size $n$. We show that this optimization\nproblem is convex in the objective and admits a unique solution. Under a new\nmutual incoherence condition, we establish sufficient conditions on the triple\n$(n,p,d)$ for which exact sparsity recovery of $B^{*}$ is possible with high\nprobability; $d$ is the degree of the graph. We also establish guarantees for\nthe recovery of $B^{*}$ in the element-wise maximum, Frobenius, and operator\nnorms. Finally, we complement these theoretical results with experimental\nvalidation of the performance of the proposed estimator on synthetic and\nreal-world data.",
    "descriptor": "",
    "authors": [
      "Anirudh Rayas",
      "Rajasekhar Anguluri",
      "Gautam Dasarathy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.07083"
  },
  {
    "id": "arXiv:2206.07113",
    "title": "Minorities in networks and algorithms",
    "abstract": "In this chapter, we provide an overview of recent advances in data-driven and\ntheory-informed complex models of social networks and their potential in\nunderstanding societal inequalities and marginalization. We focus on\ninequalities arising from networks and network-based algorithms and how they\naffect minorities. In particular, we examine how homophily and mixing biases\nshape large and small social networks, influence perception of minorities, and\naffect collaboration patterns. We also discuss dynamical processes on and of\nnetworks and the formation of norms and health inequalities. Additionally, we\nargue that network modeling is paramount for unveiling the effect of ranking\nand social recommendation algorithms on the visibility of minorities. Finally,\nwe highlight the key challenges and future opportunities in this emerging\nresearch topic.",
    "descriptor": "\nComments: 11 pages, 1 figure, book chapter\n",
    "authors": [
      "Fariba Karimi",
      "Marcos Oliveira",
      "Markus Strohmaier"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2206.07113"
  },
  {
    "id": "arXiv:2206.07114",
    "title": "Inverse design of nano-photonic wavelength demultiplexer with a deep  neural network approach",
    "abstract": "In this paper, we propose a pre-trained-combined neural network (PTCN) as a\ncomprehensive solution to the inverse design of an integrated photonic circuit.\nBy utilizing both the initially pre-trained inverse and forward model with a\njoint training process, our PTCN model shows remarkable tolerance to the\nquantity and quality of the training data. As a proof of concept demonstration,\nthe inverse design of a wavelength demultiplexer is used to verify the\neffectiveness of the PTCN model. The correlation coefficient of the prediction\nby the presented PTCN model remains greater than 0.974 even when the size of\ntraining data is decreased to 17%. The experimental results show a good\nagreement with predictions, and demonstrate a wavelength demultiplexer with an\nultra-compact footprint, a high transmission efficiency with a transmission\nloss of -2dB, a low reflection of -10dB, and low crosstalk around -7dB\nsimultaneously.",
    "descriptor": "",
    "authors": [
      "Mengwei Yuan",
      "Gang Yang",
      "Shijie Song",
      "Luping Zhou",
      "Robert Minasian",
      "Xiaoke Yi"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07114"
  },
  {
    "id": "arXiv:2206.07122",
    "title": "Loss Functions for Classification using Structured Entropy",
    "abstract": "Cross-entropy loss is the standard metric used to train classification models\nin deep learning and gradient boosting. It is well-known that this loss\nfunction fails to account for similarities between the different values of the\ntarget. We propose a generalization of entropy called {\\em structured entropy}\nwhich uses a random partition to incorporate the structure of the target\nvariable in a manner which retains many theoretical properties of standard\nentropy. We show that a structured cross-entropy loss yields better results on\nseveral classification problems where the target variable has an a priori known\nstructure. The approach is simple, flexible, easily computable, and does not\nrely on a hierarchically defined notion of structure.",
    "descriptor": "",
    "authors": [
      "Brian Lucena"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07122"
  },
  {
    "id": "arXiv:2206.07128",
    "title": "Stability of image reconstruction algorithms",
    "abstract": "Robustness and stability of image reconstruction algorithms have recently\ncome under scrutiny. Their importance to medical imaging cannot be overstated.\nWe review the known results for the topical variational regularization\nstrategies ($\\ell_2$ and $\\ell_1$ regularization), and present new stability\nresults for $\\ell_p$ regularized linear inverse problems for $p\\in(1,\\infty)$.\nOur results generalize well to the respective $L_p(\\Omega)$ function spaces.",
    "descriptor": "\nComments: 11 pages, 6 figures, 1 appendix\n",
    "authors": [
      "Pol del Aguila Pla",
      "Sebastian Neumayer",
      "Michael Unser"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07128"
  },
  {
    "id": "arXiv:2206.07156",
    "title": "Federated Multi-organ Segmentation with Partially Labeled Data",
    "abstract": "Federated learning is an emerging paradigm allowing large-scale decentralized\nlearning without sharing data across different data owners, which helps address\nthe concern of data privacy in medical image analysis. However, the requirement\nfor label consistency across clients by the existing methods largely narrows\nits application scope. In practice, each clinical site may only annotate\ncertain organs of interest with partial or no overlap with other sites.\nIncorporating such partially labeled data into a unified federation is an\nunexplored problem with clinical significance and urgency. This work tackles\nthe challenge by using a novel federated multi-encoding U-Net (Fed-MENU) method\nfor multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)\nis proposed to extract organ-specific features through different encoding\nsub-networks. Each sub-network can be seen as an expert of a specific organ and\ntrained for that client. Moreover, to encourage the organ-specific features\nextracted by different sub-networks to be informative and distinctive, we\nregularize the training of the MENU-Net by designing an auxiliary generic\ndecoder (AGD). Extensive experiments on four public datasets show that our\nFed-MENU method can effectively obtain a federated learning model using the\npartially labeled datasets with superior performance to other models trained by\neither localized or centralized learning methods. Source code will be made\npublicly available at the time of paper publication.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Xuanang Xu",
      "Pingkun Yan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07156"
  },
  {
    "id": "arXiv:2206.07199",
    "title": "Benefits of Additive Noise in Composing Classes with Bounded Capacity",
    "abstract": "We observe that given two (compatible) classes of functions $\\mathcal{F}$ and\n$\\mathcal{H}$ with small capacity as measured by their uniform covering\nnumbers, the capacity of the composition class $\\mathcal{H} \\circ \\mathcal{F}$\ncan become prohibitively large or even unbounded. We then show that adding a\nsmall amount of Gaussian noise to the output of $\\mathcal{F}$ before composing\nit with $\\mathcal{H}$ can effectively control the capacity of $\\mathcal{H}\n\\circ \\mathcal{F}$, offering a general recipe for modular design. To prove our\nresults, we define new notions of uniform covering number of random functions\nwith respect to the total variation and Wasserstein distances. We instantiate\nour results for the case of multi-layer sigmoid neural networks. Preliminary\nempirical results on MNIST dataset indicate that the amount of noise required\nto improve over existing uniform bounds can be numerically negligible (i.e.,\nelement-wise i.i.d. Gaussian noise with standard deviation $10^{-240}$). The\nsource codes are available at\nhttps://github.com/fathollahpour/composition_noise.",
    "descriptor": "",
    "authors": [
      "Alireza Fathollah Pour",
      "Hassan Ashtiani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07199"
  },
  {
    "id": "arXiv:2206.07219",
    "title": "A Projection-Based K-space Transformer Network for Undersampled Radial  MRI Reconstruction with Limited Training Subjects",
    "abstract": "The recent development of deep learning combined with compressed sensing\nenables fast reconstruction of undersampled MR images and has achieved\nstate-of-the-art performance for Cartesian k-space trajectories. However,\nnon-Cartesian trajectories such as the radial trajectory need to be transformed\nonto a Cartesian grid in each iteration of the network training, slowing down\nthe training process and posing inconvenience and delay during training.\nMultiple iterations of nonuniform Fourier transform in the networks offset the\ndeep learning advantage of fast inference. Current approaches typically either\nwork on image-to-image networks or grid the non-Cartesian trajectories before\nthe network training to avoid the repeated gridding process. However, the\nimage-to-image networks cannot ensure the k-space data consistency in the\nreconstructed images and the pre-processing of non-Cartesian k-space leads to\ngridding errors which cannot be compensated by the network training. Inspired\nby the Transformer network to handle long-range dependencies in sequence\ntransduction tasks, we propose to rearrange the radial spokes to sequential\ndata based on the chronological order of acquisition and use the Transformer to\npredict unacquired radial spokes from acquired ones. We propose novel data\naugmentation methods to generate a large amount of training data from a limited\nnumber of subjects. The network can be generated to different anatomical\nstructures. Experimental results show superior performance of the proposed\nframework compared to state-of-the-art deep neural networks.",
    "descriptor": "\nComments: Accepted at MICCAI 2022\n",
    "authors": [
      "Chang Gao",
      "Shu-Fu Shih",
      "J. Paul Finn",
      "Xiaodong Zhong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07219"
  },
  {
    "id": "arXiv:2206.07236",
    "title": "Query-Adaptive Predictive Inference with Partial Labels",
    "abstract": "The cost and scarcity of fully supervised labels in statistical machine\nlearning encourage using partially labeled data for model validation as a\ncheaper and more accessible alternative. Effectively collecting and leveraging\nweakly supervised data for large-space structured prediction tasks thus becomes\nan important part of an end-to-end learning system. We propose a new\ncomputationally-friendly methodology to construct predictive sets using only\npartially labeled data on top of black-box predictive models. To do so, we\nintroduce \"probe\" functions as a way to describe weakly supervised instances\nand define a false discovery proportion-type loss, both of which seamlessly\nadapt to partial supervision and structured prediction -- ranking, matching,\nsegmentation, multilabel or multiclass classification. Our experiments\nhighlight the validity of our predictive set construction as well as the\nattractiveness of a more flexible user-dependent loss framework.",
    "descriptor": "",
    "authors": [
      "Maxime Cauchois",
      "John Duchi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07236"
  },
  {
    "id": "arXiv:2206.07239",
    "title": "A Multiple kernel testing procedure for non-proportional hazards in  factorial designs",
    "abstract": "In this paper we propose a Multiple kernel testing procedure to infer\nsurvival data when several factors (e.g. different treatment groups, gender,\nmedical history) and their interaction are of interest simultaneously. Our\nmethod is able to deal with complex data and can be seen as an alternative to\nthe omnipresent Cox model when assumptions such as proportionality cannot be\njustified. Our methodology combines well-known concepts from Survival Analysis,\nMachine Learning and Multiple Testing: differently weighted log-rank tests,\nkernel methods and multiple contrast tests. By that, complex hazard\nalternatives beyond the classical proportional hazard set-up can be detected.\nMoreover, multiple comparisons are performed by fully exploiting the dependence\nstructure of the single testing procedures to avoid a loss of power. In all,\nthis leads to a flexible and powerful procedure for factorial survival designs\nwhose theoretical validity is proven by martingale arguments and the theory for\n$V$-statistics. We evaluate the performance of our method in an extensive\nsimulation study and illustrate it by a real data analysis.",
    "descriptor": "",
    "authors": [
      "Marc Ditzhaus",
      "Tamara Fern\u00e1ndez",
      "Nicol\u00e1s Rivera"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07239"
  },
  {
    "id": "arXiv:2206.07246",
    "title": "Quantum computing overview: discrete vs. continuous variable models",
    "abstract": "In this Near Intermediate-Scale Quantum era, there are two types of near-term\nquantum devices available on cloud: superconducting quantum processing units\n(QPUs) based on the discrete variable model and linear optics (photonics) QPUs\nbased on the continuous variable (CV) model. Quantum computation in the\ndiscrete variable model is performed in a finite dimensional quantum state\nspace and the CV model in an infinite dimensional space. In implementing\nquantum algorithms, the CV model offers more quantum gates that are not\navailable in the discrete variable model. CV-based photonic quantum computers\nprovide additional flexibility of controlling the length of the output vectors\nof quantum circuits, using different methods of measurement and the notion of\ncutoff dimension.",
    "descriptor": "",
    "authors": [
      "Sophie Choe"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07246"
  },
  {
    "id": "arXiv:2206.07252",
    "title": "Implicit Regularization or Implicit Conditioning? Exact Risk  Trajectories of SGD in High Dimensions",
    "abstract": "Stochastic gradient descent (SGD) is a pillar of modern machine learning,\nserving as the go-to optimization algorithm for a diverse array of problems.\nWhile the empirical success of SGD is often attributed to its computational\nefficiency and favorable generalization behavior, neither effect is well\nunderstood and disentangling them remains an open problem. Even in the simple\nsetting of convex quadratic problems, worst-case analyses give an asymptotic\nconvergence rate for SGD that is no better than full-batch gradient descent\n(GD), and the purported implicit regularization effects of SGD lack a precise\nexplanation. In this work, we study the dynamics of multi-pass SGD on\nhigh-dimensional convex quadratics and establish an asymptotic equivalence to a\nstochastic differential equation, which we call homogenized stochastic gradient\ndescent (HSGD), whose solutions we characterize explicitly in terms of a\nVolterra integral equation. These results yield precise formulas for the\nlearning and risk trajectories, which reveal a mechanism of implicit\nconditioning that explains the efficiency of SGD relative to GD. We also prove\nthat the noise from SGD negatively impacts generalization performance, ruling\nout the possibility of any type of implicit regularization in this context.\nFinally, we show how to adapt the HSGD formalism to include streaming SGD,\nwhich allows us to produce an exact prediction for the excess risk of\nmulti-pass SGD relative to that of streaming SGD (bootstrap risk).",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.07069\n",
    "authors": [
      "Courtney Paquette",
      "Elliot Paquette",
      "Ben Adlam",
      "Jeffrey Pennington"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.07252"
  },
  {
    "id": "arXiv:2206.07261",
    "title": "Latency Control for Keyword Spotting",
    "abstract": "Conversational agents commonly utilize keyword spotting (KWS) to initiate\nvoice interaction with the user. For user experience and privacy\nconsiderations, existing approaches to KWS largely focus on accuracy, which can\noften come at the expense of introduced latency. To address this tradeoff, we\npropose a novel approach to control KWS model latency and which generalizes to\nany loss function without explicit knowledge of the keyword endpoint. Through a\nsingle, tunable hyperparameter, our approach enables one to balance detection\nlatency and accuracy for the targeted application. Empirically, we show that\nour approach gives superior performance under latency constraints when compared\nto existing methods. Namely, we make a substantial 25\\% relative false accepts\nimprovement for a fixed latency target when compared to the baseline\nstate-of-the-art. We also show that when our approach is used in conjunction\nwith a max-pooling loss, we are able to improve relative false accepts by 25 %\nat a fixed latency when compared to cross entropy loss.",
    "descriptor": "\nComments: Proceedings of INTERSPEECH\n",
    "authors": [
      "Christin Jose",
      "Joseph Wang",
      "Grant P. Strimel",
      "Mohammad Omar Khursheed",
      "Yuriy Mishchenko",
      "Brian Kulis"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07261"
  },
  {
    "id": "arXiv:2206.07275",
    "title": "CARD: Classification and Regression Diffusion Models",
    "abstract": "Learning the distribution of a continuous or categorical response variable\n$\\boldsymbol y$ given its covariates $\\boldsymbol x$ is a fundamental problem\nin statistics and machine learning. Deep neural network-based supervised\nlearning algorithms have made great progress in predicting the mean of\n$\\boldsymbol y$ given $\\boldsymbol x$, but they are often criticized for their\nability to accurately capture the uncertainty of their predictions. In this\npaper, we introduce classification and regression diffusion (CARD) models,\nwhich combine a denoising diffusion-based conditional generative model and a\npre-trained conditional mean estimator, to accurately predict the distribution\nof $\\boldsymbol y$ given $\\boldsymbol x$. We demonstrate the outstanding\nability of CARD in conditional distribution prediction with both toy examples\nand real-world datasets, the experimental results on which show that CARD in\ngeneral outperforms state-of-the-art methods, including Bayesian neural\nnetwork-based ones that are designed for uncertainty estimation, especially\nwhen the conditional distribution of $\\boldsymbol y$ given $\\boldsymbol x$ is\nmulti-modal.",
    "descriptor": "",
    "authors": [
      "Xizewen Han",
      "Huangjie Zheng",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.07275"
  },
  {
    "id": "arXiv:2206.07280",
    "title": "ERNAS: An Evolutionary Neural Architecture Search for Magnetic Resonance  Image Reconstructions",
    "abstract": "Magnetic resonance imaging (MRI) is one of the noninvasive imaging modalities\nthat can produce high-quality images. However, the scan procedure is relatively\nslow, which causes patient discomfort and motion artifacts in images.\nAccelerating MRI hardware is constrained by physical and physiological\nlimitations. A popular alternative approach to accelerated MRI is to\nundersample the k-space data. While undersampling speeds up the scan procedure,\nit generates artifacts in the images, and advanced reconstruction algorithms\nare needed to produce artifact-free images. Recently deep learning has emerged\nas a promising MRI reconstruction method to address this problem. However,\nstraightforward adoption of the existing deep learning neural network\narchitectures in MRI reconstructions is not usually optimal in terms of\nefficiency and reconstruction quality. In this work, MRI reconstruction from\nundersampled data was carried out using an optimized neural network using a\nnovel evolutionary neural architecture search algorithm. Brain and knee MRI\ndatasets show that the proposed algorithm outperforms manually designed neural\nnetwork-based MR reconstruction models.",
    "descriptor": "\nComments: 11 pages, 9 figures, and 4 tables\n",
    "authors": [
      "Samira Vafay Eslahi",
      "Jian Tao",
      "Jim Ji"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.07280"
  },
  {
    "id": "arXiv:2206.07281",
    "title": "Super-resolution image display using diffractive decoders",
    "abstract": "High-resolution synthesis/projection of images over a large field-of-view\n(FOV) is hindered by the restricted space-bandwidth-product (SBP) of wavefront\nmodulators. We report a deep learning-enabled diffractive display design that\nis based on a jointly-trained pair of an electronic encoder and a diffractive\noptical decoder to synthesize/project super-resolved images using\nlow-resolution wavefront modulators. The digital encoder, composed of a trained\nconvolutional neural network (CNN), rapidly pre-processes the high-resolution\nimages of interest so that their spatial information is encoded into\nlow-resolution (LR) modulation patterns, projected via a low SBP wavefront\nmodulator. The diffractive decoder processes this LR encoded information using\nthin transmissive layers that are structured using deep learning to\nall-optically synthesize and project super-resolved images at its output FOV.\nOur results indicate that this diffractive image display can achieve a\nsuper-resolution factor of ~4, demonstrating a ~16-fold increase in SBP. We\nalso experimentally validate the success of this diffractive super-resolution\ndisplay using 3D-printed diffractive decoders that operate at the THz spectrum.\nThis diffractive image decoder can be scaled to operate at visible wavelengths\nand inspire the design of large FOV and high-resolution displays that are\ncompact, low-power, and computationally efficient.",
    "descriptor": "\nComments: 26 Pages, 9 Figures\n",
    "authors": [
      "Cagatay Isil",
      "Deniz Mengu",
      "Yifan Zhao",
      "Anika Tabassum",
      "Jingxi Li",
      "Yi Luo",
      "Mona Jarrahi",
      "Aydogan Ozcan"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07281"
  },
  {
    "id": "arXiv:2206.07305",
    "title": "Diffusion Transport Alignment",
    "abstract": "The integration of multimodal data presents a challenge in cases when the\nstudy of a given phenomena by different instruments or conditions generates\ndistinct but related domains. Many existing data integration methods assume a\nknown one-to-one correspondence between domains of the entire dataset, which\nmay be unrealistic. Furthermore, existing manifold alignment methods are not\nsuited for cases where the data contains domain-specific regions, i.e., there\nis not a counterpart for a certain portion of the data in the other domain. We\npropose Diffusion Transport Alignment (DTA), a semi-supervised manifold\nalignment method that exploits prior correspondence knowledge between only a\nfew points to align the domains. By building a diffusion process, DTA finds a\ntransportation plan between data measured from two heterogeneous domains with\ndifferent feature spaces, which by assumption, share a similar geometrical\nstructure coming from the same underlying data generating process. DTA can also\ncompute a partial alignment in a data-driven fashion, resulting in accurate\nalignments when some data are measured in only one domain. We empirically\ndemonstrate that DTA outperforms other methods in aligning multimodal data in\nthis semisupervised setting. We also empirically show that the alignment\nobtained by DTA can improve the performance of machine learning tasks, such as\ndomain adaptation, inter-domain feature mapping, and exploratory data analysis,\nwhile outperforming competing methods.",
    "descriptor": "",
    "authors": [
      "Andres F. Duque",
      "Guy Wolf",
      "Kevin R. Moon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07305"
  },
  {
    "id": "arXiv:2206.07327",
    "title": "Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging  Features For Elderly And Dysarthric Speech Recognition",
    "abstract": "Articulatory features are inherently invariant to acoustic signal distortion\nand have been successfully incorporated into automatic speech recognition (ASR)\nsystems designed for normal speech. Their practical application to atypical\ntask domains such as elderly and disordered speech across languages is often\nlimited by the difficulty in collecting such specialist data from target\nspeakers. This paper presents a cross-domain and cross-lingual A2A inversion\napproach that utilizes the parallel audio, visual and ultrasound tongue imaging\n(UTI) data of the 24-hour TaL corpus in A2A model pre-training before being\ncross-domain and cross-lingual adapted to three datasets across two languages:\nthe English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora;\nand the English TORGO dysarthric speech data, to produce UTI based articulatory\nfeatures. Experiments conducted on three tasks suggested incorporating the\ngenerated articulatory features consistently outperformed the baseline hybrid\nTDNN and Conformer based end-to-end systems constructed using acoustic features\nonly by statistically significant word error rate or character error rate\nreductions up to 2.64%, 1.92% and 1.21% absolute (8.17%, 7.89% and 13.28%\nrelative) after data augmentation and speaker adaptation were applied.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.10274\n",
    "authors": [
      "Shujie Hu",
      "Xurong Xie",
      "Mengzhe Geng",
      "Mingyu Cui",
      "Jiajun Deng",
      "Tianzi Wang",
      "Xunying Liu",
      "Helen Meng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07327"
  },
  {
    "id": "arXiv:2206.07334",
    "title": "Detection of magnetohydrodynamic waves by using machine learning",
    "abstract": "Nonlinear wave interactions, such as shock refraction at an inclined density\ninterface, in magnetohydrodynamic (MHD) lead to a plethora of wave patterns\nwith myriad wave types. Identification of different types of MHD waves is an\nimportant and challenging task in such complex wave patterns. Moreover, owing\nto the multiplicity of solutions and their admissibility for different systems,\nespecially for intermediate-type MHD shock waves, the identification of MHD\nwave types is complicated if one solely relies on the Rankine-Hugoniot jump\nconditions. MHD wave detection is further exacerbated by the unphysical\nsmearing of discontinuous shock waves in numerical simulations. We present two\nMHD wave detection methods based on a convolutional neural network (CNN) which\nenables the classification of waves and identification of their locations. The\nfirst method separates the output into a regression (location prediction) and a\nclassification problem assuming the number of waves for each training data is\nfixed. In the second method, the number of waves is not specified a priori and\nthe algorithm, using only regression, predicts the waves' locations and\nclassifies their types. The first fixed output model efficiently provides high\nprecision and recall, the accuracy of the entire neural network achieved is up\nto 0.99, and the classification accuracy of some waves approaches unity. The\nsecond detection model has relatively lower performance, with more sensitivity\nto the setting of parameters, such as the number of grid cells N_{grid} and the\nthresholds of confidence score and class probability, etc. The proposed two\nmethods demonstrate very strong potential to be applied for MHD wave detection\nin some complex wave structures and interactions.",
    "descriptor": "",
    "authors": [
      "Fang Chen",
      "Ravi Samtaney"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07334"
  },
  {
    "id": "arXiv:2206.07364",
    "title": "Seeking Common Ground While Reserving Differences: Multiple Anatomy  Collaborative Framework for Undersampled MRI Reconstruction",
    "abstract": "Recently, deep neural networks have greatly advanced undersampled Magnetic\nResonance Image (MRI) reconstruction, wherein most studies follow the\none-anatomy-one-network fashion, i.e., each expert network is trained and\nevaluated for a specific anatomy. Apart from inefficiency in training multiple\nindependent models, such convention ignores the shared de-aliasing knowledge\nacross various anatomies which can benefit each other. To explore the shared\nknowledge, one naive way is to combine all the data from various anatomies to\ntrain an all-round network. Unfortunately, despite the existence of the shared\nde-aliasing knowledge, we reveal that the exclusive knowledge across different\nanatomies can deteriorate specific reconstruction targets, yielding overall\nperformance degradation. Observing this, in this study, we present a novel deep\nMRI reconstruction framework with both anatomy-shared and anatomy-specific\nparameterized learners, aiming to \"seek common ground while reserving\ndifferences\" across different anatomies.Particularly, the primary\nanatomy-shared learners are exposed to different anatomies to model flourishing\nshared knowledge, while the efficient anatomy-specific learners are trained\nwith their target anatomy for exclusive knowledge. Four different\nimplementations of anatomy-specific learners are presented and explored on the\ntop of our framework in two MRI reconstruction networks. Comprehensive\nexperiments on brain, knee and cardiac MRI datasets demonstrate that three of\nthese learners are able to enhance reconstruction performance via multiple\nanatomy collaborative learning.",
    "descriptor": "\nComments: submitted to an IEEE journal\n",
    "authors": [
      "Yan Jiangpeng",
      "Yu Chenghui",
      "Chen Hanbo",
      "Xu Zhe",
      "Huang Junzhou",
      "Li Xiu",
      "Yao Jianhua"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07364"
  },
  {
    "id": "arXiv:2206.07370",
    "title": "Lattice Convolutional Networks for Learning Ground States of Quantum  Many-Body Systems",
    "abstract": "Deep learning methods have been shown to be effective in representing\nground-state wave functions of quantum many-body systems. Existing methods use\nconvolutional neural networks (CNNs) for square lattices due to their\nimage-like structures. For non-square lattices, existing method uses graph\nneural network (GNN) in which structure information is not precisely captured,\nthereby requiring additional hand-crafted sublattice encoding. In this work, we\npropose lattice convolutions in which a set of proposed operations are used to\nconvert non-square lattices into grid-like augmented lattices on which regular\nconvolution can be applied. Based on the proposed lattice convolutions, we\ndesign lattice convolutional networks (LCN) that use self-gating and attention\nmechanisms. Experimental results show that our method achieves performance on\npar or better than existing methods on spin 1/2 $J_1$-$J_2$ Heisenberg model\nover the square, honeycomb, triangular, and kagome lattices while without using\nhand-crafted encoding.",
    "descriptor": "",
    "authors": [
      "Cong Fu",
      "Xuan Zhang",
      "Huixin Zhang",
      "Hongyi Ling",
      "Shenglong Xu",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07370"
  },
  {
    "id": "arXiv:2206.07381",
    "title": "Pancyclicity in the Cartesian Product $(K_9-C_9 )^n$",
    "abstract": "A graph $G$ on $m$ vertices is pancyclic if it contains cycles of length $l$,\n$3\\leq l \\leq m$ as subgraphs in $G$. The complete graph $K_{9}$ on 9 vertices\nwith a cycle $C_{9}$ of length 9 deleted from $K_{9}$ is denoted by\n$(K_{9}-C_{9})$. In this paper, we prove that $(K_{9}-C_{9})^{n}$, the\nCartesian product of $(K_{9}-C_{9})$ taken $n$ times, is pancyclic.",
    "descriptor": "\nComments: 6 PAGES, 4 FIGURES\n",
    "authors": [
      "Syeda Afiya",
      "M Rajesh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.07381"
  },
  {
    "id": "arXiv:2206.07386",
    "title": "Finite-Sample Guarantees for High-Dimensional DML",
    "abstract": "Debiased machine learning (DML) offers an attractive way to estimate\ntreatment effects in observational settings, where identification of causal\nparameters requires a conditional independence or unconfoundedness assumption,\nsince it allows to control flexibly for a potentially very large number of\ncovariates. This paper gives novel finite-sample guarantees for joint inference\non high-dimensional DML, bounding how far the finite-sample distribution of the\nestimator is from its asymptotic Gaussian approximation. These guarantees are\nuseful to applied researchers, as they are informative about how far off the\ncoverage of joint confidence bands can be from the nominal level. There are\nmany settings where high-dimensional causal parameters may be of interest, such\nas the ATE of many treatment profiles, or the ATE of a treatment on many\noutcomes. We also cover infinite-dimensional parameters, such as impacts on the\nentire marginal distribution of potential outcomes. The finite-sample\nguarantees in this paper complement the existing results on consistency and\nasymptotic normality of DML estimators, which are either asymptotic or treat\nonly the one-dimensional case.",
    "descriptor": "",
    "authors": [
      "Victor Quintas-Martinez"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07386"
  },
  {
    "id": "arXiv:2206.07388",
    "title": "Subsurface Depths Structure Maps Reconstruction with Generative  Adversarial Networks",
    "abstract": "This paper described a method for reconstruction of detailed-resolution depth\nstructure maps, usually obtained after the 3D seismic surveys, using the data\nfrom 2D seismic depth maps. The method uses two algorithms based on the\ngenerative-adversarial neural network architecture. The first algorithm\nStyleGAN2-ADA accumulates in the hidden space of the neural network the\nsemantic images of mountainous terrain forms first, and then with help of\ntransfer learning, in the ideal case - the structure geometry of stratigraphic\nhorizons. The second algorithm, the Pixel2Style2Pixel encoder, using the\nsemantic level of generalization of the first algorithm, learns to reconstruct\nthe original high-resolution images from their degraded copies\n(super-resolution technology). There was demonstrated a methodological approach\nto transferring knowledge on the structural forms of stratigraphic horizon\nboundaries from the well-studied areas to the underexplored ones. Using the\nmultimodal synthesis of Pixel2Style2Pixel encoder, it is proposed to create a\nprobabilistic depth space, where each point of the project area is represented\nby the density of probabilistic depth distribution of equally probable\nreconstructed geological forms of structural images. Assessment of the\nreconstruction quality was carried out for two blocks. Using this method,\ncredible detailed depth reconstructions comparable with the quality of 3D\nseismic maps have been obtained from 2D seismic maps.",
    "descriptor": "\nComments: 12 pages, 12 figures, 1 table\n",
    "authors": [
      "Dmitry Ivlev"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07388"
  },
  {
    "id": "arXiv:2206.07417",
    "title": "Interpretable differential diagnosis for Alzheimer's disease and  Frontotemporal dementia",
    "abstract": "Alzheimer's disease and Frontotemporal dementia are two major types of\ndementia. Their accurate diagnosis and differentiation is crucial for\ndetermining specific intervention and treatment. However, differential\ndiagnosis of these two types of dementia remains difficult at the early stage\nof disease due to similar patterns of clinical symptoms. Therefore, the\nautomatic classification of multiple types of dementia has an important\nclinical value. So far, this challenge has not been actively explored. Recent\ndevelopment of deep learning in the field of medical image has demonstrated\nhigh performance for various classification tasks. In this paper, we propose to\ntake advantage of two types of biomarkers: structure grading and structure\natrophy. To this end, we propose first to train a large ensemble of 3D U-Nets\nto locally discriminate healthy versus dementia anatomical patterns. The result\nof these models is an interpretable 3D grading map capable of indicating\nabnormal brain regions. This map can also be exploited in various\nclassification tasks using graph convolutional neural network. Finally, we\npropose to combine deep grading and atrophy-based classifications to improve\ndementia type discrimination. The proposed framework showed competitive\nperformance compared to state-of-the-art methods for different tasks of disease\ndetection and differential diagnosis.",
    "descriptor": "",
    "authors": [
      "Huy-Dung Nguyen",
      "Micha\u00ebl Cl\u00e9ment",
      "Boris Mansencal",
      "Pierrick Coup\u00e9"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07417"
  },
  {
    "id": "arXiv:2206.07422",
    "title": "Deep Neural Network Pruning for Nuclei Instance Segmentation in  Hematoxylin & Eosin-Stained Histological Images",
    "abstract": "Recently, pruning deep neural networks (DNNs) has received a lot of attention\nfor improving accuracy and generalization power, reducing network size, and\nincreasing inference speed on specialized hardwares. Although pruning was\nmainly tested on computer vision tasks, its application in the context of\nmedical image analysis has hardly been explored. This work investigates the\nimpact of well-known pruning techniques, namely layer-wise and network-wide\nmagnitude pruning, on the nuclei instance segmentation performance in\nhistological images. Our utilized instance segmentation model consists of two\nmain branches: (1) a semantic segmentation branch, and (2) a deep regression\nbranch. We investigate the impact of weight pruning on the performance of both\nbranches separately and on the final nuclei instance segmentation result.\nEvaluated on two publicly available datasets, our results show that layer-wise\npruning delivers slightly better performance than networkwide pruning for small\ncompression ratios (CRs) while for large CRs, network-wide pruning yields\nsuperior performance. For semantic segmentation, deep regression and final\ninstance segmentation, 93.75 %, 95 %, and 80 % of the model weights can be\npruned by layer-wise pruning with less than 2 % reduction in the performance of\nrespective models.",
    "descriptor": "",
    "authors": [
      "Amirreza Mahbod",
      "Rahim Entezari",
      "Isabella Ellinger",
      "Olga Saukh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07422"
  },
  {
    "id": "arXiv:2206.07430",
    "title": "Residual Language Model for End-to-end Speech Recognition",
    "abstract": "End-to-end automatic speech recognition suffers from adaptation to unknown\ntarget domain speech despite being trained with a large amount of paired\naudio--text data. Recent studies estimate a linguistic bias of the model as the\ninternal language model (LM). To effectively adapt to the target domain, the\ninternal LM is subtracted from the posterior during inference and fused with an\nexternal target-domain LM. However, this fusion complicates the inference and\nthe estimation of the internal LM may not always be accurate. In this paper, we\npropose a simple external LM fusion method for domain adaptation, which\nconsiders the internal LM estimation in its training. We directly model the\nresidual factor of the external and internal LMs, namely the residual LM. To\nstably train the residual LM, we propose smoothing the estimated internal LM\nand optimizing it with a combination of cross-entropy and mean-squared-error\nlosses, which consider the statistical behaviors of the internal LM in the\ntarget domain data. We experimentally confirmed that the proposed residual LM\nperforms better than the internal LM estimation in most of the cross-domain and\nintra-domain scenarios.",
    "descriptor": "\nComments: Accepted for Interspeech2022\n",
    "authors": [
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Chaitanya Narisetty",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.07430"
  },
  {
    "id": "arXiv:2206.07432",
    "title": "A short note on compact embeddings of reproducing kernel Hilbert spaces  in $L^2$ for infinite-variate function approximation",
    "abstract": "This note consists of two largely independent parts. In the first part we\ngive conditions on the kernel $k: \\Omega \\times \\Omega \\rightarrow \\mathbb{R}$\nof a reproducing kernel Hilbert space $H$ continuously embedded via the\nidentity mapping into $L^2(\\Omega, \\mu),$ which are equivalent to the fact that\n$H$ is even compactly embedded into $L^2(\\Omega, \\mu).$\nIn the second part we consider a scenario from infinite-variate\n$L^2$-approximation.\nSuppose that the embedding of a reproducing kernel Hilbert space of\nunivariate functions with reproducing kernel $1+k$ into $L^2(\\Omega, \\mu)$ is\ncompact. We provide a simple criterion for checking compactness of the\nembedding of a reproducing kernel Hilbert space with the kernel given by\n$$\\sum_{u \\in \\mathcal{U}} \\gamma_u \\bigotimes_{j \\in u}k,$$ where $\\mathcal{U}\n= \\{u \\subset \\mathbb{N}: |u| < \\infty\\},$ and $(\\gamma_u)_{u \\in \\mathcal{U}}$\nis a sequence of non-negative numbers, into an appropriate $L^2$ space.",
    "descriptor": "",
    "authors": [
      "Marcin Wnuk"
    ],
    "subjectives": [
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07432"
  },
  {
    "id": "arXiv:2206.07449",
    "title": "Self-Assessment for Single-Object Tracking in Clutter Using Subjective  Logic",
    "abstract": "Reliable tracking algorithms are essential for automated driving. However,\nthe existing consistency measures are not sufficient to meet the increasing\nsafety demands in the automotive sector. Therefore, this work presents a novel\nmethod for self-assessment of single-object tracking in clutter based on Kalman\nfiltering and subjective logic. A key feature of the approach is that it\nadditionally provides a measure of the collected statistical evidence in its\nonline reliability scores. In this way, various aspects of reliability, such as\nthe correctness of the assumed measurement noise, detection probability, and\nclutter rate, can be monitored in addition to the overall assessment based on\nthe available evidence. Here, we present a mathematical derivation of the\nreference distribution used in our self-assessment module for our studied\nproblem. Moreover, we introduce a formula that describes how a threshold should\nbe chosen for the degree of conflict, the subjective logic comparison measure\nused for the reliability decision making. Our approach is evaluated in a\nchallenging simulation scenario designed to model adverse weather conditions.\nThe simulations show that our method can significantly improve the reliability\nchecking of single-object tracking in clutter in several aspects.",
    "descriptor": "\nComments: Accepted for presentation at the 2022 IEEE 25th International Conference on Information Fusion (FUSION), July 4 - 7, 2022, Link\\\"oping, Sweden\n",
    "authors": [
      "Thomas Griebel",
      "Johannes M\u00fcller",
      "Paul Geisler",
      "Charlotte Hermann",
      "Martin Herrmann",
      "Michael Buchholz",
      "Klaus Dietmayer"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Probability (math.PR)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.07449"
  },
  {
    "id": "arXiv:2206.07465",
    "title": "High-fidelity quantitative differential phase contrast deconvolution  using dark-field sparse prior",
    "abstract": "Differential phase contrast (DPC) imaging plays an important role in the\nfamily of quantitative phase measurement. However, the reconstruction algorithm\nfor quantitative DPC (qDPC) imaging is not yet optimized, as it does not\nincorporate the inborn properties of qDPC imaging. In this research, we propose\na simple but effective image prior, the dark-field sparse prior (DSP), to\nfacilitate the phase reconstruction quality for all DPC-based phase\nreconstruction algorithms. The DSP is based on the key observation that most\npixel values for an idea differential phase contrast image are zeros since the\nsubtraction of two images under anti-symmetric illumination cancels all\nbackground components. With this DSP prior, we formed a new cost function in\nwhich L0-norm was used to represent the DSP. Further, we developed two\ndifferent algorithms based on (1) the Half Quadratic Splitting, and (2) the\nRichardson-Lucy deconvolution to solve this NP-hard L0-norm problem. We tested\nour new model on both simulated and experimental data and compare against\nstate-of-the-art methods including L2-norm and total variation regularizations.\nResults show that our proposed model is superior in terms of phase\nreconstruction quality and implementation efficiency, in which it significantly\nincreases the experimental robustness, while maintaining the data fidelity.",
    "descriptor": "",
    "authors": [
      "Shuhe Zhang",
      "Tao Peng",
      "Zeyu Ke",
      "Meng Shao",
      "Tos T. J. M. Berendschot",
      "Jinhua Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2206.07465"
  },
  {
    "id": "arXiv:2206.07481",
    "title": "A Survey of Detection Methods for Die Attachment and Wire Bonding  Defects in Integrated Circuit Manufacturing",
    "abstract": "Defect detection plays a vital role in the manufacturing process of\nintegrated circuits (ICs). Die attachment and wire bonding are two steps of the\nmanufacturing process that determine the quality and reliability of the power\nand signal transmission in an IC. This paper presents a survey or literature\nreview of the methods used for detecting these defects based on different\nsensing modalities used including optical, radiological, acoustical, and\ninfrared thermography. A discussion of the detection methods used is provided\nin this survey. Both conventional and deep learning approaches for detecting\ndie attachment and wire bonding defects are considered along with challenges\nand future research directions.",
    "descriptor": "\nComments: 13 pages, 9 figures, 8 tables\n",
    "authors": [
      "Lamia Alam",
      "Nasser Kehtarnavaz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07481"
  },
  {
    "id": "arXiv:2206.07483",
    "title": "Blind Estimation of a Doubly Selective OFDM Channel: A Deep Learning  Algorithm and Theory",
    "abstract": "We provide a new generation solution to the fundamental old problem of a\ndoubly selective fading channel estimation for orthogonal frequency division\nmultiplexing (OFDM) systems. For systems based on OFDM, we propose a deep\nlearning (DL)-based blind doubly selective channel estimator. This estimator\ndoes require no pilot symbols, unlike the corresponding state-of-the-art\nestimators, even during the estimation of a deep fading doubly selective\nchannel. We also provide the first of its kind theory on the testing mean\nsquared error (MSE) performance of our investigated blind OFDM channel\nestimator based on over-parameterized ReLU FNNs.",
    "descriptor": "",
    "authors": [
      "Tilahun M. Getu",
      "Nada T. Golmie",
      "David W. Griffith"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07483"
  },
  {
    "id": "arXiv:2206.07484",
    "title": "Intelligent analysis of EEG signals to assess consumer decisions: A  Study on Neuromarketing",
    "abstract": "Neuromarketing is an emerging field that combines neuroscience and marketing\nto understand the factors that influence consumer decisions better. The study\nproposes a method to understand consumers' positive and negative reactions to\nadvertisements (ads) and products by analysing electroencephalogram (EEG)\nsignals. These signals are recorded using a low-cost single electrode headset\nfrom volunteers belonging to the ages 18-22. A detailed subject dependent (SD)\nand subject independent (SI) analysis was performed employing machine learning\nmethods like Naive Bayes (NB), Support Vector Machine (SVM), k-nearest\nneighbour and Decision Tree and the proposed deep learning (DL) model. SVM and\nNB yielded an accuracy (Acc.) of 0.63 for the SD analysis. In SI analysis, SVM\nperformed better for the advertisement, product and gender-based analysis.\nFurthermore, the performance of the DL model was on par with that of SVM,\nespecially, in product and ads-based analysis.",
    "descriptor": "\nComments: 7 pages, 6 figures\n",
    "authors": [
      "Nikunj Phutela",
      "Abhilash P",
      "Kaushik Sreevathsan",
      "B N Krupa"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07484"
  },
  {
    "id": "arXiv:2206.07486",
    "title": "Topological Simplification of Signals for Inference and Approximate  Reconstruction",
    "abstract": "As Internet of Things (IoT) devices become both cheaper and more powerful,\nresearchers are increasingly finding solutions to their scientific curiosities\nboth financially and computationally feasible. When operating with restricted\npower or communications budgets, however, devices can only send\nhighly-compressed data. Such circumstances are common for devices placed away\nfrom electric grids that can only communicate via satellite, a situation\nparticularly plausible for environmental sensor networks. These restrictions\ncan be further complicated by potential variability in the communications\nbudget, for example a solar-powered device needing to expend less energy when\ntransmitting data on a cloudy day. We propose a novel, topology-based, lossy\ncompression method well-equipped for these restrictive yet variable\ncircumstances. This technique, Topological Signal Compression, allows sending\ncompressed signals that utilize the entirety of a variable communications\nbudget. To demonstrate our algorithm's capabilities, we perform entropy\ncalculations as well as a classification exercise on increasingly topologically\nsimplified signals from the Free-Spoken Digit Dataset and explore the stability\nof the resulting performance against common baselines.",
    "descriptor": "\nComments: 10 pages, 12 figures\n",
    "authors": [
      "Gary Koplik",
      "Nathan Borggren",
      "Sam Voisin",
      "Gabrielle Angeloro",
      "Jay Hineman",
      "Tessa Johnson",
      "Paul Bendich"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07486"
  },
  {
    "id": "arXiv:2206.07488",
    "title": "IoT- Based Low-Cost Soil Moisture and Soil Temperature Monitoring System",
    "abstract": "Soil moisture (SM) is referred to as a finite amount of water molecules\nwithin the pore spaces and it is a crucial parameter of Hydro-Meteorological\nprocesses. The behaviour of soil moisture water changes spatially and\ntemporally in response to topography, soil characteristics, and climate[1].\nSoil moisture is overseen by various hydro-meteorological factors that vary\nvertically with depth, laterally across terrestrial shapes, and temporarily in\nfeedback to the climate. The precise monitoring and quantification of\nhigh-resolution surface and subsurface soil moisture observations are very\nimportant [13]. This paper highlights the outcomes of the fieldwork carried out\nat IITM, Pune, wherein we have developed a soil moisture and temperature\nmeasurement system using Raspberry Pi and the Internet of things (IoT). The\ndevelopment is classified into three stages, the first stage includes the\nassembly of the sensor with the microprocessor. The deployment of the low-cost\nsystem, data generation, and communication through a wireless sensor network is\npart of the second stage. Finally, the third stage includes real-time data\nvisualization using a mobile application and data server for analysing soil\nmoisture and temperature. The soil moisture profile obtained through the sensor\ndeployed is highly correlated (r=.9) with in-situ gravimetric observations,\nhaving a root mean square error (RMSE) of about 3.1%. Similarly, the\ntemperature observations are well-matched with the in-situ standard temperature\nobservation. Here we present the preliminary results and compare the accuracy\nwith the state-of-the-art sensors.",
    "descriptor": "\nComments: 07 pages, 08 figures, 03 tables. Submitted to 6th International Conference on Computing Communication Control and Automation (ICCUBEA) - 2022\n",
    "authors": [
      "Guruprasad Deshpande",
      "Mangesh Goswami",
      "Jayesh Kolhe",
      "Vishal Khandagale",
      "Darshan Khope",
      "Gargi Patel",
      "Radhika Doijad",
      "Rajani P. K.",
      "Milind Mujumdar",
      "Bhupendra Bahadur Singh",
      "Naresh Ganeshi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Instrumentation and Detectors (physics.ins-det)"
    ],
    "url": "https://arxiv.org/abs/2206.07488"
  },
  {
    "id": "arXiv:2206.07492",
    "title": "Preliminary study on the impact of EEG density on TMS-EEG classification  in Alzheimer's disease",
    "abstract": "Transcranial magnetic stimulation co-registered with electroencephalographic\n(TMS-EEG) has previously proven a helpful tool in the study of Alzheimer's\ndisease (AD). In this work, we investigate the use of TMS-evoked EEG responses\nto classify AD patients from healthy controls (HC). By using a dataset\ncontaining 17AD and 17HC, we extract various time domain features from\nindividual TMS responses and average them over a low, medium and high density\nEEG electrode set. Within a leave-one-subject-out validation scenario, the best\nclassification performance for AD vs. HC was obtained using a high-density\nelectrode with a Random Forest classifier. The accuracy, sensitivity and\nspecificity were of 92.7%, 96.58% and 88.2% respectively.",
    "descriptor": "\nComments: 4 pages, 4 figures, accepted to the 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 11-15 July 2022, Glasgow, Scotland, UK\n",
    "authors": [
      "Alexandra-Maria Tautan",
      "Elias Casula",
      "Ilaria Borghi",
      "Michele Maiella",
      "Sonia Bonni",
      "Marilena Minei",
      "Martina Assogna",
      "Bogdan Ionescu",
      "Giacomo Koch",
      "Emiliano Santernacchi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07492"
  },
  {
    "id": "arXiv:2206.07515",
    "title": "A Deep Learning Network for the Classification of Intracardiac  Electrograms in Atrial Tachycardia",
    "abstract": "A key technology enabling the success of catheter ablation treatment for\natrial tachycardia is activation mapping, which relies on manual local\nactivation time (LAT) annotation of all acquired intracardiac electrogram (EGM)\nsignals. This is a time-consuming and error-prone procedure, due to the\ndifficulty in identifying the signal activation peaks for fractionated signals.\nThis work presents a Deep Learning approach for the automated classification of\nEGM signals into three different types: normal, abnormal, and unclassified,\nwhich forms part of the LAT annotation pipeline, and contributes towards\nbypassing the need for manual annotations of the LAT. The Deep Learning\nnetwork, the CNN-LSTM model, is a hybrid network architecture which combines\nconvolutional neural network (CNN) layers with long short-term memory (LSTM)\nlayers. 1452 EGM signals from a total of 9 patients undergoing\nclinically-indicated 3D cardiac mapping were used for the training, validation\nand testing of our models. From our findings, the CNN-LSTM model achieved an\naccuracy of 81% for the balanced dataset. For comparison, we separately\ndeveloped a rule-based Decision Trees model which attained an accuracy of 67%\nfor the same balanced dataset. Our work elucidates that analysing the EGM\nsignals using a set of explicitly specified rules as proposed by the Decision\nTrees model is not suitable as EGM signals are complex. The CNN-LSTM model, on\nthe other hand, has the ability to learn the complex, intrinsic features within\nthe signals and identify useful features to differentiate the EGM signals.",
    "descriptor": "\nComments: 34 pages, 10 figures\n",
    "authors": [
      "Zerui Chen",
      "Sonia Xhyn Teo",
      "Andrie Ochtman",
      "Shier Nee Saw",
      "Nicholas Cheng",
      "Eric Tien Siang Lim",
      "Murphy Lyu",
      "Hwee Kuan Lee"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07515"
  },
  {
    "id": "arXiv:2206.07518",
    "title": "Binary Single-dimensional Convolutional Neural Network for Seizure  Prediction",
    "abstract": "Nowadays, several deep learning methods are proposed to tackle the challenge\nof epileptic seizure prediction. However, these methods still cannot be\nimplemented as part of implantable or efficient wearable devices due to their\nlarge hardware and corresponding high-power consumption. They usually require\ncomplex feature extraction process, large memory for storing high precision\nparameters and complex arithmetic computation, which greatly increases required\nhardware resources. Moreover, available yield poor prediction performance,\nbecause they adopt network architecture directly from image recognition\napplications fails to accurately consider the characteristics of EEG signals.\nWe propose in this paper a hardware-friendly network called Binary\nSingle-dimensional Convolutional Neural Network (BSDCNN) intended for epileptic\nseizure prediction. BSDCNN utilizes 1D convolutional kernels to improve\nprediction performance. All parameters are binarized to reduce the required\ncomputation and storage, except the first layer. Overall area under curve,\nsensitivity, and false prediction rate reaches 0.915, 89.26%, 0.117/h and\n0.970, 94.69%, 0.095/h on American Epilepsy Society Seizure Prediction\nChallenge (AES) dataset and the CHB-MIT one respectively. The proposed\narchitecture outperforms recent works while offering 7.2 and 25.5 times\nreductions on the size of parameter and computation, respectively.",
    "descriptor": "\nComments: 2020 IEEE International Symposium on Circuits and Systems (ISCAS)\n",
    "authors": [
      "Shiqi Zhao",
      "Jie Yang",
      "Yankun Xu",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07518"
  },
  {
    "id": "arXiv:2206.07519",
    "title": "Smart Meter Data Anomaly Detection using Variational Recurrent  Autoencoders with Attention",
    "abstract": "In the digitization of energy systems, sensors and smart meters are\nincreasingly being used to monitor production, operation and demand. Detection\nof anomalies based on smart meter data is crucial to identify potential risks\nand unusual events at an early stage, which can serve as a reference for timely\ninitiation of appropriate actions and improving management. However, smart\nmeter data from energy systems often lack labels and contain noise and various\npatterns without distinctively cyclical. Meanwhile, the vague definition of\nanomalies in different energy scenarios and highly complex temporal\ncorrelations pose a great challenge for anomaly detection. Many traditional\nunsupervised anomaly detection algorithms such as cluster-based or\ndistance-based models are not robust to noise and not fully exploit the\ntemporal dependency in a time series as well as other dependencies amongst\nmultiple variables (sensors). This paper proposes an unsupervised anomaly\ndetection method based on a Variational Recurrent Autoencoder with attention\nmechanism. with \"dirty\" data from smart meters, our method pre-detects missing\nvalues and global anomalies to shrink their contribution while training. This\npaper makes a quantitative comparison with the VAE-based baseline approach and\nfour other unsupervised learning methods, demonstrating its effectiveness and\nsuperiority. This paper further validates the proposed method by a real case\nstudy of detecting the anomalies of water supply temperature from an industrial\nheating plant.",
    "descriptor": "",
    "authors": [
      "Wenjing Dai",
      "Xiufeng Liu",
      "Alfred Heller",
      "Per Sieverts Nielsen"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07519"
  },
  {
    "id": "arXiv:2206.07542",
    "title": "A Deep Generative Model of Neonatal Cortical Surface Development",
    "abstract": "The neonatal cortical surface is known to be affected by preterm birth, and\nthe subsequent changes to cortical organisation have been associated with\npoorer neurodevelopmental outcomes. Deep Generative models have the potential\nto lead to clinically interpretable models of disease, but developing these on\nthe cortical surface is challenging since established techniques for learning\nconvolutional filters are inappropriate on non-flat topologies. To close this\ngap, we implement a surface-based CycleGAN using mixture model CNNs (MoNet) to\ntranslate sphericalised neonatal cortical surface features (curvature and\nT1w/T2w cortical myelin) between different stages of cortical maturity. Results\nshow our method is able to reliably predict changes in individual patterns of\ncortical organisation at later stages of gestation, validated by comparison to\nlongitudinal data; and translate appearance between preterm and term gestation\n(> 37 weeks gestation), validated through comparison with a trained\nterm/preterm classifier. Simulated differences in cortical maturation are\nconsistent with observations in the literature.",
    "descriptor": "",
    "authors": [
      "Abdulah Fawaz",
      "Logan Z. Williams",
      "Emma Robinson",
      "A. David Edwards"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07542"
  },
  {
    "id": "arXiv:2206.07560",
    "title": "Sobolev-Orthogonal Systems with Tridiagonal Skew-Hermitian  Differentiation Matrices",
    "abstract": "We introduce and develop a theory of orthogonality with respect to Sobolev\ninner products on the real line for sequences of functions with a tridiagonal,\nskew-Hermitian differentiation matrix. While a theory of such L2-orthogonal\nsystems is well established, Sobolev orthogonality requires new concepts and\ntheir analysis. We characterise such systems completely as appropriately\nweighed Fourier transforms of orthogonal polynomials and present a number of\nillustrative examples, inclusive of a Sobolev-orthogonal system whose leading N\ncoefficients can be computed in $\\mathcal{O}(N \\log N)$ operations.",
    "descriptor": "",
    "authors": [
      "Arieh Iserles",
      "Marcus Webb"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07560"
  },
  {
    "id": "arXiv:2206.07569",
    "title": "End-to-End Voice Conversion with Information Perturbation",
    "abstract": "The ideal goal of voice conversion is to convert the source speaker's speech\nto sound naturally like the target speaker while maintaining the linguistic\ncontent and the prosody of the source speech. However, current approaches are\ninsufficient to achieve comprehensive source prosody transfer and target\nspeaker timbre preservation in the converted speech, and the quality of the\nconverted speech is also unsatisfied due to the mismatch between the acoustic\nmodel and the vocoder. In this paper, we leverage the recent advances in\ninformation perturbation and propose a fully end-to-end approach to conduct\nhigh-quality voice conversion. We first adopt information perturbation to\nremove speaker-related information in the source speech to disentangle speaker\ntimbre and linguistic content and thus the linguistic information is\nsubsequently modeled by a content encoder. To better transfer the prosody of\nthe source speech to the target, we particularly introduce a speaker-related\npitch encoder which can maintain the general pitch pattern of the source\nspeaker while flexibly modifying the pitch intensity of the generated speech.\nFinally, one-shot voice conversion is set up through continuous speaker space\nmodeling. Experimental results indicate that the proposed end-to-end approach\nsignificantly outperforms the state-of-the-art models in terms of\nintelligibility, naturalness, and speaker similarity.",
    "descriptor": "",
    "authors": [
      "Qicong Xie",
      "Shan Yang",
      "Yi Lei",
      "Lei Xie",
      "Dan Su"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.07569"
  },
  {
    "id": "arXiv:2206.07571",
    "title": "Efficient decoding up to a constant fraction of the code length for  asymptotically good quantum codes",
    "abstract": "We introduce and analyse an efficient decoder for the quantum Tanner codes of\nthat can correct adversarial errors of linear weight. Previous decoders for\nquantum low-density parity-check codes could only handle adversarial errors of\nweight $O(\\sqrt{n \\log n})$. We also work on the link between quantum Tanner\ncodes and the Lifted Product codes of Panteleev and Kalachev, and show that our\ndecoder can be adapted to the latter. The decoding algorithm alternates between\nsequential and parallel procedures and converges in linear time.",
    "descriptor": "\nComments: 41 pages\n",
    "authors": [
      "Anthony Leverrier",
      "Gilles Z\u00e9mor"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07571"
  },
  {
    "id": "arXiv:2206.07588",
    "title": "Characteristic kernels on Hilbert spaces, Banach spaces, and on sets of  measures",
    "abstract": "We present new classes of positive definite kernels on non-standard spaces\nthat are integrally strictly positive definite or characteristic. In\nparticular, we discuss radial kernels on separable Hilbert spaces, and\nintroduce broad classes of kernels on Banach spaces and on metric spaces of\nstrong negative type. The general results are used to give explicit classes of\nkernels on separable $L^p$ spaces and on sets of measures.",
    "descriptor": "",
    "authors": [
      "Johanna Ziegel",
      "David Ginsbourger",
      "Lutz D\u00fcmbgen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.07588"
  },
  {
    "id": "arXiv:2206.07594",
    "title": "Robust and Sparse Estimation of Linear Regression Coefficients with  Heavy-tailed Noises and Covariates",
    "abstract": "Robust and sparse estimation of linear regression coefficients is\ninvestigated. The situation addressed by the present paper is that covariates\nand noises are sampled from heavy-tailed distributions, and the covariates and\nnoises are contaminated by malicious outliers. Our estimator can be computed\nefficiently. Further, our estimation error bound is sharp.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Takeyuki Sasai"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07594"
  },
  {
    "id": "arXiv:2206.07595",
    "title": "BIO-CXRNET: A Robust Multimodal Stacking Machine Learning Technique for  Mortality Risk Prediction of COVID-19 Patients using Chest X-Ray Images and  Clinical Data",
    "abstract": "Fast and accurate detection of the disease can significantly help in reducing\nthe strain on the healthcare facility of any country to reduce the mortality\nduring any pandemic. The goal of this work is to create a multimodal system\nusing a novel machine learning framework that uses both Chest X-ray (CXR)\nimages and clinical data to predict severity in COVID-19 patients. In addition,\nthe study presents a nomogram-based scoring technique for predicting the\nlikelihood of death in high-risk patients. This study uses 25 biomarkers and\nCXR images in predicting the risk in 930 COVID-19 patients admitted during the\nfirst wave of COVID-19 (March-June 2020) in Italy. The proposed multimodal\nstacking technique produced the precision, sensitivity, and F1-score, of\n89.03%, 90.44%, and 89.03%, respectively to identify low or high-risk patients.\nThis multimodal approach improved the accuracy by 6% in comparison to the CXR\nimage or clinical data alone. Finally, nomogram scoring system using\nmultivariate logistic regression -- was used to stratify the mortality risk\namong the high-risk patients identified in the first stage. Lactate\nDehydrogenase (LDH), O2 percentage, White Blood Cells (WBC) Count, Age, and\nC-reactive protein (CRP) were identified as useful predictor using random\nforest feature selection model. Five predictors parameters and a CXR image\nbased nomogram score was developed for quantifying the probability of death and\ncategorizing them into two risk groups: survived (<50%), and death (>=50%),\nrespectively. The multi-modal technique was able to predict the death\nprobability of high-risk patients with an F1 score of 92.88 %. The area under\nthe curves for the development and validation cohorts are 0.981 and 0.939,\nrespectively.",
    "descriptor": "\nComments: 25 pages, 8 Tables, 10 Figures\n",
    "authors": [
      "Tawsifur Rahman",
      "Muhammad E. H. Chowdhury",
      "Amith Khandakar",
      "Zaid Bin Mahbub",
      "Md Sakib Abrar Hossain",
      "Abraham Alhatou",
      "Eynas Abdalla",
      "Sreekumar Muthiyal",
      "Khandaker Farzana Islam",
      "Saad Bin Abul Kashem",
      "Muhammad Salman Khan",
      "Susu M. Zughaier",
      "Maqsud Hossain"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07595"
  },
  {
    "id": "arXiv:2206.07599",
    "title": "How GNNs Facilitate CNNs in Mining Geometric Information from  Large-Scale Medical Images",
    "abstract": "Gigapixel medical images provide massive data, both morphological textures\nand spatial information, to be mined. Due to the large data scale in histology,\ndeep learning methods play an increasingly significant role as feature\nextractors. Existing solutions heavily rely on convolutional neural networks\n(CNNs) for global pixel-level analysis, leaving the underlying local geometric\nstructure such as the interaction between cells in the tumor microenvironment\nunexplored. The topological structure in medical images, as proven to be\nclosely related to tumor evolution, can be well characterized by graphs. To\nobtain a more comprehensive representation for downstream oncology tasks, we\npropose a fusion framework for enhancing the global image-level representation\ncaptured by CNNs with the geometry of cell-level spatial information learned by\ngraph neural networks (GNN). The fusion layer optimizes an integration between\ncollaborative features of global images and cell graphs. Two fusion strategies\nhave been developed: one with MLP which is simple but turns out efficient\nthrough fine-tuning, and the other with Transformer gains a champion in fusing\nmultiple networks. We evaluate our fusion strategies on histology datasets\ncurated from large patient cohorts of colorectal and gastric cancers for three\nbiomarker prediction tasks. Both two models outperform plain CNNs or GNNs,\nreaching a consistent AUC improvement of more than 5% on various network\nbackbones. The experimental results yield the necessity for combining\nimage-level morphological features with cell spatial relations in medical image\nanalysis. Codes are available at https://github.com/yiqings/HEGnnEnhanceCnn.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Yiqing Shen",
      "Bingxin Zhou",
      "Xinye Xiong",
      "Ruitian Gao",
      "Yu Guang Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07599"
  },
  {
    "id": "arXiv:2206.07602",
    "title": "Sparse Subspace Clustering in Diverse Multiplex Network Model",
    "abstract": "The paper considers the DIverse MultiPLEx (DIMPLE) network model, introduced\nin Pensky and Wang (2021), where all layers of the network have the same\ncollection of nodes and are equipped with the Stochastic Block Models. In\naddition, all layers can be partitioned into groups with the same community\nstructures, although the layers in the same group may have different matrices\nof block connection probabilities. The DIMPLE model generalizes a multitude of\npapers that study multilayer networks with the same community structures in all\nlayers, as well as the Mixture Multilayer Stochastic Block Model (MMLSBM),\nwhere the layers in the same group have identical matrices of block connection\nprobabilities. While Pensky and Wang (2021) applied spectral clustering to the\nproxy of the adjacency tensor, the present paper uses Sparse Subspace\nClustering (SSC) for identifying groups of layers with identical community\nstructures. Under mild conditions, the latter leads to the strongly consistent\nbetween-layer clustering. In addition, SSC allows to handle much larger\nnetworks than methodology of Pensky and Wang (2021), and is perfectly suitable\nfor application of parallel computing.",
    "descriptor": "",
    "authors": [
      "Majid Noroozi",
      "Marianna Pensky"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07602"
  },
  {
    "id": "arXiv:2206.07630",
    "title": "Rethinking Initialization of the Sinkhorn Algorithm",
    "abstract": "Computing an optimal transport (OT) coupling between distributions plays an\nincreasingly important role in machine learning. While OT problems can be\nsolved as linear programs, adding an entropic smoothing term is known to result\nin solvers that are faster and more robust to outliers, differentiable and\neasier to parallelize. The Sinkhorn fixed point algorithm is the cornerstone of\nthese approaches, and, as a result, multiple attempts have been made to shorten\nits runtime using, for instance, annealing, momentum or acceleration. The\npremise of this paper is that \\textit{initialization} of the Sinkhorn algorithm\nhas received comparatively little attention, possibly due to two\npreconceptions: as the regularized OT problem is convex, it may not be worth\ncrafting a tailored initialization as \\textit{any} is guaranteed to work;\nsecondly, because the Sinkhorn algorithm is often differentiated in end-to-end\npipelines, data-dependent initializations could potentially bias gradient\nestimates obtained by unrolling iterations. We challenge this conventional\nwisdom and show that carefully chosen initializations can result in dramatic\nspeed-ups, and will not bias gradients which are computed with implicit\ndifferentiation. We detail how initializations can be recovered from\nclosed-form or approximate OT solutions, using known results in the 1D or\nGaussian settings. We show empirically that these initializations can be used\noff-the-shelf, with little to no tuning, and result in consistent speed-ups for\na variety of OT problems.",
    "descriptor": "",
    "authors": [
      "James Thornton",
      "Marco Cuturi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07630"
  },
  {
    "id": "arXiv:2206.07632",
    "title": "Exploring Chemical Space with Score-based Out-of-distribution Generation",
    "abstract": "A well-known limitation of existing works on molecule generation is that the\ngenerated molecules highly resemble those in the training set. To generate\ntruly novel molecules with completely different structures that may have even\nbetter properties than known molecules for de novo drug discovery, more\npowerful exploration in the chemical space is necessary. To this end, we\npropose Molecular Out-Of-distribution Diffusion (MOOD), a novel score-based\ndiffusion scheme that incorporates out-of-distribution (OOD) control in the\ngenerative stochastic differential equation (SDE) with simple control of a\nhyperparameter, thus requires no additional computational costs unlike existing\nmethods (e.g., RL-based methods). However, some novel molecules may be\nchemically implausible, or may not meet the basic requirements of real-world\ndrugs. Thus, MOOD performs conditional generation by utilizing the gradients\nfrom a property prediction network that guides the reverse-time diffusion to\nhigh-scoring regions according to multiple target properties such as\nprotein-ligand interactions, drug-likeness, and synthesizability. This allows\nMOOD to search for novel and meaningful molecules rather than generating unseen\nyet trivial ones. We experimentally validate that MOOD is able to explore the\nchemical space beyond the training distribution, generating molecules that\noutscore ones found with existing methods, and even the top 0.01% of the\noriginal training pool.",
    "descriptor": "",
    "authors": [
      "Seul Lee",
      "Jaehyeong Jo",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07632"
  },
  {
    "id": "arXiv:2206.07638",
    "title": "Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays",
    "abstract": "The existing analysis of asynchronous stochastic gradient descent (SGD)\ndegrades dramatically when any delay is large, giving the impression that\nperformance depends primarily on the delay. On the contrary, we prove much\nbetter guarantees for the same asynchronous SGD algorithm regardless of the\ndelays in the gradients, depending instead just on the number of parallel\ndevices used to implement the algorithm. Our guarantees are strictly better\nthan the existing analyses, and we also argue that asynchronous SGD outperforms\nsynchronous minibatch SGD in the settings we consider. For our analysis, we\nintroduce a novel recursion based on \"virtual iterates\" and delay-adaptive\nstepsizes, which allow us to derive state-of-the-art guarantees for both convex\nand non-convex objectives.",
    "descriptor": "",
    "authors": [
      "Konstantin Mishchenko",
      "Francis Bach",
      "Mathieu Even",
      "Blake Woodworth"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07638"
  },
  {
    "id": "arXiv:2206.07640",
    "title": "Statistical and Computational Phase Transitions in Group Testing",
    "abstract": "We study the group testing problem where the goal is to identify a set of k\ninfected individuals carrying a rare disease within a population of size n,\nbased on the outcomes of pooled tests which return positive whenever there is\nat least one infected individual in the tested group. We consider two different\nsimple random procedures for assigning individuals to tests: the\nconstant-column design and Bernoulli design. Our first set of results concerns\nthe fundamental statistical limits. For the constant-column design, we give a\nnew information-theoretic lower bound which implies that the proportion of\ncorrectly identifiable infected individuals undergoes a sharp \"all-or-nothing\"\nphase transition when the number of tests crosses a particular threshold. For\nthe Bernoulli design, we determine the precise number of tests required to\nsolve the associated detection problem (where the goal is to distinguish\nbetween a group testing instance and pure noise), improving both the upper and\nlower bounds of Truong, Aldridge, and Scarlett (2020). For both group testing\nmodels, we also study the power of computationally efficient (polynomial-time)\ninference procedures. We determine the precise number of tests required for the\nclass of low-degree polynomial algorithms to solve the detection problem. This\nprovides evidence for an inherent computational-statistical gap in both the\ndetection and recovery problems at small sparsity levels. Notably, our evidence\nis contrary to that of Iliopoulos and Zadik (2021), who predicted the absence\nof a computational-statistical gap in the Bernoulli design.",
    "descriptor": "\nComments: Accepted for presentation at the Conference on Learning Theory (COLT) 2022\n",
    "authors": [
      "Amin Coja-Oghlan",
      "Oliver Gebhard",
      "Max Hahn-Klimroth",
      "Alexander S. Wein",
      "Ilias Zadik"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.07640"
  },
  {
    "id": "arXiv:2206.07645",
    "title": "Simplicially driven simple contagion",
    "abstract": "Single contagion processes are known to display a continuous transition from\nan epidemic-free phase at low contagion rates to the epidemic state for rates\nabove a critical threshold. This transition can become discontinuous when two\nsimple contagion processes are coupled in a bi-directional symmetric way.\nHowever, in many cases, the coupling is not symmetric and the processes can be\nof a different nature. For example, social behaviors -- such as hand-washing or\nmask-wearing -- can affect the spread of a disease, and their adoption dynamics\nvia social reinforcement mechanisms are better described by complex contagion\nmodels, rather than by the simple contagion paradigm, which is more appropriate\nfor disease spreading phenomena. Motivated by this example, we consider a\nsimplicial contagion (describing the adoption of a behavior) that\nuni-directionally drives a simple contagion (describing a disease propagation).\nWe show that, above a critical driving strength, such driven simple contagion\ncan exhibit both discontinuous transitions and bi-stability, which are instead\nabsent in standard simple contagions. We provide a mean-field analytical\ndescription of the phase diagram of the system, and complement the results with\nMarkov-chain simulations. Our results provide a novel route for a simple\ncontagion process to display the phenomenology of a higher-order contagion,\nthrough a driving mechanism that may be hidden or unobservable in many\npractical instances.",
    "descriptor": "\nComments: 10 pages, 7 figures. I.I. and M.L. contributed equally to this work. A.B and G.P. jointly supervised this work\n",
    "authors": [
      "Maxime Lucas",
      "Iacopo Iacopini",
      "Thomas Robiglio",
      "Alain Barrat",
      "Giovanni Petri"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07645"
  },
  {
    "id": "arXiv:2206.07648",
    "title": "Classification of ECG based on Hybrid Features using CNNs for Wearable  Applications",
    "abstract": "Sudden cardiac death and arrhythmia account for a large percentage of all\ndeaths worldwide. Electrocardiography (ECG) is the most widely used screening\ntool for cardiovascular diseases. Traditionally, ECG signals are classified\nmanually, requiring experience and great skill, while being time-consuming and\nprone to error. Thus machine learning algorithms have been widely adopted\nbecause of their ability to perform complex data analysis. Features derived\nfrom the points of interest in ECG - mainly Q, R, and S, are widely used for\narrhythmia detection. In this work, we demonstrate improved performance for ECG\nclassification using hybrid features and three different models, building on a\n1-D convolutional neural network (CNN) model that we had proposed in the past.\nAn RR interval features based model proposed in this work achieved an accuracy\nof 98.98%, which is an improvement over the baseline model. To make the model\nimmune to noise, we updated the model using frequency features and achieved\ngood sustained performance in presence of noise with a slightly lower accuracy\nof 98.69%. Further, another model combining the frequency features and the RR\ninterval features was developed, which achieved a high accuracy of 99% with\ngood sustained performance in noisy environments. Due to its high accuracy and\nnoise immunity, the proposed model which combines multiple hybrid features, is\nwell suited for ambulatory wearable sensing applications.",
    "descriptor": "",
    "authors": [
      "Li Xiaolin",
      "Fang Xiang",
      "Rajesh C. Panicker",
      "Barry Cardiff",
      "Deepu John"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07648"
  },
  {
    "id": "arXiv:2206.07649",
    "title": "Atrial Fibrillation Detection Using Weight-Pruned, Log-Quantised  Convolutional Neural Networks",
    "abstract": "Deep neural networks (DNN) are a promising tool in medical applications.\nHowever, the implementation of complex DNNs on battery-powered devices is\nchallenging due to high energy costs for communication. In this work, a\nconvolutional neural network model is developed for detecting atrial\nfibrillation from electrocardiogram (ECG) signals. The model demonstrates high\nperformance despite being trained on limited, variable-length input data.\nWeight pruning and logarithmic quantisation are combined to introduce sparsity\nand reduce model size, which can be exploited for reduced data movement and\nlower computational complexity. The final model achieved a 91.1% model\ncompression ratio while maintaining high model accuracy of 91.7% and less than\n1% loss.",
    "descriptor": "",
    "authors": [
      "Xiu Qi Chang",
      "Ann Feng Chew",
      "Benjamin Chen Ming Choong",
      "Shuhui Wang",
      "Rui Han",
      "Wang He",
      "Li Xiaolin",
      "Rajesh C. Panicker",
      "Deepu John"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07649"
  },
  {
    "id": "arXiv:2206.07650",
    "title": "Flexible Raman Amplifier Optimization Based on Machine Learning-aided  Physical Stimulated Raman Scattering Model",
    "abstract": "The problem of Raman amplifier optimization is studied. A differentiable\ninterpolation function is obtained for the Raman gain coefficient using machine\nlearning (ML), which allows for the gradient descent optimization of\nforward-propagating Raman pumps. Both the frequency and power of an arbitrary\nnumber of pumps in a forward pumping configuration are then optimized for an\narbitrary data channel load and span length. The forward propagation model is\ncombined with an experimentally-trained ML model of a backward-pumping Raman\namplifier to jointly optimize the frequency and power of the forward\namplifier's pumps and the powers of the backward amplifier's pumps. The joint\nforward and backward amplifier optimization is demonstrated for an unrepeatered\ntransmission of 250 km. A gain flatness of $<$ 1~dB over 4 THz is achieved. The\noptimized amplifiers are validated using a numerical simulator.",
    "descriptor": "\nComments: submitted to Journal of Lightwave Technology. Extended version of the previous conference paper M. Yankov, D. Zibar, A. Carena, and F. Da Ros, \"Forward Raman amplifier optimization using machine learning-aided physical modeling,\" accepted, Optoelectronics and Communications Conference (OECC), 2022\n",
    "authors": [
      "Metodi Plamenov Yankov",
      "Francesco Da Ros",
      "Uiara Celine de Moura",
      "Andrea Carena",
      "Darko Zibar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07650"
  },
  {
    "id": "arXiv:2206.07652",
    "title": "Two-stage Human Activity Recognition on Microcontrollers with Decision  Trees and CNNs",
    "abstract": "Human Activity Recognition (HAR) has become an increasingly popular task for\nembedded devices such as smartwatches. Most HAR systems for ultra-low power\ndevices are based on classic Machine Learning (ML) models, whereas Deep\nLearning (DL), although reaching state-of-the-art accuracy, is less popular due\nto its high energy consumption, which poses a significant challenge for\nbattery-operated and resource-constrained devices. In this work, we bridge the\ngap between on-device HAR and DL thanks to a hierarchical architecture composed\nof a decision tree (DT) and a one dimensional Convolutional Neural Network (1D\nCNN). The two classifiers operate in a cascaded fashion on two different\nsub-tasks: the DT classifies only the easiest activities, while the CNN deals\nwith more complex ones. With experiments on a state-of-the-art dataset and\ntargeting a single-core RISC-V MCU, we show that this approach allows to save\nup to 67.7% energy w.r.t. a \"stand-alone\" DL architecture at iso-accuracy.\nAdditionally, the two-stage system either introduces a negligible memory\noverhead (up to 200 B) or on the contrary, reduces the total memory occupation.",
    "descriptor": "\nComments: Accepted as a conference paper at the 2022 IEEE International Conference on Ph. D. Research in Microelectronics and Electronics (PRIME)\n",
    "authors": [
      "Francesco Daghero",
      "Daniele Jahier Pagliari",
      "Massimo Poncino"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07652"
  },
  {
    "id": "arXiv:2206.07654",
    "title": "Human Activity Recognition on Time Series Accelerometer Sensor Data  using LSTM Recurrent Neural Networks",
    "abstract": "The use of sensors available through smart devices has pervaded everyday life\nin several applications including human activity monitoring, healthcare, and\nsocial networks. In this study, we focus on the use of smartwatch accelerometer\nsensors to recognize eating activity. More specifically, we collected sensor\ndata from 10 participants while consuming pizza. Using this information, and\nother comparable data available for similar events such as smoking and\nmedication-taking, and dissimilar activities of jogging, we developed a\nLSTM-ANN architecture that has demonstrated 90% success in identifying\nindividual bites compared to a puff, medication-taking or jogging activities.",
    "descriptor": "\nComments: 8 pages, Accepted for publication at 2022 CSCE Conference (SPRINGER NATURE - Research Book Series)\n",
    "authors": [
      "Chrisogonas O. Odhiambo",
      "Sanjoy Saha",
      "Corby K. Martin",
      "Homayoun Valafar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07654"
  },
  {
    "id": "arXiv:2206.07655",
    "title": "Classification of EEG Motor Imagery Using Deep Learning for  Brain-Computer Interface Systems",
    "abstract": "A trained T1 class Convolutional Neural Network (CNN) model will be used to\nexamine its ability to successfully identify motor imagery when fed\npre-processed electroencephalography (EEG) data. In theory, and if the model\nhas been trained accurately, it should be able to identify a class and label it\naccordingly. The CNN model will then be restored and used to try and identify\nthe same class of motor imagery data using much smaller sampled data in an\nattempt to simulate live data.",
    "descriptor": "",
    "authors": [
      "Alessandro Gallo",
      "Manh Duong Phung"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2206.07655"
  },
  {
    "id": "arXiv:2206.07656",
    "title": "Analysis of Augmentations for Contrastive ECG Representation Learning",
    "abstract": "This paper systematically investigates the effectiveness of various\naugmentations for contrastive self-supervised learning of electrocardiogram\n(ECG) signals and identifies the best parameters. The baseline of our proposed\nself-supervised framework consists of two main parts: the contrastive learning\nand the downstream task. In the first stage, we train an encoder using a number\nof augmentations to extract generalizable ECG signal representations. We then\nfreeze the encoder and finetune a few linear layers with different amounts of\nlabelled data for downstream arrhythmia detection. We then experiment with\nvarious augmentations techniques and explore a range of parameters. Our\nexperiments are done on PTB-XL, a large and publicly available 12-lead ECG\ndataset. The results show that applying augmentations in a specific range of\ncomplexities works better for self-supervised contrastive learning. For\ninstance, when adding Gaussian noise, a sigma in the range of 0.1 to 0.2\nachieves better results, while poor training occurs when the added noise is too\nsmall or too large (outside of the specified range). A similar trend is\nobserved with other augmentations, demonstrating the importance of selecting\nthe optimum level of difficulty for the added augmentations, as augmentations\nthat are too simple will not result in effective training, while augmentations\nthat are too difficult will also prevent the model from effective learning of\ngeneralized representations. Our work can influence future research on\nself-supervised contrastive learning on bio-signals and aid in selecting\noptimum parameters for different augmentations.",
    "descriptor": "\nComments: This paper has been accepted to IJCNN 2022 conference\n",
    "authors": [
      "Sahar Soltanieh",
      "Ali Etemad",
      "Javad Hashemi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07656"
  },
  {
    "id": "arXiv:2206.07664",
    "title": "CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation",
    "abstract": "Accurate uncertainty estimation is a critical need for the medical imaging\ncommunity. A variety of methods have been proposed, all direct extensions of\nclassification uncertainty estimations techniques. The independent pixel-wise\nuncertainty estimates, often based on the probabilistic interpretation of\nneural networks, do not take into account anatomical prior knowledge and\nconsequently provide sub-optimal results to many segmentation tasks. For this\nreason, we propose CRISP a ContRastive Image Segmentation for uncertainty\nPrediction method. At its core, CRISP implements a contrastive method to learn\na joint latent space which encodes a distribution of valid segmentations and\ntheir corresponding images. We use this joint latent space to compare\npredictions to thousands of latent vectors and provide anatomically consistent\nuncertainty maps. Comprehensive studies performed on four medical image\ndatabases involving different modalities and organs underlines the superiority\nof our method compared to state-of-the-art approaches.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Thierry Judge",
      "Olivier Bernard",
      "Mihaela Porumb",
      "Agis Chartsias",
      "Arian Beqiri",
      "Pierre-Marc Jodoin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07664"
  },
  {
    "id": "arXiv:2206.07673",
    "title": "Wide Bayesian neural networks have a simple weight posterior: theory and  accelerated sampling",
    "abstract": "We introduce repriorisation, a data-dependent reparameterisation which\ntransforms a Bayesian neural network (BNN) posterior to a distribution whose KL\ndivergence to the BNN prior vanishes as layer widths grow. The repriorisation\nmap acts directly on parameters, and its analytic simplicity complements the\nknown neural network Gaussian process (NNGP) behaviour of wide BNNs in function\nspace. Exploiting the repriorisation, we develop a Markov chain Monte Carlo\n(MCMC) posterior sampling algorithm which mixes faster the wider the BNN. This\ncontrasts with the typically poor performance of MCMC in high dimensions. We\nobserve up to 50x higher effective sample size relative to no reparametrisation\nfor both fully-connected and residual networks. Improvements are achieved at\nall widths, with the margin between reparametrised and standard BNNs growing\nwith layer width.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Jiri Hron",
      "Roman Novak",
      "Jeffrey Pennington",
      "Jascha Sohl-Dickstein"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07673"
  },
  {
    "id": "arXiv:2206.07679",
    "title": "Beamforming in Integrated Sensing and Communication Systems with  Reconfigurable Intelligent Surfaces",
    "abstract": "We consider transmit beamforming and reflection pattern design in\nreconfigurable intelligent surface (RIS)-assisted integrated sensing and\ncommunication (ISAC) systems to jointly precode communication symbols and radar\nwaveforms. We treat two settings of multiple users and targets. In the first,\nwe use a single RIS to enhance the communication performance of the ISAC system\nand design beams with good cross-correlation properties to match a desired beam\npattern while guaranteeing a desired signal-to-interference plus noise ratio\n(SINR) for each user. In the second setting, we use two dedicated RISs to aid\nthe ISAC system, wherein the beams are designed to maximize the worst-case\ntarget illumination power while guaranteeing a desired SINR for each user. We\npropose solvers based on alternating optimization as the design problems in\nboth cases are non-convex optimization problems. Through a number of numerical\nsimulations, we demonstrate the advantages of RIS-assisted ISAC systems. In\nparticular, we show that the proposed single-RIS assisted ISAC system improves\nthe minimum user SINR while suffering from a moderate loss in radar target\nillumination power. On the other hand, the dual-RIS assisted ISAC system\nimproves both minimum user SINR as well as worst-case target illumination power\nat the targets, especially when the users and targets are not directly visible.",
    "descriptor": "",
    "authors": [
      "R.S. Prasobh Sankar",
      "Sundeep Prabhakar Chepuri",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07679"
  },
  {
    "id": "arXiv:2206.07697",
    "title": "MACE: Higher Order Equivariant Message Passing Neural Networks for Fast  and Accurate Force Fields",
    "abstract": "Creating fast and accurate force fields is a long-standing challenge in\ncomputational chemistry and materials science. Recently, several equivariant\nmessage passing neural networks (MPNNs) have been shown to outperform models\nbuilt using other approaches in terms of accuracy. However, most MPNNs suffer\nfrom high computational cost and poor scalability. We propose that these\nlimitations arise because MPNNs only pass two-body messages leading to a direct\nrelationship between the number of layers and the expressivity of the network.\nIn this work, we introduce MACE, a new equivariant MPNN model that uses higher\nbody order messages. In particular, we show that using four-body messages\nreduces the required number of message passing iterations to just \\emph{two},\nresulting in a fast and highly parallelizable model, reaching or exceeding\nstate-of-the-art accuracy on the rMD17, 3BPA, and AcAc benchmark tasks. We also\ndemonstrate that using higher order messages leads to an improved steepness of\nthe learning curves.",
    "descriptor": "",
    "authors": [
      "Ilyes Batatia",
      "D\u00e1vid P\u00e9ter Kov\u00e1cs",
      "Gregor N. C. Simm",
      "Christoph Ortner",
      "G\u00e1bor Cs\u00e1nyi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07697"
  },
  {
    "id": "arXiv:1706.09443",
    "title": "You Are How You Walk: Uncooperative MoCap Gait Identification for Video  Surveillance with Incomplete and Noisy Data",
    "abstract": "Comments: Preprint. Full paper accepted at the IEEE/IAPR International Joint Conference on Biometrics (IJCB), Denver, Colorado, USA, Oct 2017. 8 pages",
    "descriptor": "\nComments: Preprint. Full paper accepted at the IEEE/IAPR International Joint Conference on Biometrics (IJCB), Denver, Colorado, USA, Oct 2017. 8 pages\n",
    "authors": [
      "Michal Balazia",
      "Petr Sojka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1706.09443"
  },
  {
    "id": "arXiv:1708.04970",
    "title": "Adaptive Threshold Sampling",
    "abstract": "Adaptive Threshold Sampling",
    "descriptor": "",
    "authors": [
      "Daniel Ting"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1708.04970"
  },
  {
    "id": "arXiv:1908.08098",
    "title": "BRIDGE: Byzantine-resilient Decentralized Gradient Descent",
    "abstract": "Comments: 20 pages, 10 figures, 2 tables; some expanded discussion as well as additional numerical experiments using the CIFAR-10 dataset",
    "descriptor": "\nComments: 20 pages, 10 figures, 2 tables; some expanded discussion as well as additional numerical experiments using the CIFAR-10 dataset\n",
    "authors": [
      "Cheng Fang",
      "Zhixiong Yang",
      "Waheed U. Bajwa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/1908.08098"
  },
  {
    "id": "arXiv:1908.09042",
    "title": "SIDLE: Semantically Intelligent Distributed Leader Election Algorithm  for Wireless Sensor Networks",
    "abstract": "Comments: not agreed anymore",
    "descriptor": "\nComments: not agreed anymore\n",
    "authors": [
      "Parsa Rajabzadeh",
      "Amin Pishevar",
      "Hamed Rahimi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/1908.09042"
  },
  {
    "id": "arXiv:1909.10480",
    "title": "FENCE: Feasible Evasion Attacks on Neural Networks in Constrained  Environments",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Alesia Chernikova",
      "Alina Oprea"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1909.10480"
  },
  {
    "id": "arXiv:2004.03637",
    "title": "Probabilistic Spatial Transformer Networks",
    "abstract": "Comments: UAI 2022",
    "descriptor": "\nComments: UAI 2022\n",
    "authors": [
      "Pola Schw\u00f6bel",
      "Frederik Warburg",
      "Martin J\u00f8rgensen",
      "Kristoffer H. Madsen",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.03637"
  },
  {
    "id": "arXiv:2007.04697",
    "title": "Open Data Quality Evaluation: A Comparative Analysis of Open Data in  Latvia",
    "abstract": "Comments: 24 pages, 2 tables, 3 figures, Baltic J. Modern Computing",
    "descriptor": "\nComments: 24 pages, 2 tables, 3 figures, Baltic J. Modern Computing\n",
    "authors": [
      "Anastasija Nikiforova"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Computers and Society (cs.CY)",
      "Information Retrieval (cs.IR)",
      "Applications (stat.AP)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2007.04697"
  },
  {
    "id": "arXiv:2007.05034",
    "title": "The Mean-Squared Error of Double Q-Learning",
    "abstract": "Comments: An earlier verision of this paper appeared in NeurIPS 2020. This verision updated an incorrect equation and several typos",
    "descriptor": "\nComments: An earlier verision of this paper appeared in NeurIPS 2020. This verision updated an incorrect equation and several typos\n",
    "authors": [
      "Wentao Weng",
      "Harsh Gupta",
      "Niao He",
      "Lei Ying",
      "R. Srikant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.05034"
  },
  {
    "id": "arXiv:2007.06540",
    "title": "Open Data Quality",
    "abstract": "Comments: 10 pages, 3 figures, 13th International Baltic Conference on Databases and Information Systems & The Baltic DB&IS 2018 Doctoral Consortium (Baltic DB&IS 2018) At: Lithuania, Trakai, Volume: 2158. arXiv admin note: substantial text overlap with arXiv:2007.04697",
    "descriptor": "\nComments: 10 pages, 3 figures, 13th International Baltic Conference on Databases and Information Systems & The Baltic DB&IS 2018 Doctoral Consortium (Baltic DB&IS 2018) At: Lithuania, Trakai, Volume: 2158. arXiv admin note: substantial text overlap with arXiv:2007.04697\n",
    "authors": [
      "Anastasija Nikiforova"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2007.06540"
  },
  {
    "id": "arXiv:2008.01422",
    "title": "Domain Theory in Constructive and Predicative Univalent Foundations",
    "abstract": "Comments: A shorter version of this paper has appeared in the proceedings of CSL 2021, volume 183 of LIPIcs. doi: 10.4230/LIPIcs.CSL.2021.28. v4: Fixed some typos",
    "descriptor": "\nComments: A shorter version of this paper has appeared in the proceedings of CSL 2021, volume 183 of LIPIcs. doi: 10.4230/LIPIcs.CSL.2021.28. v4: Fixed some typos\n",
    "authors": [
      "Tom de Jong",
      "Mart\u00edn H\u00f6tzel Escard\u00f3"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2008.01422"
  },
  {
    "id": "arXiv:2012.05604",
    "title": "Many-Valued Coalgebraic Modal Logic: One-step Completeness and Finite  Model Property",
    "abstract": "Comments: 23 pages, submitted version",
    "descriptor": "\nComments: 23 pages, submitted version\n",
    "authors": [
      "Chun-Yu Lin",
      "Churn-Jung Liau"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2012.05604"
  },
  {
    "id": "arXiv:2012.06530",
    "title": "Modules over monads and operational semantics (expanded version)",
    "abstract": "Modules over monads and operational semantics (expanded version)",
    "descriptor": "",
    "authors": [
      "Andr\u00e9 Hirschowitz",
      "Tom Hirschowitz",
      "Ambroise Lafont"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2012.06530"
  },
  {
    "id": "arXiv:2101.00612",
    "title": "Evolutionary Mutation-based Fuzzing as Monte Carlo Tree Search",
    "abstract": "Evolutionary Mutation-based Fuzzing as Monte Carlo Tree Search",
    "descriptor": "",
    "authors": [
      "Yiru Zhao",
      "Xiaoke Wang",
      "Lei Zhao",
      "Yueqiang Cheng",
      "Heng Yin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2101.00612"
  },
  {
    "id": "arXiv:2102.09921",
    "title": "Parallel algorithms for power circuits and the word problem of the  Baumslag group",
    "abstract": "Parallel algorithms for power circuits and the word problem of the  Baumslag group",
    "descriptor": "",
    "authors": [
      "Caroline Mattes",
      "Armin Wei\u00df"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2102.09921"
  },
  {
    "id": "arXiv:2103.02174",
    "title": "Dynamic Offloading Design in Time-Varying Mobile Edge Networks with Deep  Reinforcement Learning Approach",
    "abstract": "Dynamic Offloading Design in Time-Varying Mobile Edge Networks with Deep  Reinforcement Learning Approach",
    "descriptor": "",
    "authors": [
      "Liang Yu",
      "Rui Wang",
      "Minyan Shi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2103.02174"
  },
  {
    "id": "arXiv:2103.13424",
    "title": "Quantitative Performance Comparison of Various Traffic Shapers in  Time-Sensitive Networking",
    "abstract": "Quantitative Performance Comparison of Various Traffic Shapers in  Time-Sensitive Networking",
    "descriptor": "",
    "authors": [
      "Luxi Zhao",
      "Paul Pop",
      "Sebastian Steinhorst"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2103.13424"
  },
  {
    "id": "arXiv:2103.16255",
    "title": "Towards Understanding Adversarial Robustness of Optical Flow Networks",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Simon Schrodi",
      "Tonmoy Saikia",
      "Thomas Brox"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.16255"
  },
  {
    "id": "arXiv:2104.09850",
    "title": "A Polyhedral Abstraction for Petri nets and its Application to SMT-Based  Model Checking",
    "abstract": "A Polyhedral Abstraction for Petri nets and its Application to SMT-Based  Model Checking",
    "descriptor": "",
    "authors": [
      "Nicolas Amat",
      "Bernard Berthomieu",
      "Silvano Dal Zilio"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.09850"
  },
  {
    "id": "arXiv:2104.13057",
    "title": "Graphical Modeling for Multi-Source Domain Adaptation",
    "abstract": "Comments: Accepted by TPAMI. Code is available at this https URL",
    "descriptor": "\nComments: Accepted by TPAMI. Code is available at this https URL\n",
    "authors": [
      "Minghao Xu",
      "Hang Wang",
      "Bingbing Ni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.13057"
  },
  {
    "id": "arXiv:2105.05784",
    "title": "Particle-Based Assembly Using Precise Global Control",
    "abstract": "Comments: 25 pages, 14 figures, full version of an extended abstract that appeared in the proceedings of the 17th Algorithms and Data Structures Symposium (WADS 2021); revised version with clearer model/problem description and some additional related work",
    "descriptor": "\nComments: 25 pages, 14 figures, full version of an extended abstract that appeared in the proceedings of the 17th Algorithms and Data Structures Symposium (WADS 2021); revised version with clearer model/problem description and some additional related work\n",
    "authors": [
      "Jakob Keller",
      "Christian Rieck",
      "Christian Scheffer",
      "Arne Schmidt"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2105.05784"
  },
  {
    "id": "arXiv:2105.12697",
    "title": "Can Linear Programs Have Adversarial Examples? A Causal Perspective",
    "abstract": "Comments: Main paper: 9 pages, References: 2 page, Supplement: 2 pages. Main paper: 2 figures, 3 tables, Supplement: 1 figure, 1 table",
    "descriptor": "\nComments: Main paper: 9 pages, References: 2 page, Supplement: 2 pages. Main paper: 2 figures, 3 tables, Supplement: 1 figure, 1 table\n",
    "authors": [
      "Matej Ze\u010devi\u0107",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.12697"
  },
  {
    "id": "arXiv:2105.14403",
    "title": "Re-evaluating Word Mover's Distance",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Ryoma Sato",
      "Makoto Yamada",
      "Hisashi Kashima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2105.14403"
  },
  {
    "id": "arXiv:2106.03180",
    "title": "Vision Transformers with Hierarchical Attention",
    "abstract": "Vision Transformers with Hierarchical Attention",
    "descriptor": "",
    "authors": [
      "Yun Liu",
      "Yu-Huan Wu",
      "Guolei Sun",
      "Le Zhang",
      "Ajad Chhatkuli",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.03180"
  },
  {
    "id": "arXiv:2106.04486",
    "title": "Sketch-Based Anomaly Detection in Streaming Graphs",
    "abstract": "Sketch-Based Anomaly Detection in Streaming Graphs",
    "descriptor": "",
    "authors": [
      "Siddharth Bhatia",
      "Mohit Wadhwa",
      "Kenji Kawaguchi",
      "Neil Shah",
      "Philip S. Yu",
      "Bryan Hooi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04486"
  },
  {
    "id": "arXiv:2106.11071",
    "title": "Making Sense of Moodle Log Data",
    "abstract": "Comments: The paper has been split in two parts and totally renewed with improved solutions",
    "descriptor": "\nComments: The paper has been split in two parts and totally renewed with improved solutions\n",
    "authors": [
      "Daniela Rotelli",
      "Anna Monreale"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2106.11071"
  },
  {
    "id": "arXiv:2106.12723",
    "title": "Meaningfully Debugging Model Mistakes using Conceptual Counterfactual  Explanations",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Abubakar Abid",
      "Mert Yuksekgonul",
      "James Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.12723"
  },
  {
    "id": "arXiv:2107.00962",
    "title": "Brain over Brawn: Using a Stereo Camera to Detect, Track, and Intercept  a Faster UAV by Reconstructing the Intruder's Trajectory",
    "abstract": "Comments: Published in journal Field Robotics, March 2022. UAV-Eagle dataset available at: this https URL",
    "descriptor": "\nComments: Published in journal Field Robotics, March 2022. UAV-Eagle dataset available at: this https URL\n",
    "authors": [
      "Antonella Barisic",
      "Frano Petric",
      "Stjepan Bogdan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.00962"
  },
  {
    "id": "arXiv:2107.08676",
    "title": "Influence of a Set of Variables on a Boolean Function",
    "abstract": "Influence of a Set of Variables on a Boolean Function",
    "descriptor": "",
    "authors": [
      "Aniruddha Biswas",
      "Palash Sarkar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2107.08676"
  },
  {
    "id": "arXiv:2107.09766",
    "title": "Learning Heuristics for Template-based CEGIS of Loop Invariants with  Reinforcement Learning",
    "abstract": "Learning Heuristics for Template-based CEGIS of Loop Invariants with  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Minchao Wu",
      "Takeshi Tsukada",
      "Hiroshi Unno",
      "Taro Sekiyama",
      "Kohei Suenaga"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2107.09766"
  },
  {
    "id": "arXiv:2107.11156",
    "title": "Training a neural network with exciton-polariton optical nonlinearity",
    "abstract": "Training a neural network with exciton-polariton optical nonlinearity",
    "descriptor": "",
    "authors": [
      "Andrzej Opala",
      "Riccardo Panico",
      "Vincenzo Ardizzone",
      "Barbara Pietka",
      "Jacek Szczytko",
      "Daniele Sanvitto",
      "Micha\u0142 Matuszewski",
      "Dario Ballarini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Quantum Gases (cond-mat.quant-gas)"
    ],
    "url": "https://arxiv.org/abs/2107.11156"
  },
  {
    "id": "arXiv:2107.12211",
    "title": "A General Theory for Client Sampling in Federated Learning",
    "abstract": "A General Theory for Client Sampling in Federated Learning",
    "descriptor": "",
    "authors": [
      "Yann Fraboni",
      "Richard Vidal",
      "Laetitia Kameni",
      "Marco Lorenzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2107.12211"
  },
  {
    "id": "arXiv:2108.00774",
    "title": "A Random Matrix Perspective on Random Tensors",
    "abstract": "A Random Matrix Perspective on Random Tensors",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 Henrique de Morais Goulart",
      "Romain Couillet",
      "Pierre Comon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2108.00774"
  },
  {
    "id": "arXiv:2108.03880",
    "title": "NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis",
    "abstract": "Comments: Accepted for International Joint Conference on Neural Networks (IJCNN) 2022. Code available at this https URL",
    "descriptor": "\nComments: Accepted for International Joint Conference on Neural Networks (IJCNN) 2022. Code available at this https URL\n",
    "authors": [
      "Radu Alexandru Rosu",
      "Sven Behnke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.03880"
  },
  {
    "id": "arXiv:2108.04416",
    "title": "Improved Parallel Algorithm for Minimum Cost Submodular Cover Problem",
    "abstract": "Comments: Our paper has been accepted to 35th Annual Conference on Learning Theory",
    "descriptor": "\nComments: Our paper has been accepted to 35th Annual Conference on Learning Theory\n",
    "authors": [
      "Yingli Ran",
      "Zhao Zhang",
      "Shaojie Tang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2108.04416"
  },
  {
    "id": "arXiv:2108.05928",
    "title": "Data-driven discovery of intrinsic dynamics",
    "abstract": "Comments: 27 pages + 15 pages Supplementary Information",
    "descriptor": "\nComments: 27 pages + 15 pages Supplementary Information\n",
    "authors": [
      "Daniel Floryan",
      "Michael D. Graham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.05928"
  },
  {
    "id": "arXiv:2108.08253",
    "title": "Learned holographic light transport",
    "abstract": "Comments: 11 pages. Corrected a typo in equation 3",
    "descriptor": "\nComments: 11 pages. Corrected a typo in equation 3\n",
    "authors": [
      "Koray Kavakl\u0131",
      "Hakan Urey",
      "Kaan Ak\u015fit"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.08253"
  },
  {
    "id": "arXiv:2108.08842",
    "title": "EDEN: Communication-Efficient and Robust Distributed Mean Estimation for  Federated Learning",
    "abstract": "Comments: To appear in ICML 2022",
    "descriptor": "\nComments: To appear in ICML 2022\n",
    "authors": [
      "Shay Vargaftik",
      "Ran Ben Basat",
      "Amit Portnoy",
      "Gal Mendelson",
      "Yaniv Ben-Itzhak",
      "Michael Mitzenmacher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2108.08842"
  },
  {
    "id": "arXiv:2108.09375",
    "title": "Cascade Watchdog: A Multi-tiered Adversarial Guard for Outlier Detection",
    "abstract": "Cascade Watchdog: A Multi-tiered Adversarial Guard for Outlier Detection",
    "descriptor": "",
    "authors": [
      "Glauco Amigo",
      "Justin M. Bui",
      "Charles Baylis",
      "Robert J. Marks"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.09375"
  },
  {
    "id": "arXiv:2108.11145",
    "title": "Dynamic DV-QKD Networking in Fully-Meshed Software-Defined Optical  Networks",
    "abstract": "Comments: 9 pages, 5 figures, submitted to IEEE Journal of Lightwave Technology",
    "descriptor": "\nComments: 9 pages, 5 figures, submitted to IEEE Journal of Lightwave Technology\n",
    "authors": [
      "Obada Alia",
      "Rodrigo Stange Tessinari",
      "Emilio Hugues-Salas",
      "George T. Kanellos",
      "Reza Nejabati",
      "Dimitra Simeonidou"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2108.11145"
  },
  {
    "id": "arXiv:2108.11211",
    "title": "Clustering acoustic emission data streams with sequentially appearing  clusters using mixture models",
    "abstract": "Clustering acoustic emission data streams with sequentially appearing  clusters using mixture models",
    "descriptor": "",
    "authors": [
      "Emmanuel Ramasso",
      "Thierry Denoeux",
      "Gael Chevallier"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2108.11211"
  },
  {
    "id": "arXiv:2108.13063",
    "title": "Satisfiability and Containment of Recursive SHACL",
    "abstract": "Satisfiability and Containment of Recursive SHACL",
    "descriptor": "",
    "authors": [
      "Paolo Pareti",
      "George Konstantinidis",
      "Fabio Mogavero"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.13063"
  },
  {
    "id": "arXiv:2109.01656",
    "title": "Thompson Sampling for Bandits with Clustered Arms",
    "abstract": "Comments: IJCAI-2021. The supplementary material is not part of the IJCAI-21 Proceedings",
    "descriptor": "\nComments: IJCAI-2021. The supplementary material is not part of the IJCAI-21 Proceedings\n",
    "authors": [
      "Emil Carlsson",
      "Devdatt Dubhashi",
      "Fredrik D. Johansson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.01656"
  },
  {
    "id": "arXiv:2109.03508",
    "title": "RepNAS: Searching for Efficient Re-parameterizing Blocks",
    "abstract": "RepNAS: Searching for Efficient Re-parameterizing Blocks",
    "descriptor": "",
    "authors": [
      "Mingyang Zhang",
      "Xinyi Yu",
      "Jingtao Rong",
      "Linlin Ou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2109.03508"
  },
  {
    "id": "arXiv:2109.04352",
    "title": "PhysGNN: A Physics-Driven Graph Neural Network Based Model for  Predicting Soft Tissue Deformation in Image-Guided Neurosurgery",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Yasmin Salehi",
      "Dennis Giannacopoulos"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04352"
  },
  {
    "id": "arXiv:2109.04572",
    "title": "Deciphering Environmental Air Pollution with Large Scale City Data",
    "abstract": "Comments: Accepted as a Oral Spotlight Paper at International Joint Conference of Artificial Intelligence (IJCAI) 2022",
    "descriptor": "\nComments: Accepted as a Oral Spotlight Paper at International Joint Conference of Artificial Intelligence (IJCAI) 2022\n",
    "authors": [
      "Mayukh Bhattacharyya",
      "Sayan Nag",
      "Udita Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2109.04572"
  },
  {
    "id": "arXiv:2109.10619",
    "title": "Eliciting Thinking Hierarchy without Prior",
    "abstract": "Eliciting Thinking Hierarchy without Prior",
    "descriptor": "",
    "authors": [
      "Yuqing Kong",
      "Yunqi Li",
      "Yubo Zhang",
      "Zhihuan Huang",
      "Jinzhao Wu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2109.10619"
  },
  {
    "id": "arXiv:2109.12399",
    "title": "Self-Enhancing Multi-filter Sequence-to-Sequence Model",
    "abstract": "Comments: 5 content pages, 4 figures, and 3 tables",
    "descriptor": "\nComments: 5 content pages, 4 figures, and 3 tables\n",
    "authors": [
      "Yunhao Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.12399"
  },
  {
    "id": "arXiv:2109.13096",
    "title": "Learning Transport Processes with Machine Intelligence",
    "abstract": "Learning Transport Processes with Machine Intelligence",
    "descriptor": "",
    "authors": [
      "Francesco Miniati",
      "Gianluca Gregori"
    ],
    "subjectives": [
      "Plasma Physics (physics.plasm-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.13096"
  },
  {
    "id": "arXiv:2109.13346",
    "title": "Quantum Computational Phase Transition in Combinatorial Problems",
    "abstract": "Comments: 14 pages, 12 figures",
    "descriptor": "\nComments: 14 pages, 12 figures\n",
    "authors": [
      "Bingzhi Zhang",
      "Akira Sone",
      "Quntao Zhuang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2109.13346"
  },
  {
    "id": "arXiv:2110.01343",
    "title": "Taming singular stochastic differential equations: A numerical method",
    "abstract": "Comments: 69 pages. A new section is added: numerical solution to stochastic transport equation",
    "descriptor": "\nComments: 69 pages. A new section is added: numerical solution to stochastic transport equation\n",
    "authors": [
      "Khoa L\u00ea",
      "Chengcheng Ling"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.01343"
  },
  {
    "id": "arXiv:2110.02395",
    "title": "XAI Establishes a Common Ground Between Machine Learning and Causality",
    "abstract": "Comments: Main paper: 9 pages, References: 2.5 pages, Supplement: 17 pages. Main paper: 5 figures, 1 table, Supplement: 12 figures, 2 tables",
    "descriptor": "\nComments: Main paper: 9 pages, References: 2.5 pages, Supplement: 17 pages. Main paper: 5 figures, 1 table, Supplement: 12 figures, 2 tables\n",
    "authors": [
      "Matej Ze\u010devi\u0107",
      "Devendra Singh Dhami",
      "Constantin A. Rothkopf",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02395"
  },
  {
    "id": "arXiv:2110.02453",
    "title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity",
    "abstract": "Comments: 19 pages, 2 figures, ICML 2022 camera ready",
    "descriptor": "\nComments: 19 pages, 2 figures, ICML 2022 camera ready\n",
    "authors": [
      "Lin Zheng",
      "Huijie Pan",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02453"
  },
  {
    "id": "arXiv:2110.02911",
    "title": "Shifting Capsule Networks from the Cloud to the Deep Edge",
    "abstract": "Shifting Capsule Networks from the Cloud to the Deep Edge",
    "descriptor": "",
    "authors": [
      "Miguel Costa",
      "Diogo Costa",
      "Tiago Gomes",
      "Sandro Pinto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.02911"
  },
  {
    "id": "arXiv:2110.03031",
    "title": "RieszNet and ForestRiesz: Automatic Debiased Machine Learning with  Neural Nets and Random Forests",
    "abstract": "Comments: Accepted for a long presentation at the ICML. Code available at this https URL",
    "descriptor": "\nComments: Accepted for a long presentation at the ICML. Code available at this https URL\n",
    "authors": [
      "Victor Chernozhukov",
      "Whitney K. Newey",
      "Victor Quintas-Martinez",
      "Vasilis Syrgkanis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03031"
  },
  {
    "id": "arXiv:2110.03054",
    "title": "On the Privacy Risks of Deploying Recurrent Neural Networks in Machine  Learning Models",
    "abstract": "Comments: Under Double-Blind Review",
    "descriptor": "\nComments: Under Double-Blind Review\n",
    "authors": [
      "Yunhao Yang",
      "Parham Gohari",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03054"
  },
  {
    "id": "arXiv:2110.03299",
    "title": "End-To-End Label Uncertainty Modeling for Speech-based Arousal  Recognition Using Bayesian Neural Networks",
    "abstract": "Comments: ACCEPTED to INTERSPEECH 2022",
    "descriptor": "\nComments: ACCEPTED to INTERSPEECH 2022\n",
    "authors": [
      "Navin Raj Prabhu",
      "Guillaume Carbajal",
      "Nale Lehmann-Willenbrock",
      "Timo Gerkmann"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03299"
  },
  {
    "id": "arXiv:2110.03600",
    "title": "Fair distributions for more participants than allocations",
    "abstract": "Comments: 10 pages, 2 figures",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Pablo Sober\u00f3n"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.03600"
  },
  {
    "id": "arXiv:2110.06988",
    "title": "Spectral Convergence of Symmetrized Graph Laplacian on manifolds with  boundary",
    "abstract": "Comments: 6 figures",
    "descriptor": "\nComments: 6 figures\n",
    "authors": [
      "J. Wilson Peoples",
      "John Harlim"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.06988"
  },
  {
    "id": "arXiv:2110.07323",
    "title": "Multidisciplinary Design Optimization Approach to Integrated Space  Mission Planning and Spacecraft Design",
    "abstract": "Comments: Accepted to the Journal of Spacecraft and Rockets",
    "descriptor": "\nComments: Accepted to the Journal of Spacecraft and Rockets\n",
    "authors": [
      "Masafumi Isaji",
      "Yuji Takubo",
      "Koki Ho"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07323"
  },
  {
    "id": "arXiv:2110.08420",
    "title": "Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information",
    "abstract": "Comments: ICML 2022 (long talk)",
    "descriptor": "\nComments: ICML 2022 (long talk)\n",
    "authors": [
      "Kawin Ethayarajh",
      "Yejin Choi",
      "Swabha Swayamdipta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08420"
  },
  {
    "id": "arXiv:2110.09455",
    "title": "TLDR: Twin Learning for Dimensionality Reduction",
    "abstract": "Comments: Accepted at Transactions on Machine Learning Research (TMLR). Code available at: this https URL",
    "descriptor": "\nComments: Accepted at Transactions on Machine Learning Research (TMLR). Code available at: this https URL\n",
    "authors": [
      "Yannis Kalantidis",
      "Carlos Lassance",
      "Jon Almazan",
      "Diane Larlus"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.09455"
  },
  {
    "id": "arXiv:2110.10017",
    "title": "Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm",
    "abstract": "Comments: This paper has been accepted for presentation at the IJCNN at IEEE WCCI 2022 and for publication in the conference proceedings published by IEEE",
    "descriptor": "\nComments: This paper has been accepted for presentation at the IJCNN at IEEE WCCI 2022 and for publication in the conference proceedings published by IEEE\n",
    "authors": [
      "Raghuram Bharadwaj Diddigi",
      "Prateek Jain",
      "Prabuchandran K.J.",
      "Shalabh Bhatnagar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.10017"
  },
  {
    "id": "arXiv:2110.10026",
    "title": "Private Language Model Adaptation for Speech Recognition",
    "abstract": "Private Language Model Adaptation for Speech Recognition",
    "descriptor": "",
    "authors": [
      "Zhe Liu",
      "Ke Li",
      "Shreyan Bakshi",
      "Fuchun Peng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.10026"
  },
  {
    "id": "arXiv:2110.13549",
    "title": "Online Variational Filtering and Parameter Learning",
    "abstract": "Comments: 27 pages, 6 figures. NeurIPS 2021 (Oral); updated references",
    "descriptor": "\nComments: 27 pages, 6 figures. NeurIPS 2021 (Oral); updated references\n",
    "authors": [
      "Andrew Campbell",
      "Yuyang Shi",
      "Tom Rainforth",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.13549"
  },
  {
    "id": "arXiv:2111.03792",
    "title": "Roofline Model for UAVs:A Bottleneck Analysis Tool for Designing Compute  Systems for Autonomous Drones",
    "abstract": "Comments: The latest and updated version with conference is available here: arXiv:2204.10898",
    "descriptor": "\nComments: The latest and updated version with conference is available here: arXiv:2204.10898\n",
    "authors": [
      "Srivatsan Krishnan",
      "Zishen Wan",
      "Kshitij Bhardwaj",
      "Aleksandra Faust",
      "Vijay Janapa Reddi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2111.03792"
  },
  {
    "id": "arXiv:2111.07964",
    "title": "Deep Network Approximation in Terms of Intrinsic Parameters",
    "abstract": "Deep Network Approximation in Terms of Intrinsic Parameters",
    "descriptor": "",
    "authors": [
      "Zuowei Shen",
      "Haizhao Yang",
      "Shijun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.07964"
  },
  {
    "id": "arXiv:2111.09631",
    "title": "Neural Network Kalman filtering for 3D object tracking from linear array  ultrasound data",
    "abstract": "Comments: 13 pages, 8 figures",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Arttu Arjas",
      "Erwin J. Alles",
      "Efthymios Maneas",
      "Simon Arridge",
      "Adrien Desjardins",
      "Mikko J. Sillanp\u00e4\u00e4",
      "Andreas Hauptmann"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.09631"
  },
  {
    "id": "arXiv:2111.12553",
    "title": "CycleQ: An Efficient Basis for Cyclic Equational Reasoning",
    "abstract": "Comments: Version accepted to PLDI'22",
    "descriptor": "\nComments: Version accepted to PLDI'22\n",
    "authors": [
      "Eddie Jones",
      "C-.H. Luke Ong",
      "Steven Ramsay"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2111.12553"
  },
  {
    "id": "arXiv:2111.13336",
    "title": "MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for  Efficient Object Detection",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Zhenhong Sun",
      "Ming Lin",
      "Xiuyu Sun",
      "Zhiyu Tan",
      "Hao Li",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.13336"
  },
  {
    "id": "arXiv:2111.14250",
    "title": "Learning a model of shape selectivity in V4 cells reveals shape encoding  mechanisms in the brain",
    "abstract": "Comments: 20 pages, 7 figures",
    "descriptor": "\nComments: 20 pages, 7 figures\n",
    "authors": [
      "Paria Mehrani",
      "John K. Tsotsos"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.14250"
  },
  {
    "id": "arXiv:2112.03605",
    "title": "Some Basic Techniques allowing Petri Net Synthesis: Complexity and  Algorithmic Issues",
    "abstract": "Some Basic Techniques allowing Petri Net Synthesis: Complexity and  Algorithmic Issues",
    "descriptor": "",
    "authors": [
      "Raymond Devillers",
      "Ronny Tredup"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.03605"
  },
  {
    "id": "arXiv:2112.05307",
    "title": "Are We There Yet? Timing and Floating-Point Attacks on Differential  Privacy Systems",
    "abstract": "Comments: In Proceedings of the 43rd IEEE Symposium on Security and Privacy (IEEE S&P 2022)",
    "descriptor": "\nComments: In Proceedings of the 43rd IEEE Symposium on Security and Privacy (IEEE S&P 2022)\n",
    "authors": [
      "Jiankai Jin",
      "Eleanor McMurtry",
      "Benjamin I. P. Rubinstein",
      "Olga Ohrimenko"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.05307"
  },
  {
    "id": "arXiv:2112.05537",
    "title": "From Modular Decomposition Trees to Level-1 Networks: Pseudo-Cographs,  Polar-Cats and Prime Polar-Cats",
    "abstract": "From Modular Decomposition Trees to Level-1 Networks: Pseudo-Cographs,  Polar-Cats and Prime Polar-Cats",
    "descriptor": "",
    "authors": [
      "Marc Hellmuth",
      "Guillaume E. Scholz"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2112.05537"
  },
  {
    "id": "arXiv:2112.07055",
    "title": "Large Language Models are not Models of Natural Language: they are  Corpus Models",
    "abstract": "Large Language Models are not Models of Natural Language: they are  Corpus Models",
    "descriptor": "",
    "authors": [
      "Csaba Veres"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.07055"
  },
  {
    "id": "arXiv:2112.08884",
    "title": "Skeleton Abstraction for Universal Temporal Properties",
    "abstract": "Skeleton Abstraction for Universal Temporal Properties",
    "descriptor": "",
    "authors": [
      "Sophie Wallner",
      "Karsten Wolf"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2112.08884"
  },
  {
    "id": "arXiv:2112.09036",
    "title": "The Dual PC Algorithm for Structure Learning",
    "abstract": "The Dual PC Algorithm for Structure Learning",
    "descriptor": "",
    "authors": [
      "Enrico Giudice",
      "Jack Kuipers",
      "Giusi Moffa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.09036"
  },
  {
    "id": "arXiv:2112.11896",
    "title": "The Grassl-R\u00f6tteler cyclic and consta-cyclic MDS codes are generalised  Reed-Solomon codes",
    "abstract": "The Grassl-R\u00f6tteler cyclic and consta-cyclic MDS codes are generalised  Reed-Solomon codes",
    "descriptor": "",
    "authors": [
      "Simeon Ball"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.11896"
  },
  {
    "id": "arXiv:2112.15439",
    "title": "Facial-Sketch Synthesis: A New Challenge",
    "abstract": "Comments: Accepted to Machine Intelligence Research (MIR)",
    "descriptor": "\nComments: Accepted to Machine Intelligence Research (MIR)\n",
    "authors": [
      "Deng-Ping Fan",
      "Ziling Huang",
      "Peng Zheng",
      "Hong Liu",
      "Xuebin Qin",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.15439"
  },
  {
    "id": "arXiv:2201.00233",
    "title": "A new criterion for $\\mathcal{M}, \\mathcal{N}$-adhesivity, with an  application to hierarchical graphs",
    "abstract": "A new criterion for $\\mathcal{M}, \\mathcal{N}$-adhesivity, with an  application to hierarchical graphs",
    "descriptor": "",
    "authors": [
      "Davide Castelnovo",
      "Fabio Gadducci",
      "Marino Miculan"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2201.00233"
  },
  {
    "id": "arXiv:2201.01029",
    "title": "Weakly-supervised continual learning for class-incremental segmentation",
    "abstract": "Weakly-supervised continual learning for class-incremental segmentation",
    "descriptor": "",
    "authors": [
      "Gaston Lenczner",
      "Adrien Chan-Hon-Tong",
      "Nicola Luminari",
      "Bertrand Le Saux"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.01029"
  },
  {
    "id": "arXiv:2201.02017",
    "title": "Enhancing Egocentric 3D Pose Estimation with Third Person Views",
    "abstract": "Enhancing Egocentric 3D Pose Estimation with Third Person Views",
    "descriptor": "",
    "authors": [
      "Ameya Dhamanaskar",
      "Mariella Dimiccoli",
      "Enric Corona",
      "Albert Pumarola",
      "Francesc Moreno-Noguer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.02017"
  },
  {
    "id": "arXiv:2201.04127",
    "title": "HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular  Video",
    "abstract": "Comments: CVPR 2022 (oral). Project page with videos: this https URL",
    "descriptor": "\nComments: CVPR 2022 (oral). Project page with videos: this https URL\n",
    "authors": [
      "Chung-Yi Weng",
      "Brian Curless",
      "Pratul P. Srinivasan",
      "Jonathan T. Barron",
      "Ira Kemelmacher-Shlizerman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.04127"
  },
  {
    "id": "arXiv:2201.04387",
    "title": "Maximizing Self-supervision from Thermal Image for Effective  Self-supervised Learning of Depth and Ego-motion",
    "abstract": "Comments: 8 pages, Accepted by IEEE Robotics and Automation Letters (RA-L) with IROS 2022 option",
    "descriptor": "\nComments: 8 pages, Accepted by IEEE Robotics and Automation Letters (RA-L) with IROS 2022 option\n",
    "authors": [
      "Ukcheol Shin",
      "Kyunghyun Lee",
      "Byeong-Uk Lee",
      "In So Kweon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.04387"
  },
  {
    "id": "arXiv:2201.07098",
    "title": "Compatibility and accessibility: lattice representations for semantics  of non-classical and modal logics",
    "abstract": "Comments: Final version forthcoming in Advances in Modal Logic, Vol. 14",
    "descriptor": "\nComments: Final version forthcoming in Advances in Modal Logic, Vol. 14\n",
    "authors": [
      "Wesley H. Holliday"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2201.07098"
  },
  {
    "id": "arXiv:2201.07877",
    "title": "PDE-Based Optimal Strategy for Unconstrained Online Learning",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Zhiyu Zhang",
      "Ashok Cutkosky",
      "Ioannis Paschalidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.07877"
  },
  {
    "id": "arXiv:2201.11137",
    "title": "Born-Infeld (BI) for AI: Energy-Conserving Descent (ECD) for  Optimization",
    "abstract": "Comments: ICML 2022. 9 pages + Appendix, 8 figures. Code available at this https URL",
    "descriptor": "\nComments: ICML 2022. 9 pages + Appendix, 8 figures. Code available at this https URL\n",
    "authors": [
      "G. Bruno De Luca",
      "Eva Silverstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "High Energy Physics - Theory (hep-th)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.11137"
  },
  {
    "id": "arXiv:2201.11927",
    "title": "Constrained Variational Policy Optimization for Safe Reinforcement  Learning",
    "abstract": "Comments: ICML 2022. 25 pages",
    "descriptor": "\nComments: ICML 2022. 25 pages\n",
    "authors": [
      "Zuxin Liu",
      "Zhepeng Cen",
      "Vladislav Isenbaev",
      "Wei Liu",
      "Zhiwei Steven Wu",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.11927"
  },
  {
    "id": "arXiv:2201.12078",
    "title": "You Only Cut Once: Boosting Data Augmentation with a Single Cut",
    "abstract": "Comments: ICML 2022, Code: this https URL",
    "descriptor": "\nComments: ICML 2022, Code: this https URL\n",
    "authors": [
      "Junlin Han",
      "Pengfei Fang",
      "Weihao Li",
      "Jie Hong",
      "Mohammad Ali Armin",
      "Ian Reid",
      "Lars Petersson",
      "Hongdong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12078"
  },
  {
    "id": "arXiv:2201.12288",
    "title": "VRT: A Video Restoration Transformer",
    "abstract": "Comments: add results on VFI and STVSR; SOTA results (+up to 2.16dB) on video SR, video deblurring, video denoising, video frame interpolation and space-time video super-resolution. Code: this https URL",
    "descriptor": "\nComments: add results on VFI and STVSR; SOTA results (+up to 2.16dB) on video SR, video deblurring, video denoising, video frame interpolation and space-time video super-resolution. Code: this https URL\n",
    "authors": [
      "Jingyun Liang",
      "Jiezhang Cao",
      "Yuchen Fan",
      "Kai Zhang",
      "Rakesh Ranjan",
      "Yawei Li",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2201.12288"
  },
  {
    "id": "arXiv:2201.12733",
    "title": "TPC: Transformation-Specific Smoothing for Point Cloud Models",
    "abstract": "Comments: Accepted as a conference paper at ICML 2022",
    "descriptor": "\nComments: Accepted as a conference paper at ICML 2022\n",
    "authors": [
      "Wenda Chu",
      "Linyi Li",
      "Bo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12733"
  },
  {
    "id": "arXiv:2201.13055",
    "title": "Nystr\u00f6m Kernel Mean Embeddings",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Antoine Chatalic",
      "Nicolas Schreuder",
      "Alessandro Rudi",
      "Lorenzo Rosasco"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13055"
  },
  {
    "id": "arXiv:2202.00211",
    "title": "GNNRank: Learning Global Rankings from Pairwise Comparisons via Directed  Graph Neural Networks",
    "abstract": "Comments: ICML 2022 spotlight; 32 pages (9 pages for main text)",
    "descriptor": "\nComments: ICML 2022 spotlight; 32 pages (9 pages for main text)\n",
    "authors": [
      "Yixuan He",
      "Quan Gan",
      "David Wipf",
      "Gesine Reinert",
      "Junchi Yan",
      "Mihai Cucuringu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00211"
  },
  {
    "id": "arXiv:2202.00948",
    "title": "Eikonal Fields for Refractive Novel-View Synthesis",
    "abstract": "Comments: 8 pages, 6 figures, project webpage: this https URL",
    "descriptor": "\nComments: 8 pages, 6 figures, project webpage: this https URL\n",
    "authors": [
      "Mojtaba Bemana",
      "Karol Myszkowski",
      "Jeppe Revall Frisvad",
      "Hans-Peter Seidel",
      "Tobias Ritschel"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.00948"
  },
  {
    "id": "arXiv:2202.01627",
    "title": "Non-Vacuous Generalisation Bounds for Shallow Neural Networks",
    "abstract": "Comments: 19 pages, 12 figures",
    "descriptor": "\nComments: 19 pages, 12 figures\n",
    "authors": [
      "Felix Biggs",
      "Benjamin Guedj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.01627"
  },
  {
    "id": "arXiv:2202.02514",
    "title": "Score-based Generative Modeling of Graphs via the System of Stochastic  Differential Equations",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Jaehyeong Jo",
      "Seul Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02514"
  },
  {
    "id": "arXiv:2202.03024",
    "title": "The Input and Output Entropies of the $k$-Deletion/Insertion Channel",
    "abstract": "The Input and Output Entropies of the $k$-Deletion/Insertion Channel",
    "descriptor": "",
    "authors": [
      "Shubhransh Singhvi",
      "Omer Sabary",
      "Daniella Bar-Lev",
      "Eitan Yaakobi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.03024"
  },
  {
    "id": "arXiv:2202.03169",
    "title": "CITRIS: Causal Identifiability from Temporal Intervened Sequences",
    "abstract": "Comments: Accepted at the International Conference on Machine Learning (ICML), 2022",
    "descriptor": "\nComments: Accepted at the International Conference on Machine Learning (ICML), 2022\n",
    "authors": [
      "Phillip Lippe",
      "Sara Magliacane",
      "Sindy L\u00f6we",
      "Yuki M. Asano",
      "Taco Cohen",
      "Efstratios Gavves"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2202.03169"
  },
  {
    "id": "arXiv:2202.03350",
    "title": "Optimal Gathering over Weber Meeting Nodes in Infinite Grid",
    "abstract": "Optimal Gathering over Weber Meeting Nodes in Infinite Grid",
    "descriptor": "",
    "authors": [
      "Subhash Bhagat",
      "Abhinav Chakraborty",
      "Bibhuti Das",
      "Krishnendu Mukhopadhyaya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.03350"
  },
  {
    "id": "arXiv:2202.05186",
    "title": "Fair allocation of a multiset of indivisible items",
    "abstract": "Comments: 34 pages, 6 figures, 1 table, 1 algorithm",
    "descriptor": "\nComments: 34 pages, 6 figures, 1 table, 1 algorithm\n",
    "authors": [
      "Pranay Gorantla",
      "Kunal Marwaha",
      "Santhoshini Velusamy"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2202.05186"
  },
  {
    "id": "arXiv:2202.05630",
    "title": "Scale-free Unconstrained Online Learning for Curved Losses",
    "abstract": "Comments: 34 pages",
    "descriptor": "\nComments: 34 pages\n",
    "authors": [
      "Jack J. Mayo",
      "H\u00e9di Hadiji",
      "Tim van Erven"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05630"
  },
  {
    "id": "arXiv:2202.07995",
    "title": "Branching Reinforcement Learning",
    "abstract": "Branching Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Yihan Du",
      "Wei Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.07995"
  },
  {
    "id": "arXiv:2202.08004",
    "title": "Deep Koopman Operator with Control for Nonlinear Systems",
    "abstract": "Deep Koopman Operator with Control for Nonlinear Systems",
    "descriptor": "",
    "authors": [
      "Haojie Shi",
      "Max Q.-H. Meng"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08004"
  },
  {
    "id": "arXiv:2202.08830",
    "title": "Optimal polynomial smoothers for multigrid V-cycles",
    "abstract": "Optimal polynomial smoothers for multigrid V-cycles",
    "descriptor": "",
    "authors": [
      "James Lottes"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.08830"
  },
  {
    "id": "arXiv:2202.09048",
    "title": "Task Specific Attention is one more thing you need for object detection",
    "abstract": "Task Specific Attention is one more thing you need for object detection",
    "descriptor": "",
    "authors": [
      "Sang Yon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.09048"
  },
  {
    "id": "arXiv:2202.09244",
    "title": "Transfer and Marginalize: Explaining Away Label Noise with Privileged  Information",
    "abstract": "Comments: Accepted at ICML 2022, Baltimore",
    "descriptor": "\nComments: Accepted at ICML 2022, Baltimore\n",
    "authors": [
      "Mark Collier",
      "Rodolphe Jenatton",
      "Efi Kokiopoulou",
      "Jesse Berent"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.09244"
  },
  {
    "id": "arXiv:2202.12810",
    "title": "Measuring dynamical systems on directed hyper-graphs",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Mauro Faccin"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2202.12810"
  },
  {
    "id": "arXiv:2203.00756",
    "title": "Real time spectrogram inversion on mobile phone",
    "abstract": "Real time spectrogram inversion on mobile phone",
    "descriptor": "",
    "authors": [
      "Oleg Rybakov",
      "Marco Tagliasacchi",
      "Yunpeng Li",
      "Liyang Jiang",
      "Xia Zhang",
      "Fadi Biadsy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2203.00756"
  },
  {
    "id": "arXiv:2203.00759",
    "title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Yun He",
      "Huaixiu Steven Zheng",
      "Yi Tay",
      "Jai Gupta",
      "Yu Du",
      "Vamsi Aribandi",
      "Zhe Zhao",
      "YaGuang Li",
      "Zhao Chen",
      "Donald Metzler",
      "Heng-Tze Cheng",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.00759"
  },
  {
    "id": "arXiv:2203.01173",
    "title": "Characteristics of de Bruijn's early proof checker Automath",
    "abstract": "Characteristics of de Bruijn's early proof checker Automath",
    "descriptor": "",
    "authors": [
      "Herman Geuvers",
      "Rob Nederpelt"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2203.01173"
  },
  {
    "id": "arXiv:2203.01474",
    "title": "Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction",
    "abstract": "Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction",
    "descriptor": "",
    "authors": [
      "Chongyang Zhong",
      "Lei Hu",
      "Zihao Zhang",
      "Yongjing Ye",
      "Shihong Xia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.01474"
  },
  {
    "id": "arXiv:2203.01556",
    "title": "DareFightingICE Competition: A Fighting Game Sound Design and AI  Competition",
    "abstract": "Comments: 2022 IEEE Conference on Games",
    "descriptor": "\nComments: 2022 IEEE Conference on Games\n",
    "authors": [
      "Ibrahim Khan",
      "Thai Van Nguyen",
      "Xincheng Dai",
      "Ruck Thawonmas"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.01556"
  },
  {
    "id": "arXiv:2203.02502",
    "title": "No More Than 6ft Apart: Robust K-Means via Radius Upper Bounds",
    "abstract": "Comments: Accepted for ICASSP 2022, 8 figures, 1 table",
    "descriptor": "\nComments: Accepted for ICASSP 2022, 8 figures, 1 table\n",
    "authors": [
      "Ahmed Imtiaz Humayun",
      "Randall Balestriero",
      "Anastasios Kyrillidis",
      "Richard Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.02502"
  },
  {
    "id": "arXiv:2203.03716",
    "title": "Open-Ended Knowledge Tracing",
    "abstract": "Open-Ended Knowledge Tracing",
    "descriptor": "",
    "authors": [
      "Naiming Liu",
      "Zichao Wang",
      "Richard G. Baraniuk",
      "Andrew Lan"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.03716"
  },
  {
    "id": "arXiv:2203.05008",
    "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word  Speech Recognition",
    "abstract": "Comments: Interspeech 2022",
    "descriptor": "\nComments: Interspeech 2022\n",
    "authors": [
      "W. Ronny Huang",
      "Cal Peyser",
      "Tara N. Sainath",
      "Ruoming Pang",
      "Trevor Strohman",
      "Shankar Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.05008"
  },
  {
    "id": "arXiv:2203.05625",
    "title": "PETR: Position Embedding Transformation for Multi-View 3D Object  Detection",
    "abstract": "Comments: Tech Report. Code is available at \\url{this https URL}",
    "descriptor": "\nComments: Tech Report. Code is available at \\url{this https URL}\n",
    "authors": [
      "Yingfei Liu",
      "Tiancai Wang",
      "Xiangyu Zhang",
      "Jian Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.05625"
  },
  {
    "id": "arXiv:2203.06333",
    "title": "Stable and Efficient Shapley Value-Based Reward Reallocation for  Multi-Agent Reinforcement Learning of Autonomous Vehicles",
    "abstract": "Comments: This paper has been accepted by the 2022 IEEE International Conference on Robotics and Automation (ICRA 2022)",
    "descriptor": "\nComments: This paper has been accepted by the 2022 IEEE International Conference on Robotics and Automation (ICRA 2022)\n",
    "authors": [
      "Songyang Han",
      "He Wang",
      "Sanbao Su",
      "Yuanyuan Shi",
      "Fei Miao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2203.06333"
  },
  {
    "id": "arXiv:2203.07490",
    "title": "Achieving Downstream Fairness with Geometric Repair",
    "abstract": "Achieving Downstream Fairness with Geometric Repair",
    "descriptor": "",
    "authors": [
      "Kweku Kwegyir-Aggrey",
      "Jessica Dai",
      "John Dickerson",
      "Keegan Hines"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.07490"
  },
  {
    "id": "arXiv:2203.10636",
    "title": "Transform your Smartphone into a DSLR Camera: Learning the ISP in the  Wild",
    "abstract": "Comments: 38 pages",
    "descriptor": "\nComments: 38 pages\n",
    "authors": [
      "Ardhendu Shekhar Tripathi",
      "Martin Danelljan",
      "Samarth Shukla",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.10636"
  },
  {
    "id": "arXiv:2203.10651",
    "title": "Nonstationary Temporal Matrix Factorization for Multivariate Time Series  Forecasting",
    "abstract": "Comments: Data and Python codes: this https URL",
    "descriptor": "\nComments: Data and Python codes: this https URL\n",
    "authors": [
      "Xinyu Chen",
      "Chengyuan Zhang",
      "Xi-Le Zhao",
      "Nicolas Saunier",
      "Lijun Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.10651"
  },
  {
    "id": "arXiv:2203.11442",
    "title": "Associating Objects with Scalable Transformers for Video Object  Segmentation",
    "abstract": "Comments: Extension of arXiv:2106.02638 (NeurIPS 2021)",
    "descriptor": "\nComments: Extension of arXiv:2106.02638 (NeurIPS 2021)\n",
    "authors": [
      "Zongxin Yang",
      "Jiaxu Miao",
      "Xiaohan Wang",
      "Yunchao Wei",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.11442"
  },
  {
    "id": "arXiv:2203.12369",
    "title": "MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data",
    "abstract": "Comments: 5 pages, 4 figures, Accepted to EUSIPCO 2022",
    "descriptor": "\nComments: 5 pages, 4 figures, Accepted to EUSIPCO 2022\n",
    "authors": [
      "George Close",
      "Thomas Hain",
      "Stefan Goetze"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.12369"
  },
  {
    "id": "arXiv:2203.12683",
    "title": "Revisiting Multi-Scale Feature Fusion for Semantic Segmentation",
    "abstract": "Revisiting Multi-Scale Feature Fusion for Semantic Segmentation",
    "descriptor": "",
    "authors": [
      "Tianjian Meng",
      "Golnaz Ghiasi",
      "Reza Mahjourian",
      "Quoc V. Le",
      "Mingxing Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.12683"
  },
  {
    "id": "arXiv:2203.13351",
    "title": "Predicting Personas Using Mechanic Frequencies and Game State Traces",
    "abstract": "Comments: 8 pages, 3 tables, 2 figures",
    "descriptor": "\nComments: 8 pages, 3 tables, 2 figures\n",
    "authors": [
      "Michael Cerny Green",
      "Ahmed Khalifa",
      "M Charity",
      "Debosmita Bhaumik",
      "Julian Togelius"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.13351"
  },
  {
    "id": "arXiv:2203.13687",
    "title": "Chain-based Discriminative Autoencoders for Speech Recognition",
    "abstract": "Comments: Published in Interspeech 2022",
    "descriptor": "\nComments: Published in Interspeech 2022\n",
    "authors": [
      "Hung-Shin Lee",
      "Pin-Tuan Huang",
      "Yao-Fei Cheng",
      "Hsin-Min Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.13687"
  },
  {
    "id": "arXiv:2203.13762",
    "title": "A World-Self Model Towards Understanding Intelligence",
    "abstract": "Comments: 21 pages, 5 figures",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Yutao Yue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2203.13762"
  },
  {
    "id": "arXiv:2203.13935",
    "title": "Offline Reinforcement Learning Under Value and Density-Ratio  Realizability: The Power of Gaps",
    "abstract": "Offline Reinforcement Learning Under Value and Density-Ratio  Realizability: The Power of Gaps",
    "descriptor": "",
    "authors": [
      "Jinglin Chen",
      "Nan Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.13935"
  },
  {
    "id": "arXiv:2203.14261",
    "title": "The Lattice-Theoretic Essence of PropertyDirected Reachability Analysis",
    "abstract": "Comments: 37 pages",
    "descriptor": "\nComments: 37 pages\n",
    "authors": [
      "Mayuko Kori",
      "Natsuki Urabe",
      "Shin-ya Katsumata",
      "Kohei Suenaga",
      "Ichiro Hasuo"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2203.14261"
  },
  {
    "id": "arXiv:2203.15206",
    "title": "Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR",
    "abstract": "Comments: we need to refine this paper",
    "descriptor": "\nComments: we need to refine this paper\n",
    "authors": [
      "Fangyuan Wang",
      "Bo Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.15206"
  },
  {
    "id": "arXiv:2203.15398",
    "title": "Learning to act: a Reinforcement Learning approach to recommend the best  next activities",
    "abstract": "Comments: 16 pages, 3 figures, v2 accepted to the BPM 2022 Forum",
    "descriptor": "\nComments: 16 pages, 3 figures, v2 accepted to the BPM 2022 Forum\n",
    "authors": [
      "Stefano Branchi",
      "Chiara Di Francescomarino",
      "Chiara Ghidini",
      "David Massimo",
      "Francesco Ricci",
      "Massimiliano Ronzani"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.15398"
  },
  {
    "id": "arXiv:2203.15695",
    "title": "Performance of surface codes in realistic quantum hardware",
    "abstract": "Comments: Includes supplementary material (19 pages total)",
    "descriptor": "\nComments: Includes supplementary material (19 pages total)\n",
    "authors": [
      "Antonio deMarti iOlius",
      "Josu Etxezarreta Martinez",
      "Patricio Fuentes",
      "Pedro M. Crespo",
      "Javier Garcia-Frias"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.15695"
  },
  {
    "id": "arXiv:2203.16033",
    "title": "Optimizing Shoulder to Shoulder: A Coordinated Sub-Band Fusion Model for  Real-Time Full-Band Speech Enhancement",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2203.00472",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.00472\n",
    "authors": [
      "Guochen Yu",
      "Andong Li",
      "Wenzhe Liu",
      "Chengshi Zheng",
      "Yutian Wang",
      "Hui Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16033"
  },
  {
    "id": "arXiv:2203.16512",
    "title": "Vakyansh: ASR Toolkit for Low Resource Indic languages",
    "abstract": "Vakyansh: ASR Toolkit for Low Resource Indic languages",
    "descriptor": "",
    "authors": [
      "Harveen Singh Chadha",
      "Anirudh Gupta",
      "Priyanshi Shah",
      "Neeraj Chhimwal",
      "Ankur Dhuriya",
      "Rishabh Gaur",
      "Vivek Raghavan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16512"
  },
  {
    "id": "arXiv:2203.16595",
    "title": "Improving Speech Recognition for Indic Languages using Language Model",
    "abstract": "Comments: Need to upgrade the content completely",
    "descriptor": "\nComments: Need to upgrade the content completely\n",
    "authors": [
      "Ankur Dhuriya",
      "Harveen Singh Chadha",
      "Anirudh Gupta",
      "Priyanshi Shah",
      "Neeraj Chhimwal",
      "Rishabh Gaur",
      "Vivek Raghavan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16595"
  },
  {
    "id": "arXiv:2203.16601",
    "title": "Is Word Error Rate a good evaluation metric for Speech Recognition in  Indic Languages?",
    "abstract": "Comments: Need to upgrade the content completely",
    "descriptor": "\nComments: Need to upgrade the content completely\n",
    "authors": [
      "Priyanshi Shah",
      "Harveen Singh Chadha",
      "Anirudh Gupta",
      "Ankur Dhuriya",
      "Neeraj Chhimwal",
      "Rishabh Gaur",
      "Vivek Raghavan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.16601"
  },
  {
    "id": "arXiv:2204.01855",
    "title": "A Survey on Graph Representation Learning Methods",
    "abstract": "A Survey on Graph Representation Learning Methods",
    "descriptor": "",
    "authors": [
      "Shima Khoshraftar",
      "Aijun An"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2204.01855"
  },
  {
    "id": "arXiv:2204.02033",
    "title": "Learning to Reduce Information Bottleneck for Object Detection in Aerial  Images",
    "abstract": "Comments: 5 pages, 4 figures",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Yuchen Shen",
      "Dong Zhang",
      "Zhihao Song",
      "Xuesong Jiang",
      "Qiaolin Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.02033"
  },
  {
    "id": "arXiv:2204.02492",
    "title": "Towards End-to-end Unsupervised Speech Recognition",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Alexander H. Liu",
      "Wei-Ning Hsu",
      "Michael Auli",
      "Alexei Baevski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.02492"
  },
  {
    "id": "arXiv:2204.04667",
    "title": "Linear Complexity Randomized Self-attention Mechanism",
    "abstract": "Comments: ICML 2022 camera ready with 37 pages",
    "descriptor": "\nComments: ICML 2022 camera ready with 37 pages\n",
    "authors": [
      "Lin Zheng",
      "Chong Wang",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.04667"
  },
  {
    "id": "arXiv:2204.05112",
    "title": "FastMapSVM: Classifying Complex Objects Using the FastMap Algorithm and  Support-Vector Machines",
    "abstract": "Comments: 27 pages, 12 figures",
    "descriptor": "\nComments: 27 pages, 12 figures\n",
    "authors": [
      "Malcolm C. A. White",
      "Kushal Sharma",
      "Ang Li",
      "T. K. Satish Kumar",
      "Nori Nakata"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.05112"
  },
  {
    "id": "arXiv:2204.06924",
    "title": "Handling sign language transcription system with the computer-friendly  numerical multilabels",
    "abstract": "Comments: 6 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 6 pages, 3 figures, 3 tables\n",
    "authors": [
      "Sylwia Majchrowska",
      "Marta Plantykow",
      "Milena Olech"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.06924"
  },
  {
    "id": "arXiv:2204.08198",
    "title": "UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm  Detection using generative-based and mutation-based data augmentation",
    "abstract": "Comments: 6 pages, 2 figures, International Workshop on Semantic Evaluation co-located with NAACL",
    "descriptor": "\nComments: 6 pages, 2 figures, International Workshop on Semantic Evaluation co-located with NAACL\n",
    "authors": [
      "Amirhossein Abaskohi",
      "Arash Rasouli",
      "Tanin Zeraati",
      "Behnam Bahrak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.08198"
  },
  {
    "id": "arXiv:2204.08664",
    "title": "Toward Understanding the Use of Centralized Exchanges for Decentralized  Cryptocurrency",
    "abstract": "Comments: 13th International Conference on Applied Human Factors and Ergonomics (AHFE 2022)",
    "descriptor": "\nComments: 13th International Conference on Applied Human Factors and Ergonomics (AHFE 2022)\n",
    "authors": [
      "Zhixuan Zhou",
      "Bohui Shen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.08664"
  },
  {
    "id": "arXiv:2204.09984",
    "title": "Convergence analysis of a Local Discontinuous Galerkin approximation for  nonlinear systems with Orlicz-structure",
    "abstract": "Comments: 27 pages, 4 figures, 5 tables",
    "descriptor": "\nComments: 27 pages, 4 figures, 5 tables\n",
    "authors": [
      "Alex Kaltenbach",
      "Michael R\u016f\u017ei\u010dka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.09984"
  },
  {
    "id": "arXiv:2204.10749",
    "title": "E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR",
    "abstract": "Comments: Interspeech 2022",
    "descriptor": "\nComments: Interspeech 2022\n",
    "authors": [
      "W. Ronny Huang",
      "Shuo-yiin Chang",
      "David Rybach",
      "Rohit Prabhavalkar",
      "Tara N. Sainath",
      "Cyril Allauzen",
      "Cal Peyser",
      "Zhiyun Lu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.10749"
  },
  {
    "id": "arXiv:2204.11515",
    "title": "Multimodal Dual Emotion with Fusion of Visual Sentiment for Rumor  Detection",
    "abstract": "Comments: submitted TMM",
    "descriptor": "\nComments: submitted TMM\n",
    "authors": [
      "Ge Wang",
      "Li Tan",
      "Ziliang Shang",
      "He Liu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.11515"
  },
  {
    "id": "arXiv:2204.13843",
    "title": "VPNets: Volume-preserving neural networks for learning source-free  dynamics",
    "abstract": "VPNets: Volume-preserving neural networks for learning source-free  dynamics",
    "descriptor": "",
    "authors": [
      "Aiqing Zhu",
      "Beibei Zhu",
      "Jiawei Zhang",
      "Yifa Tang",
      "Jian Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13843"
  },
  {
    "id": "arXiv:2205.01052",
    "title": "HTTPA/2: a Trusted End-to-End Protocol for Web Services",
    "abstract": "Comments: 23 pages, 6 figures",
    "descriptor": "\nComments: 23 pages, 6 figures\n",
    "authors": [
      "Gordon King",
      "Hans Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.01052"
  },
  {
    "id": "arXiv:2205.05871",
    "title": "Towards Robust Unsupervised Disentanglement of Sequential Data -- A Case  Study Using Music Audio",
    "abstract": "Comments: The paper is accepted to IJCAI 2022",
    "descriptor": "\nComments: The paper is accepted to IJCAI 2022\n",
    "authors": [
      "Yin-Jyun Luo",
      "Sebastian Ewert",
      "Simon Dixon"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.05871"
  },
  {
    "id": "arXiv:2205.07598",
    "title": "Cell-Free MmWave Massive MIMO Systems with Low-Capacity Fronthaul Links  and Low-Resolution ADC/DACs",
    "abstract": "Comments: to appear in IEEE Transactions on Vehicular Technology",
    "descriptor": "\nComments: to appear in IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "In-soo Kim",
      "Mehdi Bennis",
      "Junil Choi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.07598"
  },
  {
    "id": "arXiv:2205.08479",
    "title": "Opportunistic Routing in Quantum Networks",
    "abstract": "Comments: This version extends our INFOCOM'2022 paper by adding more analysis and simulations",
    "descriptor": "\nComments: This version extends our INFOCOM'2022 paper by adding more analysis and simulations\n",
    "authors": [
      "Ali Farahbakhsh",
      "Chen Feng"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.08479"
  },
  {
    "id": "arXiv:2205.10643",
    "title": "Self-Supervised Speech Representation Learning: A Review",
    "abstract": "Self-Supervised Speech Representation Learning: A Review",
    "descriptor": "",
    "authors": [
      "Abdelrahman Mohamed",
      "Hung-yi Lee",
      "Lasse Borgholt",
      "Jakob D. Havtorn",
      "Joakim Edin",
      "Christian Igel",
      "Katrin Kirchhoff",
      "Shang-Wen Li",
      "Karen Livescu",
      "Lars Maal\u00f8e",
      "Tara N. Sainath",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.10643"
  },
  {
    "id": "arXiv:2205.11624",
    "title": "Effectively Incorporating Weighted Cost-to-go Heuristic in Suboptimal  CBS",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Rishi Veerapaneni",
      "Tushar Kusnur",
      "Maxim Likhachev"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.11624"
  },
  {
    "id": "arXiv:2205.11798",
    "title": "Symbolic Expression Transformer: A Computer Vision Approach for Symbolic  Regression",
    "abstract": "Symbolic Expression Transformer: A Computer Vision Approach for Symbolic  Regression",
    "descriptor": "",
    "authors": [
      "Jiachen Li",
      "Ye Yuan",
      "Hong-Bin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11798"
  },
  {
    "id": "arXiv:2205.12258",
    "title": "History Compression via Language Models in Reinforcement Learning",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Fabian Paischer",
      "Thomas Adler",
      "Vihang Patil",
      "Angela Bitto-Nemling",
      "Markus Holzleitner",
      "Sebastian Lehner",
      "Hamid Eghbal-zadeh",
      "Sepp Hochreiter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12258"
  },
  {
    "id": "arXiv:2205.12442",
    "title": "Lyapunov function approach for approximation algorithm design and  analysis: with applications in submodular maximization",
    "abstract": "Lyapunov function approach for approximation algorithm design and  analysis: with applications in submodular maximization",
    "descriptor": "",
    "authors": [
      "Donglei Du"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12442"
  },
  {
    "id": "arXiv:2205.12583",
    "title": "MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose",
    "abstract": "MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose",
    "descriptor": "",
    "authors": [
      "Chenyan Wu",
      "Yandong Li",
      "Xianfeng Tang",
      "James Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12583"
  },
  {
    "id": "arXiv:2205.13343",
    "title": "Sliding mode control with a neural network compensation scheme for  electro-hydraulic systems",
    "abstract": "Comments: References added. This is a slightly updated version of the work presented at DINAME 2013 - XV International Symposium on Dynamic Problems of Mechanics, 2013, Arma\\c{c}\\~ao de B\\'uzios, Brazil",
    "descriptor": "\nComments: References added. This is a slightly updated version of the work presented at DINAME 2013 - XV International Symposium on Dynamic Problems of Mechanics, 2013, Arma\\c{c}\\~ao de B\\'uzios, Brazil\n",
    "authors": [
      "Josiane Maria de Macedo Fernandes",
      "Marcelo Costa Tanaka",
      "Wallace Moreira Bessa"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.13343"
  },
  {
    "id": "arXiv:2205.14014",
    "title": "What Dense Graph Do You Need for Self-Attention?",
    "abstract": "Comments: Accepted by ICML 2022. Code is available at this https URL",
    "descriptor": "\nComments: Accepted by ICML 2022. Code is available at this https URL\n",
    "authors": [
      "Yuxing Wang",
      "Chu-Tak Lee",
      "Qipeng Guo",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14014"
  },
  {
    "id": "arXiv:2205.14268",
    "title": "NeuPSL: Neural Probabilistic Soft Logic",
    "abstract": "NeuPSL: Neural Probabilistic Soft Logic",
    "descriptor": "",
    "authors": [
      "Connor Pryor",
      "Charles Dickens",
      "Eriq Augustine",
      "Alon Albalak",
      "William Wang",
      "Lise Getoor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.14268"
  },
  {
    "id": "arXiv:2205.14372",
    "title": "On stationary inflection points in step responses",
    "abstract": "Comments: The Theorems as stated are false",
    "descriptor": "\nComments: The Theorems as stated are false\n",
    "authors": [
      "Maben Rabi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.14372"
  },
  {
    "id": "arXiv:2205.15783",
    "title": "Benchmarks for infinite medium, time dependent transport problems with  isotropic scattering",
    "abstract": "Benchmarks for infinite medium, time dependent transport problems with  isotropic scattering",
    "descriptor": "",
    "authors": [
      "William Bennett",
      "Ryan G. McClarren"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2205.15783"
  },
  {
    "id": "arXiv:2206.01230",
    "title": "Formalizing Human Ingenuity: A Quantitative Framework for Copyright  Law's Substantial Similarity",
    "abstract": "Formalizing Human Ingenuity: A Quantitative Framework for Copyright  Law's Substantial Similarity",
    "descriptor": "",
    "authors": [
      "Sarah Scheffler",
      "Eran Tromer",
      "Mayank Varia"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.01230"
  },
  {
    "id": "arXiv:2206.01726",
    "title": "Average-case analysis of the Gaussian Elimination with Partial Pivoting",
    "abstract": "Comments: a revised version",
    "descriptor": "\nComments: a revised version\n",
    "authors": [
      "Han Huang",
      "Konstantin Tikhomirov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2206.01726"
  },
  {
    "id": "arXiv:2206.01992",
    "title": "CAINNFlow: Convolutional block Attention modules and Invertible Neural  Networks Flow for anomaly detection and localization tasks",
    "abstract": "CAINNFlow: Convolutional block Attention modules and Invertible Neural  Networks Flow for anomaly detection and localization tasks",
    "descriptor": "",
    "authors": [
      "Ruiqing Yan",
      "Fan Zhang",
      "Mengyuan Huang",
      "Wu Liu",
      "Dongyu Hu",
      "Jinfeng Li",
      "Qiang Liu",
      "Jingrong Jiang",
      "Qianjin Guo",
      "Linghan Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01992"
  },
  {
    "id": "arXiv:2206.02330",
    "title": "Linear MSRD codes with Various Matrix Sizes and Unrestricted Lengths",
    "abstract": "Comments: 11 pages, no restriction on lengths of codes. arXiv admin note: text overlap with arXiv:2205.13087",
    "descriptor": "\nComments: 11 pages, no restriction on lengths of codes. arXiv admin note: text overlap with arXiv:2205.13087\n",
    "authors": [
      "Hao Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.02330"
  },
  {
    "id": "arXiv:2206.02873",
    "title": "No Parameter Left Behind: How Distillation and Model Size Affect  Zero-Shot Retrieval",
    "abstract": "No Parameter Left Behind: How Distillation and Model Size Affect  Zero-Shot Retrieval",
    "descriptor": "",
    "authors": [
      "Guilherme Moraes Rosa",
      "Luiz Bonifacio",
      "Vitor Jeronymo",
      "Hugo Abonizio",
      "Marzieh Fadaee",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2206.02873"
  },
  {
    "id": "arXiv:2206.03132",
    "title": "CitySpec: An Intelligent Assistant System for Requirement Specification  in Smart Cities",
    "abstract": "Comments: This paper is accepted by SMARTCOMP 2022",
    "descriptor": "\nComments: This paper is accepted by SMARTCOMP 2022\n",
    "authors": [
      "Zirong Chen",
      "Isaac Li",
      "Haoxiang Zhang",
      "Sarah Preum",
      "John A. Stankovic",
      "Meiyi Ma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.03132"
  },
  {
    "id": "arXiv:2206.03164",
    "title": "Utility of Equivariant Message Passing in Cortical Mesh Segmentation",
    "abstract": "Comments: 13 pages, 3 figures, accepted for MIUA 2022",
    "descriptor": "\nComments: 13 pages, 3 figures, accepted for MIUA 2022\n",
    "authors": [
      "D\u00e1niel Unyi",
      "Ferdinando Insalata",
      "Petar Veli\u010dkovi\u0107",
      "B\u00e1lint Gyires-T\u00f3th"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03164"
  },
  {
    "id": "arXiv:2206.03287",
    "title": "NeMF: Neural Motion Fields for Kinematic Animation",
    "abstract": "Comments: Our project page is available at: this https URL",
    "descriptor": "\nComments: Our project page is available at: this https URL\n",
    "authors": [
      "Chengan He",
      "Jun Saito",
      "James Zachary",
      "Holly Rushmeier",
      "Yi Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.03287"
  },
  {
    "id": "arXiv:2206.03383",
    "title": "On the Role of Discount Factor in Offline Reinforcement Learning",
    "abstract": "Comments: Thirty-ninth International Conference on Machine Learning",
    "descriptor": "\nComments: Thirty-ninth International Conference on Machine Learning\n",
    "authors": [
      "Hao Hu",
      "Yiqin Yang",
      "Qianchuan Zhao",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03383"
  },
  {
    "id": "arXiv:2206.03487",
    "title": "Formalization of the principles of brain Programming (Brain Principles  Programming)",
    "abstract": "Comments: 28 pages, in Russian, 4 figures",
    "descriptor": "\nComments: 28 pages, in Russian, 4 figures\n",
    "authors": [
      "E.E. Vityaev",
      "A.G. Kolonin",
      "A.V. Kurpatov A.A. Molchanov"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.03487"
  },
  {
    "id": "arXiv:2206.03657",
    "title": "Delving into the Pre-training Paradigm of Monocular 3D Object Detection",
    "abstract": "Delving into the Pre-training Paradigm of Monocular 3D Object Detection",
    "descriptor": "",
    "authors": [
      "Zhuoling Li",
      "Chuanrui Zhang",
      "En Yu",
      "Haoqian Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.03657"
  },
  {
    "id": "arXiv:2206.03693",
    "title": "Autoregressive Perturbations for Data Poisoning",
    "abstract": "Comments: 22 pages, 13 figures. Code available at this https URL",
    "descriptor": "\nComments: 22 pages, 13 figures. Code available at this https URL\n",
    "authors": [
      "Pedro Sandoval-Segura",
      "Vasu Singla",
      "Jonas Geiping",
      "Micah Goldblum",
      "Tom Goldstein",
      "David W. Jacobs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.03693"
  },
  {
    "id": "arXiv:2206.03832",
    "title": "Constructive TT-representation of the tensors given as index interaction  functions with applications",
    "abstract": "Constructive TT-representation of the tensors given as index interaction  functions with applications",
    "descriptor": "",
    "authors": [
      "Gleb Ryzhakov",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.03832"
  },
  {
    "id": "arXiv:2206.03936",
    "title": "Linear Precoder Design in Massive MIMO under Realistic Power Amplifier  Consumption Constraint",
    "abstract": "Linear Precoder Design in Massive MIMO under Realistic Power Amplifier  Consumption Constraint",
    "descriptor": "",
    "authors": [
      "Emanuele Peschiera",
      "Fran\u00e7ois Rottenberg"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.03936"
  },
  {
    "id": "arXiv:2206.04105",
    "title": "Words are all you need? Capturing human sensory similarity with textual  descriptors",
    "abstract": "Comments: Fixed fonts in Figure 1",
    "descriptor": "\nComments: Fixed fonts in Figure 1\n",
    "authors": [
      "Raja Marjieh",
      "Pol van Rijn",
      "Ilia Sucholutsky",
      "Theodore R. Sumers",
      "Harin Lee",
      "Thomas L. Griffiths",
      "Nori Jacoby"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04105"
  },
  {
    "id": "arXiv:2206.04199",
    "title": "Deep Surrogate Assisted Generation of Environments",
    "abstract": "Comments: 24 pages, 15 figures, supplemental website at this https URL",
    "descriptor": "\nComments: 24 pages, 15 figures, supplemental website at this https URL\n",
    "authors": [
      "Varun Bhatt",
      "Bryon Tjanaka",
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.04199"
  },
  {
    "id": "arXiv:2206.04583",
    "title": "Clustering with Queries under Semi-Random Noise",
    "abstract": "Comments: Accepted for presentation at the Conference on Learning Theory (COLT) 2022",
    "descriptor": "\nComments: Accepted for presentation at the Conference on Learning Theory (COLT) 2022\n",
    "authors": [
      "Alberto Del Pia",
      "Mingchen Ma",
      "Christos Tzamos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.04583"
  },
  {
    "id": "arXiv:2206.04922",
    "title": "A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural  Machine Translation",
    "abstract": "Comments: 5 pages,5 figures",
    "descriptor": "\nComments: 5 pages,5 figures\n",
    "authors": [
      "Wudi Bao",
      "Junhui Zhang",
      "Junjie Pan",
      "Xiang Yin",
      "Zejun Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04922"
  },
  {
    "id": "arXiv:2206.04967",
    "title": "Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G",
    "abstract": "Comments: To be published on IEICE Transactions on Communications",
    "descriptor": "\nComments: To be published on IEICE Transactions on Communications\n",
    "authors": [
      "Xin Wang",
      "Xiaolin Hou",
      "Lan Chen",
      "Yoshihisa Kishiyama",
      "Takahiro Asai"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.04967"
  },
  {
    "id": "arXiv:2206.05199",
    "title": "Bayesian Estimation of Differential Privacy",
    "abstract": "Comments: 17 pages, 8 figures. Joint main authors: Santiago Zanella-B\\'eguelin, Lukas Wutschitz, and Shruti Tople",
    "descriptor": "\nComments: 17 pages, 8 figures. Joint main authors: Santiago Zanella-B\\'eguelin, Lukas Wutschitz, and Shruti Tople\n",
    "authors": [
      "Santiago Zanella-B\u00e9guelin",
      "Lukas Wutschitz",
      "Shruti Tople",
      "Ahmed Salem",
      "Victor R\u00fchle",
      "Andrew Paverd",
      "Mohammad Naseri",
      "Boris K\u00f6pf",
      "Daniel Jones"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.05199"
  },
  {
    "id": "arXiv:2206.05862",
    "title": "X-Risk Analysis for AI Research",
    "abstract": "X-Risk Analysis for AI Research",
    "descriptor": "",
    "authors": [
      "Dan Hendrycks",
      "Mantas Mazeika"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05862"
  },
  {
    "id": "arXiv:2206.05966",
    "title": "Coordinating Monetary Contributions in Participatory Budgeting",
    "abstract": "Coordinating Monetary Contributions in Participatory Budgeting",
    "descriptor": "",
    "authors": [
      "Haris Aziz",
      "Sujit Gujar",
      "Manisha Padala",
      "Mashbat Suzuki",
      "Jeremy Vollen"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)",
      "Multiagent Systems (cs.MA)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2206.05966"
  },
  {
    "id": "arXiv:2206.06346",
    "title": "Bringing Image Scene Structure to Video via Frame-Clip Consistency of  Object Tokens",
    "abstract": "Comments: Tech report",
    "descriptor": "\nComments: Tech report\n",
    "authors": [
      "Elad Ben-Avraham",
      "Roei Herzig",
      "Karttikeya Mangalam",
      "Amir Bar",
      "Anna Rohrbach",
      "Leonid Karlinsky",
      "Trevor Darrell",
      "Amir Globerson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06346"
  },
  {
    "id": "arXiv:2206.06445",
    "title": "Fitting Segmentation Networks on Varying Image Resolutions using  Splatting",
    "abstract": "Comments: Accepted for MIUA 2022",
    "descriptor": "\nComments: Accepted for MIUA 2022\n",
    "authors": [
      "Mikael Brudfors",
      "Yael Balbastre",
      "John Ashburner",
      "Geraint Rees",
      "Parashkev Nachev",
      "Sebastien Ourselin",
      "M. Jorge Cardoso"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06445"
  },
  {
    "id": "arXiv:2206.06565",
    "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning  Tasks",
    "abstract": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning  Tasks",
    "descriptor": "",
    "authors": [
      "Tuan Dinh",
      "Yuchen Zeng",
      "Ruisu Zhang",
      "Ziqian Lin",
      "Michael Gira",
      "Shashank Rajput",
      "Jy-yong Sohn",
      "Dimitris Papailiopoulos",
      "Kangwook Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.06565"
  },
  {
    "id": "arXiv:2206.06581",
    "title": "CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization",
    "abstract": "CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization",
    "descriptor": "",
    "authors": [
      "Shweta Yadav",
      "Deepak Gupta",
      "Dina Demner-Fushman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.06581"
  },
  {
    "id": "arXiv:2206.06637",
    "title": "RF-Next: Efficient Receptive Field Search for Convolutional Neural  Networks",
    "abstract": "Comments: Accepted by TPAMI. This paper is a journal extension of our CVPR 2021 paper (arXiv:2101.00910)",
    "descriptor": "\nComments: Accepted by TPAMI. This paper is a journal extension of our CVPR 2021 paper (arXiv:2101.00910)\n",
    "authors": [
      "Shanghua Gao",
      "Zhong-Yu Li",
      "Qi Han",
      "Ming-Ming Cheng",
      "Liang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06637"
  },
  {
    "id": "arXiv:2206.06686",
    "title": "Bandwidth Enables Generalization in Quantum Kernel Models",
    "abstract": "Bandwidth Enables Generalization in Quantum Kernel Models",
    "descriptor": "",
    "authors": [
      "Abdulkadir Canatar",
      "Evan Peters",
      "Cengiz Pehlevan",
      "Stefan M. Wild",
      "Ruslan Shaydulin"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06686"
  },
  {
    "id": "arXiv:2206.06714",
    "title": "Interpretable Gait Recognition by Granger Causality",
    "abstract": "Comments: Preprint. Full paper accepted at the IEEE/IAPR International Conference on Pattern Recognition (ICPR), Montreal, Canada, Aug 2022. 7 pages",
    "descriptor": "\nComments: Preprint. Full paper accepted at the IEEE/IAPR International Conference on Pattern Recognition (ICPR), Montreal, Canada, Aug 2022. 7 pages\n",
    "authors": [
      "Michal Balazia",
      "Katerina Hlavackova-Schindler",
      "Petr Sojka",
      "Claudia Plant"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06714"
  },
  {
    "id": "arXiv:2206.06827",
    "title": "Variance Reduction for Policy-Gradient Methods via Empirical Variance  Minimization",
    "abstract": "Variance Reduction for Policy-Gradient Methods via Empirical Variance  Minimization",
    "descriptor": "",
    "authors": [
      "Maxim Kaledin",
      "Alexander Golubev",
      "Denis Belomestny"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06827"
  },
  {
    "id": "arXiv:2206.06829",
    "title": "Efficient Decoder-free Object Detection with Transformers",
    "abstract": "Efficient Decoder-free Object Detection with Transformers",
    "descriptor": "",
    "authors": [
      "Peixian Chen",
      "Mengdan Zhang",
      "Yunhang Shen",
      "Kekai Sheng",
      "Yuting Gao",
      "Xing Sun",
      "Ke Li",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06829"
  },
  {
    "id": "arXiv:2206.06879",
    "title": "Creating a Secure Underlay for the Internet",
    "abstract": "Comments: Usenix Security 2022",
    "descriptor": "\nComments: Usenix Security 2022\n",
    "authors": [
      "Henry Birge-Lee",
      "Joel Wanner",
      "Grace Cimaszewski",
      "Jonghoon Kwon",
      "Liang Wang",
      "Francois Wirz",
      "Prateek Mittal",
      "Adrian Perrig",
      "Yixin Sun"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.06879"
  },
  {
    "id": "arXiv:2206.06885",
    "title": "Neural interval-censored Cox regression with feature selection",
    "abstract": "Neural interval-censored Cox regression with feature selection",
    "descriptor": "",
    "authors": [
      "Carlos Garc\u00eda Meixide",
      "Marcos Matabuena",
      "Michael R. Kosorok"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.06885"
  }
]