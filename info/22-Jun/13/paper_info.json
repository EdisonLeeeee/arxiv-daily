[
  {
    "id": "arXiv:2206.04675",
    "title": "Deep Convolutional Ritz Method: Parametric PDE surrogates without  labeled data",
    "abstract": "Parametric surrogate models for partial differential equations (PDEs) are a\nnecessary component for many applications in the computational sciences, and\nconvolutional neural networks (CNNs) have proved as an excellent tool to\ngenerate these surrogates when parametric fields are present. CNNs are commonly\ntrained on labeled data based on one-to-one sets of parameter-input and\nPDE-output fields. Recently, residual-based convolutional physics-informed\nneural network (CPINN) solvers for parametric PDEs have been proposed to build\nsurrogates without the need for labeled data. These allow for the generation of\nsurrogates without an expensive offline-phase. In this work, we present an\nalternative formulation termed Deep Convolutional Ritz Method (DCRM) as a\nparametric PDE solver. The approach is based on the minimization of energy\nfunctionals, which lowers the order of the differential operators compared to\nresidual-based methods. Based on studies involving the Poisson equation with a\nspatially parameterized source term and boundary conditions, we found that CNNs\ntrained on labeled data outperform CPINNs in convergence speed and\ngeneralization ability. Surrogates generated from DCRM, however, converge\nsignificantly faster than their CPINN counterparts and prove to generalize\nfaster and better than surrogates obtained from both CNNs trained on labeled\ndata and CPINNs. This hints that DCRM could make PDE solution surrogates\ntrained without labeled data possible.",
    "descriptor": "\nComments: 20 pages, 12 figures\n",
    "authors": [
      "Jan Niklas Fuhg",
      "Arnav Karmarkar",
      "Teeratorn Kadeethum",
      "Hongkyu Yoon",
      "Nikolaos Bouklas"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.04675"
  },
  {
    "id": "arXiv:2206.04676",
    "title": "Extending Momentum Contrast with Cross Similarity Consistency  Regularization",
    "abstract": "Contrastive self-supervised representation learning methods maximize the\nsimilarity between the positive pairs, and at the same time tend to minimize\nthe similarity between the negative pairs. However, in general the interplay\nbetween the negative pairs is ignored as they do not put in place special\nmechanisms to treat negative pairs differently according to their specific\ndifferences and similarities. In this paper, we present Extended Momentum\nContrast (XMoCo), a self-supervised representation learning method founded upon\nthe legacy of the momentum-encoder unit proposed in the MoCo family\nconfigurations. To this end, we introduce a cross consistency regularization\nloss, with which we extend the transformation consistency to dissimilar images\n(negative pairs). Under the cross consistency regularization rule, we argue\nthat semantic representations associated with any pair of images (positive or\nnegative) should preserve their cross-similarity under pretext transformations.\nMoreover, we further regularize the training loss by enforcing a uniform\ndistribution of similarity over the negative pairs across a batch. The proposed\nregularization can easily be added to existing self-supervised learning\nalgorithms in a plug-and-play fashion. Empirically, we report a competitive\nperformance on the standard Imagenet-1K linear head classification benchmark.\nIn addition, by transferring the learned representations to common downstream\ntasks, we show that using XMoCo with the prevalently utilized augmentations can\nlead to improvements in the performance of such tasks. We hope the findings of\nthis paper serve as a motivation for researchers to take into consideration the\nimportant interplay among the negative examples in self-supervised learning.",
    "descriptor": "\nComments: IEEE Transactions on Circuits and Systems for Video Technology\n",
    "authors": [
      "Mehdi Seyfi",
      "Amin Banitalebi-Dehkordi",
      "Yong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04676"
  },
  {
    "id": "arXiv:2206.04677",
    "title": "Can Backdoor Attacks Survive Time-Varying Models?",
    "abstract": "Backdoors are powerful attacks against deep neural networks (DNNs). By\npoisoning training data, attackers can inject hidden rules (backdoors) into\nDNNs, which only activate on inputs containing attack-specific triggers. While\nexisting work has studied backdoor attacks on a variety of DNN models, they\nonly consider static models, which remain unchanged after initial deployment.\nIn this paper, we study the impact of backdoor attacks on a more realistic\nscenario of time-varying DNN models, where model weights are updated\nperiodically to handle drifts in data distribution over time. Specifically, we\nempirically quantify the \"survivability\" of a backdoor against model updates,\nand examine how attack parameters, data drift behaviors, and model update\nstrategies affect backdoor survivability. Our results show that one-shot\nbackdoor attacks (i.e., only poisoning training data once) do not survive past\na few model updates, even when attackers aggressively increase trigger size and\npoison ratio. To stay unaffected by model update, attackers must continuously\nintroduce corrupted data into the training pipeline. Together, these results\nindicate that when models are updated to learn new data, they also \"forget\"\nbackdoors as hidden, malicious features. The larger the distribution shift\nbetween old and new training data, the faster backdoors are forgotten.\nLeveraging these insights, we apply a smart learning rate scheduler to further\naccelerate backdoor forgetting during model updates, which prevents one-shot\nbackdoors from surviving past a single model update.",
    "descriptor": "",
    "authors": [
      "Huiying Li",
      "Arjun Nitin Bhagoji",
      "Ben Y. Zhao",
      "Haitao Zheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04677"
  },
  {
    "id": "arXiv:2206.04678",
    "title": "ReCo: A Dataset for Residential Community Layout Planning",
    "abstract": "Layout planning is centrally important in the field of architecture and urban\ndesign. Among the various basic units carrying urban functions, residential\ncommunity plays a vital part for supporting human life. Therefore, the layout\nplanning of residential community has always been of concern, and has attracted\nparticular attention since the advent of deep learning that facilitates the\nautomated layout generation and spatial pattern recognition. However, the\nresearch circles generally suffer from the insufficiency of residential\ncommunity layout benchmark or high-quality datasets, which hampers the future\nexploration of data-driven methods for residential community layout planning.\nThe lack of datasets is largely due to the difficulties of large-scale\nreal-world residential data acquisition and long-term expert screening. In\norder to address the issues and advance a benchmark dataset for various\nintelligent spatial design and analysis applications in the development of\nsmart city, we introduce Residential Community Layout Planning (ReCo) Dataset,\nwhich is the first and largest open-source vector dataset related to real-world\ncommunity to date. ReCo Dataset is presented in multiple data formats with\n37,646 residential community layout plans, covering 598,728 residential\nbuildings with height information. ReCo can be conveniently adapted for\nresidential community layout related urban design tasks, e.g., generative\nlayout design, morphological pattern recognition and spatial evaluation. To\nvalidate the utility of ReCo in automated residential community layout\nplanning, a Generative Adversarial Network (GAN) based generative model is\nfurther applied to the dataset. We expect ReCo Dataset to inspire more creative\nand practical work in intelligent design and beyond. The ReCo Dataset is\npublished at: https://www.kaggle.com/fdudsde/reco-dataset.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Xi Chen",
      "Yun Xiong",
      "Siqi Wang",
      "Haofen Wang",
      "Tao Sheng",
      "Yao Zhang",
      "Yu Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04678"
  },
  {
    "id": "arXiv:2206.04679",
    "title": "POODLE: Improving Few-shot Learning via Penalizing Out-of-Distribution  Samples",
    "abstract": "In this work, we propose to use out-of-distribution samples, i.e., unlabeled\nsamples coming from outside the target classes, to improve few-shot learning.\nSpecifically, we exploit the easily available out-of-distribution samples to\ndrive the classifier to avoid irrelevant features by maximizing the distance\nfrom prototypes to out-of-distribution samples while minimizing that of\nin-distribution samples (i.e., support, query data). Our approach is simple to\nimplement, agnostic to feature extractors, lightweight without any additional\ncost for pre-training, and applicable to both inductive and transductive\nsettings. Extensive experiments on various standard benchmarks demonstrate that\nthe proposed method consistently improves the performance of pretrained\nnetworks with different architectures.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021 (First two authors contribute equally)\n",
    "authors": [
      "Duong H. Le",
      "Khoi D. Nguyen",
      "Khoi Nguyen",
      "Quoc-Huy Tran",
      "Rang Nguyen",
      "Binh-Son Hua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04679"
  },
  {
    "id": "arXiv:2206.04685",
    "title": "Predictive Exit: Prediction of Fine-Grained Early Exits for Computation-  and Energy-Efficient Inference",
    "abstract": "By adding exiting layers to the deep learning networks, early exit can\nterminate the inference earlier with accurate results. The passive\ndecision-making of whether to exit or continue the next layer has to go through\nevery pre-placed exiting layer until it exits. In addition, it is also hard to\nadjust the configurations of the computing platforms alongside the inference\nproceeds. By incorporating a low-cost prediction engine, we propose a\nPredictive Exit framework for computation- and energy-efficient deep learning\napplications. Predictive Exit can forecast where the network will exit (i.e.,\nestablish the number of remaining layers to finish the inference), which\neffectively reduces the network computation cost by exiting on time without\nrunning every pre-placed exiting layer. Moreover, according to the number of\nremaining layers, proper computing configurations (i.e., frequency and voltage)\nare selected to execute the network to further save energy. Extensive\nexperimental results demonstrate that Predictive Exit achieves up to 96.2%\ncomputation reduction and 72.9% energy-saving compared with classic deep\nlearning networks; and 12.8% computation reduction and 37.6% energy-saving\ncompared with the early exit under state-of-the-art exiting strategies, given\nthe same inference accuracy and latency.",
    "descriptor": "",
    "authors": [
      "Xiangjie Li",
      "Chenfei Lou",
      "Zhengping Zhu",
      "Yuchi Chen",
      "Yingtao Shen",
      "Yehan Ma",
      "An Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.04685"
  },
  {
    "id": "arXiv:2206.04686",
    "title": "Unsupervised Deep Discriminant Analysis Based Clustering",
    "abstract": "This work presents an unsupervised deep discriminant analysis for clustering.\nThe method is based on deep neural networks and aims to minimize the\nintra-cluster discrepancy and maximize the inter-cluster discrepancy in an\nunsupervised manner. The method is able to project the data into a nonlinear\nlow-dimensional latent space with compact and distinct distribution patterns\nsuch that the data clusters can be effectively identified. We further provide\nan extension of the method such that available graph information can be\neffectively exploited to improve the clustering performance. Extensive\nnumerical results on image and non-image data with or without graph information\ndemonstrate the effectiveness of the proposed methods.",
    "descriptor": "",
    "authors": [
      "Jinyu Cai",
      "Wenzhong Guo",
      "Jicong Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04686"
  },
  {
    "id": "arXiv:2206.04687",
    "title": "Swan: A Neural Engine for Efficient DNN Training on Smartphone SoCs",
    "abstract": "The need to train DNN models on end-user devices (e.g., smartphones) is\nincreasing with the need to improve data privacy and reduce communication\noverheads. Unlike datacenter servers with powerful CPUs and GPUs, modern\nsmartphones consist of a diverse collection of specialized cores following a\nsystem-on-a-chip (SoC) architecture that together perform a variety of tasks.\nWe observe that training DNNs on a smartphone SoC without carefully considering\nits resource constraints can not only lead to suboptimal training performance\nbut significantly affect user experience as well. In this paper, we present\nSwan, a neural engine to optimize DNN training on smartphone SoCs without\nhurting user experience. Extensive large-scale evaluations show that Swan can\nimprove performance by 1.2 - 23.3x over the state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Sanjay Sri Vallabh Singapuram",
      "Fan Lai",
      "Chuheng Hu",
      "Mosharaf Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04687"
  },
  {
    "id": "arXiv:2206.04688",
    "title": "NNTrainer: Light-Weight On-Device Training Framework",
    "abstract": "Modern consumer electronic devices have adopted deep learning-based\nintelligence services for their key features. Vendors have recently started to\nexecute intelligence services on devices to preserve personal data in devices,\nreduce network and cloud costs. We find such a trend as the opportunity to\npersonalize intelligence services by updating neural networks with user data\nwithout exposing the data out of devices: on-device training. For example, we\nmay add a new class, my dog, Alpha, for robotic vacuums, adapt speech\nrecognition for the users accent, let text-to-speech speak as if the user\nspeaks. However, the resource limitations of target devices incur significant\ndifficulties. We propose NNTrainer, a light-weight on-device training\nframework. We describe optimization techniques for neural networks implemented\nby NNTrainer, which are evaluated along with the conventional. The evaluations\nshow that NNTrainer can reduce memory consumption down to 1/28 without\ndeteriorating accuracy or training time and effectively personalizes\napplications on devices. NNTrainer is cross-platform and practical open source\nsoftware, which is being deployed to millions of devices in the authors\naffiliation.",
    "descriptor": "",
    "authors": [
      "Ji Joong Moon",
      "Parichay Kapoor",
      "Ji Hoon Lee",
      "Myung Joo Ham",
      "Hyun Suk Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04688"
  },
  {
    "id": "arXiv:2206.04710",
    "title": "Discussion of Quantum Consensus Algorithms",
    "abstract": "Leader election is a crucial process in many areas such as cloud computing,\ndistributed systems, task orchestration, and blockchain. Oftentimes, in a\ndistributed system, the network needs to choose a leader, which would be\nresponsible for synchronization between different processors, data storage,\ninformation distribution, and more.",
    "descriptor": "",
    "authors": [
      "Lifu Zhang",
      "Samuel Fulton"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04710"
  },
  {
    "id": "arXiv:2206.04713",
    "title": "A Resilient Distributed Boosting Algorithm",
    "abstract": "Given a learning task where the data is distributed among several parties,\ncommunication is one of the fundamental resources which the parties would like\nto minimize. We present a distributed boosting algorithm which is resilient to\na limited amount of noise. Our algorithm is similar to classical boosting\nalgorithms, although it is equipped with a new component, inspired by\nImpagliazzo's hard-core lemma \\cite{impagliazzo1995hard}, adding a robustness\nquality to the algorithm. We also complement this result by showing that\nresilience to any asymptotically larger noise is not achievable by a\ncommunication-efficient algorithm.",
    "descriptor": "",
    "authors": [
      "Yuval Filmus",
      "Idan Mehalel",
      "Shay Moran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.04713"
  },
  {
    "id": "arXiv:2206.04716",
    "title": "Gender and Robots: A Literature Review",
    "abstract": "Here, I ask what we can learn about how how gender affects how people engage\nwith robots. I review 46 empirical studies of social robots, published 2018 or\nearlier, which report on the gender of their participants or the perceived or\nintended gender of the robot, or both, and perform some analysis with respect\nto either participant or robot gender. From these studies, I find that robots\nare by default perceived as male, that robots absorb human gender stereotypes,\nand that men tend to engage with robots more than women. I highlight open\nquestions about how such gender effects may be different in younger\nparticipants, and whether one should seek to match the gender of the robot to\nthe gender of the participant to ensure positive interaction outcomes.\nI conclude by suggesting that future research should: include gender diverse\nparticipant pools, include non-binary participants, rely on self-identification\nfor discerning gender rather than researcher perception, control for known\ncovariates of gender, test for different study outcomes with respect to gender,\nand test whether the robot used was perceived as gendered by participants. I\ninclude an appendix with a narrative summary of gender-relevant findings from\neach of the 46 papers to aid in future literature reviews.",
    "descriptor": "",
    "authors": [
      "David Gray Widder"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04716"
  },
  {
    "id": "arXiv:2206.04720",
    "title": "How Algorithms Shape the Distribution of Political Advertising: Case  Studies of Facebook, Google, and TikTok",
    "abstract": "Online platforms play an increasingly important role in shaping democracy by\ninfluencing the distribution of political information to the electorate. In\nrecent years, political campaigns have spent heavily on the platforms'\nalgorithmic tools to target voters with online advertising. While the public\ninterest in understanding how platforms perform the task of shaping the\npolitical discourse has never been higher, the efforts of the major platforms\nto make the necessary disclosures to understand their practices falls woefully\nshort. In this study, we collect and analyze a dataset containing over 800,000\nads and 2.5 million videos about the 2020 U.S. presidential election from\nFacebook, Google, and TikTok. We conduct the first large scale data analysis of\npublic data to critically evaluate how these platforms amplified or moderated\nthe distribution of political advertisements. We conclude with recommendations\nfor how to improve the disclosures so that the public can hold the platforms\nand political advertisers accountable.",
    "descriptor": "\nComments: Forthcoming in: Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES'22), August 1-3, 2022, Oxford, United Kingdom. ACM, New York, NY, USA, 15 pages\n",
    "authors": [
      "Orestis Papakyriakopoulos",
      "Christelle Tessono",
      "Arvind Narayanan",
      "Mihir Kshirsagar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04720"
  },
  {
    "id": "arXiv:2206.04723",
    "title": "On the Unreasonable Effectiveness of Federated Averaging with  Heterogeneous Data",
    "abstract": "Existing theory predicts that data heterogeneity will degrade the performance\nof the Federated Averaging (FedAvg) algorithm in federated learning. However,\nin practice, the simple FedAvg algorithm converges very well. This paper\nexplains the seemingly unreasonable effectiveness of FedAvg that contradicts\nthe previous theoretical predictions. We find that the key assumption of\nbounded gradient dissimilarity in previous theoretical analyses is too\npessimistic to characterize data heterogeneity in practical applications. For a\nsimple quadratic problem, we demonstrate there exist regimes where large\ngradient dissimilarity does not have any negative impact on the convergence of\nFedAvg. Motivated by this observation, we propose a new quantity, average drift\nat optimum, to measure the effects of data heterogeneity, and explicitly use it\nto present a new theoretical analysis of FedAvg. We show that the average drift\nat optimum is nearly zero across many real-world federated training tasks,\nwhereas the gradient dissimilarity can be large. And our new analysis suggests\nFedAvg can have identical convergence rates in homogeneous and heterogeneous\ndata settings, and hence, leads to better understanding of its empirical\nsuccess.",
    "descriptor": "",
    "authors": [
      "Jianyu Wang",
      "Rudrajit Das",
      "Gauri Joshi",
      "Satyen Kale",
      "Zheng Xu",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04723"
  },
  {
    "id": "arXiv:2206.04724",
    "title": "Modular design patterns for neural-symbolic integration: refinement and  combination",
    "abstract": "We formalise some aspects of the neural-symbol design patterns of van Bekkum\net al., such that we can formally define notions of refinement of patterns, as\nwell as modular combination of larger patterns from smaller building blocks.\nThese formal notions are being implemented in the heterogeneous tool set\n(Hets), such that patterns and refinements can be checked for well-formedness,\nand combinations can be computed.",
    "descriptor": "",
    "authors": [
      "Till Mossakowski"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04724"
  },
  {
    "id": "arXiv:2206.04726",
    "title": "COSTA: Covariance-Preserving Feature Augmentation for Graph Contrastive  Learning",
    "abstract": "Graph contrastive learning (GCL) improves graph representation learning,\nleading to SOTA on various downstream tasks. The graph augmentation step is a\nvital but scarcely studied step of GCL. In this paper, we show that the node\nembedding obtained via the graph augmentations is highly biased, somewhat\nlimiting contrastive models from learning discriminative features for\ndownstream tasks.Thus, instead of investigating graph augmentation in the input\nspace, we alternatively propose to perform augmentations on the hidden features\n(feature augmentation). Inspired by so-called matrix sketching, we propose\nCOSTA, a novel COvariance-preServing feaTure space Augmentation framework for\nGCL, which generates augmented features by maintaining a ``good sketch'' of\noriginal features. To highlight the superiority of feature augmentation with\nCOSTA, we investigate a single-view setting (in addition to multi-view one)\nwhich conserves memory and computations. We show that the feature augmentation\nwith COSTA achieves comparable/better results than graph augmentation based\nmodels.",
    "descriptor": "\nComments: This paper is accepted by the ACM KDD 2022\n",
    "authors": [
      "Yifei Zhang",
      "Hao Zhu",
      "Zixing Song",
      "Piotr Koniusz",
      "Iriwn King"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04726"
  },
  {
    "id": "arXiv:2206.04728",
    "title": "Towards Target Sequential Rules",
    "abstract": "In many real-world applications, sequential rule mining (SRM) can provide\nprediction and recommendation functions for a variety of services. It is an\nimportant technique of pattern mining to discover all valuable rules that\nbelong to high-frequency and high-confidence sequential rules. Although several\nalgorithms of SRM are proposed to solve various practical problems, there are\nno studies on target sequential rules. Targeted sequential rule mining aims at\nmining the interesting sequential rules that users focus on, thus avoiding the\ngeneration of other invalid and unnecessary rules. This approach can further\nimprove the efficiency of users in analyzing rules and reduce the consumption\nof data resources. In this paper, we provide the relevant definitions of target\nsequential rule and formulate the problem of targeted sequential rule mining.\nFurthermore, we propose an efficient algorithm, called targeted sequential rule\nmining (TaSRM). Several pruning strategies and an optimization are introduced\nto improve the efficiency of TaSRM. Finally, a large number of experiments are\nconducted on different benchmarks, and we analyze the results in terms of their\nrunning time, memory consumption, and scalability, as well as query cases with\ndifferent query rules. It is shown that the novel algorithm TaSRM and its\nvariants can achieve better experimental performance compared to the existing\nbaseline algorithm.",
    "descriptor": "\nComments: Preprint. 6 figures, 3 tables\n",
    "authors": [
      "Wensheng Gan",
      "Gengsen Huang",
      "Jian Weng",
      "Tianlong Gu",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04728"
  },
  {
    "id": "arXiv:2206.04730",
    "title": "A Neural Network Architecture for Program Understanding Inspired by  Human Behaviors",
    "abstract": "Program understanding is a fundamental task in program language processing.\nDespite the success, existing works fail to take human behaviors as reference\nin understanding programs. In this paper, we consider human behaviors and\npropose the PGNN-EK model that consists of two main components. On the one\nhand, inspired by the \"divide-and-conquer\" reading behaviors of humans, we\npresent a partitioning-based graph neural network model PGNN on the upgraded\nAST of codes. On the other hand, to characterize human behaviors of resorting\nto other resources to help code comprehension, we transform raw codes with\nexternal knowledge and apply pre-training techniques for information\nextraction. Finally, we combine the two embeddings generated from the two\ncomponents to output code embeddings. We conduct extensive experiments to show\nthe superior performance of PGNN-EK on the code summarization and code clone\ndetection tasks. In particular, to show the generalization ability of our\nmodel, we release a new dataset that is more challenging for code clone\ndetection and could advance the development of the community. Our codes and\ndata are publicly available at https://github.com/RecklessRonan/PGNN-EK.",
    "descriptor": "",
    "authors": [
      "Renyu Zhu",
      "Lei Yuan",
      "Xiang Li",
      "Ming Gao",
      "Wenyuan Cai"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04730"
  },
  {
    "id": "arXiv:2206.04731",
    "title": "Leveraging Centric Data Federated Learning Using Blockchain For  Integrity Assurance",
    "abstract": "Machine learning abilities have become a vital component for various\nsolutions across industries, applications, and sectors. Many organizations seek\nto leverage AI-based solutions across their business services to unlock better\nefficiency and increase productivity. Problems, however, can arise if there is\na lack of quality data for AI-model training, scalability, and maintenance. We\npropose a data-centric federated learning architecture leveraged by a public\nblockchain and smart contracts to overcome this significant issue. Our proposed\nsolution provides a virtual public marketplace where developers, data\nscientists, and AI-engineer can publish their models and collaboratively create\nand access quality data for training. We enhance data quality and integrity\nthrough an incentive mechanism that rewards contributors for data contribution\nand verification. Those combined with the proposed framework helped increase\nwith only one user simulation the training dataset with an average of 100 input\ndaily and the model accuracy by approximately 4\\%.",
    "descriptor": "\nComments: Published at International Workshop on Trustable, Verifiable and Auditable Federated Learning in Conjunction with AAAI 2022 (FL-AAAI-22) this https URL\n",
    "authors": [
      "Riadh Ben Chaabene",
      "Darine Amayed",
      "Mohamed Cheriet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04731"
  },
  {
    "id": "arXiv:2206.04734",
    "title": "Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel  Recombination",
    "abstract": "Calculation of Bayesian posteriors and model evidences typically requires\nnumerical integration. Bayesian quadrature (BQ), a surrogate-model-based\napproach to numerical integration, is capable of superb sample efficiency, but\nits lack of parallelisation has hindered its practical applications. In this\nwork, we propose a parallelised (batch) BQ method, employing techniques from\nkernel quadrature, that possesses a provably-exponential convergence rate.\nAdditionally, just as with Nested Sampling, our method permits simultaneous\ninference of both posteriors and model evidence. Samples from our BQ surrogate\nmodel are re-selected to give a sparse set of samples, via a kernel\nrecombination algorithm, requiring negligible additional time to increase the\nbatch size. Empirically, we find that our approach significantly outperforms\nthe sampling efficiency of both state-of-the-art BQ techniques and Nested\nSampling in various real-world datasets, including lithium-ion battery\nanalytics.",
    "descriptor": "\nComments: 28 pages, 4 figures\n",
    "authors": [
      "Masaki Adachi",
      "Satoshi Hayakawa",
      "Martin J\u00f8rgensen",
      "Harald Oberhauser",
      "Michael A. Osborne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04734"
  },
  {
    "id": "arXiv:2206.04736",
    "title": "A Novel Partitioned Approach for Reduced Order Model -- Finite Element  Model (ROM-FEM) and ROM-ROM Coupling",
    "abstract": "Partitioned methods allow one to build a simulation capability for coupled\nproblems by reusing existing single-component codes. In so doing, partitioned\nmethods can shorten code development and validation times for multiphysics and\nmultiscale applications. In this work, we consider a scenario in which one or\nmore of the \"codes\" being coupled are projection-based reduced order models\n(ROMs), introduced to lower the computational cost associated with a particular\ncomponent. We simulate this scenario by considering a model interface problem\nthat is discretized independently on two non-overlapping subdomains. We then\nformulate a partitioned scheme for this problem that allows the coupling\nbetween a ROM \"code\" for one of the subdomains with a finite element model\n(FEM) or ROM \"code\" for the other subdomain. The ROM \"codes\" are constructed by\nperforming proper orthogonal decomposition (POD) on a snapshot ensemble to\nobtain a low-dimensional reduced order basis, followed by a Galerkin projection\nonto this basis. The ROM and/or FEM \"codes\" on each subdomain are then coupled\nusing a Lagrange multiplier representing the interface flux. To partition the\nresulting monolithic problem, we first eliminate the flux through a dual Schur\ncomplement. Application of an explicit time integration scheme to the\ntransformed monolithic problem decouples the subdomain equations, allowing\ntheir independent solution for the next time step. We show numerical results\nthat demonstrate the proposed method's efficacy in achieving both ROM-FEM and\nROM-ROM coupling.",
    "descriptor": "",
    "authors": [
      "Amy de Castro",
      "Paul Kuberry",
      "Irina Tezaur",
      "Pavel Bochev"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04736"
  },
  {
    "id": "arXiv:2206.04737",
    "title": "Outsider Oversight: Designing a Third Party Audit Ecosystem for AI  Governance",
    "abstract": "Much attention has focused on algorithmic audits and impact assessments to\nhold developers and users of algorithmic systems accountable. But existing\nalgorithmic accountability policy approaches have neglected the lessons from\nnon-algorithmic domains: notably, the importance of interventions that allow\nfor the effective participation of third parties. Our paper synthesizes lessons\nfrom other fields on how to craft effective systems of external oversight for\nalgorithmic deployments. First, we discuss the challenges of third party\noversight in the current AI landscape. Second, we survey audit systems across\ndomains - e.g., financial, environmental, and health regulation - and show that\nthe institutional design of such audits are far from monolithic. Finally, we\nsurvey the evidence base around these design components and spell out the\nimplications for algorithmic auditing. We conclude that the turn toward audits\nalone is unlikely to achieve actual algorithmic accountability, and sustained\nfocus on institutional design will be required for meaningful third party\ninvolvement.",
    "descriptor": "\nComments: Presented at 5th Annual ACM/AAAI AI Ethics and Society (AIES) conference\n",
    "authors": [
      "Inioluwa Deborah Raji",
      "Peggy Xu",
      "Colleen Honigsberg",
      "Daniel E. Ho"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.04737"
  },
  {
    "id": "arXiv:2206.04739",
    "title": "I'm Me, We're Us, and I'm Us: Tri-directional Contrastive Learning on  Hypergraphs",
    "abstract": "Although machine learning on hypergraphs has attracted considerable\nattention, most of the works have focused on (semi-)supervised learning, which\nmay cause heavy labeling costs and poor generalization. Recently, contrastive\nlearning has emerged as a successful unsupervised representation learning\nmethod. Despite the prosperous development of contrastive learning in other\ndomains, contrastive learning on hypergraphs remains little explored. In this\npaper, we propose TriCon (Tri-directional Contrastive learning), a general\nframework for contrastive learning on hypergraphs. Its main idea is\ntri-directional contrast, and specifically, it aims to maximize in two\naugmented views the agreement (a) between the same node, (b) between the same\ngroup of nodes, and (c) between each group and its members. Together with\nsimple but surprisingly effective data augmentation and negative sampling\nschemes, these three forms of contrast enable TriCon to capture both\nmicroscopic and mesoscopic structural information in node embeddings. Our\nextensive experiments using 13 baseline approaches, five datasets, and two\ntasks demonstrate the effectiveness of TriCon, and most noticeably, TriCon\nconsistently outperforms not just unsupervised competitors but also\n(semi-)supervised competitors mostly by significant margins for node\nclassification.",
    "descriptor": "",
    "authors": [
      "Dongjin Lee",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04739"
  },
  {
    "id": "arXiv:2206.04740",
    "title": "A Learning-Theoretic Framework for Certified Auditing of Machine  Learning Models",
    "abstract": "Responsible use of machine learning requires that models be audited for\nundesirable properties. However, how to do principled auditing in a general\nsetting has remained ill-understood. In this paper, we propose a formal\nlearning-theoretic framework for auditing. We propose algorithms for auditing\nlinear classifiers for feature sensitivity using label queries as well as\ndifferent kinds of explanations, and provide performance guarantees. Our\nresults illustrate that while counterfactual explanations can be extremely\nhelpful for auditing, anchor explanations may not be as beneficial in the worst\ncase.",
    "descriptor": "",
    "authors": [
      "Chhavi Yadav",
      "Michal Moshkovitz",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04740"
  },
  {
    "id": "arXiv:2206.04742",
    "title": "Mobility Improves the Convergence of Asynchronous Federated Learning",
    "abstract": "This paper studies asynchronous Federated Learning (FL) subject to clients'\nindividual arbitrary communication patterns with the parameter server. We\npropose FedMobile, a new asynchronous FL algorithm that exploits the mobility\nattribute of the mobile FL system to improve the learning performance. The key\nidea is to leverage the random client-to-client communication in a mobile\nnetwork to create additional indirect communication opportunities with the\nserver via upload and download relaying. We prove that FedMobile achieves a\nconvergence rate $O(\\frac{1}{\\sqrt{NT}})$, where $N$ is the number of clients\nand $T$ is the number of communication slots, and show that the optimal design\ninvolves an interesting trade-off on the best timing of relaying. Our analysis\nsuggests that with an increased level of mobility, asynchronous FL converges\nfaster using FedMobile. Experiment results on a synthetic dataset and two\nreal-world datasets verify our theoretical findings.",
    "descriptor": "\nComments: 11 pages, preprint version\n",
    "authors": [
      "Jieming Bian",
      "Jie Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04742"
  },
  {
    "id": "arXiv:2206.04743",
    "title": "Strong Memory Lower Bounds for Learning Natural Models",
    "abstract": "We give lower bounds on the amount of memory required by one-pass streaming\nalgorithms for solving several natural learning problems. In a setting where\nexamples lie in $\\{0,1\\}^d$ and the optimal classifier can be encoded using\n$\\kappa$ bits, we show that algorithms which learn using a near-minimal number\nof examples, $\\tilde O(\\kappa)$, must use $\\tilde \\Omega( d\\kappa)$ bits of\nspace. Our space bounds match the dimension of the ambient space of the\nproblem's natural parametrization, even when it is quadratic in the size of\nexamples and the final classifier. For instance, in the setting of $d$-sparse\nlinear classifiers over degree-2 polynomial features, for which\n$\\kappa=\\Theta(d\\log d)$, our space lower bound is $\\tilde\\Omega(d^2)$. Our\nbounds degrade gracefully with the stream length $N$, generally having the form\n$\\tilde\\Omega\\left(d\\kappa \\cdot \\frac{\\kappa}{N}\\right)$.\nBounds of the form $\\Omega(d\\kappa)$ were known for learning parity and other\nproblems defined over finite fields. Bounds that apply in a narrow range of\nsample sizes are also known for linear regression. Ours are the first such\nbounds for problems of the type commonly seen in recent learning applications\nthat apply for a large range of input sizes.",
    "descriptor": "\nComments: 39 Pages. To appear at COLT 2022\n",
    "authors": [
      "Gavin Brown",
      "Mark Bun",
      "Adam Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04743"
  },
  {
    "id": "arXiv:2206.04744",
    "title": "The Developers' Design Thinking Toolbox in Hackathons: A Study on the  Recurring Design Methods in Software Development Marathons",
    "abstract": "Hackathons are time-bounded collaborative events of intense teamwork to build\nprototypes usually in the form of software, aiming to specific challenges\nproposed by the organizers. These events became a widespread practice in the IT\nindustry, universities and many other scenarios, as a result of a growing\nopen-innovation trend in the last decade. Since the main deliverable of these\nevents is a demonstrable version of an idea, such as early hardware or software\nprototypes, the short time frame requires participants to quickly understand\nthe proposed challenge or even identify issues related to a given domain. To\ncreate solutions, teams follow an ad-hoc but effective design approach, that\nmany times seems informal since the background of the participants is rather\ncentered on technical aspects (e.g., web and mobile programming) and does not\ninvolve any training in Design Thinking.\nTo understand this creative process, we conducted 37 interviews (32\nhackathons winners and 5 hackathon organizers) with people from 16 countries.\nWe aimed to identify the design processes and recurring design methods applied\nby winners in these events. Also, we conducted a focus group with 8 people\nexperienced in hackathons (participants and organizers) to discuss our\nfindings. Our analysis revealed that although hackathon winners with IT\nbackground have no formal training on Design Thinking, they are aware of many\ndesign methods, typically following a sequence of phases that involve divergent\nand convergent thinking to explore the problem space and propose alternatives\nin a solution space, which is the rationale behind Design Thinking. We derived\na set of recommendations based on design strategies that seem to lead to\nsuccessful hackathon participation. These recommendations can also be useful to\norganizers who intend to enhance the experience of newcomers in hackathons.",
    "descriptor": "\nComments: To be published in the International Journal of Human-Computer Interaction (Taylor and Francis)\n",
    "authors": [
      "Kiev Gama",
      "George Valen\u00e7a",
      "Pedro Alessio",
      "Rafael Formiga",
      "Andr\u00e9 Neves",
      "Nycolas Lacerda"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04744"
  },
  {
    "id": "arXiv:2206.04745",
    "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning (RL) defines the task of learning from a\nstatic logged dataset without continually interacting with the environment. The\ndistribution shift between the learned policy and the behavior policy makes it\nnecessary for the value function to stay conservative such that\nout-of-distribution (OOD) actions will not be severely overestimated. However,\nexisting approaches, penalizing the unseen actions or regularizing with the\nbehavior policy, are too pessimistic, which suppresses the generalization of\nthe value function and hinders the performance improvement. This paper explores\nmild but enough conservatism for offline learning while not harming\ngeneralization. We propose Mildly Conservative Q-learning (MCQ), where OOD\nactions are actively trained by assigning them proper pseudo Q values. We\ntheoretically show that MCQ induces a policy that behaves at least as well as\nthe behavior policy and no erroneous overestimation will occur for OOD actions.\nExperimental results on the D4RL benchmarks demonstrate that MCQ achieves\nremarkable performance compared with prior work. Furthermore, MCQ shows\nsuperior generalization ability when transferring from offline to online, and\nsignificantly outperforms baselines.",
    "descriptor": "",
    "authors": [
      "Jiafei Lyu",
      "Xiaoteng Ma",
      "Xiu Li",
      "Zongqing Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04745"
  },
  {
    "id": "arXiv:2206.04746",
    "title": "HDTorch: Accelerating Hyperdimensional Computing with GP-GPUs for Design  Space Exploration",
    "abstract": "HyperDimensional Computing (HDC) as a machine learning paradigm is highly\ninteresting for applications involving continuous, semi-supervised learning for\nlong-term monitoring. However, its accuracy is not yet on par with other\nMachine Learning (ML) approaches. Frameworks enabling fast design space\nexploration to find practical algorithms are necessary to make HD computing\ncompetitive with other ML techniques. To this end, we introduce HDTorch, an\nopen-source, PyTorch-based HDC library with CUDA extensions for hypervector\noperations. We demonstrate HDTorch's utility by analyzing four HDC benchmark\ndatasets in terms of accuracy, runtime, and memory consumption, utilizing both\nclassical and online HD training methodologies. We demonstrate average\n(training)/inference speedups of (111x/68x)/87x for classical/online HD,\nrespectively. Moreover, we analyze the effects of varying hyperparameters on\nruntime and accuracy. Finally, we demonstrate how HDTorch enables exploration\nof HDC strategies applied to large, real-world datasets. We perform the\nfirst-ever HD training and inference analysis of the entirety of the CHB-MIT\nEEG epilepsy database. Results show that the typical approach of training on a\nsubset of the data does not necessarily generalize to the entire dataset, an\nimportant factor when developing future HD models for medical wearable devices.",
    "descriptor": "\nComments: Submitted to the ICCAD 2022 conference (23.5.2022.)\n",
    "authors": [
      "William Andrew Simon",
      "Una Pale",
      "Tomas Teijeiro",
      "David Atienza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04746"
  },
  {
    "id": "arXiv:2206.04751",
    "title": "Defending Compositionality in Emergent Languages",
    "abstract": "Compositionality has traditionally been understood as a major factor in\nproductivity of language and, more broadly, human cognition. Yet, recently,\nsome research started to question its status, showing that artificial neural\nnetworks are good at generalization even without noticeable compositional\nbehavior. We argue that some of these conclusions are too strong and/or\nincomplete. In the context of a two-agent communication game, we show that\ncompositionality indeed seems essential for successful generalization when the\nevaluation is done on a proper dataset.",
    "descriptor": "\nComments: Accepted to NAACL SRW 22\n",
    "authors": [
      "Michal Auersperger",
      "Pavel Pecina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04751"
  },
  {
    "id": "arXiv:2206.04754",
    "title": "AFIA: ATPG-Guided Fault Injection Attack on Secure Logic Locking",
    "abstract": "The outsourcing of the design and manufacturing of integrated circuits has\nraised severe concerns about the piracy of Intellectual Properties and illegal\noverproduction. Logic locking has emerged as an obfuscation technique to\nprotect outsourced chip designs, where the circuit netlist is locked and can\nonly be functional once a secure key is programmed. However, Boolean\nSatisfiability-based attacks have shown to break logic locking, simultaneously\nmotivating researchers to develop more secure countermeasures. In this paper,\nwe present a novel fault injection attack to break any locking technique that\nrelies on a stored secret key, and denote this attack as AFIA, ATPG-guided\nFault Injection Attack. The proposed attack is based on sensitizing a key bit\nto the primary output while injecting faults at a few other key lines that\nblock the propagation of the targeted key bit. AIFA is very effective in\ndetermining a key bit as there exists a stuck-at fault pattern that detects a\nstuck-at 1 (or stuck-at 0) fault at any key line. The average complexity of\nnumber of injected faults for AFIA is linear with the key size and requires\nonly |K| test patterns to determine a secret key, K. AFIA requires a fewer\nnumber of injected faults to sensitize a bit to the primary output, compared to\n2|K|-1 faults for the differential fault analysis attack [26].",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2007.10512\n",
    "authors": [
      "Yadi Zhong",
      "Ujjwal Guin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04754"
  },
  {
    "id": "arXiv:2206.04756",
    "title": "An Empirical Study on Disentanglement of Negative-free Contrastive  Learning",
    "abstract": "Negative-free contrastive learning has attracted a lot of attention with\nsimplicity and impressive performance for large-scale pretraining. But its\ndisentanglement property remains unexplored. In this paper, we take different\nnegative-free contrastive learning methods to study the disentanglement\nproperty of this genre of self-supervised methods empirically. We find the\nexisting disentanglement metrics fail to make meaningful measurements for the\nhigh-dimensional representation model so we propose a new disentanglement\nmetric based on Mutual Information between representation and data factors.\nWith the proposed metric, we benchmark the disentanglement property of\nnegative-free contrastive learning for the first time, on both popular\nsynthetic datasets and a real-world dataset CelebA. Our study shows that the\ninvestigated methods can learn a well-disentangled subset of representation. We\nextend the study of the disentangled representation learning to\nhigh-dimensional representation space and negative-free contrastive learning\nfor the first time. The implementation of the proposed metric is available at\n\\url{https://github.com/noahcao/disentanglement_lib_med}.",
    "descriptor": "\nComments: Implementation available at this https URL\n",
    "authors": [
      "Jinkun Cao",
      "Ruiqian Nai",
      "Qing Yang",
      "Jialei Huang",
      "Yang Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04756"
  },
  {
    "id": "arXiv:2206.04762",
    "title": "Data-Efficient Double-Win Lottery Tickets from Robust Pre-training",
    "abstract": "Pre-training serves as a broadly adopted starting point for transfer learning\non various downstream tasks. Recent investigations of lottery tickets\nhypothesis (LTH) demonstrate such enormous pre-trained models can be replaced\nby extremely sparse subnetworks (a.k.a. matching subnetworks) without\nsacrificing transferability. However, practical security-crucial applications\nusually pose more challenging requirements beyond standard transfer, which also\ndemand these subnetworks to overcome adversarial vulnerability. In this paper,\nwe formulate a more rigorous concept, Double-Win Lottery Tickets, in which a\nlocated subnetwork from a pre-trained model can be independently transferred on\ndiverse downstream tasks, to reach BOTH the same standard and robust\ngeneralization, under BOTH standard and adversarial training regimes, as the\nfull pre-trained model can do. We comprehensively examine various pre-training\nmechanisms and find that robust pre-training tends to craft sparser double-win\nlottery tickets with superior performance over the standard counterparts. For\nexample, on downstream CIFAR-10/100 datasets, we identify double-win matching\nsubnetworks with the standard, fast adversarial, and adversarial pre-training\nfrom ImageNet, at 89.26%/73.79%, 89.26%/79.03%, and 91.41%/83.22% sparsity,\nrespectively. Furthermore, we observe the obtained double-win lottery tickets\ncan be more data-efficient to transfer, under practical data-limited (e.g., 1%\nand 10%) downstream schemes. Our results show that the benefits from robust\npre-training are amplified by the lottery ticket scheme, as well as the\ndata-limited transfer setting. Codes are available at\nhttps://github.com/VITA-Group/Double-Win-LTH.",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Sijia Liu",
      "Yang Zhang",
      "Shiyu Chang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04762"
  },
  {
    "id": "arXiv:2206.04763",
    "title": "Neural Bregman Divergences for Distance Learning",
    "abstract": "Many metric learning tasks, such as triplet learning, nearest neighbor\nretrieval, and visualization, are treated primarily as embedding tasks where\nthe ultimate metric is some variant of the Euclidean distance (e.g., cosine or\nMahalanobis), and the algorithm must learn to embed points into the pre-chosen\nspace. The study of non-Euclidean geometries or appropriateness is often not\nexplored, which we believe is due to a lack of tools for learning non-Euclidean\nmeasures of distance. Under the belief that the use of asymmetric methods in\nparticular have lacked sufficient study, we propose a new approach to learning\narbitrary Bergman divergences in a differentiable manner via input convex\nneural networks. Over a set of both new and previously studied tasks, including\nasymmetric regression, ranking, and clustering, we demonstrate that our method\nmore faithfully learns divergences than prior Bregman learning approaches. In\ndoing so we obtain the first method for learning neural Bregman divergences and\nwith it inherit the many nice mathematical properties of Bregman divergences,\nproviding the foundation and tooling for better developing and studying\nasymmetric distance learning.",
    "descriptor": "",
    "authors": [
      "Fred Lu",
      "Edward Raff",
      "Francis Ferraro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04763"
  },
  {
    "id": "arXiv:2206.04767",
    "title": "A Programmatic Definition of Visualization Tasks, Insights, and  Objectives",
    "abstract": "Researchers have developed several theoretical models for identifying and\ncategorizing data analysis tasks for visualization systems. However, these\nmodels focus primarily on abstraction or generalizing specific tasks into\nhigher-level concepts, resulting in broad guidelines that are not always\nstraightforward to implement within visualization systems. Few models flow in\nthe opposite direction to enable instantiation or a precise approach to\napplying high-level task concepts to specific analysis scenarios or user\ninteraction logs. This paper presents a synthesis of existing task theory into\na new instantiation-focused model and Pyxis, a specification language for\napplying this model to existing evaluation methods. Specifically, Pyxis enables\nresearchers to dissect theoretical and study-driven analysis sessions to\nidentify instances of tasks that users have performed. Further, it formalizes\nthe relationship between tasks, insights, and objectives implied in prior work.\nWe present three use cases that apply Pyxis to a wide range of analysis\nscenarios from the literature to demonstrate its utility. Finally, we discuss\nthe model's implications and opportunities for future work.",
    "descriptor": "",
    "authors": [
      "Leilani Battle",
      "Alvitta Ottley"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04767"
  },
  {
    "id": "arXiv:2206.04769",
    "title": "CLAP: Learning Audio Concepts From Natural Language Supervision",
    "abstract": "Mainstream Audio Analytics models are trained to learn under the paradigm of\none class label to many recordings focusing on one task. Learning under such\nrestricted supervision limits the flexibility of models because they require\nlabeled audio for training and can only predict the predefined categories.\nInstead, we propose to learn audio concepts from natural language supervision.\nWe call our approach Contrastive Language-Audio Pretraining (CLAP), which\nlearns to connect language and audio by using two encoders and a contrastive\nlearning to bring audio and text descriptions into a joint multimodal space. We\ntrained CLAP with 128k audio and text pairs and evaluated it on 16 downstream\ntasks across 8 domains, such as Sound Event Classification, Music tasks, and\nSpeech-related tasks. Although CLAP was trained with significantly less pairs\nthan similar computer vision models, it establishes SoTA for Zero-Shot\nperformance. Additionally, we evaluated CLAP in a supervised learning setup and\nachieve SoTA in 5 tasks. Hence, CLAP's Zero-Shot capability removes the need of\ntraining with class labels, enables flexible class prediction at inference\ntime, and generalizes to multiple downstream tasks.",
    "descriptor": "",
    "authors": [
      "Benjamin Elizalde",
      "Soham Deshmukh",
      "Mahmoud Al Ismail",
      "Huaming Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04769"
  },
  {
    "id": "arXiv:2206.04771",
    "title": "Joint Entropy Search For Maximally-Informed Bayesian Optimization",
    "abstract": "Information-theoretic Bayesian optimization techniques have become popular\nfor optimizing expensive-to-evaluate black-box functions due to their\nnon-myopic qualities. Entropy Search and Predictive Entropy Search both\nconsider the entropy over the optimum in the input space, while the recent\nMax-value Entropy Search considers the entropy over the optimal value in the\noutput space. We propose Joint Entropy Search (JES), a novel\ninformation-theoretic acquisition function that considers an entirely new\nquantity, namely the entropy over the joint optimal probability density over\nboth input and output space. To incorporate this information, we consider the\nreduction in entropy from conditioning on fantasized optimal input/output\npairs. The resulting approach primarily relies on standard GP machinery and\nremoves complex approximations typically associated with information-theoretic\nmethods. With minimal computational overhead, JES shows superior\ndecision-making, and yields state-of-the-art performance for\ninformation-theoretic approaches across a wide suite of tasks. As a\nlight-weight approach with superior results, JES provides a new go-to\nacquisition function for Bayesian optimization.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Carl Hvarfner",
      "Frank Hutter",
      "Luigi Nardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04771"
  },
  {
    "id": "arXiv:2206.04774",
    "title": "An approximation algorithm for random generation of capacities",
    "abstract": "Capacities on a finite set are sets functions vanishing on the empty set and\nbeing monotonic w.r.t. inclusion. Since the set of capacities is an order\npolytope, the problem of randomly generating capacities amounts to generating\nall linear extensions of the Boolean lattice. This problem is known to be\nintractable even as soon as $n>5$, therefore approximate methods have been\nproposed, most notably one based on Markov chains. Although quite accurate,\nthis method is time consuming. In this paper, we propose the 2-layer\napproximation method, which generates a subset of linear extensions,\neliminating those with very low probability. We show that our method has\nsimilar performance compared to the Markov chain but is much less time\nconsuming.",
    "descriptor": "",
    "authors": [
      "Michel Grabisch",
      "Christophe Labreuche",
      "Peiqi Sun"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.04774"
  },
  {
    "id": "arXiv:2206.04776",
    "title": "What should AI see? Using the Public's Opinion to Determine the  Perception of an AI",
    "abstract": "Deep neural networks (DNN) have made impressive progress in the\ninterpretation of image data, so that it is conceivable and to some degree\nrealistic to use them in safety critical applications like automated driving.\nFrom an ethical standpoint, the AI algorithm should take into account the\nvulnerability of objects or subjects on the street that ranges from \"not at\nall\", e.g. the road itself, to \"high vulnerability\" of pedestrians. One way to\ntake this into account is to define the cost of confusion of one semantic\ncategory with another and use cost-based decision rules for the interpretation\nof probabilities, which are the output of DNNs. However, it is an open problem\nhow to define the cost structure, who should be in charge to do that, and\nthereby define what AI-algorithms will actually \"see\". As one possible answer,\nwe follow a participatory approach and set up an online survey to ask the\npublic to define the cost structure. We present the survey design and the data\nacquired along with an evaluation that also distinguishes between perspective\n(car passenger vs. external traffic participant) and gender. Using simulation\nbased $F$-tests, we find highly significant differences between the groups.\nThese differences have consequences on the reliable detection of pedestrians in\na safety critical distance to the self-driving car. We discuss the ethical\nproblems that are related to this approach and also discuss the problems\nemerging from human-machine interaction through the survey from a psychological\npoint of view. Finally, we include comments from industry leaders in the field\nof AI safety on the applicability of survey based elements in the design of AI\nfunctionalities in automated driving.",
    "descriptor": "\nComments: 26 pages, 12 figures\n",
    "authors": [
      "Robin Chan",
      "Radin Dardashti",
      "Meike Osinski",
      "Matthias Rottmann",
      "Dominik Br\u00fcggemann",
      "Cilia R\u00fccker",
      "Peter Schlicht",
      "Fabian H\u00fcger",
      "Nikol Rummel",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04776"
  },
  {
    "id": "arXiv:2206.04777",
    "title": "Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized  Linear Models",
    "abstract": "We study the problem of learning generalized linear models under adversarial\ncorruptions. We analyze a classical heuristic called the iterative trimmed\nmaximum likelihood estimator which is known to be effective against label\ncorruptions in practice. Under label corruptions, we prove that this simple\nestimator achieves minimax near-optimal risk on a wide range of generalized\nlinear models, including Gaussian regression, Poisson regression and Binomial\nregression. Finally, we extend the estimator to the more challenging setting of\nlabel and covariate corruptions and demonstrate its robustness and optimality\nin that setting as well.",
    "descriptor": "",
    "authors": [
      "Weihao Kong",
      "Rajat Sen",
      "Pranjal Awasthi",
      "Abhimanyu Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04777"
  },
  {
    "id": "arXiv:2206.04779",
    "title": "Challenges and Opportunities in Offline Reinforcement Learning from  Visual Observations",
    "abstract": "Offline reinforcement learning has shown great promise in leveraging large\npre-collected datasets for policy learning, allowing agents to forgo\noften-expensive online data collection. However, to date, offline reinforcement\nlearning from has been relatively under-explored, and there is a lack of\nunderstanding of where the remaining challenges lie. In this paper, we seek to\nestablish simple baselines for continuous control in the visual domain. We show\nthat simple modifications to two state-of-the-art vision-based online\nreinforcement learning algorithms, DreamerV2 and DrQ-v2, suffice to outperform\nprior work and establish a competitive baseline. We rigorously evaluate these\nalgorithms on both existing offline datasets and a new testbed for offline\nreinforcement learning from visual observations that better represents the data\ndistributions present in real-world offline reinforcement learning problems,\nand open-source our code and data to facilitate progress in this important\ndomain. Finally, we present and analyze several key desiderata unique to\noffline RL from visual observations, including visual distractions and visually\nidentifiable changes in dynamics.",
    "descriptor": "",
    "authors": [
      "Cong Lu",
      "Philip J. Ball",
      "Tim G. J. Rudner",
      "Jack Parker-Holder",
      "Michael A. Osborne",
      "Yee Whye Teh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04779"
  },
  {
    "id": "arXiv:2206.04780",
    "title": "Speak Like a Dog: Human to Non-human creature Voice Conversion",
    "abstract": "This paper proposes a new voice conversion (VC) task from human speech to\ndog-like speech while preserving linguistic information as an example of human\nto non-human creature voice conversion (H2NH-VC) tasks. Although most VC\nstudies deal with human to human VC, H2NH-VC aims to convert human speech into\nnon-human creature-like speech. Non-parallel VC allows us to develop H2NH-VC,\nbecause we cannot collect a parallel dataset that non-human creatures speak\nhuman language. In this study, we propose to use dogs as an example of a\nnon-human creature target domain and define the \"speak like a dog\" task. To\nclarify the possibilities and characteristics of the \"speak like a dog\" task,\nwe conducted a comparative experiment using existing representative\nnon-parallel VC methods in acoustic features (Mel-cepstral coefficients and\nMel-spectrograms), network architectures (five different kernel-size settings),\nand training criteria (variational autoencoder (VAE)- based and generative\nadversarial network-based). Finally, the converted voices were evaluated using\nmean opinion scores: dog-likeness, sound quality and intelligibility, and\ncharacter error rate (CER). The experiment showed that the employment of the\nMel-spectrogram improved the dog-likeness of the converted speech, while it is\nchallenging to preserve linguistic information. Challenges and limitations of\nthe current VC methods for H2NH-VC are highlighted.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Kohei Suzuki",
      "Shoki Sakamoto",
      "Tadahiro Taniguchi",
      "Hirokazu Kameoka"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04780"
  },
  {
    "id": "arXiv:2206.04783",
    "title": "ReFace: Real-time Adversarial Attacks on Face Recognition Systems",
    "abstract": "Deep neural network based face recognition models have been shown to be\nvulnerable to adversarial examples. However, many of the past attacks require\nthe adversary to solve an input-dependent optimization problem using gradient\ndescent which makes the attack impractical in real-time. These adversarial\nexamples are also tightly coupled to the attacked model and are not as\nsuccessful in transferring to different models. In this work, we propose\nReFace, a real-time, highly-transferable attack on face recognition models\nbased on Adversarial Transformation Networks (ATNs). ATNs model adversarial\nexample generation as a feed-forward neural network. We find that the white-box\nattack success rate of a pure U-Net ATN falls substantially short of\ngradient-based attacks like PGD on large face recognition datasets. We\ntherefore propose a new architecture for ATNs that closes this gap while\nmaintaining a 10000x speedup over PGD. Furthermore, we find that at a given\nperturbation magnitude, our ATN adversarial perturbations are more effective in\ntransferring to new face recognition models than PGD. ReFace attacks can\nsuccessfully deceive commercial face recognition services in a transfer attack\nsetting and reduce face identification accuracy from 82% to 16.4% for AWS\nSearchFaces API and Azure face verification accuracy from 91% to 50.1%.",
    "descriptor": "",
    "authors": [
      "Shehzeen Hussain",
      "Todd Huster",
      "Chris Mesterharm",
      "Paarth Neekhara",
      "Kevin An",
      "Malhar Jere",
      "Harshvardhan Sikka",
      "Farinaz Koushanfar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04783"
  },
  {
    "id": "arXiv:2206.04784",
    "title": "On the Bias-Variance Characteristics of LIME and SHAP in High Sparsity  Movie Recommendation Explanation Tasks",
    "abstract": "We evaluate two popular local explainability techniques, LIME and SHAP, on a\nmovie recommendation task. We discover that the two methods behave very\ndifferently depending on the sparsity of the data set. LIME does better than\nSHAP in dense segments of the data set and SHAP does better in sparse segments.\nWe trace this difference to the differing bias-variance characteristics of the\nunderlying estimators of LIME and SHAP. We find that SHAP exhibits lower\nvariance in sparse segments of the data compared to LIME. We attribute this\nlower variance to the completeness constraint property inherent in SHAP and\nmissing in LIME. This constraint acts as a regularizer and therefore increases\nthe bias of the SHAP estimator but decreases its variance, leading to a\nfavorable bias-variance trade-off especially in high sparsity data settings.\nWith this insight, we introduce the same constraint into LIME and formulate a\nnovel local explainabilty framework called Completeness-Constrained LIME\n(CLIMB) that is superior to LIME and much faster than SHAP.",
    "descriptor": "",
    "authors": [
      "Claudia V. Roberts",
      "Ehtsham Elahi",
      "Ashok Chandrashekar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.04784"
  },
  {
    "id": "arXiv:2206.04785",
    "title": "Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation",
    "abstract": "Egocentric 3D human pose estimation (HPE) from images is challenging due to\nsevere self-occlusions and strong distortion introduced by the fish-eye view\nfrom the head mounted camera. Although existing works use intermediate\nheatmap-based representations to counter distortion with some success,\naddressing self-occlusion remains an open problem. In this work, we leverage\ninformation from past frames to guide our self-attention-based 3D HPE\nestimation procedure -- Ego-STAN. Specifically, we build a spatio-temporal\nTransformer model that attends to semantically rich convolutional neural\nnetwork-based feature maps. We also propose feature map tokens: a new set of\nlearnable parameters to attend to these feature maps. Finally, we demonstrate\nEgo-STAN's superior performance on the xR-EgoPose dataset where it achieves a\n30.6% improvement on the overall mean per-joint position error, while leading\nto a 22% drop in parameters compared to the state-of-the-art.",
    "descriptor": "\nComments: 4 pages, Extended abstract, Joint International Workshop on Egocentric Perception, Interaction and Computing (EPIC) and Ego4D, IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2022\n",
    "authors": [
      "Jinman Park",
      "Kimathi Kaai",
      "Saad Hossain",
      "Norikatsu Sumi",
      "Sirisha Rambhatla",
      "Paul Fieguth"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04785"
  },
  {
    "id": "arXiv:2206.04789",
    "title": "Comprehensive Fair Meta-learned Recommender System",
    "abstract": "In recommender systems, one common challenge is the cold-start problem, where\ninteractions are very limited for fresh users in the systems. To address this\nchallenge, recently, many works introduce the meta-optimization idea into the\nrecommendation scenarios, i.e. learning to learn the user preference by only a\nfew past interaction items. The core idea is to learn global shared\nmeta-initialization parameters for all users and rapidly adapt them into local\nparameters for each user respectively. They aim at deriving general knowledge\nacross preference learning of various users, so as to rapidly adapt to the\nfuture new user with the learned prior and a small amount of training data.\nHowever, previous works have shown that recommender systems are generally\nvulnerable to bias and unfairness. Despite the success of meta-learning at\nimproving the recommendation performance with cold-start, the fairness issues\nare largely overlooked. In this paper, we propose a comprehensive fair\nmeta-learning framework, named CLOVER, for ensuring the fairness of\nmeta-learned recommendation models. We systematically study three kinds of\nfairness - individual fairness, counterfactual fairness, and group fairness in\nthe recommender systems, and propose to satisfy all three kinds via a\nmulti-task adversarial learning scheme. Our framework offers a generic training\nparadigm that is applicable to different meta-learned recommender systems. We\ndemonstrate the effectiveness of CLOVER on the representative meta-learned user\npreference estimator on three real-world data sets. Empirical results show that\nCLOVER achieves comprehensive fairness without deteriorating the overall\ncold-start recommendation performance.",
    "descriptor": "\nComments: Accepted to SIGKDD 2022\n",
    "authors": [
      "Tianxin Wei",
      "Jingrui He"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04789"
  },
  {
    "id": "arXiv:2206.04790",
    "title": "Learn2Augment: Learning to Composite Videos for Data Augmentation in  Action Recognition",
    "abstract": "We address the problem of data augmentation for video action recognition.\nStandard augmentation strategies in video are hand-designed and sample the\nspace of possible augmented data points either at random, without knowing which\naugmented points will be better, or through heuristics. We propose to learn\nwhat makes a good video for action recognition and select only high-quality\nsamples for augmentation. In particular, we choose video compositing of a\nforeground and a background video as the data augmentation process, which\nresults in diverse and realistic new samples. We learn which pairs of videos to\naugment without having to actually composite them. This reduces the space of\npossible augmentations, which has two advantages: it saves computational cost\nand increases the accuracy of the final trained classifier, as the augmented\npairs are of higher quality than average. We present experimental results on\nthe entire spectrum of training settings: few-shot, semi-supervised and fully\nsupervised. We observe consistent improvements across all of them over prior\nwork and baselines on Kinetics, UCF101, HMDB51, and achieve a new\nstate-of-the-art on settings with limited data. We see improvements of up to\n8.6% in the semi-supervised setting.",
    "descriptor": "",
    "authors": [
      "Shreyank N Gowda",
      "Marcus Rohrbach",
      "Frank Keller",
      "Laura Sevilla-Lara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04790"
  },
  {
    "id": "arXiv:2206.04791",
    "title": "Learning Reduced Nonlinear State-Space Models: an Output-Error Based  Canonical Approach",
    "abstract": "The identification of a nonlinear dynamic model is an open topic in control\ntheory, especially from sparse input-output measurements. A fundamental\nchallenge of this problem is that very few to zero prior knowledge is available\non both the state and the nonlinear system model. To cope with this challenge,\nwe investigate the effectiveness of deep learning in the modeling of dynamic\nsystems with nonlinear behavior by advocating an approach which relies on three\nmain ingredients: (i) we show that under some structural conditions on the\nto-be-identified model, the state can be expressed in function of a sequence of\nthe past inputs and outputs; (ii) this relation which we call the state map can\nbe modelled by resorting to the well-documented approximation power of deep\nneural networks; (iii) taking then advantage of existing learning schemes, a\nstate-space model can be finally identified. After the formulation and analysis\nof the approach, we show its ability to identify three different nonlinear\nsystems. The performances are evaluated in terms of open-loop prediction on\ntest data generated in simulation as well as a real world data-set of unmanned\naerial vehicle flight measurements.",
    "descriptor": "",
    "authors": [
      "Steeven Janny",
      "Quentin Possamai",
      "Laurent Bako",
      "Madiha Nadri",
      "Christian Wolf"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04791"
  },
  {
    "id": "arXiv:2206.04792",
    "title": "Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex  Evolving Data Stream",
    "abstract": "Online anomaly detection from a data stream is critical for the safety and\nsecurity of many applications but is facing severe challenges due to complex\nand evolving data streams from IoT devices and cloud-based infrastructures.\nUnfortunately, existing approaches fall too short for these challenges; online\nanomaly detection methods bear the burden of handling the complexity while\noffline deep anomaly detection methods suffer from the evolving data\ndistribution. This paper presents a framework for online deep anomaly\ndetection, ARCUS, which can be instantiated with any autoencoder-based deep\nanomaly detection methods. It handles the complex and evolving data streams\nusing an adaptive model pooling approach with two novel techniques:\nconcept-driven inference and drift-aware model pool update; the former detects\nanomalies with a combination of models most appropriate for the complexity, and\nthe latter adapts the model pool dynamically to fit the evolving data streams.\nIn comprehensive experiments with ten data sets which are both high-dimensional\nand concept-drifted, ARCUS improved the anomaly detection accuracy of the\nstreaming variants of state-of-the-art autoencoder-based methods and that of\nthe state-of-the-art streaming anomaly detection methods by up to 22% and 37%,\nrespectively.",
    "descriptor": "\nComments: Accepted by KDD 2022 Research Track\n",
    "authors": [
      "Susik Yoon",
      "Youngjun Lee",
      "Jae-Gil Lee",
      "Byung Suk Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.04792"
  },
  {
    "id": "arXiv:2206.04793",
    "title": "Securing AI-based Healthcare Systems using Blockchain Technology: A  State-of-the-Art Systematic Literature Review and Future Research Directions",
    "abstract": "Healthcare systems are increasingly incorporating Artificial Intelligence\ninto their systems, but it is not a solution for all difficulties. AI's\nextraordinary potential is being held back by challenges such as a lack of\nmedical datasets for training AI models, adversarial attacks, and a lack of\ntrust due to its black box working style. We explored how blockchain technology\ncan improve the reliability and trustworthiness of AI-based healthcare. This\npaper has conducted a Systematic Literature Review to explore the\nstate-of-the-art research studies conducted in healthcare applications\ndeveloped with different AI techniques and Blockchain Technology. This\nsystematic literature review proceeds with three different paths as natural\nlanguage processing-based healthcare systems, computer vision-based healthcare\nsystems and acoustic AI-based healthcare systems. We found that 1) Defence\ntechniques for adversarial attacks on AI are available for specific kind of\nattacks and even adversarial training is AI based technique which in further\nprone to different attacks. 2) Blockchain can address security and privacy\nissues in healthcare fraternity. 3) Medical data verification and user\nprovenance can be enabled with Blockchain. 4) Blockchain can protect\ndistributed learning on heterogeneous medical data. 5) The issues like single\npoint of failure, non-transparency in healthcare systems can be resolved with\nBlockchain. Nevertheless, it has been identified that research is at the\ninitial stage. As a result, we have synthesized a conceptual framework using\nBlockchain Technology for AI-based healthcare applications that considers the\nneeds of each NLP, Computer Vision, and Acoustic AI application. A global\nsolution for all sort of adversarial attacks on AI based healthcare. However,\nthis technique has significant limits and challenges that need to be addressed\nin future studies.",
    "descriptor": "\nComments: 44 Pages\n",
    "authors": [
      "Rucha Shinde",
      "Shruti Patil",
      "Ketan Kotecha",
      "Vidyasagar Potdar",
      "Ganeshsree Selvachandran",
      "Ajith Abraham"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04793"
  },
  {
    "id": "arXiv:2206.04794",
    "title": "Optimization for Infrastructure Cyber-Physical Systems",
    "abstract": "Cyber-physical systems (CPS) are systems where a decision making\n(cyber/control) component is tightly integrated with a physical system (with\nsensing/actuation) to enable real-time monitoring and control. Recently, there\nhas been significant research effort in viewing and optimizing physical\ninfrastructure in built environments as CPS, even if the control action is not\nin real-time. Some examples of infrastructure CPS include electrical power\ngrids; water distribution networks; transportation and logistics networks;\nheating, ventilation, and air conditioning (HVAC) in buildings; etc. Complexity\narises in infrastructure CPS from the large scale of operations; heterogeneity\nof system components; dynamic and uncertain operating conditions; and\ngoal-driven decision making and control with time-bounded task completion\nguarantees. For control optimization, an infrastructure CPS is typically viewed\nas a system of semi-autonomous sub-systems with a network of sensors and uses\ndistributed control optimization to achieve system-wide objectives that are\ntypically measured and quantified by better, cheaper, or faster system\nperformance. In this article, we first illustrate the scope for control\noptimization in common infrastructure CPS. Next, we present a brief overview of\ncurrent optimization techniques. Finally, we share our research position with a\ndescription of specific optimization approaches and their challenges for\ninfrastructure CPS of the future.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Arunchandar Vasan",
      "Prasant Misra",
      "Srinarayana Nagarathinam",
      "Venkata Ramakrishna",
      "Ramasubramanian Suriyanarayanan",
      "Yashovardhan Chati"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04794"
  },
  {
    "id": "arXiv:2206.04795",
    "title": "Precise Calculation of Electrical Capacitance by means of Quadruple  Integrals in Method of Moments Technique",
    "abstract": "In this paper, the capacitance of a parallel plate air-gap rectangular\ncapacitor, and a unit cube capacitor have been calculated. Because of its\ngenerality and simplicity, the method of moments (MOM) Technique is utilized.\nIn order to improve the accuracy of the calculations, the use of quadratic\nintegrals instead of binary integrals has been proposed. A neat form is\nprovided for the analytical solution of the integrals required for the method\nof moment. The results show that there is a very small error in calculating the\ncapacity even with coarse boundary division. The described formulas and codes\ncan easily be used for similar purposes.",
    "descriptor": "",
    "authors": [
      "Saeed Sarkarati",
      "Mohammad Mehdi Tehranchi",
      "Esfandiar Mehrshahi"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04795"
  },
  {
    "id": "arXiv:2206.04796",
    "title": "Scale up your In-Memory Accelerator: Leveraging Wireless-on-Chip  Communication for AIMC-based CNN Inference",
    "abstract": "Analog In-Memory Computing (AIMC) is emerging as a disruptive paradigm for\nheterogeneous computing, potentially delivering orders of magnitude better peak\nperformance and efficiency over traditional digital signal processing\narchitectures on Matrix-Vector multiplication. However, to sustain this\nthroughput in real-world applications, AIMC tiles must be supplied with data at\nvery high bandwidth and low latency; this poses an unprecedented pressure on\nthe on-chip communication infrastructure, which becomes the system's\nperformance and efficiency bottleneck. In this context, the performance and\nplasticity of emerging on-chip wireless communication paradigms provide the\nrequired breakthrough to up-scale on-chip communication in large AIMC devices.\nThis work presents a many-tile AIMC architecture with inter-tile wireless\ncommunication that integrates multiple heterogeneous computing clusters,\nembedding a mix of parallel RISC-V cores and AIMC tiles. We perform an\nextensive design space exploration of the proposed architecture and discuss the\nbenefits of exploiting emerging on-chip communication technologies such as\nwireless transceivers in the millimeter-wave and terahertz bands.",
    "descriptor": "",
    "authors": [
      "Nazareno Bruschi",
      "Giuseppe Tagliavini",
      "Francesco Conti",
      "Sergi Abadal",
      "Alberto Cabellos-Aparicio",
      "Eduard Alarc\u00f3n",
      "Geethan Karunaratne",
      "Irem Boybat",
      "Luca Benini",
      "Davide Rossi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04796"
  },
  {
    "id": "arXiv:2206.04797",
    "title": "Stable and memory-efficient image recovery using monotone operator  learning (MOL)",
    "abstract": "We introduce a monotone deep equilibrium learning framework for large-scale\ninverse problems in imaging. The proposed algorithm relies on forward-backward\nsplitting, where each iteration consists of a gradient descent involving the\nscore function and a conjugate gradient algorithm to encourage data\nconsistency. The score function is modeled as a monotone convolutional neural\nnetwork. The use of a monotone operator offers several benefits, including\nguaranteed convergence, uniqueness of fixed point, and robustness to input\nperturbations, similar to the use of convex priors in compressive sensing. In\naddition, the proposed formulation is significantly more memory-efficient than\nunrolled methods, which allows us to apply it to 3D problems that current\nunrolled algorithms cannot handle. Experiments show that the proposed scheme\ncan offer improved performance in 3D settings while being stable in the\npresence of input perturbations.",
    "descriptor": "",
    "authors": [
      "Aniket Pramanik",
      "Mathews Jacob"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04797"
  },
  {
    "id": "arXiv:2206.04798",
    "title": "Learning to Efficiently Propagate for Reasoning on Knowledge Graphs",
    "abstract": "Path-based methods are more appealing solutions than embedding methods for\nknowledge graph reasoning, due to their interpretability and generalization\nability to unseen graphs. However, path-based methods usually suffer from the\nproblem of scalability, as the time complexity grows exponentially w.r.t. the\nlength of paths. While recent methods compute reasoning paths with the\nBellman-Ford algorithm in polynomial time, the time and memory cost remains\nvery high, as they need to propagate through all the nodes and edges in the\ngraph. In this paper, we propose A*Net, an efficient model for path-based\nreasoning on knowledge graphs. Inspired by the classical A* algorithm for\nshortest path problems, our A*Net prioritizes important nodes and edges at each\npropagation step, to reduce the time and memory footprint. Unlike the classical\nA* algorithm that uses a heuristic function, we propose to learn the priority\nfunction for each node to capture the complex semantics in knowledge graphs.\nThe priority function and the propagation steps are jointly optimized through\nbackpropagation. Experiments on both transductive and inductive knowledge graph\nreasoning benchmarks show that A*Net achieves competitive performance with\nexisting state-of-the-art path-based methods, and meanwhile reduces the number\nof messages, the time and the memory cost up to 7.2$\\times$, 3.4$\\times$ and\n4.9$\\times$ respectively.",
    "descriptor": "",
    "authors": [
      "Zhaocheng Zhu",
      "Xinyu Yuan",
      "Louis-Pascal Xhonneux",
      "Ming Zhang",
      "Maxime Gazeau",
      "Jian Tang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04798"
  },
  {
    "id": "arXiv:2206.04799",
    "title": "Computational thermal multi-phase flow for metal additive manufacturing",
    "abstract": "Thermal multi-phase flow simulations are indispensable to understanding the\nmulti-scale and multi-physics phenomena in metal additive manufacturing (AM)\nprocesses, yet accurate and robust predictions remain challenging. This book\nchapter summarizes the recent method development at UIUC for simulating thermal\nmultiphase flows in laser powder bed fusion (LPBF) and directed energy\ndeposition (DED) processes. Two main method developments are discussed. The\nfirst is a mixed interface-capturing/interface-tracking computational framework\naiming to explicitly treat the gas-metal interface without mesh\nmotion/re-meshing. The second is a physics-based and non-empirical deposit\ngeometry model for DED processes. The proposed framework's accuracy is assessed\nby thoroughly comparing the simulated results against experimental measurements\non various quantities. We also report critical quantities that experiments can\nnot measure to show the predictive capability of the developed methods.",
    "descriptor": "",
    "authors": [
      "Jinhui Yan",
      "Qiming Zhu",
      "Ze Zhao"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.04799"
  },
  {
    "id": "arXiv:2206.04800",
    "title": "Explainable Artificial Intelligence (XAI) for Internet of Things: A  Survey",
    "abstract": "Black-box nature of Artificial Intelligence (AI) models do not allow users to\ncomprehend and sometimes trust the output created by such model. In AI\napplications, where not only the results but also the decision paths to the\nresults are critical, such black-box AI models are not sufficient. Explainable\nArtificial Intelligence (XAI) addresses this problem and defines a set of AI\nmodels that are interpretable by the users. Recently, several number of XAI\nmodels have been to address the issues surrounding by lack of interpretability\nand explainability of black-box models in various application areas such as\nhealthcare, military, energy, financial and industrial domains. Although the\nconcept of XAI has gained great deal of attention recently, its integration\ninto the IoT domain has not yet been fully defined. In this paper, we provide\nan in-depth and systematic review of recent studies using XAI models in the\nscope of IoT domain. We categorize the studies according to their methodology\nand applications areas. In addition, we aim to focus on the challenging\nproblems and open issues and give future directions to guide the developers and\nresearchers for prospective future investigations.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Ibrahim Kok",
      "Feyza Yildirim Okay",
      "Ozgecan Muyanli",
      "Suat Ozdemir"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.04800"
  },
  {
    "id": "arXiv:2206.04801",
    "title": "Learning Attention-based Representations from Multiple Patterns for  Relation Prediction in Knowledge Graphs",
    "abstract": "Knowledge bases, and their representations in the form of knowledge graphs\n(KGs), are naturally incomplete. Since scientific and industrial applications\nhave extensively adopted them, there is a high demand for solutions that\ncomplete their information. Several recent works tackle this challenge by\nlearning embeddings for entities and relations, then employing them to predict\nnew relations among the entities. Despite their aggrandizement, most of those\nmethods focus only on the local neighbors of a relation to learn the\nembeddings. As a result, they may fail to capture the KGs' context information\nby neglecting long-term dependencies and the propagation of entities'\nsemantics. In this manuscript, we propose {\\AE}MP (Attention-based Embeddings\nfrom Multiple Patterns), a novel model for learning contextualized\nrepresentations by: (i) acquiring entities' context information through an\nattention-enhanced message-passing scheme, which captures the entities' local\nsemantics while focusing on different aspects of their neighborhood; and (ii)\ncapturing the semantic context, by leveraging the paths and their relationships\nbetween entities. Our empirical findings draw insights into how attention\nmechanisms can improve entities' context representation and how combining\nentities and semantic path contexts improves the general representation of\nentities and the relation predictions. Experimental results on several large\nand small knowledge graph benchmarks show that {\\AE}MP either outperforms or\ncompetes with state-of-the-art relation prediction methods.",
    "descriptor": "\nComments: Accepted to publication at Knowledge-based Systems, 2022\n",
    "authors": [
      "V\u00edtor Louren\u00e7o",
      "Aline Paes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04801"
  },
  {
    "id": "arXiv:2206.04803",
    "title": "Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application  of Machine Learning-based Forensics",
    "abstract": "The rise of blockchain and distributed ledger technologies (DLTs) in the\nfinancial sector has generated a socio-economic shift that triggered legal\nconcerns and regulatory initiatives. While the anonymity of DLTs may safeguard\nthe right to privacy, data protection and other civil liberties, lack of\nidentification hinders accountability, investigation and enforcement. The\nresulting challenges extend to the rules to combat money laundering and the\nfinancing of terrorism and proliferation (AML/CFT). As law enforcement agencies\nand analytics companies have begun to successfully apply forensics to track\ncurrency across blockchain ecosystems, in this paper we focus on the increasing\nrelevance of these techniques. In particular, we offer insights into the\napplication to the Internet of Money (IoM) of machine learning, network and\ntransaction graph analysis. After providing some background on the notion of\nanonymity in the IoM and on the interplay between AML/CFT and blockchain\nforensics, we focus on anomaly detection approaches leading to our experiments.\nNamely, we analyzed a real-world dataset of Bitcoin transactions represented as\na directed graph network through various machine learning techniques. Our claim\nis that the AML/CFT domain could benefit from novel graph analysis methods in\nmachine learning. Indeed, our findings show that the Graph Convolutional\nNetworks (GCN) and Graph Attention Networks (GAT) neural network types\nrepresent a promising solution for AML/CFT compliance.",
    "descriptor": "",
    "authors": [
      "Nadia Pocher",
      "Mirko Zichichi",
      "Fabio Merizzi",
      "Muhammad Zohaib Shafiq",
      "Stefano Ferretti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04803"
  },
  {
    "id": "arXiv:2206.04805",
    "title": "Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022",
    "abstract": "We build a classification model for the BirdCLEF 2022 challenge using\nunsupervised methods. We implement an unsupervised representation of the\ntraining dataset using a triplet loss on spectrogram representation of audio\nmotifs. Our best model performs with a score of 0.48 on the public leaderboard.",
    "descriptor": "\nComments: Submitted to CEUR-WS under LifeCLEF for the BirdCLEF 2022 challenge as a working note\n",
    "authors": [
      "Anthony Miyaguchi",
      "Jiangyue Yu",
      "Bryan Cheungvivatpant",
      "Dakota Dudley",
      "Aniketh Swain"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04805"
  },
  {
    "id": "arXiv:2206.04806",
    "title": "Syntactic Inductive Biases for Deep Learning Methods",
    "abstract": "In this thesis, we try to build a connection between the two schools by\nintroducing syntactic inductive biases for deep learning models. We propose two\nfamilies of inductive biases, one for constituency structure and another one\nfor dependency structure. The constituency inductive bias encourages deep\nlearning models to use different units (or neurons) to separately process\nlong-term and short-term information. This separation provides a way for deep\nlearning models to build the latent hierarchical representations from\nsequential inputs, that a higher-level representation is composed of and can be\ndecomposed into a series of lower-level representations. For example, without\nknowing the ground-truth structure, our proposed model learns to process\nlogical expression through composing representations of variables and operators\ninto representations of expressions according to its syntactic structure. On\nthe other hand, the dependency inductive bias encourages models to find the\nlatent relations between entities in the input sequence. For natural language,\nthe latent relations are usually modeled as a directed dependency graph, where\na word has exactly one parent node and zero or several children nodes. After\napplying this constraint to a Transformer-like model, we find the model is\ncapable of inducing directed graphs that are close to human expert annotations,\nand it also outperforms the standard transformer model on different tasks. We\nbelieve that these experimental results demonstrate an interesting alternative\nfor the future development of deep learning models.",
    "descriptor": "\nComments: Ph.D. Thesis\n",
    "authors": [
      "Yikang Shen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04806"
  },
  {
    "id": "arXiv:2206.04811",
    "title": "Deep learning-enhanced ensemble-based data assimilation for  high-dimensional nonlinear dynamical systems",
    "abstract": "Data assimilation (DA) is a key component of many forecasting models in\nscience and engineering. DA allows one to estimate better initial conditions\nusing an imperfect dynamical model of the system and noisy/sparse observations\navailable from the system. Ensemble Kalman filter (EnKF) is a DA algorithm that\nis widely used in applications involving high-dimensional nonlinear dynamical\nsystems. However, EnKF requires evolving large ensembles of forecasts using the\ndynamical model of the system. This often becomes computationally intractable,\nespecially when the number of states of the system is very large, e.g., for\nweather prediction. With small ensembles, the estimated background error\ncovariance matrix in the EnKF algorithm suffers from sampling error, leading to\nan erroneous estimate of the analysis state (initial condition for the next\nforecast cycle). In this work, we propose hybrid ensemble Kalman filter\n(H-EnKF), which is applied to a two-layer quasi-geostrophic flow system as a\ntest case. This framework utilizes a pre-trained deep learning-based\ndata-driven surrogate that inexpensively generates and evolves a large\ndata-driven ensemble of the states of the system to accurately compute the\nbackground error covariance matrix with less sampling error. The H-EnKF\nframework estimates a better initial condition without the need for any ad-hoc\nlocalization strategies. H-EnKF can be extended to any ensemble-based DA\nalgorithm, e.g., particle filters, which are currently difficult to use for\nhigh dimensional systems.",
    "descriptor": "",
    "authors": [
      "Ashesh Chattopadhyay",
      "Ebrahim Nabizadeh",
      "Eviatar Bach",
      "Pedram Hassanzadeh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Fluid Dynamics (physics.flu-dyn)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04811"
  },
  {
    "id": "arXiv:2206.04812",
    "title": "The Case for a Single Model that can Both Generate Continuations and  Fill in the Blank",
    "abstract": "The task of inserting text into a specified position in a passage, known as\nfill in the blank (FitB), is useful for a variety of applications where writers\ninteract with a natural language generation (NLG) system to craft text. While\nprevious work has tackled this problem with models trained specifically to do\nthe fill-in-the-blank task, a more useful model is one that can effectively\nperform _both_ FitB and continuation. In this work, we evaluate the feasibility\nof using a single model to do both tasks. We show that models pre-trained with\na FitB-style objective are capable of both tasks, while models pre-trained for\ncontinuation are not. Finally, we show how FitB models can be easily finetuned\nto allow for fine-grained control over the length and word choice of the\ngeneration.",
    "descriptor": "",
    "authors": [
      "Daphne Ippolito",
      "Liam Dugan",
      "Emily Reif",
      "Ann Yuan",
      "Andy Coenen",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04812"
  },
  {
    "id": "arXiv:2206.04816",
    "title": "Empirical Bayes approach to Truth Discovery problems",
    "abstract": "When aggregating information from conflicting sources, one's goal is to find\nthe truth. Most real-value \\emph{truth discovery} (TD) algorithms try to\nachieve this goal by estimating the competence of each source and then\naggregating the conflicting information by weighing each source's answer\nproportionally to her competence. However, each of those algorithms requires\nmore than a single source for such estimation and usually does not consider\ndifferent estimation methods other than a weighted mean. Therefore, in this\nwork we formulate, prove, and empirically test the conditions for an Empirical\nBayes Estimator (EBE) to dominate the weighted mean aggregation. Our main\nresult demonstrates that EBE, under mild conditions, can be used as a second\nstep of any TD algorithm in order to reduce the expected error.",
    "descriptor": "\nComments: full version of a paper accepted to UAI'22\n",
    "authors": [
      "Tsviel Ben Shabat",
      "Reshef Meir",
      "David Azriel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04816"
  },
  {
    "id": "arXiv:2206.04817",
    "title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and  the \\emph{Grokking Phenomenon}",
    "abstract": "The \\emph{grokking phenomenon} as reported by Power et\nal.~\\cite{power2021grokking} refers to a regime where a long period of\noverfitting is followed by a seemingly sudden transition to perfect\ngeneralization. In this paper, we attempt to reveal the underpinnings of\nGrokking via a series of empirical studies. Specifically, we uncover an\noptimization anomaly plaguing adaptive optimizers at extremely late stages of\ntraining, referred to as the \\emph{Slingshot Mechanism}. A prominent artifact\nof the Slingshot Mechanism can be measured by the cyclic phase transitions\nbetween stable and unstable training regimes, and can be easily monitored by\nthe cyclic behavior of the norm of the last layers weights. We empirically\nobserve that without explicit regularization, Grokking as reported in\n\\cite{power2021grokking} almost exclusively happens at the onset of\n\\emph{Slingshots}, and is absent without it. While common and easily reproduced\nin more general settings, the Slingshot Mechanism does not follow from any\nknown optimization theories that we are aware of, and can be easily overlooked\nwithout an in depth examination. Our work points to a surprising and useful\ninductive bias of adaptive gradient optimizers at late stages of training,\ncalling for a revised theoretical analysis of their origin.",
    "descriptor": "",
    "authors": [
      "Vimal Thilak",
      "Etai Littwin",
      "Shuangfei Zhai",
      "Omid Saremi",
      "Roni Paiss",
      "Joshua Susskind"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.04817"
  },
  {
    "id": "arXiv:2206.04823",
    "title": "Membership Inference via Backdooring",
    "abstract": "Recently issued data privacy regulations like GDPR (General Data Protection\nRegulation) grant individuals the right to be forgotten. In the context of\nmachine learning, this requires a model to forget about a training data sample\nif requested by the data owner (i.e., machine unlearning). As an essential step\nprior to machine unlearning, it is still a challenge for a data owner to tell\nwhether or not her data have been used by an unauthorized party to train a\nmachine learning model. Membership inference is a recently emerging technique\nto identify whether a data sample was used to train a target model, and seems\nto be a promising solution to this challenge. However, straightforward adoption\nof existing membership inference approaches fails to address the challenge\neffectively due to being originally designed for attacking membership privacy\nand suffering from several severe limitations such as low inference accuracy on\nwell-generalized models. In this paper, we propose a novel membership inference\napproach inspired by the backdoor technology to address the said challenge.\nSpecifically, our approach of Membership Inference via Backdooring (MIB)\nleverages the key observation that a backdoored model behaves very differently\nfrom a clean model when predicting on deliberately marked samples created by a\ndata owner. Appealingly, MIB requires data owners' marking a small number of\nsamples for membership inference and only black-box access to the target model,\nwith theoretical guarantees for inference results. We perform extensive\nexperiments on various datasets and deep neural network architectures, and the\nresults validate the efficacy of our approach, e.g., marking only 0.1% of the\ntraining dataset is practically sufficient for effective membership inference.",
    "descriptor": "\nComments: This paper has been accepted by IJCAI-22\n",
    "authors": [
      "Hongsheng Hu",
      "Zoran Salcic",
      "Gillian Dobbie",
      "Jinjun Chen",
      "Lichao Sun",
      "Xuyun Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04823"
  },
  {
    "id": "arXiv:2206.04827",
    "title": "A Fast Spectral Solver for the Heat Equation, with Applications to  Navier--Stokes",
    "abstract": "We develop a spectral method to solve the heat equation in a closed cylinder,\nachieving a near-optimal $\\mathcal{O}(N\\log N)$ complexity and high-order,\n\\emph{spectral} accuracy. The algorithm relies on a novel\nChebyshev--Chebyshev--Fourier (CCF) discretization of the cylinder, which is\neasily implemented and decouples the heat equation into a collection of\nsmaller, sparse Sylvester equations. In turn, each of these equations is solved\nusing the alternating direction implicit (ADI) method, which improves the\ncomplexity of each solve from cubic in the matrix size (in more traditional\nmethods) to log-linear; overall, this represents an improvement in the heat\nequation solver from $\\mathcal{O}(N^{7/3})$ (in traditional methods) to\n$\\mathcal{O}(N\\log N)$. Lastly, we provide numerical simulations demonstrating\nsignificant speed-ups over traditional spectral collocation methods and finite\ndifference methods, and we provide a framework by which this heat equation\nsolver could be applied to the incompressible Navier--Stokes equations. For the\nlatter, we decompose the equations using a poloidal--toroidal (PT)\ndecomposition, turning them into heat equations with nonlinear forcing from the\nadvection term; by using implicit--explicit methods to integrate these, we can\nachieve the same $\\mathcal{O}(N\\log N)$ complexity and spectral accuracy\nachieved here in the heat equation.",
    "descriptor": "\nComments: 18 pages, 3 figures. Completed as part of MIT PRIMES 2017, but revised for publication\n",
    "authors": [
      "David Darrow"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2206.04827"
  },
  {
    "id": "arXiv:2206.04831",
    "title": "R4D: Utilizing Reference Objects for Long-Range Distance Estimation",
    "abstract": "Estimating the distance of objects is a safety-critical task for autonomous\ndriving. Focusing on short-range objects, existing methods and datasets neglect\nthe equally important long-range objects. In this paper, we introduce a\nchallenging and under-explored task, which we refer to as Long-Range Distance\nEstimation, as well as two datasets to validate new methods developed for this\ntask. We then proposeR4D, the first framework to accurately estimate the\ndistance of long-range objects by using references with known distances in the\nscene. Drawing inspiration from human perception, R4D builds a graph by\nconnecting a target object to all references. An edge in the graph encodes the\nrelative distance information between a pair of target and reference objects.\nAn attention module is then used to weigh the importance of reference objects\nand combine them into one target object distance prediction. Experiments on the\ntwo proposed datasets demonstrate the effectiveness and robustness of R4D by\nshowing significant improvements compared to existing baselines. We are looking\nto make the proposed dataset, Waymo OpenDataset - Long-Range Labels, available\npublicly at waymo.com/open/download.",
    "descriptor": "\nComments: ICLR 2022\n",
    "authors": [
      "Yingwei Li",
      "Tiffany Chen",
      "Maya Kabkab",
      "Ruichi Yu",
      "Longlong Jing",
      "Yurong You",
      "Hang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04831"
  },
  {
    "id": "arXiv:2206.04832",
    "title": "Transformer-Graph Neural Network with Global-Local Attention for  Multimodal Rumour Detection with Knowledge Distillation",
    "abstract": "Misinformation spreading becomes a critical issue in online conversation.\nDetecting rumours is an important research topic in social media analysis. Most\nexisting methods, based on Convolutional Neural Networks (CNNs) and Graph\nNeural Networks (GNNs), do not make use of the relationship between the global\nand local information of a conversation for detection. In this paper, we\npropose a Transformer-Graph Neural Network (TGNN), to fuse the local\ninformation with the global representation, through an attention mechanism.\nThen, we extend the proposed TGNN for multimodal rumour detection, by\nconsidering the latent relationship between the multimodal feature and node\nfeature to form a more comprehensive graph representation. To verify the\neffectiveness of our proposed method for multimodal rumour detection, we extend\nthe existing PHEME-2016, PHEME-2018, and Weibo data sets, by collecting\navailable and relevant images for training the proposal framework. To improve\nthe performance of single-modal rumour detection, i.e., based on text input\nonly, a teacher-student framework is employed to distil the knowledge from the\nmultimodal model to the single-modal model. Experimental results show that our\nproposed TGNN can achieve state-of-the-art performance and generalization\nability evaluated on the PHEME-2016, PHEME-2018, and Weibo data sets.",
    "descriptor": "",
    "authors": [
      "Tsun-hin Cheung",
      "Kin-man Lam"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.04832"
  },
  {
    "id": "arXiv:2206.04833",
    "title": "Training Neural Networks using SAT solvers",
    "abstract": "We propose an algorithm to explore the global optimization method, using SAT\nsolvers, for training a neural net. Deep Neural Networks have achieved great\nfeats in tasks like-image recognition, speech recognition, etc. Much of their\nsuccess can be attributed to the gradient-based optimisation methods, which\nscale well to huge datasets while still giving solutions, better than any other\nexisting methods. However, there exist learning problems like the parity\nfunction and the Fast Fourier Transform, where a neural network using\ngradient-based optimisation algorithm can not capture the underlying structure\nof the learning task properly. Thus, exploring global optimisation methods is\nof utmost interest as the gradient-based methods get stuck in local optima. In\nthe experiments, we demonstrate the effectiveness of our algorithm against the\nADAM optimiser in certain tasks like parity learning. However, in the case of\nimage classification on the MNIST Dataset, the performance of our algorithm was\nless than satisfactory. We further discuss the role of the size of the training\ndataset and the hyper-parameter settings in keeping things scalable for a SAT\nsolver.",
    "descriptor": "",
    "authors": [
      "Subham S. Sahoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04833"
  },
  {
    "id": "arXiv:2206.04834",
    "title": "Persistent Homology for Resource Coverage: A Case Study of Access to  Polling Sites",
    "abstract": "It is important to choose the geographical distribution of public resources\nin a fair and equitable manner. However, it is complicated to quantify the\nequity of such a distribution; important factors include distances to resource\nsites, availability of transportation, and ease of travel. In this paper, we\nuse persistent homology, which is a tool from topological data analysis, to\nstudy the effective availability and coverage of polling sites. The information\nfrom persistent homology allows us to infer holes in the distribution of\npolling sites. We analyze and compare the coverage of polling sites in Los\nAngeles County and five cities (Atlanta, Chicago, Jacksonville, New York City,\nand Salt Lake City), and we conclude that computation of persistent homology\nappears to be a reasonable approach to analyzing resource coverage.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Abigail Hickok",
      "Benjamin Jarman",
      "Michael Johnson",
      "Jiajie Luo",
      "Mason A. Porter"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04834"
  },
  {
    "id": "arXiv:2206.04835",
    "title": "Communication Efficient Distributed Learning for Kernelized Contextual  Bandits",
    "abstract": "We tackle the communication efficiency challenge of learning kernelized\ncontextual bandits in a distributed setting. Despite the recent advances in\ncommunication-efficient distributed bandit learning, existing solutions are\nrestricted to simple models like multi-armed bandits and linear bandits, which\nhamper their practical utility. In this paper, instead of assuming the\nexistence of a linear reward mapping from the features to the expected rewards,\nwe consider non-linear reward mappings, by letting agents collaboratively\nsearch in a reproducing kernel Hilbert space (RKHS). This introduces\nsignificant challenges in communication efficiency as distributed kernel\nlearning requires the transfer of raw data, leading to a communication cost\nthat grows linearly w.r.t. time horizon $T$. We addresses this issue by\nequipping all agents to communicate via a common Nystr\\\"{o}m embedding that\ngets updated adaptively as more data points are collected. We rigorously proved\nthat our algorithm can attain sub-linear rate in both regret and communication\ncost.",
    "descriptor": "\nComments: 29 pages, 3 figures\n",
    "authors": [
      "Chuanhao Li",
      "Huazheng Wang",
      "Mengdi Wang",
      "Hongning Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04835"
  },
  {
    "id": "arXiv:2206.04838",
    "title": "In Defense of Core-set: A Density-aware Core-set Selection for Active  Learning",
    "abstract": "Active learning enables the efficient construction of a labeled dataset by\nlabeling informative samples from an unlabeled dataset. In a real-world active\nlearning scenario, considering the diversity of the selected samples is crucial\nbecause many redundant or highly similar samples exist. Core-set approach is\nthe promising diversity-based method selecting diverse samples based on the\ndistance between samples. However, the approach poorly performs compared to the\nuncertainty-based approaches that select the most difficult samples where\nneural models reveal low confidence. In this work, we analyze the feature space\nthrough the lens of the density and, interestingly, observe that locally sparse\nregions tend to have more informative samples than dense regions. Motivated by\nour analysis, we empower the core-set approach with the density-awareness and\npropose a density-aware core-set (DACS). The strategy is to estimate the\ndensity of the unlabeled samples and select diverse samples mainly from sparse\nregions. To reduce the computational bottlenecks in estimating the density, we\nalso introduce a new density approximation based on locality-sensitive hashing.\nExperimental results clearly demonstrate the efficacy of DACS in both\nclassification and regression tasks and specifically show that DACS can produce\nstate-of-the-art performance in a practical scenario. Since DACS is weakly\ndependent on neural architectures, we present a simple yet effective\ncombination method to show that the existing methods can be beneficially\ncombined with DACS.",
    "descriptor": "\nComments: Proceedings of the ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2022 (KDD'22)\n",
    "authors": [
      "Yeachan Kim",
      "Bonggun Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04838"
  },
  {
    "id": "arXiv:2206.04841",
    "title": "Hierarchical mixtures of Gaussians for combined dimensionality reduction  and clustering",
    "abstract": "To avoid the curse of dimensionality, a common approach to clustering\nhigh-dimensional data is to first project the data into a space of reduced\ndimension, and then cluster the projected data. Although effective, this\ntwo-stage approach prevents joint optimization of the dimensionality-reduction\nand clustering models, and obscures how well the complete model describes the\ndata. Here, we show how a family of such two-stage models can be combined into\na single, hierarchical model that we call a hierarchical mixture of Gaussians\n(HMoG). An HMoG simultaneously captures both dimensionality-reduction and\nclustering, and its performance is quantified in closed-form by the likelihood\nfunction. By formulating and extending existing models with exponential family\ntheory, we show how to maximize the likelihood of HMoGs with\nexpectation-maximization. We apply HMoGs to synthetic data and RNA sequencing\ndata, and demonstrate how they exceed the limitations of two-stage models.\nUltimately, HMoGs are a rigorous generalization of a common statistical\nframework, and provide researchers with a method to improve model performance\nwhen clustering high-dimensional data.",
    "descriptor": "",
    "authors": [
      "Sacha Sokoloski",
      "Philipp Berens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04841"
  },
  {
    "id": "arXiv:2206.04843",
    "title": "Neural Laplace: Learning diverse classes of differential equations in  the Laplace domain",
    "abstract": "Neural Ordinary Differential Equations model dynamical systems with\n\\textit{ODE}s learned by neural networks. However, ODEs are fundamentally\ninadequate to model systems with long-range dependencies or discontinuities,\nwhich are common in engineering and biological systems. Broader classes of\ndifferential equations (DE) have been proposed as remedies, including delay\ndifferential equations and integro-differential equations. Furthermore, Neural\nODE suffers from numerical instability when modelling stiff ODEs and ODEs with\npiecewise forcing functions. In this work, we propose \\textit{Neural Laplace},\na unified framework for learning diverse classes of DEs including all the\naforementioned ones. Instead of modelling the dynamics in the time domain, we\nmodel it in the Laplace domain, where the history-dependencies and\ndiscontinuities in time can be represented as summations of complex\nexponentials. To make learning more efficient, we use the geometrical\nstereographic map of a Riemann sphere to induce more smoothness in the Laplace\ndomain. In the experiments, Neural Laplace shows superior performance in\nmodelling and extrapolating the trajectories of diverse classes of DEs,\nincluding the ones with complex history dependency and abrupt changes.",
    "descriptor": "\nComments: Proceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s)\n",
    "authors": [
      "Samuel Holt",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04843"
  },
  {
    "id": "arXiv:2206.04846",
    "title": "Masked Autoencoders are Robust Data Augmentors",
    "abstract": "Deep neural networks are capable of learning powerful representations to\ntackle complex vision tasks but expose undesirable properties like the\nover-fitting issue. To this end, regularization techniques like image\naugmentation are necessary for deep neural networks to generalize well.\nNevertheless, most prevalent image augmentation recipes confine themselves to\noff-the-shelf linear transformations like scale, flip, and colorjitter. Due to\ntheir hand-crafted property, these augmentations are insufficient to generate\ntruly hard augmented examples. In this paper, we propose a novel perspective of\naugmentation to regularize the training process. Inspired by the recent success\nof applying masked image modeling to self-supervised learning, we adopt the\nself-supervised masked autoencoder to generate the distorted view of the input\nimages. We show that utilizing such model-based nonlinear transformation as\ndata augmentation can improve high-level recognition tasks. We term the\nproposed method as \\textbf{M}ask-\\textbf{R}econstruct \\textbf{A}ugmentation\n(MRA). The extensive experiments on various image classification benchmarks\nverify the effectiveness of the proposed augmentation. Specifically, MRA\nconsistently enhances the performance on supervised, semi-supervised as well as\nfew-shot classification. The code will be available at\n\\url{https://github.com/haohang96/MRA}.",
    "descriptor": "",
    "authors": [
      "Haohang Xu",
      "Shuangrui Ding",
      "Xiaopeng Zhang",
      "Hongkai Xiong",
      "Qi Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04846"
  },
  {
    "id": "arXiv:2206.04852",
    "title": "Robot Control for Simultaneous Impact Tasks through Time-Invariant  Reference Spreading",
    "abstract": "With the goal of enabling the exploitation of impacts in robotic\nmanipulation, a new framework is presented for control of robotic manipulators\nthat are tasked to execute nominally simultaneous impacts. In this framework,\nwe employ tracking of time-invariant reference vector fields corresponding to\nthe ante- and post-impact motion, increasing its applicability over similar\nconventional tracking control approaches. The ante- and post-impact references\nare coupled through a rigid impact map, and are extended to overlap around the\narea where the impact is expected to take place, such that the reference\ncorresponding to the actual contact state of the robot can always be followed.\nAs a sequence of impacts at the different contact points will typically occur,\nresulting in uncertainty of the contact mode and unreliable velocity\nmeasurements, a new interim control mode catered towards time-invariant\nreferences is formulated. In this mode, a position feedback signal is derived\nfrom the ante-impact velocity reference, which is used to enforce sustained\ncontact in all contact points without using velocity feedback. With an eye\ntowards real implementation, the approach is formulated using a QP control\nframework, and is validated using numerical simulations both on a rigid robot\nwith a hard inelastic contact model and on a realistic robot model with\nflexible joints and compliant partially elastic contact model.",
    "descriptor": "\nComments: 8 pages, 3 figures. Submitted for publication to the IEEE Conference on Decision and Control (CDC) 2022\n",
    "authors": [
      "Jari J. van Steen",
      "Nathan van de Wouw",
      "Alessandro Saccon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04852"
  },
  {
    "id": "arXiv:2206.04853",
    "title": "Machop: an End-to-End Generalized Entity Matching Framework",
    "abstract": "Real-world applications frequently seek to solve a general form of the Entity\nMatching (EM) problem to find associated entities. Such scenarios include\nmatching jobs to candidates in job targeting, matching students with courses in\nonline education, matching products with user reviews on e-commercial websites,\nand beyond. These tasks impose new requirements such as matching data entries\nwith diverse formats or having a flexible and semantics-rich matching\ndefinition, which are beyond the current EM task formulation or approaches. In\nthis paper, we introduce the problem of Generalized Entity Matching (GEM) that\nsatisfies these practical requirements and presents an end-to-end pipeline\nMachop as the solution. Machop allows end-users to define new matching tasks\nfrom scratch and apply them to new domains in a step-by-step manner. Machop\ncasts the GEM problem as sequence pair classification so as to utilize the\nlanguage understanding capability of Transformers-based language models (LMs)\nsuch as BERT. Moreover, it features a novel external knowledge injection\napproach with structure-aware pooling methods that allow domain experts to\nguide the LM to focus on the key matching information thus further contributing\nto the overall performance. Our experiments and case studies on real-world\ndatasets from a popular recruiting platform show a significant 17.1% gain in F1\nscore against state-of-the-art methods along with meaningful matching results\nthat are human-understandable.",
    "descriptor": "\nComments: aiDM 2022\n",
    "authors": [
      "Jin Wang",
      "Yuliang Li",
      "Wataru Hirota",
      "Eser Kandogan"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.04853"
  },
  {
    "id": "arXiv:2206.04854",
    "title": "Heterogeneous Face Recognition via Face Synthesis with  Identity-Attribute Disentanglement",
    "abstract": "Heterogeneous Face Recognition (HFR) aims to match faces across different\ndomains (e.g., visible to near-infrared images), which has been widely applied\nin authentication and forensics scenarios. However, HFR is a challenging\nproblem because of the large cross-domain discrepancy, limited heterogeneous\ndata pairs, and large variation of facial attributes. To address these\nchallenges, we propose a new HFR method from the perspective of heterogeneous\ndata augmentation, named Face Synthesis with Identity-Attribute Disentanglement\n(FSIAD). Firstly, the identity-attribute disentanglement (IAD) decouples face\nimages into identity-related representations and identity-unrelated\nrepresentations (called attributes), and then decreases the correlation between\nidentities and attributes. Secondly, we devise a face synthesis module (FSM) to\ngenerate a large number of images with stochastic combinations of disentangled\nidentities and attributes for enriching the attribute diversity of synthetic\nimages. Both the original images and the synthetic ones are utilized to train\nthe HFR network for tackling the challenges and improving the performance of\nHFR. Extensive experiments on five HFR databases validate that FSIAD obtains\nsuperior performance than previous HFR approaches. Particularly, FSIAD obtains\n4.8% improvement over state of the art in terms of VR@FAR=0.01% on LAMP-HQ, the\nlargest HFR database so far.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Information Forensics and Security (TIFS)\n",
    "authors": [
      "Ziming Yang",
      "Jian Liang",
      "Chaoyou Fu",
      "Mandi Luo",
      "Xiao-Yu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04854"
  },
  {
    "id": "arXiv:2206.04855",
    "title": "Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and  Attention-based Graph Neural Network for Human Activity Recognition",
    "abstract": "Human activity recognition (HAR) through wearable devices has received much\ninterest due to its numerous applications in fitness tracking, wellness\nscreening, and supported living. As a result, we have seen a great deal of work\nin this field. Traditional deep learning (DL) has set a state of the art\nperformance for HAR domain. However, it ignores the data's structure and the\nassociation between consecutive time stamps. To address this constraint, we\noffer an approach based on Graph Neural Networks (GNNs) for structuring the\ninput representation and exploiting the relations among the samples. However,\neven when using a simple graph convolution network to eliminate this shortage,\nthere are still several limiting factors, such as inter-class activities\nissues, skewed class distribution, and a lack of consideration for sensor data\npriority, all of which harm the HAR model's performance. To improve the current\nHAR model's performance, we investigate novel possibilities within the\nframework of graph structure to achieve highly discriminated and rich activity\nfeatures. We propose a model for (1) time-series-graph module that converts raw\ndata from HAR dataset into graphs; (2) Graph Convolutional Neural Networks\n(GCNs) to discover local dependencies and correlations between neighboring\nnodes; and (3) self-attention GNN encoder to identify sensors interactions and\ndata priorities. To the best of our knowledge, this is the first work for HAR,\nwhich introduces a GNN-based approach that incorporates both the GCN and the\nattention mechanism. By employing a uniform evaluation method, our framework\nsignificantly improves the performance on hospital patient's activities dataset\ncomparatively considered other state of the art baseline methods.",
    "descriptor": "",
    "authors": [
      "Nafees Ahmad",
      "Savio Ho-Chit Chow",
      "Ho-fung Leung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.04855"
  },
  {
    "id": "arXiv:2206.04857",
    "title": "Mixed integer linear optimization formulations for learning optimal  binary classification trees",
    "abstract": "Decision trees are powerful tools for classification and regression that\nattract many researchers working in the burgeoning area of machine learning.\nOne advantage of decision trees over other methods is their interpretability,\nwhich is often preferred over other higher accuracy methods that are relatively\nuninterpretable. A binary classification tree has two types of vertices: (i)\nbranching vertices which have exactly two children and where datapoints are\nassessed on a set of discrete features; and (ii) leaf vertices at which\ndatapoints are given a discrete prediction. An optimal binary classification\ntree can be obtained by solving a biobjective optimization problem that seeks\nto (i) maximize the number of correctly classified datapoints and (ii) minimize\nthe number of branching vertices. In this paper, we propose four mixed integer\nlinear optimization (MILO) formulations for designing optimal binary\nclassification trees: two flow-based formulations and two-cut based\nformulations. We provide theoretical comparisons between our proposed\nformulations and the strongest flow-based MILO formulation of Aghaei et al.\n(2021). We conduct experiments on 13 publicly available datasets to show the\nmodels' ability to scale and the strength of a biobjective approach using\nPareto frontiers. Our code and data are available on GitHub.",
    "descriptor": "",
    "authors": [
      "Brandon Alston",
      "Hamidreza Validi",
      "Illya V. Hicks"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.04857"
  },
  {
    "id": "arXiv:2206.04860",
    "title": "Conformal Prediction Intervals for Markov Decision Process Trajectories",
    "abstract": "Before delegating a task to an autonomous system, a human operator may want a\nguarantee about the behavior of the system. This paper extends previous work on\nconformal prediction for functional data and conformalized quantile regression\nto provide conformal prediction intervals over the future behavior of an\nautonomous system executing a fixed control policy on a Markov Decision Process\n(MDP). The prediction intervals are constructed by applying conformal\ncorrections to prediction intervals computed by quantile regression. The\nresulting intervals guarantee that with probability $1-\\delta$ the observed\ntrajectory will lie inside the prediction interval, where the probability is\ncomputed with respect to the starting state distribution and the stochasticity\nof the MDP. The method is illustrated on MDPs for invasive species management\nand StarCraft2 battles.",
    "descriptor": "\nComments: 25 pages, 15 figures, 2 tables\n",
    "authors": [
      "Thomas G. Dietterich",
      "Jesse Hostetler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04860"
  },
  {
    "id": "arXiv:2206.04863",
    "title": "Symbolic image detection using scene and knowledge graphs",
    "abstract": "Sometimes the meaning conveyed by images goes beyond the list of objects they\ncontain; instead, images may express a powerful message to affect the viewers'\nminds. Inferring this message requires reasoning about the relationships\nbetween the objects, and general common-sense knowledge about the components.\nIn this paper, we use a scene graph, a graph representation of an image, to\ncapture visual components. In addition, we generate a knowledge graph using\nfacts extracted from ConceptNet to reason about objects and attributes. To\ndetect the symbols, we propose a neural network framework named SKG-Sym. The\nframework first generates the representations of the scene graph of the image\nand its knowledge graph using Graph Convolution Network. The framework then\nfuses the representations and uses an MLP to classify them. We extend the\nnetwork further to use an attention mechanism which learn the importance of the\ngraph representations. We evaluate our methods on a dataset of advertisements,\nand compare it with baseline symbolism classification methods (ResNet and VGG).\nResults show that our methods outperform ResNet in terms of F-score and the\nattention-based mechanism is competitive with VGG while it has much lower model\ncomplexity.",
    "descriptor": "",
    "authors": [
      "Nasrin Kalanat",
      "Adriana Kovashka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04863"
  },
  {
    "id": "arXiv:2206.04864",
    "title": "Binarizing Split Learning for Data Privacy Enhancement and Computation  Reduction",
    "abstract": "Split learning (SL) enables data privacy preservation by allowing clients to\ncollaboratively train a deep learning model with the server without sharing raw\ndata. However, SL still has limitations such as potential data privacy leakage\nand high computation at clients. In this study, we propose to binarize the SL\nlocal layers for faster computation (up to 17.5 times less forward-propagation\ntime in both training and inference phases on mobile devices) and reduced\nmemory usage (up to 32 times less memory and bandwidth requirements). More\nimportantly, the binarized SL (B-SL) model can reduce privacy leakage from SL\nsmashed data with merely a small degradation in model accuracy. To further\nenhance the privacy preservation, we also propose two novel approaches: 1)\ntraining with additional local leak loss and 2) applying differential privacy,\nwhich could be integrated separately or concurrently into the B-SL model.\nExperimental results with different datasets have affirmed the advantages of\nthe B-SL models compared with several benchmark models. The effectiveness of\nB-SL models against feature-space hijacking attack (FSHA) is also illustrated.\nOur results have demonstrated B-SL models are promising for lightweight\nIoT/mobile applications with high privacy-preservation requirements such as\nmobile healthcare applications.",
    "descriptor": "",
    "authors": [
      "Ngoc Duy Pham",
      "Alsharif Abuadbba",
      "Yansong Gao",
      "Tran Khoa Phan",
      "Naveen Chilamkurti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04864"
  },
  {
    "id": "arXiv:2206.04865",
    "title": "An alternative approach to the exact network reliability assessment  through the quickest path",
    "abstract": "Extending the quickest path problem to the network reliability, a new problem\nemerged which aims to assess the network reliability for transmitting at least\nd units of data from a source node to a sink node through one minimal path (MP)\nwithin given T units of time. Many of the proposed approaches in the literature\ncheck all the MPs of the network for doing the job and then construct desired\nsystem state vectors based on the accepted MPs. Hence, they need to have all\nthe MPs of the network in advance. Here, we propose a simple approach that does\nnot need any MP in advance. The algorithm is shown to be corrected and is\nillustrated through an example.",
    "descriptor": "",
    "authors": [
      "Majid Forghani-elahabad"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.04865"
  },
  {
    "id": "arXiv:2206.04867",
    "title": "The Gender Gap in Face Recognition Accuracy Is a Hairy Problem",
    "abstract": "It is broadly accepted that there is a \"gender gap\" in face recognition\naccuracy, with females having higher false match and false non-match rates.\nHowever, relatively little is known about the cause(s) of this gender gap. Even\nthe recent NIST report on demographic effects lists \"analyze cause and effect\"\nunder \"what we did not do\". We first demonstrate that female and male\nhairstyles have important differences that impact face recognition accuracy. In\nparticular, compared to females, male facial hair contributes to creating a\ngreater average difference in appearance between different male faces. We then\ndemonstrate that when the data used to estimate recognition accuracy is\nbalanced across gender for how hairstyles occlude the face, the initially\nobserved gender gap in accuracy largely disappears. We show this result for two\ndifferent matchers, and analyzing images of Caucasians and of\nAfrican-Americans. These results suggest that future research on demographic\nvariation in accuracy should include a check for balanced quality of the test\ndata as part of the problem formulation. To promote reproducible research,\nmatchers, attribute classifiers, and datasets used in this research are/will be\npublicly available.",
    "descriptor": "",
    "authors": [
      "Aman Bhatta",
      "V\u00edtor Albiero",
      "Kevin W. Bowyer",
      "Michael C. King"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04867"
  },
  {
    "id": "arXiv:2206.04869",
    "title": "Ask to Know More: Generating Counterfactual Explanations for Fake Claims",
    "abstract": "In this paper, we propose elucidating fact checking predictions using\ncounterfactual explanations to help people understand why a specific piece of\nnews was identified as fake. In this work, generating counterfactual\nexplanations for fake news involves three steps: asking good questions, finding\ncontradictions, and reasoning appropriately. We frame this research question as\ncontradicted entailment reasoning through question answering (QA). We first ask\nquestions towards the false claim and retrieve potential answers from the\nrelevant evidence documents. Then, we identify the most contradictory answer to\nthe false claim by use of an entailment classifier. Finally, a counterfactual\nexplanation is created using a matched QA pair with three different\ncounterfactual explanation forms. Experiments are conducted on the FEVER\ndataset for both system and human evaluations. Results suggest that the\nproposed approach generates the most helpful explanations compared to\nstate-of-the-art methods.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Shih-Chieh Dai",
      "Yi-Li Hsu",
      "Aiping Xiong",
      "Lun-Wei Ku"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04869"
  },
  {
    "id": "arXiv:2206.04872",
    "title": "Multi-fidelity Hierarchical Neural Processes",
    "abstract": "Science and engineering fields use computer simulation extensively. These\nsimulations are often run at multiple levels of sophistication to balance\naccuracy and efficiency. Multi-fidelity surrogate modeling reduces the\ncomputational cost by fusing different simulation outputs. Cheap data generated\nfrom low-fidelity simulators can be combined with limited high-quality data\ngenerated by an expensive high-fidelity simulator. Existing methods based on\nGaussian processes rely on strong assumptions of the kernel functions and can\nhardly scale to high-dimensional settings. We propose Multi-fidelity\nHierarchical Neural Processes (MF-HNP), a unified neural latent variable model\nfor multi-fidelity surrogate modeling. MF-HNP inherits the flexibility and\nscalability of Neural Processes. The latent variables transform the\ncorrelations among different fidelity levels from observations to latent space.\nThe predictions across fidelities are conditionally independent given the\nlatent states. It helps alleviate the error propagation issue in existing\nmethods. MF-HNP is flexible enough to handle non-nested high dimensional data\nat different fidelity levels with varying input and output dimensions. We\nevaluate MF-HNP on epidemiology and climate modeling tasks, achieving\ncompetitive performance in terms of accuracy and uncertainty estimation. In\ncontrast to deep Gaussian Processes with only low-dimensional (< 10) tasks, our\nmethod shows great promise for speeding up high-dimensional complex simulations\n(over 7000 for epidemiology modeling and 45000 for climate modeling).",
    "descriptor": "",
    "authors": [
      "Dongxia Wu",
      "Matteo Chinazzi",
      "Alessandro Vespignani",
      "Yi-An Ma",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04872"
  },
  {
    "id": "arXiv:2206.04873",
    "title": "Imitation Learning via Differentiable Physics",
    "abstract": "Existing imitation learning (IL) methods such as inverse reinforcement\nlearning (IRL) usually have a double-loop training process, alternating between\nlearning a reward function and a policy and tend to suffer long training time\nand high variance. In this work, we identify the benefits of differentiable\nphysics simulators and propose a new IL method, i.e., Imitation Learning via\nDifferentiable Physics (ILD), which gets rid of the double-loop design and\nachieves significant improvements in final performance, convergence speed, and\nstability. The proposed ILD incorporates the differentiable physics simulator\nas a physics prior into its computational graph for policy learning. It unrolls\nthe dynamics by sampling actions from a parameterized policy, simply minimizing\nthe distance between the expert trajectory and the agent trajectory, and\nback-propagating the gradient into the policy via temporal physics operators.\nWith the physics prior, ILD policies can not only be transferable to unseen\nenvironment specifications but also yield higher final performance on a variety\nof tasks. In addition, ILD naturally forms a single-loop structure, which\nsignificantly improves the stability and training speed. To simplify the\ncomplex optimization landscape induced by temporal physics operations, ILD\ndynamically selects the learning objectives for each state during optimization.\nIn our experiments, we show that ILD outperforms state-of-the-art methods in a\nvariety of continuous control tasks with Brax, requiring only one expert\ndemonstration. In addition, ILD can be applied to challenging deformable object\nmanipulation tasks and can be generalized to unseen configurations.",
    "descriptor": "",
    "authors": [
      "Siwei Chen",
      "Xiao Ma",
      "Zhongwen Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.04873"
  },
  {
    "id": "arXiv:2206.04874",
    "title": "The 1st Data Science for Pavements Challenge",
    "abstract": "The Data Science for Pavement Challenge (DSPC) seeks to accelerate the\nresearch and development of automated vision systems for pavement condition\nmonitoring and evaluation by providing a platform with benchmarked datasets and\ncodes for teams to innovate and develop machine learning algorithms that are\npractice-ready for use by industry. The first edition of the competition\nattracted 22 teams from 8 countries. Participants were required to\nautomatically detect and classify different types of pavement distresses\npresent in images captured from multiple sources, and under different\nconditions. The competition was data-centric: teams were tasked to increase the\naccuracy of a predefined model architecture by utilizing various data\nmodification methods such as cleaning, labeling and augmentation. A real-time,\nonline evaluation system was developed to rank teams based on the F1 score.\nLeaderboard results showed the promise and challenges of machine for advancing\nautomation in pavement monitoring and evaluation. This paper summarizes the\nsolutions from the top 5 teams. These teams proposed innovations in the areas\nof data cleaning, annotation, augmentation, and detection parameter tuning. The\nF1 score for the top-ranked team was approximately 0.9. The paper concludes\nwith a review of different experiments that worked well for the current\nchallenge and those that did not yield any significant improvement in model\naccuracy.",
    "descriptor": "",
    "authors": [
      "Ashkan Behzadian",
      "Tanner Wambui Muturi",
      "Tianjie Zhang",
      "Hongmin Kim",
      "Amanda Mullins",
      "Yang Lu",
      "Neema Jasika Owor",
      "Yaw Adu-Gyamfi",
      "William Buttlar",
      "Majidifard Hamed",
      "Armstrong Aboah",
      "David Mensching",
      "Spragg Robert",
      "Matthew Corrigan",
      "Jack Youtchef",
      "Dave Eshan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04874"
  },
  {
    "id": "arXiv:2206.04875",
    "title": "Smallset Timelines: A Visual Representation of Data Preprocessing  Decisions",
    "abstract": "Data preprocessing is a crucial stage in the data analysis pipeline, with\nboth technical and social aspects to consider. Yet, the attention it receives\nis often lacking in research practice and dissemination. We present the\nSmallset Timeline, a visualisation to help reflect on and communicate data\npreprocessing decisions. A \"Smallset\" is a small selection of rows from the\noriginal dataset containing instances of dataset alterations. The Timeline is\ncomprised of Smallset snapshots representing different points in the\npreprocessing stage and captions to describe the alterations visualised at each\npoint. Edits, additions, and deletions to the dataset are highlighted with\ncolour. We develop the R software package, smallsets, that can create Smallset\nTimelines from R and Python data preprocessing scripts. Constructing the figure\nasks practitioners to reflect on and revise decisions as necessary, while\nsharing it aims to make the process accessible to a diverse range of audiences.\nWe present two case studies to illustrate use of the Smallset Timeline for\nvisualising preprocessing decisions. Case studies include software defect data\nand income survey benchmark data, in which preprocessing affects levels of data\nloss and group fairness in prediction tasks, respectively. We envision Smallset\nTimelines as a go-to data provenance tool, enabling better documentation and\ncommunication of preprocessing tasks at large.",
    "descriptor": "\nComments: In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea\n",
    "authors": [
      "Lydia R. Lucchesi",
      "Petra M. Kuhnert",
      "Jenny L. Davis",
      "Lexing Xie"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04875"
  },
  {
    "id": "arXiv:2206.04879",
    "title": "Unsupervised Foggy Scene Understanding via Self Spatial-Temporal Label  Diffusion",
    "abstract": "Understanding foggy image sequence in the driving scenes is critical for\nautonomous driving, but it remains a challenging task due to the difficulty in\ncollecting and annotating real-world images of adverse weather. Recently, the\nself-training strategy has been considered a powerful solution for unsupervised\ndomain adaptation, which iteratively adapts the model from the source domain to\nthe target domain by generating target pseudo labels and re-training the model.\nHowever, the selection of confident pseudo labels inevitably suffers from the\nconflict between sparsity and accuracy, both of which will lead to suboptimal\nmodels. To tackle this problem, we exploit the characteristics of the foggy\nimage sequence of driving scenes to densify the confident pseudo labels.\nSpecifically, based on the two discoveries of local spatial similarity and\nadjacent temporal correspondence of the sequential image data, we propose a\nnovel Target-Domain driven pseudo label Diffusion (TDo-Dif) scheme. It employs\nsuperpixels and optical flows to identify the spatial similarity and temporal\ncorrespondence, respectively and then diffuses the confident but sparse pseudo\nlabels within a superpixel or a temporal corresponding pair linked by the flow.\nMoreover, to ensure the feature similarity of the diffused pixels, we introduce\nlocal spatial similarity loss and temporal contrastive loss in the model\nre-training stage. Experimental results show that our TDo-Dif scheme helps the\nadaptive model achieve 51.92% and 53.84% mean intersection-over-union (mIoU) on\ntwo publicly available natural foggy datasets (Foggy Zurich and Foggy Driving),\nwhich exceeds the state-of-the-art unsupervised domain adaptive semantic\nsegmentation methods. Models and data can be found at\nhttps://github.com/velor2012/TDo-Dif.",
    "descriptor": "\nComments: IEEE Transactions on Image Processing 2022\n",
    "authors": [
      "Liang Liao",
      "Wenyi Chen",
      "Jing Xiao",
      "Zheng Wang",
      "Chia-Wen Lin",
      "Shin'ichi Satoh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04879"
  },
  {
    "id": "arXiv:2206.04881",
    "title": "Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers",
    "abstract": "Backdoor attacks threaten Deep Neural Networks (DNNs). Towards stealthiness,\nresearchers propose clean-label backdoor attacks, which require the adversaries\nnot to alter the labels of the poisoned training datasets. Clean-label settings\nmake the attack more stealthy due to the correct image-label pairs, but some\nproblems still exist: first, traditional methods for poisoning training data\nare ineffective; second, traditional triggers are not stealthy which are still\nperceptible. To solve these problems, we propose a two-phase and image-specific\ntriggers generation method to enhance clean-label backdoor attacks. Our methods\nare (1) powerful: our triggers can both promote the two phases (i.e., the\nbackdoor implantation and activation phase) in backdoor attacks simultaneously;\n(2) stealthy: our triggers are generated from each image. They are\nimage-specific instead of fixed triggers. Extensive experiments demonstrate\nthat our approach can achieve a fantastic attack success rate~(98.98%) with low\npoisoning rate~(5%), high stealthiness under many evaluation metrics and is\nresistant to backdoor defense methods.",
    "descriptor": "",
    "authors": [
      "Nan Luo",
      "Yuanzhang Li",
      "Yajie Wang",
      "Shangbo Wu",
      "Yu-an Tan",
      "Quanxin Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04881"
  },
  {
    "id": "arXiv:2206.04882",
    "title": "$\\mathsf{G^2Retro}$: Two-Step Graph Generative Models for Retrosynthesis  Prediction",
    "abstract": "Retrosynthesis is a procedure where a molecule is transformed into potential\nreactants and thus the synthesis routes are identified. We propose a novel\ngenerative framework, denoted as $\\mathsf{G^2Retro}$, for one-step\nretrosynthesis prediction. $\\mathsf{G^2Retro}$ imitates the reversed logic of\nsynthetic reactions, that is, first predicting the reaction centers to convert\nthe target molecule into fragments named synthons, and then transforming\nsynthons into reactants, following previous semi-template-based methods. In\npredicting reaction centers, $\\mathsf{G^2Retro}$ defines a comprehensive set of\nreaction center types, and enables diversity in the predicted reactions by\nconsidering multiple reaction center candidates. In completing synthons,\n$\\mathsf{G^2Retro}$ deploys a sequence of substructure attachments to transform\nsynthons into reactants, which utilize a holistic view of the most updated\nstructures of the synthons to be completed, as well as all the involved synthon\nand product structures. Here we show that $\\mathsf{G^2Retro}$ is able to better\nprioritize the most possible reactants in the benchmark dataset than the\nstate-of-the-art methods, and discover novel and highly likely reactions that\nare not included in the benchmark dataset.",
    "descriptor": "\nComments: 26 pages, 9 figures\n",
    "authors": [
      "Ziqi Chen",
      "Oluwatosin R. Ayinde",
      "James R. Fuchs",
      "Huan Sun",
      "Xia Ning"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2206.04882"
  },
  {
    "id": "arXiv:2206.04883",
    "title": "On the Complexity of Sampling Redistricting Plans",
    "abstract": "A crucial task in the political redistricting problem is to sample\nredistricting plans i.e. a partitioning of the graph of census blocks into\ndistricts.\nWe show that Recombination [DeFord-Duchin-Solomon'21]-a popular Markov chain\nto sample redistricting plans-is exponentially slow mixing on simple subgraph\nof $\\mathbb{Z}_2.$ We show an alternative way to sample balance, compact and\ncontiguous redistricting plans using a \"relaxed\" version of ReCom and rejection\nsampling.",
    "descriptor": "",
    "authors": [
      "Moses Charikar",
      "Paul Liu",
      "Tianyu Liu",
      "Thuy-Duong Vuong"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.04883"
  },
  {
    "id": "arXiv:2206.04887",
    "title": "Deep Leakage from Model in Federated Learning",
    "abstract": "Distributed machine learning has been widely used in recent years to tackle\nthe large and complex dataset problem. Therewith, the security of distributed\nlearning has also drawn increasing attentions from both academia and industry.\nIn this context, federated learning (FL) was developed as a \"secure\"\ndistributed learning by maintaining private training data locally and only\npublic model gradients are communicated between. However, to date, a variety of\ngradient leakage attacks have been proposed for this procedure and prove that\nit is insecure. For instance, a common drawback of these attacks is shared:\nthey require too much auxiliary information such as model weights, optimizers,\nand some hyperparameters (e.g., learning rate), which are difficult to obtain\nin real situations. Moreover, many existing algorithms avoid transmitting model\ngradients in FL and turn to sending model weights, such as FedAvg, but few\npeople consider its security breach. In this paper, we present two novel\nframeworks to demonstrate that transmitting model weights is also likely to\nleak private local data of clients, i.e., (DLM and DLM+), under the FL\nscenario. In addition, a number of experiments are performed to illustrate the\neffect and generality of our attack frameworks. At the end of this paper, we\nalso introduce two defenses to the proposed attacks and evaluate their\nprotection effects. Comprehensively, the proposed attack and defense schemes\ncan be applied to the general distributed learning scenario as well, just with\nsome appropriate customization.",
    "descriptor": "",
    "authors": [
      "Zihao Zhao",
      "Mengen Luo",
      "Wenbo Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04887"
  },
  {
    "id": "arXiv:2206.04888",
    "title": "AntPivot: Livestream Highlight Detection via Hierarchical Attention  Mechanism",
    "abstract": "In recent days, streaming technology has greatly promoted the development in\nthe field of livestream. Due to the excessive length of livestream records,\nit's quite essential to extract highlight segments with the aim of effective\nreproduction and redistribution. Although there are lots of approaches proven\nto be effective in the highlight detection for other modals, the challenges\nexisting in livestream processing, such as the extreme durations, large topic\nshifts, much irrelevant information and so forth, heavily hamper the adaptation\nand compatibility of these methods. In this paper, we formulate a new task\nLivestream Highlight Detection, discuss and analyze the difficulties listed\nabove and propose a novel architecture AntPivot to solve this problem.\nConcretely, we first encode the original data into multiple views and model\ntheir temporal relations to capture clues in a hierarchical attention\nmechanism. Afterwards, we try to convert the detection of highlight clips into\nthe search for optimal decision sequences and use the fully integrated\nrepresentations to predict the final results in a dynamic-programming\nmechanism. Furthermore, we construct a fully-annotated dataset AntHighlight to\ninstantiate this task and evaluate the performance of our model. The extensive\nexperiments indicate the effectiveness and validity of our proposed method.",
    "descriptor": "",
    "authors": [
      "Yang Zhao",
      "Xuan Lin",
      "Wenqiang Xu",
      "Maozong Zheng",
      "Zhengyong Liu",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04888"
  },
  {
    "id": "arXiv:2206.04890",
    "title": "Adversarial Counterfactual Environment Model Learning",
    "abstract": "A good model for action-effect prediction, named environment model, is\nimportant to achieve sample-efficient decision-making policy learning in many\ndomains like robot control, recommender systems, and patients' treatment\nselection. We can take unlimited trials with such a model to identify the\nappropriate actions so that the costs of queries in the real world can be\nsaved. It requires the model to handle unseen data correctly, also called\ncounterfactual data. However, standard data fitting techniques do not\nautomatically achieve such generalization ability and commonly result in\nunreliable models. In this work, we introduce counterfactual-query risk\nminimization (CQRM) in model learning for generalizing to a counterfactual\ndataset queried by a specific target policy. Since the target policies can be\nvarious and unknown in policy learning, we propose an adversarial CQRM\nobjective in which the model learns on counterfactual data queried by\nadversarial policies, and finally derive a tractable solution GALILEO. We also\ndiscover that adversarial CQRM is closely related to the adversarial model\nlearning, explaining the effectiveness of the latter. We apply GALILEO in\nsynthetic tasks and a real-world application. The results show that GALILEO\nmakes accurate predictions on counterfactual data and thus significantly\nimproves policies in real-world testing.",
    "descriptor": "",
    "authors": [
      "Xiong-Hui Chen",
      "Yang Yu",
      "Zheng-Mao Zhu",
      "Zhihua Yu",
      "Zhenjun Chen",
      "Chenghe Wang",
      "Yinan Wu",
      "Hongqiu Wu",
      "Rong-Jun Qin",
      "Ruijin Ding",
      "Fangsheng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04890"
  },
  {
    "id": "arXiv:2206.04891",
    "title": "Explaining Neural Networks without Access to Training Data",
    "abstract": "We consider generating explanations for neural networks in cases where the\nnetwork's training data is not accessible, for instance due to privacy or\nsafety issues. Recently, $\\mathcal{I}$-Nets have been proposed as a sample-free\napproach to post-hoc, global model interpretability that does not require\naccess to training data. They formulate interpretation as a machine learning\ntask that maps network representations (parameters) to a representation of an\ninterpretable function. In this paper, we extend the $\\mathcal{I}$-Net\nframework to the cases of standard and soft decision trees as surrogate models.\nWe propose a suitable decision tree representation and design of the\ncorresponding $\\mathcal{I}$-Net output layers. Furthermore, we make\n$\\mathcal{I}$-Nets applicable to real-world tasks by considering more realistic\ndistributions when generating the $\\mathcal{I}$-Net's training data. We\nempirically evaluate our approach against traditional global, post-hoc\ninterpretability approaches and show that it achieves superior results when the\ntraining data is not accessible.",
    "descriptor": "",
    "authors": [
      "Sascha Marton",
      "Stefan L\u00fcdtke",
      "Christian Bartelt",
      "Andrej Tschalzev",
      "Heiner Stuckenschmidt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04891"
  },
  {
    "id": "arXiv:2206.04893",
    "title": "Provable Guarantees for Sparsity Recovery with Deterministic Missing  Data Patterns",
    "abstract": "We study the problem of consistently recovering the sparsity pattern of a\nregression parameter vector from correlated observations governed by\ndeterministic missing data patterns using Lasso. We consider the case in which\nthe observed dataset is censored by a deterministic, non-uniform filter.\nRecovering the sparsity pattern in datasets with deterministic missing\nstructure can be arguably more challenging than recovering in a\nuniformly-at-random scenario. In this paper, we propose an efficient algorithm\nfor missing value imputation by utilizing the topological property of the\ncensorship filter. We then provide novel theoretical results for exact recovery\nof the sparsity pattern using the proposed imputation strategy. Our analysis\nshows that, under certain statistical and topological conditions, the hidden\nsparsity pattern can be recovered consistently with high probability in\npolynomial time and logarithmic sample complexity.",
    "descriptor": "",
    "authors": [
      "Chuyang Ke",
      "Jean Honorio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04893"
  },
  {
    "id": "arXiv:2206.04901",
    "title": "NeRF-In: Free-Form NeRF Inpainting with RGB-D Priors",
    "abstract": "Though Neural Radiance Field (NeRF) demonstrates compelling novel view\nsynthesis results, it is still unintuitive to edit a pre-trained NeRF because\nthe neural network's parameters and the scene geometry/appearance are often not\nexplicitly associated. In this paper, we introduce the first framework that\nenables users to remove unwanted objects or retouch undesired regions in a 3D\nscene represented by a pre-trained NeRF without any category-specific data and\ntraining. The user first draws a free-form mask to specify a region containing\nunwanted objects over a rendered view from the pre-trained NeRF. Our framework\nfirst transfers the user-provided mask to other rendered views and estimates\nguiding color and depth images within these transferred masked regions. Next,\nwe formulate an optimization problem that jointly inpaints the image content in\nall masked regions across multiple views by updating the NeRF model's\nparameters. We demonstrate our framework on diverse scenes and show it obtained\nvisual plausible and structurally consistent results across multiple views\nusing shorter time and less user manual efforts.",
    "descriptor": "\nComments: Hao-Kang Liu and I-Chao Shen contributed equally to the paper. Project page: this https URL\n",
    "authors": [
      "Hao-Kang Liu",
      "I-Chao Shen",
      "Bing-Yu Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.04901"
  },
  {
    "id": "arXiv:2206.04906",
    "title": "Out of Sight, Out of Mind: A Source-View-Wise Feature Aggregation for  Multi-View Image-Based Rendering",
    "abstract": "To estimate the volume density and color of a 3D point in the multi-view\nimage-based rendering, a common approach is to inspect the consensus existence\namong the given source image features, which is one of the informative cues for\nthe estimation procedure. To this end, most of the previous methods utilize\nequally-weighted aggregation features. However, this could make it hard to\ncheck the consensus existence when some outliers, which frequently occur by\nocclusions, are included in the source image feature set. In this paper, we\npropose a novel source-view-wise feature aggregation method, which facilitates\nus to find out the consensus in a robust way by leveraging local structures in\nthe feature set. We first calculate the source-view-wise distance distribution\nfor each source feature for the proposed aggregation. After that, the distance\ndistribution is converted to several similarity distributions with the proposed\nlearnable similarity mapping functions. Finally, for each element in the\nfeature set, the aggregation features are extracted by calculating the weighted\nmeans and variances, where the weights are derived from the similarity\ndistributions. In experiments, we validate the proposed method on various\nbenchmark datasets, including synthetic and real image scenes. The experimental\nresults demonstrate that incorporating the proposed features improves the\nperformance by a large margin, resulting in the state-of-the-art performance.",
    "descriptor": "",
    "authors": [
      "Geonho Cha",
      "Chaehun Shin",
      "Sungroh Yoon",
      "Dongyoon Wee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04906"
  },
  {
    "id": "arXiv:2206.04907",
    "title": "Efficient Heterogeneous Treatment Effect Estimation With Multiple  Experiments and Multiple Outcomes",
    "abstract": "Learning heterogeneous treatment effects (HTEs) is an important problem\nacross many fields. Most existing methods consider the setting with a single\ntreatment arm and a single outcome metric. However, in many real world domains,\nexperiments are run consistently - for example, in internet companies, A/B\ntests are run every day to measure the impacts of potential changes across many\ndifferent metrics of interest. We show that even if an analyst cares only about\nthe HTEs in one experiment for one metric, precision can be improved greatly by\nanalyzing all of the data together to take advantage of cross-experiment and\ncross-outcome metric correlations. We formalize this idea in a tensor\nfactorization framework and propose a simple and scalable model which we refer\nto as the low rank or LR-learner. Experiments in both synthetic and real data\nsuggest that the LR-learner can be much more precise than independent HTE\nestimation.",
    "descriptor": "",
    "authors": [
      "Leon Yao",
      "Caroline Lo",
      "Israel Nir",
      "Sarah Tan",
      "Ariel Evnine",
      "Adam Lerer",
      "Alex Peysakhovich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.04907"
  },
  {
    "id": "arXiv:2206.04909",
    "title": "ABCDE: An Agent-Based Cognitive Development Environment",
    "abstract": "Children's cognitive abilities are sometimes cited as AI benchmarks. How can\nthe most common 1,000 concepts (89\\% of everyday use) be learnt in a\nnaturalistic children's setting? Cognitive development in children is about\nquality, and new concepts can be conveyed via simple examples. Our approach of\nknowledge scaffolding uses simple objects and actions to convey concepts, like\nhow children are taught. We introduce ABCDE, an interactive 3D environment\nmodeled after a typical playroom for children. It comes with 300+ unique 3D\nobject assets (mostly toys), and a large action space for child and parent\nagents to interact with objects and each other. ABCDE is the first environment\naimed at mimicking a naturalistic setting for cognitive development in\nchildren; no other environment focuses on high-level concept learning through\nlearner-teacher interactions. The simulator can be found at\nhttps://pypi.org/project/ABCDESim/1.0.0/",
    "descriptor": "\nComments: Accepted to CVPRW 2022,Embodied AI Workshop (Extended Abstract)\n",
    "authors": [
      "Jieyi Ye",
      "Jiafei Duan",
      "Samson Yu",
      "Bihan Wen",
      "Cheston Tan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04909"
  },
  {
    "id": "arXiv:2206.04910",
    "title": "NAGphormer: Neighborhood Aggregation Graph Transformer for Node  Classification in Large Graphs",
    "abstract": "Graph Transformers have demonstrated superiority on various graph learning\ntasks in recent years. However, the complexity of existing Graph Transformers\nscales quadratically with the number of nodes, making it hard to scale to\ngraphs with thousands of nodes. To this end, we propose a Neighborhood\nAggregation Graph Transformer (NAGphormer) that is scalable to large graphs\nwith millions of nodes. Before feeding the node features into the Transformer\nmodel, NAGphormer constructs tokens for each node by a neighborhood aggregation\nmodule called Hop2Token. For each node, Hop2Token aggregates neighborhood\nfeatures from each hop into a representation, and thereby produces a sequence\nof token vectors. Subsequently, the resulting sequence of different hop\ninformation serves as input to the Transformer model. By considering each node\nas a sequence, NAGphormer could be trained in a mini-batch manner and thus\ncould scale to large graphs. NAGphormer further develops an attention-based\nreadout function so as to learn the importance of each hop adaptively. We\nconduct extensive experiments on various popular benchmarks, including six\nsmall datasets and three large datasets. The results demonstrate that\nNAGphormer consistently outperforms existing Graph Transformers and mainstream\nGraph Neural Networks.",
    "descriptor": "",
    "authors": [
      "Jinsong Chen",
      "Kaiyuan Gao",
      "Gaichao Li",
      "Kun He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04910"
  },
  {
    "id": "arXiv:2206.04911",
    "title": "NSSIA: A New Self-Sovereign Identity Scheme with Accountability",
    "abstract": "Self-Sovereign Identity (SSI) is a new distributed method for identity\nmanagement, commonly used to address the problem that users are lack of control\nover their identities. However, the excessive pursuit of self-sovereignty in\nthe most existing SSI schemes hinders sanctions against attackers. To deal with\nthe malicious behavior, a few SSI schemes introduce accountability mechanisms,\nbut they sacrifice users' privacy. What's more, the digital identities (static\nstrings or updatable chains) in the existing SSI schemes are as inputs to a\nthird-party executable program (mobile app, smart contract, etc.) to achieve\nidentity reading, storing and proving, users' self-sovereignty are weakened. To\nsolve the above problems, we present a new self-sovereign identity scheme to\nstrike a balance between privacy and accountability and get rid of the\ndependence on the third-party program. In our scheme, one and only\nindividual-specific executable code is generated as a digital avatar-i for each\nhuman to interact with others in cyberspace without a third-party program, in\nwhich the embedding of biometrics enhances uniqueness and user control over\ntheir identity. In addition, a joint accountability mechanism, which is based\non the shamir (t, n) threshold algorithm and a consortium blockchain, is\ndesigned to restrict the power of each regulatory authority and protect users'\nprivacy. Finally, we analyze the security, SSI properties and conduct detailed\nexperiments in term of the cost of computation, storage and blockchain gas. The\nanalysis results indicate that our scheme resists the known attacks and\nfulfills all the six SSI properties. Compared with the state-of-the-art\nschemes, the extensive experiment results show that the cost is larger in\nserver storage, blockchain storage and blockchain gas, but is still low enough\nfor practical situations.",
    "descriptor": "",
    "authors": [
      "Qiuyun Lyu",
      "Shaopeng Cheng",
      "Hao Li",
      "Junliang Liu",
      "Yanzhao Shen",
      "Zhen Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04911"
  },
  {
    "id": "arXiv:2206.04916",
    "title": "PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape  Completion on Unseen Categories",
    "abstract": "While 3D shape representations enable powerful reasoning in many visual and\nperception applications, learning 3D shape priors tends to be constrained to\nthe specific categories trained on, leading to an inefficient learning process,\nparticularly for general applications with unseen categories. Thus, we propose\nPatchComplete, which learns effective shape priors based on multi-resolution\nlocal patches, which are often more general than full shapes (e.g., chairs and\ntables often both share legs) and thus enable geometric reasoning about unseen\nclass categories. To learn these shared substructures, we learn\nmulti-resolution patch priors across all train categories, which are then\nassociated to input partial shape observations by attention across the patch\npriors, and finally decoded into a complete shape reconstruction. Such\npatch-based priors avoid overfitting to specific train categories and enable\nreconstruction on entirely unseen categories at test time. We demonstrate the\neffectiveness of our approach on synthetic ShapeNet data as well as challenging\nreal-scanned objects from ScanNet, which include noise and clutter, improving\nover state of the art in novel-category shape completion by 19.3% in chamfer\ndistance on ShapeNet, and 9.0% for ScanNet.",
    "descriptor": "\nComments: Video link: this https URL ; Project page: this https URL\n",
    "authors": [
      "Yuchen Rao",
      "Yinyu Nie",
      "Angela Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04916"
  },
  {
    "id": "arXiv:2206.04920",
    "title": "Fisher SAM: Information Geometry and Sharpness Aware Minimisation",
    "abstract": "Recent sharpness-aware minimisation (SAM) is known to find flat minima which\nis beneficial for better generalisation with improved robustness. SAM\nessentially modifies the loss function by reporting the maximum loss value\nwithin the small neighborhood around the current iterate. However, it uses the\nEuclidean ball to define the neighborhood, which can be inaccurate since loss\nfunctions for neural networks are typically defined over probability\ndistributions (e.g., class predictive probabilities), rendering the parameter\nspace non Euclidean. In this paper we consider the information geometry of the\nmodel parameter space when defining the neighborhood, namely replacing SAM's\nEuclidean balls with ellipsoids induced by the Fisher information. Our\napproach, dubbed Fisher SAM, defines more accurate neighborhood structures that\nconform to the intrinsic metric of the underlying statistical manifold. For\ninstance, SAM may probe the worst-case loss value at either a too nearby or\ninappropriately distant point due to the ignorance of the parameter space\ngeometry, which is avoided by our Fisher SAM. Another recent Adaptive SAM\napproach stretches/shrinks the Euclidean ball in accordance with the scale of\nthe parameter magnitudes. This might be dangerous, potentially destroying the\nneighborhood structure. We demonstrate improved performance of the proposed\nFisher SAM on several benchmark datasets/tasks.",
    "descriptor": "",
    "authors": [
      "Minyoung Kim",
      "Da Li",
      "Shell Xu Hu",
      "Timothy M. Hospedales"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04920"
  },
  {
    "id": "arXiv:2206.04921",
    "title": "Offline Stochastic Shortest Path: Learning, Evaluation and Towards  Optimality",
    "abstract": "Goal-oriented Reinforcement Learning, where the agent needs to reach the goal\nstate while simultaneously minimizing the cost, has received significant\nattention in real-world applications. Its theoretical formulation, stochastic\nshortest path (SSP), has been intensively researched in the online setting.\nNevertheless, it remains understudied when such an online interaction is\nprohibited and only historical data is provided. In this paper, we consider the\noffline stochastic shortest path problem when the state space and the action\nspace are finite. We design the simple value iteration-based algorithms for\ntackling both offline policy evaluation (OPE) and offline policy learning\ntasks. Notably, our analysis of these simple algorithms yields strong\ninstance-dependent bounds which can imply worst-case bounds that are\nnear-minimax optimal. We hope our study could help illuminate the fundamental\nstatistical limits of the offline SSP problem and motivate further studies\nbeyond the scope of current consideration.",
    "descriptor": "\nComments: UAI 2022\n",
    "authors": [
      "Ming Yin",
      "Wenjing Chen",
      "Mengdi Wang",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04921"
  },
  {
    "id": "arXiv:2206.04922",
    "title": "A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural  Machine Translation",
    "abstract": "Chinese dialect text-to-speech(TTS) system usually can only be utilized by\nnative linguists, because the written form of Chinese dialects has different\ncharacters, idioms, grammar and usage from Mandarin, and even the local speaker\ncannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS\ncan only generate partly-meaningful speech with relatively poor prosody and\nnaturalness. To lower the bar of use and make it more practical in commercial,\nwe propose a novel Chinese dialect TTS frontend with a translation module. It\nhelps to convert Mandarin text into idiomatic expressions with correct\northography and grammar, so that the intelligibility and naturalness of the\nsynthesized speech can be improved. A non-autoregressive neural machine\ntranslation model with a glancing sampling strategy is proposed for the\ntranslation task. It is the first known work to incorporate translation with\nTTS frontend. Our experiments on Cantonese approve that the proposed frontend\ncan help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin\ninputs.",
    "descriptor": "\nComments: Submitted to INTERSPEECH 2022, 5 pages,5 figures\n",
    "authors": [
      "Wudi Bao",
      "Junhui Zhang",
      "Junjie Pan",
      "Xiang Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04922"
  },
  {
    "id": "arXiv:2206.04924",
    "title": "A bio-inspired implementation of a sparse-learning spike-based  hippocampus memory model",
    "abstract": "The nervous system, more specifically, the brain, is capable of solving\ncomplex problems simply and efficiently, far surpassing modern computers. In\nthis regard, neuromorphic engineering is a research field that focuses on\nmimicking the basic principles that govern the brain in order to develop\nsystems that achieve such computational capabilities. Within this field,\nbio-inspired learning and memory systems are still a challenge to be solved,\nand this is where the hippocampus is involved. It is the region of the brain\nthat acts as a short-term memory, allowing the learning and unstructured and\nrapid storage of information from all the sensory nuclei of the cerebral cortex\nand its subsequent recall. In this work, we propose a novel bio-inspired memory\nmodel based on the hippocampus with the ability to learn memories, recall them\nfrom a cue (a part of the memory associated with the rest of the content) and\neven forget memories when trying to learn others with the same cue. This model\nhas been implemented on the SpiNNaker hardware platform using Spiking Neural\nNetworks, and a set of experiments and tests were performed to demonstrate its\ncorrect and expected operation. The proposed spike-based memory model generates\nspikes only when it receives an input, being energy efficient, and it needs 7\ntimesteps for the learning step and 6 timesteps for recalling a\npreviously-stored memory. This work presents the first hardware implementation\nof a fully functional bio-inspired spike-based hippocampus memory model, paving\nthe road for the development of future more complex neuromorphic systems.",
    "descriptor": "\nComments: 15 pages, 7 figures, 3 tables, journal, Neural Networks\n",
    "authors": [
      "Daniel Casanueva-Morato",
      "Alvaro Ayuso-Martinez",
      "Juan P. Dominguez-Morales",
      "Angel Jimenez-Fernandez",
      "Gabriel Jimenez-Moreno"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04924"
  },
  {
    "id": "arXiv:2206.04925",
    "title": "RuCoCo: a new Russian corpus with coreference annotation",
    "abstract": "We present a new corpus with coreference annotation, Russian Coreference\nCorpus (RuCoCo). The goal of RuCoCo is to obtain a large number of annotated\ntexts while maintaining high inter-annotator agreement. RuCoCo contains news\ntexts in Russian, part of which were annotated from scratch, and for the rest\nthe machine-generated annotations were refined by human annotators. The size of\nour corpus is one million words and around 150,000 mentions. We make the corpus\npublicly available.",
    "descriptor": "",
    "authors": [
      "Vladimir Dobrovolskii",
      "Mariia Michurina",
      "Alexandra Ivoylova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04925"
  },
  {
    "id": "arXiv:2206.04927",
    "title": "Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose  Estimation",
    "abstract": "Color-based two-hand 3D pose estimation in the global coordinate system is\nessential in many applications. However, there are very few datasets dedicated\nto this task and no existing dataset supports estimation in a non-laboratory\nenvironment. This is largely attributed to the sophisticated data collection\nprocess required for 3D hand pose annotations, which also leads to difficulty\nin obtaining instances with the level of visual diversity needed for estimation\nin the wild. Progressing towards this goal, a large-scale dataset Ego2Hands was\nrecently proposed to address the task of two-hand segmentation and detection in\nthe wild. The proposed composition-based data generation technique can create\ntwo-hand instances with quality, quantity and diversity that generalize well to\nunseen domains. In this work, we present Ego2HandsPose, an extension of\nEgo2Hands that contains 3D hand pose annotation and is the first dataset that\nenables color-based two-hand 3D tracking in unseen domains. To this end, we\ndevelop a set of parametric fitting algorithms to enable 1) 3D hand pose\nannotation using a single image, 2) automatic conversion from 2D to 3D hand\nposes and 3) accurate two-hand tracking with temporal consistency. We provide\nincremental quantitative analysis on the multi-stage pipeline and show that\ntraining on our dataset achieves state-of-the-art results that significantly\noutperforms other datasets for the task of egocentric two-hand global 3D pose\nestimation.",
    "descriptor": "",
    "authors": [
      "Fanqing Lin",
      "Tony Martinez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04927"
  },
  {
    "id": "arXiv:2206.04928",
    "title": "MAREO: Memory- and Attention- based visual REasOning",
    "abstract": "Humans continue to vastly outperform modern AI systems in their ability to\nparse and understand complex visual scenes flexibly. Attention and memory are\ntwo systems known to play a critical role in our ability to selectively\nmaintain and manipulate behaviorally-relevant visual information to solve some\nof the most challenging visual reasoning tasks. Here, we present a novel\narchitecture for visual reasoning inspired by the cognitive-science literature\non visual reasoning, the Memory- and Attention-based (visual) REasOning (MAREO)\narchitecture. MAREO instantiates an active-vision theory, which posits that the\nbrain solves complex visual reasoning problems compositionally by learning to\ncombine previously-learned elementary visual operations to form more complex\nvisual routines. MAREO learns to solve visual reasoning tasks via sequences of\nattention shifts to route and maintain task-relevant visual information into a\nmemory bank via a multi-head transformer module. Visual routines are then\ndeployed by a dedicated reasoning module trained to judge various relations\nbetween objects in the scenes. Experiments on four types of reasoning tasks\ndemonstrate MAREO's ability to learn visual routines in a robust and\nsample-efficient manner.",
    "descriptor": "",
    "authors": [
      "Mohit Vaishnav",
      "Thomas Serre"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.04928"
  },
  {
    "id": "arXiv:2206.04933",
    "title": "Availability-Aware Dynamic RSA with Protection using Consecutive  Sub-Channels",
    "abstract": "Flexible grid Optical Networks provide efficient spectrum utilization by\nemploying the mechanisms to provide flexibility in the optical channel\n(spectrum slot) sizes. One of the research problems in Flexible grid Optical\nNetworks is their survivability against failure. On the other hand, p-Cycles\nhave not found practical use due to the significant compute time required for\nfinding optimal configuration for the size of networks seen in real-life.\nTherefore, for real-time scenarios, we can write heuristics which can assign\nprotection to the new working paths without disturbing the existing traffic on\nall the other routes in flexible grid networks. The provisioning of protection\nto each link or path of the lightpath requests can be done using Dynamic Cycles\n(D-Cycles) or Dynamic Shared Backup Resource Protection (D-SBRP). However,\nprotecting each link or path can lead to the wastage of the resources in the\nnetwork.",
    "descriptor": "",
    "authors": [
      "Varsha Lohani",
      "Anjali Sharma",
      "Yatindra Nath Singh"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.04933"
  },
  {
    "id": "arXiv:2206.04934",
    "title": "Response to: Significance and stability of deep learning-based  identification of subtypes within major psychiatric disorders. Molecular  Psychiatry (2022)",
    "abstract": "Recently, Winter and Hahn [1] commented on our work on identifying subtypes\nof major psychiatry disorders (MPDs) based on neurobiological features using\nmachine learning [2]. They questioned the generalizability of our methods and\nthe statistical significance, stability, and overfitting of the results, and\nproposed a pipeline for disease subtyping. We appreciate their earnest\nconsideration of our work, however, we need to point out their misconceptions\nof basic machine-learning concepts and delineate some key issues involved.",
    "descriptor": "",
    "authors": [
      "Xizhe Zhang",
      "Fei Wang",
      "Weixiong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04934"
  },
  {
    "id": "arXiv:2206.04935",
    "title": "Sort by Structure: Language Model Ranking as Dependency Probing",
    "abstract": "Making an informed choice of pre-trained language model (LM) is critical for\nperformance, yet environmentally costly, and as such widely underexplored. The\nfield of Computer Vision has begun to tackle encoder ranking, with promising\nforays into Natural Language Processing, however they lack coverage of\nlinguistic tasks such as structured prediction. We propose probing to rank LMs,\nspecifically for parsing dependencies in a given language, by measuring the\ndegree to which labeled trees are recoverable from an LM's contextualized\nembeddings. Across 46 typologically and architecturally diverse LM-language\npairs, our probing approach predicts the best LM choice 79% of the time using\norders of magnitude less compute than training a full parser. Within this\nstudy, we identify and analyze one recently proposed decoupled LM - RemBERT -\nand find it strikingly contains less inherent dependency information, but often\nyields the best parser after full fine-tuning. Without this outlier our\napproach identifies the best LM in 89% of cases.",
    "descriptor": "\nComments: Accepted at NAACL 2022 (Main Conference)\n",
    "authors": [
      "Max M\u00fcller-Eberstein",
      "Rob van der Goot",
      "Barbara Plank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04935"
  },
  {
    "id": "arXiv:2206.04936",
    "title": "Improved lower and upper bounds for LCD codes?",
    "abstract": "Linear complementary dual (LCD) codes are linear codes which intersect their\ndual codes trivially, which have been of interest and extensively studied due\nto its wide applications. In this paper, we give some methods for constructing\nLCD codes over small fields by modifying some known methods. We show that all\nodd-like binary LCD codes, ternary LCD codes and quaternary Hermitian LCD codes\ncan be constructed by the modified methods. Using these methods, we construct a\nlot of optimal binary LCD codes, ternary LCD codes and quaternary Hermitian LCD\ncodes, which improve the known lower bounds on the largest minimum weights.\nFurthermore, we give two counterexamples to show that the conjecture proposed\nby Bouyuklieva (Des. Codes Cryptogr. 89(11): 2445-2461, 2021) is invalid.",
    "descriptor": "",
    "authors": [
      "Shitao Li",
      "Minjia Shi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.04936"
  },
  {
    "id": "arXiv:2206.04937",
    "title": "Generate, Evaluate, and Select: A Dialogue System with a Response  Evaluator for Diversity-Aware Response Generation",
    "abstract": "We aim to overcome the lack of diversity in responses of current dialogue\nsystems and to develop a dialogue system that is engaging as a conversational\npartner. We propose a generator-evaluator model that evaluates multiple\nresponses generated by a response generator and selects the best response by an\nevaluator. By generating multiple responses, we obtain diverse responses. We\nconduct human evaluations to compare the output of the proposed system with\nthat of a baseline system. The results of the human evaluations showed that the\nproposed system's responses were often judged to be better than the baseline\nsystem's, and indicated the effectiveness of the proposed method.",
    "descriptor": "\nComments: NAACL 2022 SRW Accepted\n",
    "authors": [
      "Ryoma Sakaeda",
      "Daisuke Kawahara"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04937"
  },
  {
    "id": "arXiv:2206.04942",
    "title": "Neural Template: Topology-aware Reconstruction and Disentangled  Generation of 3D Meshes",
    "abstract": "This paper introduces a novel framework called DTNet for 3D mesh\nreconstruction and generation via Disentangled Topology. Beyond previous works,\nwe learn a topology-aware neural template specific to each input then deform\nthe template to reconstruct a detailed mesh while preserving the learned\ntopology. One key insight is to decouple the complex mesh reconstruction into\ntwo sub-tasks: topology formulation and shape deformation. Thanks to the\ndecoupling, DT-Net implicitly learns a disentangled representation for the\ntopology and shape in the latent space. Hence, it can enable novel disentangled\ncontrols for supporting various shape generation applications, e.g., remix the\ntopologies of 3D objects, that are not achievable by previous reconstruction\nworks. Extensive experimental results demonstrate that our method is able to\nproduce high-quality meshes, particularly with diverse topologies, as compared\nwith the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Ka-Hei Hui",
      "Ruihui Li",
      "Jingyu Hu",
      "Chi-Wing Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04942"
  },
  {
    "id": "arXiv:2206.04944",
    "title": "A Report on Achieving Complete Regular-Expression Matching using Mealy  Machines",
    "abstract": "While regexp matching is a powerful mechanism for finding patterns in data\nstreams, regexp engines in general only find matches that do not overlap.\nMoreover, different forms of nondeterministic exploration, where symbols read\nare processed more than once, are often used, which can be costly in real-time\nmatching. We present an algorithm that constructs from any regexp a Mealy\nmachine that finds all matches and while reading each input symbol only once.\nThe machine computed can also detect and distinguish different patterns or\nsub-patterns inside patterns. Additionally, we show how to compute a minimal\nMealy machine via a variation of DFA minimization, by formalizing Mealy\nmachines in terms of regular languages.",
    "descriptor": "\nComments: Local technical report\n",
    "authors": [
      "Ricardo Almeida"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.04944"
  },
  {
    "id": "arXiv:2206.04947",
    "title": "Semantic Technology based Usage Control for Decentralized Systems",
    "abstract": "The sharing of data and digital assets in a decentralized settling is\nassociated with various legislative challenges, including, but not limited to,\nthe need to adhere to legal requirements with respect to privacy (e.g. data\nprotection legislation) and copyright (e.g. copyright legislation). In order to\nenable software platform providers to manage data and digital assets\nappropriately and to provide more control to data and digital asset owners,\nusage control technologies could be used to make sure that consumers handle\ndata according to privacy preferences, licenses, regulatory requirements, among\nothers. In this research proposal, we explore the application of usage control\nin decentralized environments. In particular, we address the challenges related\nto the specification of usage control policies, the enforcement of the\nrespective policies, and the usability of the tools that are used to administer\nthem.",
    "descriptor": "",
    "authors": [
      "Ines Akaichi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)",
      "Logic in Computer Science (cs.LO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04947"
  },
  {
    "id": "arXiv:2206.04948",
    "title": "A Holistic Robust Motion Controller Framework for Autonomous Platooning",
    "abstract": "Safety is the foremost concern for autonomous platooning. The\nvehicle-to-vehicle (V2V) communication delay and the sudden appearance of\nobstacles will trigger the safety of the intended functionality (SOTIF) issues\nfor autonomous platooning. This research proposes a holistic robust motion\ncontroller framework (MCF) for an intelligent and connected vehicle platoon\nsystem. The MCF utilizes a hierarchical structure to resolve the longitudinal\nstring stability and the lateral control problem under the complex driving\nenvironment and time-varying communication delay. Firstly, the H-infinity\nfeedback controller is developed to ensure the robustness of the platoon under\ntime-varying communication delay in the upper-level coordination layer (UCL).\nThe output from UCL will be delivered to the lower-level motion-planning layer\n(LML) as reference signals. Secondly, the model predictive control (MPC)\nalgorithm is implemented in the LML to achieve multi-objective control, which\ncomprehensively considers the reference signals, the artificial potential\nfield, and multiple vehicle dynamics constraints. Furthermore, three critical\nscenarios are co-simulated for case studies, including platooning under\ntime-varying communication delay, merging, and obstacle avoidance scenarios.\nThe simulation results indicate that, compared with single-structure MPC, the\nproposed MCF can offer a better suppression on position error propagation, and\nget improvements on maximum position error in the three scenarios by $19.2\\%$,\n$59.8\\%$, and $15.3\\%$, respectively. Last, the practicability and\neffectiveness of the proposed MCF are verified via hardware-in-the-loop\nexperiment. The average conducting time of the proposed method on Speedgoat\nreal-time target machine is 1.1 milliseconds, which meets the real-time\nrequirements.",
    "descriptor": "\nComments: 13 pages, 20 figures\n",
    "authors": [
      "Hong Wang",
      "Li-Ming Peng",
      "Zi-Chun Wei",
      "Kai Yang",
      "Xian-Xu Bai",
      "Luo Jiang",
      "Ehsan Hashemi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04948"
  },
  {
    "id": "arXiv:2206.04949",
    "title": "Deep Multi-view Semi-supervised Clustering with Sample Pairwise  Constraints",
    "abstract": "Multi-view clustering has attracted much attention thanks to the capacity of\nmulti-source information integration. Although numerous advanced methods have\nbeen proposed in past decades, most of them generally overlook the significance\nof weakly-supervised information and fail to preserve the feature properties of\nmultiple views, thus resulting in unsatisfactory clustering performance. To\naddress these issues, in this paper, we propose a novel Deep Multi-view\nSemi-supervised Clustering (DMSC) method, which jointly optimizes three kinds\nof losses during networks finetuning, including multi-view clustering loss,\nsemi-supervised pairwise constraint loss and multiple autoencoders\nreconstruction loss. Specifically, a KL divergence based multi-view clustering\nloss is imposed on the common representation of multi-view data to perform\nheterogeneous feature optimization, multi-view weighting and clustering\nprediction simultaneously. Then, we innovatively propose to integrate pairwise\nconstraints into the process of multi-view clustering by enforcing the learned\nmulti-view representation of must-link samples (cannot-link samples) to be\nsimilar (dissimilar), such that the formed clustering architecture can be more\ncredible. Moreover, unlike existing rivals that only preserve the encoders for\neach heterogeneous branch during networks finetuning, we further propose to\ntune the intact autoencoders frame that contains both encoders and decoders. In\nthis way, the issue of serious corruption of view-specific and view-shared\nfeature space could be alleviated, making the whole training procedure more\nstable. Through comprehensive experiments on eight popular image datasets, we\ndemonstrate that our proposed approach performs better than the\nstate-of-the-art multi-view and single-view competitors.",
    "descriptor": "",
    "authors": [
      "Rui Chen",
      "Yongqiang Tang",
      "Wensheng Zhang",
      "Wenlong Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04949"
  },
  {
    "id": "arXiv:2206.04951",
    "title": "Evolutionary Echo State Network: evolving reservoirs in the Fourier  space",
    "abstract": "The Echo State Network (ESN) is a class of Recurrent Neural Network with a\nlarge number of hidden-hidden weights (in the so-called reservoir). Canonical\nESN and its variations have recently received significant attention due to\ntheir remarkable success in the modeling of non-linear dynamical systems. The\nreservoir is randomly connected with fixed weights that don't change in the\nlearning process. Only the weights from reservoir to output are trained. Since\nthe reservoir is fixed during the training procedure, we may wonder if the\ncomputational power of the recurrent structure is fully harnessed. In this\narticle, we propose a new computational model of the ESN type, that represents\nthe reservoir weights in the Fourier space and performs a fine-tuning of these\nweights applying genetic algorithms in the frequency domain. The main interest\nis that this procedure will work in a much smaller space compared to the\nclassical ESN, thus providing a dimensionality reduction transformation of the\ninitial method. The proposed technique allows us to exploit the benefits of the\nlarge recurrent structure avoiding the training problems of gradient-based\nmethod. We provide a detailed experimental study that demonstrates the good\nperformances of our approach with well-known chaotic systems and real-world\ndata.",
    "descriptor": "\nComments: This manuscript was accepted at the 2022 International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "Sebastian Basterrech",
      "Gerardo Rubino"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04951"
  },
  {
    "id": "arXiv:2206.04954",
    "title": "A priori error analysis of linear and nonlinear periodic Schr{\u00f6}dinger  equations with analytic potentials",
    "abstract": "This paper is concerned with the numerical analysis of linear and nonlinear\nSchr{\\\"o}dinger equations with analytic potentials. While the regularity of the\npotential (and the source term when there is one) automatically conveys to the\nsolution in the linear cases, this is no longer true in general in the\nnonlinear case. We also study the rate of convergence of the planewave\n(Fourier) discretization method for computing numerical approximations of the\nsolution.",
    "descriptor": "",
    "authors": [
      "Eric Canc\u00e8s",
      "Gaspard Kemlin",
      "Antoine Levitt"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.04954"
  },
  {
    "id": "arXiv:2206.04958",
    "title": "Self-Supervised Deep Subspace Clustering with Entropy-norm",
    "abstract": "Auto-Encoder based deep subspace clustering (DSC) is widely used in computer\nvision, motion segmentation and image processing. However, it suffers from the\nfollowing three issues in the self-expressive matrix learning process: the\nfirst one is less useful information for learning self-expressive weights due\nto the simple reconstruction loss; the second one is that the construction of\nthe self-expression layer associated with the sample size requires\nhigh-computational cost; and the last one is the limited connectivity of the\nexisting regularization terms. In order to address these issues, in this paper\nwe propose a novel model named Self-Supervised deep Subspace Clustering with\nEntropy-norm (S$^{3}$CE). Specifically, S$^{3}$CE exploits a self-supervised\ncontrastive network to gain a more effetive feature vector. The local structure\nand dense connectivity of the original data benefit from the self-expressive\nlayer and additional entropy-norm constraint. Moreover, a new module with data\nenhancement is designed to help S$^{3}$CE focus on the key information of data,\nand improve the clustering performance of positive and negative instances\nthrough spectral clustering. Extensive experimental results demonstrate the\nsuperior performance of S$^{3}$CE in comparison to the state-of-the-art\napproaches.",
    "descriptor": "",
    "authors": [
      "Guangyi Zhao",
      "Simin Kou",
      "Xuesong Yin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04958"
  },
  {
    "id": "arXiv:2206.04959",
    "title": "Merak: A Efficient Distributed DNN Training Framework with Automated 3D  Parallelism for Giant Foundation Models",
    "abstract": "Foundation models are becoming the dominant deep learning technologies.\nPretraining a foundation model is always time-consumed due to the large scale\nof both the model parameter and training dataset. Besides being\ncomputing-intensive, the training process is extremely memory-intensive and\ncommunication-intensive. These features make it necessary to apply 3D\nparallelism, which integrates data parallelism, pipeline model parallelism and\ntensor model parallelism, to achieve high training efficiency.\nTo achieve this goal, some custom software frameworks such as Megatron-LM and\nDeepSpeed are developed. However, current 3D parallelism frameworks still meet\ntwo issues: i) they are not transparent to model developers, which need to\nmanually modify the model to parallelize training. ii) their utilization of\ncomputation, GPU memory and network bandwidth are not sufficient. We propose\nMerak, an automated 3D parallelism deep learning training framework with high\nresource utilization. Merak automatically deploys with an automatic model\npartitioner, which uses a graph sharding algorithm on a proxy representation of\nthe model. Merak also presents the non-intrusive API for scaling out foundation\nmodel training with minimal code modification. In addition, we design a\nhigh-performance 3D parallel runtime engine in Merak. It uses several\ntechniques to exploit available training resources, including shifted critical\npath pipeline schedule that brings a higher computation utilization,\nstage-aware recomputation that makes use of idle worker memory, and\nsub-pipelined tensor model parallelism that overlaps communication and\ncomputation. Experiments on 64 GPUs show Merak can speedup the training\nperformance over the state-of-the-art 3D parallelism frameworks of models with\n1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42X, 1.39X, 1.43X, and\n1.61X, respectively.",
    "descriptor": "",
    "authors": [
      "Zhiquan Lai",
      "Shengwei Li",
      "Xudong Tang",
      "Keshi Ge",
      "Weijie Liu",
      "Yabo Duan",
      "Linbo Qiao",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.04959"
  },
  {
    "id": "arXiv:2206.04962",
    "title": "Feature Learning and Ensemble Pre-Tasks Based Self-Supervised Speech  Denoising and Dereverberation",
    "abstract": "Self-supervised learning (SSL) achieves great success in monaural speech\nenhancement, while the accuracy of the target speech estimation, particularly\nfor unseen speakers, remains inadequate with existing pre-tasks. As speech\nsignal contains multi-faceted information including speaker identity,\nparalinguistics, and spoken content, the latent representation for speech\nenhancement becomes a tough task. In this paper, we study the effectiveness of\neach feature which is commonly used in speech enhancement and exploit the\nfeature combination in the SSL case. Besides, we propose an ensemble training\nstrategy. The latent representation of the clean speech signal is learned,\nmeanwhile, the dereverberated mask and the estimated ratio mask are exploited\nto denoise and dereverberate the mixture. The latent representation learning\nand the masks estimation are considered as two pre-tasks in the training stage.\nIn addition, to study the effectiveness between the pre-tasks, we compare\ndifferent training routines to train the model and further refine the\nperformance. The NOISEX and DAPS corpora are used to evaluate the efficacy of\nthe proposed method, which also outperforms the state-of-the-art methods.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.11142\n",
    "authors": [
      "Yi Li",
      "ShuangLin Li",
      "Yang Sun",
      "Syed Mohsen Naqvi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04962"
  },
  {
    "id": "arXiv:2206.04973",
    "title": "Borrowing or Codeswitching? Annotating for Finer-Grained Distinctions in  Language Mixing",
    "abstract": "We present a new corpus of Twitter data annotated for codeswitching and\nborrowing between Spanish and English. The corpus contains 9,500 tweets\nannotated at the token level with codeswitches, borrowings, and named entities.\nThis corpus differs from prior corpora of codeswitching in that we attempt to\nclearly define and annotate the boundary between codeswitching and borrowing\nand do not treat common \"internet-speak\" ('lol', etc.) as codeswitching when\nused in an otherwise monolingual context. The result is a corpus that enables\nthe study and modeling of Spanish-English borrowing and codeswitching on\nTwitter in one dataset. We present baseline scores for modeling the labels of\nthis corpus using Transformer-based language models. The annotation itself is\nreleased with a CC BY 4.0 license, while the text it applies to is distributed\nin compliance with the Twitter terms of service.",
    "descriptor": "\nComments: LREC 2022\n",
    "authors": [
      "Elena Alvarez Mellado",
      "Constantine Lignos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04973"
  },
  {
    "id": "arXiv:2206.04975",
    "title": "NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression  Recognition",
    "abstract": "Dynamic facial expression recognition (DFER) in the wild is an extremely\nchallenging task, due to a large number of noisy frames in the video sequences.\nPrevious works focus on extracting more discriminative features, but ignore\ndistinguishing the key frames from the noisy frames. To tackle this problem, we\npropose a noise-robust dynamic facial expression recognition network\n(NR-DFERNet), which can effectively reduce the interference of noisy frames on\nthe DFER task. Specifically, at the spatial stage, we devise a dynamic-static\nfusion module (DSF) that introduces dynamic features to static features for\nlearning more discriminative spatial features. To suppress the impact of target\nirrelevant frames, we introduce a novel dynamic class token (DCT) for the\ntransformer at the temporal stage. Moreover, we design a snippet-based filter\n(SF) at the decision stage to reduce the effect of too many neutral frames on\nnon-neutral sequence classification. Extensive experimental results demonstrate\nthat our NR-DFERNet outperforms the state-of-the-art methods on both the DFEW\nand AFEW benchmarks.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Hanting Li",
      "Mingzhe Sui",
      "Zhaoqing Zhu",
      "Feng zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04975"
  },
  {
    "id": "arXiv:2206.04976",
    "title": "Refining neural network predictions using background knowledge",
    "abstract": "Recent work has showed we can use logical background knowledge in learning\nsystem to compensate for a lack of labeled training data. Many such methods\nwork by creating a loss function that encodes this knowledge. However, often\nthe logic is discarded after training, even if it is still useful at test-time.\nInstead, we ensure neural network predictions satisfy the knowledge by refining\nthe predictions with an extra computation step. We introduce differentiable\nrefinement functions that find a corrected prediction close to the original\nprediction.\nWe study how to effectively and efficiently compute these refinement\nfunctions. Using a new algorithm, we combine refinement functions to find\nrefined predictions for logical formulas of any complexity. This algorithm\nfinds optimal refinements on complex SAT formulas in significantly fewer\niterations and frequently finds solutions where gradient descent can not.",
    "descriptor": "",
    "authors": [
      "Alessandro Daniele",
      "Emile van Krieken",
      "Luciano Serafini",
      "Frank van Harmelen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04976"
  },
  {
    "id": "arXiv:2206.04979",
    "title": "Convolutional Layers Are Not Translation Equivariant",
    "abstract": "The purpose of this paper is to correct a misconception about convolutional\nneural networks (CNNs). CNNs are made up of convolutional layers which are\nshift equivariant due to weight sharing. However, contrary to popular belief,\nconvolutional layers are not translation equivariant, even when boundary\neffects are ignored and when pooling and subsampling are absent. This is\nbecause shift equivariance is a discrete symmetry while translation\nequivariance is a continuous symmetry. That discrete systems do not in general\ninherit continuous equivariances is a fundamental limitation of equivariant\ndeep learning. We discuss two implications of this fact. First, CNNs have\nachieved success in image processing despite not inheriting the translation\nequivariance of the physical systems they model. Second, using CNNs to solve\npartial differential equations (PDEs) will not result in translation\nequivariant solvers.",
    "descriptor": "",
    "authors": [
      "Nick McGreivy",
      "Ammar Hakim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04979"
  },
  {
    "id": "arXiv:2206.04980",
    "title": "Unsupervised and Few-shot Parsing from Pretrained Language Models",
    "abstract": "Pretrained language models are generally acknowledged to be able to encode\nsyntax [Tenney et al., 2019, Jawahar et al., 2019, Hewitt and Manning, 2019].\nIn this article, we propose UPOA, an Unsupervised constituent Parsing model\nthat calculates an Out Association score solely based on the self-attention\nweight matrix learned in a pretrained language model as the syntactic distance\nfor span segmentation. We further propose an enhanced version, UPIO, which\nexploits both inside association and outside association scores for estimating\nthe likelihood of a span. Experiments with UPOA and UPIO disclose that the\nlinear projection matrices for the query and key in the self-attention\nmechanism play an important role in parsing. We therefore extend the\nunsupervised models to few-shot parsing models (FPOA, FPIO) that use a few\nannotated trees to learn better linear projection matrices for parsing.\nExperiments on the Penn Treebank demonstrate that our unsupervised parsing\nmodel UPIO achieves results comparable to the state of the art on short\nsentences (length <= 10). Our few-shot parsing model FPIO trained with only 20\nannotated trees outperforms a previous few-shot parsing method trained with 50\nannotated trees. Experiments on cross-lingual parsing show that both\nunsupervised and few-shot parsing methods are better than previous methods on\nmost languages of SPMRL [Seddah et al., 2013].",
    "descriptor": "\nComments: Published in Artificial Intelligence\n",
    "authors": [
      "Zhiyuan Zeng",
      "Deyi Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.04980"
  },
  {
    "id": "arXiv:2206.04981",
    "title": "Position Labels for Self-Supervised Vision Transformer",
    "abstract": "Position encoding is important for vision transformer (ViT) to capture the\nspatial structure of the input image. General efficacy has been proven in ViT.\nIn our work we propose to train ViT to recognize the 2D position encoding of\npatches of the input image, this apparently simple task actually yields a\nmeaningful self-supervisory task. Based on previous work on ViT position\nencoding, we propose two position labels dedicated to 2D images including\nabsolute position and relative position. Our position labels can be easily\nplugged into transformer, combined with the various current ViT variants. It\ncan work in two ways: 1.As an auxiliary training target for vanilla ViT (e.g.,\nViT-B and Swin-B) to improve model performance. 2. Combine the self-supervised\nViT (e.g., MAE) to provide a more powerful self-supervised signal for semantic\nfeature learning. Experiments demonstrate that solely due to the proposed\nself-supervised methods, Swin-B and ViT-B obtained improvements of 1.9% (top-1\nAcc) and 5.6% (top-1 Acc) on Mini-ImageNet, respectively.",
    "descriptor": "",
    "authors": [
      "Zhemin Zhang",
      "Xun Gong",
      "Jinyi Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04981"
  },
  {
    "id": "arXiv:2206.04984",
    "title": "Zero-Shot Audio Classification using Image Embeddings",
    "abstract": "Supervised learning methods can solve the given problem in the presence of a\nlarge set of labeled data. However, the acquisition of a dataset covering all\nthe target classes typically requires manual labeling which is expensive and\ntime-consuming. Zero-shot learning models are capable of classifying the unseen\nconcepts by utilizing their semantic information. The present study introduces\nimage embeddings as side information on zero-shot audio classification by using\na nonlinear acoustic-semantic projection. We extract the semantic image\nrepresentations from the Open Images dataset and evaluate the performance of\nthe models on an audio subset of AudioSet using semantic information in\ndifferent domains; image, audio, and textual. We demonstrate that the image\nembeddings can be used as semantic information to perform zero-shot audio\nclassification. The experimental results show that the image and textual\nembeddings display similar performance both individually and together. We\nadditionally calculate the semantic acoustic embeddings from the test samples\nto provide an upper limit to the performance. The results show that the\nclassification performance is highly sensitive to the semantic relation between\ntest and training classes and textual and image embeddings can reach up to the\nsemantic acoustic embeddings when the seen and unseen classes are semantically\nsimilar.",
    "descriptor": "\nComments: Accepted to the European Signal Processing Conference (EUSIPCO) 2022\n",
    "authors": [
      "Duygu Dogan",
      "Huang Xie",
      "Toni Heittola",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.04984"
  },
  {
    "id": "arXiv:2206.04987",
    "title": "Group Threat, Political Extremity, and Collective Dynamics in Online  Discussions",
    "abstract": "Collectives adapt their network structure to the challenges they face. It has\nbeen hypothesized that collectives experiencing a real or imagined threat from\nan outgroup tend to consolidate behind a few influential group members, and\nthat network structures in which a few members have a very strong influence are\nmore likely in politically extreme groups. These hypotheses have not been\ntested in large-scale real-world settings. We reconstruct networks of tens of\nthousands of commenters participating in comment sections of high-profile U.S.\npolitical news websites spanning the political spectrum from left to right,\nincluding Mother Jones, The Atlantic, The Hill, and Breitbart. We investigate\nthe relationship between different indices of inequality of influence in\ncommenters' networks and perceived group threat associated with significant\nsocietal events, from elections and political rallies to mass shootings. Our\nfindings support the hypotheses that groups facing a real or imagined outgroup\nthreat and groups that are more politically extreme are more likely to include\ndisproportionately influential commenters. These results provide an extensive\nreal-world test of theoretical accounts of collective adaptation to outgroup\nthreats.",
    "descriptor": "",
    "authors": [
      "N. Gizem Bacaksizlar Turbic",
      "Mirta Galesic"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.04987"
  },
  {
    "id": "arXiv:2206.04988",
    "title": "Conjunctive queries with self-joins, towards a fine-grained complexity  analysis",
    "abstract": "The complexity of evaluating conjunctive queries without self-joins is well\nunderstood: Acyclicity is the boundary for being able to test whether there is\na solution in linear time, and free-connexity is the boundary for being able to\ncompute the set of solutions in linear input + output time. Moreover,\nfree-connex acyclic queries can be enumerated with constant delay. In the\npresence of self-joins, the situation is not that clear. All the upper-bounds\nmentioned above remain, but not the lower bounds. We provide preliminary\nresults for the fine-grained complexity analysis of the evaluation of\nconjunctive queries with self-joins. We settle the case of queries with arity\ntwo or less and for queries with cyclic cores. We also provide a toolbox that\ncan be used to show hardness or easiness of queries with self-joins. We\nillustrate the toolbox by identifying tractable cases that were not known to be\nefficiently solvable and identifying sufficient conditions for hardness.",
    "descriptor": "",
    "authors": [
      "Nofar Carmeli",
      "Luc Segoufin"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.04988"
  },
  {
    "id": "arXiv:2206.04992",
    "title": "Artificial Intelligence Enabled NOMA Towards Next Generation Multiple  Access",
    "abstract": "This article focuses on the application of artificial intelligence (AI) in\nnon-orthogonal multiple-access (NOMA), which aims to achieve automated,\nadaptive, and high-efficiency multi-user communications towards next generation\nmultiple access (NGMA). First, the limitations of current scenario-specific\nmulti-antenna NOMA schemes are discussed, and the importance of AI for NGMA is\nhighlighted. Then, to achieve the vision of NGMA, a novel cluster-free NOMA\nframework is proposed for providing scenario-adaptive NOMA communications, and\nseveral promising machine learning solutions are identified. To elaborate\nfurther, novel centralized and distributed machine learning paradigms are\nconceived for efficiently employing the proposed cluster-free NOMA framework in\nsingle-cell and multi-cell networks, where numerical results are provided to\ndemonstrate the effectiveness. Furthermore, the interplays between the proposed\ncluster-free NOMA and emerging wireless techniques are presented. Finally,\nseveral open research issues of AI enabled NGMA are discussed.",
    "descriptor": "",
    "authors": [
      "Xiaoxia Xu",
      "Yuanwei Liu",
      "Xidong Mu",
      "Qimei Chen",
      "Hao Jiang",
      "Zhiguo Ding"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.04992"
  },
  {
    "id": "arXiv:2206.04993",
    "title": "The Generalized Eigenvalue Problem as a Nash Equilibrium",
    "abstract": "The generalized eigenvalue problem (GEP) is a fundamental concept in\nnumerical linear algebra. It captures the solution of many classical machine\nlearning problems such as canonical correlation analysis, independent\ncomponents analysis, partial least squares, linear discriminant analysis,\nprincipal components, successor features and others. Despite this, most general\nsolvers are prohibitively expensive when dealing with massive data sets and\nresearch has instead concentrated on finding efficient solutions to specific\nproblem instances. In this work, we develop a game-theoretic formulation of the\ntop-$k$ GEP whose Nash equilibrium is the set of generalized eigenvectors. We\nalso present a parallelizable algorithm with guaranteed asymptotic convergence\nto the Nash. Current state-of-the-art methods require $\\mathcal{O}(d^2k)$\ncomplexity per iteration which is prohibitively expensive when the number of\ndimensions ($d$) is large. We show how to achieve $\\mathcal{O}(dk)$ complexity,\nscaling to datasets $100\\times$ larger than those evaluated by prior methods.\nEmpirically we demonstrate that our algorithm is able to solve a variety of GEP\nproblem instances including a large-scale analysis of neural network\nactivations.",
    "descriptor": "",
    "authors": [
      "Ian Gemp",
      "Charlie Chen",
      "Brian McWilliams"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04993"
  },
  {
    "id": "arXiv:2206.04995",
    "title": "Density-optimized Intersection-free Mapping and Matrix Multiplication  for Join-Project Operations (extended version)",
    "abstract": "A Join-Project operation is a join operation followed by a duplicate\neliminating projection operation. It is used in a large variety of\napplications, including entity matching, set analytics, and graph analytics.\nPrevious work proposes a hybrid design that exploits the classical solution\n(i.e., join and deduplication), and MM (matrix multiplication) to process the\nsparse and the dense portions of the input data, respectively. However, we\nobserve three problems in the state-of-the-art solution: 1) The outputs of the\nsparse and dense portions overlap, requiring an extra deduplication step; 2)\nIts table-to-matrix transformation makes an over-simplified assumption of the\nattribute values; and 3) There is a mismatch between the employed MM in BLAS\npackages and the characteristics of the Join-Project operation. In this paper,\nwe propose DIM3, an optimized algorithm for the Join-Project operation. To\naddress 1), we propose an intersection-free partition method to completely\nremove the final deduplication step. For 2), we develop an optimized design for\nmapping attribute values to natural numbers. For 3), we propose DenseEC and\nSparseBMM algorithms to exploit the structure of Join-Project for better\nefficiency. Moreover, we extend DIM3 to consider partial result caching and\nsupport Join-op queries, including Join-Aggregate and MJP (Multi-way Joins with\nProjection). Experimental results using both real-world and synthetic data sets\nshow that DIM3 outperforms previous Join-Project solutions by a factor of\n2.3x-18x. Compared to RDBMSs, DIM3 achieves orders of magnitude speedups.",
    "descriptor": "",
    "authors": [
      "Zichun Huang",
      "Shimin Chen"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.04995"
  },
  {
    "id": "arXiv:2206.05000",
    "title": "An Open Framework to Model Diffraction by Dynamic Blockers in Millimeter  Wave Simulations",
    "abstract": "The millimeter wave (mmWave) band will be exploited to address the growing\ndemand for high data rates and low latency. The higher frequencies, however,\nare prone to limitations on the propagation of the signal in the environment.\nThus, highly directional beamforming is needed to increase the antenna gain.\nAnother crucial problem of the mmWave frequencies is their vulnerability to\nblockage by physical obstacles. To this aim, we studied the problem of modeling\nthe impact of second-order effects on mmWave channels, specifically the\nsusceptibility of the mmWave signals to physical blockers. With respect to\nexisting works on this topic, our project focuses on scenarios where mmWaves\ninteract with multiple, dynamic blockers. Our open source software includes\ndiffraction-based blockage models and interfaces directly with an open source\nRadio Frequency (RF) ray-tracing software.",
    "descriptor": "\nComments: 9 pages, 7 figures, 1 tables. This paper has been accepted for presentation at the 20th Mediterranean Communication and Computer Networking Conference. Copyright IEEE 2022\n",
    "authors": [
      "Paolo Testolina",
      "Mattia Lecci",
      "Alessandro Traspadini",
      "Michele Zorzi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.05000"
  },
  {
    "id": "arXiv:2206.05008",
    "title": "Subjective Quality Assessment for Images Generated by Computer Graphics",
    "abstract": "With the development of rendering techniques, computer graphics generated\nimages (CGIs) have been widely used in practical application scenarios such as\narchitecture design, video games, simulators, movies, etc. Different from\nnatural scene images (NSIs), the distortions of CGIs are usually caused by poor\nrending settings and limited computation resources. What's more, some CGIs may\nalso suffer from compression distortions in transmission systems like cloud\ngaming and stream media. However, limited work has been put forward to tackle\nthe problem of computer graphics generated images' quality assessment (CG-IQA).\nTherefore, in this paper, we establish a large-scale subjective CG-IQA database\nto deal with the challenge of CG-IQA tasks. We collect 25,454 in-the-wild CGIs\nthrough previous databases and personal collection. After data cleaning, we\ncarefully select 1,200 CGIs to conduct the subjective experiment. Several\npopular no-reference image quality assessment (NR-IQA) methods are tested on\nour database. The experimental results show that the handcrafted-based methods\nachieve low correlation with subjective judgment and deep learning based\nmethods obtain relatively better performance, which demonstrates that the\ncurrent NR-IQA models are not suitable for CG-IQA tasks and more effective\nmodels are urgently needed.",
    "descriptor": "",
    "authors": [
      "Tao Wang",
      "Zicheng Zhang",
      "Wei Sun",
      "Xiongkuo Min",
      "Wei Lu",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05008"
  },
  {
    "id": "arXiv:2206.05009",
    "title": "Weighted Ensembles for Active Learning with Adaptivity",
    "abstract": "Labeled data can be expensive to acquire in several application domains,\nincluding medical imaging, robotics, and computer vision. To efficiently train\nmachine learning models under such high labeling costs, active learning (AL)\njudiciously selects the most informative data instances to label on-the-fly.\nThis active sampling process can benefit from a statistical function model,\nthat is typically captured by a Gaussian process (GP). While most GP-based AL\napproaches rely on a single kernel function, the present contribution advocates\nan ensemble of GP models with weights adapted to the labeled data collected\nincrementally. Building on this novel EGP model, a suite of acquisition\nfunctions emerges based on the uncertainty and disagreement rules. An\nadaptively weighted ensemble of EGP-based acquisition functions is also\nintroduced to further robustify performance. Extensive tests on synthetic and\nreal datasets showcase the merits of the proposed EGP-based approaches with\nrespect to the single GP-based AL alternatives.",
    "descriptor": "",
    "authors": [
      "Konstantinos D. Polyzos",
      "Qin Lu",
      "Georgios B. Giannakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05009"
  },
  {
    "id": "arXiv:2206.05010",
    "title": "Highlights of Semantics in Multi-objective Genetic Programming",
    "abstract": "Semantics is a growing area of research in Genetic programming (GP) and\nrefers to the behavioural output of a Genetic Programming individual when\nexecuted. This research expands upon the current understanding of semantics by\nproposing a new approach: Semantic-based Distance as an additional criteriOn\n(SDO), in the thus far, somewhat limited researched area of semantics in\nMulti-objective GP (MOGP). Our work included an expansive analysis of the GP in\nterms of performance and diversity metrics, using two additional semantic-based\napproaches, namely Semantic Similarity-based Crossover (SCC) and Semantic-based\nCrowding Distance (SCD). Each approach is integrated into two evolutionary\nmulti-objective (EMO) frameworks: Non-dominated Sorting Genetic Algorithm II\n(NSGA-II) and the Strength Pareto Evolutionary Algorithm 2 (SPEA2), and along\nwith the three semantic approaches, the canonical form of NSGA-II and SPEA2 are\nrigorously compared. Using highly-unbalanced binary classification datasets, we\ndemonstrated that the newly proposed approach of SDO consistently generated\nmore non-dominated solutions, with better diversity and improved hypervolume\nresults.",
    "descriptor": "\nComments: Accepted in Genetic and Evolutionary Computation Conference Companion (GECCO '22 Companion), July 9--13, 2022, Boston, MA, USA, 2 pages, 1 figure. This Hot-off-the-Press paper summarises \"Semantics in Multi-objective Genetic Programming\" by Edgar Galv\\'an, Leonardo Trujillo and Fergal Stapleton, published in the journal of Applied Soft Computing 2022, this https URL [arXiv:2105.02944]\n",
    "authors": [
      "Edgar Galv\u00e1n",
      "Leonardo Trujillo",
      "Fergal Stapleton"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.05010"
  },
  {
    "id": "arXiv:2206.05014",
    "title": "Building an Icelandic Entity Linking Corpus",
    "abstract": "In this paper, we present the first Entity Linking corpus for Icelandic. We\ndescribe our approach of using a multilingual entity linking model (mGENRE) in\ncombination with Wikipedia API Search (WAPIS) to label our data and compare it\nto an approach using WAPIS only. We find that our combined method reaches 53.9%\ncoverage on our corpus, compared to 30.9% using only WAPIS. We analyze our\nresults and explain the value of using a multilingual system when working with\nIcelandic. Additionally, we analyze the data that remain unlabeled, identify\npatterns and discuss why they may be more difficult to annotate.",
    "descriptor": "\nComments: 9 pages, 5 figures, submitted to Dataset Creation for Lower-Resourced Languages, an LREC 2022 Workshop, 9am-1pm June 24th, 2022\n",
    "authors": [
      "Steinunn Rut Fri\u00f0riksd\u00f3ttir",
      "Valdimar \u00c1g\u00fast Eggertsson",
      "Benedikt Geir J\u00f3hannesson",
      "Hjalti Dan\u00edelsson",
      "Hrafn Loftsson",
      "Hafsteinn Einarsson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05014"
  },
  {
    "id": "arXiv:2206.05015",
    "title": "A Simple Yet Efficient Method for Adversarial Word-Substitute Attack",
    "abstract": "NLP researchers propose different word-substitute black-box attacks that can\nfool text classification models. In such attack, an adversary keeps sending\ncrafted adversarial queries to the target model until it can successfully\nachieve the intended outcome. State-of-the-art attack methods usually require\nhundreds or thousands of queries to find one adversarial example. In this\npaper, we study whether a sophisticated adversary can attack the system with\nmuch less queries. We propose a simple yet efficient method that can reduce the\naverage number of adversarial queries by 3-30 times and maintain the attack\neffectiveness. This research highlights that an adversary can fool a deep NLP\nmodel with much less cost.",
    "descriptor": "",
    "authors": [
      "Tianle Li",
      "Yi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05015"
  },
  {
    "id": "arXiv:2206.05016",
    "title": "Frictional Authors",
    "abstract": "I present a method for text analysis based on an analogy with the dynamic\nfriction of sliding surfaces. One surface is an array of points with a\n'friction coefficient' derived from the distribution frequency of a text's\nalphabetic characters. The other surface is a test patch having points with\nthis friction coefficient equal to a median value. Examples are presented from\nan analysis of a broad range of public domain texts, and comparison is made to\nthe Flesch Reading Ease. Source code for the analysis program is provided.",
    "descriptor": "\nComments: 16 page PDF file with 6 figures and two tables. Source code for analysis is included\n",
    "authors": [
      "Devlin Gualtieri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05016"
  },
  {
    "id": "arXiv:2206.05017",
    "title": "Empathetic Conversational Systems: A Review of Current Advances, Gaps,  and Opportunities",
    "abstract": "The concept of empathy is vital in human-agent systems as it contributes to\nmutual understanding, problem-solving and sustained relationships. Despite the\nincreasing adoption of conversational systems as one of the most significant\nevents in the recent decade, the emotional aspects require considerable\nimprovements, particularly in effectively displaying empathy. This paper\nprovides a critical review of this rapidly growing field by examining the\ncurrent advances in four dimensions: (i) conceptual empathy models and\nframeworks, (ii) the adopted empathy-related concepts, (iii) the datasets and\nalgorithmic techniques developed, and (iv) the evaluation strategies. The\nreview findings show that the most studies centred on the use of the\nEMPATHETICDIALOGUES dataset, and the text-based modality dominated research in\nthis field. Moreover, studies have focused mainly on extracting features from\nthe messages of both users and the conversational systems, with minimal\nemphasis on user modelling and profiling. For implementation in variegated\nreal-world domain settings, we recommend that future studies address the gaps\nin detecting and authenticating emotions at the entity level, handling\nmultimodal inputs, displaying more nuanced empathetic behaviours, and\nencompassing additional dialogue system features.",
    "descriptor": "\nComments: 13 pages, 2 figures, 2 tables\n",
    "authors": [
      "Aravind Sesagiri Raamkumar",
      "Yinping Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05017"
  },
  {
    "id": "arXiv:2206.05018",
    "title": "Going Beyond the Cookie Theft Picture Test: Detecting Cognitive  Impairments using Acoustic Features",
    "abstract": "Standardized tests play a crucial role in the detection of cognitive\nimpairment. Previous work demonstrated that automatic detection of cognitive\nimpairment is possible using audio data from a standardized picture description\ntask. The presented study goes beyond that, evaluating our methods on data\ntaken from two standardized neuropsychological tests, namely the German SKT and\na German version of the CERAD-NB, and a semi-structured clinical interview\nbetween a patient and a psychologist. For the tests, we focus on speech\nrecordings of three sub-tests: reading numbers (SKT 3), interference (SKT 7),\nand verbal fluency (CERAD-NB 1). We show that acoustic features from\nstandardized tests can be used to reliably discriminate cognitively impaired\nindividuals from non-impaired ones. Furthermore, we provide evidence that even\nfeatures extracted from random speech samples of the interview can be a\ndiscriminator of cognitive impairment. In our baseline experiments, we use\nOpenSMILE features and Support Vector Machine classifiers. In an improved\nsetup, we show that using wav2vec 2.0 features instead, we can achieve an\naccuracy of up to 85%.",
    "descriptor": "\nComments: Accepted at the 25th International Conference on Text, Speech and Dialogue (TSD 2022)\n",
    "authors": [
      "Franziska Braun",
      "Andreas Erzigkeit",
      "Hartmut Lehfeld",
      "Thomas Hillemacher",
      "Korbinian Riedhammer",
      "Sebastian P. Bayerl"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.05018"
  },
  {
    "id": "arXiv:2206.05020",
    "title": "Feature-aware Diversified Re-ranking with Disentangled Representations  for Relevant Recommendation",
    "abstract": "Relevant recommendation is a special recommendation scenario which provides\nrelevant items when users express interests on one target item (e.g., click,\nlike and purchase). Besides considering the relevance between recommendations\nand trigger item, the recommendations should also be diversified to avoid\ninformation cocoons. However, existing diversified recommendation methods\nmainly focus on item-level diversity which is insufficient when the recommended\nitems are all relevant to the target item. Moreover, redundant or noisy item\nfeatures might affect the performance of simple feature-aware recommendation\napproaches. Faced with these issues, we propose a Feature Disentanglement\nSelf-Balancing Re-ranking framework (FDSB) to capture feature-aware diversity.\nThe framework consists of two major modules, namely disentangled attention\nencoder (DAE) and self-balanced multi-aspect ranker. In DAE, we use multi-head\nattention to learn disentangled aspects from rich item features. In the ranker,\nwe develop an aspect-specific ranking mechanism that is able to adaptively\nbalance the relevance and diversity for each aspect. In experiments, we conduct\noffline evaluation on the collected dataset and deploy FDSB on KuaiShou app for\nonline A/B test on the function of relevant recommendation. The significant\nimprovements on both recommendation quality and user experience verify the\neffectiveness of our approach.",
    "descriptor": "\nComments: 10 pages, 5 figures, Accepted by SIGKDD 2022 Applied Data Science Track\n",
    "authors": [
      "Zihan Lin",
      "Hui Wang",
      "Jingshu Mao",
      "Wayne Xin Zhao",
      "Cheng Wang",
      "Peng Jiang",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.05020"
  },
  {
    "id": "arXiv:2206.05025",
    "title": "Comparative study in fair division algorithms",
    "abstract": "A comparison of four fair division algorithms performed on real data from the\nspliddit website. The comparison was made on the sum of agent's utilities, and\nthe minimum utility for an agent in an allocation.",
    "descriptor": "\nComments: 3 pages, 4 figures\n",
    "authors": [
      "Liad Nagi",
      "Moriya Elgrabli"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.05025"
  },
  {
    "id": "arXiv:2206.05028",
    "title": "Spatial Cross-Attention Improves Self-Supervised Visual Representation  Learning",
    "abstract": "Unsupervised representation learning methods like SwAV are proved to be\neffective in learning visual semantics of a target dataset. The main idea\nbehind these methods is that different views of a same image represent the same\nsemantics. In this paper, we further introduce an add-on module to facilitate\nthe injection of the knowledge accounting for spatial cross correlations among\nthe samples. This in turn results in distilling intra-class information\nincluding feature level locations and cross similarities between same-class\ninstances. The proposed add-on can be added to existing methods such as the\nSwAV. We can later remove the add-on module for inference without any\nmodification of the learned weights. Through an extensive set of empirical\nevaluations, we verify that our method yields an improved performance in\ndetecting the class activation maps, top-1 classification accuracy, and\ndown-stream tasks such as object detection, with different configuration\nsettings.",
    "descriptor": "",
    "authors": [
      "Mehdi Seyfi",
      "Amin Banitalebi-Dehkordi",
      "Yong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05028"
  },
  {
    "id": "arXiv:2206.05030",
    "title": "Explanation as Question Answering based on a Task Model of the Agent's  Design",
    "abstract": "We describe a stance towards the generation of explanations in AI agents that\nis both human-centered and design-based. We collect questions about the working\nof an AI agent through participatory design by focus groups. We capture an\nagent's design through a Task-Method-Knowledge model that explicitly specifies\nthe agent's tasks and goals, as well as the mechanisms, knowledge and\nvocabulary it uses for accomplishing the tasks. We illustrate our approach\nthrough the generation of explanations in Skillsync, an AI agent that links\ncompanies and colleges for worker upskilling and reskilling. In particular, we\nembed a question-answering agent called AskJill in Skillsync, where AskJill\ncontains a TMK model of Skillsync's design. AskJill presently answers\nhuman-generated questions about Skillsync's tasks and vocabulary, and thereby\nhelps explain how it produces its recommendations.",
    "descriptor": "\nComments: 7 Pages, 10 Figures, IJCAI Explainable AI Workshop\n",
    "authors": [
      "Ashok Goel",
      "Harshvardhan Sikka",
      "Vrinda Nandan",
      "Jeonghyun Lee",
      "Matt Lisle",
      "Spencer Rugaber"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05030"
  },
  {
    "id": "arXiv:2206.05033",
    "title": "Solution of DeBERTaV3 on CommonsenseQA",
    "abstract": "We report the performance of DeBERTaV3 on CommonsenseQA in this report. We\nsimply formalize the answer selection as a text classification for DeBERTaV3.\nThe strong natural language inference ability of DeBERTaV3 helps its single and\nensemble model set the new state-of-the-art on CommonsenseQA.",
    "descriptor": "",
    "authors": [
      "Letian Peng",
      "Zuchao Li",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05033"
  },
  {
    "id": "arXiv:2206.05034",
    "title": "The Construction and Evaluation of the LEAFTOP Dataset of Automatically  Extracted Nouns in 1480 Languages",
    "abstract": "The LEAFTOP (language extracted automatically from thousands of passages)\ndataset consists of nouns that appear in multiple places in the four gospels of\nthe New Testament. We use a naive approach -- probabilistic inference -- to\nidentify likely translations in 1480 other languages. We evaluate this process\nand find that it provides lexiconaries with accuracy from 42% (Korafe) to 99%\n(Runyankole), averaging 72% correct across evaluated languages. The process\ntranslates up to 161 distinct lemmas from Koine Greek (average 159). We\nidentify nouns which appear to be easy and hard to translate, language families\nwhere this technique works, and future possible improvements and extensions.\nThe claims to novelty are: the use of a Koine Greek New Testament as the source\nlanguage; using a fully-annotated manually-created grammatically parse of the\nsource text; a custom scraper for texts in the target languages; a new metric\nfor language similarity; a novel strategy for evaluation on low-resource\nlanguages.",
    "descriptor": "\nComments: LREC2022\n",
    "authors": [
      "Greg Baker",
      "Diego Molla-Aliod"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05034"
  },
  {
    "id": "arXiv:2206.05035",
    "title": "Divide (CPU Load) and Conquer: Semi-Flexible Cloud Resource Allocation",
    "abstract": "Cloud resource management is often modeled by two-dimensional bin packing\nwith a set of items that correspond to tasks having fixed CPU and memory\nrequirements. However, applications running in clouds are much more flexible:\nmodern frameworks allow to (horizontally) scale a single application to dozens,\neven hundreds of instances; and then the load balancer can precisely divide the\nworkload between them.\nWe analyze a model that captures this (semi)-flexibility of cloud resource\nmanagement. Each cloud application is characterized by its memory footprint and\nits momentary CPU load. Combining the scheduler and the load balancer, the\nresource manager decides how many instances of each application will be created\nand how the CPU load will be balanced between them. In contrast to the\ndivisible load model, each instance of the application requires a certain\namount of memory, independent of the number of instances. Thus, the resource\nmanager effectively trades additional memory for more evenly balanced load.\nWe study two objectives: the bin-packing-like minimization of the number of\nmachines used; and the makespan-like minimization of the maximum load among all\nthe machines. We prove NP-hardness of the general problems, but also propose\npolynomial-time exact algorithms for boundary special cases. Notably, we show\nthat (semi)-flexibility may result in reducing the required number of machines\nby a tight factor of $2-\\varepsilon$. For the general case, we propose\nheuristics that we validate by simulation on instances derived from the Azure\ntrace.",
    "descriptor": "",
    "authors": [
      "Bart\u0142omiej Przybylski",
      "Pawe\u0142 \u017buk",
      "Krzysztof Rzadca"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.05035"
  },
  {
    "id": "arXiv:2206.05039",
    "title": "Image Generation with Multimodal Priors using Denoising Diffusion  Probabilistic Models",
    "abstract": "Image synthesis under multi-modal priors is a useful and challenging task\nthat has received increasing attention in recent years. A major challenge in\nusing generative models to accomplish this task is the lack of paired data\ncontaining all modalities (i.e. priors) and corresponding outputs. In recent\nwork, a variational auto-encoder (VAE) model was trained in a weakly supervised\nmanner to address this challenge. Since the generative power of VAEs is usually\nlimited, it is difficult for this method to synthesize images belonging to\ncomplex distributions. To this end, we propose a solution based on a denoising\ndiffusion probabilistic models to synthesise images under multi-model priors.\nBased on the fact that the distribution over each time step in the diffusion\nmodel is Gaussian, in this work we show that there exists a closed-form\nexpression to the generate the image corresponds to the given modalities. The\nproposed solution does not require explicit retraining for all modalities and\ncan leverage the outputs of individual modalities to generate realistic images\naccording to different constraints. We conduct studies on two real-world\ndatasets to demonstrate the effectiveness of our approach",
    "descriptor": "",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Wele Gedara Chaminda Bandara",
      "Vishal M Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05039"
  },
  {
    "id": "arXiv:2206.05042",
    "title": "Sentiment analysis on electricity twitter posts",
    "abstract": "In today's world, everyone is expressive in some way, and the focus of this\nproject is on people's opinions about rising electricity prices in United\nKingdom and India using data from Twitter, a micro-blogging platform on which\npeople post messages, known as tweets. Because many people's incomes are not\ngood and they have to pay so many taxes and bills, maintaining a home has\nbecome a disputed issue these days. Despite the fact that Government offered\nsubsidy schemes to compensate people electricity bills but it is not welcomed\nby people. In this project, the aim is to perform sentiment analysis on\npeople's expressions and opinions expressed on Twitter. In order to grasp the\nelectricity prices opinion, it is necessary to carry out sentiment analysis for\nthe government and consumers in energy market. Furthermore, text present on\nthese medias are unstructured in nature, so to process them we firstly need to\npre-process the data. There are so many feature extraction techniques such as\nBag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word\nembedding, NLP based features like word count. In this project, we analysed the\nimpact of feature TF-IDF word level on electricity bills dataset of sentiment\nanalysis. We found that by using TF-IDF word level performance of sentiment\nanalysis is 3-4 higher than using N-gram features. Analysis is done using four\nclassification algorithms including Naive Bayes, Decision Tree, Random Forest,\nand Logistic Regression and considering F-Score, Accuracy, Precision, and\nRecall performance parameters.",
    "descriptor": "\nComments: Keywords: Sentiment Analysis, Machine Learning, Electricity, opinion mining, polarity assessment\n",
    "authors": [
      "Pardeep Kaur",
      "Maryam Edalati"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05042"
  },
  {
    "id": "arXiv:2206.05048",
    "title": "Equilibrium and stiffness study of clustered tensegrity structures with  the consideration of pulley sizes",
    "abstract": "This paper presents the equilibrium and stiffness study of clustered\ntensegrity structures (CTS) considering pulley sizes. We first derive the\ngeometric relationship between clustered strings and pulleys, where the nodal\nvector is chosen as the generalized coordinate. Then, the equilibrium equations\nof the clustered tensegrity structure with pulleys based on the Lagrangian\nmethod are given. Since the stiffness of a structure is usually weakened when\nusing clustering strings, we formulate the tangent stiffness matrix equations\nfor analysis. It is also shown that as pulley sizes go to zero, the governing\nequations of the clustered tensegrity system with pulleys yield to the\nclassical clustered tensegrity structure without pulleys, which is consistent\nwith the existing literature. Three examples are demonstrated to validate the\ngiven theory. The proposed method allows one to conduct equilibrium, stiffness,\nand robustness studies of cluster tensegrity structures with pulleys.\nNevertheless, the approach developed in this paper is not limited to the\ntensegrity structures. It can also be applied to a wide range of applications\nwith pulley-rope systems, such as drilling rigs, ocean platform anchors, and\ncargo cranes.",
    "descriptor": "\nComments: 23 pages, 16 figures\n",
    "authors": [
      "Shuo Ma",
      "Yiqian Chen",
      "Muhao Chen",
      "Robert E. Skelton"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.05048"
  },
  {
    "id": "arXiv:2206.05050",
    "title": "Improved Approximation for Fair Correlation Clustering",
    "abstract": "Correlation clustering is a ubiquitous paradigm in unsupervised machine\nlearning where addressing unfairness is a major challenge. Motivated by this,\nwe study Fair Correlation Clustering where the data points may belong to\ndifferent protected groups and the goal is to ensure fair representation of all\ngroups across clusters. Our paper significantly generalizes and improves on the\nquality guarantees of previous work of Ahmadi et al. and Ahmadian et al. as\nfollows.\n- We allow the user to specify an arbitrary upper bound on the representation\nof each group in a cluster.\n- Our algorithm allows individuals to have multiple protected features and\nensure fairness simultaneously across them all.\n- We prove guarantees for clustering quality and fairness in this general\nsetting. Furthermore, this improves on the results for the special cases\nstudied in previous work. Our experiments on real-world data demonstrate that\nour clustering quality compared to the optimal solution is much better than\nwhat our theoretical result suggests.",
    "descriptor": "",
    "authors": [
      "Sara Ahmadian",
      "Maryam Negahbani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.05050"
  },
  {
    "id": "arXiv:2206.05051",
    "title": "Temporal Inductive Logic Reasoning",
    "abstract": "Inductive logic reasoning is one of the fundamental tasks on graphs, which\nseeks to generalize patterns from the data. This task has been studied\nextensively for traditional graph datasets such as knowledge graphs (KGs), with\nrepresentative techniques such as inductive logic programming (ILP). Existing\nILP methods typically assume learning from KGs with static facts and binary\nrelations. Beyond KGs, graph structures are widely present in other\napplications such as video instructions, scene graphs and program executions.\nWhile inductive logic reasoning is also beneficial for these applications,\napplying ILP to the corresponding graphs is nontrivial: they are more complex\nthan KGs, which usually involve timestamps and n-ary relations, effectively a\ntype of hypergraph with temporal events.\nIn this work, we study two of such applications and propose to represent them\nas hypergraphs with time intervals. To reason on this graph, we propose the\nmulti-start random B-walk that traverses this hypergraph. Combining it with a\npath-consistency algorithm, we propose an efficient backward-chaining ILP\nmethod that learns logic rules by generalizing from both the temporal and the\nrelational data.",
    "descriptor": "",
    "authors": [
      "Yuan Yang",
      "Siheng Xiong",
      "James C Kerce",
      "Faramarz Fekri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.05051"
  },
  {
    "id": "arXiv:2206.05052",
    "title": "Meta-data Study in Autism Spectrum Disorder Classification Based on  Structural MRI",
    "abstract": "Accurate diagnosis of autism spectrum disorder (ASD) based on neuroimaging\ndata has significant implications, as extracting useful information from\nneuroimaging data for ASD detection is challenging. Even though machine\nlearning techniques have been leveraged to improve the information extraction\nfrom neuroimaging data, the varying data quality caused by different meta-data\nconditions (i.e., data collection strategies) limits the effective information\nthat can be extracted, thus leading to data-dependent predictive accuracies in\nASD detection, which can be worse than random guess in some cases. In this\nwork, we systematically investigate the impact of three kinds of meta-data on\nthe predictive accuracy of classifying ASD based on structural MRI collected\nfrom 20 different sites, where meta-data conditions vary.",
    "descriptor": "",
    "authors": [
      "Ruimin Ma",
      "Yanlin Wang",
      "Yanjie Wei",
      "Yi Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2206.05052"
  },
  {
    "id": "arXiv:2206.05053",
    "title": "Coswara: A website application enabling COVID-19 screening by analysing  respiratory sound samples and health symptoms",
    "abstract": "The COVID-19 pandemic has accelerated research on design of alternative,\nquick and effective COVID-19 diagnosis approaches. In this paper, we describe\nthe Coswara tool, a website application designed to enable COVID-19 detection\nby analysing respiratory sound samples and health symptoms. A user using this\nservice can log into a website using any device connected to the internet,\nprovide there current health symptom information and record few sound sampled\ncorresponding to breathing, cough, and speech. Within a minute of analysis of\nthis information on a cloud server the website tool will output a COVID-19\nprobability score to the user. As the COVID-19 pandemic continues to demand\nmassive and scalable population level testing, we hypothesize that the proposed\ntool provides a potential solution towards this.",
    "descriptor": "",
    "authors": [
      "Debarpan Bhattacharya",
      "Debottam Dutta",
      "Neeraj Kumar Sharma",
      "Srikanth Raj Chetupalli",
      "Pravin Mote",
      "Sriram Ganapathy",
      "Chandrakiran C",
      "Sahiti Nori",
      "Suhail K K",
      "Sadhana Gonuguntla",
      "Murali Alagesan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.05053"
  },
  {
    "id": "arXiv:2206.05056",
    "title": "On Neural Architecture Inductive Biases for Relational Tasks",
    "abstract": "Current deep learning approaches have shown good in-distribution\ngeneralization performance, but struggle with out-of-distribution\ngeneralization. This is especially true in the case of tasks involving abstract\nrelations like recognizing rules in sequences, as we find in many intelligence\ntests. Recent work has explored how forcing relational representations to\nremain distinct from sensory representations, as it seems to be the case in the\nbrain, can help artificial systems. Building on this work, we further explore\nand formalize the advantages afforded by 'partitioned' representations of\nrelations and sensory details, and how this inductive bias can help recompose\nlearned relational structure in newly encountered settings. We introduce a\nsimple architecture based on similarity scores which we name Compositional\nRelational Network (CoRelNet). Using this model, we investigate a series of\ninductive biases that ensure abstract relations are learned and represented\ndistinctly from sensory data, and explore their effects on out-of-distribution\ngeneralization for a series of relational psychophysics tasks. We find that\nsimple architectural choices can outperform existing models in\nout-of-distribution generalization. Together, these results show that\npartitioning relational representations from other information streams may be a\nsimple way to augment existing network architectures' robustness when\nperforming out-of-distribution relational computations.",
    "descriptor": "",
    "authors": [
      "Giancarlo Kerg",
      "Sarthak Mittal",
      "David Rolnick",
      "Yoshua Bengio",
      "Blake Richards",
      "Guillaume Lajoie"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05056"
  },
  {
    "id": "arXiv:2206.05060",
    "title": "Social Network Structure Shapes Innovation: Experience-sharing in RL  with SAPIENS",
    "abstract": "The human cultural repertoire relies on innovation: our ability to\ncontinuously and hierarchically explore how existing elements can be combined\nto create new ones. Innovation is not solitary, it relies on collective\naccumulation and merging of previous solutions. Machine learning approaches\ncommonly assume that fully connected multi-agent networks are best suited for\ninnovation. However, human laboratory and field studies have shown that\nhierarchical innovation is more robustly achieved by dynamic communication\ntopologies. In dynamic topologies, humans oscillate between innovating\nindividually or in small clusters, and then sharing outcomes with others. To\nour knowledge, the role of multi-agent topology on innovation has not been\nsystematically studied in machine learning. It remains unclear a) which\ncommunication topologies are optimal for which innovation tasks, and b) which\nproperties of experience sharing improve multi-level innovation. Here we use a\nmulti-level hierarchical problem setting (WordCraft), with three different\ninnovation tasks. We systematically design networks of DQNs sharing experiences\nfrom their replay buffers in varying topologies (fully connected, small world,\ndynamic, ring). Comparing the level of innovation achieved by different\nexperience-sharing topologies across different tasks shows that, first,\nconsistent with human findings, experience sharing within a dynamic topology\nachieves the highest level of innovation across tasks. Second, experience\nsharing is not as helpful when there is a single clear path to innovation.\nThird, two metrics we propose, conformity and diversity of shared experience,\ncan explain the success of different topologies on different tasks. These\ncontributions can advance our understanding of optimal AI-AI, human-human, and\nhuman-AI collaborative networks, inspiring future tools for fostering\ncollective innovation in large organizations.",
    "descriptor": "",
    "authors": [
      "Eleni Nisioti",
      "Mateo Mahaut",
      "Pierre-Yves Oudeyer",
      "Ida Momennejad",
      "Cl\u00e9ment Moulin-Frier"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.05060"
  },
  {
    "id": "arXiv:2206.05061",
    "title": "Glyph from Icon -- Automated Generation of Metaphoric Glyphs",
    "abstract": "Metaphoric glyphs enhance the readability and learnability of abstract glyphs\nused for the visualization of quantitative multidimensional data by building\nupon graphical entities that are intuitively related to the underlying problem\ndomain. Their construction is, however, a predominantly manual process. In this\npaper, we introduce the Glyph-from-Icon (GfI) approach that allows the\nautomated generation of metaphoric glyphs from user specified icons. Our\napproach modifies the icon's visual appearance using up to seven quantifiable\nvisual variables, three of which manipulate its geometry while four affect its\ncolor. Depending on the visualization goal, specific combinations of these\nvisual variables define the glyphs's variables used for data encoding.\nTechnically, we propose a diffusion-curve based parametric icon representation,\nwhich comprises the degrees-of-freedom related to the geometric and color-based\nvisual variables. Moreover, we extend our GfI approach to achieve scalability\nof the generated glyphs. Based on a user study we evaluate the perception of\nthe glyph's main variables, i.e., amplitude and frequency of geometric and\ncolor modulation, as function of the stimuli and deduce functional relations as\nwell as quantization levels to achieve perceptual monotonicity and readability.\nFinally, we propose a robustly perceivable combination of visual variables,\nwhich we apply to the visualization of COVID-19 data.",
    "descriptor": "",
    "authors": [
      "Dmitri Presnov",
      "Andreas Kolb"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.05061"
  },
  {
    "id": "arXiv:2206.05066",
    "title": "Experimental Evaluation of Visual-Inertial Odometry Systems for Arable  Farming",
    "abstract": "The farming industry constantly seeks the automation of different processes\ninvolved in agricultural production, such as sowing, harvesting and weed\ncontrol. The use of mobile autonomous robots to perform those tasks is of great\ninterest. Arable lands present hard challenges for Simultaneous Localization\nand Mapping (SLAM) systems, key for mobile robotics, given the visual\ndifficulty due to the highly repetitive scene and the crop leaves movement\ncaused by the wind.\nIn recent years, several Visual-Inertial Odometry (VIO) and SLAM systems have\nbeen developed. They have proved to be robust and capable of achieving high\naccuracy in indoor and outdoor urban environments. However, they were not\nproperly assessed in agricultural fields. In this work we assess the most\nrelevant state-of-the-art VIO systems in terms of accuracy and processing time\non arable lands in order to better understand how they behave on these\nenvironments. In particular, the evaluation is carried out on a collection of\nsensor data recorded by our wheeled robot in a soybean field, which was\npublicly released as the Rosario Dataset. The evaluation shows that the highly\nrepetitive appearance of the environment, the strong vibration produced by the\nrough terrain and the movement of the leaves caused by the wind, expose the\nlimitations of the current state-of-the-art VIO and SLAM systems. We analyze\nthe systems failures and highlight the observed drawbacks, including\ninitialization failures, tracking loss and sensitivity to IMU saturation.\nFinally, we conclude that even though certain systems like ORB-SLAM3 and\nS-MSCKF show good results with respect to others, more improvements should be\ndone to make them reliable in agricultural fields for certain applications such\nas soil tillage of crop rows and pesticide spraying.",
    "descriptor": "\nComments: This paper has been accepted for publication in Journal of Field Robotics\n",
    "authors": [
      "Javier Cremona",
      "Rom\u00e1n Comelli",
      "Taih\u00fa Pire"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05066"
  },
  {
    "id": "arXiv:2206.05070",
    "title": "We Cannot Guarantee Safety: The Undecidability of Graph Neural Network  Verification",
    "abstract": "Graph Neural Networks (GNN) are commonly used for two tasks: (whole) graph\nclassification and node classification. We formally introduce generically\nformulated decision problems for both tasks, corresponding to the following\npattern: given a GNN, some specification of valid inputs, and some\nspecification of valid outputs, decide whether there is a valid input\nsatisfying the output specification. We then prove that graph classifier\nverification is undecidable in general, implying that there cannot be an\nalgorithm surely guaranteeing the absence of misclassification of any kind.\nAdditionally, we show that verification in the node classification case becomes\ndecidable as soon as we restrict the degree of the considered graphs.\nFurthermore, we discuss possible changes to these results depending on the\nconsidered GNN model and specifications.",
    "descriptor": "",
    "authors": [
      "Marco S\u00e4lzer",
      "Martin Lange"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.05070"
  },
  {
    "id": "arXiv:2206.05075",
    "title": "Diffeomorphic Counterfactuals with Generative Models",
    "abstract": "Counterfactuals can explain classification decisions of neural networks in a\nhuman interpretable way. We propose a simple but effective method to generate\nsuch counterfactuals. More specifically, we perform a suitable diffeomorphic\ncoordinate transformation and then perform gradient ascent in these coordinates\nto find counterfactuals which are classified with great confidence as a\nspecified target class. We propose two methods to leverage generative models to\nconstruct such suitable coordinate systems that are either exactly or\napproximately diffeomorphic. We analyze the generation process theoretically\nusing Riemannian differential geometry and validate the quality of the\ngenerated counterfactuals using various qualitative and quantitative measures.",
    "descriptor": "",
    "authors": [
      "Ann-Kathrin Dombrowski",
      "Jan E. Gerken",
      "Klaus-Robert M\u00fcller",
      "Pan Kessel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05075"
  },
  {
    "id": "arXiv:2206.05077",
    "title": "Tensor Train for Global Optimization Problems in Robotics",
    "abstract": "The convergence of many numerical optimization techniques is highly sensitive\nto the initial guess provided to the solver. We propose an approach based on\ntensor methods to initialize the existing optimization solvers close to global\noptima. The approach uses only the definition of the cost function and does not\nneed access to any database of good solutions. We first transform the cost\nfunction, which is a function of task parameters and optimization variables,\ninto a probability density function. Unlike existing approaches that set the\ntask parameters as constant, we consider them as another set of random\nvariables and approximate the joint probability distribution of the task\nparameters and the optimization variables using a surrogate probability model.\nFor a given task, we then generate samples from the conditional distribution\nwith respect to the given task parameter and use them as initialization for the\noptimization solver. As conditioning and sampling from an arbitrary density\nfunction are challenging, we use Tensor Train decomposition to obtain a\nsurrogate probability model from which we can efficiently obtain the\nconditional model and the samples. The method can produce multiple solutions\ncoming from different modes (when they exist) for a given task. We first\nevaluate the approach by applying it to various challenging benchmark functions\nfor numerical optimization that are difficult to solve using gradient-based\noptimization solvers with a naive initialization, showing that the proposed\nmethod can produce samples close to the global optima and coming from multiple\nmodes. We then demonstrate the generality of the framework and its relevance to\nrobotics by applying the proposed method to inverse kinematics and motion\nplanning problems with a 7-DoF manipulator.",
    "descriptor": "",
    "authors": [
      "Suhan Shetty",
      "Teguh Lembono",
      "Tobias Loew",
      "Sylvain Calinon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.05077"
  },
  {
    "id": "arXiv:2206.05080",
    "title": "Extremal Fitting Problems for Conjunctive Queries",
    "abstract": "The fitting problem for conjunctive queries (CQs) is the problem to construct\na CQ that fits a given set of labeled data examples. When a fitting CQ exists,\nit is in general not unique. This leads us to proposing natural refinements of\nthe notion of a fitting CQ, such as most-general fitting CQ, most-specific\nfitting CQ, and unique fitting CQ. We give structural characterizations of\nthese notions in terms of (suitable refinements of) homomorphism dualities,\nfrontiers, and direct products, which enable the construction of the refined\nfitting CQs when they exist. We also pinpoint the complexity of the associated\nexistence and verification problems, and determine the size of fitting CQs. We\nstudy the same problems for UCQs and for the more restricted class of tree CQs.",
    "descriptor": "",
    "authors": [
      "Balder ten Cate",
      "Victor Dalmau",
      "Maurice Funk",
      "Carsten Lutz"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.05080"
  },
  {
    "id": "arXiv:2206.05082",
    "title": "A Fine Line: Total Least-Squares Line Fitting as QCQP Optimization",
    "abstract": "This note uses the Total Least-Squares (TLS) line-fitting problem as a canvas\nto explore some modern optimization tools. The contribution is meant to be\ntutorial in nature. The TLS problem has a lot of mathematical similarities to\nimportant problems in robotics and computer vision but is easier to visualize\nand understand. We demonstrate how to turn this problem into a Quadratically\nConstrained Quadratic Program (QCQP) so that it can be cast either as an\neigenproblem or a Semi-Definite Program (SDP). We then turn to the more\nchallenging situation where a Geman-McClure cost function and M-estimation are\nused to reject outlier datapoints. Using Black-Rangarajan duality, we show this\ncan also be cast as a QCQP and solved as an SDP; however, with a lot of data\nthe SDP can be slow and as such we show how we can construct a certificate of\noptimality for a faster method such as Iteratively Reweighted Least-Squares\n(IRLS).",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Timothy D Barfoot",
      "Connor Holmes",
      "Frederike Dumbgen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.05082"
  },
  {
    "id": "arXiv:2206.05085",
    "title": "Improved Direct Voxel Grid Optimization for Radiance Fields  Reconstruction",
    "abstract": "In this technical report, we improve the DVGO framework (called DVGOv2),\nwhich is based on Pytorch and uses the simplest dense grid representation.\nFirst, we re-implement part of the Pytorch operations with cuda, achieving 2-3x\nspeedup. The cuda extension is automatically compiled just in time. Second, we\nextend DVGO to support Forward-facing and Unbounded Inward-facing capturing.\nThird, we improve the space time complexity of the distortion loss proposed by\nmip-NeRF 360 from O(N^2) to O(N). The distortion loss improves our quality and\ntraining speed. Our efficient implementation could allow more future works to\nbenefit from the loss.",
    "descriptor": "\nComments: Project page this https URL ; Code this https URL\n",
    "authors": [
      "Cheng Sun",
      "Min Sun",
      "Hwann-Tzong Chen"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.05085"
  },
  {
    "id": "arXiv:2206.05086",
    "title": "Finite Model Theory and Proof Complexity revisited: Distinguishing  graphs in Choiceless Polynomial Time and the Extended Polynomial Calculus",
    "abstract": "This paper extends prior work on the connections between logics from finite\nmodel theory and propositional/algebraic proof systems. We show that if all\nnon-isomorphic graphs in a given graph class can be distinguished in the logic\nChoiceless Polynomial Time with counting (CPT), then they can also be\ndistinguished in the bounded-degree extended polynomial calculus (EPC), and the\nrefutations have roughly the same size as the resource consumption of the\nCPT-sentence. This allows to transfer lower bounds for EPC to CPT and thus\nconstitutes a new potential approach towards better understanding the limits of\nCPT. A super-polynomial EPC lower bound for a PTIME-instance of the graph\nisomorphism problem would separate CPT from PTIME and thus solve a major open\nquestion in finite model theory. Further, using our result, we provide a model\ntheoretic proof for the separation of bounded-degree polynomial calculus and\nbounded-degree extended polynomial calculus.",
    "descriptor": "",
    "authors": [
      "Benedikt Pago"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.05086"
  },
  {
    "id": "arXiv:2206.05089",
    "title": "Policy Gradient Reinforcement Learning for Uncertain Polytopic LPV  Systems based on MHE-MPC",
    "abstract": "In this paper, we propose a learning-based Model Predictive Control (MPC)\napproach for the polytopic Linear Parameter-Varying (LPV) systems with inexact\nscheduling parameters (as exogenous signals with inexact bounds), where the\nLinear Time Invariant (LTI) models (vertices) captured by combinations of the\nscheduling parameters becomes wrong. We first propose to adopt a Moving Horizon\nEstimation (MHE) scheme to simultaneously estimate the convex combination\nvector and unmeasured states based on the observations and model matching\nerror. To tackle the wrong LTI models used in both the MPC and MHE schemes, we\nthen adopt a Policy Gradient (PG) Reinforcement Learning (RL) to learn both the\nestimator (MHE) and controller (MPC) so that the best closed-loop performance\nis achieved. The effectiveness of the proposed RL-based MHE/MPC design is\ndemonstrated using an illustrative example.",
    "descriptor": "",
    "authors": [
      "Hossein Nejatbakhsh Esfahani",
      "Sebastien Gros"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.05089"
  },
  {
    "id": "arXiv:2206.05091",
    "title": "Muffliato: Peer-to-Peer Privacy Amplification for Decentralized  Optimization and Averaging",
    "abstract": "Decentralized optimization is increasingly popular in machine learning for\nits scalability and efficiency. Intuitively, it should also provide better\nprivacy guarantees, as nodes only observe the messages sent by their neighbors\nin the network graph. But formalizing and quantifying this gain is challenging:\nexisting results are typically limited to Local Differential Privacy (LDP)\nguarantees that overlook the advantages of decentralization. In this work, we\nintroduce pairwise network differential privacy, a relaxation of LDP that\ncaptures the fact that the privacy leakage from a node $u$ to a node $v$ may\ndepend on their relative position in the graph. We then analyze the combination\nof local noise injection with (simple or randomized) gossip averaging protocols\non fixed and random communication graphs. We also derive a differentially\nprivate decentralized optimization algorithm that alternates between local\ngradient descent steps and gossip averaging. Our results show that our\nalgorithms amplify privacy guarantees as a function of the distance between\nnodes in the graph, matching the privacy-utility trade-off of the trusted\ncurator, up to factors that explicitly depend on the graph topology. Finally,\nwe illustrate our privacy gains with experiments on synthetic and real-world\ndatasets.",
    "descriptor": "",
    "authors": [
      "Edwige Cyffers",
      "Mathieu Even",
      "Aur\u00e9lien Bellet",
      "Laurent Massouli\u00e9"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05091"
  },
  {
    "id": "arXiv:2206.05093",
    "title": "Federated Momentum Contrastive Clustering",
    "abstract": "We present federated momentum contrastive clustering (FedMCC), a learning\nframework that can not only extract discriminative representations over\ndistributed local data but also perform data clustering. In FedMCC, a\ntransformed data pair passes through both the online and target networks,\nresulting in four representations over which the losses are determined. The\nresulting high-quality representations generated by FedMCC can outperform\nseveral existing self-supervised learning methods for linear evaluation and\nsemi-supervised learning tasks. FedMCC can easily be adapted to ordinary\ncentralized clustering through what we call momentum contrastive clustering\n(MCC). We show that MCC achieves state-of-the-art clustering accuracy results\nin certain datasets such as STL-10 and ImageNet-10. We also present a method to\nreduce the memory footprint of our clustering schemes.",
    "descriptor": "\nComments: Originally submitted March 2022\n",
    "authors": [
      "Runxuan Miao",
      "Erdem Koyuncu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05093"
  },
  {
    "id": "arXiv:2206.05096",
    "title": "Skill Transfer for Temporally-Extended Task Specifications",
    "abstract": "Deploying robots in real-world domains, such as households and flexible\nmanufacturing lines, requires the robots to be taskable on demand. Linear\ntemporal logic (LTL) is a widely-used specification language with a\ncompositional grammar that naturally induces commonalities across tasks.\nHowever, the majority of prior research on reinforcement learning with LTL\nspecifications treats every new formula independently. We propose LTL-Transfer,\na novel algorithm that enables subpolicy reuse across tasks by segmenting\npolicies for training tasks into portable transition-centric skills capable of\nsatisfying a wide array of unseen LTL specifications while respecting\nsafety-critical constraints. Our experiments in a Minecraft-inspired domain\ndemonstrate the capability of LTL-Transfer to satisfy over 90% of 500 unseen\ntasks while training on only 50 task specifications and never violating a\nsafety constraint. We also deployed LTL-Transfer on a quadruped mobile\nmanipulator in a household environment to show its ability to transfer to many\nfetch and delivery tasks in a zero-shot fashion.",
    "descriptor": "",
    "authors": [
      "Jason Xinyu Liu",
      "Ankit Shah",
      "Eric Rosen",
      "George Konidaris",
      "Stefanie Tellex"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05096"
  },
  {
    "id": "arXiv:2206.05099",
    "title": "SimVP: Simpler yet Better Video Prediction",
    "abstract": "From CNN, RNN, to ViT, we have witnessed remarkable advancements in video\nprediction, incorporating auxiliary inputs, elaborate neural architectures, and\nsophisticated training strategies. We admire these progresses but are confused\nabout the necessity: is there a simple method that can perform comparably well?\nThis paper proposes SimVP, a simple video prediction model that is completely\nbuilt upon CNN and trained by MSE loss in an end-to-end fashion. Without\nintroducing any additional tricks and complicated strategies, we can achieve\nstate-of-the-art performance on five benchmark datasets. Through extended\nexperiments, we demonstrate that SimVP has strong generalization and\nextensibility on real-world datasets. The significant reduction of training\ncost makes it easier to scale to complex scenarios. We believe SimVP can serve\nas a solid baseline to stimulate the further development of video prediction.\nThe code is available at\n\\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}.",
    "descriptor": "",
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05099"
  },
  {
    "id": "arXiv:2206.05100",
    "title": "State complexity of the star of a Boolean operation",
    "abstract": "Monsters and modifiers are two concepts recently developed in the state\ncomplexity theory.\nA monster is an automaton in which every function from states to states is\nrepresented by at least one letter. A modifier is a set of functions allowing\none to transform a set of automata into one automaton. The paper describes a\ngeneral strategy that can be used to compute the state complexity of many\noperations. We illustrate it on the problem of the star of a Boolean operation.\nAfter applying modifiers on monsters, the states of the resulting automata are\nassimilated to combinatorial objects: the tableaux. We investigate the\ncombinatorics of these tableaux in order to deduce the state complexity.\nSpecifically, we recover the state complexity of star of intersection and star\nof union, and we also give the exact state complexity of star of symmetrical\ndifference.\nWe thus harmonize the search strategy for the state complexity of star of any\nBoolean operations.",
    "descriptor": "\nComments: 48 pages, submitted to JCSS\n",
    "authors": [
      "Pascal Caron",
      "Edwin Hamel-de-le court",
      "Jean-Gabriel Luque"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.05100"
  },
  {
    "id": "arXiv:2206.05102",
    "title": "Saccade Mechanisms for Image Classification, Object Detection and  Tracking",
    "abstract": "We examine how the saccade mechanism from biological vision can be used to\nmake deep neural networks more efficient for classification and object\ndetection problems. Our proposed approach is based on the ideas of\nattention-driven visual processing and saccades, miniature eye movements\ninfluenced by attention. We conduct experiments by analyzing: i) the robustness\nof different deep neural network (DNN) feature extractors to partially-sensed\nimages for image classification and object detection, and ii) the utility of\nsaccades in masking image patches for image classification and object tracking.\nExperiments with convolutional nets (ResNet-18) and transformer-based models\n(ViT, DETR, TransTrack) are conducted on several datasets (CIFAR-10, DAVSOD,\nMSCOCO, and MOT17). Our experiments show intelligent data reduction via\nlearning to mimic human saccades when used in conjunction with state-of-the-art\nDNNs for classification, detection, and tracking tasks. We observed minimal\ndrop in performance for the classification and detection tasks while only using\nabout 30\\% of the original sensor data. We discuss how the saccade mechanism\ncan inform hardware design via ``in-pixel'' processing.",
    "descriptor": "\nComments: 4 Pages, 6 figures, will be presented at CVPR2022-NeuroVision workshop as a Lightning talk\n",
    "authors": [
      "Saurabh Farkya",
      "Zachary Daniels",
      "Aswin Nadamuni Raghavan",
      "David Zhang",
      "Michael Piacentino"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.05102"
  },
  {
    "id": "arXiv:2206.05103",
    "title": "Hankel low-rank approximation and completion in time series analysis and  forecasting: a brief review",
    "abstract": "In this paper we offer a review and bibliography of work on Hankel low-rank\napproximation and completion, with particular emphasis on how this methodology\ncan be used for time series analysis and forecasting. We begin by describing\npossible formulations of the problem and offer commentary on related topics and\nchallenges in obtaining globally optimal solutions. Key theorems are provided,\nand the paper closes with some expository examples.",
    "descriptor": "",
    "authors": [
      "Jonathan Gillard",
      "Konstantin Usevich"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05103"
  },
  {
    "id": "arXiv:2206.05107",
    "title": "A Reddit Dataset for the Russo-Ukrainian Conflict in 2022",
    "abstract": "Reddit consists of sub-communities that cover a focused topic. This paper\nprovides a list of relevant subreddits for the ongoing Russo-Ukrainian crisis.\nWe perform an exhaustive subreddit exploration using keyword search and\nshortlist 12 subreddits as potential candidates that contain nominal discourse\nrelated to the crisis. These subreddits contain over 300,000 posts and 8\nmillion comments collectively. We provide an additional categorization of\ncontent into two categories, \"R-U Conflict\", and \"Military Related\", based on\ntheir primary focus. We further perform content characterization of those\nsubreddits. The results show a surge of posts and comments soon after Russia\nlaunched the invasion. \"Military Related\" posts are more likely to receive more\nreplies than \"R-U Conflict\" posts. Our textual analysis shows an apparent\npreference for the Pro-Ukraine stance in \"R-U Conflict\", while \"Military\nRelated\" retain a neutral stance.",
    "descriptor": "",
    "authors": [
      "Yiming Zhu",
      "Ehsan-ul Haq",
      "Lik-Hang Lee",
      "Gareth Tyson",
      "Pan Hui"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.05107"
  },
  {
    "id": "arXiv:2206.05108",
    "title": "Deep Multi-Agent Reinforcement Learning with Hybrid Action Spaces based  on Maximum Entropy",
    "abstract": "Multi-agent deep reinforcement learning has been applied to address a variety\nof complex problems with either discrete or continuous action spaces and\nachieved great success. However, most real-world environments cannot be\ndescribed by only discrete action spaces or only continuous action spaces. And\nthere are few works having ever utilized deep reinforcement learning (drl) to\nmulti-agent problems with hybrid action spaces. Therefore, we propose a novel\nalgorithm: Deep Multi-Agent Hybrid Soft Actor-Critic (MAHSAC) to fill this gap.\nThis algorithm follows the centralized training but decentralized execution\n(CTDE) paradigm, and extend the Soft Actor-Critic algorithm (SAC) to handle\nhybrid action space problems in Multi-Agent environments based on maximum\nentropy. Our experiences are running on an easy multi-agent particle world with\na continuous observation and discrete action space, along with some basic\nsimulated physics. The experimental results show that MAHSAC has good\nperformance in training speed, stability, and anti-interference ability. At the\nsame time, it outperforms existing independent deep hybrid learning method in\ncooperative scenarios and competitive scenarios.",
    "descriptor": "",
    "authors": [
      "Hongzhi Hua",
      "Kaigui Wu",
      "Guixuan Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05108"
  },
  {
    "id": "arXiv:2206.05109",
    "title": "A Proof of the Tree of Shapes in n-D",
    "abstract": "In this paper, we prove that the self-dual morphological hierarchical\nstructure computed on a n-D gray-level wellcomposed image u by the algorithm of\nG{\\'e}raud et al. [1] is exactly the mathematical structure defined to be the\ntree of shape of u in Najman et al [2]. We recall that this algorithm is in\nquasi-linear time and thus considered to be optimal. The tree of shapes leads\nto many applications in mathematical morphology and in image processing like\ngrain filtering, shapings, image segmentation, and so on.",
    "descriptor": "",
    "authors": [
      "Thierry G\u00c9raud",
      "Nicolas Boutry",
      "S\u00e9bastien Crozet",
      "Edwin Carlinet",
      "Laurent Najman"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Image and Video Processing (eess.IV)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.05109"
  },
  {
    "id": "arXiv:2206.05111",
    "title": "PAVI: Plate-Amortized Variational Inference",
    "abstract": "Given some observed data and a probabilistic generative model, Bayesian\ninference aims at obtaining the distribution of a model's latent parameters\nthat could have yielded the data. This task is challenging for large population\nstudies where thousands of measurements are performed over a cohort of hundreds\nof subjects, resulting in a massive latent parameter space. This large\ncardinality renders off-the-shelf Variational Inference (VI) computationally\nimpractical. In this work, we design structured VI families that can\nefficiently tackle large population studies. To this end, our main idea is to\nshare the parameterization and learning across the different i.i.d. variables\nin a generative model -symbolized by the model's plates. We name this concept\nplate amortization, and illustrate the powerful synergies it entitles,\nresulting in expressive, parsimoniously parameterized and orders of magnitude\nfaster to train large scale hierarchical variational distributions. We\nillustrate the practical utility of PAVI through a challenging Neuroimaging\nexample featuring a million latent parameters, demonstrating a significant step\ntowards scalable and expressive Variational Inference.",
    "descriptor": "",
    "authors": [
      "Louis Rouillard",
      "Thomas Moreau",
      "Demian Wassermann"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05111"
  },
  {
    "id": "arXiv:2206.05113",
    "title": "A Comprehensive Review on Power System Risk-Based Transient Stability",
    "abstract": "Power systems are getting more complex than ever and are consequently\noperating close to their limit of stability. Moreover, with the increasing\ndemand of renewable wind generation, and the requirement to maintain a secure\npower system, the importance of transient stability cannot be overestimated.\nConsidering its significance in power system security, it is important to\nsuggest a different methodology for enhancing the transient stability,\nconsidering uncertainties. Current deterministic industry practices of\ntransient stability assessment ignore the probabilistic nature of variables\n(fault type, fault location, fault clearing time, etc.). These approaches\ntypically provide a cautious principle and can result in high-priced expansion\nprojects or operational limits. With the increasing system uncertainties and\nwidespread electricity market deregulation, there is a strong inevitability to\nincorporate risk in the traditional transient stability analysis. Accurate\nassessment of transient stability in a modern power network is becoming a\nstrict requirement both in planning and in real-time operation, due to the\nincreasingly intricate dynamics of a power system. Further, increasing sources\nof uncertainty in forecast state and in the reaction to faults highly implies\nthe implementation of risk-based approach in assessing transient stability.\nThus, this paper aims to provide a comprehensive review of risk-based transient\nstability in power networks and the accompanying research. It is believed that\nthis review can be an inception for researchers in the domain of power system\nplanning and security.",
    "descriptor": "",
    "authors": [
      "Umair Shahzad"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.05113"
  },
  {
    "id": "arXiv:2206.05123",
    "title": "REKnow: Enhanced Knowledge for Joint Entity and Relation Extraction",
    "abstract": "Relation extraction is an important but challenging task that aims to extract\nall hidden relational facts from the text. With the development of deep\nlanguage models, relation extraction methods have achieved good performance on\nvarious benchmarks. However, we observe two shortcomings of previous methods:\nfirst, there is no unified framework that works well under various relation\nextraction settings; second, effectively utilizing external knowledge as\nbackground information is absent. In this work, we propose a knowledge-enhanced\ngenerative model to mitigate these two issues. Our generative model is a\nunified framework to sequentially generate relational triplets under various\nrelation extraction settings and explicitly utilizes relevant knowledge from\nKnowledge Graph (KG) to resolve ambiguities. Our model achieves superior\nperformance on multiple benchmarks and settings, including WebNLG, NYT10, and\nTACRED.",
    "descriptor": "",
    "authors": [
      "Sheng Zhang",
      "Patrick Ng",
      "Zhiguo Wang",
      "Bing Xiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.05123"
  },
  {
    "id": "arXiv:2206.05127",
    "title": "Globally-Optimal Contrast Maximisation for Event Cameras",
    "abstract": "Event cameras are bio-inspired sensors that perform well in challenging\nillumination conditions and have high temporal resolution. However, their\nconcept is fundamentally different from traditional frame-based cameras. The\npixels of an event camera operate independently and asynchronously. They\nmeasure changes of the logarithmic brightness and return them in the highly\ndiscretised form of time-stamped events indicating a relative change of a\ncertain quantity since the last event. New models and algorithms are needed to\nprocess this kind of measurements. The present work looks at several motion\nestimation problems with event cameras. The flow of the events is modelled by a\ngeneral homographic warping in a space-time volume, and the objective is\nformulated as a maximisation of contrast within the image of warped events. Our\ncore contribution consists of deriving globally optimal solutions to these\ngenerally non-convex problems, which removes the dependency on a good initial\nguess plaguing existing methods. Our methods rely on branch-and-bound\noptimisation and employ novel and efficient, recursive upper and lower bounds\nderived for six different contrast estimation functions. The practical validity\nof our approach is demonstrated by a successful application to three different\nevent camera motion estimation problems.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2203.03914\n",
    "authors": [
      "Xin Peng",
      "Ling Gao",
      "Yifu Wang",
      "Laurent Kneip"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05127"
  },
  {
    "id": "arXiv:2206.05128",
    "title": "Real-time Hyper-Dimensional Reconfiguration at the Edge using Hardware  Accelerators",
    "abstract": "In this paper we present Hyper-Dimensional Reconfigurable Analytics at the\nTactical Edge (HyDRATE) using low-SWaP embedded hardware that can perform\nreal-time reconfiguration at the edge leveraging non-MAC (free of\nfloating-point MultiplyACcumulate operations) deep neural nets (DNN) combined\nwith hyperdimensional (HD) computing accelerators. We describe the algorithm,\ntrained quantized model generation, and simulated performance of a feature\nextractor free of multiply-accumulates feeding a hyperdimensional logic-based\nclassifier. Then we show how performance increases with the number of\nhyperdimensions. We describe the realized low-SWaP FPGA hardware and embedded\nsoftware system compared to traditional DNNs and detail the implemented\nhardware accelerators. We discuss the measured system latency and power, noise\nrobustness due to use of learnable quantization and HD computing, actual versus\nsimulated system performance for a video activity classification task and\ndemonstration of reconfiguration on this same dataset. We show that\nreconfigurability in the field is achieved by retraining only the feed-forward\nHD classifier without gradient descent backpropagation (gradient-free), using\nfew-shot learning of new classes at the edge. Initial work performed used LRCN\nDNN and is currently extended to use Two-stream DNN with improved performance.",
    "descriptor": "\nComments: 9 pages, 15 figures. Will be presented in Embedded Vision Workshop at CVPR2022\n",
    "authors": [
      "Indhumathi Kandaswamy",
      "Saurabh Farkya",
      "Zachary Daniels",
      "Gooitzen van der Wal",
      "Aswin Raghavan",
      "Yuzheng Zhang",
      "Jun Hu",
      "Michael Lomnitz",
      "Michael Isnardi",
      "David Zhang",
      "Michael Piacentino"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.05128"
  },
  {
    "id": "arXiv:2206.05129",
    "title": "Inverting Incomplete Fourier Transforms by a Sparse Regularization Model  and Applications in Seismic Wavefield Modeling",
    "abstract": "We propose a sparse regularization model for inversion of incomplete Fourier\ntransforms and apply it to seismic wavefield modeling. The objective function\nof the proposed model employs the Moreau envelope of the $\\ell_0$ norm under a\ntight framelet system as a regularization to promote sparsity. This model leads\nto a non-smooth, non-convex optimization problem for which traditional\niteration schemes are inefficient or even divergent. By exploiting special\nstructures of the $\\ell_0$ norm, we identify a local minimizer of the proposed\nnon-convex optimization problem with a global minimizer of a convex\noptimization problem, which provides us insights for the development of\nefficient and convergence guaranteed algorithms to solve it. We characterize\nthe solution of the regularization model in terms of a fixed-point of a map\ndefined by the proximity operator of the $\\ell_0$ norm and develop a\nfixed-point iteration algorithm to solve it. By connecting the map with an\n$\\alpha$-averaged nonexpansive operator, we prove that the sequence generated\nby the proposed fixed-point proximity algorithm converges to a local minimizer\nof the proposed model. Our numerical examples confirm that the proposed model\noutperforms significantly the existing model based on the $\\ell_1$-norm. The\nseismic wavefield modeling in the frequency domain requires solving a series of\nthe Helmholtz equation with large wave numbers, which is a computationally\nintensive task. Applying the proposed sparse regularization model to the\nseismic wavefield modeling requires data of only a few low frequencies,\navoiding solving the Helmholtz equation with large wave numbers. Numerical\nresults show that the proposed method performs better than the existing method\nbased on the $\\ell_1$ norm in terms of the SNR values and visual quality of the\nrestored synthetic seismograms.",
    "descriptor": "",
    "authors": [
      "Tingting Wu",
      "Yuesheng Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.05129"
  },
  {
    "id": "arXiv:2206.05133",
    "title": "On the square-root approximation finite volume scheme for nonlinear  drift-diffusion equations",
    "abstract": "We study a finite volume scheme for the approximation of the solution to\nconvection diffusion equations with nonlinear convection and Robin boundary\nconditions. The scheme builds on the interpretation of such a continuous\nequation as the hydrodynamic limit of some simple exclusion jump process. We\nshow that the scheme admits a unique discrete solution, that the natural bounds\non the solution are preserved, and that it encodes the second principle of\nthermodynamics in the sense that some free energy is dissipated along time. The\nconvergence of the scheme is then rigorously established thanks to compactness\narguments. Numerical simulations are finally provided, highlighting the overall\ngood behavior of the scheme.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Canc\u00e8s",
      "Juliette Venel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.05133"
  },
  {
    "id": "arXiv:2206.05135",
    "title": "On some properties of random and pseudorandom codes",
    "abstract": "We describe some pseudorandom properties of binary linear codes achieving\ncapacity on the binary erasure channel under bit-MAP decoding (as shown in\nKudekar et al this includes doubly transitive codes and, in particular,\nReed-Muller codes). We show that for all integer $q \\ge 2$ the $\\ell_q$ norm of\nthe characteristic function of such 'pseudorandom' code decreases as fast as\nthat of any code of the same rate (and equally fast as that of a random code)\nunder the action of the noise operator. In information-theoretic terms this\nmeans that the $q^{th}$ R\\'enyi entropy of this code increases as fast as\npossible over the binary symmetric channel. In particular (taking $q = \\infty$)\nthis shows that such codes have the smallest asymptotic undetected error\nprobability (equal to that of a random code) over the BSC, for a certain range\nof parameters.\nWe also study the number of times a certain local pattern, a 'rhombic'\n$4$-tuple of codewords, appears in a linear code, and show that for a certain\nrange of parameters this number for pseudorandom codes is similar to that for a\nrandom code.",
    "descriptor": "",
    "authors": [
      "Alex Samorodnitsky"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.05135"
  },
  {
    "id": "arXiv:2206.05136",
    "title": "Fast Deep Autoencoder for Federated learning",
    "abstract": "This paper presents a novel, fast and privacy preserving implementation of\ndeep autoencoders. DAEF (Deep Autoencoder for Federated learning), unlike\ntraditional neural networks, trains a deep autoencoder network in a\nnon-iterative way, which drastically reduces its training time. Its training\ncan be carried out in a distributed way (several partitions of the dataset in\nparallel) and incrementally (aggregation of partial models), and due to its\nmathematical formulation, the data that is exchanged does not endanger the\nprivacy of the users. This makes DAEF a valid method for edge computing and\nfederated learning scenarios. The method has been evaluated and compared to\ntraditional (iterative) deep autoencoders using seven real anomaly detection\ndatasets, and their performance have been shown to be similar despite DAEF's\nfaster training.",
    "descriptor": "",
    "authors": [
      "David Novoa-Paradela",
      "Oscar Romero-Fontenla",
      "Bertha Guijarro-Berdi\u00f1as"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05136"
  },
  {
    "id": "arXiv:2206.05139",
    "title": "Finite electro-elasticity with physics-augmented neural networks",
    "abstract": "In the present work, a machine learning based constitutive model for\nelectro-mechanically coupled material behavior at finite deformations is\nproposed. Using different sets of invariants as inputs, an internal energy\ndensity is formulated as a convex neural network. In this way, the model\nfulfills the polyconvexity condition which ensures material stability, as well\nas thermodynamic consistency, objectivity, material symmetry, and growth\nconditions. Depending on the considered invariants, this physics-augmented\nmachine learning model can either be applied for compressible or nearly\nincompressible material behavior, as well as for arbitrary material symmetry\nclasses. The applicability and versatility of the approach is demonstrated by\ncalibrating it on transversely isotropic data generated with an analytical\npotential, as well as for the effective constitutive modeling of an\nanalytically homogenized, transversely isotropic rank-one laminate composite\nand a numerically homogenized cubic metamaterial. These examinations show the\nexcellent generalization properties that physics-augmented neural networks\noffer also for multi-physical material modeling such as nonlinear\nelectro-elasticity.",
    "descriptor": "",
    "authors": [
      "Dominik K. Klein",
      "Rogelio Ortigosa",
      "Jes\u00fas Mart\u00ednez-Frutos",
      "Oliver Weeger"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.05139"
  },
  {
    "id": "arXiv:2206.05146",
    "title": "Global Internet public peering capacity of interconnection: a complex  network analysis",
    "abstract": "A massive and growing part of Autonomous System (AS)-level traffic exchanges\ntakes place at Internet Exchange Points (IXPs). This paper leverages PeeringDB,\na database providing a partial but reasonable view of the global\ninterconnection of ASes at IXPs, to model a complex graph enabling the\ncharacterization of the key Internet peering players and their interactions\nover time. We model a PeeringDB snapshot as a weighted directed bipartite\ngraph, called the pDB c-graph, that captures the port size ASes possess at IXPs\nusing available metadata. This novel model of the Internet is shown to picture\nrelevant features of a complex network that groups ASes and IXPs in\ngeographical areas of influence. From this model, we extract central players of\npublic peering such as hypergiant AS content providers and major regional\ntraffic receivers. Most importantly, this graph model opens the way to apply\nspectral analysis using reduced Google matrix in order to retrieve the\nintensity of possible interactions between ASes on the basis of pure\nconnectivity information. As an illustration, we retrieve the timely evolution\nof the peering network to show how the central content and cloud providers have\nincreased their reach to eyeball networks during Covid-19 pandemic.",
    "descriptor": "",
    "authors": [
      "Justin Loye",
      "Sandrine Mouysset",
      "Marc Bruy\u00e8re",
      "Katia Jaffr\u00e8s-Runser"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.05146"
  },
  {
    "id": "arXiv:2206.05149",
    "title": "Referring Image Matting",
    "abstract": "Image matting refers to extracting the accurate foregrounds in the image.\nCurrent automatic methods tend to extract all the salient objects in the image\nindiscriminately. In this paper, we propose a new task named Referring Image\nMatting (RIM), referring to extracting the meticulous alpha matte of the\nspecific object that can best match the given natural language description.\nHowever, prevalent visual grounding methods are all limited to the segmentation\nlevel, probably due to the lack of high-quality datasets for RIM. To fill the\ngap, we establish the first large-scale challenging dataset RefMatte by\ndesigning a comprehensive image composition and expression generation engine to\nproduce synthetic images on top of current public high-quality matting\nforegrounds with flexible logics and re-labelled diverse attributes. RefMatte\nconsists of 230 object categories, 47,500 images, 118,749 expression-region\nentities, and 474,996 expressions, which can be further extended easily in the\nfuture. Besides this, we also construct a real-world test set with manually\ngenerated phrase annotations consisting of 100 natural images to further\nevaluate the generalization of RIM models. We first define the task of RIM in\ntwo settings, i.e., prompt-based and expression-based, and then benchmark\nseveral representative methods together with specific model designs for image\nmatting. The results provide empirical insights into the limitations of\nexisting methods as well as possible solutions. We believe the new task RIM\nalong with the RefMatte dataset will open new research directions in this area\nand facilitate future studies. The dataset and code will be made publicly\navailable at https://github.com/JizhiziLi/RIM.",
    "descriptor": "\nComments: The dataset and code are available at this https URL\n",
    "authors": [
      "Jizhizi Li",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05149"
  },
  {
    "id": "arXiv:2206.05153",
    "title": "Preconditioned infinite GMRES for parameterized linear systems",
    "abstract": "We are interested in obtaining approximate solutions to parameterized linear\nsystems of the form $A(\\mu) x(\\mu) = b$ for many values of the parameter $\\mu$.\nHere $A(\\mu)$ is large, sparse, and nonsingular, with a nonlinear analytic\ndependence on $\\mu$. Our approach is based on a companion linearization for\nparameterized linear systems. The companion matrix is similar to the operator\nin the infinite Arnoldi method, and we use this to adapt the flexible GMRES\nsetting. In this way, our method returns a function $\\tilde{x}(\\mu)$ which is\ncheap to evaluate for different $\\mu$, and the preconditioner is applied only\napproximately. This novel approach leads to increased freedom to carry out the\naction of the operation inexactly, which provides performance improvement over\nthe method infinite GMRES, without a loss of accuracy in general. We show that\nthe error of our method is estimated based on the magnitude of the parameter\n$\\mu$, the inexactness of the preconditioning, and the spectrum of the linear\ncompanion matrix. Numerical examples from a finite element discretization of a\nHelmholtz equation with a parameterized material coefficient illustrate the\ncompetitiveness of our approach. The simulations are reproducible and publicly\navailable online.",
    "descriptor": "",
    "authors": [
      "Siobh\u00e1n Correnty",
      "Elias Jarlebring",
      "Kirk M. Soodhalter"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.05153"
  },
  {
    "id": "arXiv:2206.05154",
    "title": "Teacher Perception of Automatically Extracted Grammar Concepts for L2  Language Learning",
    "abstract": "One of the challenges of language teaching is how to organize the rules\nregarding syntax, semantics, or phonology of the language in a meaningful\nmanner. This not only requires pedagogical skills, but also requires a deep\nunderstanding of that language. While comprehensive materials to develop such\ncurricula are available in English and some broadly spoken languages, for many\nother languages, teachers need to manually create them in response to their\nstudents' needs. This process is challenging because i) it requires that such\nexperts be accessible and have the necessary resources, and ii) even if there\nare such experts, describing all the intricacies of a language is\ntime-consuming and prone to omission. In this article, we present an automatic\nframework that aims to facilitate this process by automatically discovering and\nvisualizing descriptions of different aspects of grammar. Specifically, we\nextract descriptions from a natural text corpus that answer questions about\nmorphosyntax (learning of word order, agreement, case marking, or word\nformation) and semantics (learning of vocabulary) and show illustrative\nexamples. We apply this method for teaching the Indian languages, Kannada and\nMarathi, which, unlike English, do not have well-developed pedagogical\nresources and, therefore, are likely to benefit from this exercise. To assess\nthe perceived utility of the extracted material, we enlist the help of language\neducators from schools in North America who teach these languages to perform a\nmanual evaluation. Overall, teachers find the materials to be interesting as a\nreference material for their own lesson preparation or even for learner\nevaluation.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Aditi Chaudhary",
      "Arun Sampath",
      "Ashwin Sheshadri",
      "Antonios Anastasopoulos",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05154"
  },
  {
    "id": "arXiv:2206.05158",
    "title": "MEAT: Maneuver Extraction from Agent Trajectories",
    "abstract": "Advances in learning-based trajectory prediction are enabled by large-scale\ndatasets. However, in-depth analysis of such datasets is limited. Moreover, the\nevaluation of prediction models is limited to metrics averaged over all samples\nin the dataset. We propose an automated methodology that allows to extract\nmaneuvers (e.g., left turn, lane change) from agent trajectories in such\ndatasets. The methodology considers information about the agent dynamics and\ninformation about the lane segments the agent traveled along. Although it is\npossible to use the resulting maneuvers for training classification networks,\nwe exemplary use them for extensive trajectory dataset analysis and\nmaneuver-specific evaluation of multiple state-of-the-art trajectory prediction\nmodels. Additionally, an analysis of the datasets and an evaluation of the\nprediction models based on the agent dynamics is provided.",
    "descriptor": "\nComments: Accepted at IEEE Intelligent Vehicles Symposium (IV) 2022 2nd Workshop on Autonomy@Scale\n",
    "authors": [
      "Julian Schmidt",
      "Julian Jordan",
      "David Raba",
      "Tobias Welz",
      "Klaus Dietmayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05158"
  },
  {
    "id": "arXiv:2206.05159",
    "title": "An Image Processing Pipeline for Camera Trap Time-Lapse Recordings",
    "abstract": "A new open-source image processing pipeline for analyzing camera trap\ntime-lapse recordings is described. This pipeline includes machine learning\nmodels to assist human-in-the-loop video segmentation and animal\nre-identification. We present some performance results and observations on the\nutility of this pipeline after using it in a year-long project studying the\nspatial ecology and social behavior of the gopher tortoise.",
    "descriptor": "\nComments: 5 pages, 2 figures, presented at the CV4Animals workshop of CVIP2022\n",
    "authors": [
      "Michael L. Hilton",
      "Mark T. Yamane",
      "Leah M. Knezevich"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05159"
  },
  {
    "id": "arXiv:2206.05165",
    "title": "Multifidelity Reinforcement Learning with Control Variates",
    "abstract": "In many computational science and engineering applications, the output of a\nsystem of interest corresponding to a given input can be queried at different\nlevels of fidelity with different costs. Typically, low-fidelity data is cheap\nand abundant, while high-fidelity data is expensive and scarce. In this work we\nstudy the reinforcement learning (RL) problem in the presence of multiple\nenvironments with different levels of fidelity for a given control task. We\nfocus on improving the RL agent's performance with multifidelity data.\nSpecifically, a multifidelity estimator that exploits the cross-correlations\nbetween the low- and high-fidelity returns is proposed to reduce the variance\nin the estimation of the state-action value function. The proposed estimator,\nwhich is based on the method of control variates, is used to design a\nmultifidelity Monte Carlo RL (MFMCRL) algorithm that improves the learning of\nthe agent in the high-fidelity environment. The impacts of variance reduction\non policy evaluation and policy improvement are theoretically analyzed by using\nprobability bounds. Our theoretical analysis and numerical experiments\ndemonstrate that for a finite budget of high-fidelity data samples, our\nproposed MFMCRL agent attains superior performance compared with that of a\nstandard RL agent that uses only the high-fidelity environment data for\nlearning the optimal policy.",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Sami Khairy",
      "Prasanna Balaprakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05165"
  },
  {
    "id": "arXiv:2206.05166",
    "title": "Multi-dimensional dual-blind deconvolution approach toward joint  radar-communications",
    "abstract": "We consider a joint multiple-antenna radar-communications system in a\nco-existence scenario. Contrary to conventional applications, wherein at least\nthe radar waveform and communications channel are known or estimated \\textit{a\npriori}, we investigate the case when the channels and transmit signals of both\nsystems are unknown. In radar applications, this problem arises in multistatic\nor passive systems, where transmit signal is not known. Similarly, highly\ndynamic vehicular or mobile communications may render prior estimates of\nwireless channel unhelpful. In particular, the radar signal reflected-off\nmultiple targets is overlaid with the multi-carrier communications signal. In\norder to extract the unknown continuous-valued target parameters (range,\nDoppler velocity, and direction-of-arrival) and communications messages, we\nformulate the problem as a sparse dual-blind deconvolution and solve it using\natomic norm minimization. Numerical experiments validate our proposed approach\nand show that precise estimation of continuous-valued channel parameters, radar\nwaveform, and communications messages is possible up to scaling ambiguities.",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Roman Jacome",
      "Kumar Vijay Mishra",
      "Edwin Vargas",
      "Brian M. Sadler",
      "Henry Arguello"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.05166"
  },
  {
    "id": "arXiv:2206.05169",
    "title": "Bayesian calibration of coupled computational mechanics models under  uncertainty based on interface deformation",
    "abstract": "Calibration or parameter identification is used with computational mechanics\nmodels related to observed data of the modeled process to find model parameters\nsuch that good similarity between model prediction and observation is achieved.\nWe present a Bayesian calibration approach for surface coupled problems in\ncomputational mechanics based on measured deformation of an interface when no\ndisplacement data of material points is available. The interpretation of such a\ncalibration problem as a statistical inference problem, in contrast to\ndeterministic model calibration, is computationally more robust and allows the\nanalyst to find a posterior distribution over possible solutions rather than a\nsingle point estimate. The proposed framework also enables the consideration of\nunavoidable uncertainties that are present in every experiment and are expected\nto play an important role in the model calibration process. To mitigate the\ncomputational costs of expensive forward model evaluations, we propose to learn\nthe log-likelihood function from a controllable amount of parallel simulation\nruns using Gaussian process regression. We introduce and specifically study the\neffect of three different discrepancy measures for deformed interfaces between\nreference data and simulation. We show that a statistically based discrepancy\nmeasure results in the most expressive posterior distribution. We further apply\nthe approach to numerical examples in higher model parameter dimensions and\ninterpret the resulting posterior under uncertainty. In the examples, we\ninvestigate coupled multi-physics models of fluid-structure interaction effects\nin biofilms and find that the model parameters affect the results in a coupled\nmanner.",
    "descriptor": "",
    "authors": [
      "Harald Willmann",
      "Jonas Nitzler",
      "Sebastian Brandstaeter",
      "Wolfgang A. Wall"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.05169"
  },
  {
    "id": "arXiv:2206.05171",
    "title": "Spectral analysis and fast methods for large matrices arising from PDE  approximation",
    "abstract": "The main goal of this thesis is to show the crucial role that plays the\nsymbol in analysing the spectrum the sequence of matrices resulting from PDE\napproximation and in designing a fast method to solve the associated linear\nproblem. In the first part, we study the spectral properties of the matrices\narising from $\\mathbb{P}_k$ Lagrangian Finite Elements approximation of second\norder elliptic differential problem with Dirichlet boundary conditions and\nwhere the operator is $\\mathrm{div} \\left(-a(\\mathbf{x}) \\nabla\\cdot\\right)$,\nwith $a$ continuous and positive over $\\overline \\Omega$, $\\Omega$ being an\nopen and bounded subset of $\\mathbb{R}^d$, $d\\ge 1$. We investigate the\nspectral distribution in the Weyl sense, with a concise overview on\nlocalization, clustering, extremal eigenvalues, and asymptotic conditioning. We\nstudy in detail the case of constant coefficients on $\\Omega=(0,1)^2$ and we\ngive a brief account in the case of variable coefficients and more general\ndomains. While in the second part, we design a fast method of multigrid type\nfor the resolution of linear systems arising from the $\\mathbb{Q}_k$ Finite\nElements approximation of the same considered problem in one and higher\ndimensional. The analysis is performed in one dimension, while the numerics are\ncarried out also in higher dimension $d\\ge 2$, demonstrating an optimal\nbehavior in terms of the dependency on the matrix size and a robustness with\nrespect to the dimensionality $d$ and to the polynomial degree $k$.",
    "descriptor": "",
    "authors": [
      "Ryma Imene Rahla"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.05171"
  },
  {
    "id": "arXiv:2206.05174",
    "title": "Near-Optimal Distributed Dominating Set in Bounded Arboricity Graphs",
    "abstract": "We describe a simple deterministic $O( \\varepsilon^{-1} \\log \\Delta)$ round\ndistributed algorithm for $(2\\alpha+1)(1 + \\varepsilon)$ approximation of\nminimum weighted dominating set on graphs with arboricity at most $\\alpha$.\nHere $\\Delta$ denotes the maximum degree. We also show a lower bound proving\nthat this round complexity is nearly optimal even for the unweighted case, via\na reduction from the celebrated KMW lower bound on distributed vertex cover\napproximation [Kuhn, Moscibroda, and Wattenhofer JACM'16].\nOur algorithm improves on all the previous results (that work only for\nunweighted graphs) including a randomized $O(\\alpha^2)$ approximation in\n$O(\\log n)$ rounds [Lenzen and Wattenhofer DISC'10], a deterministic $O(\\alpha\n\\log \\Delta)$ approximation in $O(\\log \\Delta)$ rounds [Lenzen and Wattenhofer\nDISC'10], a deterministic $O(\\alpha)$ approximation in $O(\\log^2 \\Delta)$\nrounds [implicit in Bansal and Umboh IPL'17 and Kuhn, Moscibroda, and\nWattenhofer SODA'06], and a randomized $O(\\alpha)$ approximation in\n$O(\\alpha\\log n)$ rounds [Morgan, Solomon and Wein DISC'21].\nWe also provide a randomized $O(\\alpha \\log\\Delta)$ round distributed\nalgorithm that sharpens the approximation factor to $\\alpha(1+o(1))$. If each\nnode is restricted to do polynomial-time computations, our approximation factor\nis tight in the first order as it is NP-hard to achieve $\\alpha - 1 -\n\\varepsilon$ approximation [Bansal and Umboh IPL'17].",
    "descriptor": "\nComments: PODC 2022\n",
    "authors": [
      "Michal Dory",
      "Mohsen Ghaffari",
      "Saeed Ilchi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.05174"
  },
  {
    "id": "arXiv:2206.05181",
    "title": "Lightweight Conditional Model Extrapolation for Streaming Data under  Class-Prior Shift",
    "abstract": "We introduce LIMES, a new method for learning with non-stationary streaming\ndata, inspired by the recent success of meta-learning. The main idea is not to\nattempt to learn a single classifier that would have to work well across all\noccurring data distributions, nor many separate classifiers, but to exploit a\nhybrid strategy: we learn a single set of model parameters from which a\nspecific classifier for any specific data distribution is derived via\nclassifier adaptation. Assuming a multi-class classification setting with\nclass-prior shift, the adaptation step can be performed analytically with only\nthe classifier's bias terms being affected. Another contribution of our work is\nan extrapolation step that predicts suitable adaptation parameters for future\ntime steps based on the previous data. In combination, we obtain a lightweight\nprocedure for learning from streaming data with varying class distribution that\nadds no trainable parameters and almost no memory or computational overhead\ncompared to training a single model. Experiments on a set of exemplary tasks\nusing Twitter data show that LIMES achieves higher accuracy than alternative\napproaches, especially with respect to the relevant real-world metric of lowest\nwithin-day accuracy.",
    "descriptor": "",
    "authors": [
      "Paulina Tomaszewska",
      "Christoph H. Lampert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05181"
  },
  {
    "id": "arXiv:2206.05182",
    "title": "Human-AI Interaction Design in Machine Teaching",
    "abstract": "Machine Teaching (MT) is an interactive process where a human and a machine\ninteract with the goal of training a machine learning model (ML) for a\nspecified task. The human teacher communicates their task expertise and the\nmachine student gathers the required data and knowledge to produce an ML model.\nMT systems are developed to jointly minimize the time spent on teaching and the\nlearner's error rate. The design of human-AI interaction in an MT system not\nonly impacts the teaching efficiency, but also indirectly influences the ML\nperformance by affecting the teaching quality. In this paper, we build upon our\nprevious work where we proposed an MT framework with three components, viz.,\nthe teaching interface, the machine learner, and the knowledge base, and focus\non the human-AI interaction design involved in realizing the teaching\ninterface. We outline design decisions that need to be addressed in developing\nan MT system beginning from an ML task. The paper follows the Socratic method\nentailing a dialogue between a curious student and a wise teacher.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Karan Taneja",
      "Harshvardhan Sikka",
      "Ashok Goel"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05182"
  },
  {
    "id": "arXiv:2206.05183",
    "title": "GD-VAEs: Geometric Dynamic Variational Autoencoders for Learning  Nonlinear Dynamics and Dimension Reductions",
    "abstract": "We develop data-driven methods incorporating geometric and topological\ninformation to learn parsimonious representations of nonlinear dynamics from\nobservations. We develop approaches for learning nonlinear state space models\nof the dynamics for general manifold latent spaces using training strategies\nrelated to Variational Autoencoders (VAEs). Our methods are referred to as\nGeometric Dynamic (GD) Variational Autoencoders (GD-VAEs). We learn encoders\nand decoders for the system states and evolution based on deep neural network\narchitectures that include general Multilayer Perceptrons (MLPs), Convolutional\nNeural Networks (CNNs), and Transpose CNNs (T-CNNs). Motivated by problems\narising in parameterized PDEs and physics, we investigate the performance of\nour methods on tasks for learning low dimensional representations of the\nnonlinear Burgers equations, constrained mechanical systems, and spatial fields\nof reaction-diffusion systems. GD-VAEs provide methods for obtaining\nrepresentations for use in learning tasks involving dynamics.",
    "descriptor": "\nComments: 15 figures. arXiv admin note: substantial text overlap with arXiv:2012.03448\n",
    "authors": [
      "Ryan Lopez",
      "Paul J. Atzberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05183"
  },
  {
    "id": "arXiv:2206.05184",
    "title": "Exploring Feature Self-relation for Self-supervised Transformer",
    "abstract": "Learning representations with self-supervision for convolutional networks\n(CNN) has proven effective for vision tasks. As an alternative for CNN, vision\ntransformers (ViTs) emerge strong representation ability with the pixel-level\nself-attention and channel-level feed-forward networks. Recent works reveal\nthat self-supervised learning helps unleash the great potential of ViTs. Still,\nmost works follow self-supervised strategy designed for CNNs, e.g.,\ninstance-level discrimination of samples, but they ignore the unique properties\nof ViTs. We observe that modeling relations among pixels and channels\ndistinguishes ViTs from other networks. To enforce this property, we explore\nthe feature self-relations for training self-supervised ViTs. Specifically,\ninstead of conducting self-supervised learning solely on feature embeddings\nfrom multiple views, we utilize the feature self-relations, i.e.,\npixel/channel-level self-relations, for self-supervised learning. Self-relation\nbased learning further enhance the relation modeling ability of ViTs, resulting\nin strong representations that stably improve performance on multiple\ndownstream tasks. Our source code will be made publicly available.",
    "descriptor": "",
    "authors": [
      "Zhong-Yu Li",
      "Shanghua Gao",
      "Ming-Ming Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05184"
  },
  {
    "id": "arXiv:2206.05194",
    "title": "Learning the Space of Deep Models",
    "abstract": "Embedding of large but redundant data, such as images or text, in a hierarchy\nof lower-dimensional spaces is one of the key features of representation\nlearning approaches, which nowadays provide state-of-the-art solutions to\nproblems once believed hard or impossible to solve. In this work, in a plot\ntwist with a strong meta aftertaste, we show how trained deep models are as\nredundant as the data they are optimized to process, and how it is therefore\npossible to use deep learning models to embed deep learning models. In\nparticular, we show that it is possible to use representation learning to learn\na fixed-size, low-dimensional embedding space of trained deep models and that\nsuch space can be explored by interpolation or optimization to attain\nready-to-use models. We find that it is possible to learn an embedding space of\nmultiple instances of the same architecture and of multiple architectures. We\naddress image classification and neural representation of signals, showing how\nour embedding space can be learnt so as to capture the notions of performance\nand 3D shape, respectively. In the Multi-Architecture setting we also show how\nan embedding trained only on a subset of architectures can learn to generate\nalready-trained instances of architectures it never sees instantiated at\ntraining time.",
    "descriptor": "\nComments: Accepted at ICPR2022\n",
    "authors": [
      "Gianluca Berardi",
      "Luca De Luigi",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05194"
  },
  {
    "id": "arXiv:2206.05195",
    "title": "Nominal Metaphor Generation with Multitask Learning",
    "abstract": "Nominal metaphors are frequently used in human language and have been shown\nto be effective in persuading, expressing emotion, and stimulating interest.\nThis paper tackles the problem of Chinese Nominal Metaphor (NM) generation. We\nintroduce a novel multitask framework, which jointly optimizes three tasks: NM\nidentification, NM component identification, and NM generation. The metaphor\nidentification module is able to perform a self-training procedure, which\ndiscovers novel metaphors from a large-scale unlabeled corpus for NM\ngeneration. The NM component identification module emphasizes components during\ntraining and conditions the generation on these NM components for more coherent\nresults. To train the NM identification and component identification modules,\nwe construct an annotated corpus consisting of 6.3k sentences that contain\ndiverse metaphorical patterns. Automatic metrics show that our method can\nproduce diverse metaphors with good readability, where 92\\% of them are novel\nmetaphorical comparisons. Human evaluation shows our model significantly\noutperforms baselines on consistency and creativity.",
    "descriptor": "\nComments: INLG 2022\n",
    "authors": [
      "Yucheng Li",
      "Chenghua Lin",
      "Frank Geurin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05195"
  },
  {
    "id": "arXiv:2206.05199",
    "title": "Bayesian Estimation of Differential Privacy",
    "abstract": "Algorithms such as Differentially Private SGD enable training machine\nlearning models with formal privacy guarantees. However, there is a discrepancy\nbetween the protection that such algorithms guarantee in theory and the\nprotection they afford in practice. An emerging strand of work empirically\nestimates the protection afforded by differentially private training as a\nconfidence interval for the privacy budget $\\varepsilon$ spent on training a\nmodel. Existing approaches derive confidence intervals for $\\varepsilon$ from\nconfidence intervals for the false positive and false negative rates of\nmembership inference attacks. Unfortunately, obtaining narrow high-confidence\nintervals for $\\epsilon$ using this method requires an impractically large\nsample size and training as many models as samples. We propose a novel Bayesian\nmethod that greatly reduces sample size, and adapt and validate a heuristic to\ndraw more than one sample per trained model. Our Bayesian method exploits the\nhypothesis testing interpretation of differential privacy to obtain a posterior\nfor $\\varepsilon$ (not just a confidence interval) from the joint posterior of\nthe false positive and false negative rates of membership inference attacks.\nFor the same sample size and confidence, we derive confidence intervals for\n$\\varepsilon$ around 40% narrower than prior work. The heuristic, which we\nadapt from label-only DP, can be used to further reduce the number of trained\nmodels needed to get enough samples by up to 2 orders of magnitude.",
    "descriptor": "\nComments: 17 pages, 8 figures. Joint main authors: Santiago Zanella-B\\'eguelin, Lukas Wutschitz, and Shruti Tople\n",
    "authors": [
      "Santiago Zanella-B\u00e9guelin",
      "Lukas Wutschitz",
      "Shruti Tople",
      "Ahmed Salem",
      "Victor R\u00fchle",
      "Andrew Paverd",
      "Mohammad Naseri",
      "Boris K\u00f6pf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.05199"
  },
  {
    "id": "arXiv:2206.05202",
    "title": "Bidirectional Communication Control for Human-Robot Collaboration",
    "abstract": "A fruitful collaboration is based on the mutual knowledge of each other\nskills and on the possibility of communicating their own limits and proposing\nalternatives to adapt the execution of a task to the capabilities of the\ncollaborators. This paper aims at reproducing such a scenario in a human-robot\ncollaboration setting by proposing a novel communication control architecture.\nExploiting control barrier functions, the robot is made aware of its (dynamic)\nskills and limits and, thanks to a local predictor, it is able to assess if it\nis possible to execute a requested task and, if not, to propose alternative by\nrelaxing some constraints. The controller is interfaced with a communication\ninfrastructure that enables human and robot to set up a bidirectional\ncommunication about the task to execute and the human to take an informed\ndecision on the behavior of the robot. A comparative experimental validation is\nproposed.",
    "descriptor": "\nComments: 7 pages, 4 figures, 1 table\n",
    "authors": [
      "Davide Ferrari",
      "Federico Benzi",
      "Cristian Secchi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05202"
  },
  {
    "id": "arXiv:2206.05208",
    "title": "Reducing the local alphabet size in tiling systems by means of 2D  comma-free codes",
    "abstract": "A recognizable picture language is defined as the projection of a local\npicture language defined by a set of two-by-two tiles, i.e. by a\nstrictly-locally-testable (SLT) language of order 2. The family of recognizable\npicture languages is also defined, using larger $k$ by $k$ tiles, $k>2$, by the\nprojection of the corresponding SLT language. A basic measure of the\ndescriptive complexity of a picture language is given by the size of the SLT\nalphabet using two-by-two tiles, more precisely by the so-called alphabetic\nratio of sizes: SLT-alphabet / picture-alphabet. We study how the alphabetic\nratio changes moving from tiles of size two to tiles of larger size, and we\nobtain the following result: any recognizable picture language over an alphabet\nof size $n$ is the projection of an SLT language over an alphabet of size $2n$.\nMoreover, two is the minimal alphabetic ratio possible in general. The proof\nrelies on a new family of comma-free picture codes, for which a lower bound on\nnumerosity is established; and on the relation of languages of encoded pictures\nwith SLT languages. Our result reproduces in two dimensions a similar property\n(known as Extended Medvedev's theorem) of the regular word languages,\nconcerning the minimal alphabetic ratio needed to define a language by means of\na projection of an SLT word language.",
    "descriptor": "",
    "authors": [
      "Stefano {Crespi Reghizzi}",
      "Antonio Restivo",
      "Pierluigi {San Pietro}"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.05208"
  },
  {
    "id": "arXiv:2206.05209",
    "title": "Hierarchical Federated Learning with Privacy",
    "abstract": "Federated learning (FL), where data remains at the federated clients, and\nwhere only gradient updates are shared with a central aggregator, was assumed\nto be private. Recent work demonstrates that adversaries with gradient-level\naccess can mount successful inference and reconstruction attacks. In such\nsettings, differentially private (DP) learning is known to provide resilience.\nHowever, approaches used in the status quo (\\ie central and local DP) introduce\ndisparate utility vs. privacy trade-offs. In this work, we take the first step\ntowards mitigating such trade-offs through {\\em hierarchical FL (HFL)}. We\ndemonstrate that by the introduction of a new intermediary level where\ncalibrated DP noise can be added, better privacy vs. utility trade-offs can be\nobtained; we term this {\\em hierarchical DP (HDP)}. Our experiments with 3\ndifferent datasets (commonly used as benchmarks for FL) suggest that HDP\nproduces models as accurate as those obtained using central DP, where noise is\nadded at a central aggregator. Such an approach also provides comparable\nbenefit against inference adversaries as in the local DP case, where noise is\nadded at the federated clients.",
    "descriptor": "",
    "authors": [
      "Varun Chandrasekaran",
      "Suman Banerjee",
      "Diego Perino",
      "Nicolas Kourtellis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.05209"
  },
  {
    "id": "arXiv:2206.05215",
    "title": "A new distance measurement and its application in K-Means Algorithm",
    "abstract": "K-Means clustering algorithm is one of the most commonly used clustering\nalgorithms because of its simplicity and efficiency. K-Means clustering\nalgorithm based on Euclidean distance only pays attention to the linear\ndistance between samples, but ignores the overall distribution structure of the\ndataset (i.e. the fluid structure of dataset). Since it is difficult to\ndescribe the internal structure of two data points by Euclidean distance in\nhigh-dimensional data space, we propose a new distance measurement, namely,\nview-distance, and apply it to the K-Means algorithm. On the classical manifold\nlearning datasets, S-curve and Swiss roll datasets, not only this new distance\ncan cluster the data according to the structure of the data itself, but also\nthe boundaries between categories are neat dividing lines. Moreover, we also\ntested the classification accuracy and clustering effect of the K-Means\nalgorithm based on view-distance on some real-world datasets. The experimental\nresults show that, on most datasets, the K-Means algorithm based on\nview-distance has a certain degree of improvement in classification accuracy\nand clustering effect.",
    "descriptor": "\nComments: 12 pages, 6 figures, 3 Tables\n",
    "authors": [
      "Yiqun Zhang",
      "Houbiao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.05215"
  },
  {
    "id": "arXiv:2206.05224",
    "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and  Judgement Prediction",
    "abstract": "The recent advances of deep learning have dramatically changed how machine\nlearning, especially in the domain of natural language processing, can be\napplied to legal domain. However, this shift to the data-driven approaches\ncalls for larger and more diverse datasets, which are nevertheless still small\nin number, especially in non-English languages. Here we present the first\nlarge-scale benchmark of Korean legal AI datasets, LBox Open, that consists of\none legal corpus, two classification tasks, two legal judgement prediction\n(LJP) tasks, and one summarization task. The legal corpus consists of 150k\nKorean precedents (264M tokens), of which 63k are sentenced in last 4 years and\n96k are from the first and the second level courts in which factual issues are\nreviewed. The two classification tasks are case names (10k) and statutes (3k)\nprediction from the factual description of individual cases. The LJP tasks\nconsist of (1) 11k criminal examples where the model is asked to predict fine\namount, imprisonment with labor, and imprisonment without labor ranges for the\ngiven facts, and (2) 5k civil examples where the inputs are facts and claim for\nrelief and outputs are the degrees of claim acceptance. The summarization task\nconsists of the Supreme Court precedents and the corresponding summaries. We\nalso release LCube, the first Korean legal language model trained on the legal\ncorpus from this study. Given the uniqueness of the Law of South Korea and the\ndiversity of the legal tasks covered in this work, we believe that LBox Open\ncontributes to the multilinguality of global legal research. LBox Open and\nLCube will be publicly available.",
    "descriptor": "",
    "authors": [
      "Wonseok Hwang",
      "Dongjun Lee",
      "Kyoungyeon Cho",
      "Hanuhl Lee",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05224"
  },
  {
    "id": "arXiv:2206.05225",
    "title": "ClamNet: Using contrastive learning with variable depth Unets for  medical image segmentation",
    "abstract": "Unets have become the standard method for semantic segmentation of medical\nimages, along with fully convolutional networks (FCN). Unet++ was introduced as\na variant of Unet, in order to solve some of the problems facing Unet and FCNs.\nUnet++ provided networks with an ensemble of variable depth Unets, hence\neliminating the need for professionals estimating the best suitable depth for a\ntask. While Unet and all its variants, including Unet++ aimed at providing\nnetworks that were able to train well without requiring large quantities of\nannotated data, none of them attempted to eliminate the need for pixel-wise\nannotated data altogether. Obtaining such data for each disease to be diagnosed\ncomes at a high cost. Hence such data is scarce. In this paper we use\ncontrastive learning to train Unet++ for semantic segmentation of medical\nimages using medical images from various sources including magnetic resonance\nimaging (MRI) and computed tomography (CT), without the need for pixel-wise\nannotations. Here we describe the architecture of the proposed model and the\ntraining method used. This is still a work in progress and so we abstain from\nincluding results in this paper. The results and the trained model would be\nmade available upon publication or in subsequent versions of this paper on\narxiv.",
    "descriptor": "",
    "authors": [
      "Samayan Bhattacharya",
      "Sk Shahnawaz",
      "Avigyan Bhattacharya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05225"
  },
  {
    "id": "arXiv:2206.05229",
    "title": "Measuring the Carbon Intensity of AI in Cloud Instances",
    "abstract": "By providing unprecedented access to computational resources, cloud computing\nhas enabled rapid growth in technologies such as machine learning, the\ncomputational demands of which incur a high energy cost and a commensurate\ncarbon footprint. As a result, recent scholarship has called for better\nestimates of the greenhouse gas impact of AI: data scientists today do not have\neasy or reliable access to measurements of this information, precluding\ndevelopment of actionable tactics. Cloud providers presenting information about\nsoftware carbon intensity to users is a fundamental stepping stone towards\nminimizing emissions. In this paper, we provide a framework for measuring\nsoftware carbon intensity, and propose to measure operational carbon emissions\nby using location-based and time-specific marginal emissions data per energy\nunit. We provide measurements of operational software carbon intensity for a\nset of modern models for natural language processing and computer vision, and a\nwide range of model sizes, including pretraining of a 6.1 billion parameter\nlanguage model. We then evaluate a suite of approaches for reducing emissions\non the Microsoft Azure cloud compute platform: using cloud instances in\ndifferent geographic regions, using cloud instances at different times of day,\nand dynamically pausing cloud instances when the marginal carbon intensity is\nabove a certain threshold. We confirm previous results that the geographic\nregion of the data center plays a significant role in the carbon intensity for\na given cloud instance, and find that choosing an appropriate region can have\nthe largest operational emissions reduction impact. We also show that the time\nof day has notable impact on operational software carbon intensity. Finally, we\nconclude with recommendations for how machine learning practitioners can use\nsoftware carbon intensity information to reduce environmental impact.",
    "descriptor": "\nComments: In ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) 2022\n",
    "authors": [
      "Jesse Dodge",
      "Taylor Prewitt",
      "Remi Tachet Des Combes",
      "Erika Odmark",
      "Roy Schwartz",
      "Emma Strubell",
      "Alexandra Sasha Luccioni",
      "Noah A. Smith",
      "Nicole DeCario",
      "Will Buchanan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05229"
  },
  {
    "id": "arXiv:2206.05238",
    "title": "Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus  Creation, Annotation Reliability, and Prediction",
    "abstract": "The most prominent tasks in emotion analysis are to assign emotions to texts\nand to understand how emotions manifest in language. An important observation\nfor natural language processing is that emotions can be communicated implicitly\nby referring to events alone, appealing to an empathetic, intersubjective\nunderstanding of events, even without explicitly mentioning an emotion name. In\npsychology, the class of emotion theories known as appraisal theories aims at\nexplaining the link between events and emotions. Appraisals can be formalized\nas variables that measure a cognitive evaluation by people living through an\nevent that they consider relevant. They include the assessment if an event is\nnovel, if the person considers themselves to be responsible, if it is in line\nwith the own goals, and many others. Such appraisals explain which emotions are\ndeveloped based on an event, e.g., that a novel situation can induce surprise\nor one with uncertain consequences could evoke fear. We analyze the suitability\nof appraisal theories for emotion analysis in text with the goal of\nunderstanding if appraisal concepts can reliably be reconstructed by\nannotators, if they can be predicted by text classifiers, and if appraisal\nconcepts help to identify emotion categories. To achieve that, we compile a\ncorpus by asking people to textually describe events that triggered particular\nemotions and to disclose their appraisals. Then, we ask readers to reconstruct\nemotions and appraisals from the text. This setup allows us to measure if\nemotions and appraisals can be recovered purely from text and provides a human\nbaseline to judge model's performance measures. Our comparison of text\nclassification methods to human annotators shows that both can reliably detect\nemotions and appraisals with similar performance. We further show that\nappraisal concepts improve the categorization of emotions in text.",
    "descriptor": "\nComments: 69 pages, 13 figures, 19 tables\n",
    "authors": [
      "Enrica Troiano",
      "Laura Oberl\u00e4nder",
      "Roman Klinger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05238"
  },
  {
    "id": "arXiv:2206.05239",
    "title": "StructCoder: Structure-Aware Transformer for Code Generation",
    "abstract": "There has been a recent surge of interest in automating software engineering\ntasks using deep learning. This work addresses the problem of code generation\nwhere the goal is to generate target code given source code in a different\nlanguage or a natural language description. Most of the state-of-the-art deep\nlearning models for code generation use training strategies that are primarily\ndesigned for natural language. However, understanding and generating code\nrequires a more rigorous comprehension of the code syntax and semantics. With\nthis motivation, we develop an encoder-decoder Transformer model where both the\nencoder and decoder are trained to recognize the syntax and data flow in the\nsource and target codes, respectively. We not only make the encoder\nstructure-aware by leveraging the source code's syntax tree and data flow\ngraph, but we also ensure that our decoder preserves the syntax and data flow\nof the target code by introducing two auxiliary tasks: AST (Abstract Syntax\nTree) paths prediction and data flow prediction. To the best of our knowledge,\nthis is the first work to introduce a structure-aware Transformer decoder to\nenhance the quality of generated code by modeling target syntax and data flow.\nThe proposed StructCoder model achieves state-of-the-art performance on code\ntranslation and text-to-code generation tasks in the CodeXGLUE benchmark.",
    "descriptor": "",
    "authors": [
      "Sindhu Tipirneni",
      "Ming Zhu",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.05239"
  },
  {
    "id": "arXiv:2206.05240",
    "title": "ROI Constrained Bidding via Curriculum-Guided Bayesian Reinforcement  Learning",
    "abstract": "Real-Time Bidding (RTB) is an important mechanism in modern online\nadvertising systems. Advertisers employ bidding strategies in RTB to optimize\ntheir advertising effects subject to various financial requirements, among\nwhich a widely adopted one is the return-on-investment (ROI) constraint. ROIs\nchange non-monotonically during the sequential bidding process, usually\npresenting a see-saw effect between constraint satisfaction and objective\noptimization. Existing solutions to the constraint-objective trade-off are\ntypically established in static or mildly changing markets. However, these\nmethods fail significantly in non-stationary advertising markets due to their\ninability to adapt to varying dynamics and partial observability. In this work,\nwe specialize in ROI-Constrained Bidding in non-stationary markets. Based on a\nPartially Observable Constrained Markov Decision Process, we propose the first\nhard barrier solution to accommodate non-monotonic constraints. Our method\nexploits a parameter-free indicator-augmented reward function and develops a\nCurriculum-Guided Bayesian Reinforcement Learning (CBRL) framework to\nadaptively control the constraint-objective trade-off in non-stationary\nadvertising markets. Extensive experiments on a large-scale industrial dataset\nwith two problem settings reveal that CBRL generalizes well in both\nin-distribution and out-of-distribution data regimes, and enjoys outstanding\nstability.",
    "descriptor": "\nComments: Accepted by SIGKDD 2022\n",
    "authors": [
      "Haozhe Wang",
      "Chao Du",
      "Panyan Fang",
      "Shuo Yuan",
      "Xuming He",
      "Liang Wang",
      "Bo Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05240"
  },
  {
    "id": "arXiv:2206.05245",
    "title": "List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering",
    "abstract": "We study the problem of list-decodable sparse mean estimation. Specifically,\nfor a parameter $\\alpha \\in (0, 1/2)$, we are given $m$ points in\n$\\mathbb{R}^n$, $\\lfloor \\alpha m \\rfloor$ of which are i.i.d. samples from a\ndistribution $D$ with unknown $k$-sparse mean $\\mu$. No assumptions are made on\nthe remaining points, which form the majority of the dataset. The goal is to\nreturn a small list of candidates containing a vector $\\widehat \\mu$ such that\n$\\| \\widehat \\mu - \\mu \\|_2$ is small. Prior work had studied the problem of\nlist-decodable mean estimation in the dense setting. In this work, we develop a\nnovel, conceptually simpler technique for list-decodable mean estimation. As\nthe main application of our approach, we provide the first sample and\ncomputationally efficient algorithm for list-decodable sparse mean estimation.\nIn particular, for distributions with ``certifiably bounded'' $t$-th moments in\n$k$-sparse directions and sufficiently light tails, our algorithm achieves\nerror of $(1/\\alpha)^{O(1/t)}$ with sample complexity $m =\n(k\\log(n))^{O(t)}/\\alpha$ and running time $\\mathrm{poly}(mn^t)$. For the\nspecial case of Gaussian inliers, our algorithm achieves the optimal error\nguarantee of $\\Theta (\\sqrt{\\log(1/\\alpha)})$ with quasi-polynomial sample and\ncomputational complexity. We complement our upper bounds with nearly-matching\nstatistical query and low-degree polynomial testing lower bounds.",
    "descriptor": "",
    "authors": [
      "Ilias Diakonikolas",
      "Daniel M. Kane",
      "Sushrut Karmalkar",
      "Ankit Pensia",
      "Thanasis Pittas"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05245"
  },
  {
    "id": "arXiv:2206.05252",
    "title": "Lost in Transmission: On the Impact of Networking Corruptions on Video  Machine Learning Models",
    "abstract": "We study how networking corruptions--data corruptions caused by networking\nerrors--affect video machine learning (ML) models. We discover apparent\nnetworking corruptions in Kinetics-400, a benchmark video ML dataset. In a\nsimulation study, we investigate (1) what artifacts networking corruptions\ncause, (2) how such artifacts affect ML models, and (3) whether standard\nrobustness methods can mitigate their negative effects. We find that networking\ncorruptions cause visual and temporal artifacts (i.e., smeared colors or frame\ndrops). These networking corruptions degrade performance on a variety of video\nML tasks, but effects vary by task and dataset, depending on how much temporal\ncontext the tasks require. Lastly, we evaluate data augmentation--a standard\ndefense for data corruptions--but find that it does not recover performance.",
    "descriptor": "\nComments: 12 pages, 12 figures (with supplemental: 34 pages)\n",
    "authors": [
      "Trenton Chang",
      "Daniel Y. Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05252"
  },
  {
    "id": "arXiv:2206.05253",
    "title": "Rethinking Spatial Invariance of Convolutional Networks for Object  Counting",
    "abstract": "Previous work generally believes that improving the spatial invariance of\nconvolutional networks is the key to object counting. However, after verifying\nseveral mainstream counting networks, we surprisingly found too strict\npixel-level spatial invariance would cause overfit noise in the density map\ngeneration. In this paper, we try to use locally connected Gaussian kernels to\nreplace the original convolution filter to estimate the spatial position in the\ndensity map. The purpose of this is to allow the feature extraction process to\npotentially stimulate the density map generation process to overcome the\nannotation noise. Inspired by previous work, we propose a low-rank\napproximation accompanied with translation invariance to favorably implement\nthe approximation of massive Gaussian convolution. Our work points a new\ndirection for follow-up research, which should investigate how to properly\nrelax the overly strict pixel-level spatial invariance for object counting. We\nevaluate our methods on 4 mainstream object counting networks (i.e., MCNN,\nCSRNet, SANet, and ResNet-50). Extensive experiments were conducted on 7\npopular benchmarks for 3 applications (i.e., crowd, vehicle, and plant\ncounting). Experimental results show that our methods significantly outperform\nother state-of-the-art methods and achieve promising learning of the spatial\nposition of objects.",
    "descriptor": "\nComments: Accepted to CVPR 2022, Code: this https URL\n",
    "authors": [
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Hong Li",
      "JingKuan Song",
      "Xiao Wu",
      "Alexander G. Hauptmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.05253"
  },
  {
    "id": "arXiv:2206.05255",
    "title": "Interactively Learning Preference Constraints in Linear Bandits",
    "abstract": "We study sequential decision-making with known rewards and unknown\nconstraints, motivated by situations where the constraints represent\nexpensive-to-evaluate human preferences, such as safe and comfortable driving\nbehavior. We formalize the challenge of interactively learning about these\nconstraints as a novel linear bandit problem which we call constrained linear\nbest-arm identification. To solve this problem, we propose the Adaptive\nConstraint Learning (ACOL) algorithm. We provide an instance-dependent lower\nbound for constrained linear best-arm identification and show that ACOL's\nsample complexity matches the lower bound in the worst-case. In the average\ncase, ACOL's sample complexity bound is still significantly tighter than bounds\nof simpler approaches. In synthetic experiments, ACOL performs on par with an\noracle solution and outperforms a range of baselines. As an application, we\nconsider learning constraints to represent human preferences in a driving\nsimulation. ACOL is significantly more sample efficient than alternatives for\nthis application. Further, we find that learning preferences as constraints is\nmore robust to changes in the driving scenario than encoding the preferences\ndirectly in the reward function.",
    "descriptor": "\nComments: Accepted to International Conference on Machine Learning (ICML), 2022\n",
    "authors": [
      "David Lindner",
      "Sebastian Tschiatschek",
      "Katja Hofmann",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05255"
  },
  {
    "id": "arXiv:2206.05256",
    "title": "Generic Reed-Solomon codes achieve list-decoding capacity",
    "abstract": "In a recent paper, Brakensiek, Gopi and Makam introduced higher order MDS\ncodes as a generalization of MDS codes. An order-$\\ell$ MDS code, denoted by\n$\\operatorname{MDS}(\\ell)$, has the property that any $\\ell$ subspaces formed\nfrom columns of its generator matrix intersect as minimally as possible. An\nindependent work by Roth defined a different notion of higher order MDS codes\nas those achieving a generalized singleton bound for list-decoding. In this\nwork, we show that these two notions of higher order MDS codes are (nearly)\nequivalent.\nWe also show that generic Reed-Solomon codes are $\\operatorname{MDS}(\\ell)$\nfor all $\\ell$, relying crucially on the GM-MDS theorem which shows that\ngenerator matrices of generic Reed-Solomon codes achieve any possible zero\npattern. As a corollary, this implies that generic Reed-Solomon codes achieve\nlist decoding capacity. More concretely, we show that, with high probability, a\nrandom Reed-Solomon code of rate $R$ over an exponentially large field is list\ndecodable from radius $1-R-\\epsilon$ with list size at most\n$\\frac{1-R-\\epsilon}{\\epsilon}$, resolving a conjecture of Shangguan and Tamo.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Joshua Brakensiek",
      "Sivakanth Gopi",
      "Visu Makam"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.05256"
  },
  {
    "id": "arXiv:2206.05257",
    "title": "Explaining Image Classifiers Using Contrastive Counterfactuals in  Generative Latent Spaces",
    "abstract": "Despite their high accuracies, modern complex image classifiers cannot be\ntrusted for sensitive tasks due to their unknown decision-making process and\npotential biases. Counterfactual explanations are very effective in providing\ntransparency for these black-box algorithms. Nevertheless, generating\ncounterfactuals that can have a consistent impact on classifier outputs and yet\nexpose interpretable feature changes is a very challenging task. We introduce a\nnovel method to generate causal and yet interpretable counterfactual\nexplanations for image classifiers using pretrained generative models without\nany re-training or conditioning. The generative models in this technique are\nnot bound to be trained on the same data as the target classifier. We use this\nframework to obtain contrastive and causal sufficiency and necessity scores as\nglobal explanations for black-box classifiers. On the task of face attribute\nclassification, we show how different attributes influence the classifier\noutput by providing both causal and contrastive feature attributions, and the\ncorresponding counterfactual images.",
    "descriptor": "",
    "authors": [
      "Kamran Alipour",
      "Aditya Lahiri",
      "Ehsan Adeli",
      "Babak Salimi",
      "Michael Pazzani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05257"
  },
  {
    "id": "arXiv:2206.05259",
    "title": "Is Self-Supervised Learning More Robust Than Supervised Learning?",
    "abstract": "Self-supervised contrastive learning is a powerful tool to learn visual\nrepresentation without labels. Prior work has primarily focused on evaluating\nthe recognition accuracy of various pre-training algorithms, but has overlooked\nother behavioral aspects. In addition to accuracy, distributional robustness\nplays a critical role in the reliability of machine learning models. We design\nand conduct a series of robustness tests to quantify the behavioral differences\nbetween contrastive learning and supervised learning to downstream or\npre-training data distribution changes. These tests leverage data corruptions\nat multiple levels, ranging from pixel-level gamma distortion to patch-level\nshuffling and to dataset-level distribution shift. Our tests unveil intriguing\nrobustness behaviors of contrastive and supervised learning. On the one hand,\nunder downstream corruptions, we generally observe that contrastive learning is\nsurprisingly more robust than supervised learning. On the other hand, under\npre-training corruptions, we find contrastive learning vulnerable to patch\nshuffling and pixel intensity change, yet less sensitive to dataset-level\ndistribution change. We attempt to explain these results through the role of\ndata augmentation and feature space properties. Our insight has implications in\nimproving the downstream robustness of supervised learning.",
    "descriptor": "",
    "authors": [
      "Yuanyi Zhong",
      "Haoran Tang",
      "Junkun Chen",
      "Jian Peng",
      "Yu-Xiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05259"
  },
  {
    "id": "arXiv:2206.05260",
    "title": "Balanced Product of Experts for Long-Tailed Recognition",
    "abstract": "Many real-world recognition problems suffer from an imbalanced or long-tailed\nlabel distribution. Those distributions make representation learning more\nchallenging due to limited generalization over the tail classes. If the test\ndistribution differs from the training distribution, e.g. uniform versus\nlong-tailed, the problem of the distribution shift needs to be addressed. To\nthis aim, recent works have extended softmax cross-entropy using margin\nmodifications, inspired by Bayes' theorem. In this paper, we generalize several\napproaches with a Balanced Product of Experts (BalPoE), which combines a family\nof models with different test-time target distributions to tackle the imbalance\nin the data. The proposed experts are trained in a single stage, either jointly\nor independently, and fused seamlessly into a BalPoE. We show that BalPoE is\nFisher consistent for minimizing the balanced error and perform extensive\nexperiments to validate the effectiveness of our approach. Finally, we\ninvestigate the effect of Mixup in this setting, discovering that\nregularization is a key ingredient for learning calibrated experts. Our\nexperiments show that a regularized BalPoE can perform remarkably well in test\naccuracy and calibration metrics, leading to state-of-the-art results on\nCIFAR-100-LT, ImageNet-LT, and iNaturalist-2018 datasets. The code will be made\npublicly available upon paper acceptance.",
    "descriptor": "\nComments: 19 pages, under review\n",
    "authors": [
      "Emanuel Sanchez Aimar",
      "Arvi Jonnarth",
      "Michael Felsberg",
      "Marco Kuhlmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05260"
  },
  {
    "id": "arXiv:2206.05262",
    "title": "Meta Optimal Transport",
    "abstract": "We study the use of amortized optimization to predict optimal transport (OT)\nmaps from the input measures, which we call Meta OT. This helps repeatedly\nsolve similar OT problems between different measures by leveraging the\nknowledge and information present from past problems to rapidly predict and\nsolve new problems. Otherwise, standard methods ignore the knowledge of the\npast solutions and suboptimally re-solve each problem from scratch. Meta OT\nmodels surpass the standard convergence rates of log-Sinkhorn solvers in the\ndiscrete setting and convex potentials in the continuous setting. We improve\nthe computational time of standard OT solvers by multiple orders of magnitude\nin discrete and continuous transport settings between images, spherical data,\nand color palettes. Our source code is available at\nthis http URL",
    "descriptor": "",
    "authors": [
      "Brandon Amos",
      "Samuel Cohen",
      "Giulia Luise",
      "Ievgen Redko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05262"
  },
  {
    "id": "arXiv:2206.05263",
    "title": "Causal Balancing for Domain Generalization",
    "abstract": "While machine learning models rapidly advance the state-of-the-art on various\nreal-world tasks, out-of-domain (OOD) generalization remains a challenging\nproblem given the vulnerability of these models to spurious correlations. While\ncurrent domain generalization methods usually focus on enforcing certain\ninvariance properties across different domains by new loss function designs, we\npropose a balanced mini-batch sampling strategy to reduce the domain-specific\nspurious correlations in the observed training distributions. More\nspecifically, we propose a two-phased method that 1) identifies the source of\nspurious correlations, and 2) builds balanced mini-batches free from spurious\ncorrelations by matching on the identified source. We provide an\nidentifiability guarantee of the source of spuriousness and show that our\nproposed approach provably samples from a balanced, spurious-free distribution\nover all training environments. Experiments are conducted on three computer\nvision datasets with documented spurious correlations, demonstrating\nempirically that our balanced mini-batch sampling strategy improves the\nperformance of four different established domain generalization model baselines\ncompared to the random mini-batch sampling strategy.",
    "descriptor": "\nComments: 16 pages, 3 figures, 4 tables\n",
    "authors": [
      "Xinyi Wang",
      "Michael Saxon",
      "Jiachen Li",
      "Hongyang Zhang",
      "Kun Zhang",
      "William Yang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05263"
  },
  {
    "id": "arXiv:2206.05266",
    "title": "Does Self-supervised Learning Really Improve Reinforcement Learning from  Pixels?",
    "abstract": "We investigate whether self-supervised learning (SSL) can improve online\nreinforcement learning (RL) from pixels. We extend the contrastive\nreinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL\nlosses and conduct an extensive amount of experiments with various\nself-supervised losses. Our observations suggest that the existing SSL\nframework for RL fails to bring meaningful improvement over the baselines only\ntaking advantage of image augmentation when the same amount of data and\naugmentation is used. We further perform an evolutionary search to find the\noptimal combination of multiple self-supervised losses for RL, but find that\neven such a loss combination fails to meaningfully outperform the methods that\nonly utilize carefully designed image augmentations. Often, the use of\nself-supervised losses under the existing framework lowered RL performances. We\nevaluate the approach in multiple different environments including a real-world\nrobot environment and confirm that no single self-supervised loss or image\naugmentation method can dominate all environments and that the current\nframework for joint optimization of SSL and RL is limited. Finally, we\nempirically investigate the pretraining framework for SSL + RL and the\nproperties of representations learned with different approaches.",
    "descriptor": "",
    "authors": [
      "Xiang Li",
      "Jinghuan Shang",
      "Srijan Das",
      "Michael S. Ryoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05266"
  },
  {
    "id": "arXiv:2206.04681",
    "title": "Gaussian Fourier Pyramid for Local Laplacian Filter",
    "abstract": "Multi-scale processing is essential in image processing and computer\ngraphics. Halos are a central issue in multi-scale processing. Several\nedge-preserving decompositions resolve halos, e.g., local Laplacian filtering\n(LLF), by extending the Laplacian pyramid to have an edge-preserving property.\nIts processing is costly; thus, an approximated acceleration of fast LLF was\nproposed to linearly interpolate multiple Laplacian pyramids. This paper\nfurther improves the accuracy by Fourier series expansion, named Fourier LLF.\nOur results showed that Fourier LLF has a higher accuracy for the same number\nof pyramids. Moreover, Fourier LLF exhibits parameter-adaptive property for\ncontent-adaptive filtering. The code is available at:\nhttps://norishigefukushima.github.io/GaussianFourierPyramid/.",
    "descriptor": "",
    "authors": [
      "Yuto Sumiya",
      "Tomoki Otsuka",
      "Yoshihiro Maeda",
      "Norishige Fukushima"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.04681"
  },
  {
    "id": "arXiv:2206.04682",
    "title": "RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search  for 3D Cardiac Cine MRI Segmentation",
    "abstract": "Accurately segmenting temporal frames of cine magnetic resonance imaging\n(MRI) is a crucial step in various real-time MRI guided cardiac interventions.\nTo achieve fast and accurate visual assistance, there are strict requirements\non the maximum latency and minimum throughput of the segmentation framework.\nState-of-the-art neural networks on this task are mostly hand-crafted to\nsatisfy these constraints while achieving high accuracy. On the other hand,\nwhile existing literature have demonstrated the power of neural architecture\nsearch (NAS) in automatically identifying the best neural architectures for\nvarious medical applications, they are mostly guided by accuracy, sometimes\nwith computation complexity, and the importance of real-time constraints are\noverlooked. A major challenge is that such constraints are non-differentiable\nand are thus not compatible with the widely used differentiable NAS frameworks.\nIn this paper, we present a strategy that directly handles real-time\nconstraints in a differentiable NAS framework named RT-DNAS. Experiments on\nextended 2017 MICCAI ACDC dataset show that compared with state-of-the-art\nmanually and automatically designed architectures, RT-DNAS is able to identify\nones with better accuracy while satisfying the real-time constraints.",
    "descriptor": "",
    "authors": [
      "Qing Lu",
      "Xiaowei Xu",
      "Shunjie Dong",
      "Callie Hao",
      "Lei Yang",
      "Cheng Zhuo",
      "Yiyu Shi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04682"
  },
  {
    "id": "arXiv:2206.04683",
    "title": "Molecular dynamics without molecules: searching the conformational space  of proteins with generative neural networks",
    "abstract": "All-atom and coarse-grained molecular dynamics are two widely used\ncomputational tools to study the conformational states of proteins. Yet, these\ntwo simulation methods suffer from the fact that without access to\nsupercomputing resources, the time and length scales at which these states\nbecome detectable are difficult to achieve. One alternative to such methods is\nbased on encoding the atomistic trajectory of molecular dynamics as a shorthand\nversion devoid of physical particles, and then learning to propagate the\nencoded trajectory through the use of artificial intelligence. Here we show\nthat a simple textual representation of the frames of molecular dynamics\ntrajectories as vectors of Ramachandran basin classes retains most of the\nstructural information of the full atomistic representation of a protein in\neach frame, and can be used to generate equivalent atom-less trajectories\nsuitable to train different types of generative neural networks. In turn, the\ntrained generative models can be used to extend indefinitely the atom-less\ndynamics or to sample the conformational space of proteins from their\nrepresentation in the models latent space. We define intuitively this\nmethodology as molecular dynamics without molecules, and show that it enables\nto cover physically relevant states of proteins that are difficult to access\nwith traditional molecular dynamics.",
    "descriptor": "\nComments: 12 pages, 9 figures, 3 tables\n",
    "authors": [
      "Gregory Schwing",
      "Luigi L. Palese",
      "Ariel Fern\u00e1ndez",
      "Loren Schwiebert",
      "Domenico L. Gatti"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2206.04683"
  },
  {
    "id": "arXiv:2206.04684",
    "title": "Structure-consistent Restoration Network for Cataract Fundus Image  Enhancement",
    "abstract": "Fundus photography is a routine examination in clinics to diagnose and\nmonitor ocular diseases. However, for cataract patients, the fundus image\nalways suffers quality degradation caused by the clouding lens. The degradation\nprevents reliable diagnosis by ophthalmologists or computer-aided systems. To\nimprove the certainty in clinical diagnosis, restoration algorithms have been\nproposed to enhance the quality of fundus images. Unfortunately, challenges\nremain in the deployment of these algorithms, such as collecting sufficient\ntraining data and preserving retinal structures. In this paper, to circumvent\nthe strict deployment requirement, a structure-consistent restoration network\n(SCR-Net) for cataract fundus images is developed from synthesized data that\nshares an identical structure. A cataract simulation model is firstly designed\nto collect synthesized cataract sets (SCS) formed by cataract fundus images\nsharing identical structures. Then high-frequency components (HFCs) are\nextracted from the SCS to constrain structure consistency such that the\nstructure preservation in SCR-Net is enforced. The experiments demonstrate the\neffectiveness of SCR-Net in the comparison with state-of-the-art methods and\nthe follow-up clinical applications. The code is available at\nhttps://github.com/liamheng/ArcNet-Medical-Image-Enhancement.",
    "descriptor": "",
    "authors": [
      "Heng Li",
      "Haofeng Liu",
      "Huazhu Fu",
      "Hai Shu",
      "Yitian Zhao",
      "Xiaoling Luo",
      "Yan Hu",
      "Jiang Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04684"
  },
  {
    "id": "arXiv:2206.04689",
    "title": "AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding  Biomechanical Testing",
    "abstract": "$\\mathbf{Purpose}$: To use artificial intelligence (AI) to: (1) exploit\nbiomechanical knowledge of the optic nerve head (ONH) from a relatively large\npopulation; (2) assess ONH robustness from a single optical coherence\ntomography (OCT) scan of the ONH; (3) identify what critical three-dimensional\n(3D) structural features make a given ONH robust.\n$\\mathbf{Design}$: Retrospective cross-sectional study.\n$\\mathbf{Methods}$: 316 subjects had their ONHs imaged with OCT before and\nafter acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry.\nIOP-induced lamina-cribrosa deformations were then mapped in 3D and used to\nclassify ONHs. Those with LC deformations superior to 4% were considered\nfragile, while those with deformations inferior to 4% robust. Learning from\nthese data, we compared three AI algorithms to predict ONH robustness strictly\nfrom a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an\nautoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also\nallowed us to identify what critical 3D structural features make a given ONH\nrobust.\n$\\mathbf{Results}$: All 3 methods were able to predict ONH robustness from 3D\nstructural information alone and without the need to perform biomechanical\ntesting. The DGCNN (area under the receiver operating curve [AUC]: 0.76 $\\pm$\n0.08) outperformed the autoencoder (AUC: 0.70 $\\pm$ 0.07) and the random forest\nclassifier (AUC: 0.69 $\\pm$ 0.05). Interestingly, to assess ONH robustness, the\nDGCNN mainly used information from the scleral canal and the LC insertion\nsites.\n$\\mathbf{Conclusions}$: We propose an AI-driven approach that can assess the\nrobustness of a given ONH solely from a single OCT scan of the ONH, and without\nthe need to perform biomechanical testing. Longitudinal studies should\nestablish whether ONH robustness could help us identify fast visual field loss\nprogressors.",
    "descriptor": "",
    "authors": [
      "Fabian A. Braeu",
      "Thanadet Chuangsuwanich",
      "Tin A. Tun",
      "Alexandre H. Thiery",
      "Tin Aung",
      "George Barbastathis",
      "Micha\u00ebl J.A. Girard"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04689"
  },
  {
    "id": "arXiv:2206.04727",
    "title": "STNDT: Modeling Neural Population Activity with a Spatiotemporal  Transformer",
    "abstract": "Modeling neural population dynamics underlying noisy single-trial spiking\nactivities is essential for relating neural observation and behavior. A recent\nnon-recurrent method - Neural Data Transformers (NDT) - has shown great success\nin capturing neural dynamics with low inference latency without an explicit\ndynamical model. However, NDT focuses on modeling the temporal evolution of the\npopulation activity while neglecting the rich covariation between individual\nneurons. In this paper we introduce SpatioTemporal Neural Data Transformer\n(STNDT), an NDT-based architecture that explicitly models responses of\nindividual neurons in the population across time and space to uncover their\nunderlying firing rates. In addition, we propose a contrastive learning loss\nthat works in accordance with mask modeling objective to further improve the\npredictive performance. We show that our model achieves state-of-the-art\nperformance on ensemble level in estimating neural activities across four\nneural datasets, demonstrating its capability to capture autonomous and\nnon-autonomous dynamics spanning different cortical regions while being\ncompletely agnostic to the specific behaviors at hand. Furthermore, STNDT\nspatial attention mechanism reveals consistently important subsets of neurons\nthat play a vital role in driving the response of the entire population,\nproviding interpretability and key insights into how the population of neurons\nperforms computation.",
    "descriptor": "",
    "authors": [
      "Trung Le",
      "Eli Shlizerman"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04727"
  },
  {
    "id": "arXiv:2206.04732",
    "title": "AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging",
    "abstract": "This paper presents the baseline approach for the organized 2nd Covid-19\nCompetition, occurring in the framework of the AIMIA Workshop in the European\nConference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database\nwhich is annotated for COVID-19 detction, consisting of about 7,700 3-D CT\nscans. Part of the database consisting of Covid-19 cases is further annotated\nin terms of four Covid-19 severity conditions. We have split the database and\nthe latter part of it in training, validation and test datasets. The former two\ndatasets are used for training and validation of machine learning models, while\nthe latter will be used for evaluation of the developed models. The baseline\napproach consists of a deep learning approach, based on a CNN-RNN network and\nreport its performance on the COVID19-CT-DB database.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2106.07524\n",
    "authors": [
      "Dimitrios Kollias",
      "Anastasios Arsenos",
      "Stefanos Kollias"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04732"
  },
  {
    "id": "arXiv:2206.04733",
    "title": "On Low-Complexity Quickest Intervention of Mutated Diffusion Processes  Through Local Approximation",
    "abstract": "We consider the problem of controlling a mutated diffusion process with an\nunknown mutation time. The problem is formulated as the quickest intervention\nproblem with the mutation modeled by a change-point, which is a generalization\nof the quickest change-point detection (QCD). Our goal is to intervene in the\nmutated process as soon as possible while maintaining a low intervention cost\nwith optimally chosen intervention actions. This model and the proposed\nalgorithms can be applied to pandemic prevention (such as Covid-19) or\nmisinformation containment. We formulate the problem as a partially observed\nMarkov decision process (POMDP) and convert it to an MDP through the belief\nstate of the change-point. We first propose a grid approximation approach to\ncalculate the optimal intervention policy, whose computational complexity could\nbe very high when the number of grids is large. In order to reduce the\ncomputational complexity, we further propose a low-complexity threshold-based\npolicy through the analysis of the first-order approximation of the value\nfunctions in the ``local intervention'' regime. Simulation results show the\nlow-complexity algorithm has a similar performance as the grid approximation\nand both perform much better than the QCD-based algorithms.",
    "descriptor": "",
    "authors": [
      "Qining Zhang",
      "Honghao Wei",
      "Weina Wang",
      "Lei Ying"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Systems and Control (eess.SY)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.04733"
  },
  {
    "id": "arXiv:2206.04741",
    "title": "Quantum Policy Iteration via Amplitude Estimation and Grover Search --  Towards Quantum Advantage for Reinforcement Learning",
    "abstract": "We present a full implementation and simulation of a novel quantum\nreinforcement learning (RL) method and mathematically prove a quantum\nadvantage. Our approach shows in detail how to combine amplitude estimation and\nGrover search into a policy evaluation and improvement scheme. We first develop\nquantum policy evaluation (QPE) which is quadratically more efficient compared\nto an analogous classical Monte Carlo estimation and is based on a quantum\nmechanical realization of a finite Markov decision process (MDP). Building on\nQPE, we derive a quantum policy iteration that repeatedly improves an initial\npolicy using Grover search until the optimum is reached. Finally, we present an\nimplementation of our algorithm for a two-armed bandit MDP which we then\nsimulate. The results confirm that QPE provides a quantum advantage in RL\nproblems.",
    "descriptor": "",
    "authors": [
      "Simon Wiedemann",
      "Daniel Hein",
      "Steffen Udluft",
      "Christian Mendl"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04741"
  },
  {
    "id": "arXiv:2206.04760",
    "title": "A theory explaining the limits and performances of algorithms based on  simulated annealing in solving sparse hard inference problems",
    "abstract": "The planted coloring problem is a prototypical inference problem for which\nthresholds for Bayes optimal algorithms, like Belief Propagation (BP), can be\ncomputed analytically. In this paper, we analyze the limits and performances of\nthe Simulated Annealing (SA), a Monte Carlo-based algorithm that is more\ngeneral and robust than BP, and thus of broader applicability. We show that SA\nis sub-optimal in the recovery of the planted solution because it gets\nattracted by glassy states that, instead, do not influence the BP algorithm. At\nvariance with previous conjectures, we propose an analytic estimation for the\nSA algorithmic threshold by comparing the spinodal point of the paramagnetic\nphase and the dynamical critical temperature. This is a fundamental connection\nbetween thermodynamical phase transitions and out of equilibrium behavior of\nGlauber dynamics. We also study an improved version of SA, called replicated SA\n(RSA), where several weakly coupled replicas are cooled down together. We show\nnumerical evidence that the algorithmic threshold for the RSA coincides with\nthe Bayes optimal one. Finally, we develop an approximated analytical theory\nexplaining the optimal performances of RSA and predicting the location of the\ntransition towards the planted solution in the limit of a very large number of\nreplicas. Our results for RSA support the idea that mismatching the parameters\nin the prior with respect to those of the generative model may produce an\nalgorithm that is optimal and very robust.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Maria Chiara Angelini",
      "Federico Ricci-Tersenghi"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.04760"
  },
  {
    "id": "arXiv:2206.04770",
    "title": "A Continuous-Time Perspective on Monotone Equation Problems",
    "abstract": "We study \\textit{rescaled gradient dynamical systems} in a Hilbert space\n$\\mathcal{H}$, where the implicit discretization in a finite-dimensional\nEuclidean space leads to high-order methods for solving monotone equations\n(MEs). Our framework generalizes the celebrated dual extrapolation\nmethod~\\citep{Nesterov-2007-Dual} from first order to high order via appeal to\nthe regularization toolbox of optimization\ntheory~\\citep{Nesterov-2021-Implementable, Nesterov-2021-Inexact}. We establish\nthe existence and uniqueness of a global solution and analyze the convergence\nproperties of solution trajectories. We also present discrete-time counterparts\nof our high-order continuous-time methods, and we show that the $p^{th}$-order\nmethod achieves an ergodic rate of $O(k^{-(p+1)/2})$ in terms of a restricted\nmerit function and a pointwise rate of $O(k^{-p/2})$ in terms of a residue\nfunction. Under regularity conditions, the restarted version of $p^{th}$-order\nmethods achieves local convergence with the order $p \\geq 2$.",
    "descriptor": "\nComments: 35 Pages\n",
    "authors": [
      "Tianyi Lin",
      "Michael. I. Jordan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.04770"
  },
  {
    "id": "arXiv:2206.04804",
    "title": "Theoretical Error Performance Analysis for Variational Quantum Circuit  Based Functional Regression",
    "abstract": "The noisy intermediate-scale quantum (NISQ) devices enable the implementation\nof the variational quantum circuit (VQC) for quantum neural networks (QNN).\nAlthough the VQC-based QNN has succeeded in many machine learning tasks, the\nrepresentation and generalization powers of VQC still require further\ninvestigation, particularly when the dimensionality reduction of classical\ninputs is concerned. In this work, we first put forth an end-to-end quantum\nneural network, namely, TTN-VQC, which consists of a quantum tensor network\nbased on a tensor-train network (TTN) for dimensionality reduction and a VQC\nfor functional regression. Then, we aim at the error performance analysis for\nthe TTN-VQC in terms of representation and generalization powers. We also\ncharacterize the optimization properties of TTN-VQC by leveraging the\nPolyak-Lojasiewicz (PL) condition. Moreover, we conduct the experiments of\nfunctional regression on a handwritten digit classification dataset to justify\nour theoretical analysis.",
    "descriptor": "\nComments: Preprint version. 16 pages\n",
    "authors": [
      "Jun Qi",
      "Chao-Han Huck Yang",
      "Pin-Yu Chen",
      "Min-Hsiu Hsieh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.04804"
  },
  {
    "id": "arXiv:2206.04814",
    "title": "Universal Properties of Partial Quantum Maps",
    "abstract": "We provide a universal construction of the category of finite-dimensional\nC*-algebras and completely positive trace-nonincreasing maps from the rig\ncategory of finite-dimensional Hilbert spaces and unitaries. This construction,\nwhich can be applied to any dagger rig category, is described in three steps,\neach associated with their own universal property, and draws on results from\ndilation theory in finite dimension. In this way, we explicitly construct the\ncategory that captures hybrid quantum/classical computation with possible\nnontermination from the category of its reversible foundations. We discuss how\nthis construction can be used in the design and semantics of quantum\nprogramming languages.",
    "descriptor": "\nComments: 17 pages, accepted for QPL 2022\n",
    "authors": [
      "Pablo Andr\u00e9s-Mart\u00ednez",
      "Chris Heunen",
      "Robin Kaarsgaard"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Operator Algebras (math.OA)"
    ],
    "url": "https://arxiv.org/abs/2206.04814"
  },
  {
    "id": "arXiv:2206.04815",
    "title": "Connections between graphs and matrix spaces",
    "abstract": "Given a bipartite graph $G$, the graphical matrix space $\\mathcal{S}_G$\nconsists of matrices whose non-zero entries can only be at those positions\ncorresponding to edges in $G$. Tutte (J. London Math. Soc., 1947), Edmonds (J.\nRes. Nat. Bur. Standards Sect. B, 1967) and Lov\\'asz (FCT, 1979) observed\nconnections between perfect matchings in $G$ and full-rank matrices in\n$\\mathcal{S}_G$. Dieudonn\\'e ({Arch. Math., 1948) proved a tight upper bound on\nthe dimensions of those matrix spaces containing only singular matrices. The\nstarting point of this paper is a simultaneous generalization of these two\nclassical results: we show that the largest dimension over subspaces of\n$\\mathcal{S}_G$ containing only singular matrices is equal to the maximum size\nover subgraphs of $G$ without perfect matchings, based on Meshulam's proof of\nDieudonn\\'e's result (Quart. J. Math., 1985).\nStarting from this result, we go on to establish more connections between\nproperties of graphs and matrix spaces. For example, we establish connections\nbetween acyclicity and nilpotency, between strong connectivity and\nirreducibility, and between isomorphism and conjugacy/congruence. For each\nconnection, we study three types of correspondences, namely the basic\ncorrespondence, the inherited correspondence (for subgraphs and subspaces), and\nthe induced correspondence (for induced subgraphs and restrictions). Some\ncorrespondences lead to intriguing generalizations of classical results, such\nas for Dieudonn\\'e's result mentioned above, and for a celebrated theorem of\nGerstenhaber regarding the largest dimension of nil matrix spaces (Amer. J.\nMath., 1958).\nFinally, we show some implications of our results to quantum information and\npresent open problems in computational complexity motivated by these results.",
    "descriptor": "\nComments: 45 pages\n",
    "authors": [
      "Yinan Li",
      "Youming Qiao",
      "Avi Wigderson",
      "Yuval Wigderson",
      "Chuanqi Zhang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)",
      "Rings and Algebras (math.RA)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.04815"
  },
  {
    "id": "arXiv:2206.04837",
    "title": "Some Extremal Symmetric Inequalities",
    "abstract": "Let $\\mathcal{H}_{n,d}^s := \\mathbb{R}[x_1$,$\\ldots$,\n$x_n]_d^{\\mathfrak{S}_n}$ be the set of symmetric polynomials of degree $d$,\nand $A$ be a compact semialgebraic subset of $\\mathbb{P}_{\\mathbb{R}}^{n-1}$.\nWe shall study some PSD cones $\\mathbb{P}(A$, $\\mathcal{H}_{n,d}^s) := \\big\\{f\n\\in\n\\mathcal{H}_{n,d}^s$ $\\big|$ $f(a) \\geq 0$ ($\\forall a \\in A$)$\\big\\}$. In\nthis article, we study extremal elements of $\\mathcal{P}(\\mathbb{R}_+^3$,\n$\\mathcal{H}_{3,5}^{s+})$, $\\mathcal{P}(\\mathbb{R}_+^3$,\n$\\mathcal{H}_{3,6}^{s0})$, and $\\mathcal{P}(\\mathbb{R}^4$,\n$\\mathcal{H}_{4,4}^s)$ where $\\mathcal{P}(\\mathbb{R}_+^3$,\n$\\mathcal{H}_{3,6}^{s0}) := \\big\\{f \\in \\mathcal{P}(\\mathbb{R}_+^3$,\n$\\mathcal{H}_{3,6}^s)$ $\\big|$ $f(1,\\ldots,1)=0 \\big\\}$.",
    "descriptor": "",
    "authors": [
      "Tetsuya Ando"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.04837"
  },
  {
    "id": "arXiv:2206.04850",
    "title": "Feature-informed Embedding Space Regularization For Audio Classification",
    "abstract": "Feature representations derived from models pre-trained on large-scale\ndatasets have shown their generalizability on a variety of audio analysis\ntasks. Despite this generalizability, however, task-specific features can\noutperform if sufficient training data is available, as specific task-relevant\nproperties can be learned. Furthermore, the complex pre-trained models bring\nconsiderable computational burdens during inference. We propose to leverage\nboth detailed task-specific features from spectrogram input and generic\npre-trained features by introducing two regularization methods that integrate\nthe information of both feature classes. The workload is kept low during\ninference as the pre-trained features are only necessary for training. In\nexperiments with the pre-trained features VGGish, OpenL3, and a combination of\nboth, we show that the proposed methods not only outperform baseline methods,\nbut also can improve state-of-the-art models on several audio classification\ntasks. The results also suggest that using the mixture of features performs\nbetter than using individual features.",
    "descriptor": "",
    "authors": [
      "Yun-Ning Hung",
      "Alexander Lerch"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.04850"
  },
  {
    "id": "arXiv:2206.04877",
    "title": "Efficient Per-Shot Convex Hull Prediction By Recurrent Learning",
    "abstract": "Adaptive video streaming relies on the construction of efficient bitrate\nladders to deliver the best possible visual quality to viewers under bandwidth\nconstraints. The traditional method of content dependent bitrate ladder\nselection requires a video shot to be pre-encoded with multiple encoding\nparameters to find the optimal operating points given by the convex hull of the\nresulting rate-quality curves. However, this pre-encoding step is equivalent to\nan exhaustive search process over the space of possible encoding parameters,\nwhich causes significant overhead in terms of both computation and time\nexpenditure. To reduce this overhead, we propose a deep learning based method\nof content aware convex hull prediction. We employ a recurrent convolutional\nnetwork (RCN) to implicitly analyze the spatiotemporal complexity of video\nshots in order to predict their convex hulls. A two-step transfer learning\nscheme is adopted to train our proposed RCN-Hull model, which ensures\nsufficient content diversity to analyze scene complexity, while also making it\npossible capture the scene statistics of pristine source videos. Our\nexperimental results reveal that our proposed model yields better\napproximations of the optimal convex hulls, and offers competitive time savings\nas compared to existing approaches. On average, the pre-encoding time was\nreduced by 58.0% by our method, while the average Bjontegaard delta bitrate\n(BD-rate) of the predicted convex hulls against ground truth was 0.08%, while\nthe mean absolute deviation of the BD-rate distribution was 0.44%",
    "descriptor": "",
    "authors": [
      "Somdyuti Paul",
      "Andrey Norkin",
      "Alan C. Bovik"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04877"
  },
  {
    "id": "arXiv:2206.04967",
    "title": "Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G",
    "abstract": "Recently, inspired by successful applications in many fields, deep learning\n(DL) technologies for CSI acquisition have received considerable research\ninterest from both academia and industry. Considering the practical feedback\nmechanism of 5th generation (5G) New radio (NR) networks, we propose two\nimplementation schemes for artificial intelligence for CSI (AI4CSI), the\nDL-based receiver and end-to-end design, respectively. The proposed AI4CSI\nschemes were evaluated in 5G NR networks in terms of spectrum efficiency (SE),\nfeedback overhead, and computational complexity, and compared with legacy\nschemes. To demonstrate whether these schemes can be used in real-life\nscenarios, both the modeled-based channel data and practically measured\nchannels were used in our investigations. When DL-based CSI acquisition is\napplied to the receiver only, which has little air interface impact, it\nprovides approximately 25\\% SE gain at a moderate feedback overhead level. It\nis feasible to deploy it in current 5G networks during 5G evolutions. For the\nend-to-end DL-based CSI enhancements, the evaluations also demonstrated their\nadditional performance gain on SE, which is 6% -- 26% compared with DL-based\nreceivers and 33% -- 58% compared with legacy CSI schemes. Considering its\nlarge impact on air-interface design, it will be a candidate technology for 6th\ngeneration (6G) networks, in which an air interface designed by artificial\nintelligence can be used.",
    "descriptor": "\nComments: To be published on IEICE Transactions on Communications\n",
    "authors": [
      "Xin Wang",
      "Xiaolin Hou",
      "Lan Chen",
      "Yoshihisa Kishiyama",
      "Takahiro Asai"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.04967"
  },
  {
    "id": "arXiv:2206.04990",
    "title": "Efficient Quantum Circuit Design with a Standard Cell Approach",
    "abstract": "We design quantum circuits by using a standard cell approach. Standard cells\nare a representation before deciding to compile either NISQ or surface code\nlattice surgery circuits. This approach can speed-up the layout of circuits\nwith a regular structure. Our approach to co-designing quantum circuits can be\nused to estimate the resources necessary for the computation without\neffectively using complex compilation methods. Furthermore, we form standard\ncells to support Toffoli gate decompositions and starting from a 3D design of a\nmultiplication circuit, we present evidence that, compared to the existing\nquantum circuit compilers, our method achieves shallower 3D circuits (by at\nleast 2.5x) and with less SWAPs.",
    "descriptor": "",
    "authors": [
      "Evan E. Dobbs",
      "Joseph S. Friedman",
      "Alexandru Paler"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2206.04990"
  },
  {
    "id": "arXiv:2206.05032",
    "title": "Scalable Deep Gaussian Markov Random Fields for General Graphs",
    "abstract": "Machine learning methods on graphs have proven useful in many applications\ndue to their ability to handle generally structured data. The framework of\nGaussian Markov Random Fields (GMRFs) provides a principled way to define\nGaussian models on graphs by utilizing their sparsity structure. We propose a\nflexible GMRF model for general graphs built on the multi-layer structure of\nDeep GMRFs, originally proposed for lattice graphs only. By designing a new\ntype of layer we enable the model to scale to large graphs. The layer is\nconstructed to allow for efficient training using variational inference and\nexisting software frameworks for Graph Neural Networks. For a Gaussian\nlikelihood, close to exact Bayesian inference is available for the latent\nfield. This allows for making predictions with accompanying uncertainty\nestimates. The usefulness of the proposed model is verified by experiments on a\nnumber of synthetic and real world datasets, where it compares favorably to\nother both Bayesian and deep learning methods.",
    "descriptor": "\nComments: 22 pages, 10 figures. Accepted at ICML 2022. Code available at this https URL\n",
    "authors": [
      "Joel Oskarsson",
      "Per Sid\u00e9n",
      "Fredrik Lindsten"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.05032"
  },
  {
    "id": "arXiv:2206.05043",
    "title": "Fast synchronization of inhomogenous random automata",
    "abstract": "We examine the reset threshold of randomly generated deterministic automata.\nWe claim that an automaton with a random mapping and two random permutation\nletters has a reset threshold of $\\mathcal{O}\\big( \\sqrt{n \\log^3 n} \\big)$\nwith high probability. Our observation is motivated by the breakthrough of\nNicaud in 2014 providing a near-linear bound in a similar case, among multiple\nother results. Recent numerical analyses have conjectured that the expected\nreset threshold is closer to $\\mathcal{O}(\\sqrt{n})$ but not even a sublinear\nbound was confirmed for any variant.",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs Gerencs\u00e9r",
      "Zsombor V\u00e1rkonyi"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2206.05043"
  },
  {
    "id": "arXiv:2206.05047",
    "title": "A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed  Noise Model and Weighted Regularization",
    "abstract": "This paper presents a GPU-accelerated computational framework for\nreconstructing high resolution (HR) LF images under a mixed Gaussian-Impulse\nnoise condition. The main focus is on developing a high-performance approach\nconsidering processing speed and reconstruction quality. From a statistical\nperspective, we derive a joint $\\ell^1$-$\\ell^2$ data fidelity term for\npenalizing the HR reconstruction error taking into account the mixed noise\nsituation. For regularization, we employ the weighted non-local total variation\napproach, which allows us to effectively realize LF image prior through a\nproper weighting scheme. We show that the alternating direction method of\nmultipliers algorithm (ADMM) can be used to simplify the computation complexity\nand results in a high-performance parallel computation on the GPU Platform. An\nextensive experiment is conducted on both synthetic 4D LF dataset and natural\nimage dataset to validate the proposed SR model's robustness and evaluate the\naccelerated optimizer's performance. The experimental results show that our\napproach achieves better reconstruction quality under severe mixed-noise\nconditions as compared to the state-of-the-art approaches. In addition, the\nproposed approach overcomes the limitation of the previous work in handling\nlarge-scale SR tasks. While fitting within a single off-the-shelf GPU, the\nproposed accelerator provides an average speedup of 2.46$\\times$ and\n1.57$\\times$ for $\\times 2$ and $\\times 3$ SR tasks, respectively. In addition,\na speedup of $77\\times$ is achieved as compared to CPU execution.",
    "descriptor": "",
    "authors": [
      "Trung-Hieu Tran",
      "Kaicong Sun",
      "Sven Simon"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2206.05047"
  },
  {
    "id": "arXiv:2206.05049",
    "title": "Denoising Generalized Expectation-Consistent Approximation for MRI Image  Recovery",
    "abstract": "To solve inverse problems, plug-and-play (PnP) methods have been developed\nthat replace the proximal step in a convex optimization algorithm with a call\nto an application-specific denoiser, often implemented using a deep neural\nnetwork (DNN). Although such methods have been successful, they can be\nimproved. For example, denoisers are usually designed/trained to remove white\nGaussian noise, but the denoiser input error in PnP algorithms is usually far\nfrom white or Gaussian. Approximate message passing (AMP) methods provide white\nand Gaussian denoiser input error, but only when the forward operator is a\nlarge random matrix. In this work, for Fourier-based forward operators, we\npropose a PnP algorithm based on generalized expectation-consistent (GEC)\napproximation -- a close cousin of AMP -- that offers predictable error\nstatistics at each iteration, as well as a new DNN denoiser that leverages\nthose statistics. We apply our approach to magnetic resonance imaging (MRI)\nimage recovery and demonstrate its advantages over existing PnP and AMP\nmethods.",
    "descriptor": "",
    "authors": [
      "Saurav K. Shastri",
      "Rizwan Ahmad",
      "Christopher A. Metzler",
      "Philip Schniter"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.05049"
  },
  {
    "id": "arXiv:2206.05054",
    "title": "A No-reference Quality Assessment Metric for Point Cloud Based on  Captured Video Sequences",
    "abstract": "Point cloud is one of the most widely used digital formats of 3D models, the\nvisual quality of which is quite sensitive to distortions such as downsampling,\nnoise, and compression. To tackle the challenge of point cloud quality\nassessment (PCQA) in scenarios where reference is not available, we propose a\nno-reference quality assessment metric for colored point cloud based on\ncaptured video sequences. Specifically, three video sequences are obtained by\nrotating the camera around the point cloud through three specific orbits. The\nvideo sequences not only contain the static views but also include the\nmulti-frame temporal information, which greatly helps understand the human\nperception of the point clouds. Then we modify the ResNet3D as the feature\nextraction model to learn the correlation between the capture videos and\ncorresponding subjective quality scores. The experimental results show that our\nmethod outperforms most of the state-of-the-art full-reference and no-reference\nPCQA metrics, which validates the effectiveness of the proposed method.",
    "descriptor": "",
    "authors": [
      "Yu Fan",
      "Zicheng Zhang",
      "Wei Sun",
      "Xiongkuo Min",
      "Wei Lu",
      "Tao Wang",
      "Ning Liu",
      "Guangtao Zhai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05054"
  },
  {
    "id": "arXiv:2206.05081",
    "title": "The Evolution Of Centralisation on Cryptocurrency Platforms",
    "abstract": "More than ten years ago the blockchain was acclaimed as the solution to\novercome centralised trusted third parties for online payments. Through the\nyears the crypto-movement changed and evolved, although decentralisation\nremained the core ideology and the necessary feature every new crypto-project\nshould provide. In this paper we study the concept of centralisation in\ncryptocurrencies using a wide array of methodologies from the complex systems\nliterature, on a comparative collection of blockchains, in order to define the\nmany different levels a blockchain system may display (de-)centralisation and\nto question whether the present state of cryptocurrencies is, in a\ntechnological and economical sense, actually decentralised.",
    "descriptor": "",
    "authors": [
      "Carlo Campajola",
      "Raffaele Cristodaro",
      "Francesco Maria De Collibus",
      "Tao Yan",
      "Nicolo' Vallarano",
      "Claudio J. Tessone"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Cryptography and Security (cs.CR)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2206.05081"
  },
  {
    "id": "arXiv:2206.05092",
    "title": "Learning self-calibrated optic disc and cup segmentation from  multi-rater annotations",
    "abstract": "The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an\nimportant fundamental task for glaucoma diagnosis. In the clinical practice, it\nis often necessary to collect opinions from multiple experts to obtain the\nfinal OD/OC annotation. This clinical routine helps to mitigate the individual\nbias. But when data is multiply annotated, standard deep learning models will\nbe inapplicable. In this paper, we propose a novel neural network framework to\nlearn OD/OC segmentation from multi-rater annotations. The segmentation results\nare self-calibrated through the iterative optimization of multi-rater\nexpertness estimation and calibrated OD/OC segmentation. In this way, the\nproposed method can realize a mutual improvement of both tasks and finally\nobtain a refined segmentation result. Specifically, we propose Diverging\nModel(DivM) and Converging Model(ConM) to process the two tasks respectively.\nConM segments the raw image based on the multi-rater expertness map provided by\nDivM. DivM generates multi-rater expertness map from the segmentation mask\nprovided by ConM. The experiment results show that by recurrently running ConM\nand DivM, the results can be self-calibrated so as to outperform a range of\nstate-of-the-art(SOTA) multi-rater segmentation methods.",
    "descriptor": "",
    "authors": [
      "Junde Wu",
      "Huihui Fang",
      "Fangxin Shang",
      "Zhaowei Wang",
      "Dalu Yang",
      "Wenshuo Zhou",
      "Yehui Yang",
      "Yanwu Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05092"
  },
  {
    "id": "arXiv:2206.05120",
    "title": "Active information, missing data and prevalence estimation",
    "abstract": "The topic of this paper is prevalence estimation from the perspective of\nactive information. Prevalence among tested individuals has an upward bias\nunder the assumption that individuals' willingness to be tested for the disease\nincreases with the strength of their symptoms. Active information due to\ntesting bias quantifies the degree at which the willingness to be tested\ncorrelates with infection status. Interpreting incomplete testing as a missing\ndata problem, the missingness mechanism impacts the degree at which the bias of\nthe original prevalence estimate can be removed. The reduction in prevalence,\nwhen testing bias is adjusted for, translates into an active information due to\nbias correction, with opposite sign to active information due to testing bias.\nPrevalence and active information estimates are asymptotically normal, a\nbehavior also illustrated through simulations.",
    "descriptor": "\nComments: 18 pages, 5 tables, 2 figures\n",
    "authors": [
      "Ola H\u00f6ssjer",
      "Daniel Andr\u00e9s D\u00edaz-Pach\u00f3n",
      "Chen Zhao",
      "J. Sunil Rao"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Populations and Evolution (q-bio.PE)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2206.05120"
  },
  {
    "id": "arXiv:2206.05124",
    "title": "Stochastic Zeroth order Descent with Structured Directions",
    "abstract": "We introduce and analyze Structured Stochastic Zeroth order Descent (S-SZD),\na finite difference approach which approximates a stochastic gradient on a set\nof $l\\leq d$ orthogonal directions, where $d$ is the dimension of the ambient\nspace. These directions are randomly chosen, and may change at each step. For\nsmooth convex functions we prove almost sure convergence of the iterates and a\nconvergence rate on the function values of the form $O(d/l k^{-c})$ for every\n$c<1/2$, which is arbitrarily close to the one of Stochastic Gradient Descent\n(SGD) in terms of number of iterations. Our bound also shows the benefits of\nusing $l$ multiple directions instead of one. For non-convex functions\nsatisfying the Polyak-{\\L}ojasiewicz condition, we establish the first\nconvergence rates for stochastic zeroth order algorithms under such an\nassumption. We corroborate our theoretical findings in numerical simulations\nwhere assumptions are satisfied and on the real-world problem of\nhyper-parameter optimization, observing that S-SZD has very good practical\nperformances.",
    "descriptor": "",
    "authors": [
      "Marco Rando",
      "Cesare Molinari",
      "Silvia Villa",
      "Lorenzo Rosasco"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05124"
  },
  {
    "id": "arXiv:2206.05134",
    "title": "Distributionally Robust End-to-End Portfolio Construction",
    "abstract": "We propose an end-to-end distributionally robust system for portfolio\nconstruction that integrates the asset return prediction model with a\ndistributionally robust portfolio optimization model. We also show how to learn\nthe risk-tolerance parameter and the degree of robustness directly from data.\nEnd-to-end systems have an advantage in that information can be communicated\nbetween the prediction and decision layers during training, allowing the\nparameters to be trained for the final task rather than solely for predictive\nperformance. However, existing end-to-end systems are not able to quantify and\ncorrect for the impact of model risk on the decision layer. Our proposed\ndistributionally robust end-to-end portfolio selection system explicitly\naccounts for the impact of model risk. The decision layer chooses portfolios by\nsolving a minimax problem where the distribution of the asset returns is\nassumed to belong to an ambiguity set centered around a nominal distribution.\nUsing convex duality, we recast the minimax problem in a form that allows for\nefficient training of the end-to-end system.",
    "descriptor": "",
    "authors": [
      "Giorgio Costa",
      "Garud N. Iyengar"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Portfolio Management (q-fin.PM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05134"
  },
  {
    "id": "arXiv:2206.05148",
    "title": "Weakly-supervised segmentation using inherently-explainable  classification models and their application to brain tumour classification",
    "abstract": "Deep learning models have shown their potential for several applications.\nHowever, most of the models are opaque and difficult to trust due to their\ncomplex reasoning - commonly known as the black-box problem. Some fields, such\nas medicine, require a high degree of transparency to accept and adopt such\ntechnologies. Consequently, creating explainable/interpretable models or\napplying post-hoc methods on classifiers to build trust in deep learning models\nare required. Moreover, deep learning methods can be used for segmentation\ntasks, which typically require hard-to-obtain, time-consuming\nmanually-annotated segmentation labels for training. This paper introduces\nthree inherently-explainable classifiers to tackle both of these problems as\none. The localisation heatmaps provided by the networks -- representing the\nmodels' focus areas and being used in classification decision-making -- can be\ndirectly interpreted, without requiring any post-hoc methods to derive\ninformation for model explanation. The models are trained by using the input\nimage and only the classification labels as ground-truth in a supervised\nfashion - without using any information about the location of the region of\ninterest (i.e. the segmentation labels), making the segmentation training of\nthe models weakly-supervised through classification labels. The final\nsegmentation is obtained by thresholding these heatmaps. The models were\nemployed for the task of multi-class brain tumour classification using two\ndifferent datasets, resulting in the best F1-score of 0.93 for the supervised\nclassification task while securing a median Dice score of 0.67$\\pm$0.08 for the\nweakly-supervised segmentation task. Furthermore, the obtained accuracy on a\nsubset of tumour-only images outperformed the state-of-the-art glioma tumour\ngrading binary classifiers with the best model achieving 98.7\\% accuracy.",
    "descriptor": "",
    "authors": [
      "Soumick Chatterjee",
      "Hadya Yassin",
      "Florian Dubost",
      "Andreas N\u00fcrnberger",
      "Oliver Speck"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05148"
  },
  {
    "id": "arXiv:2206.05173",
    "title": "How Much is Enough? A Study on Diffusion Times in Score-based Generative  Models",
    "abstract": "Score-based diffusion models are a class of generative models whose dynamics\nis described by stochastic differential equations that map noise into data.\nWhile recent works have started to lay down a theoretical foundation for these\nmodels, an analytical understanding of the role of the diffusion time T is\nstill lacking. Current best practice advocates for a large T to ensure that the\nforward dynamics brings the diffusion sufficiently close to a known and simple\nnoise distribution; however, a smaller value of T should be preferred for a\nbetter approximation of the score-matching objective and higher computational\nefficiency. Starting from a variational interpretation of diffusion models, in\nthis work we quantify this trade-off, and suggest a new method to improve\nquality and efficiency of both training and sampling, by adopting smaller\ndiffusion times. Indeed, we show how an auxiliary model can be used to bridge\nthe gap between the ideal and the simulated forward dynamics, followed by a\nstandard reverse diffusion process. Empirical results support our analysis; for\nimage data, our method is competitive w.r.t. the state-of-the-art, according to\nstandard sample quality metrics and log-likelihood.",
    "descriptor": "",
    "authors": [
      "Giulio Franzese",
      "Simone Rossi",
      "Lixuan Yang",
      "Alessandro Finamore",
      "Dario Rossi",
      "Maurizio Filippone",
      "Pietro Michiardi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05173"
  },
  {
    "id": "arXiv:2206.05187",
    "title": "On Convergence of FedProx: Local Dissimilarity Invariant Bounds,  Non-smoothness and Beyond",
    "abstract": "The FedProx algorithm is a simple yet powerful distributed proximal point\noptimization method widely used for federated learning (FL) over heterogeneous\ndata. Despite its popularity and remarkable success witnessed in practice, the\ntheoretical understanding of FedProx is largely underinvestigated: the\nappealing convergence behavior of FedProx is so far characterized under certain\nnon-standard and unrealistic dissimilarity assumptions of local functions, and\nthe results are limited to smooth optimization problems. In order to remedy\nthese deficiencies, we develop a novel local dissimilarity invariant\nconvergence theory for FedProx and its minibatch stochastic extension through\nthe lens of algorithmic stability. As a result, we contribute to derive several\nnew and deeper insights into FedProx for non-convex federated optimization\nincluding: 1) convergence guarantees independent on local dissimilarity type\nconditions; 2) convergence guarantees for non-smooth FL problems; and 3) linear\nspeedup with respect to size of minibatch and number of sampled devices. Our\ntheory for the first time reveals that local dissimilarity and smoothness are\nnot must-have for FedProx to get favorable complexity bounds. Preliminary\nexperimental results on a series of benchmark FL datasets are reported to\ndemonstrate the benefit of minibatching for improving the sample efficiency of\nFedProx.",
    "descriptor": "",
    "authors": [
      "Xiao-Tong Yuan",
      "Ping Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05187"
  },
  {
    "id": "arXiv:2206.05188",
    "title": "Splitted Levenberg-Marquardt Method for Large-Scale Sparse Problems",
    "abstract": "We consider large-scale nonlinear least squares problems with sparse\nresiduals, each one of them depending on a small number of variables. A\ndecoupling procedure which results in a splitting of the original problems into\na sequence of independent problems of smaller sizes is proposed and analysed.\nThe smaller size problems are modified in a way that offsets the error made by\ndisregarding dependencies that allow us to split the original problem. The\nresulting method is a modification of the Levenberg-Marquardt method with\nsmaller computational costs. Global convergence is proved as well as local\nlinear convergence under suitable assumptions on sparsity. The method is tested\non the network localization simulated problems with up to one million variables\nand its efficiency is demonstrated.",
    "descriptor": "",
    "authors": [
      "Natasa Krejic",
      "Greta Malaspina",
      "Lense Swaenen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.05188"
  },
  {
    "id": "arXiv:2206.05200",
    "title": "Dynamic mean field programming",
    "abstract": "A dynamic mean field theory is developed for model based Bayesian\nreinforcement learning in the large state space limit. In an analogy with the\nstatistical physics of disordered systems, the transition probabilities are\ninterpreted as couplings, and value functions as deterministic spins, and thus\nthe sampled transition probabilities are considered to be quenched random\nvariables. The results reveal that, under standard assumptions, the posterior\nover Q-values is asymptotically independent and Gaussian across state-action\npairs, for infinite horizon problems. The finite horizon case exhibits the same\nbehaviour for all state-actions pairs at each time but has an additional\ncorrelation across time, for each state-action pair. The results also hold for\npolicy evaluation. The Gaussian statistics can be computed from a set of\ncoupled mean field equations derived from the Bellman equation, which we call\ndynamic mean field programming (DMFP). For Q-value iteration, approximate\nequations are obtained by appealing to extreme value theory, and closed form\nexpressions are found in the independent and identically distributed case. The\nLyapunov stability of these closed form equations is studied.",
    "descriptor": "",
    "authors": [
      "George Stamatescu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05200"
  },
  {
    "id": "arXiv:2206.05236",
    "title": "Optical Diffraction Tomography based on 3D Physics-Inspired Neural  Network (PINN)",
    "abstract": "Optical diffraction tomography (ODT) is an emerging 3D imaging technique that\nis used for the 3D reconstruction of the refractive index (RI) for\nsemi-transparent samples. Various inverse models have been proposed to\nreconstruct the 3D RI based on the holographic detection of different samples\nsuch as the Born and the Rytov approximations. However, such approximations\nusually suffer from the so-called missing-cone problem that results in an\nelongation of the final reconstruction along the optical axis. Different\niterative schemes have been proposed to solve the missing cone problem relying\non physical forward models and an error function that aims at filling in the\nk-space and thus eliminating the missing-cone problem and reaching better\nreconstruction accuracy. In this paper, we propose a different approach where a\n3D neural network (NN) is employed. The NN is trained with a cost function\nderived from a physical model based on the physics of optical wave propagation.\nThe 3D NN starts with an initial guess for the 3D RI reconstruction (i.e. Born,\nor Rytov) and aims at reconstructing better 3D reconstruction based on an error\nfunction. With this technique, the NN can be trained without any examples of\nthe relation between the ill-posed reconstruction (Born or Rytov) and the\nground truth (true shape).",
    "descriptor": "",
    "authors": [
      "Ahmed B. Ayoub",
      "Amirhossein Saba",
      "Carlo Gigli",
      "Demetri Psaltis"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05236"
  },
  {
    "id": "arXiv:2206.05241",
    "title": "Credible equilibrium",
    "abstract": "Credible equilibrium is a solution concept that imposes a stronger\ncredibility notion than subgame perfect equilibrium. A credible equilibrium is\na refinement of subgame perfect equilibrium such that if a threat in a subgame\ng is \"credible,\" then it must also be credible in every subgame g' that is\n\"equivalent\" to g. I show that (i) a credible equilibrium exists in multi-stage\ngames, and (ii) if every stage game has a unique Nash equilibrium, then the\ncredible equilibrium is unique even in infinite horizon multi-stage games.\nMoreover, in perfect information games, credible equilibrium is equivalent to\nsubgame perfect equilibrium.",
    "descriptor": "",
    "authors": [
      "Mehmet S. Ismail"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.05241"
  },
  {
    "id": "arXiv:2206.05248",
    "title": "Accelerated Algorithms for Monotone Inclusions and Constrained  Nonconvex-Nonconcave Min-Max Optimization",
    "abstract": "We study monotone inclusions and monotone variational inequalities, as well\nas their generalizations to non-monotone settings. We first show that the Extra\nAnchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu [2021]\nfor unconstrained convex-concave min-max optimization, can be applied to solve\nthe more general problem of Lipschitz monotone inclusion. More specifically, we\nprove that the EAG solves Lipschitz monotone inclusion problems with an\n\\emph{accelerated convergence rate} of $O(\\frac{1}{T})$, which is \\emph{optimal\namong all first-order methods} [Diakonikolas, 2020, Yoon and Ryu, 2021]. Our\nsecond result is a new algorithm, called Extra Anchored Gradient Plus (EAG+),\nwhich not only achieves the accelerated $O(\\frac{1}{T})$ convergence rate for\nall monotone inclusion problems, but also exhibits the same accelerated rate\nfor a family of general (non-monotone) inclusion problems that concern negative\ncomonotone operators. As a special case of our second result, EAG+ enjoys the\n$O(\\frac{1}{T})$ convergence rate for solving a non-trivial class of\nnonconvex-nonconcave min-max optimization problems. Our analyses are based on\nsimple potential function arguments, which might be useful for analysing other\naccelerated algorithms.",
    "descriptor": "",
    "authors": [
      "Yang Cai",
      "Argyris Oikonomou",
      "Weiqiang Zheng"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05248"
  },
  {
    "id": "arXiv:2206.05265",
    "title": "Tight Bounds for State Tomography with Incoherent Measurements",
    "abstract": "We consider the classic question of state tomography: given copies of an\nunknown quantum state $\\rho\\in\\mathbb{C}^{d\\times d}$, output $\\widehat{\\rho}$\nfor which $\\|\\rho - \\widehat{\\rho}\\|_{\\mathsf{tr}} \\le \\varepsilon$. When one\nis allowed to make coherent measurements entangled across all copies,\n$\\Theta(d^2/\\varepsilon^2)$ copies are necessary and sufficient [Haah et al.\n'17, O'Donnell-Wright '16]. Unfortunately, the protocols achieving this rate\nincur large quantum memory overheads that preclude implementation on current or\nnear-term devices. On the other hand, the best known protocol using incoherent\n(single-copy) measurements uses $O(d^3/\\varepsilon^2)$ copies\n[Kueng-Rauhut-Terstiege '17], and multiple papers have posed it as an open\nquestion to understand whether or not this rate is tight. In this work, we\nfully resolve this question, by showing that any protocol using incoherent\nmeasurements, even if they are chosen adaptively, requires\n$\\Omega(d^3/\\varepsilon^2)$ copies, matching the upper bound of\n[Kueng-Rauhut-Terstiege '17].\nWe do so by a new proof technique which directly bounds the \"tilt\" of the\nposterior distribution after measurements, which yields a surprisingly short\nproof of our lower bound, and which we believe may be of independent interest.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Sitan Chen",
      "Brice Huang",
      "Jerry Li",
      "Allen Liu",
      "Mark Sellke"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05265"
  },
  {
    "id": "arXiv:1504.06859",
    "title": "When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization",
    "abstract": "Comments: Accepted for publication in Evolutionary Computation Journal, MIT Press",
    "descriptor": "\nComments: Accepted for publication in Evolutionary Computation Journal, MIT Press\n",
    "authors": [
      "Fernando G. Lobo",
      "Mosab Bazargani"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/1504.06859"
  },
  {
    "id": "arXiv:1809.02399",
    "title": "Sampling-based optimal kinodynamic planning with motion primitives",
    "abstract": "Sampling-based optimal kinodynamic planning with motion primitives",
    "descriptor": "",
    "authors": [
      "Basak Sakcak",
      "Luca Bascetta",
      "Gianni Ferretti",
      "Maria Prandini"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/1809.02399"
  },
  {
    "id": "arXiv:1812.10410",
    "title": "A multiple criteria methodology for priority based portfolio selection",
    "abstract": "A multiple criteria methodology for priority based portfolio selection",
    "descriptor": "",
    "authors": [
      "Maria Barbati",
      "Jos\u00e8 Rui Figueira",
      "Salvatore Greco",
      "Alessio Ishizaka",
      "Simona Panaro"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1812.10410"
  },
  {
    "id": "arXiv:1908.04902",
    "title": "Planar graphs without normally adjacent short cycles",
    "abstract": "Comments: 17 pages, 3 figures",
    "descriptor": "\nComments: 17 pages, 3 figures\n",
    "authors": [
      "Fangyao Lu",
      "Mengjiao Rao",
      "Qianqian Wang",
      "Tao Wang"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1908.04902"
  },
  {
    "id": "arXiv:1909.09598",
    "title": "Street Crossing Aid Using Light-weight CNNs for the Visually Impaired",
    "abstract": "Comments: 10 pages, 5 figures, 7 tables, ICCV 2019 - 7th International Workshop on Assistive Computer Vision and Robotics (ACVR 2019)",
    "descriptor": "\nComments: 10 pages, 5 figures, 7 tables, ICCV 2019 - 7th International Workshop on Assistive Computer Vision and Robotics (ACVR 2019)\n",
    "authors": [
      "Samuel Yu",
      "Heon Lee",
      "Jung Hoon Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1909.09598"
  },
  {
    "id": "arXiv:1910.06151",
    "title": "Sampling-based sublinear low-rank matrix arithmetic framework for  dequantizing quantum machine learning",
    "abstract": "Comments: 77 pages, 2 figures. revised structure, introduction rewritten for clarity",
    "descriptor": "\nComments: 77 pages, 2 figures. revised structure, introduction rewritten for clarity\n",
    "authors": [
      "Nai-Hui Chia",
      "Andr\u00e1s Gily\u00e9n",
      "Tongyang Li",
      "Han-Hsuan Lin",
      "Ewin Tang",
      "Chunhao Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/1910.06151"
  },
  {
    "id": "arXiv:1910.13742",
    "title": "Unifying mirror descent and dual averaging",
    "abstract": "Unifying mirror descent and dual averaging",
    "descriptor": "",
    "authors": [
      "Anatoli Juditsky",
      "Joon Kwon",
      "\u00c9ric Moulines"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1910.13742"
  },
  {
    "id": "arXiv:2004.04344",
    "title": "Revisiting 2-3 Red-Black Trees with a Pedagogically Sound yet Efficient  Deletion Algorithm: The Parity-Seeking Delete Algorithm",
    "abstract": "Revisiting 2-3 Red-Black Trees with a Pedagogically Sound yet Efficient  Deletion Algorithm: The Parity-Seeking Delete Algorithm",
    "descriptor": "",
    "authors": [
      "Kamaledin Ghiasi-Shirazi",
      "Taraneh Ghandi",
      "Ali Taghizadeh",
      "Ali Rahimi-Baigi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2004.04344"
  },
  {
    "id": "arXiv:2005.12458",
    "title": "Trainability of Dissipative Perceptron-Based Quantum Neural Networks",
    "abstract": "Comments: 5 + 21 pages, 3+2 figures, final version accepted for publication in Physical Review Letters",
    "descriptor": "\nComments: 5 + 21 pages, 3+2 figures, final version accepted for publication in Physical Review Letters\n",
    "authors": [
      "Kunal Sharma",
      "M. Cerezo",
      "Lukasz Cincio",
      "Patrick J. Coles"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2005.12458"
  },
  {
    "id": "arXiv:2005.14549",
    "title": "Improving Automated Driving through POMDP Planning with Human Internal  States",
    "abstract": "Comments: Preprint before submission to IEEE Transactions on Intelligent Transportation Systems. arXiv admin note: text overlap with arXiv:1702.00858",
    "descriptor": "\nComments: Preprint before submission to IEEE Transactions on Intelligent Transportation Systems. arXiv admin note: text overlap with arXiv:1702.00858\n",
    "authors": [
      "Zachary Sunberg",
      "Mykel Kochenderfer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2005.14549"
  },
  {
    "id": "arXiv:2006.03535",
    "title": "CoCon: A Self-Supervised Approach for Controlled Text Generation",
    "abstract": "Comments: ICLR 2021 Camera-Ready",
    "descriptor": "\nComments: ICLR 2021 Camera-Ready\n",
    "authors": [
      "Alvin Chan",
      "Yew-Soon Ong",
      "Bill Pung",
      "Aston Zhang",
      "Jie Fu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2006.03535"
  },
  {
    "id": "arXiv:2006.16516",
    "title": "Mixed Logit Models and Network Formation",
    "abstract": "Comments: revision; 33 pages",
    "descriptor": "\nComments: revision; 33 pages\n",
    "authors": [
      "Harsh Gupta",
      "Mason A. Porter"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Theoretical Economics (econ.TH)",
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.16516"
  },
  {
    "id": "arXiv:2007.06728",
    "title": "On the Parallel Tower of Hanoi Puzzle: Acyclicity and a Conditional  Triangle Inequality",
    "abstract": "Comments: To be presented at the SIAM Conference on Discrete Mathematics 2022; Updates/Edits to the statement of Theorem 3.2 included",
    "descriptor": "\nComments: To be presented at the SIAM Conference on Discrete Mathematics 2022; Updates/Edits to the statement of Theorem 3.2 included\n",
    "authors": [
      "Andrey Rukhin"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2007.06728"
  },
  {
    "id": "arXiv:2009.02031",
    "title": "Joint Resource Allocation to Minimize Execution Time of Federated  Learning in Cell-Free Massive MIMO",
    "abstract": "Comments: accepted to appear in IEEE Internet of Things Journal, Jun. 2022",
    "descriptor": "\nComments: accepted to appear in IEEE Internet of Things Journal, Jun. 2022\n",
    "authors": [
      "Tung T. Vu",
      "Duy T. Ngo",
      "Hien Quoc Ngo",
      "Minh N. Dao",
      "Nguyen H. Tran",
      "Richard H. Middleton"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.02031"
  },
  {
    "id": "arXiv:2009.13092",
    "title": "Learning Classifiers under Delayed Feedback with a Time Window  Assumption",
    "abstract": "Comments: accepted at KDD 2022",
    "descriptor": "\nComments: accepted at KDD 2022\n",
    "authors": [
      "Masahiro Kato",
      "Shota Yasui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.13092"
  },
  {
    "id": "arXiv:2010.03688",
    "title": "AxFormer: Accuracy-driven Approximation of Transformers for Faster,  Smaller and more Accurate NLP Models",
    "abstract": "Comments: International Joint Conference on Neural Networks (IJCNN) 2022",
    "descriptor": "\nComments: International Joint Conference on Neural Networks (IJCNN) 2022\n",
    "authors": [
      "Amrit Nagarajan",
      "Sanchari Sen",
      "Jacob R. Stevens",
      "Anand Raghunathan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.03688"
  },
  {
    "id": "arXiv:2010.14982",
    "title": "Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity  Detection",
    "abstract": "Comments: Toyota Smarthome Untrimmed dataset, project page: this https URL",
    "descriptor": "\nComments: Toyota Smarthome Untrimmed dataset, project page: this https URL\n",
    "authors": [
      "Rui Dai",
      "Srijan Das",
      "Saurav Sharma",
      "Luca Minciullo",
      "Lorenzo Garattoni",
      "Francois Bremond",
      "Gianpiero Francesca"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.14982"
  },
  {
    "id": "arXiv:2012.03436",
    "title": "Low-Rank Tensor Recovery with Euclidean-Norm-Induced Schatten-p  Quasi-Norm Regularization",
    "abstract": "Low-Rank Tensor Recovery with Euclidean-Norm-Induced Schatten-p  Quasi-Norm Regularization",
    "descriptor": "",
    "authors": [
      "Jicong Fan",
      "Lijun Ding",
      "Chengrun Yang",
      "Zhao Zhang",
      "Madeleine Udell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.03436"
  },
  {
    "id": "arXiv:2102.07433",
    "title": "Measuring the Internet during Covid-19 to Evaluate Work-from-Home",
    "abstract": "Measuring the Internet during Covid-19 to Evaluate Work-from-Home",
    "descriptor": "",
    "authors": [
      "Xiao Song",
      "John Heidemann"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2102.07433"
  },
  {
    "id": "arXiv:2102.08466",
    "title": "Robust Factorization of Real-world Tensor Streams with Patterns, Missing  Values, and Outliers",
    "abstract": "Comments: Published at ICDE 2021",
    "descriptor": "\nComments: Published at ICDE 2021\n",
    "authors": [
      "Dongjin Lee",
      "Kijung Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2102.08466"
  },
  {
    "id": "arXiv:2103.02729",
    "title": "Linear Bandit Algorithms with Sublinear Time Complexity",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Shuo Yang",
      "Tongzheng Ren",
      "Sanjay Shakkottai",
      "Eric Price",
      "Inderjit S. Dhillon",
      "Sujay Sanghavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02729"
  },
  {
    "id": "arXiv:2103.13338",
    "title": "Incremental Nonlinear Stability Analysis of Stochastic Systems Perturbed  by L\u00e9vy Noise",
    "abstract": "Comments: To be published. See this https URL for final version",
    "descriptor": "\nComments: To be published. See this https URL for final version\n",
    "authors": [
      "SooJean Han",
      "Soon-Jo Chung"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2103.13338"
  },
  {
    "id": "arXiv:2104.07060",
    "title": "Membership-Mappings for Data Representation Learning: Measure Theoretic  Conceptualization",
    "abstract": "Membership-Mappings for Data Representation Learning: Measure Theoretic  Conceptualization",
    "descriptor": "",
    "authors": [
      "Mohit Kumar",
      "Bernhard A. Moser",
      "Lukas Fischer",
      "Bernhard Freudenthaler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)"
    ],
    "url": "https://arxiv.org/abs/2104.07060"
  },
  {
    "id": "arXiv:2104.08101",
    "title": "Embedding Dependencies Between Wind Farms in Uncertainty-Aware Optimal  Power Flow",
    "abstract": "Embedding Dependencies Between Wind Farms in Uncertainty-Aware Optimal  Power Flow",
    "descriptor": "",
    "authors": [
      "Adriano Arrigo",
      "Jalal Kazempour",
      "Zacharie De Gr\u00e8ve",
      "Jean-Fran\u00e7ois Toubeau",
      "Fran\u00e7ois Vall\u00e9e"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.08101"
  },
  {
    "id": "arXiv:2104.10267",
    "title": "On reduction and normalization in the computational core",
    "abstract": "On reduction and normalization in the computational core",
    "descriptor": "",
    "authors": [
      "Claudia Faggian",
      "Giulio Guerrieri",
      "Ugo de'Liguoro",
      "Riccardo Treglia"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2104.10267"
  },
  {
    "id": "arXiv:2104.14811",
    "title": "Why scholars are diagramming neural network models",
    "abstract": "Comments: 16 pages, 4 figures",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Guy Clarke Marshall",
      "Caroline Jay",
      "Andre Freitas"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.14811"
  },
  {
    "id": "arXiv:2105.08925",
    "title": "Practical Lossless Federated Singular Vector Decomposition over  Billion-Scale Data",
    "abstract": "Comments: 10 pages",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Di Chai",
      "Leye Wang",
      "Junxue Zhang",
      "Liu Yang",
      "Shuowei Cai",
      "Kai Chen",
      "Qiang Yang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.08925"
  },
  {
    "id": "arXiv:2106.06057",
    "title": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
    "abstract": "Domain Transformer: Predicting Samples of Unseen, Future Domains",
    "descriptor": "",
    "authors": [
      "Johannes Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.06057"
  },
  {
    "id": "arXiv:2106.06682",
    "title": "Solving PDEs on Unknown Manifolds with Machine Learning",
    "abstract": "Solving PDEs on Unknown Manifolds with Machine Learning",
    "descriptor": "",
    "authors": [
      "Senwei Liang",
      "Shixiao W. Jiang",
      "John Harlim",
      "Haizhao Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06682"
  },
  {
    "id": "arXiv:2106.10540",
    "title": "Simultaneous Suspension Control and Energy Harvesting through Novel  Design and Control of a New Nonlinear Energy Harvesting Shock Absorber",
    "abstract": "Comments: Accepted for publication in IEEE Transactions on Vehicular Technology",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Mohammad R. Hajidavalloo",
      "Joel Cosner",
      "Zhaojian Li",
      "Wei-Che Tai",
      "Ziyou Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.10540"
  },
  {
    "id": "arXiv:2106.11180",
    "title": "Generalization Bounds with Minimal Dependency on Hypothesis Class via  Distributionally Robust Optimization",
    "abstract": "Generalization Bounds with Minimal Dependency on Hypothesis Class via  Distributionally Robust Optimization",
    "descriptor": "",
    "authors": [
      "Yibo Zeng",
      "Henry Lam"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.11180"
  },
  {
    "id": "arXiv:2106.13054",
    "title": "Weighted majority tournaments and Kemeny ranking with 2-dimensional  Euclidean preferences",
    "abstract": "Comments: Accepted at Discrete Applied Mathematics",
    "descriptor": "\nComments: Accepted at Discrete Applied Mathematics\n",
    "authors": [
      "Bruno Escoffier",
      "Olivier Spanjaard",
      "Magdal\u00e9na Tydrichov\u00e1"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.13054"
  },
  {
    "id": "arXiv:2107.09207",
    "title": "Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix  Manifold",
    "abstract": "Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix  Manifold",
    "descriptor": "",
    "authors": [
      "Thomas Y. Hou",
      "Zhenzhen Li",
      "Ziyun Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.09207"
  },
  {
    "id": "arXiv:2107.11445",
    "title": "Self-Correcting Neural Networks For Safe Classification",
    "abstract": "Self-Correcting Neural Networks For Safe Classification",
    "descriptor": "",
    "authors": [
      "Klas Leino",
      "Aymeric Fromherz",
      "Ravi Mangal",
      "Matt Fredrikson",
      "Bryan Parno",
      "Corina P\u0103s\u0103reanu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2107.11445"
  },
  {
    "id": "arXiv:2108.00632",
    "title": "Skeena: Efficient and Consistent Cross-Engine Transactions",
    "abstract": "Comments: To appear at SIGMOD 2022",
    "descriptor": "\nComments: To appear at SIGMOD 2022\n",
    "authors": [
      "Jianqiu Zhang",
      "Kaisong Huang",
      "Tianzheng Wang",
      "King Lv"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2108.00632"
  },
  {
    "id": "arXiv:2108.07140",
    "title": "MTG: A Benchmark Suite for Multilingual Text Generation",
    "abstract": "Comments: NAACL2022 findings",
    "descriptor": "\nComments: NAACL2022 findings\n",
    "authors": [
      "Yiran Chen",
      "Zhenqiao Song",
      "Xianze Wu",
      "Danqing Wang",
      "Jingjing Xu",
      "Jiaze Chen",
      "Hao Zhou",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.07140"
  },
  {
    "id": "arXiv:2108.07347",
    "title": "Issues with Positivity-Preserving Patankar-type Schemes",
    "abstract": "Issues with Positivity-Preserving Patankar-type Schemes",
    "descriptor": "",
    "authors": [
      "Davide Torlo",
      "Philipp \u00d6ffner",
      "Hendrik Ranocha"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.07347"
  },
  {
    "id": "arXiv:2108.07644",
    "title": "Wireless Federated Langevin Monte Carlo: Repurposing Channel Noise for  Bayesian Sampling and Privacy",
    "abstract": "Comments: submitted",
    "descriptor": "\nComments: submitted\n",
    "authors": [
      "Dongzhu Liu",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2108.07644"
  },
  {
    "id": "arXiv:2108.08908",
    "title": "Entropy-Preserving and Entropy-Stable Relaxation IMEX and Multirate  Time-Stepping Methods",
    "abstract": "Comments: 37 pages, 16 figures, 4 tables",
    "descriptor": "\nComments: 37 pages, 16 figures, 4 tables\n",
    "authors": [
      "Shinhoo Kang",
      "Emil M. Constantinescu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2108.08908"
  },
  {
    "id": "arXiv:2108.09875",
    "title": "Anarchic Federated Learning",
    "abstract": "Comments: Accepted for Long Presentation in ICML 2022",
    "descriptor": "\nComments: Accepted for Long Presentation in ICML 2022\n",
    "authors": [
      "Haibo Yang",
      "Xin Zhang",
      "Prashant Khanduri",
      "Jia Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2108.09875"
  },
  {
    "id": "arXiv:2109.04010",
    "title": "Popularity Adjusted Block Models are Generalized Random Dot Product  Graphs",
    "abstract": "Comments: 36 pages, 9 figures",
    "descriptor": "\nComments: 36 pages, 9 figures\n",
    "authors": [
      "John Koo",
      "Minh Tang",
      "Michael W. Trosset"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04010"
  },
  {
    "id": "arXiv:2109.04640",
    "title": "Projected State-action Balancing Weights for Offline Reinforcement  Learning",
    "abstract": "Projected State-action Balancing Weights for Offline Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Jiayi Wang",
      "Zhengling Qi",
      "Raymond K.W. Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2109.04640"
  },
  {
    "id": "arXiv:2109.07628",
    "title": "Connecting Low-Loss Subspace for Personalized Federated Learning",
    "abstract": "Comments: Appears at ACM SIGKDD 2022. Code available at this http URL",
    "descriptor": "\nComments: Appears at ACM SIGKDD 2022. Code available at this http URL\n",
    "authors": [
      "Seok-Ju Hahn",
      "Minwoo Jeong",
      "Junghye Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.07628"
  },
  {
    "id": "arXiv:2109.07670",
    "title": "Transaction Placement in Sharded Blockchains",
    "abstract": "Transaction Placement in Sharded Blockchains",
    "descriptor": "",
    "authors": [
      "Liuyang Ren",
      "Paul A. S. Ward"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2109.07670"
  },
  {
    "id": "arXiv:2109.10636",
    "title": "Weak-strong Uniqueness for Heat Conducting non-Newtonian Incompressible  Fluids",
    "abstract": "Weak-strong Uniqueness for Heat Conducting non-Newtonian Incompressible  Fluids",
    "descriptor": "",
    "authors": [
      "Pablo Alexei Gazca-Orozco",
      "Victoria Patel"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.10636"
  },
  {
    "id": "arXiv:2109.10957",
    "title": "Real Robot Challenge: A Robotics Competition in the Cloud",
    "abstract": "Real Robot Challenge: A Robotics Competition in the Cloud",
    "descriptor": "",
    "authors": [
      "Stefan Bauer",
      "Felix Widmaier",
      "Manuel W\u00fcthrich",
      "Annika Buchholz",
      "Sebastian Stark",
      "Anirudh Goyal",
      "Thomas Steinbrenner",
      "Joel Akpo",
      "Shruti Joshi",
      "Vincent Berenz",
      "Vaibhav Agrawal",
      "Niklas Funk",
      "Julen Urain De Jesus",
      "Jan Peters",
      "Joe Watson",
      "Claire Chen",
      "Krishnan Srinivasan",
      "Junwu Zhang",
      "Jeffrey Zhang",
      "Matthew R. Walter",
      "Rishabh Madan",
      "Charles Schaff",
      "Takahiro Maeda",
      "Takuma Yoneda",
      "Denis Yarats",
      "Arthur Allshire",
      "Ethan K. Gordon",
      "Tapomayukh Bhattacharjee",
      "Siddhartha S. Srinivasa",
      "Animesh Garg",
      "Harshit Sikchi",
      "Jilong Wang",
      "Qingfeng Yao",
      "Shuyu Yang",
      "Robert McCarthy",
      "Francisco Roldan Sanchez",
      "Qiang Wang",
      "David Cordova Bulens",
      "Kevin McGuinness",
      "Noel O'Connor",
      "Stephen J. Redmond",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2109.10957"
  },
  {
    "id": "arXiv:2109.12372",
    "title": "Know it to Defeat it: Exploring Health Rumor Characteristics and  Debunking Efforts on Chinese Social Media during COVID-19 Crisis",
    "abstract": "Comments: ICWSM 2022",
    "descriptor": "\nComments: ICWSM 2022\n",
    "authors": [
      "Wenjie Yang",
      "Sitong Wang",
      "Zhenhui Peng",
      "Chuhan Shi",
      "Xiaojuan Ma",
      "Diyi Yang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2109.12372"
  },
  {
    "id": "arXiv:2109.12430",
    "title": "Staggered mesh method for correlation energy calculations of solids:  Random phase approximation in direct ring coupled cluster doubles and  adiabatic connection formalisms",
    "abstract": "Staggered mesh method for correlation energy calculations of solids:  Random phase approximation in direct ring coupled cluster doubles and  adiabatic connection formalisms",
    "descriptor": "",
    "authors": [
      "Xin Xing",
      "Lin Lin"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.12430"
  },
  {
    "id": "arXiv:2110.03171",
    "title": "Assemblies of neurons learn to classify well-separated distributions",
    "abstract": "Comments: 33 pages, 7 figures, accepted to COLT 2022",
    "descriptor": "\nComments: 33 pages, 7 figures, accepted to COLT 2022\n",
    "authors": [
      "Max Dabagia",
      "Christos H. Papadimitriou",
      "Santosh S. Vempala"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03171"
  },
  {
    "id": "arXiv:2110.06150",
    "title": "Sparsity in Partially Controllable Linear Systems",
    "abstract": "Comments: ICML2022",
    "descriptor": "\nComments: ICML2022\n",
    "authors": [
      "Yonathan Efroni",
      "Sham Kakade",
      "Akshay Krishnamurthy",
      "Cyril Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06150"
  },
  {
    "id": "arXiv:2110.06915",
    "title": "Object-Region Video Transformers",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Roei Herzig",
      "Elad Ben-Avraham",
      "Karttikeya Mangalam",
      "Amir Bar",
      "Gal Chechik",
      "Anna Rohrbach",
      "Trevor Darrell",
      "Amir Globerson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06915"
  },
  {
    "id": "arXiv:2110.06977",
    "title": "Cloud-Assisted Collaborative Road Information Discovery with Gaussian  Process: Application to Road Profile Estimation",
    "abstract": "Comments: Submitted to IEEE Transactions on Intelligent Transportation Systems",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Mohammad R. Hajidavalloo",
      "Zhaojian Li",
      "Xin Xia",
      "Ali Louati",
      "Minghui Zheng",
      "Weichao Zhuang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.06977"
  },
  {
    "id": "arXiv:2110.11084",
    "title": "Grafting Transformer on Automatically Designed Convolutional Neural  Network for Hyperspectral Image Classification",
    "abstract": "Comments: 15 pages, 10 figures",
    "descriptor": "\nComments: 15 pages, 10 figures\n",
    "authors": [
      "Xizhe Xue",
      "Haokui Zhang",
      "Bei Fang",
      "Zongwen Bai",
      "Ying Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.11084"
  },
  {
    "id": "arXiv:2110.11584",
    "title": "Multiwave COVID-19 Prediction from Social Awareness using Web Search and  Mobility Data",
    "abstract": "Comments: 11 pages, 8 figures. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA",
    "descriptor": "\nComments: 11 pages, 8 figures. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA\n",
    "authors": [
      "J. Xue",
      "T. Yabe",
      "K. Tsubouchi",
      "J. Ma",
      "S. V. Ukkusuri"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.11584"
  },
  {
    "id": "arXiv:2110.12351",
    "title": "Integrated Conditional Estimation-Optimization",
    "abstract": "Integrated Conditional Estimation-Optimization",
    "descriptor": "",
    "authors": [
      "Paul Grigas",
      "Meng Qi",
      "Zuo-Jun",
      "Shen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.12351"
  },
  {
    "id": "arXiv:2110.13749",
    "title": "Topologically penalized regression on manifolds",
    "abstract": "Topologically penalized regression on manifolds",
    "descriptor": "",
    "authors": [
      "Olympio Hacquard",
      "Krishnakumar Balasubramanian",
      "Gilles Blanchard",
      "Cl\u00e9ment Levrard",
      "Wolfgang Polonik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.13749"
  },
  {
    "id": "arXiv:2110.14053",
    "title": "NeuroComb: Improving SAT Solving with Graph Neural Networks",
    "abstract": "NeuroComb: Improving SAT Solving with Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Wenxi Wang",
      "Yang Hu",
      "Mohit Tiwari",
      "Sarfraz Khurshid",
      "Kenneth McMillan",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14053"
  },
  {
    "id": "arXiv:2111.02673",
    "title": "Recurrent Neural Network Training with Convex Loss and Regularization  Functions by Extended Kalman Filtering",
    "abstract": "Comments: 22 pages, 4 figures, submitted for publication",
    "descriptor": "\nComments: 22 pages, 4 figures, submitted for publication\n",
    "authors": [
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2111.02673"
  },
  {
    "id": "arXiv:2111.02709",
    "title": "Analog MIMO Communication for One-shot Distributed Principal Component  Analysis",
    "abstract": "Analog MIMO Communication for One-shot Distributed Principal Component  Analysis",
    "descriptor": "",
    "authors": [
      "Xu Chen",
      "Erik G. Larsson",
      "Kaibin Huang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2111.02709"
  },
  {
    "id": "arXiv:2111.02790",
    "title": "LassoBench: A High-Dimensional Hyperparameter Optimization Benchmark  Suite for Lasso",
    "abstract": "Comments: 21 pages, 13 figures, Accepted as a conference paper at AutoML2022",
    "descriptor": "\nComments: 21 pages, 13 figures, Accepted as a conference paper at AutoML2022\n",
    "authors": [
      "Kenan \u0160ehi\u0107",
      "Alexandre Gramfort",
      "Joseph Salmon",
      "Luigi Nardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.02790"
  },
  {
    "id": "arXiv:2111.08534",
    "title": "Finite element based model order reduction for parametrized one-way  coupled steady state linear thermomechanical problems",
    "abstract": "Comments: 32 pages, 54 references",
    "descriptor": "\nComments: 32 pages, 54 references\n",
    "authors": [
      "Nirav Vasant Shah",
      "Michele Girfoglio",
      "Peregrina Quintela",
      "Gianluigi Rozza",
      "Alejandro Lengomin",
      "Francesco Ballarin",
      "Patricia Barral"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.08534"
  },
  {
    "id": "arXiv:2111.09191",
    "title": "Preference Communication in Multi-Objective Normal-Form Games",
    "abstract": "Preference Communication in Multi-Objective Normal-Form Games",
    "descriptor": "",
    "authors": [
      "Willem R\u00f6pke",
      "Diederik M. Roijers",
      "Ann Now\u00e9",
      "Roxana R\u0103dulescu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2111.09191"
  },
  {
    "id": "arXiv:2111.11191",
    "title": "Deep Learning Based Automated COVID-19 Classification from Computed  Tomography Images",
    "abstract": "Deep Learning Based Automated COVID-19 Classification from Computed  Tomography Images",
    "descriptor": "",
    "authors": [
      "Kenan Morani",
      "Devrim Unay"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.11191"
  },
  {
    "id": "arXiv:2111.11755",
    "title": "Guided-TTS: A Diffusion Model for Text-to-Speech via Classifier Guidance",
    "abstract": "Comments: 15 pages, 5 figures, ICML'2022",
    "descriptor": "\nComments: 15 pages, 5 figures, ICML'2022\n",
    "authors": [
      "Heeseung Kim",
      "Sungwon Kim",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2111.11755"
  },
  {
    "id": "arXiv:2111.13336",
    "title": "MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for  Efficient Object Detection",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Zhenhong Sun",
      "Ming Lin",
      "Xiuyu Sun",
      "Zhiyu Tan",
      "Hao Li",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.13336"
  },
  {
    "id": "arXiv:2111.13680",
    "title": "GMFlow: Learning Optical Flow via Global Matching",
    "abstract": "Comments: CVPR 2022, Oral",
    "descriptor": "\nComments: CVPR 2022, Oral\n",
    "authors": [
      "Haofei Xu",
      "Jing Zhang",
      "Jianfei Cai",
      "Hamid Rezatofighi",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.13680"
  },
  {
    "id": "arXiv:2111.15309",
    "title": "Deep Auto-encoder with Neural Response",
    "abstract": "Deep Auto-encoder with Neural Response",
    "descriptor": "",
    "authors": [
      "Xuming Ran",
      "Jie Zhang",
      "Ziyuan Ye",
      "Haiyan Wu",
      "Qi Xu",
      "Huihui Zhou",
      "Quanying Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.15309"
  },
  {
    "id": "arXiv:2112.00234",
    "title": "MC-Blur: A Comprehensive Benchmark for Image Deblurring",
    "abstract": "MC-Blur: A Comprehensive Benchmark for Image Deblurring",
    "descriptor": "",
    "authors": [
      "Kaihao Zhang",
      "Tao Wang",
      "Wenhan Luo",
      "Boheng Chen",
      "Wenqi Ren",
      "Bjorn Stenger",
      "Wei Liu",
      "Hongdong Li",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00234"
  },
  {
    "id": "arXiv:2112.00976",
    "title": "Gaussian Mixture Variational Autoencoder with Contrastive Learning for  Multi-Label Classification",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Junwen Bai",
      "Shufeng Kong",
      "Carla P. Gomes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.00976"
  },
  {
    "id": "arXiv:2112.01174",
    "title": "Multi-task Self-distillation for Graph-based Semi-Supervised Learning",
    "abstract": "Multi-task Self-distillation for Graph-based Semi-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Yating Ren",
      "Junzhong Ji",
      "Lingfeng Niu",
      "Minglong Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.01174"
  },
  {
    "id": "arXiv:2112.02845",
    "title": "Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence  Model Tackles All SMAC Tasks",
    "abstract": "Comments: 17 pages, 6 figures",
    "descriptor": "\nComments: 17 pages, 6 figures\n",
    "authors": [
      "Linghui Meng",
      "Muning Wen",
      "Yaodong Yang",
      "Chenyang Le",
      "Xiyun Li",
      "Weinan Zhang",
      "Ying Wen",
      "Haifeng Zhang",
      "Jun Wang",
      "Bo Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.02845"
  },
  {
    "id": "arXiv:2112.10374",
    "title": "CGIBNet: Bandwidth-constrained Communication with Graph Information  Bottleneck in Multi-Agent Reinforcement Learning",
    "abstract": "CGIBNet: Bandwidth-constrained Communication with Graph Information  Bottleneck in Multi-Agent Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Qi Tian",
      "Kun Kuang",
      "Baoxiang Wang",
      "Furui Liu",
      "Fei Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.10374"
  },
  {
    "id": "arXiv:2112.12033",
    "title": "Encoding protein dynamic information in graph representation for  functional residue identification",
    "abstract": "Encoding protein dynamic information in graph representation for  functional residue identification",
    "descriptor": "",
    "authors": [
      "Yuan Chiang",
      "Wei-Han Hui",
      "Shu-Wei Chang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)",
      "Atomic and Molecular Clusters (physics.atm-clus)",
      "Biological Physics (physics.bio-ph)"
    ],
    "url": "https://arxiv.org/abs/2112.12033"
  },
  {
    "id": "arXiv:2112.12435",
    "title": "A Divide-and-Conquer Approach to Dicke State Preparation",
    "abstract": "A Divide-and-Conquer Approach to Dicke State Preparation",
    "descriptor": "",
    "authors": [
      "Shamminuj Aktar",
      "Andreas B\u00e4rtschi",
      "Abdel-Hameed A. Badawy",
      "Stephan Eidenbenz"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2112.12435"
  },
  {
    "id": "arXiv:2201.00703",
    "title": "Stochastic Continuous Submodular Maximization: Boosting via  Non-oblivious Function",
    "abstract": "Comments: Accepted to ICML 2022. 29 pages, 5 figures, 2 tables",
    "descriptor": "\nComments: Accepted to ICML 2022. 29 pages, 5 figures, 2 tables\n",
    "authors": [
      "Qixin Zhang",
      "Zengde Deng",
      "Zaiyi Chen",
      "Haoyuan Hu",
      "Yu Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.00703"
  },
  {
    "id": "arXiv:2201.01838",
    "title": "Lumbar Bone Mineral Density Estimation from Chest X-ray Images:  Anatomy-aware Attentive Multi-ROI Modeling",
    "abstract": "Lumbar Bone Mineral Density Estimation from Chest X-ray Images:  Anatomy-aware Attentive Multi-ROI Modeling",
    "descriptor": "",
    "authors": [
      "Fakai Wang",
      "Kang Zheng",
      "Le Lu",
      "Jing Xiao",
      "Min Wu",
      "Chang-Fu Kuo",
      "Shun Miao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.01838"
  },
  {
    "id": "arXiv:2201.11500",
    "title": "Head and eye egocentric gesture recognition for human-robot interaction  using eyewear cameras",
    "abstract": "Comments: Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "descriptor": "\nComments: Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Javier Marina-Miranda",
      "V. Javier Traver"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.11500"
  },
  {
    "id": "arXiv:2201.12250",
    "title": "Gradient Descent on Neurons and its Link to Approximate Second-Order  Optimization",
    "abstract": "Comments: ICML 2022, camera ready",
    "descriptor": "\nComments: ICML 2022, camera ready\n",
    "authors": [
      "Frederik Benzing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12250"
  },
  {
    "id": "arXiv:2202.02363",
    "title": "Meta-Reinforcement Learning with Self-Modifying Networks",
    "abstract": "Meta-Reinforcement Learning with Self-Modifying Networks",
    "descriptor": "",
    "authors": [
      "Mathieu Chalvidal",
      "Thomas Serre",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.02363"
  },
  {
    "id": "arXiv:2202.02990",
    "title": "Comparison and Combination of Sentence Embeddings Derived from Different  Supervision Signals",
    "abstract": "Comments: Accepted at *SEM 2022",
    "descriptor": "\nComments: Accepted at *SEM 2022\n",
    "authors": [
      "Hayato Tsukagoshi",
      "Ryohei Sasano",
      "Koichi Takeda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.02990"
  },
  {
    "id": "arXiv:2202.04736",
    "title": "Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets",
    "abstract": "Comments: Accepted by ICML 2022",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Tianlong Chen",
      "Xuxi Chen",
      "Xiaolong Ma",
      "Yanzhi Wang",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.04736"
  },
  {
    "id": "arXiv:2202.05120",
    "title": "Low-Rank Approximation with $1/\u03b5^{1/3}$ Matrix-Vector Products",
    "abstract": "Comments: STOC '22",
    "descriptor": "\nComments: STOC '22\n",
    "authors": [
      "Ainesh Bakshi",
      "Kenneth L. Clarkson",
      "David P. Woodruff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.05120"
  },
  {
    "id": "arXiv:2202.06742",
    "title": "Trace norm regularization for multi-task learning with scarce data",
    "abstract": "Comments: COLT 2022",
    "descriptor": "\nComments: COLT 2022\n",
    "authors": [
      "Etienne Boursier",
      "Mikhail Konobeev",
      "Nicolas Flammarion"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06742"
  },
  {
    "id": "arXiv:2202.07028",
    "title": "One Step at a Time: Long-Horizon Vision-and-Language Navigation with  Milestones",
    "abstract": "Comments: 10 pages, 5 figures. Accepted to CVPR 2022",
    "descriptor": "\nComments: 10 pages, 5 figures. Accepted to CVPR 2022\n",
    "authors": [
      "Chan Hee Song",
      "Jihyung Kil",
      "Tai-Yu Pan",
      "Brian M. Sadler",
      "Wei-Lun Chao",
      "Yu Su"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.07028"
  },
  {
    "id": "arXiv:2202.08053",
    "title": "Image translation of Ultrasound to Pseudo Anatomical Display by CycleGAN",
    "abstract": "Comments: 9 pages, 5 figures",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Lilach Barkat",
      "Moti Freiman",
      "Haim Azhari"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.08053"
  },
  {
    "id": "arXiv:2202.09096",
    "title": "A Free Lunch with Influence Functions? Improving Neural Network  Estimates with Concepts from Semiparametric Statistics",
    "abstract": "A Free Lunch with Influence Functions? Improving Neural Network  Estimates with Concepts from Semiparametric Statistics",
    "descriptor": "",
    "authors": [
      "Matthew J. Vowels",
      "Sina Akbari",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.09096"
  },
  {
    "id": "arXiv:2202.14036",
    "title": "How Well Do My Results Generalize Now? The External Validity of Online  Privacy and Security Surveys",
    "abstract": "How Well Do My Results Generalize Now? The External Validity of Online  Privacy and Security Surveys",
    "descriptor": "",
    "authors": [
      "Jenny Tang",
      "Eleanor Birrell",
      "Ada Lerner"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2202.14036"
  },
  {
    "id": "arXiv:2203.00026",
    "title": "Predicting the Thermal Sunyaev-Zel'dovich Field using Modular and  Equivariant Set-Based Neural Networks",
    "abstract": "Comments: 11 pages, 5 figures; condensed version accepted at the Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021) as \"Equivariant and Modular DeepSets with Applications in Cluster Cosmology\"; revised version accepted by MLST includes numerous clarifications and a new appendix comparing to CNN",
    "descriptor": "\nComments: 11 pages, 5 figures; condensed version accepted at the Fourth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2021) as \"Equivariant and Modular DeepSets with Applications in Cluster Cosmology\"; revised version accepted by MLST includes numerous clarifications and a new appendix comparing to CNN\n",
    "authors": [
      "Leander Thiele",
      "Miles Cranmer",
      "William Coulton",
      "Shirley Ho",
      "David N. Spergel"
    ],
    "subjectives": [
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.00026"
  },
  {
    "id": "arXiv:2203.02214",
    "title": "Plan Your Target and Learn Your Skills: Transferable State-Only  Imitation Learning via Decoupled Policy Optimization",
    "abstract": "Comments: 22 pages, 3 tables, 17 figures. Published at ICML 2022. Fix minor typos",
    "descriptor": "\nComments: 22 pages, 3 tables, 17 figures. Published at ICML 2022. Fix minor typos\n",
    "authors": [
      "Minghuan Liu",
      "Zhengbang Zhu",
      "Yuzheng Zhuang",
      "Weinan Zhang",
      "Jianye Hao",
      "Yong Yu",
      "Jun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.02214"
  },
  {
    "id": "arXiv:2203.08733",
    "title": "The Outliers Theorem Revisited",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1907.01018",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1907.01018\n",
    "authors": [
      "Samuel Epstein"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2203.08733"
  },
  {
    "id": "arXiv:2203.09127",
    "title": "ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its  Applications in Baidu Maps",
    "abstract": "Comments: Accepted by KDD 2022, camera-ready version",
    "descriptor": "\nComments: Accepted by KDD 2022, camera-ready version\n",
    "authors": [
      "Jizhou Huang",
      "Haifeng Wang",
      "Yibo Sun",
      "Yunsheng Shi",
      "Zhengjie Huang",
      "An Zhuo",
      "Shikun Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.09127"
  },
  {
    "id": "arXiv:2203.12344",
    "title": "How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs",
    "abstract": "Comments: Accepted to CVPR 2022",
    "descriptor": "\nComments: Accepted to CVPR 2022\n",
    "authors": [
      "Hazel Doughty",
      "Cees G. M. Snoek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.12344"
  },
  {
    "id": "arXiv:2203.16067",
    "title": "Decision-Focused Learning without Decision-Making: Learning Locally  Optimized Decision Losses",
    "abstract": "Comments: 9 pages, 2 figures, 3 tables",
    "descriptor": "\nComments: 9 pages, 2 figures, 3 tables\n",
    "authors": [
      "Sanket Shah",
      "Kai Wang",
      "Bryan Wilder",
      "Andrew Perrault",
      "Milind Tambe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.16067"
  },
  {
    "id": "arXiv:2203.16094",
    "title": "Computing critical points for algebraic systems defined by  hyperoctahedral invariant polynomials",
    "abstract": "Computing critical points for algebraic systems defined by  hyperoctahedral invariant polynomials",
    "descriptor": "",
    "authors": [
      "Thi Xuan Vu"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2203.16094"
  },
  {
    "id": "arXiv:2203.16527",
    "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
    "abstract": "Comments: Tech report. arXiv v2: add RetinaNet results",
    "descriptor": "\nComments: Tech report. arXiv v2: add RetinaNet results\n",
    "authors": [
      "Yanghao Li",
      "Hanzi Mao",
      "Ross Girshick",
      "Kaiming He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.16527"
  },
  {
    "id": "arXiv:2204.00943",
    "title": "Efficient Convolutional Neural Networks on Raspberry Pi for Image  Classification",
    "abstract": "Comments: 12 pages, 2 figures",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "Rui-Yang Ju",
      "Ting-Yu Lin",
      "Jia-Hao Jian",
      "Jen-Shiun Chiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.00943"
  },
  {
    "id": "arXiv:2204.01773",
    "title": "Contracts with Information Acquisition, via Scoring Rules",
    "abstract": "Contracts with Information Acquisition, via Scoring Rules",
    "descriptor": "",
    "authors": [
      "Maneesha Papireddygari",
      "Bo Waggoner"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2204.01773"
  },
  {
    "id": "arXiv:2204.04452",
    "title": "Refined Convergence and Topology Learning for Decentralized Optimization  with Heterogeneous Data",
    "abstract": "Refined Convergence and Topology Learning for Decentralized Optimization  with Heterogeneous Data",
    "descriptor": "",
    "authors": [
      "B. Le Bars",
      "A. Bellet",
      "M. Tommasi",
      "E. Lavoie",
      "AM. Kermarrec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.04452"
  },
  {
    "id": "arXiv:2204.10380",
    "title": "The 6th AI City Challenge",
    "abstract": "Comments: Summary of the 6th AI City Challenge Workshop in conjunction with CVPR 2022. arXiv admin note: text overlap with arXiv:2104.12233",
    "descriptor": "\nComments: Summary of the 6th AI City Challenge Workshop in conjunction with CVPR 2022. arXiv admin note: text overlap with arXiv:2104.12233\n",
    "authors": [
      "Milind Naphade",
      "Shuo Wang",
      "David C. Anastasiu",
      "Zheng Tang",
      "Ming-Ching Chang",
      "Yue Yao",
      "Liang Zheng",
      "Mohammed Shaiqur Rahman",
      "Archana Venkatachalapathy",
      "Anuj Sharma",
      "Qi Feng",
      "Vitaly Ablavsky",
      "Stan Sclaroff",
      "Pranamesh Chakraborty",
      "Alice Li",
      "Shangru Li",
      "Rama Chellappa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.10380"
  },
  {
    "id": "arXiv:2205.00162",
    "title": "A Novel Work-Efficient APSP Algorithm for GPUs",
    "abstract": "Comments: The next version of the manuscript will be complete",
    "descriptor": "\nComments: The next version of the manuscript will be complete\n",
    "authors": [
      "Yelai Feng",
      "Huaixi Wang",
      "Hongyi Lu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Complexity (cs.CC)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.00162"
  },
  {
    "id": "arXiv:2205.02112",
    "title": "Combining Reciprocity and CSI Feedback in MIMO Systems",
    "abstract": "Comments: 35 pages, 7 figures. Extended version of a paper accepted for publication in IEEE Transactions on Wireless Communications. An additional appendix with a proof that due to space limitations is not included in the published manuscript is added",
    "descriptor": "\nComments: 35 pages, 7 figures. Extended version of a paper accepted for publication in IEEE Transactions on Wireless Communications. An additional appendix with a proof that due to space limitations is not included in the published manuscript is added\n",
    "authors": [
      "Ema Becirovic",
      "Emil Bj\u00f6rnson",
      "Erik G. Larsson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.02112"
  },
  {
    "id": "arXiv:2205.02168",
    "title": "Separations in Proof Complexity and TFNP",
    "abstract": "Separations in Proof Complexity and TFNP",
    "descriptor": "",
    "authors": [
      "Mika G\u00f6\u00f6s",
      "Alexandros Hollender",
      "Siddhartha Jain",
      "Gilbert Maystre",
      "William Pires",
      "Robert Robere",
      "Ran Tao"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.02168"
  },
  {
    "id": "arXiv:2205.02880",
    "title": "CompactIE: Compact Facts in Open Information Extraction",
    "abstract": "Comments: NAACL 2022 main conference (Long paper)",
    "descriptor": "\nComments: NAACL 2022 main conference (Long paper)\n",
    "authors": [
      "Farima Fatahi Bayat",
      "Nikita Bhutani",
      "H.V. Jagadish"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.02880"
  },
  {
    "id": "arXiv:2205.04522",
    "title": "Assessing Confidence with Assurance 2.0",
    "abstract": "Assessing Confidence with Assurance 2.0",
    "descriptor": "",
    "authors": [
      "John Rushby",
      "Robin Bloomfield"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.04522"
  },
  {
    "id": "arXiv:2205.05476",
    "title": "Contrastive Supervised Distillation for Continual Representation  Learning",
    "abstract": "Comments: Paper published as Oral and awarded as Best Student Paper at ICIAP21",
    "descriptor": "\nComments: Paper published as Oral and awarded as Best Student Paper at ICIAP21\n",
    "authors": [
      "Tommaso Barletti",
      "Niccolo' Biondi",
      "Federico Pernici",
      "Matteo Bruni",
      "Alberto Del Bimbo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05476"
  },
  {
    "id": "arXiv:2205.10608",
    "title": "SERVFAIL: The Unintended Consequences of Algorithm Agility in DNSSEC",
    "abstract": "Comments: Withdrawn on request of one of the persons listed as authors",
    "descriptor": "\nComments: Withdrawn on request of one of the persons listed as authors\n",
    "authors": [
      "Elias Heftrig",
      "Jean-Pierre Seifert",
      "Haya Shulman",
      "Peter Thomassen",
      "Michael Waidner",
      "Nils Wisiol"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.10608"
  },
  {
    "id": "arXiv:2205.11508",
    "title": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "abstract": "Contrastive and Non-Contrastive Self-Supervised Learning Recover Global  and Local Spectral Embedding Methods",
    "descriptor": "",
    "authors": [
      "Randall Balestriero",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11508"
  },
  {
    "id": "arXiv:2205.11585",
    "title": "An intelligent controller for underactuated mechanical systems",
    "abstract": "Comments: References added. This is a slightly expanded version of the work originally presented at DINAME 2017 - 17th International Symposium on Dynamic Problems of Mechanics, 2017, S\\~ao Sebasti\\~ao, Brazil",
    "descriptor": "\nComments: References added. This is a slightly expanded version of the work originally presented at DINAME 2017 - 17th International Symposium on Dynamic Problems of Mechanics, 2017, S\\~ao Sebasti\\~ao, Brazil\n",
    "authors": [
      "Josiane Maria de Macedo Fernande",
      "Marcelo Costa Tanaka",
      "Wallace Moreira Bessa",
      "Edwin Kreuzer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11585"
  },
  {
    "id": "arXiv:2205.11664",
    "title": "Towards Model Generalization for Monocular 3D Object Detection",
    "abstract": "Comments: Some mistakes are raised up and we need to re-write the paper and re-order the paper structure",
    "descriptor": "\nComments: Some mistakes are raised up and we need to re-write the paper and re-order the paper structure\n",
    "authors": [
      "Zhenyu Li",
      "Zehui Chen",
      "Ang Li",
      "Liangji Fang",
      "Qinhong Jiang",
      "Xianming Liu",
      "Junjun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11664"
  },
  {
    "id": "arXiv:2205.13220",
    "title": "DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph",
    "abstract": "Comments: 11 pages, 9 figures",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Baofeng Chang",
      "Sujia Zhu",
      "Qi Jiang",
      "Wang Xia",
      "Jingwei Tang",
      "Lvhan Pan",
      "Ronghua Liang",
      "Guodao Sun"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13220"
  },
  {
    "id": "arXiv:2205.13635",
    "title": "RIGID: Robust Linear Regression with Missing Data",
    "abstract": "RIGID: Robust Linear Regression with Missing Data",
    "descriptor": "",
    "authors": [
      "Alireza Aghasi",
      "MohammadJavad Feizollahi",
      "Saeed Ghadimi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.13635"
  },
  {
    "id": "arXiv:2205.14871",
    "title": "Illumination Adaptive Transformer",
    "abstract": "Comments: 19 pages, 7 figures",
    "descriptor": "\nComments: 19 pages, 7 figures\n",
    "authors": [
      "Ziteng Cui",
      "Kunchang Li",
      "Lin Gu",
      "Shenghan Su",
      "Peng Gao",
      "Zhengkai Jiang",
      "Yu Qiao",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14871"
  },
  {
    "id": "arXiv:2205.14999",
    "title": "CompleteDT: Point Cloud Completion with Dense Augment Inference  Transformers",
    "abstract": "Comments: Submitted to International Journal of Computer Vision (IJCV)",
    "descriptor": "\nComments: Submitted to International Journal of Computer Vision (IJCV)\n",
    "authors": [
      "Jun Li",
      "Shangwei Guo",
      "Shaokun Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14999"
  },
  {
    "id": "arXiv:2205.15952",
    "title": "Knowledge Graph - Deep Learning: A Case Study in Question Answering in  Aviation Safety Domain",
    "abstract": "Comments: LREC 2022 Main Conference Accepted Paper",
    "descriptor": "\nComments: LREC 2022 Main Conference Accepted Paper\n",
    "authors": [
      "Ankush Agarwal",
      "Raj Gite",
      "Shreya Laddha",
      "Pushpak Bhattacharyya",
      "Satyanarayan Kar",
      "Asif Ekbal",
      "Prabhjit Thind",
      "Rajesh Zele",
      "Ravi Shankar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15952"
  },
  {
    "id": "arXiv:2206.00858",
    "title": "Bayesian Inference of Stochastic Dynamical Networks",
    "abstract": "Comments: 12 pages, 2 figures, and 7 tables",
    "descriptor": "\nComments: 12 pages, 2 figures, and 7 tables\n",
    "authors": [
      "Yasen Wang",
      "Junyang Jin",
      "Jorge Goncalves"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00858"
  },
  {
    "id": "arXiv:2206.00897",
    "title": "xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture  Radar Imagery",
    "abstract": "Comments: 9 pages (21 with references and supplement). In review",
    "descriptor": "\nComments: 9 pages (21 with references and supplement). In review\n",
    "authors": [
      "Fernando Paolo",
      "Tsu-ting Tim Lin",
      "Ritwik Gupta",
      "Bryce Goodman",
      "Nirav Patel",
      "Daniel Kuster",
      "David Kroodsma",
      "Jared Dunnmon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.00897"
  },
  {
    "id": "arXiv:2206.01121",
    "title": "The Loop of the Rings: A Distributed Cooperative System",
    "abstract": "The Loop of the Rings: A Distributed Cooperative System",
    "descriptor": "",
    "authors": [
      "Arash Vaezi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.01121"
  },
  {
    "id": "arXiv:2206.01256",
    "title": "PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images",
    "abstract": "Comments: Tech Report. Code is available at \\url{this https URL}",
    "descriptor": "\nComments: Tech Report. Code is available at \\url{this https URL}\n",
    "authors": [
      "Yingfei Liu",
      "Junjie Yan",
      "Fan Jia",
      "Shuailin Li",
      "Qi Gao",
      "Tiancai Wang",
      "Xiangyu Zhang",
      "Jian Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01256"
  },
  {
    "id": "arXiv:2206.01288",
    "title": "Decentralized Training of Foundation Models in Heterogeneous  Environments",
    "abstract": "Decentralized Training of Foundation Models in Heterogeneous  Environments",
    "descriptor": "",
    "authors": [
      "Binhang Yuan",
      "Yongjun He",
      "Jared Quincy Davis",
      "Tianyi Zhang",
      "Tri Dao",
      "Beidi Chen",
      "Percy Liang",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01288"
  },
  {
    "id": "arXiv:2206.01299",
    "title": "Fine-tuning Language Models over Slow Networks using Activation  Compression with Guarantees",
    "abstract": "Fine-tuning Language Models over Slow Networks using Activation  Compression with Guarantees",
    "descriptor": "",
    "authors": [
      "Jue Wang",
      "Binhang Yuan",
      "Luka Rimanic",
      "Yongjun He",
      "Tri Dao",
      "Beidi Chen",
      "Christopher Re",
      "Ce Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.01299"
  },
  {
    "id": "arXiv:2206.02066",
    "title": "PIDNet: A Real-time Semantic Segmentation Network Inspired from PID  Controller",
    "abstract": "Comments: 11 pages, 10 figures",
    "descriptor": "\nComments: 11 pages, 10 figures\n",
    "authors": [
      "Jiacong Xu",
      "Zixiang Xiong",
      "Shankar P. Bhattacharyya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.02066"
  },
  {
    "id": "arXiv:2206.02144",
    "title": "Product safety idioms: a method for building causal Bayesian networks  for product safety and risk assessment",
    "abstract": "Product safety idioms: a method for building causal Bayesian networks  for product safety and risk assessment",
    "descriptor": "",
    "authors": [
      "Joshua Hunte",
      "Martin Neil",
      "Norman Fenton"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.02144"
  },
  {
    "id": "arXiv:2206.02290",
    "title": "A knowledge graph representation learning approach to predict novel  kinase-substrate interactions",
    "abstract": "A knowledge graph representation learning approach to predict novel  kinase-substrate interactions",
    "descriptor": "",
    "authors": [
      "Sachin Gavali",
      "Karen Ross",
      "Chuming Chen",
      "Julie Cowart",
      "Cathy H. Wu"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2206.02290"
  },
  {
    "id": "arXiv:2206.02435",
    "title": "Tackling covariate shift with node-based Bayesian neural networks",
    "abstract": "Comments: Published at ICML 2022 (long oral presentation). Code is available at this https URL",
    "descriptor": "\nComments: Published at ICML 2022 (long oral presentation). Code is available at this https URL\n",
    "authors": [
      "Trung Trinh",
      "Markus Heinonen",
      "Luigi Acerbi",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02435"
  },
  {
    "id": "arXiv:2206.02570",
    "title": "RODIAN: Robustified Median",
    "abstract": "Comments: Fixed a typo in the project grant number",
    "descriptor": "\nComments: Fixed a typo in the project grant number\n",
    "authors": [
      "Seong Hun Lee",
      "Javier Civera"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.02570"
  },
  {
    "id": "arXiv:2206.02652",
    "title": "Crust Macrofracturing as the Evidence of the Last Deglaciation",
    "abstract": "Crust Macrofracturing as the Evidence of the Last Deglaciation",
    "descriptor": "",
    "authors": [
      "Igor Aleshin",
      "Kirill Kholodkov",
      "Elena Kozlovskaya",
      "Ivan Malygin"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02652"
  },
  {
    "id": "arXiv:2206.02969",
    "title": "A Simple and Optimal Policy Design for Online Learning with Safety  against Heavy-tailed Risk",
    "abstract": "A Simple and Optimal Policy Design for Online Learning with Safety  against Heavy-tailed Risk",
    "descriptor": "",
    "authors": [
      "David Simchi-Levi",
      "Zeyu Zheng",
      "Feng Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.02969"
  },
  {
    "id": "arXiv:2206.03285",
    "title": "Nezha: Deployable and High-Performance Consensus Using Synchronized  Clocks",
    "abstract": "Comments: Under conference submission",
    "descriptor": "\nComments: Under conference submission\n",
    "authors": [
      "Jinkun Geng",
      "Anirudh Sivaraman",
      "Balaji Prabhakar",
      "Mendel Rosenblum"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Databases (cs.DB)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.03285"
  },
  {
    "id": "arXiv:2206.03426",
    "title": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage",
    "abstract": "Improving Fairness in Graph Neural Networks via Mitigating Sensitive  Attribute Leakage",
    "descriptor": "",
    "authors": [
      "Yu Wang",
      "Yuying Zhao",
      "Yushun Dong",
      "Huiyuan Chen",
      "Jundong Li",
      "Tyler Derr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.03426"
  },
  {
    "id": "arXiv:2206.03436",
    "title": "A Benchmark for Federated Hetero-Task Learning",
    "abstract": "A Benchmark for Federated Hetero-Task Learning",
    "descriptor": "",
    "authors": [
      "Liuyi Yao",
      "Dawei Gao",
      "Zhen Wang",
      "Yuexiang Xie",
      "Weirui Kuang",
      "Daoyuan Chen",
      "Haohui Wang",
      "Chenhe Dong",
      "Bolin Ding",
      "Yaliang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03436"
  },
  {
    "id": "arXiv:2206.03644",
    "title": "Neural Bandit with Arm Group Graph",
    "abstract": "Comments: Accepted to SIGKDD 2022",
    "descriptor": "\nComments: Accepted to SIGKDD 2022\n",
    "authors": [
      "Yunzhe Qi",
      "Yikun Ban",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.03644"
  },
  {
    "id": "arXiv:2206.03655",
    "title": "pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning",
    "abstract": "pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning",
    "descriptor": "",
    "authors": [
      "Daoyuan Chen",
      "Dawei Gao",
      "Weirui Kuang",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03655"
  },
  {
    "id": "arXiv:2206.03826",
    "title": "Towards Understanding Why Mask-Reconstruction Pretraining Helps in  Downstream Tasks",
    "abstract": "Towards Understanding Why Mask-Reconstruction Pretraining Helps in  Downstream Tasks",
    "descriptor": "",
    "authors": [
      "Jiachun Pan",
      "Pan Zhou",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.03826"
  },
  {
    "id": "arXiv:2206.03827",
    "title": "$p$-Sparsified Sketches for Fast Multiple Output Kernel Methods",
    "abstract": "$p$-Sparsified Sketches for Fast Multiple Output Kernel Methods",
    "descriptor": "",
    "authors": [
      "Tamim El Ahmad",
      "Pierre Laforgue",
      "Florence d'Alch\u00e9-Buc"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03827"
  },
  {
    "id": "arXiv:2206.03966",
    "title": "FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization",
    "abstract": "FedHPO-B: A Benchmark Suite for Federated Hyperparameter Optimization",
    "descriptor": "",
    "authors": [
      "Zhen Wang",
      "Weirui Kuang",
      "Ce Zhang",
      "Bolin Ding",
      "Yaliang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03966"
  },
  {
    "id": "arXiv:2206.03970",
    "title": "Narrowing the Coordinate-frame Gap in Behavior Prediction Models:  Distillation for Efficient and Accurate Scene-centric Motion Forecasting",
    "abstract": "Comments: Accepted at ICRA 2022",
    "descriptor": "\nComments: Accepted at ICRA 2022\n",
    "authors": [
      "DiJia Su",
      "Bertrand Douillard",
      "Rami Al-Rfou",
      "Cheolho Park",
      "Benjamin Sapp"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.03970"
  },
  {
    "id": "arXiv:2206.03996",
    "title": "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning",
    "abstract": "Comments: accepted to ICML 2022",
    "descriptor": "\nComments: accepted to ICML 2022\n",
    "authors": [
      "Momin Abbas",
      "Quan Xiao",
      "Lisha Chen",
      "Pin-Yu Chen",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03996"
  },
  {
    "id": "arXiv:2206.04029",
    "title": "Accelerating Score-based Generative Models for High-Resolution Image  Synthesis",
    "abstract": "Accelerating Score-based Generative Models for High-Resolution Image  Synthesis",
    "descriptor": "",
    "authors": [
      "Hengyuan Ma",
      "Li Zhang",
      "Xiatian Zhu",
      "Jingfeng Zhang",
      "Jianfeng Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04029"
  },
  {
    "id": "arXiv:2206.04170",
    "title": "CASS: Cross Architectural Self-Supervision for Medical Image Analysis",
    "abstract": "Comments: 15 pages, 4 figures",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Pranav Singh",
      "Elena Sizikova",
      "Jacopo Cirrone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.04170"
  },
  {
    "id": "arXiv:2206.04460",
    "title": "Open ERP System Data For Occupational Fraud Detection",
    "abstract": "Open ERP System Data For Occupational Fraud Detection",
    "descriptor": "",
    "authors": [
      "Julian Tritscher",
      "Fabian Gwinner",
      "Daniel Schl\u00f6r",
      "Anna Krause",
      "Andreas Hotho"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04460"
  },
  {
    "id": "arXiv:2206.04613",
    "title": "Explicit Regularization in Overparametrized Models via Noise Injection",
    "abstract": "Comments: 32 pages",
    "descriptor": "\nComments: 32 pages\n",
    "authors": [
      "Antonio Orvieto",
      "Anant Raj",
      "Hans Kersting",
      "Francis Bach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04613"
  },
  {
    "id": "arXiv:2206.04615",
    "title": "Beyond the Imitation Game: Quantifying and extrapolating the  capabilities of language models",
    "abstract": "Comments: 27 pages, 17 figures + references and appendices, repo: this https URL",
    "descriptor": "\nComments: 27 pages, 17 figures + references and appendices, repo: this https URL\n",
    "authors": [
      "Aarohi Srivastava",
      "Abhinav Rastogi",
      "Abhishek Rao",
      "Abu Awal Md Shoeb",
      "Abubakar Abid",
      "Adam Fisch",
      "Adam R. Brown",
      "Adam Santoro",
      "Aditya Gupta",
      "Adri\u00e0 Garriga-Alonso",
      "Agnieszka Kluska",
      "Aitor Lewkowycz",
      "Akshat Agarwal",
      "Alethea Power",
      "Alex Ray",
      "Alex Warstadt",
      "Alexander W. Kocurek",
      "Ali Safaya",
      "Ali Tazarv",
      "Alice Xiang",
      "Alicia Parrish",
      "Allen Nie",
      "Aman Hussain",
      "Amanda Askell",
      "Amanda Dsouza",
      "Ambrose Slone",
      "Ameet Rahane",
      "Anantharaman S. Iyer",
      "Anders Andreassen",
      "Andrea Madotto",
      "Andrea Santilli",
      "Andreas Stuhlm\u00fcller",
      "Andrew Dai",
      "Andrew La",
      "Andrew Lampinen",
      "Andy Zou",
      "Angela Jiang",
      "Angelica Chen",
      "Anh Vuong",
      "Animesh Gupta",
      "Anna Gottardi",
      "Antonio Norelli",
      "Anu Venkatesh",
      "Arash Gholamidavoodi",
      "Arfa Tabassum",
      "Arul Menezes",
      "Arun Kirubarajan",
      "Asher Mullokandov",
      "Ashish Sabharwal",
      "Austin Herrick",
      "Avia Efrat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04615"
  },
  {
    "id": "arXiv:2206.04646",
    "title": "Globally Optimal Algorithms for Fixed-Budget Best Arm Identification",
    "abstract": "Comments: fixed title typo",
    "descriptor": "\nComments: fixed title typo\n",
    "authors": [
      "Junpei Komiyama",
      "Taira Tsuchiya",
      "Junya Honda"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04646"
  },
  {
    "id": "arXiv:2206.04656",
    "title": "Simple Cues Lead to a Strong Multi-Object Tracker",
    "abstract": "Simple Cues Lead to a Strong Multi-Object Tracker",
    "descriptor": "",
    "authors": [
      "Jenny Seidenschwarz",
      "Guillem Bras\u00f3",
      "Ismail Elezi",
      "Laura Leal-Taix\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04656"
  }
]