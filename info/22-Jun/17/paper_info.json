[
  {
    "id": "arXiv:2206.07729",
    "title": "Taxonomy of Benchmarks in Graph Representation Learning",
    "abstract": "Graph Neural Networks (GNNs) extend the success of neural networks to\ngraph-structured data by accounting for their intrinsic geometry. While\nextensive research has been done on developing GNN models with superior\nperformance according to a collection of graph representation learning\nbenchmarks, it is currently not well understood what aspects of a given model\nare probed by them. For example, to what extent do they test the ability of a\nmodel to leverage graph structure vs. node features? Here, we develop a\nprincipled approach to taxonomize benchmarking datasets according to a\n$\\textit{sensitivity profile}$ that is based on how much GNN performance\nchanges due to a collection of graph perturbations. Our data-driven analysis\nprovides a deeper understanding of which benchmarking data characteristics are\nleveraged by GNNs. Consequently, our taxonomy can aid in selection and\ndevelopment of adequate graph benchmarks, and better informed evaluation of\nfuture GNN methods. Finally, our approach and implementation in\n$\\texttt{GTaxoGym}$ package are extendable to multiple graph prediction task\ntypes and future datasets.",
    "descriptor": "",
    "authors": [
      "Renming Liu",
      "Semih Cant\u00fcrk",
      "Frederik Wenkel",
      "Dylan Sandfelder",
      "Devin Kreuzer",
      "Anna Little",
      "Sarah McGuire",
      "Leslie O'Bray",
      "Michael Perlmutter",
      "Bastian Rieck",
      "Matthew Hirn",
      "Guy Wolf",
      "Ladislav Ramp\u00e1\u0161ek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07729"
  },
  {
    "id": "arXiv:2206.07736",
    "title": "Improving Diversity with Adversarially Learned Transformations for  Domain Generalization",
    "abstract": "To be successful in single source domain generalization, maximizing diversity\nof synthesized domains has emerged as one of the most effective strategies.\nMany of the recent successes have come from methods that pre-specify the types\nof diversity that a model is exposed to during training, so that it can\nultimately generalize well to new domains. However, na\\\"ive diversity based\naugmentations do not work effectively for domain generalization either because\nthey cannot model large domain shift, or because the span of transforms that\nare pre-specified do not cover the types of shift commonly occurring in domain\ngeneralization. To address this issue, we present a novel framework that uses\nadversarially learned transformations (ALT) using a neural network to model\nplausible, yet hard image transformations that fool the classifier. This\nnetwork is randomly initialized for each batch and trained for a fixed number\nof steps to maximize classification error. Further, we enforce consistency\nbetween the classifier's predictions on the clean and transformed images. With\nextensive empirical analysis, we find that this new form of adversarial\ntransformations achieve both objectives of diversity and hardness\nsimultaneously, outperforming all existing techniques on competitive benchmarks\nfor single source domain generalization. We also show that ALT can naturally\nwork with existing diversity modules to produce highly distinct, and large\ntransformations of the source domain leading to state-of-the-art performance.",
    "descriptor": "\nComments: Code for ALT is available at this https URL\n",
    "authors": [
      "Tejas Gokhale",
      "Rushil Anirudh",
      "Jayaraman J. Thiagarajan",
      "Bhavya Kailkhura",
      "Chitta Baral",
      "Yezhou Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07736"
  },
  {
    "id": "arXiv:2206.07737",
    "title": "Disparate Impact in Differential Privacy from Gradient Misalignment",
    "abstract": "As machine learning becomes more widespread throughout society, aspects\nincluding data privacy and fairness must be carefully considered, and are\ncrucial for deployment in highly regulated industries. Unfortunately, the\napplication of privacy enhancing technologies can worsen unfair tendencies in\nmodels. In particular, one of the most widely used techniques for private model\ntraining, differentially private stochastic gradient descent (DPSGD),\nfrequently intensifies disparate impact on groups within data. In this work we\nstudy the fine-grained causes of unfairness in DPSGD and identify gradient\nmisalignment due to inequitable gradient clipping as the most significant\nsource. This observation leads us to a new method for reducing unfairness by\npreventing gradient misalignment in DPSGD.",
    "descriptor": "\nComments: Accepted as a ICML workshop paper at TPDP 2022\n",
    "authors": [
      "Maria S. Esipova",
      "Atiyeh Ashari Ghomi",
      "Yaqiao Luo",
      "Jesse C. Cresswell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07737"
  },
  {
    "id": "arXiv:2206.07741",
    "title": "Edge Inference with Fully Differentiable Quantized Mixed Precision  Neural Networks",
    "abstract": "The large computing and memory cost of deep neural networks (DNNs) often\nprecludes their use in resource-constrained devices. Quantizing the parameters\nand operations to lower bit-precision offers substantial memory and energy\nsavings for neural network inference, facilitating the use of DNNs on edge\ncomputing platforms. Recent efforts at quantizing DNNs have employed a range of\ntechniques encompassing progressive quantization, step-size adaptation, and\ngradient scaling. This paper proposes a new quantization approach for mixed\nprecision convolutional neural networks (CNNs) targeting edge-computing. Our\nmethod establishes a new pareto frontier in model accuracy and memory footprint\ndemonstrating a range of quantized models, delivering best-in-class accuracy\nbelow 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions\nare: (i) hardware-aware heterogeneous differentiable quantization with\ntensor-sliced learned precision, (ii) targeted gradient modification for wgts.\nand acts. to mitigate quantization errors, and (iii) a multi-phase learning\nschedule to address instability in learning arising from updates to the learned\nquantizer and model parameters. We demonstrate the effectiveness of our\ntechniques on the ImageNet dataset across a range of models including\nEfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and\nMobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).",
    "descriptor": "",
    "authors": [
      "Clemens JS Schaefer",
      "Siddharth Joshi",
      "Shan Li",
      "Raul Blazquez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07741"
  },
  {
    "id": "arXiv:2206.07743",
    "title": "Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective",
    "abstract": "Recent years have witnessed remarkable success achieved by graph neural\nnetworks (GNNs) in many real-world applications such as recommendation and drug\ndiscovery. Despite the success, oversmoothing has been identified as one of the\nkey issues which limit the performance of deep GNNs. It indicates that the\nlearned node representations are highly indistinguishable due to the stacked\naggregators. In this paper, we propose a new perspective to look at the\nperformance degradation of deep GNNs, i.e., feature overcorrelation. Through\nempirical and theoretical study on this matter, we demonstrate the existence of\nfeature overcorrelation in deeper GNNs and reveal potential reasons leading to\nthis issue. To reduce the feature correlation, we propose a general framework\nDeCorr which can encourage GNNs to encode less redundant information. Extensive\nexperiments have demonstrated that DeCorr can help enable deeper GNNs and is\ncomplementary to existing techniques tackling the oversmoothing issue.",
    "descriptor": "\nComments: Accepted by KDD 2022\n",
    "authors": [
      "Wei Jin",
      "Xiaorui Liu",
      "Yao Ma",
      "Charu Aggarwal",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07743"
  },
  {
    "id": "arXiv:2206.07745",
    "title": "When to intervene? Prescriptive Process Monitoring Under Uncertainty and  Resource Constraints",
    "abstract": "Prescriptive process monitoring approaches leverage historical data to\nprescribe runtime interventions that will likely prevent negative case outcomes\nor improve a process's performance. A centerpiece of a prescriptive process\nmonitoring method is its intervention policy: a decision function determining\nif and when to trigger an intervention on an ongoing case. Previous proposals\nin this field rely on intervention policies that consider only the current\nstate of a given case. These approaches do not consider the tradeoff between\ntriggering an intervention in the current state, given the level of uncertainty\nof the underlying predictive models, versus delaying the intervention to a\nlater state. Moreover, they assume that a resource is always available to\nperform an intervention (infinite capacity). This paper addresses these gaps by\nintroducing a prescriptive process monitoring method that filters and ranks\nongoing cases based on prediction scores, prediction uncertainty, and causal\neffect of the intervention, and triggers interventions to maximize a gain\nfunction, considering the available resources. The proposal is evaluated using\na real-life event log. The results show that the proposed method outperforms\nexisting baselines regarding total gain.",
    "descriptor": "\nComments: BPM 2022 (20th International Conference on Business Process Management)\n",
    "authors": [
      "Mahmoud Shoush",
      "Marlon Dumas"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07745"
  },
  {
    "id": "arXiv:2206.07746",
    "title": "Condensing Graphs via One-Step Gradient Matching",
    "abstract": "As training deep learning models on large dataset takes a lot of time and\nresources, it is desired to construct a small synthetic dataset with which we\ncan train deep learning models sufficiently. There are recent works that have\nexplored solutions on condensing image datasets through complex bi-level\noptimization. For instance, dataset condensation (DC) matches network gradients\nw.r.t. large-real data and small-synthetic data, where the network weights are\noptimized for multiple steps at each outer iteration. However, existing\napproaches have their inherent limitations: (1) they are not directly\napplicable to graphs where the data is discrete; and (2) the condensation\nprocess is computationally expensive due to the involved nested optimization.\nTo bridge the gap, we investigate efficient dataset condensation tailored for\ngraph datasets where we model the discrete graph structure as a probabilistic\nmodel. We further propose a one-step gradient matching scheme, which performs\ngradient matching for only one single step without training the network\nweights. Our theoretical analysis shows this strategy can generate synthetic\ngraphs that lead to lower classification loss on real graphs. Extensive\nexperiments on various graph datasets demonstrate the effectiveness and\nefficiency of the proposed method. In particular, we are able to reduce the\ndataset size by 90% while approximating up to 98% of the original performance\nand our method is significantly faster than multi-step gradient matching (e.g.\n15x in CIFAR10 for synthesizing 500 graphs).",
    "descriptor": "\nComments: 11 pages, KDD2022\n",
    "authors": [
      "Wei Jin",
      "Xianfeng Tang",
      "Haoming Jiang",
      "Zheng Li",
      "Danqing Zhang",
      "Jiliang Tang",
      "Bin Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07746"
  },
  {
    "id": "arXiv:2206.07748",
    "title": "Immersion Metrics for Virtual Reality",
    "abstract": "Technological advances in recent years have promoted the development of\nvirtual reality systems that have a wide variety of hardware and software\ncharacteristics, providing varying degrees of immersion. Immersion is an\nobjective property of the virtual reality system that depends on both its\nhardware and software characteristics. Virtual reality systems are currently\nattempting to improve immersion as much as possible. However, there is no\nmetric to measure the level of immersion of a virtual reality system based on\nits characteristics. To date, the influence of these hardware and software\nvariables on immersion has only been considered individually or in small\ngroups. The way these system variables simultaneously affect immersion has not\nbeen analyzed either. In this paper, we propose immersion metrics for virtual\nreality systems based on their hardware and software variables, as well as the\ndevelopment process that led to their formulation. From the conducted\nexperiment and the obtained data, we followed a methodology to find immersion\nmodels based on the variables of the system. The immersion metrics presented in\nthis work offer a useful tool in the area of virtual reality and immersive\ntechnologies, not only to measure the immersion of any virtual reality system\nbut also to analyze the relationship and importance of the variables of these\nsystems.",
    "descriptor": "",
    "authors": [
      "Matias N. Selzer",
      "Silvia M. Castro"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.07748"
  },
  {
    "id": "arXiv:2206.07751",
    "title": "On the Identifiability of Nonlinear ICA: Sparsity and Beyond",
    "abstract": "Nonlinear independent component analysis (ICA) aims to recover the underlying\nindependent latent sources from their observable nonlinear mixtures. How to\nmake the nonlinear ICA model identifiable up to certain trivial indeterminacies\nis a long-standing problem in unsupervised learning. Recent breakthroughs\nreformulate the standard independence assumption of sources as conditional\nindependence given some auxiliary variables (e.g., class labels and/or\ndomain/time indexes) as weak supervision or inductive bias. However, nonlinear\nICA with unconditional priors cannot benefit from such developments. We explore\nan alternative path and consider only assumptions on the mixing process, such\nas Structural Sparsity or Independent Influences. We show that under specific\ninstantiations of such constraints, the independent latent sources can be\nidentified from their nonlinear mixtures up to a permutation and a\ncomponent-wise transformation, thus achieving nontrivial identifiability of\nnonlinear ICA without auxiliary variables. We provide estimation methods and\nvalidate the theoretical results experimentally. The results on image data\nsuggest that our conditions may hold in a number of practical data generating\nprocesses.",
    "descriptor": "",
    "authors": [
      "Yujia Zheng",
      "Ignavier Ng",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07751"
  },
  {
    "id": "arXiv:2206.07754",
    "title": "Novelty and Cultural Evolution in Modern Popular Music",
    "abstract": "The ubiquity of digital music consumption has made it possible to extract\ninformation about modern music that allows us to perform large scale analysis\nof stylistic change over time. In order to uncover underlying patterns in\ncultural evolution, we examine the relationship between the established\ncharacteristics of different genres and styles, and the introduction of novel\nideas that fuel this ongoing creative evolution. To understand how this dynamic\nplays out and shapes the cultural ecosystem, we compare musical artifacts to\ntheir contemporaries to identify novel artifacts, study the relationship\nbetween novelty and commercial success, and connect this to the changes in\nmusical content that we can observe over time. Using Music Information\nRetrieval (MIR) data and lyrics from Billboard Hot 100 songs between 1974-2013,\nwe calculate a novelty score for each song's aural attributes and lyrics.\nComparing both scores to the popularity of the song following its release, we\nuncover key patterns in the relationship between novelty and audience\nreception. Additionally, we look at the link between novelty and the likelihood\nthat a song was influential given where its MIR and lyrical features fit within\nthe larger trends we observed.",
    "descriptor": "",
    "authors": [
      "Katherine O'Toole",
      "Em\u0151ke-\u00c1gnes Horv\u00e1t"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.07754"
  },
  {
    "id": "arXiv:2206.07756",
    "title": "Hybrid full-field thermal characterization of additive manufacturing  processes using physics-informed neural networks with data",
    "abstract": "Understanding the thermal behavior of additive manufacturing (AM) processes\nis crucial for enhancing the quality control and enabling customized process\ndesign. Most purely physics-based computational models suffer from intensive\ncomputational costs, thus not suitable for online control and iterative design\napplication. Data-driven models taking advantage of the latest developed\ncomputational tools can serve as a more efficient surrogate, but they are\nusually trained over a large amount of simulation data and often fail to\neffectively use small but high-quality experimental data. In this work, we\ndeveloped a hybrid physics-based data-driven thermal modeling approach of AM\nprocesses using physics-informed neural networks. Specifically, partially\nobserved temperature data measured from an infrared camera is combined with the\nphysics laws to predict full-field temperature history and to discover unknown\nmaterial and process parameters. In the numerical and experimental examples,\nthe effectiveness of adding auxiliary training data and using the technique of\ntransfer learning on training efficiency and prediction accuracy, as well as\nthe ability to identify unknown parameters with partially observed data, are\ndemonstrated. The results show that the hybrid thermal model can effectively\nidentify unknown parameters and capture the full-field temperature accurately,\nand thus it has the potential to be used in iterative process design and\nreal-time process control of AM.",
    "descriptor": "",
    "authors": [
      "Shuheng Liao",
      "Tianju Xue",
      "Jihoon Jeong",
      "Samantha Webster",
      "Kornel Ehmann",
      "Jian Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07756"
  },
  {
    "id": "arXiv:2206.07758",
    "title": "Reconstructing Training Data from Trained Neural Networks",
    "abstract": "Understanding to what extent neural networks memorize training data is an\nintriguing question with practical and theoretical implications. In this paper\nwe show that in some cases a significant fraction of the training data can in\nfact be reconstructed from the parameters of a trained neural network\nclassifier. We propose a novel reconstruction scheme that stems from recent\ntheoretical results about the implicit bias in training neural networks with\ngradient-based methods. To the best of our knowledge, our results are the first\nto show that reconstructing a large portion of the actual training samples from\na trained neural network classifier is generally possible. This has negative\nimplications on privacy, as it can be used as an attack for revealing sensitive\ntraining data. We demonstrate our method for binary MLP classifiers on a few\nstandard computer vision datasets.",
    "descriptor": "",
    "authors": [
      "Niv Haim",
      "Gal Vardi",
      "Gilad Yehudai",
      "Ohad Shamir",
      "Michal Irani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07758"
  },
  {
    "id": "arXiv:2206.07762",
    "title": "Physics-Infused Fuzzy Generative Adversarial Network for Robust Failure  Prognosis",
    "abstract": "Prognostics aid in the longevity of fielded systems or products. Quantifying\nthe system's current health enable prognosis to enhance the operator's\ndecision-making to preserve the system's health. Creating a prognosis for a\nsystem can be difficult due to (a) unknown physical relationships and/or (b)\nirregularities in data appearing well beyond the initiation of a problem.\nTraditionally, three different modeling paradigms have been used to develop a\nprognostics model: physics-based (PbM), data-driven (DDM), and hybrid modeling.\nRecently, the hybrid modeling approach that combines the strength of both PbM\nand DDM based approaches and alleviates their limitations is gaining traction\nin the prognostics domain. In this paper, a novel hybrid modeling approach for\nprognostics applications based on combining concepts from fuzzy logic and\ngenerative adversarial networks (GANs) is outlined. The FuzzyGAN based method\nembeds a physics-based model in the aggregation of the fuzzy implications. This\ntechnique constrains the output of the learning method to a realistic solution.\nResults on a bearing problem showcases the efficacy of adding a physics-based\naggregation in a fuzzy logic model to improve GAN's ability to model health and\ngive a more accurate system prognosis.",
    "descriptor": "",
    "authors": [
      "Ryan Nguyen",
      "Shubhendu Kumar Singh",
      "Rahul Rai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07762"
  },
  {
    "id": "arXiv:2206.07764",
    "title": "SAVi++: Towards End-to-End Object-Centric Learning from Real-World  Videos",
    "abstract": "The visual world can be parsimoniously characterized in terms of distinct\nentities with sparse interactions. Discovering this compositional structure in\ndynamic visual scenes has proven challenging for end-to-end computer vision\napproaches unless explicit instance-level supervision is provided. Slot-based\nmodels leveraging motion cues have recently shown great promise in learning to\nrepresent, segment, and track objects without direct supervision, but they\nstill fail to scale to complex real-world multi-object videos. In an effort to\nbridge this gap, we take inspiration from human development and hypothesize\nthat information about scene geometry in the form of depth signals can\nfacilitate object-centric learning. We introduce SAVi++, an object-centric\nvideo model which is trained to predict depth signals from a slot-based video\nrepresentation. By further leveraging best practices for model scaling, we are\nable to train SAVi++ to segment complex dynamic scenes recorded with moving\ncameras, containing both static and moving objects of diverse appearance on\nnaturalistic backgrounds, without the need for segmentation supervision.\nFinally, we demonstrate that by using sparse depth signals obtained from LiDAR,\nSAVi++ is able to learn emergent object segmentation and tracking from videos\nin the real-world Waymo Open dataset.",
    "descriptor": "\nComments: Project page at this https URL\n",
    "authors": [
      "Gamaleldin F. Elsayed",
      "Aravindh Mahendran",
      "Sjoerd van Steenkiste",
      "Klaus Greff",
      "Michael C. Mozer",
      "Thomas Kipf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07764"
  },
  {
    "id": "arXiv:2206.07765",
    "title": "US News and Social Media Framing around Vaping",
    "abstract": "In this paper, we investigate how vaping is framed differently (2008-2021)\nbetween US news and social media. We analyze 15,711 news articles and 1,231,379\nFacebook posts about vaping to study the differences in framing between media\nvarieties. We use word embeddings to provide two-dimensional visualizations of\nthe semantic changes around vaping for news and for social media. We detail\nthat news media framing of vaping shifted over time in line with emergent\nregulatory trends, such as; flavored vaping bans, with little discussion around\nvaping as a smoking cessation tool. We found that social media discussions were\nfar more varied, with transitions toward vaping both as a public health harm\nand as a smoking cessation tool. Our cloze test, dynamic topic model, and\nquestion answering showed similar patterns, where social media, but not news\nmedia, characterizes vaping as combustible cigarette substitute. We use n-grams\nto detail that social media data first centered on vaping as a smoking\ncessation tool, and in 2019 moved toward narratives around vaping regulation,\nsimilar to news media frames. Overall, social media tracks the evolution of\nvaping as a social practice, while news media reflects more risk based\nconcerns. A strength of our work is how the different techniques we have\napplied validate each other. Stakeholders may utilize our findings to intervene\naround the framing of vaping, and may design communications campaigns that\nimprove the way society sees vaping, thus possibly aiding smoking cessation;\nand reducing youth vaping.",
    "descriptor": "",
    "authors": [
      "Keyu Chen",
      "Marzieh Babaeianjelodar",
      "Yiwen Shi",
      "Rohan Aanegola",
      "Lam Yin Cheung",
      "Preslav Ivanov Nakov",
      "Shweta Yadav",
      "Angus Bancroft",
      "Ashique Khudabukhsh",
      "Munmun De Choudhury",
      "Frederick L. Altice1",
      "Navin Kumar"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.07765"
  },
  {
    "id": "arXiv:2206.07766",
    "title": "Pareto Invariant Risk Minimization",
    "abstract": "Despite the success of invariant risk minimization (IRM) in tackling the\nOut-of-Distribution generalization problem, IRM can compromise the optimality\nwhen applied in practice. The practical variants of IRM, e.g., IRMv1, have been\nshown to have significant gaps with IRM and thus could fail to capture the\ninvariance even in simple problems. Moreover, the optimization procedure in\nIRMv1 involves two intrinsically conflicting objectives, and often requires\ncareful tuning for the objective weights. To remedy the above issues, we\nreformulate IRM as a multi-objective optimization problem, and propose a new\noptimization scheme for IRM, called PAreto Invariant Risk Minimization (PAIR).\nPAIR can adaptively adjust the optimization direction under the objective\nconflicts. Furthermore, we show PAIR can empower the practical IRM variants to\novercome the barriers with the original IRM when provided with proper guidance.\nWe conduct experiments with ColoredMNIST to confirm our theory and the\neffectiveness of PAIR.",
    "descriptor": "\nComments: A preprint version, 9 pages, 12 figures\n",
    "authors": [
      "Yongqiang Chen",
      "Kaiwen Zhou",
      "Yatao Bian",
      "Binghui Xie",
      "Kaili Ma",
      "Yonggang Zhang",
      "Han Yang",
      "Bo Han",
      "James Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07766"
  },
  {
    "id": "arXiv:2206.07767",
    "title": "Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?",
    "abstract": "Wasserstein Generative Adversarial Networks (WGANs) are the popular\ngenerative models built on the theory of Optimal Transport (OT) and the\nKantorovich duality. Despite the success of WGANs, it is still unclear how well\nthe underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance,\n$\\mathbb{W}_{1}$) and the OT gradient needed to update the generator. In this\npaper, we address these questions. We construct 1-Lipschitz functions and use\nthem to build ray monotone transport plans. This strategy yields pairs of\ncontinuous benchmark distributions with the analytically known OT plan, OT cost\nand OT gradient in high-dimensional spaces such as spaces of images. We\nthoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral\nnormalization, entropic regularization, etc.) using these benchmark pairs. Even\nthough these solvers perform well in WGANs, none of them faithfully compute\n$\\mathbb{W}_{1}$ in high dimensions. Nevertheless, many provide a meaningful\napproximation of the OT gradient. These observations suggest that these solvers\nshould not be treated as good estimators of $\\mathbb{W}_{1}$, but to some\nextent they indeed can be used in variational problems requiring the\nminimization of $\\mathbb{W}_{1}$.",
    "descriptor": "",
    "authors": [
      "Alexander Korotin",
      "Alexander Kolesov",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07767"
  },
  {
    "id": "arXiv:2206.07771",
    "title": "Discrete Contrastive Diffusion for Cross-Modal and Conditional  Generation",
    "abstract": "Diffusion probabilistic models (DPMs) have become a popular approach to\nconditional generation, due to their promising results and support for\ncross-modal synthesis. A key desideratum in conditional synthesis is to achieve\nhigh correspondence between the conditioning input and generated output. Most\nexisting methods learn such relationships implicitly, by incorporating the\nprior into the variational lower bound. In this work, we take a different route\n-- we enhance input-output connections by maximizing their mutual information\nusing contrastive learning. To this end, we introduce a Conditional Discrete\nContrastive Diffusion (CDCD) loss and design two contrastive diffusion\nmechanisms to effectively incorporate it into the denoising process. We\nformulate CDCD by connecting it with the conventional variational objectives.\nWe demonstrate the efficacy of our approach in evaluations with three diverse,\nmultimodal conditional synthesis tasks: dance-to-music generation,\ntext-to-image synthesis, and class-conditioned image synthesis. On each, we\nachieve state-of-the-art or higher synthesis quality and improve the\ninput-output correspondence. Furthermore, the proposed approach improves the\nconvergence of diffusion models, reducing the number of required diffusion\nsteps by more than 35% on two benchmarks, significantly increasing the\ninference speed.",
    "descriptor": "\nComments: Project at this https URL\n",
    "authors": [
      "Ye Zhu",
      "Yu Wu",
      "Kyle Olszewski",
      "Jian Ren",
      "Sergey Tulyakov",
      "Yan Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07771"
  },
  {
    "id": "arXiv:2206.07772",
    "title": "Deep Learning and Handheld Augmented Reality Based System for Optimal  Data Collection in Fault Diagnostics Domain",
    "abstract": "Compared to current AI or robotic systems, humans navigate their environment\nwith ease, making tasks such as data collection trivial. However, humans find\nit harder to model complex relationships hidden in the data. AI systems,\nespecially deep learning (DL) algorithms, impressively capture those complex\nrelationships. Symbiotically coupling humans and computational machines'\nstrengths can simultaneously minimize the collected data required and build\ncomplex input-to-output mapping models. This paper enables this coupling by\npresenting a novel human-machine interaction framework to perform fault\ndiagnostics with minimal data. Collecting data for diagnosing faults for\ncomplex systems is difficult and time-consuming. Minimizing the required data\nwill increase the practicability of data-driven models in diagnosing faults.\nThe framework provides instructions to a human user to collect data that\nmitigates the difference between the data used to train and test the fault\ndiagnostics model. The framework is composed of three components: (1) a\nreinforcement learning algorithm for data collection to develop a training\ndataset, (2) a deep learning algorithm for diagnosing faults, and (3) a\nhandheld augmented reality application for data collection for testing data.\nThe proposed framework has provided above 100\\% precision and recall on a novel\ndataset with only one instance of each fault condition. Additionally, a\nusability study was conducted to gauge the user experience of the handheld\naugmented reality application, and all users were able to follow the provided\nsteps.",
    "descriptor": "",
    "authors": [
      "Ryan Nguyen",
      "Rahul Rai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07772"
  },
  {
    "id": "arXiv:2206.07776",
    "title": "Robust Attack Graph Generation",
    "abstract": "We present a method to learn automaton models that are more robust to input\nmodifications. It iteratively aligns sequences to a learned model, modifies the\nsequences to their aligned versions, and re-learns the model. Automaton\nlearning algorithms are typically very good at modeling the frequent behavior\nof a software system. Our solution can be used to also learn the behavior\npresent in infrequent sequences, as these will be aligned to the frequent ones\nrepresented by the model. We apply our method to the SAGE tool for modeling\nattacker behavior from intrusion alerts. In experiments, we demonstrate that\nour algorithm learns models that can handle noise such as added and removed\nsymbols from sequences. Furthermore, it learns more concise models that fit\nbetter to the training data.",
    "descriptor": "\nComments: Appeared at LearnAut '22\n",
    "authors": [
      "Dennis Mouwen",
      "Sicco Verwer",
      "Azqa Nadeem"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07776"
  },
  {
    "id": "arXiv:2206.07784",
    "title": "Evaluating Short-Term Forecasting of Multiple Time Series in IoT  Environments",
    "abstract": "Modern Internet of Things (IoT) environments are monitored via a large number\nof IoT enabled sensing devices, with the data acquisition and processing\ninfrastructure setting restrictions in terms of computational power and energy\nresources. To alleviate this issue, sensors are often configured to operate at\nrelatively low sampling frequencies, yielding a reduced set of observations.\nNevertheless, this can hamper dramatically subsequent decision-making, such as\nforecasting. To address this problem, in this work we evaluate short-term\nforecasting in highly underdetermined cases, i.e., the number of sensor streams\nis much higher than the number of observations. Several statistical, machine\nlearning and neural network-based models are thoroughly examined with respect\nto the resulting forecasting accuracy on five different real-world datasets.\nThe focus is given on a unified experimental protocol especially designed for\nshort-term prediction of multiple time series at the IoT edge. The proposed\nframework can be considered as an important step towards establishing a solid\nforecasting strategy in resource constrained IoT applications.",
    "descriptor": "\nComments: Accepted for publication in the \"Edge-Fog-Cloud Machine Learning for Smart Cities Applications\" Special Session at the European Signal Processing Conference (EUSIPCO) 2022\n",
    "authors": [
      "Christos Tzagkarakis",
      "Pavlos Charalampidis",
      "Stylianos Roubakis",
      "Alexandros Fragkiadakis",
      "Sotiris Ioannidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.07784"
  },
  {
    "id": "arXiv:2206.07785",
    "title": "Participation and Data Valuation in IoT Data Markets through Distributed  Coalitions",
    "abstract": "This paper considers a market for Internet of Things (IoT) data that is used\nto train machine learning models. The data is supplied to the market platform\nthrough a network and the price of the data is controlled based on the value it\nbrings to the machine learning model. We explore the correlation property of\ndata in a game-theoretical setting to eventually derive a simplified\ndistributed solution for a data trading mechanism that emphasizes the mutual\nbenefit of devices and the market. The key proposal is an efficient algorithm\nfor markets that jointly addresses the challenges of availability and\nheterogeneity in participation, as well as the transfer of trust and the\neconomic value of data exchange in IoT networks. The proposed approach\nestablishes the data market by reinforcing collaboration opportunities between\ndevices with correlated data to avoid information leakage. Therein, we develop\na network-wide optimization problem that maximizes the social value of\ncoalition among the IoT devices of similar data types; at the same time, it\nminimizes the cost due to network externalities, i.e., the impact of\ninformation leakage due to data correlation, as well as the opportunity costs.\nFinally, we reveal the structure of the formulated problem as a distributed\ncoalition game and solve it following the simplified split-and-merge algorithm.\nSimulation results show the efficacy of our proposed mechanism design toward a\ntrusted IoT data market, with up to 32.72% gain in the average payoff for each\nseller.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Shashi Raj Pandey",
      "Pierre Pinson",
      "Petar Popovski"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07785"
  },
  {
    "id": "arXiv:2206.07795",
    "title": "On Calibrated Model Uncertainty in Deep Learning",
    "abstract": "Estimated uncertainty by approximate posteriors in Bayesian neural networks\nare prone to miscalibration, which leads to overconfident predictions in\ncritical tasks that have a clear asymmetric cost or significant losses. Here,\nwe extend the approximate inference for the loss-calibrated Bayesian framework\nto dropweights based Bayesian neural networks by maximising expected utility\nover a model posterior to calibrate uncertainty in deep learning. Furthermore,\nwe show that decisions informed by loss-calibrated uncertainty can improve\ndiagnostic performance to a greater extent than straightforward alternatives.\nWe propose Maximum Uncertainty Calibration Error (MUCE) as a metric to measure\ncalibrated confidence, in addition to its prediction especially for high-risk\napplications, where the goal is to minimise the worst-case deviation between\nerror and estimated uncertainty. In experiments, we show the correlation\nbetween error in prediction and estimated uncertainty by interpreting\nWasserstein distance as the accuracy of prediction. We evaluated the\neffectiveness of our approach to detecting Covid-19 from X-Ray images.\nExperimental results show that our method reduces miscalibration considerably,\nwithout impacting the models accuracy and improves reliability of\ncomputer-based diagnostics.",
    "descriptor": "\nComments: The European Conference on Machine Learning (ECML PKDD 2020). arXiv admin note: text overlap with arXiv:2103.11214\n",
    "authors": [
      "Biraja Ghoshal",
      "Allan Tucker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07795"
  },
  {
    "id": "arXiv:2206.07796",
    "title": "FixEval: Execution-based Evaluation of Program Fixes for Competitive  Programming Problems",
    "abstract": "Source code repositories consist of large codebases, often containing\nerror-prone programs. The increasing complexity of software has led to a\ndrastic rise in time and costs for identifying and fixing these defects.\nVarious methods exist to automatically generate fixes for buggy code. However,\ndue to the large combinatorial space of possible solutions for a particular\nbug, there are not many tools and datasets available to evaluate generated code\neffectively. In this work, we introduce FixEval, a benchmark comprising buggy\ncode submissions to competitive programming problems and their respective\nfixes. We introduce a rich test suite to evaluate and assess the correctness of\nmodel-generated program fixes. We consider two Transformer language models\npretrained on programming languages as our baselines, and compare them using\nmatch-based and execution-based evaluation metrics. Our experiments show that\nmatch-based metrics do not reflect model-generated program fixes accurately,\nwhile execution-based methods evaluate programs through all cases and scenarios\nspecifically designed for that solution. Therefore, we believe FixEval provides\na step towards real-world automatic bug fixing and model-generated code\nevaluation.",
    "descriptor": "",
    "authors": [
      "Md Mahim Anjum Haque",
      "Wasi Uddin Ahmad",
      "Ismini Lourentzou",
      "Chris Brown"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07796"
  },
  {
    "id": "arXiv:2206.07798",
    "title": "Gaussian Blue Noise",
    "abstract": "Among the various approaches for producing point distributions with blue\nnoise spectrum, we argue for an optimization framework using Gaussian kernels.\nWe show that with a wise selection of optimization parameters, this approach\nattains unprecedented quality, provably surpassing the current state of the art\nattained by the optimal transport (BNOT) approach. Further, we show that our\nalgorithm scales smoothly and feasibly to high dimensions while maintaining the\nsame quality, realizing unprecedented high-quality high-dimensional blue noise\nsets. Finally, we show an extension to adaptive sampling.",
    "descriptor": "",
    "authors": [
      "Abdalla G. M. Ahmed",
      "Jing Ren",
      "Peter Wonka"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.07798"
  },
  {
    "id": "arXiv:2206.07801",
    "title": "Beyond Adult and COMPAS: Fairness in Multi-Class Prediction",
    "abstract": "We consider the problem of producing fair probabilistic classifiers for\nmulti-class classification tasks. We formulate this problem in terms of\n\"projecting\" a pre-trained (and potentially unfair) classifier onto the set of\nmodels that satisfy target group-fairness requirements. The new, projected\nmodel is given by post-processing the outputs of the pre-trained classifier by\na multiplicative factor. We provide a parallelizable iterative algorithm for\ncomputing the projected classifier and derive both sample complexity and\nconvergence guarantees. Comprehensive numerical comparisons with\nstate-of-the-art benchmarks demonstrate that our approach maintains competitive\nperformance in terms of accuracy-fairness trade-off curves, while achieving\nfavorable runtime on large datasets. We also evaluate our method at scale on an\nopen dataset with multiple classes, multiple intersectional protected groups,\nand over 1M samples.",
    "descriptor": "\nComments: 46 pages, 15 figures\n",
    "authors": [
      "Wael Alghamdi",
      "Hsiang Hsu",
      "Haewon Jeong",
      "Hao Wang",
      "P. Winston Michalak",
      "Shahab Asoodeh",
      "Flavio P. Calmon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07801"
  },
  {
    "id": "arXiv:2206.07802",
    "title": "What makes domain generalization hard?",
    "abstract": "While several methodologies have been proposed for the daunting task of\ndomain generalization, understanding what makes this task challenging has\nreceived little attention. Here we present SemanticDG (Semantic Domain\nGeneralization): a benchmark with 15 photo-realistic domains with the same\ngeometry, scene layout and camera parameters as the popular 3D ScanNet dataset,\nbut with controlled domain shifts in lighting, materials, and viewpoints. Using\nthis benchmark, we investigate the impact of each of these semantic shifts on\ngeneralization independently. Visual recognition models easily generalize to\nnovel lighting, but struggle with distribution shifts in materials and\nviewpoints. Inspired by human vision, we hypothesize that scene context can\nserve as a bridge to help models generalize across material and viewpoint\ndomain shifts and propose a context-aware vision transformer along with a\ncontrastive loss over material and viewpoint changes to address these domain\nshifts. Our approach (dubbed as CDCNet) outperforms existing domain\ngeneralization methods by over an 18% margin. As a critical benchmark, we also\nconduct psychophysics experiments and find that humans generalize equally well\nacross lighting, materials and viewpoints. The benchmark and computational\nmodel introduced here help understand the challenges associated with\ngeneralization across domains and provide initial steps towards extrapolation\nto semantic distribution shifts. We include all data and source code in the\nsupplement.",
    "descriptor": "",
    "authors": [
      "Spandan Madan",
      "Li You",
      "Mengmi Zhang",
      "Hanspeter Pfister",
      "Gabriel Kreiman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.07802"
  },
  {
    "id": "arXiv:2206.07807",
    "title": "How Adults Understand What Young Children Say",
    "abstract": "Children's early speech often bears little resemblance to adult speech in\nform or content, and yet caregivers often find meaning in young children's\nutterances. Precisely how caregivers are able to do this remains poorly\nunderstood. We propose that successful early communication (an essential\nbuilding block of language development) relies not just on children's growing\nlinguistic knowledge, but also on adults' sophisticated inferences. These\ninferences, we further propose, are optimized for fine-grained details of how\nchildren speak. We evaluate these ideas using a set of candidate computational\nmodels of spoken word recognition based on deep learning and Bayesian\ninference, which instantiate competing hypotheses regarding the information\nsources used by adults to understand children. We find that the best-performing\nmodels (evaluated on datasets of adult interpretations of child speech) are\nthose that have strong prior expectations about what children are likely to\nwant to communicate, rather than the actual phonetic contents of what children\nsay. We further find that adults' behavior is best characterized as well-tuned\nto specific children: the more closely a word recognition model is tuned to the\nparticulars of an individual child's actual linguistic behavior, the better it\npredicts adults' inferences about what the child has said. These results offer\na comprehensive investigation into the role of caregivers as child-directed\nlisteners, with broader consequences for theories of language acquisition.",
    "descriptor": "\nComments: 19 pages, 6 figures, 2 tables\n",
    "authors": [
      "Stephan C. Meylan",
      "Ruthe Foushee",
      "Nicole H. Wong",
      "Elika Bergelson",
      "Roger P. Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07807"
  },
  {
    "id": "arXiv:2206.07808",
    "title": "Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter  Encoders for Natural Language Understanding Systems",
    "abstract": "We present results from a large-scale experiment on pretraining encoders with\nnon-embedding parameter counts ranging from 700M to 9.3B, their subsequent\ndistillation into smaller models ranging from 17M-170M parameters, and their\napplication to the Natural Language Understanding (NLU) component of a virtual\nassistant system. Though we train using 70% spoken-form data, our teacher\nmodels perform comparably to XLM-R and mT5 when evaluated on the written-form\nCross-lingual Natural Language Inference (XNLI) corpus. We perform a second\nstage of pretraining on our teacher models using in-domain data from our\nsystem, improving error rates by 3.86% relative for intent classification and\n7.01% relative for slot filling. We find that even a 170M-parameter model\ndistilled from our Stage 2 teacher model has 2.88% better intent classification\nand 7.69% better slot filling error rates when compared to the 2.3B-parameter\nteacher trained only on public data (Stage 1), emphasizing the importance of\nin-domain data for pretraining. When evaluated offline using labeled NLU data,\nour 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M\nparams) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally,\nwe present results from a full virtual assistant experimentation platform,\nwhere we find that models trained using our pretraining and distillation\npipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91%\non an automatic measurement of full-system user dissatisfaction.",
    "descriptor": "\nComments: KDD 2022\n",
    "authors": [
      "Jack FitzGerald",
      "Shankar Ananthakrishnan",
      "Konstantine Arkoudas",
      "Davide Bernardi",
      "Abhishek Bhagia",
      "Claudio Delli Bovi",
      "Jin Cao",
      "Rakesh Chada",
      "Amit Chauhan",
      "Luoxin Chen",
      "Anurag Dwarakanath",
      "Satyam Dwivedi",
      "Turan Gojayev",
      "Karthik Gopalakrishnan",
      "Thomas Gueudre",
      "Dilek Hakkani-Tur",
      "Wael Hamza",
      "Jonathan Hueser",
      "Kevin Martin Jose",
      "Haidar Khan",
      "Beiye Liu",
      "Jianhua Lu",
      "Alessandro Manzotti",
      "Pradeep Natarajan",
      "Karolina Owczarzak",
      "Gokmen Oz",
      "Enrico Palumbo",
      "Charith Peris",
      "Chandana Satya Prakash",
      "Stephen Rawls",
      "Andy Rosenbaum",
      "Anjali Shenoy",
      "Saleh Soltan",
      "Mukund Harakere Sridhar",
      "Liz Tan",
      "Fabian Triefenbach",
      "Pan Wei",
      "Haiyang Yu",
      "Shuai Zheng",
      "Gokhan Tur",
      "Prem Natarajan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07808"
  },
  {
    "id": "arXiv:2206.07810",
    "title": "High-Resolution Bathymetric Reconstruction From Sidescan Sonar With Deep  Neural Networks",
    "abstract": "We propose a novel data-driven approach for high-resolution bathymetric\nreconstruction from sidescan. Sidescan sonar (SSS) intensities as a function of\nrange do contain some information about the slope of the seabed. However, that\ninformation must be inferred. Additionally, the navigation system provides the\nestimated trajectory, and normally the altitude along this trajectory is also\navailable. From these we obtain a very coarse seabed bathymetry as an input.\nThis is then combined with the indirect but high-resolution seabed slope\ninformation from the sidescan to estimate the full bathymetry. This sparse\ndepth could be acquired by single-beam echo sounder, Doppler Velocity Log\n(DVL), other bottom tracking sensors or bottom tracking algorithm from sidescan\nitself. In our work, a fully convolutional network is used to estimate the\ndepth contour and its aleatoric uncertainty from the sidescan images and sparse\ndepth in an end-to-end fashion. The estimated depth is then used together with\nthe range to calculate the point's 3D location on the seafloor. A high-quality\nbathymetric map can be reconstructed after fusing the depth predictions and the\ncorresponding confidence measures from the neural networks. We show the\nimprovement of the bathymetric map gained by using sparse depths with sidescan\nover estimates with sidescan alone. We also show the benefit of confidence\nweighting when fusing multiple bathymetric estimates into a single map.",
    "descriptor": "\nComments: submitted to IEEE Journal of Oceanic Engineering\n",
    "authors": [
      "Yiping Xie",
      "Nils Bore",
      "John Folkesson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07810"
  },
  {
    "id": "arXiv:2206.07811",
    "title": "Safety Guarantees for Neural Network Dynamic Systems via Stochastic  Barrier Functions",
    "abstract": "Neural Networks (NNs) have been successfully employed to represent the state\nevolution of complex dynamical systems. Such models, referred to as NN dynamic\nmodels (NNDMs), use iterative noisy predictions of NN to estimate a\ndistribution of system trajectories over time. Despite their accuracy, safety\nanalysis of NNDMs is known to be a challenging problem and remains largely\nunexplored. To address this issue, in this paper, we introduce a method of\nproviding safety guarantees for NNDMs. Our approach is based on stochastic\nbarrier functions, whose relation with safety are analogous to that of Lyapunov\nfunctions with stability. We first show a method of synthesizing stochastic\nbarrier functions for NNDMs via a convex optimization problem, which in turn\nprovides a lower bound on the system's safety probability. A key step in our\nmethod is the employment of the recent convex approximation results for NNs to\nfind piece-wise linear bounds, which allow the formulation of the barrier\nfunction synthesis problem as a sum-of-squares optimization program. If the\nobtained safety probability is above the desired threshold, the system is\ncertified. Otherwise, we introduce a method of generating controls for the\nsystem that robustly maximizes the safety probability in a minimally-invasive\nmanner. We exploit the convexity property of the barrier function to formulate\nthe optimal control synthesis problem as a linear program. Experimental results\nillustrate the efficacy of the method. Namely, they show that the method can\nscale to multi-dimensional NNDMs with multiple layers and hundreds of neurons\nper layer, and that the controller can significantly improve the safety\nprobability.",
    "descriptor": "",
    "authors": [
      "Rayan Mazouz",
      "Karan Muvvala",
      "Akash Ratheesh",
      "Luca Laurenti",
      "Morteza Lahijanian"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07811"
  },
  {
    "id": "arXiv:2206.07813",
    "title": "Search-Based Testing Approach for Deep Reinforcement Learning Agents",
    "abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed\nduring the last decade to solve various decision-making problems such as\nautonomous driving and robotics. However, these algorithms have faced great\nchallenges when deployed in safety-critical environments since they often\nexhibit erroneous behaviors that can lead to potentially critical errors. One\nway to assess the safety of DRL agents is to test them to detect possible\nfaults leading to critical failures during their execution. This raises the\nquestion of how we can efficiently test DRL policies to ensure their\ncorrectness and adherence to safety requirements. Most existing works on\ntesting DRL agents use adversarial attacks that perturb states or actions of\nthe agent. However, such attacks often lead to unrealistic states of the\nenvironment. Their main goal is to test the robustness of DRL agents rather\nthan testing the compliance of agents' policies with respect to requirements.\nDue to the huge state space of DRL environments, the high cost of test\nexecution, and the black-box nature of DRL algorithms, the exhaustive testing\nof DRL agents is impossible. In this paper, we propose a Search-based Testing\nApproach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL\nagent by effectively searching for failing executions of the agent within a\nlimited testing budget. We use machine learning models and a dedicated genetic\nalgorithm to narrow the search towards faulty episodes. We apply STARLA on a\nDeep-Q-Learning agent which is widely used as a benchmark and show that it\nsignificantly outperforms Random Testing by detecting more faults related to\nthe agent's policy. We also investigate how to extract rules that characterize\nfaulty episodes of the DRL agent using our search results. Such rules can be\nused to understand the conditions under which the agent fails and thus assess\nits deployment risks.",
    "descriptor": "",
    "authors": [
      "Amirhossein Zolfagharian",
      "Manel Abdellatif",
      "Lionel Briand",
      "Mojtaba Bagherzadeh",
      "Ramesh S"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07813"
  },
  {
    "id": "arXiv:2206.07817",
    "title": "Low-Cost Superconducting Fan-Out with Repurposed Josephson Junctions",
    "abstract": "Superconductor electronics (SCE) promise computer systems with orders of\nmagnitude higher speeds and lower energy consumption than their complementary\nmetal-oxide semiconductor (CMOS) counterpart. At the same time, the scalability\nand area utilization of superconducting systems are major concerns. Some of\nthese concerns come from device-level challenges and the gap between SCE and\nCMOS technology nodes, and others come from the way Josephson Junctions (JJs)\nare used. Towards this end, we notice that a considerable fraction of hardware\nresources are not involved in logic operations, but rather are used for fan-out\nand buffering purposes. In this paper, we ask if there is a way to reduce these\noverheads; propose the repurposing of JJs at the cell boundaries for fan-out;\nand establish a set of rules to discretize critical currents in a way that is\nconducive to this reassignment. Finally, we demonstrate the accomplished gains\nthrough detailed analog simulations and modeling analyses. Our experiments\nindicate that this method leads to a 47.6% savings in the JJ count in a tree\nwith a fan-out of 1024, as well as an average of 43.3% of the JJ count for\nsignal splitting in ISCAS85 benchmarks.",
    "descriptor": "\nComments: 11 pages, 15 figures, submitted to IEEE TAS\n",
    "authors": [
      "Jennifer Volk",
      "George Tzimpragos",
      "Alex Wynn",
      "Evan Golden",
      "Timothy Sherwood"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2206.07817"
  },
  {
    "id": "arXiv:2206.07819",
    "title": "Neural Network Normal Estimation and Bathymetry Reconstruction from  Sidescan Sonar",
    "abstract": "Sidescan sonar intensity encodes information about the changes of surface\nnormal of the seabed. However, other factors such as seabed geometry as well as\nits material composition also affect the return intensity. One can model these\nintensity changes in a forward direction from the surface normals from\nbathymetric map and physical properties to the measured intensity or\nalternatively one can use an inverse model which starts from the intensities\nand models the surface normals. Here we use an inverse model which leverages\ndeep learning's ability to learn from data; a convolutional neural network is\nused to estimate the surface normal from the sidescan. Thus the internal\nproperties of the seabed are only implicitly learned. Once this information is\nestimated, a bathymetric map can be reconstructed through an optimization\nframework that also includes altimeter readings to provide a sparse depth\nprofile as a constraint. Implicit neural representation learning was recently\nproposed to represent the bathymetric map in such an optimization framework. In\nthis article, we use a neural network to represent the map and optimize it\nunder constraints of altimeter points and estimated surface normal from\nsidescan. By fusing multiple observations from different angles from several\nsidescan lines, the estimated results are improved through optimization. We\ndemonstrate the efficiency and scalability of the approach by reconstructing a\nhigh-quality bathymetry using sidescan data from a large sidescan survey. We\ncompare the proposed data-driven inverse model approach of modeling a sidescan\nwith a forward Lambertian model. We assess the quality of each reconstruction\nby comparing it with data constructed from a multibeam sensor. We are thus able\nto discuss the strengths and weaknesses of each approach.",
    "descriptor": "\nComments: submitted to IEEE Journal of Oceanic Engineering\n",
    "authors": [
      "Yiping Xie",
      "Nils Bore",
      "John Folkesson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07819"
  },
  {
    "id": "arXiv:2206.07821",
    "title": "Towards Differentiable Rendering for Sidescan Sonar Imagery",
    "abstract": "Recent advances in differentiable rendering, which allow calculating the\ngradients of 2D pixel values with respect to 3D object models, can be applied\nto estimation of the model parameters by gradient-based optimization with only\n2D supervision. It is easy to incorporate deep neural networks into such an\noptimization pipeline, allowing the leveraging of deep learning techniques.\nThis also largely reduces the requirement for collecting and annotating 3D\ndata, which is very difficult for applications, for example when constructing\ngeometry from 2D sensors. In this work, we propose a differentiable renderer\nfor sidescan sonar imagery. We further demonstrate its ability to solve the\ninverse problem of directly reconstructing a 3D seafloor mesh from only 2D\nsidescan sonar data.",
    "descriptor": "\nComments: submitted to 2022 IEEE OES Autonomous Underwater Vehicles (AUV) SYMPOSIUM\n",
    "authors": [
      "Yiping Xie",
      "Nils Bore",
      "John Folkesson"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07821"
  },
  {
    "id": "arXiv:2206.07823",
    "title": "An Investigation on Kripke-style Modal Type Theories",
    "abstract": "This technical report investigates Kripke-style modal type theories, both\nsimply typed and dependently typed. We examine basic meta-theories of the type\ntheories, develop their substitution calculi, and give normalization by\nevaluation algorithms.",
    "descriptor": "",
    "authors": [
      "Jason Z. S. Hu",
      "Brigitte Pientka"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.07823"
  },
  {
    "id": "arXiv:2206.07826",
    "title": "Metric-Fair Classifier Derandomization",
    "abstract": "We study the problem of \\emph{classifier derandomization} in machine\nlearning: given a stochastic binary classifier $f: X \\to [0,1]$, sample a\ndeterministic classifier $\\hat{f}: X \\to \\{0,1\\}$ that approximates the output\nof $f$ in aggregate over any data distribution. Recent work revealed how to\nefficiently derandomize a stochastic classifier with strong output\napproximation guarantees, but at the cost of individual fairness -- that is, if\n$f$ treated similar inputs similarly, $\\hat{f}$ did not. In this paper, we\ninitiate a systematic study of classifier derandomization with metric fairness\nguarantees. We show that the prior derandomization approach is almost maximally\nmetric-unfair, and that a simple ``random threshold'' derandomization achieves\noptimal fairness preservation but with weaker output approximation. We then\ndevise a derandomization procedure that provides an appealing tradeoff between\nthese two: if $f$ is $\\alpha$-metric fair according to a metric $d$ with a\nlocality-sensitive hash (LSH) family, then our derandomized $\\hat{f}$ is, with\nhigh probability, $O(\\alpha)$-metric fair and a close approximation of $f$. We\nalso prove generic results applicable to all (fair and unfair) classifier\nderandomization procedures, including a bias-variance decomposition and\nreductions between various notions of metric fairness.",
    "descriptor": "\nComments: Proceedings of ICML 2022\n",
    "authors": [
      "Jimmy Wu",
      "Yatong Chen",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07826"
  },
  {
    "id": "arXiv:2206.07828",
    "title": "Searching Entangled Program Spaces",
    "abstract": "Many problem domains, including program synthesis and rewrite-based\noptimization, require searching astronomically large spaces of programs.\nExisting approaches often rely on building specialized data structures --\nversion-space algebras, finite tree automata, or e-graphs -- to compactly\nrepresent these programs. To find a compact representation, existing data\nstructures exploit independence of subterms; they blow up when the choices of\nsubterms are entangled. We introduce equality-constrained tree automata\n(ECTAs), a generalization of the three aforementioned data structures that can\nefficiently represent large spaces of programs with entangled subterms. We\npresent efficient algorithms for extracting programs from ECTAs, implemented in\na performant Haskell library, \\texttt{ecta}. Using \\texttt{ecta} we construct\n\\textsc{Hectare}, a type-driven program synthesizer for Haskell.\n\\textsc{Hectare} significantly outperforms a state-of-the-art synthesizer\nHoogle+ -- providing an average speedup of 8x -- despite its implementation\nbeing an order of magnitude smaller.",
    "descriptor": "",
    "authors": [
      "James Koppel",
      "Zheng Guo",
      "Edsko de Vries",
      "Armando Solar-Lezama",
      "Nadia Polikarpova"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.07828"
  },
  {
    "id": "arXiv:2206.07832",
    "title": "Adaptive Expert Models for Personalization in Federated Learning",
    "abstract": "Federated Learning (FL) is a promising framework for distributed learning\nwhen data is private and sensitive. However, the state-of-the-art solutions in\nthis framework are not optimal when data is heterogeneous and non-Independent\nand Identically Distributed (non-IID). We propose a practical and robust\napproach to personalization in FL that adjusts to heterogeneous and non-IID\ndata by balancing exploration and exploitation of several global models. To\nachieve our aim of personalization, we use a Mixture of Experts (MoE) that\nlearns to group clients that are similar to each other, while using the global\nmodels more efficiently. We show that our approach achieves an accuracy up to\n29.78 % and up to 4.38 % better compared to a local model in a pathological\nnon-IID setting, even though we tune our approach in the IID setting.",
    "descriptor": "\nComments: To appear in International Workshop on Trustworthy Federated Learning in Conjunction with IJCAI 2022 (FL-IJCAI'22). 8 pages\n",
    "authors": [
      "Martin Isaksson",
      "Edvin Listo Zec",
      "Rickard C\u00f6ster",
      "Daniel Gillblad",
      "\u0160ar\u016bnas Girdzijauskas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07832"
  },
  {
    "id": "arXiv:2206.07834",
    "title": "Efficient Approximation of Expected Hypervolume Improvement using  Gauss-Hermite Quadrature",
    "abstract": "Many methods for performing multi-objective optimisation of computationally\nexpensive problems have been proposed recently. Typically, a probabilistic\nsurrogate for each objective is constructed from an initial dataset. The\nsurrogates can then be used to produce predictive densities in the objective\nspace for any solution. Using the predictive densities, we can compute the\nexpected hypervolume improvement (EHVI) due to a solution. Maximising the EHVI,\nwe can locate the most promising solution that may be expensively evaluated\nnext. There are closed-form expressions for computing the EHVI, integrating\nover the multivariate predictive densities. However, they require partitioning\nthe objective space, which can be prohibitively expensive for more than three\nobjectives. Furthermore, there are no closed-form expressions for a problem\nwhere the predictive densities are dependent, capturing the correlations\nbetween objectives. Monte Carlo approximation is used instead in such cases,\nwhich is not cheap. Hence, the need to develop new accurate but cheaper\napproximation methods remains. Here we investigate an alternative approach\ntoward approximating the EHVI using Gauss-Hermite quadrature. We show that it\ncan be an accurate alternative to Monte Carlo for both independent and\ncorrelated predictive densities with statistically significant rank\ncorrelations for a range of popular test problems.",
    "descriptor": "",
    "authors": [
      "Alma Rahat",
      "Tinkle Chugh",
      "Jonathan Fieldsend",
      "Richard Allmendinger",
      "Kaisa Miettinen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.07834"
  },
  {
    "id": "arXiv:2206.07835",
    "title": "Disentangling visual and written concepts in CLIP",
    "abstract": "The CLIP network measures the similarity between natural text and images; in\nthis work, we investigate the entanglement of the representation of word images\nand natural images in its image encoder. First, we find that the image encoder\nhas an ability to match word images with natural images of scenes described by\nthose words. This is consistent with previous research that suggests that the\nmeaning and the spelling of a word might be entangled deep within the network.\nOn the other hand, we also find that CLIP has a strong ability to match\nnonsense words, suggesting that processing of letters is separated from\nprocessing of their meaning. To explicitly determine whether the spelling\ncapability of CLIP is separable, we devise a procedure for identifying\nrepresentation subspaces that selectively isolate or eliminate spelling\ncapabilities. We benchmark our methods against a range of retrieval tasks, and\nwe also test them by measuring the appearance of text in CLIP-guided generated\nimages. We find that our methods are able to cleanly separate spelling\ncapabilities of CLIP from the visual processing of natural images.",
    "descriptor": "",
    "authors": [
      "Joanna Materzynska",
      "Antonio Torralba",
      "David Bau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07835"
  },
  {
    "id": "arXiv:2206.07836",
    "title": "Personal Entity, Concept, and Named Entity Linking in Conversations",
    "abstract": "Building conversational agents that can have natural and knowledge-grounded\ninteractions with humans requires understanding user utterances. Entity Linking\n(EL) is an effective and widely used method for understanding natural language\ntext and connecting it to external knowledge. It is, however, shown that\nexisting EL methods developed for annotating documents are suboptimal for\nconversations, where personal entities (e.g., \"my cars\") and concepts are\nessential for understanding user utterances. In this paper, we introduce a\ncollection and a tool for entity linking in conversations. We collect EL\nannotations for 1327 conversational utterances, consisting of links to named\nentities, concepts, and personal entities. The dataset is used for training our\ntoolkit for conversational entity linking, CREL. Unlike existing EL methods,\nCREL is developed to identify both named entities and concepts. It also\nutilizes coreference resolution techniques to identify personal entities and\nreferences to the explicit entity mentions in the conversations. We compare\nCREL with state-of-the-art techniques and show that it outperforms all existing\nbaselines.",
    "descriptor": "",
    "authors": [
      "Hideaki Joko",
      "Faegheh Hasibi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.07836"
  },
  {
    "id": "arXiv:2206.07837",
    "title": "Modeling the Data-Generating Process is Necessary for  Out-of-Distribution Generalization",
    "abstract": "Real-world data collected from multiple domains can have multiple, distinct\ndistribution shifts over multiple attributes. However, state-of-the art\nadvances in domain generalization (DG) algorithms focus only on specific shifts\nover a single attribute. We introduce datasets with multi-attribute\ndistribution shifts and find that existing DG algorithms fail to generalize. To\nexplain this, we use causal graphs to characterize the different types of\nshifts based on the relationship between spurious attributes and the\nclassification label. Each multi-attribute causal graph entails different\nconstraints over observed variables, and therefore any algorithm based on a\nsingle, fixed independence constraint cannot work well across all shifts. We\npresent Causally Adaptive Constraint Minimization (CACM), a new algorithm for\nidentifying the correct independence constraints for regularization. Results on\nfully synthetic, MNIST and small NORB datasets, covering binary and\nmulti-valued attributes and labels, confirm our theoretical claim: correct\nindependence constraints lead to the highest accuracy on unseen domains whereas\nincorrect constraints fail to do so. Our results demonstrate the importance of\nmodeling the causal relationships inherent in the data-generating process: in\nmany cases, it is impossible to know the correct regularization constraints\nwithout this information.",
    "descriptor": "",
    "authors": [
      "Jivat Neet Kaur",
      "Emre Kiciman",
      "Amit Sharma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07837"
  },
  {
    "id": "arXiv:2206.07839",
    "title": "Linearity Grafting: Relaxed Neuron Pruning Helps Certifiable Robustness",
    "abstract": "Certifiable robustness is a highly desirable property for adopting deep\nneural networks (DNNs) in safety-critical scenarios, but often demands tedious\ncomputations to establish. The main hurdle lies in the massive amount of\nnon-linearity in large DNNs. To trade off the DNN expressiveness (which calls\nfor more non-linearity) and robustness certification scalability (which prefers\nmore linearity), we propose a novel solution to strategically manipulate\nneurons, by \"grafting\" appropriate levels of linearity. The core of our\nproposal is to first linearize insignificant ReLU neurons, to eliminate the\nnon-linear components that are both redundant for DNN performance and harmful\nto its certification. We then optimize the associated slopes and intercepts of\nthe replaced linear activations for restoring model performance while\nmaintaining certifiability. Hence, typical neuron pruning could be viewed as a\nspecial case of grafting a linear function of the fixed zero slopes and\nintercept, that might overly restrict the network flexibility and sacrifice its\nperformance. Extensive experiments on multiple datasets and network backbones\nshow that our linearity grafting can (1) effectively tighten certified bounds;\n(2) achieve competitive certifiable robustness without certified robust\ntraining (i.e., over 30% improvements on CIFAR-10 models); and (3) scale up\ncomplete verification to large adversarially trained models with 17M\nparameters. Codes are available at\nhttps://github.com/VITA-Group/Linearity-Grafting.",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Tianlong Chen",
      "Huan Zhang",
      "Zhenyu Zhang",
      "Shiyu Chang",
      "Sijia Liu",
      "Pin-Yu Chen",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07839"
  },
  {
    "id": "arXiv:2206.07840",
    "title": "Architectural Backdoors in Neural Networks",
    "abstract": "Machine learning is vulnerable to adversarial manipulation. Previous\nliterature has demonstrated that at the training stage attackers can manipulate\ndata and data sampling procedures to control model behaviour. A common attack\ngoal is to plant backdoors i.e. force the victim model to learn to recognise a\ntrigger known only by the adversary. In this paper, we introduce a new class of\nbackdoor attacks that hide inside model architectures i.e. in the inductive\nbias of the functions used to train. These backdoors are simple to implement,\nfor instance by publishing open-source code for a backdoored model architecture\nthat others will reuse unknowingly. We demonstrate that model architectural\nbackdoors represent a real threat and, unlike other approaches, can survive a\ncomplete re-training from scratch. We formalise the main construction\nprinciples behind architectural backdoors, such as a link between the input and\nthe output, and describe some possible protections against them. We evaluate\nour attacks on computer vision benchmarks of different scales and demonstrate\nthe underlying vulnerability is pervasive in a variety of training settings.",
    "descriptor": "",
    "authors": [
      "Mikel Bober-Irizar",
      "Ilia Shumailov",
      "Yiren Zhao",
      "Robert Mullins",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07840"
  },
  {
    "id": "arXiv:2206.07841",
    "title": "TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained  Language Models",
    "abstract": "Transferring knowledge from one domain to another is of practical importance\nfor many tasks in natural language processing, especially when the amount of\navailable data in the target domain is limited. In this work, we propose a\nnovel few-shot approach to domain adaptation in the context of Named Entity\nRecognition (NER). We propose a two-step approach consisting of a variable base\nmodule and a template module that leverages the knowledge captured in\npre-trained language models with the help of simple descriptive patterns. Our\napproach is simple yet versatile and can be applied in few-shot and zero-shot\nsettings. Evaluating our lightweight approach across a number of different\ndatasets shows that it can boost the performance of state-of-the-art baselines\nby 2-5% F1-score.",
    "descriptor": "\nComments: Accepted to 25th International Conference on Text, Speech and Dialogue (TSD 2022)\n",
    "authors": [
      "Ali Davody",
      "David Ifeoluwa Adelani",
      "Thomas Kleinbauer",
      "Dietrich Klakow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07841"
  },
  {
    "id": "arXiv:2206.07842",
    "title": "Queried Unlabeled Data Improves and Robustifies Class-Incremental  Learning",
    "abstract": "Class-incremental learning (CIL) suffers from the notorious dilemma between\nlearning newly added classes and preserving previously learned class knowledge.\nThat catastrophic forgetting issue could be mitigated by storing historical\ndata for replay, which yet would cause memory overheads as well as imbalanced\nprediction updates. To address this dilemma, we propose to leverage \"free\"\nexternal unlabeled data querying in continual learning. We first present a CIL\nwith Queried Unlabeled Data (CIL-QUD) scheme, where we only store a handful of\npast training samples as anchors and use them to query relevant unlabeled\nexamples each time. Along with new and past stored data, the queried unlabeled\nare effectively utilized, through learning-without-forgetting (LwF)\nregularizers and class-balance training. Besides preserving model\ngeneralization over past and current tasks, we next study the problem of\nadversarial robustness for CIL-QUD. Inspired by the recent success of learning\nrobust models with unlabeled data, we explore a new robustness-aware CIL\nsetting, where the learned adversarial robustness has to resist forgetting and\nbe transferred as new tasks come in continually. While existing options easily\nfail, we show queried unlabeled data can continue to benefit, and seamlessly\nextend CIL-QUD into its robustified versions, RCIL-QUD. Extensive experiments\ndemonstrate that CIL-QUD achieves substantial accuracy gains on CIFAR-10 and\nCIFAR-100, compared to previous state-of-the-art CIL approaches. Moreover,\nRCIL-QUD establishes the first strong milestone for robustness-aware CIL. Codes\nare available in https://github.com/VITA-Group/CIL-QUD.",
    "descriptor": "\nComments: Accepted by TMLR 2022\n",
    "authors": [
      "Tianlong Chen",
      "Sijia Liu",
      "Shiyu Chang",
      "Lisa Amini",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07842"
  },
  {
    "id": "arXiv:2206.07846",
    "title": "Action Spotting using Dense Detection Anchors Revisited: Submission to  the SoccerNet Challenge 2022",
    "abstract": "This technical report describes our submission to the Action Spotting\nSoccerNet Challenge 2022. The challenge is part of the CVPR 2022 ActivityNet\nWorkshop. Our submission is based on a method that we proposed recently, which\nfocuses on increasing temporal precision via a densely sampled set of detection\nanchors. Due to its emphasis on temporal precision, this approach is able to\nproduce competitive results on the tight average-mAP metric, which uses small\ntemporal evaluation tolerances. This recently proposed metric is the evaluation\ncriterion used for the challenge. In order to further improve results, here we\nintroduce small changes in the pre- and post-processing steps, and also combine\ndifferent input feature types via late fusion. This report describes the\nresulting overall approach, focusing on the modifications introduced. We also\ndescribe the training procedures used, and present our results.",
    "descriptor": "\nComments: We are working on a new version of this report, which will contain a few more experiments\n",
    "authors": [
      "Jo\u00e3o V. B. Soares",
      "Avijit Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07846"
  },
  {
    "id": "arXiv:2206.07850",
    "title": "Improved surface reconstruction using high-frequency details",
    "abstract": "Neural rendering can be used to reconstruct implicit representations of\nshapes without 3D supervision. However, current neural surface reconstruction\nmethods have difficulty learning high-frequency details of shapes, so that the\nreconstructed shapes are often oversmoothed. We propose a novel method to\nimprove the quality of surface reconstruction in neural rendering. We follow\nrecent work to model surfaces as signed distance fields. First, we offer a\nderivation to analyze the relationship between the signed distance function,\nthe volume density, the transparency function, and the weighting function used\nin the volume rendering equation. Second, we observe that attempting to jointly\nencode high-frequency and low frequency components in a single signed distance\nfunction leads to unstable optimization. We propose to decompose the signed\ndistance function in a base function and a displacement function together with\na coarse-to-fine strategy to gradually increase the high-frequency details.\nFinally, we propose to use an adaptive strategy that enables the optimization\nto focus on improving certain regions near the surface where the signed\ndistance fields have artifacts. Our qualitative and quantitative results show\nthat our method can reconstruct high-frequency surface details and obtain\nbetter surface reconstruction quality than the current state of the art. Code\nwill be released at https://github.com/yiqun-wang/HFS.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Yiqun Wang",
      "Ivan Skorokhodov",
      "Peter Wonka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.07850"
  },
  {
    "id": "arXiv:2206.07860",
    "title": "EPG2S: Speech Generation and Speech Enhancement based on  Electropalatography and Audio Signals using Multimodal Learning",
    "abstract": "Speech generation and enhancement based on articulatory movements facilitate\ncommunication when the scope of verbal communication is absent, e.g., in\npatients who have lost the ability to speak. Although various techniques have\nbeen proposed to this end, electropalatography (EPG), which is a monitoring\ntechnique that records contact between the tongue and hard palate during\nspeech, has not been adequately explored. Herein, we propose a novel multimodal\nEPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech\ngeneration and enhancement. Different fusion strategies based on multiple\ncombinations of EPG and noisy speech signals are examined, and the viability of\nthe proposed method is investigated. Experimental results indicate that EPG2S\nachieves desirable speech generation outcomes based solely on EPG signals.\nFurther, the addition of noisy speech signals is observed to improve quality\nand intelligibility. Additionally, EPG2S is observed to achieve high-quality\nspeech enhancement based solely on audio signals, with the addition of EPG\nsignals further improving the performance. The late fusion strategy is deemed\nto be the most effective approach for simultaneous speech generation and\nenhancement.",
    "descriptor": "\nComments: Accepted By IEEE Signal Processing Letter\n",
    "authors": [
      "Li-Chin Chen",
      "Po-Hsun Chen",
      "Richard Tzong-Han Tsai",
      "Yu Tsao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07860"
  },
  {
    "id": "arXiv:2206.07861",
    "title": "Text normalization for endangered languages: the case of Ligurian",
    "abstract": "Text normalization is a crucial technology for low-resource languages which\nlack rigid spelling conventions. Low-resource text normalization has so far\nrelied upon hand-crafted rules, which are perceived to be more data efficient\nthan neural methods.\nIn this paper we examine the case of text normalization for Ligurian, an\nendangered Romance language. We collect 4,394 Ligurian sentences paired with\ntheir normalized versions, as well as the first monolingual corpus for\nLigurian. We show that, in spite of the small amounts of data available, a\ncompact transformer-based model can be trained to achieve very low error rates\nby the use of backtranslation and appropriate tokenization. Our datasets are\nreleased to the public.",
    "descriptor": "",
    "authors": [
      "Stefano Lusito",
      "Edoardo Ferrante",
      "Jean Maillard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07861"
  },
  {
    "id": "arXiv:2206.07862",
    "title": "Unifying Framework for Optimizations in non-boolean Formalisms",
    "abstract": "Search-optimization problems are plentiful in scientific and engineering\ndomains. Artificial intelligence has long contributed to the development of\nsearch algorithms and declarative programming languages geared towards solving\nand modeling search-optimization problems. Automated reasoning and knowledge\nrepresentation are the subfields of AI that are particularly vested in these\ndevelopments. Many popular automated reasoning paradigms provide users with\nlanguages supporting optimization statements. Recall integer linear\nprogramming, MaxSAT, optimization satisfiability modulo theory, and\n(constraint) answer set programming. These paradigms vary significantly in\ntheir languages in ways they express quality conditions on computed solutions.\nHere we propose a unifying framework of so called extended weight systems that\neliminates syntactic distinctions between paradigms. They allow us to see\nessential similarities and differences between optimization statements provided\nby distinct automated reasoning languages. We also study formal properties of\nthe proposed systems that immediately translate into formal properties of\nparadigms that can be captured within our framework. Under consideration in\nTheory and Practice of Logic Programming (TPLP).",
    "descriptor": "\nComments: Under consideration in Theory and Practice of Logic Programming (TPLP). arXiv admin note: text overlap with arXiv:2206.06440\n",
    "authors": [
      "Yuliya Lierler"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07862"
  },
  {
    "id": "arXiv:2206.07867",
    "title": "A visual introduction to information theory",
    "abstract": "Though originally developed for communications engineering, information\ntheory contains mathematical tools with numerous applications in science and\nengineering. These tools can be used to characterize the fundamental limits of\ndata compression and transmission in the presence of noise. Here, we present a\npractical guide to key concepts in information theory, focusing on intuitions\nand providing visual explanations wherever possible. Our presentation assumes\nonly a familiarity with basic probability theory.",
    "descriptor": "",
    "authors": [
      "Henry Pinkard",
      "Laura Waller"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07867"
  },
  {
    "id": "arXiv:2206.07869",
    "title": "Let Invariant Rationale Discovery Inspire Graph Contrastive Learning",
    "abstract": "Leading graph contrastive learning (GCL) methods perform graph augmentations\nin two fashions: (1) randomly corrupting the anchor graph, which could cause\nthe loss of semantic information, or (2) using domain knowledge to maintain\nsalient features, which undermines the generalization to other domains. Taking\nan invariance look at GCL, we argue that a high-performing augmentation should\npreserve the salient semantics of anchor graphs regarding\ninstance-discrimination. To this end, we relate GCL with invariant rationale\ndiscovery, and propose a new framework, Rationale-aware Graph Contrastive\nLearning (RGCL). Specifically, without supervision signals, RGCL uses a\nrationale generator to reveal salient features about graph\ninstance-discrimination as the rationale, and then creates rationale-aware\nviews for contrastive learning. This rationale-aware pre-training scheme endows\nthe backbone model with the powerful representation ability, further\nfacilitating the fine-tuning on downstream tasks. On MNIST-Superpixel and MUTAG\ndatasets, visual inspections on the discovered rationales showcase that the\nrationale generator successfully captures the salient features (i.e.\ndistinguishing semantic nodes in graphs). On biochemical molecule and social\nnetwork benchmark datasets, the state-of-the-art performance of RGCL\ndemonstrates the effectiveness of rationale-aware views for contrastive\nlearning. Our codes are available at https://github.com/lsh0520/RGCL.",
    "descriptor": "\nComments: Graph contrastive Learning, Invariant Rationale Discovery, ICML2022\n",
    "authors": [
      "Sihang Li",
      "Xiang Wang",
      "An zhang",
      "Yingxin Wu",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07869"
  },
  {
    "id": "arXiv:2206.07870",
    "title": "How to talk so your robot will learn: Instructions, descriptions, and  pragmatics",
    "abstract": "From the earliest years of our lives, humans use language to express our\nbeliefs and desires. Being able to talk to artificial agents about our\npreferences would thus fulfill a central goal of value alignment. Yet today, we\nlack computational models explaining such flexible and abstract language use.\nTo address this challenge, we consider social learning in a linear bandit\nsetting and ask how a human might communicate preferences over behaviors (i.e.\nthe reward function). We study two distinct types of language: instructions,\nwhich provide information about the desired policy, and descriptions, which\nprovide information about the reward function. To explain how humans use these\nforms of language, we suggest they reason about both known present and unknown\nfuture states: instructions optimize for the present, while descriptions\ngeneralize to the future. We formalize this choice by extending reward design\nto consider a distribution over states. We then define a pragmatic listener\nagent that infers the speaker's reward function by reasoning about how the\nspeaker expresses themselves. We validate our models with a behavioral\nexperiment, demonstrating that (1) our speaker model predicts spontaneous human\nbehavior, and (2) our pragmatic listener is able to recover their reward\nfunctions. Finally, we show that in traditional reinforcement learning\nsettings, pragmatic social learning can integrate with and accelerate\nindividual learning. Our findings suggest that social learning from a wider\nrange of language -- in particular, expanding the field's present focus on\ninstructions to include learning from descriptions -- is a promising approach\nfor value alignment and reinforcement learning more broadly.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Theodore R Sumers",
      "Robert D Hawkins",
      "Mark K Ho",
      "Thomas L Griffiths",
      "Dylan Hadfield-Menell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07870"
  },
  {
    "id": "arXiv:2206.07875",
    "title": "Optimization-Derived Learning with Essential Convergence Analysis of  Training and Hyper-training",
    "abstract": "Recently, Optimization-Derived Learning (ODL) has attracted attention from\nlearning and vision areas, which designs learning models from the perspective\nof optimization. However, previous ODL approaches regard the training and\nhyper-training procedures as two separated stages, meaning that the\nhyper-training variables have to be fixed during the training process, and thus\nit is also impossible to simultaneously obtain the convergence of training and\nhyper-training variables. In this work, we design a Generalized\nKrasnoselskii-Mann (GKM) scheme based on fixed-point iterations as our\nfundamental ODL module, which unifies existing ODL methods as special cases.\nUnder the GKM scheme, a Bilevel Meta Optimization (BMO) algorithmic framework\nis constructed to solve the optimal training and hyper-training variables\ntogether. We rigorously prove the essential joint convergence of the\nfixed-point iteration for training and the process of optimizing\nhyper-parameters for hyper-training, both on the approximation quality, and on\nthe stationary analysis. Experiments demonstrate the efficiency of BMO with\ncompetitive performance on sparse coding and real-world applications such as\nimage deconvolution and rain streak removal.",
    "descriptor": "\nComments: Accepted by ICML 2022\n",
    "authors": [
      "Risheng Liu",
      "Xuan Liu",
      "Shangzhi Zeng",
      "Jin Zhang",
      "Yixuan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07875"
  },
  {
    "id": "arXiv:2206.07876",
    "title": "Domain Generalization via Selective Consistency Regularization for Time  Series Classification",
    "abstract": "Domain generalization methods aim to learn models robust to domain shift with\ndata from a limited number of source domains and without access to target\ndomain samples during training. Popular domain alignment methods for domain\ngeneralization seek to extract domain-invariant features by minimizing the\ndiscrepancy between feature distributions across all domains, disregarding\ninter-domain relationships. In this paper, we instead propose a novel\nrepresentation learning methodology that selectively enforces prediction\nconsistency between source domains estimated to be closely-related.\nSpecifically, we hypothesize that domains share different class-informative\nrepresentations, so instead of aligning all domains which can cause negative\ntransfer, we only regularize the discrepancy between closely-related domains.\nWe apply our method to time-series classification tasks and conduct\ncomprehensive experiments on three public real-world datasets. Our method\nsignificantly improves over the baseline and achieves better or competitive\nperformance in comparison with state-of-the-art methods in terms of both\naccuracy and model calibration.",
    "descriptor": "\nComments: Accepted to ICPR 2022\n",
    "authors": [
      "Wenyu Zhang",
      "Mohamed Ragab",
      "Chuan-Sheng Foo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07876"
  },
  {
    "id": "arXiv:2206.07877",
    "title": "Roadblocks to Attracting Students to Software Testing Careers:  Comparisons of Replicated Studies",
    "abstract": "Context. Recently, a family of studies highlighted the unpopularity of\nsoftware testing careers among undergraduate students in software engineering\nand computer science courses. The original study and its replications explored\nthe perception of students in universities in four countries (Cana-da, China,\nIndia, and Malaysia), and indicated that most students do not consider a career\nin software testing as an option after graduation. This scenario represents a\nproblem for the software industry since the lack of skilled testing\nprofessionals might decrease the quality of software projects and increase the\nnumber of unsuccessful projects. Goal. The present study aims to replicate, in\nBrazil, the studies conducted in the other four countries to establish\ncomparisons and support the development of strategies to improve the visibility\nand importance of software testing among undergraduate students across the\nglobe. Method. We followed the same protocol in the original study to collect\ndata using a questionnaire and analyzed the answers using descriptive\nstatistics and qualitative data analysis. Results. Our findings indicate\nsimilarities among the results obtained in Brazil in comparison to those\nobtained from other countries. We observed that students are not motivated to\nfollow a testing career in the software industry based on a belief that testing\nactivities lack challenges and opportunities for continuous learning.\nConclusions. In summary, students seem to be interested in learning more about\nsoftware testing. However, the lack of discussions about the theme in software\ndevelopment courses, as well as the limited offer of courses focused on\nsoftware quality at the university level reduce the visibility of this area,\nwhich causes a decrease in the interest in this career.",
    "descriptor": "",
    "authors": [
      "Rodrigo E. C. Souza",
      "Ronnie E. de Souza Santos",
      "Luiz Fernando Capretz",
      "Marlon A. S. de Sousa",
      "Cleyton V. C. de Magalhaes"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.07877"
  },
  {
    "id": "arXiv:2206.07879",
    "title": "Extreme ratio between spectral and Frobenius norms of nonnegative  tensors",
    "abstract": "One of the fundamental problems in multilinear algebra, the minimum ratio\nbetween the spectral and Frobenius norms of tensors, has received considerable\nattention in recent years. While most values are unknown for real and complex\ntensors, the asymptotic order of magnitude and tight lower bounds have been\nestablished. However, little is known about nonnegative tensors. In this paper,\nwe present an almost complete picture of the ratio for nonnegative tensors. In\nparticular, we provide a tight lower bound that can be achieved by a wide class\nof nonnegative tensors under a simple necessary and sufficient condition, which\nhelps to characterize the extreme tensors and obtain results such as the\nasymptotic order of magnitude. We show that the ratio for symmetric tensors is\nno more than that for general tensors multiplied by a constant depending only\non the order of tensors, hence determining the asymptotic order of magnitude\nfor real, complex, and nonnegative symmetric tensors. We also find that the\nratio is in general different to the minimum ratio between the Frobenius and\nnuclear norms for nonnegative tensors, a sharp contrast to the case for real\ntensors and complex tensors.",
    "descriptor": "",
    "authors": [
      "Shengyu Cao",
      "Simai He",
      "Zhening Li",
      "Zhen Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07879"
  },
  {
    "id": "arXiv:2206.07882",
    "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural  Network Transducers via End-to-End 4-bit Quantization",
    "abstract": "We report on aggressive quantization strategies that greatly accelerate\ninference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit\ninteger representation for both weights and activations and apply Quantization\nAware Training (QAT) to retrain the full model (acoustic encoder and language\nmodel) and achieve near-iso-accuracy. We show that customized quantization\nschemes that are tailored to the local properties of the network are essential\nto achieve good performance while limiting the computational overhead of QAT.\nDensity ratio Language Model fusion has shown remarkable accuracy gains on\nRNN-T workloads but it severely increases the computational cost of inference.\nWe show that our quantization strategies enable using large beam widths for\nhypothesis search while achieving streaming-compatible runtimes and a full\nmodel compression ratio of 7.6$\\times$ compared to the full precision model.\nVia hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to\nINT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a\nReal Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03\ntest sets, we retain most of the gains associated with LM fusion, improving the\naverage WER by $>$1.5%.",
    "descriptor": "\nComments: 5 pages, 2 figures, 1 table. Paper accepted to Interspeech 2022\n",
    "authors": [
      "Andrea Fasoli",
      "Chia-Yu Chen",
      "Mauricio Serrano",
      "Swagath Venkataramani",
      "George Saon",
      "Xiaodong Cui",
      "Brian Kingsbury",
      "Kailash Gopalakrishnan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07882"
  },
  {
    "id": "arXiv:2206.07883",
    "title": "Pure Exploration of Causal Bandits",
    "abstract": "Causal bandit problem integrates causal inference with multi-armed bandits.\nThe pure exploration of causal bandits is the following online learning task:\ngiven a causal graph with unknown causal inference distributions, in each round\nwe can choose to either intervene one variable or do no intervention, and\nobserve the random outcomes of all random variables, with the goal that using\nas few rounds as possible, we can output an intervention that gives the best\n(or almost best) expected outcome on the reward variable $Y$ with probability\nat least $1-\\delta$, where $\\delta$ is a given confidence level. We provide\nfirst gap-dependent fully adaptive pure exploration algorithms on three types\nof causal models including parallel graphs, general graphs with small number of\nbackdoor parents, and binary generalized linear models. Our algorithms improve\nboth prior causal bandit algorithms, which are not adaptive to reward gaps, and\nprior adaptive pure exploration algorithms, which do not utilize the special\nfeatures of causal bandits.",
    "descriptor": "\nComments: 27 paages, 2 figures\n",
    "authors": [
      "Nuoya Xiong",
      "Wei Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07883"
  },
  {
    "id": "arXiv:2206.07886",
    "title": "Generalization Bounds for Data-Driven Numerical Linear Algebra",
    "abstract": "Data-driven algorithms can adapt their internal structure or parameters to\ninputs from unknown application-specific distributions, by learning from a\ntraining sample of inputs. Several recent works have applied this approach to\nproblems in numerical linear algebra, obtaining significant empirical gains in\nperformance. However, no theoretical explanation for their success was known.\nIn this work we prove generalization bounds for those algorithms, within the\nPAC-learning framework for data-driven algorithm selection proposed by Gupta\nand Roughgarden (SICOMP 2017). Our main results are closely matching upper and\nlower bounds on the fat shattering dimension of the learning-based low rank\napproximation algorithm of Indyk et al.~(NeurIPS 2019). Our techniques are\ngeneral, and provide generalization bounds for many other recently proposed\ndata-driven algorithms in numerical linear algebra, covering both\nsketching-based and multigrid-based methods. This considerably broadens the\nclass of data-driven algorithms for which a PAC-learning analysis is available.",
    "descriptor": "\nComments: COLT 2022\n",
    "authors": [
      "Peter Bartlett",
      "Piotr Indyk",
      "Tal Wagner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.07886"
  },
  {
    "id": "arXiv:2206.07887",
    "title": "Resilient Operational Planning for Microgrids Against Extreme Events",
    "abstract": "This paper proposes a novel resilience index, a microgrid survivability rate\n(SR) under extreme events, and then proposes a novel Resilient Operational\nPlanning (ROP) algorithm to maximize the proposed resilience index SR. The\nproposed ROP algorithm can incorporate predetermined inverter failure\nprobabilities and generate multiple scenarios accordingly to optimize resilient\noperations during an extreme event. The implemented ROP algorithm consists of\ntwo main steps: (i) optimization of resilient operational planning, and (ii)\npreventive resilience enhancement if minimum SR is not met per the analysis in\nstep 1. A typical microgrid (MG) is studied to compare the proposed ROP\nalgorithm against a traditional microgrid energy management (MEM) model.\nResults indicate that an enhanced resilience operation is achieved by the ROP\nalgorithm, which is demonstrated by the quantification of resilience via the\nSR. Moreover, the proposed ROP algorithm is able to obtain a greater SR overall\ncompared to that achieved by the traditional MEM, and this benefit of using the\nproposed ROP increases as the inverter failure probabilities increase.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Cunzhi Zhao",
      "Jesus Silva-Rodriguez",
      "Xingpeng Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07887"
  },
  {
    "id": "arXiv:2206.07889",
    "title": "Arnoldi-based orthonormal and hierarchical divergence-free polynomial  basis and its applications",
    "abstract": "This paper presents a methodology to construct a divergence-free polynomial\nbasis of an arbitrary degree in a simplex (triangles in 2D and tetrahedra in\n3D) of arbitrary dimension. It allows for fast computation of all numerical\nsolutions from degree zero to a specified degree \\textit{k} for certain PDEs.\nThe generated divergence-free basis is orthonormal, hierarchical, and robust in\nfinite-precision arithmetic. At the core is an Arnoldi-based procedure. It\nconstructs an orthonormal and hierarchical basis for multi-dimensional\npolynomials of degree less than or equal to \\textit{k}. The divergence-free\nbasis is generated by combining these polynomial basis functions. An efficient\nimplementation of the hybridized BDM mixed method is developed using these\nbasis functions. Hierarchy allows for incremental construction of the global\nmatrix and the global vector for all degrees (zero to \\textit{k}) using the\nlocal problem solution computed just for degree \\textit{k}. Orthonormality and\ndivergence-free properties simplify the local problem. PDEs considered are\nHelmholtz, Laplace, and Poisson problems in smooth domains and in a corner\ndomain. These advantages extend to other PDEs such as incompressible Stokes,\nincompressible Navier-Stokes, and Maxwell equations.",
    "descriptor": "",
    "authors": [
      "Sreevatsa Anantharamu",
      "Krishnan Mahesh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07889"
  },
  {
    "id": "arXiv:2206.07892",
    "title": "Max-Margin Works while Large Margin Fails: Generalization without  Uniform Convergence",
    "abstract": "A major challenge in modern machine learning is theoretically understanding\nthe generalization properties of overparameterized models. Many existing tools\nrely on \\em uniform convergence \\em (UC), a property that, when it holds,\nguarantees that the test loss will be close to the training loss, uniformly\nover a class of candidate models. Nagarajan and Kolter (2019) show that in\ncertain simple linear and neural-network settings, any uniform convergence\nbound will be vacuous, leaving open the question of how to prove generalization\nin settings where UC fails. Our main contribution is proving novel\ngeneralization bounds in two such settings, one linear, and one non-linear. We\nstudy the linear classification setting of Nagarajan and Kolter, and a\nquadratic ground truth function learned via a two-layer neural network in the\nnon-linear regime. We prove a new type of margin bound showing that above a\ncertain signal-to-noise threshold, any near-max-margin classifier will achieve\nalmost no test loss in these two settings. Our results show that\nnear-max-margin is important: while any model that achieves at least a $(1 -\n\\epsilon)$-fraction of the max-margin generalizes well, a classifier achieving\nhalf of the max-margin may fail terribly. We additionally strengthen the UC\nimpossibility results of Nagarajan and Kolter, proving that \\em one-sided \\em\nUC bounds and classical margin bounds will fail on near-max-margin classifiers.\nOur analysis provides insight on why memorization can coexist with\ngeneralization: we show that in this challenging regime where generalization\noccurs but UC fails, near-max-margin classifiers simultaneously contain some\ngeneralizable components and some overfitting components that memorize the\ndata. The presence of the overfitting components is enough to preclude UC, but\nthe near-extremal margin guarantees that sufficient generalizable components\nare present.",
    "descriptor": "",
    "authors": [
      "Margalit Glasgow",
      "Colin Wei",
      "Mary Wootters",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07892"
  },
  {
    "id": "arXiv:2206.07893",
    "title": "PeQuENet: Perceptual Quality Enhancement of Compressed Video with  Adaptation- and Attention-based Network",
    "abstract": "In this paper we propose a generative adversarial network (GAN) framework to\nenhance the perceptual quality of compressed videos. Our framework includes\nattention and adaptation to different quantization parameters (QPs) in a single\nmodel. The attention module exploits global receptive fields that can capture\nand align long-range correlations between consecutive frames, which can be\nbeneficial for enhancing perceptual quality of videos. The frame to be enhanced\nis fed into the deep network together with its neighboring frames, and in the\nfirst stage features at different depths are extracted. Then extracted features\nare fed into attention blocks to explore global temporal correlations, followed\nby a series of upsampling and convolution layers. Finally, the resulting\nfeatures are processed by the QP-conditional adaptation module which leverages\nthe corresponding QP information. In this way, a single model can be used to\nenhance adaptively to various QPs without requiring multiple models specific\nfor every QP value, while having similar performance. Experimental results\ndemonstrate the superior performance of the proposed PeQuENet compared with the\nstate-of-the-art compressed video quality enhancement algorithms.",
    "descriptor": "",
    "authors": [
      "Saiping Zhang",
      "Luis Herranz",
      "Marta Mrak",
      "Marc Gorriz Blanch",
      "Shuai Wan",
      "Fuzheng Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07893"
  },
  {
    "id": "arXiv:2206.07895",
    "title": "Dual-channel Early Warning Framework for Ethereum Ponzi Schemes",
    "abstract": "Blockchain technology supports the generation and record of transactions, and\nmaintains the fairness and openness of the cryptocurrency system. However, many\nfraudsters utilize smart contracts to create fraudulent Ponzi schemes for\nprofiting on Ethereum, which seriously affects financial security. Most\nexisting Ponzi scheme detection techniques suffer from two major restricted\nproblems: the lack of motivation for temporal early warning and failure to fuse\nmulti-source information finally cause the lagging and unsatisfactory\nperformance of Ethereum Ponzi scheme detection. In this paper, we propose a\ndual-channel early warning framework for Ethereum Ponzi schemes, named\nPonzi-Warning, which performs feature extraction and fusion on both code and\ntransaction levels. Moreover, we represent a temporal evolution augmentation\nstrategy for generating transaction graph sequences, which can effectively\nincrease the data scale and introduce temporal information. Comprehensive\nexperiments on our Ponzi scheme datasets demonstrate the effectiveness and\ntimeliness of our framework for detecting the Ponzi contract accounts.",
    "descriptor": "",
    "authors": [
      "Jie Jin",
      "Jiajun Zhou",
      "Chengxiang Jin",
      "Shanqing Yu",
      "Ziwan Zheng",
      "Qi Xuan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07895"
  },
  {
    "id": "arXiv:2206.07896",
    "title": "CuPBoP: CUDA for Parallelized and Broad-range Processors",
    "abstract": "CUDA is one of the most popular choices for GPU programming, but it can only\nbe executed on NVIDIA GPUs. Executing CUDA on non-NVIDIA devices not only\nbenefits the hardware community, but also allows data-parallel computation in\nheterogeneous systems. To make CUDA programs portable, some researchers have\nproposed using source-to-source translators to translate CUDA to portable\nprogramming languages that can be executed on non-NVIDIA devices. However, most\nCUDA translators require additional manual modifications on the translated\ncode, which imposes a heavy workload on developers. In this paper, CuPBoP is\nproposed to execute CUDA on non-NVIDIA devices without relying on any portable\nprogramming languages. Compared with existing work that executes CUDA on\nnon-NVIDIA devices, CuPBoP does not require manual modification of the CUDA\nsource code, but it still achieves the highest coverage (69.6%), much higher\nthan existing frameworks (56.6%) on the Rodinia benchmark. In particular, for\nCPU backends, CuPBoP supports several ISAs (e.g., X86, RISC-V, AArch64) and has\nclose or even higher performance compared with other projects. We also compare\nand analyze the performance among CuPBoP, manually optimized OpenMP/MPI\nprograms, and CUDA programs on the latest Ampere architecture GPU, and show\nfuture directions for supporting CUDA programs on non-NVIDIA devices with high\nperformance",
    "descriptor": "",
    "authors": [
      "Ruobing Han",
      "Jun Chen",
      "Bhanu Garg",
      "Jeffrey Young",
      "Jaewoong Sim",
      "Hyesoon Kim"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.07896"
  },
  {
    "id": "arXiv:2206.07897",
    "title": "Dual Contrastive Attributed Graph Clustering Network",
    "abstract": "Attributed graph clustering is one of the most important tasks in graph\nanalysis field, the goal of which is to group nodes with similar\nrepresentations into the same cluster without manual guidance. Recent studies\nbased on graph contrastive learning have achieved impressive results in\nprocessing graph-structured data. However, existing graph contrastive learning\nbased methods 1) do not directly address the clustering task, since the\nrepresentation learning and clustering process are separated; 2) depend too\nmuch on graph data augmentation, which greatly limits the capability of\ncontrastive learning; 3) ignore the contrastive message for subspace\nclustering. To accommodate the aforementioned issues, we propose a generic\nframework called Dual Contrastive Attributed Graph Clustering Network (DCAGC).\nIn DCAGC, by leveraging Neighborhood Contrast Module, the similarity of the\nneighbor nodes will be maximized and the quality of the node representation\nwill be improved. Meanwhile, the Contrastive Self-Expression Module is built by\nminimizing the node representation before and after the reconstruction of the\nself-expression layer to obtain a discriminative self-expression matrix for\nspectral clustering. All the modules of DCAGC are trained and optimized in a\nunified framework, so the learned node representation contains\nclustering-oriented messages. Extensive experimental results on four attributed\ngraph datasets show the superiority of DCAGC compared with 16 state-of-the-art\nclustering methods. The code of this paper is available at\nhttps://github.com/wangtong627/Dual-Contrastive-Attributed-Graph-Clustering-Network.",
    "descriptor": "",
    "authors": [
      "Tong Wang",
      "Guanyu Yang",
      "Junhua Wu",
      "Qijia He",
      "Zhenquan Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07897"
  },
  {
    "id": "arXiv:2206.07898",
    "title": "Multimodal Dialogue State Tracking",
    "abstract": "Designed for tracking user goals in dialogues, a dialogue state tracker is an\nessential component in a dialogue system. However, the research of dialogue\nstate tracking has largely been limited to unimodality, in which slots and slot\nvalues are limited by knowledge domains (e.g. restaurant domain with slots of\nrestaurant name and price range) and are defined by specific database schema.\nIn this paper, we propose to extend the definition of dialogue state tracking\nto multimodality. Specifically, we introduce a novel dialogue state tracking\ntask to track the information of visual objects that are mentioned in\nvideo-grounded dialogues. Each new dialogue utterance may introduce a new video\nsegment, new visual objects, or new object attributes, and a state tracker is\nrequired to update these information slots accordingly. We created a new\nsynthetic benchmark and designed a novel baseline, Video-Dialogue Transformer\nNetwork (VDTN), for this task. VDTN combines both object-level features and\nsegment-level features and learns contextual dependencies between videos and\ndialogues to generate multimodal dialogue states. We optimized VDTN for a state\ngeneration task as well as a self-supervised video understanding task which\nrecovers video segment or object representations. Finally, we trained VDTN to\nuse the decoded states in a response prediction task. Together with\ncomprehensive ablation and qualitative analysis, we discovered interesting\ninsights towards building more capable multimodal dialogue systems.",
    "descriptor": "\nComments: Accepted at NAACL 2022 (Oral)\n",
    "authors": [
      "Hung Le",
      "Nancy F. Chen",
      "Steven C.H. Hoi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07898"
  },
  {
    "id": "arXiv:2206.07902",
    "title": "On Privacy and Personalization in Cross-Silo Federated Learning",
    "abstract": "While the application of differential privacy (DP) has been well-studied in\ncross-device federated learning (FL), there is a lack of work considering DP\nfor cross-silo FL, a setting characterized by a limited number of clients each\ncontaining many data subjects. In cross-silo FL, usual notions of client-level\nprivacy are less suitable as real-world privacy regulations typically concern\nin-silo data subjects rather than the silos themselves. In this work, we\ninstead consider the more realistic notion of silo-specific item-level privacy,\nwhere silos set their own privacy targets for their local examples. Under this\nsetting, we reconsider the roles of personalization in federated learning. In\nparticular, we show that mean-regularized multi-task learning (MR-MTL), a\nsimple personalization framework, is a strong baseline for cross-silo FL: under\nstronger privacy, silos are further incentivized to \"federate\" with each other\nto mitigate DP noise, resulting in consistent improvements relative to standard\nbaseline methods. We provide a thorough empirical study of competing methods as\nwell as a theoretical characterization of MR-MTL for a mean estimation problem,\nhighlighting the interplay between privacy and cross-silo data heterogeneity.\nOur work serves to establish baselines for private cross-silo FL as well as\nidentify key directions of future work in this area.",
    "descriptor": "",
    "authors": [
      "Ziyu Liu",
      "Shengyuan Hu",
      "Zhiwei Steven Wu",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07902"
  },
  {
    "id": "arXiv:2206.07904",
    "title": "Explainable Models via Compression of Tree Ensembles",
    "abstract": "Ensemble models (bagging and gradient-boosting) of relational decision trees\nhave proved to be one of the most effective learning methods in the area of\nprobabilistic logic models (PLMs). While effective, they lose one of the most\nimportant aspect of PLMs -- interpretability. In this paper we consider the\nproblem of compressing a large set of learned trees into a single explainable\nmodel. To this effect, we propose CoTE -- Compression of Tree Ensembles -- that\nproduces a single small decision list as a compressed representation. CoTE\nfirst converts the trees to decision lists and then performs the combination\nand compression with the aid of the original training set. An experimental\nevaluation demonstrates the effectiveness of CoTE in several benchmark\nrelational data sets.",
    "descriptor": "\nComments: 24 pages, 14 figures\n",
    "authors": [
      "Siwen Yan",
      "Sriraam Natarajan",
      "Saket Joshi",
      "Roni Khardon",
      "Prasad Tadepalli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07904"
  },
  {
    "id": "arXiv:2206.07908",
    "title": "Simultaneously Learning Stochastic and Adversarial Bandits with General  Graph Feedback",
    "abstract": "The problem of online learning with graph feedback has been extensively\nstudied in the literature due to its generality and potential to model various\nlearning tasks. Existing works mainly study the adversarial and stochastic\nfeedback separately. If the prior knowledge of the feedback mechanism is\nunavailable or wrong, such specially designed algorithms could suffer great\nloss. To avoid this problem, \\citet{erez2021towards} try to optimize for both\nenvironments. However, they assume the feedback graphs are undirected and each\nvertex has a self-loop, which compromises the generality of the framework and\nmay not be satisfied in applications. With a general feedback graph, the\nobservation of an arm may not be available when this arm is pulled, which makes\nthe exploration more expensive and the algorithms more challenging to perform\noptimally in both environments. In this work, we overcome this difficulty by a\nnew trade-off mechanism with a carefully-designed proportion for exploration\nand exploitation. We prove the proposed algorithm simultaneously achieves\n$\\mathrm{poly} \\log T$ regret in the stochastic setting and minimax-optimal\nregret of $\\tilde{O}(T^{2/3})$ in the adversarial setting where $T$ is the\nhorizon and $\\tilde{O}$ hides parameters independent of $T$ as well as\nlogarithmic terms. To our knowledge, this is the first best-of-both-worlds\nresult for general feedback graphs.",
    "descriptor": "\nComments: Accepted in ICML 2022\n",
    "authors": [
      "Fang Kong",
      "Yichi Zhou",
      "Shuai Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07908"
  },
  {
    "id": "arXiv:2206.07910",
    "title": "Introducing the Huber mechanism for differentially private low-rank  matrix completion",
    "abstract": "Performing low-rank matrix completion with sensitive user data calls for\nprivacy-preserving approaches. In this work, we propose a novel noise addition\nmechanism for preserving differential privacy where the noise distribution is\ninspired by Huber loss, a well-known loss function in robust statistics. The\nproposed Huber mechanism is evaluated against existing differential privacy\nmechanisms while solving the matrix completion problem using the Alternating\nLeast Squares approach. We also propose using the Iteratively Re-Weighted Least\nSquares algorithm to complete low-rank matrices and study the performance of\ndifferent noise mechanisms in both synthetic and real datasets. We prove that\nthe proposed mechanism achieves {\\epsilon}-differential privacy similar to the\nLaplace mechanism. Furthermore, empirical results indicate that the Huber\nmechanism outperforms Laplacian and Gaussian in some cases and is comparable,\notherwise.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "R Adithya Gowtham",
      "Gokularam M",
      "Thulasi Tholeti",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07910"
  },
  {
    "id": "arXiv:2206.07912",
    "title": "Double Sampling Randomized Smoothing",
    "abstract": "Neural networks (NNs) are known to be vulnerable against adversarial\nperturbations, and thus there is a line of work aiming to provide robustness\ncertification for NNs, such as randomized smoothing, which samples smoothing\nnoises from a certain distribution to certify the robustness for a smoothed\nclassifier. However, as previous work shows, the certified robust radius in\nrandomized smoothing suffers from scaling to large datasets (\"curse of\ndimensionality\"). To overcome this hurdle, we propose a Double Sampling\nRandomized Smoothing (DSRS) framework, which exploits the sampled probability\nfrom an additional smoothing distribution to tighten the robustness\ncertification of the previous smoothed classifier. Theoretically, under mild\nassumptions, we prove that DSRS can certify $\\Theta(\\sqrt d)$ robust radius\nunder $\\ell_2$ norm where $d$ is the input dimension, which implies that DSRS\nmay be able to break the curse of dimensionality of randomized smoothing. We\ninstantiate DSRS for a generalized family of Gaussian smoothing and propose an\nefficient and sound computing method based on customized dual optimization\nconsidering sampling error. Extensive experiments on MNIST, CIFAR-10, and\nImageNet verify our theory and show that DSRS certifies larger robust radii\nthan existing baselines consistently under different settings. Code is\navailable at https://github.com/llylly/DSRS.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Linyi Li",
      "Jiawei Zhang",
      "Tao Xie",
      "Bo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.07912"
  },
  {
    "id": "arXiv:2206.07915",
    "title": "Barrier Certified Safety Learning Control: When Sum-of-Square  Programming Meets Reinforcement Learning",
    "abstract": "Safety guarantee is essential in many engineering implementations.\nReinforcement learning provides a useful way to strengthen safety. However,\nreinforcement learning algorithms cannot completely guarantee safety over\nrealistic operations. To address this issue, this work adopts control barrier\nfunctions over reinforcement learning, and proposes a compensated algorithm to\ncompletely maintain safety. Specifically, a sum-of-squares programming has been\nexploited to search for the optimal controller, and tune the learning\nhyperparameters simultaneously. Thus, the control actions are pledged to be\nalways within the safe region. The effectiveness of proposed method is\ndemonstrated via an inverted pendulum model. Compared to quadratic programming\nbased reinforcement learning methods, our sum-of-squares programming based\nreinforcement learning has shown its superiority.",
    "descriptor": "\nComments: 6 pages, 6 figures, CCTA2022 conference\n",
    "authors": [
      "Hejun Huang",
      "Zhenglong Li",
      "Dongkun Han"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07915"
  },
  {
    "id": "arXiv:2206.07918",
    "title": "\"Understanding Robustness Lottery\": A Comparative Visual Analysis of  Neural Network Pruning Approaches",
    "abstract": "Deep learning approaches have provided state-of-the-art performance in many\napplications by relying on extremely large and heavily overparameterized neural\nnetworks. However, such networks have been shown to be very brittle, not\ngeneralize well to new uses cases, and are often difficult if not impossible to\ndeploy on resources limited platforms. Model pruning, i.e., reducing the size\nof the network, is a widely adopted strategy that can lead to more robust and\ngeneralizable network -- usually orders of magnitude smaller with the same or\neven improved performance. While there exist many heuristics for model pruning,\nour understanding of the pruning process remains limited. Empirical studies\nshow that some heuristics improve performance while others can make models more\nbrittle or have other side effects. This work aims to shed light on how\ndifferent pruning methods alter the network's internal feature representation,\nand the corresponding impact on model performance. To provide a meaningful\ncomparison and characterization of model feature space, we use three geometric\nmetrics that are decomposed from the common adopted classification loss. With\nthese metrics, we design a visualization system to highlight the impact of\npruning on model prediction as well as the latent feature embedding. The\nproposed tool provides an environment for exploring and studying differences\namong pruning methods and between pruned and original model. By leveraging our\nvisualization, the ML researchers can not only identify samples that are\nfragile to model pruning and data corruption but also obtain insights and\nexplanations on how some pruned models achieve superior robustness performance.",
    "descriptor": "",
    "authors": [
      "Zhimin Li",
      "Shusen Liu",
      "Xin Yu",
      "Kailkhura Bhavya",
      "Jie Cao",
      "Diffenderfer James Daniel",
      "Peer-Timo Bremer",
      "Valerio Pascucci"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07918"
  },
  {
    "id": "arXiv:2206.07919",
    "title": "An Empirical Study on the Effectiveness of Data Resampling Approaches  for Cross-Project Software Defect Prediction",
    "abstract": "Crossp-roject defect prediction (CPDP), where data from different software\nprojects are used to predict defects, has been proposed as a way to provide\ndata for software projects that lack historical data. Evaluations of CPDP\nmodels using the Nearest Neighbour (NN) Filter approach have shown promising\nresults in recent studies. A key challenge with defect-prediction datasets is\nclass imbalance, that is highly skewed datasets where non buggy modules\ndominate the buggy modules. In the past, data resampling approaches have been\napplied to within-projects defect prediction models to help alleviate the\nnegative effects of class imbalance in the datasets. To address the class\nimbalance issue in CPDP, the authors assess the impact of data resampling\napproaches on CPDP models after the NN Filter is applied. The impact on\nprediction performance of five oversampling approaches (MAHAKIL, SMOTE,\nBorderline-SMOTE, Random Oversampling, and ADASYN) and three undersampling\napproaches (Random Undersampling, Tomek Links, and Onesided selection) is\ninvestigated and results are compared to approaches without data resampling.\nThe authors' examined six defect prediction models on 34 datasets extracted\nfrom the PROMISE repository. The authors results show that there is a\nsignificant positive effect of data resampling on CPDP performance, suggesting\nthat software quality teams and researchers should consider applying data\nresampling approaches for improved recall (pd) and g-measure prediction\nperformance. However if the goal is to improve precision and reduce false alarm\n(pf) then data resampling approaches should be avoided.",
    "descriptor": "\nComments: Journal article, 14 pages, 5 tables, 4 figures\n",
    "authors": [
      "Kwabena Ebo Bennin",
      "Amjed Tahir",
      "Stephen G. MacDonell",
      "J\u00fcrgen B\u00f6rstler"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.07919"
  },
  {
    "id": "arXiv:2206.07920",
    "title": "PInKS: Preconditioned Commonsense Inference with Minimal Supervision",
    "abstract": "Reasoning with preconditions such as \"glass can be used for drinking water\nunless the glass is shattered\" remains an open problem for language models. The\nmain challenge lies in the scarcity of preconditions data and the model's lack\nof support for such reasoning. We present PInKS, Preconditioned Commonsense\nInference with WeaK Supervision, an improved model for reasoning with\npreconditions through minimum supervision. We show, both empirically and\ntheoretically, that PInKS improves the results on benchmarks focused on\nreasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1\nscores). We further investigate PInKS through PAC-Bayesian informativeness\nanalysis, precision measures, and ablation study.",
    "descriptor": "",
    "authors": [
      "Ehsan Qasemi",
      "Piyush Khanna",
      "Qiang Ning",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07920"
  },
  {
    "id": "arXiv:2206.07922",
    "title": "Challenges and Opportunities in Deep Reinforcement Learning with Graph  Neural Networks: A Comprehensive review of Algorithms and Applications",
    "abstract": "Deep reinforcement learning (DRL) has empowered a variety of artificial\nintelligence fields, including pattern recognition, robotics,\nrecommendation-systems, and gaming. Similarly, graph neural networks (GNN) have\nalso demonstrated their superior performance in supervised learning for\ngraph-structured data. In recent times, the fusion of GNN with DRL for\ngraph-structured environments has attracted a lot of attention. This paper\nprovides a comprehensive review of these hybrid works. These works can be\nclassified into two categories: (1) algorithmic enhancement, where DRL and GNN\ncomplement each other for better utility; (2) application-specific enhancement,\nwhere DRL and GNN support each other. This fusion effectively addresses various\ncomplex problems in engineering and life sciences. Based on the review, we\nfurther analyze the applicability and benefits of fusing these two domains,\nespecially in terms of increasing generalizability and reducing computational\ncomplexity. Finally, the key challenges in integrating DRL and GNN, and\npotential future research directions are highlighted, which will be of interest\nto the broader machine learning community.",
    "descriptor": "\nComments: 16 pages, 2 figures, 2 tables\n",
    "authors": [
      "Sai Munikoti",
      "Deepesh Agarwal",
      "Laya Das",
      "Mahantesh Halappanavar",
      "Balasubramaniam Natarajan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07922"
  },
  {
    "id": "arXiv:2206.07923",
    "title": "Anonymous Expression in an Online Community for Women in China",
    "abstract": "Gender issues faced by women can range from workplace harassment to domestic\nviolence. While publicly disclosing these issues on social media can be hard,\nsome may incline to express themselves anonymously. We approached such an\nanonymous female community on Chinese social media where discussion on gender\nissues takes place with a qualitative content analysis. By observing anonymous\nexperiences contributed by female users and made publicly available by an\ninfluencer, we identified 20 issues commonly discussed, with cheating-partner,\ncontrolling parents and age anxiety taking the lead. The results are placed\ninto context with Chinese culture and expectations about gender. By describing\nthe results in context with the social challenges faced by women in China, and\nunderstanding how these issues are anonymously and openly discussed by them, we\naim to motivate more policies and platform designs to accommodate the needs of\nthe affected population.",
    "descriptor": "",
    "authors": [
      "Zhixuan Zhou",
      "Zixin Wang",
      "Franziska Zimmer"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.07923"
  },
  {
    "id": "arXiv:2206.07932",
    "title": "Lifelong Wandering: A realistic few-shot online continual learning  setting",
    "abstract": "Online few-shot learning describes a setting where models are trained and\nevaluated on a stream of data while learning emerging classes. While prior work\nin this setting has achieved very promising performance on instance\nclassification when learning from data-streams composed of a single indoor\nenvironment, we propose to extend this setting to consider object\nclassification on a series of several indoor environments, which is likely to\noccur in applications such as robotics. Importantly, our setting, which we\nrefer to as online few-shot continual learning, injects the well-studied issue\nof catastrophic forgetting into the few-shot online learning paradigm. In this\nwork, we benchmark several existing methods and adapted baselines within our\nsetting, and show there exists a trade-off between catastrophic forgetting and\nonline performance. Our findings motivate the need for future work in this\nsetting, which can achieve better online performance without catastrophic\nforgetting.",
    "descriptor": "\nComments: CVPR 2022 Workshop on Continual Learning\n",
    "authors": [
      "Mayank Lunayach",
      "James Smith",
      "Zsolt Kira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07932"
  },
  {
    "id": "arXiv:2206.07934",
    "title": "Technical Report for Argoverse2 Challenge 2022 -- Motion Forecasting  Task",
    "abstract": "We propose a motion forecasting model called BANet, which means\nBoundary-Aware Network, and it is a variant of LaneGCN. We believe that it is\nnot enough to use only the lane centerline as input to obtain the embedding\nfeatures of the vector map nodes. The lane centerline can only provide the\ntopology of the lanes, and other elements of the vector map also contain rich\ninformation. For example, the lane boundary can provide traffic rule constraint\ninformation such as whether it is possible to change lanes which is very\nimportant. Therefore, we achieved better performance by encoding more vector\nmap elements in the motion forecasting model.We report our results on the 2022\nArgoverse2 Motion Forecasting challenge and rank 2nd on the test leaderboard.",
    "descriptor": "",
    "authors": [
      "Chen Zhang",
      "Honglin Sun",
      "Chen Chen",
      "Yandong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.07934"
  },
  {
    "id": "arXiv:2206.07940",
    "title": "PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series",
    "abstract": "Probabilistic hierarchical time-series forecasting is an important variant of\ntime-series forecasting, where the goal is to model and forecast multivariate\ntime-series that have underlying hierarchical relations. Most methods focus on\npoint predictions and do not provide well-calibrated probabilistic forecasts\ndistributions. Recent state-of-art probabilistic forecasting methods also\nimpose hierarchical relations on point predictions and samples of distribution\nwhich does not account for coherency of forecast distributions. Previous works\nalso silently assume that datasets are always consistent with given\nhierarchical relations and do not adapt to real-world datasets that show\ndeviation from this assumption. We close both these gaps and propose PROFHIT,\nwhich is a fully probabilistic hierarchical forecasting model that jointly\nmodels forecast distribution of entire hierarchy. PROFHIT uses a flexible\nprobabilistic Bayesian approach and introduces a novel Distributional Coherency\nregularization to learn from hierarchical relations for entire forecast\ndistribution that enables robust and calibrated forecasts as well as adapt to\ndatasets of varying hierarchical consistency. On evaluating PROFHIT over wide\nrange of datasets, we observed 41-88% better performance in accuracy and\ncalibration. Due to modeling the coherency over full distribution, we observed\nthat PROFHIT can robustly provide reliable forecasts even if up to 10% of input\ntime-series data is missing where other methods' performance severely degrade\nby over 70%.",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Harshavardhan Kamarthi",
      "Lingkai Kong",
      "Alexander Rodr\u00edguez",
      "Chao Zhang",
      "B. Aditya Prakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07940"
  },
  {
    "id": "arXiv:2206.07948",
    "title": "Forming Effective Human-AI Teams: Building Machine Learning Models that  Complement the Capabilities of Multiple Experts",
    "abstract": "Machine learning (ML) models are increasingly being used in application\ndomains that often involve working together with human experts. In this\ncontext, it can be advantageous to defer certain instances to a single human\nexpert when they are difficult to predict for the ML model. While previous work\nhas focused on scenarios with one distinct human expert, in many real-world\nsituations several human experts with varying capabilities may be available. In\nthis work, we propose an approach that trains a classification model to\ncomplement the capabilities of multiple human experts. By jointly training the\nclassifier together with an allocation system, the classifier learns to\naccurately predict those instances that are difficult for the human experts,\nwhile the allocation system learns to pass each instance to the most suitable\nteam member -- either the classifier or one of the human experts. We evaluate\nour proposed approach in multiple experiments on public datasets with\n\"synthetic\" experts and a real-world medical dataset annotated by multiple\nradiologists. Our approach outperforms prior work and is more accurate than the\nbest human expert or a classifier. Furthermore, it is flexibly adaptable to\nteams of varying sizes and different levels of expert diversity.",
    "descriptor": "\nComments: Accepted at IJCAI 2022\n",
    "authors": [
      "Patrick Hemmer",
      "Sebastian Schellhammer",
      "Michael V\u00f6ssing",
      "Johannes Jakubik",
      "Gerhard Satzger"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07948"
  },
  {
    "id": "arXiv:2206.07951",
    "title": "Predicting Geometric Errors and Failures in Additive Manufacturing",
    "abstract": "Additive manufacturing is a process that has facilitated the cost effective\nproduction of complicated designs. Objects fabricated via additive\nmanufacturing technologies often suffer from dimensional accuracy issues and\nother part specific problems such as thin part robustness, overhang geometries\nthat may collapse, support structures that cannot be removed, engraved and\nembossed details that are indistinguishable. In this work we present an\napproach to predict the dimensional accuracy per vertex and per part.\nFurthermore, we provide a framework for estimating the probability that a model\nis fabricated correctly via an additive manufacturing technology for a specific\napplication. This framework can be applied to several 3D printing technologies\nand applications. In the context of this paper, a thorough experimental\nevaluation is presented for binder jetting technology and applications.",
    "descriptor": "\nComments: Submitted for journal publication. Contains links to source code for error prediction ANN and printability estimation tool\n",
    "authors": [
      "Ioannis Fudos",
      "Spyros Moschopoulos",
      "Margarita Ntousia",
      "Vasiliki Stamati"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.07951"
  },
  {
    "id": "arXiv:2206.07953",
    "title": "Analysis and Extensions of Adversarial Training for Video Classification",
    "abstract": "Adversarial training (AT) is a simple yet effective defense against\nadversarial attacks to image classification systems, which is based on\naugmenting the training set with attacks that maximize the loss. However, the\neffectiveness of AT as a defense for video classification has not been\nthoroughly studied. Our first contribution is to show that generating optimal\nattacks for video requires carefully tuning the attack parameters, especially\nthe step size. Notably, we show that the optimal step size varies linearly with\nthe attack budget. Our second contribution is to show that using a smaller\n(sub-optimal) attack budget at training time leads to a more robust performance\nat test time. Based on these findings, we propose three defenses against\nattacks with variable attack budgets. The first one, Adaptive AT, is a\ntechnique where the attack budget is drawn from a distribution that is adapted\nas training iterations proceed. The second, Curriculum AT, is a technique where\nthe attack budget is increased as training iterations proceed. The third,\nGenerative AT, further couples AT with a denoising generative adversarial\nnetwork to boost robust performance. Experiments on the UCF101 dataset\ndemonstrate that the proposed methods improve adversarial robustness against\nmultiple attack types.",
    "descriptor": "",
    "authors": [
      "Kaleab A. Kinfu",
      "Ren\u00e9 Vidal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07953"
  },
  {
    "id": "arXiv:2206.07956",
    "title": "Automatic Prosody Annotation with Pre-Trained Text-Speech Model",
    "abstract": "Prosodic boundary plays an important role in text-to-speech synthesis (TTS)\nin terms of naturalness and readability. However, the acquisition of prosodic\nboundary labels relies on manual annotation, which is costly and\ntime-consuming. In this paper, we propose to automatically extract prosodic\nboundary labels from text-audio data via a neural text-speech model with\npre-trained audio encoders. This model is pre-trained on text and speech data\nseparately and jointly fine-tuned on TTS data in a triplet format: {speech,\ntext, prosody}. The experimental results on both automatic evaluation and human\nevaluation demonstrate that: 1) the proposed text-speech prosody annotation\nframework significantly outperforms text-only baselines; 2) the quality of\nautomatic prosodic boundary annotations is comparable to human annotations; 3)\nTTS systems trained with model-annotated boundaries are slightly better than\nsystems that use manual ones.",
    "descriptor": "\nComments: accepted by INTERSPEECH2022\n",
    "authors": [
      "Ziqian Dai",
      "Jianwei Yu",
      "Yan Wang",
      "Nuo Chen",
      "Yanyao Bian",
      "Guangzhi Li",
      "Deng Cai",
      "Dong Yu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.07956"
  },
  {
    "id": "arXiv:2206.07959",
    "title": "A Simple Baseline for BEV Perception Without LiDAR",
    "abstract": "Building 3D perception systems for autonomous vehicles that do not rely on\nLiDAR is a critical research problem because of the high expense of LiDAR\nsystems compared to cameras and other sensors. Current methods use multi-view\nRGB data collected from cameras around the vehicle and neurally \"lift\" features\nfrom the perspective images to the 2D ground plane, yielding a \"bird's eye\nview\" (BEV) feature representation of the 3D space around the vehicle. Recent\nresearch focuses on the way the features are lifted from images to the BEV\nplane. We instead propose a simple baseline model, where the \"lifting\" step\nsimply averages features from all projected image locations, and find that it\noutperforms the current state-of-the-art in BEV vehicle segmentation. Our\nablations show that batch size, data augmentation, and input resolution play a\nlarge part in performance. Additionally, we reconsider the utility of radar\ninput, which has previously been either ignored or found non-helpful by recent\nworks. With a simple RGB-radar fusion module, we obtain a sizable boost in\nperformance, approaching the accuracy of a LiDAR-enabled system.",
    "descriptor": "",
    "authors": [
      "Adam W. Harley",
      "Zhaoyuan Fang",
      "Jie Li",
      "Rares Ambrus",
      "Katerina Fragkiadaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07959"
  },
  {
    "id": "arXiv:2206.07960",
    "title": "Towards Better Understanding with Uniformity and Explicit Regularization  of Embeddings in Embedding-based Neural Topic Models",
    "abstract": "Embedding-based neural topic models could explicitly represent words and\ntopics by embedding them to a homogeneous feature space, which shows higher\ninterpretability. However, there are no explicit constraints for the training\nof embeddings, leading to a larger optimization space. Also, a clear\ndescription of the changes in embeddings and the impact on model performance is\nstill lacking. In this paper, we propose an embedding regularized neural topic\nmodel, which applies the specially designed training constraints on word\nembedding and topic embedding to reduce the optimization space of parameters.\nTo reveal the changes and roles of embeddings, we introduce \\textbf{uniformity}\ninto the embedding-based neural topic model as the evaluation metric of\nembedding space. On this basis, we describe how embeddings tend to change\nduring training via the changes in the uniformity of embeddings. Furthermore,\nwe demonstrate the impact of changes in embeddings in embedding-based neural\ntopic models through ablation studies. The results of experiments on two\nmainstream datasets indicate that our model significantly outperforms baseline\nmodels in terms of the harmony between topic quality and document modeling.\nThis work is the first attempt to exploit uniformity to explore changes in\nembeddings of embedding-based neural topic models and their impact on model\nperformance to the best of our knowledge.",
    "descriptor": "\nComments: IJCNN 2022\n",
    "authors": [
      "Wei Shao",
      "Lei Huang",
      "Shuqi Liu",
      "Shihua Ma",
      "Linqi Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07960"
  },
  {
    "id": "arXiv:2206.07967",
    "title": "DreamNet: A Deep Riemannian Network based on SPD Manifold Learning for  Visual Classification",
    "abstract": "Image set-based visual classification methods have achieved remarkable\nperformance, via characterising the image set in terms of a non-singular\ncovariance matrix on a symmetric positive definite (SPD) manifold. To adapt to\ncomplicated visual scenarios better, several Riemannian networks (RiemNets) for\nSPD matrix nonlinear processing have recently been studied. However, it is\npertinent to ask, whether greater accuracy gains can be achieved by simply\nincreasing the depth of RiemNets. The answer appears to be negative, as deeper\nRiemNets tend to lose generalization ability. To explore a possible solution to\nthis issue, we propose a new architecture for SPD matrix learning.\nSpecifically, to enrich the deep representations, we adopt SPDNet [1] as the\nbackbone, with a stacked Riemannian autoencoder (SRAE) built on the tail. The\nassociated reconstruction error term can make the embedding functions of both\nSRAE and of each RAE an approximate identity mapping, which helps to prevent\nthe degradation of statistical information. We then insert several\nresidual-like blocks with shortcut connections to augment the representational\ncapacity of SRAE, and to simplify the training of a deeper network. The\nexperimental evidence demonstrates that our DreamNet can achieve improved\naccuracy with increased depth of the network.",
    "descriptor": "\nComments: 9 pages, 7 figures\n",
    "authors": [
      "Rui Wang",
      "Xiao-Jun Wu",
      "Ziheng Chen",
      "Tianyang Xu",
      "Josef Kittler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07967"
  },
  {
    "id": "arXiv:2206.07970",
    "title": "Well-posedness and variational numerical scheme for an adaptive model in  highly heterogeneous porous media",
    "abstract": "Mathematical modeling of fluid flow in a porous medium is usually described\nby a continuity equation and a chosen constitutive law. The latter, depending\non the problem at hand, may be a nonlinear relation between the fluid's\npressure gradient and velocity. The actual shape of this relation is normally\nchosen at the outset of the problem, even though, in practice, the fluid may\nexperience velocities outside of its range of applicability. We propose here an\nadaptive model, so that the most appropriate law is locally selected depending\non the computed velocity. From the analytical point of view, we show\nwell-posedness of the problem when the law is monotone in velocity and show\nexistence in one space dimension otherwise. From the computational point of\nview, we present a new approach based on regularizing via mollification the\nunderlying dissipation, i.e., the power lost by the fluid to the porous medium\nthrough drag. The resulting regularization is shown to converge to the original\nproblem using $\\Gamma$-convergence on the dissipation in the monotone case.\nThis approach gives rise to a variational numerical scheme which applies to\nvery general problems and which we validate on three test cases.",
    "descriptor": "",
    "authors": [
      "Alessio Fumagalli",
      "Francesco Saverio Patacchini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.07970"
  },
  {
    "id": "arXiv:2206.07975",
    "title": "BlindFL: Vertical Federated Machine Learning without Peeking into Your  Data",
    "abstract": "Due to the rising concerns on privacy protection, how to build machine\nlearning (ML) models over different data sources with security guarantees is\ngaining more popularity. Vertical federated learning (VFL) describes such a\ncase where ML models are built upon the private data of different participated\nparties that own disjoint features for the same set of instances, which fits\nmany real-world collaborative tasks. Nevertheless, we find that existing\nsolutions for VFL either support limited kinds of input features or suffer from\npotential data leakage during the federated execution. To this end, this paper\naims to investigate both the functionality and security of ML modes in the VFL\nscenario.\nTo be specific, we introduce BlindFL, a novel framework for VFL training and\ninference. First, to address the functionality of VFL models, we propose the\nfederated source layers to unite the data from different parties. Various kinds\nof features can be supported efficiently by the federated source layers,\nincluding dense, sparse, numerical, and categorical features. Second, we\ncarefully analyze the security during the federated execution and formalize the\nprivacy requirements. Based on the analysis, we devise secure and accurate\nalgorithm protocols, and further prove the security guarantees under the\nideal-real simulation paradigm. Extensive experiments show that BlindFL\nsupports diverse datasets and models efficiently whilst achieves robust privacy\nguarantees.",
    "descriptor": "\nComments: SIGMOD 2022\n",
    "authors": [
      "Fangcheng Fu",
      "Huanran Xue",
      "Yong Cheng",
      "Yangyu Tao",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.07975"
  },
  {
    "id": "arXiv:2206.07977",
    "title": "Personalized Federated Learning via Variational Bayesian Inference",
    "abstract": "Federated learning faces huge challenges from model overfitting due to the\nlack of data and statistical diversity among clients. To address these\nchallenges, this paper proposes a novel personalized federated learning method\nvia Bayesian variational inference named pFedBayes. To alleviate the\noverfitting, weight uncertainty is introduced to neural networks for clients\nand the server. To achieve personalization, each client updates its local\ndistribution parameters by balancing its construction error over private data\nand its KL divergence with global distribution from the server. Theoretical\nanalysis gives an upper bound of averaged generalization error and illustrates\nthat the convergence rate of the generalization error is minimax optimal up to\na logarithmic factor. Experiments show that the proposed method outperforms\nother advanced personalized methods on personalized models, e.g., pFedBayes\nrespectively outperforms other SOTA algorithms by 1.25%, 0.42% and 11.71% on\nMNIST, FMNIST and CIFAR-10 under non-i.i.d. limited data.",
    "descriptor": "\nComments: accepted for publication in 39th International Conference on Machine Learning (ICML), 2022\n",
    "authors": [
      "Xu Zhang",
      "Yinchuan Li",
      "Wenpeng Li",
      "Kaiyang Guo",
      "Yunfeng Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07977"
  },
  {
    "id": "arXiv:2206.07980",
    "title": "Research Topic Flows in Co-Authorship Networks",
    "abstract": "In scientometrics, scientific collaboration is often analyzed by means of\nco-authorships. An aspect which is often overlooked and more difficult to\nquantify is the flow of expertise between authors from different research\ntopics, which is an important part of scientific progress. With the Topic Flow\nNetwork (TFN) we propose a graph structure for the analysis of research topic\nflows between scientific authors and their respective research fields.\nBased on a multi-graph and a topic model, our proposed network structure\naccounts for intratopic as well as intertopic flows. Our method requires for\nthe construction of a TFN solely a corpus of publications (i.e., author and\nabstract information). From this, research topics are discovered automatically\nthrough non-negative matrix factorization. The thereof derived TFN allows for\nthe application of social network analysis techniques, such as common metrics\nand community detection. Most importantly, it allows for the analysis of\nintertopic flows on a large, macroscopic scale, i.e., between research topic,\nas well as on a microscopic scale, i.e., between certain sets of authors.\nWe demonstrate the utility of TFNs by applying our method to two\ncomprehensive corpora of altogether 20 Mio. publications spanning more than 60\nyears of research in the fields computer science and mathematics. Our results\ngive evidence that TFNs are suitable, e.g., for the analysis of topical\ncommunities, the discovery of important authors in different fields, and, most\nnotably, the analysis of intertopic flows, i.e., the transfer of topical\nexpertise. Besides that, our method opens new directions for future research,\nsuch as the investigation of influence relationships between research fields.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Bastian Sch\u00e4fermeier",
      "Johannes Hirth",
      "Tom Hanika"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07980"
  },
  {
    "id": "arXiv:2206.07981",
    "title": "Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment  Analysis in Videos",
    "abstract": "Multimodal sentiment analysis in videos is a key task in many real-world\napplications, which usually requires integrating multimodal streams including\nvisual, verbal and acoustic behaviors. To improve the robustness of multimodal\nfusion, some of the existing methods let different modalities communicate with\neach other and modal the crossmodal interaction via transformers. However,\nthese methods only use the single-scale representations during the interaction\nbut forget to exploit multi-scale representations that contain different levels\nof semantic information. As a result, the representations learned by\ntransformers could be biased especially for unaligned multimodal data. In this\npaper, we propose a multi-scale cooperative multimodal transformer (MCMulT)\narchitecture for multimodal sentiment analysis. On the whole, the \"multi-scale\"\nmechanism is capable of exploiting the different levels of semantic information\nof each modality which are used for fine-grained crossmodal interactions.\nMeanwhile, each modality learns its feature hierarchies via integrating the\ncrossmodal interactions from multiple level features of its source modality. In\nthis way, each pair of modalities progressively builds feature hierarchies\nrespectively in a cooperative manner. The empirical results illustrate that our\nMCMulT model not only outperforms existing approaches on unaligned multimodal\nsequences but also has strong performance on aligned multimodal sequences.",
    "descriptor": "",
    "authors": [
      "Lianyang Ma",
      "Yu Yao",
      "Tao Liang",
      "Tongliang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07981"
  },
  {
    "id": "arXiv:2206.07984",
    "title": "Vesyla-II: An Algorithm Library Development Tool for Synchoros VLSI  Design Style",
    "abstract": "High-level synthesis (HLS) has been researched for decades and is still\nlimited to fast FPGA prototyping and algorithmic RTL generation. A feasible\nend-to-end system-level synthesis solution has never been rigorously proven.\nModularity and composability are the keys to enabling such a system-level\nsynthesis framework that bridges the huge gap between system-level\nspecification and physical level design. It implies that 1) modules in each\nabstraction level should be physically composable without any irregular glue\nlogic involved and 2) the cost of each module in each abstraction level is\naccurately predictable. The ultimate reasons that limit how far the\nconventional HLS can go are precisely that it cannot generate modular designs\nthat are physically composable and cannot accurately predict the cost of its\ndesign. In this paper, we propose Vesyla, not as yet another HLS tool, but as a\nsynthesis tool that positions itself in a promising end-to-end synthesis\nframework and preserving its ability to generate physically composable modular\ndesign and to accurately predict its cost metrics. We present in the paper how\nVesyla is constructed focusing on the novel platform it targets and the\ninternal data structures that highlights the uniqueness of Vesyla. We also show\nhow Vesyla will be positioned in the end-to-end synchoros synthesis framework\ncalled SiLago.",
    "descriptor": "",
    "authors": [
      "Yu Yang",
      "Ahmed Hemani"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.07984"
  },
  {
    "id": "arXiv:2206.07986",
    "title": "Image Captioning based on Feature Refinement and Reflective Decoding",
    "abstract": "Automatically generating a description of an image in natural language is\ncalled image captioning. It is an active research topic that lies at the\nintersection of two major fields in artificial intelligence, computer vision,\nand natural language processing. Image captioning is one of the significant\nchallenges in image understanding since it requires not only recognizing\nsalient objects in the image but also their attributes and the way they\ninteract. The system must then generate a syntactically and semantically\ncorrect caption that describes the image content in natural language. With the\nsignificant progress in deep learning models and their ability to effectively\nencode large sets of images and generate correct sentences, several\nneural-based captioning approaches have been proposed recently, each trying to\nachieve better accuracy and caption quality. This paper introduces an\nencoder-decoder-based image captioning system in which the encoder extracts\nspatial and global features for each region in the image using the Faster R-CNN\nwith ResNet-101 as a backbone. This stage is followed by a refining model,\nwhich uses an attention-on-attention mechanism to extract the visual features\nof the target image objects, then determine their interactions. The decoder\nconsists of an attention-based recurrent module and a reflective attention\nmodule, which collaboratively apply attention to the visual and textual\nfeatures to enhance the decoder's ability to model long-term sequential\ndependencies. Extensive experiments performed on two benchmark datasets, MSCOCO\nand Flickr30K, show the effectiveness the proposed approach and the high\nquality of the generated captions.",
    "descriptor": "",
    "authors": [
      "Ghadah Alabduljabbar",
      "Hafida Benhidour",
      "Said Kerrache"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07986"
  },
  {
    "id": "arXiv:2206.07987",
    "title": "Reconfigurable Intelligent Surfaces Empowered Green Wireless Networks  with User Admission Control",
    "abstract": "Reconfigurable intelligent surface (RIS) has emerged as a cost-effective and\nenergy-efficient technique for 6G. By adjusting the phase shifts of passive\nreflecting elements, RIS is capable of suppressing the interference and\ncombining the desired signals constructively at receivers, thereby\nsignificantly enhancing the performance of communication In this paper, we\nconsider a green multi-user multi-antenna cellular network, where multiple RISs\nare deployed to provide energy-efficient communication service to end users. We\njointly optimize the phase shifts of RISs, beamforming of the base stations,\nand the active RIS set with the aim of minimizing the power consumption of the\nbase station (BS) and RISs subject to the quality of service (QoS) constraints\nof users and the transmit power constraint of the BS. However, the problem is\nmixed combinatorial and nonconvex, and there is a potential infeasibility issue\nwhen the QoS constraints cannot be guaranteed by all users. To deal with the\ninfeasibility issue, we further investigate a user admission control problem to\njointly optimize the transmit beamforming, RIS phase shifts, and the admitted\nuser set. A unified alternating optimization (AO) framework is then proposed to\nsolve both the power minimization and user admission control problems.\nSpecifically, we first decompose the original nonconvex problem into several\nrank-one constrained optimization subproblems via matrix lifting. The proposed\nAO framework efficiently minimizes the power consumption of wireless networks\nas well as user admission control when the QoS constraints cannot be guaranteed\nby all users. Compared with the baseline algorithms, we illustrate that the\nproposed algorithm can achieve lower power consumption for given QoS\nconstraints. Most importantly, the proposed algorithm successfully addresses\nthe infeasibility issue with a QoS guarantee for active users.",
    "descriptor": "\nComments: Submitted to TCOM\n",
    "authors": [
      "Jinglian He",
      "Yijie Mao",
      "Yong Zhou",
      "Ting Wang",
      "Yuanming Shi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.07987"
  },
  {
    "id": "arXiv:2206.07988",
    "title": "PreCogIIITH at HinglishEval : Leveraging Code-Mixing Metrics & Language  Model Embeddings To Estimate Code-Mix Quality",
    "abstract": "Code-Mixing is a phenomenon of mixing two or more languages in a speech event\nand is prevalent in multilingual societies. Given the low-resource nature of\nCode-Mixing, machine generation of code-mixed text is a prevalent approach for\ndata augmentation. However, evaluating the quality of such machine generated\ncode-mixed text is an open problem. In our submission to HinglishEval, a\nshared-task collocated with INLG2022, we attempt to build models factors that\nimpact the quality of synthetically generated code-mix text by predicting\nratings for code-mix quality.",
    "descriptor": "",
    "authors": [
      "Prashant Kodali",
      "Tanmay Sachan",
      "Akshay Goindani",
      "Anmol Goel",
      "Naman Ahuja",
      "Manish Shrivastava",
      "Ponnurangam Kumaraguru"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07988"
  },
  {
    "id": "arXiv:2206.07989",
    "title": "Double Check Your State Before Trusting It: Confidence-Aware  Bidirectional Offline Model-Based Imagination",
    "abstract": "The learned policy of model-free offline reinforcement learning (RL) methods\nis often constrained to stay within the support of datasets to avoid possible\ndangerous out-of-distribution actions or states, making it challenging to\nhandle out-of-support region. Model-based RL methods offer a richer dataset and\nbenefit generalization by generating imaginary trajectories with either trained\nforward or reverse dynamics model. However, the imagined transitions may be\ninaccurate, thus downgrading the performance of the underlying offline RL\nmethod. In this paper, we propose to augment the offline dataset by using\ntrained bidirectional dynamics models and rollout policies with double check.\nWe introduce conservatism by trusting samples that the forward model and\nbackward model agree on. Our method, confidence-aware bidirectional offline\nmodel-based imagination, generates reliable samples and can be combined with\nany model-free offline RL method. Experimental results on the D4RL benchmarks\ndemonstrate that our method significantly boosts the performance of existing\nmodel-free offline RL algorithms and achieves competitive or better scores\nagainst baseline methods.",
    "descriptor": "",
    "authors": [
      "Jiafei Lyu",
      "Xiu Li",
      "Zongqing Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07989"
  },
  {
    "id": "arXiv:2206.07990",
    "title": "Patch-level Representation Learning for Self-supervised Vision  Transformers",
    "abstract": "Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.",
    "descriptor": "\nComments: Accepted to CVPR 2022. Code is available at this https URL\n",
    "authors": [
      "Sukmin Yun",
      "Hankook Lee",
      "Jaehyung Kim",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07990"
  },
  {
    "id": "arXiv:2206.07992",
    "title": "Deconstructing written rules and hierarchy in peer produced software  communities",
    "abstract": "We employ recent advances in computational institutional analysis and NLP to\ninvestigate the systems of authority that are reflected in the written policy\ndocuments of the ASF. Our study to decipher the effective similarities or\ndepartures of the ASF model from conventional software companies reveals\nevidence of both flat and bureaucratic governance in a peer production set up,\nsuggesting a complicated relationship between business-based theories of\nadministrative hierarchy and foundational principles of the OSS movement.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Mahasweta Chakraborti",
      "Beril Bulat",
      "Qiankun Zhong",
      "Anamika Sen",
      "Seth Frey"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.07992"
  },
  {
    "id": "arXiv:2206.07994",
    "title": "Joint Class-Affinity Loss Correction for Robust Medical Image  Segmentation with Noisy Labels",
    "abstract": "Noisy labels collected with limited annotation cost prevent medical image\nsegmentation algorithms from learning precise semantic correlations. Previous\nsegmentation arts of learning with noisy labels merely perform a pixel-wise\nmanner to preserve semantics, such as pixel-wise label correction, but neglect\nthe pair-wise manner. In fact, we observe that the pair-wise manner capturing\naffinity relations between pixels can greatly reduce the label noise rate.\nMotivated by this observation, we present a novel perspective for noisy\nmitigation by incorporating both pixel-wise and pair-wise manners, where\nsupervisions are derived from noisy class and affinity labels, respectively.\nUnifying the pixel-wise and pair-wise manners, we propose a robust Joint\nClass-Affinity Segmentation (JCAS) framework to combat label noise issues in\nmedical image segmentation. Considering the affinity in pair-wise manner\nincorporates contextual dependencies, a differentiated affinity reasoning (DAR)\nmodule is devised to rectify the pixel-wise segmentation prediction by\nreasoning about intra-class and inter-class affinity relations. To further\nenhance the noise resistance, a class-affinity loss correction (CALC) strategy\nis designed to correct supervision signals via the modeled noise label\ndistributions in class and affinity labels. Meanwhile, CALC strategy interacts\nthe pixel-wise and pair-wise manners through the theoretically derived\nconsistency regularization. Extensive experiments under both synthetic and\nreal-world noisy labels corroborate the efficacy of the proposed JCAS framework\nwith a minimum gap towards the upper bound performance. The source code is\navailable at \\url{https://github.com/CityU-AIM-Group/JCAS}.",
    "descriptor": "\nComments: Accepted to MICCAI 2022\n",
    "authors": [
      "Xiaoqing Guo",
      "Yixuan Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07994"
  },
  {
    "id": "arXiv:2206.07995",
    "title": "On the Size of Balls and Anticodes of Small Diameter under the  Fixed-Length Levenshtein Metric",
    "abstract": "The rapid development of DNA storage has brought the deletion and insertion\nchannel to the front line of research. When the number of deletions is equal to\nthe number of insertions, the Fixed Length Levenshtein (FLL) metric is the\nright measure for the distance between two words of the same length. Similar to\nany other metric, the size of a ball is one of the most fundamental parameters.\nIn this work, we consider the minimum, maximum, and average size of a ball with\nradius one, in the FLL metric. The related minimum and the maximum size of a\nmaximal anticode with diameter one are also considered.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2103.01681\n",
    "authors": [
      "Daniella Bar-Lev",
      "Tuvi Etzion",
      "Eitan Yaakobi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2206.07995"
  },
  {
    "id": "arXiv:2206.07996",
    "title": "Continual Learning with Guarantees via Weight Interval Constraints",
    "abstract": "We introduce a new training paradigm that enforces interval constraints on\nneural network parameter space to control forgetting. Contemporary Continual\nLearning (CL) methods focus on training neural networks efficiently from a\nstream of data, while reducing the negative impact of catastrophic forgetting,\nyet they do not provide any firm guarantees that network performance will not\ndeteriorate uncontrollably over time. In this work, we show how to put bounds\non forgetting by reformulating continual learning of a model as a continual\ncontraction of its parameter space. To that end, we propose Hyperrectangle\nTraining, a new training methodology where each task is represented by a\nhyperrectangle in the parameter space, fully contained in the hyperrectangles\nof the previous tasks. This formulation reduces the NP-hard CL problem back to\npolynomial time while providing full resilience against forgetting. We validate\nour claim by developing InterContiNet (Interval Continual Learning) algorithm\nwhich leverages interval arithmetic to effectively model parameter regions as\nhyperrectangles. Through experimental results, we show that our approach\nperforms well in a continual learning setup without storing data from previous\ntasks.",
    "descriptor": "\nComments: Short presentation at ICML 2022\n",
    "authors": [
      "Maciej Wo\u0142czyk",
      "Karol J. Piczak",
      "Bartosz W\u00f3jcik",
      "\u0141ukasz Pustelnik",
      "Pawe\u0142 Morawiecki",
      "Jacek Tabor",
      "Tomasz Trzci\u0144ski",
      "Przemys\u0142aw Spurek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07996"
  },
  {
    "id": "arXiv:2206.07998",
    "title": "Differentially Private Multi-Party Data Release for Linear Regression",
    "abstract": "Differentially Private (DP) data release is a promising technique to\ndisseminate data without compromising the privacy of data subjects. However the\nmajority of prior work has focused on scenarios where a single party owns all\nthe data. In this paper we focus on the multi-party setting, where different\nstakeholders own disjoint sets of attributes belonging to the same group of\ndata subjects. Within the context of linear regression that allow all parties\nto train models on the complete data without the ability to infer private\nattributes or identities of individuals, we start with directly applying\nGaussian mechanism and show it has the small eigenvalue problem. We further\npropose our novel method and prove it asymptotically converges to the optimal\n(non-private) solutions with increasing dataset size. We substantiate the\ntheoretical results through experiments on both artificial and real-world\ndatasets.",
    "descriptor": "",
    "authors": [
      "Ruihan Wu",
      "Xin Yang",
      "Yuanshun Yao",
      "Jiankai Sun",
      "Tianyi Liu",
      "Kilian Q. Weinberger",
      "Chong Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07998"
  },
  {
    "id": "arXiv:2206.08004",
    "title": "When a RF Beats a CNN and GRU, Together -- A Comparison of Deep Learning  and Classical Machine Learning Approaches for Encrypted Malware Traffic  Classification",
    "abstract": "Internet traffic classification is widely used to facilitate network\nmanagement. It plays a crucial role in Quality of Services (QoS), Quality of\nExperience (QoE), network visibility, intrusion detection, and traffic trend\nanalyses. While there is no theoretical guarantee that deep learning (DL)-based\nsolutions perform better than classic machine learning (ML)-based ones,\nDL-based models have become the common default. This paper compares well-known\nDL-based and ML-based models and shows that in the case of malicious traffic\nclassification, state-of-the-art DL-based solutions do not necessarily\noutperform the classical ML-based ones. We exemplify this finding using two\nwell-known datasets for a varied set of tasks, such as: malware detection,\nmalware family classification, detection of zero-day attacks, and\nclassification of an iteratively growing dataset. Note that, it is not feasible\nto evaluate all possible models to make a concrete statement, thus, the above\nfinding is not a recommendation to avoid DL-based models, but rather empirical\nproof that in some cases, there are more simplistic solutions, that may perform\neven better.",
    "descriptor": "",
    "authors": [
      "Adi Lichy",
      "Ofek Bader",
      "Ran Dubin",
      "Amit Dvir",
      "Chen Hajaj"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08004"
  },
  {
    "id": "arXiv:2206.08005",
    "title": "Evaluating Self-Supervised Learning for Molecular Graph Embeddings",
    "abstract": "Graph Self-Supervised Learning (GSSL) paves the way for learning graph\nembeddings without expert annotation, which is particularly impactful for\nmolecular graphs since the number of possible molecules is enormous and labels\nare expensive to obtain. However, by design, GSSL methods are not trained to\nperform well on one downstream task but aim for transferability to many, making\nevaluating them less straightforward. As a step toward obtaining profiles of\nmolecular graph embeddings with diverse and interpretable attributes, we\nintroduce Molecular Graph Representation Evaluation (MolGraphEval), a suite of\nprobe tasks, categorised into (i) topological-, (ii) substructure-, and (iii)\nembedding space properties. By benchmarking existing GSSL methods on both\nexisting downstream datasets and MolGraphEval, we discover surprising\ndiscrepancies between conclusions drawn from existing datasets alone versus\nmore fine-grained probing, suggesting that current evaluation protocols do not\nprovide the whole picture. Our modular, automated end-to-end GSSL pipeline code\nwill be released upon acceptance, including standardised graph loading,\nexperiment management, and embedding evaluation.",
    "descriptor": "",
    "authors": [
      "Hanchen Wang",
      "Jean Kaddour",
      "Shengchao Liu",
      "Jian Tang",
      "Matt Kusner",
      "Joan Lasenby",
      "Qi Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2206.08005"
  },
  {
    "id": "arXiv:2206.08006",
    "title": "Energy-Grade Double Pricing Rule in the Heating Market",
    "abstract": "The problem of heating system pricing is considered. A direct extension of\nlocational marginal prices (LMP) of electricity markets to heating systems may\nlead to revenue inadequate issues. The underlying reason for such a problem is\nthat, unlike electric power, heating energy has the issue of grade and cannot\nbe considered as homogeneous goods. Accordingly, an energy-grade double pricing\nrule is proposed in this paper. The resulting merchandise surplus can be\ndecomposed into several explainable parts. Simulations verify the effectiveness\nof the proposed mechanism.",
    "descriptor": "",
    "authors": [
      "Xinyi Yi",
      "Ye Guo",
      "Hongbin Sun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08006"
  },
  {
    "id": "arXiv:2206.08007",
    "title": "DCASE 2022: Comparative Analysis Of CNNs For Acoustic Scene  Classification Under Low-Complexity Considerations",
    "abstract": "Acoustic scene classification is an automatic listening problem that aims to\nassign an audio recording to a pre-defined scene based on its audio data. Over\nthe years (and in past editions of the DCASE) this problem has often been\nsolved with techniques known as ensembles (use of several machine learning\nmodels to combine their predictions in the inference phase). While these\nsolutions can show performance in terms of accuracy, they can be very expensive\nin terms of computational capacity, making it impossible to deploy them in IoT\ndevices. Due to the drift in this field of study, this task has two limitations\nin terms of model complexity. It should be noted that there is also the added\ncomplexity of mismatching devices (the audios provided are recorded by\ndifferent sources of information). This technical report makes a comparative\nstudy of two different network architectures: conventional CNN and Conv-mixer.\nAlthough both networks exceed the baseline required by the competition, the\nconventional CNN shows a higher performance, exceeding the baseline by 8\npercentage points. Solutions based on Conv-mixer architectures show worse\nperformance although they are much lighter solutions.",
    "descriptor": "",
    "authors": [
      "Josep Zaragoza-Paredes",
      "Javier Naranjo-Alcazar",
      "Valery Naranjo",
      "Pedro Zuccarello"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08007"
  },
  {
    "id": "arXiv:2206.08009",
    "title": "Balancing Discriminability and Transferability for Source-Free Domain  Adaptation",
    "abstract": "Conventional domain adaptation (DA) techniques aim to improve domain\ntransferability by learning domain-invariant representations; while\nconcurrently preserving the task-discriminability knowledge gathered from the\nlabeled source data. However, the requirement of simultaneous access to labeled\nsource and unlabeled target renders them unsuitable for the challenging\nsource-free DA setting. The trivial solution of realizing an effective original\nto generic domain mapping improves transferability but degrades task\ndiscriminability. Upon analyzing the hurdles from both theoretical and\nempirical standpoints, we derive novel insights to show that a mixup between\noriginal and corresponding translated generic samples enhances the\ndiscriminability-transferability trade-off while duly respecting the\nprivacy-oriented source-free setting. A simple but effective realization of the\nproposed insights on top of the existing source-free DA approaches yields\nstate-of-the-art performance with faster convergence. Beyond single-source, we\nalso outperform multi-source prior-arts across both classification and semantic\nsegmentation benchmarks.",
    "descriptor": "\nComments: ICML 2022. Project page: this https URL\n",
    "authors": [
      "Jogendra Nath Kundu",
      "Akshay Kulkarni",
      "Suvaansh Bhambri",
      "Deepesh Mehta",
      "Shreyas Kulkarni",
      "Varun Jampani",
      "R. Venkatesh Babu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08009"
  },
  {
    "id": "arXiv:2206.08010",
    "title": "MoDi: Unconditional Motion Synthesis from Diverse Data",
    "abstract": "The emergence of neural networks has revolutionized the field of motion\nsynthesis. Yet, learning to unconditionally synthesize motions from a given\ndistribution remains a challenging task, especially when the motions are highly\ndiverse. We present MoDi, an unconditional generative model that synthesizes\ndiverse motions. Our model is trained in a completely unsupervised setting from\na diverse, unstructured and unlabeled motion dataset and yields a well-behaved,\nhighly semantic latent space. The design of our model follows the prolific\narchitecture of StyleGAN and adapts two of its key technical components into\nthe motion domain: a set of style-codes injected into each level of the\ngenerator hierarchy and a mapping function that learns and forms a disentangled\nlatent space. We show that despite the lack of any structure in the dataset,\nthe latent space can be semantically clustered, and facilitates semantic\nediting and motion interpolation. In addition, we propose a technique to invert\nunseen motions into the latent space, and demonstrate latent-based motion\nediting operations that otherwise cannot be achieved by naive manipulation of\nexplicit motion representations. Our qualitative and quantitative experiments\nshow that our framework achieves state-of-the-art synthesis quality that can\nfollow the distribution of highly diverse motion datasets. Code and trained\nmodels will be released at https://sigal-raab.github.io/MoDi.",
    "descriptor": "\nComments: Video: this https URL, Code: Coming soon\n",
    "authors": [
      "Sigal Raab",
      "Inbal Leibovitch",
      "Peizhuo Li",
      "Kfir Aberman",
      "Olga Sorkine-Hornung",
      "Daniel Cohen-Or"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08010"
  },
  {
    "id": "arXiv:2206.08014",
    "title": "On Error and Compression Rates for Prototype Rules",
    "abstract": "We study the close interplay between error and compression in the\nnon-parametric multiclass classification setting in terms of prototype learning\nrules. We focus in particular on a close variant of a recently proposed\ncompression-based learning rule termed OptiNet. Beyond its computational\nmerits, this rule has been recently shown to be universally consistent in any\nmetric instance space that admits a universally consistent rule -- the first\nlearning algorithm known to enjoy this property. However, its error and\ncompression rates have been left open. Here we derive such rates in the case\nwhere instances reside in Euclidean space under commonly posed smoothness and\ntail conditions on the data distribution. We first show that OptiNet achieves\nnon-trivial compression rates while enjoying near minimax-optimal error rates.\nWe then proceed to study a novel general compression scheme for further\ncompressing prototype rules that locally adapts to the noise level without\nsacrificing accuracy. Applying it to OptiNet, we show that under a geometric\nmargin condition, further gain in the compression rate is achieved.\nExperimental results comparing the performance of the various methods are\npresented.",
    "descriptor": "",
    "authors": [
      "Omer Kerem",
      "Roi Weiss"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08014"
  },
  {
    "id": "arXiv:2206.08016",
    "title": "Backbones-Review: Feature Extraction Networks for Deep Learning and Deep  Reinforcement Learning Approaches",
    "abstract": "To understand the real world using various types of data, Artificial\nIntelligence (AI) is the most used technique nowadays. While finding the\npattern within the analyzed data represents the main task. This is performed by\nextracting representative features step, which is proceeded using the\nstatistical algorithms or using some specific filters. However, the selection\nof useful features from large-scale data represented a crucial challenge. Now,\nwith the development of convolution neural networks (CNNs), the feature\nextraction operation has become more automatic and easier. CNNs allow to work\non large-scale size of data, as well as cover different scenarios for a\nspecific task. For computer vision tasks, convolutional networks are used to\nextract features also for the other parts of a deep learning model. The\nselection of a suitable network for feature extraction or the other parts of a\nDL model is not random work. So, the implementation of such a model can be\nrelated to the target task as well as the computational complexity of it. Many\nnetworks have been proposed and become the famous networks used for any DL\nmodels in any AI task. These networks are exploited for feature extraction or\nat the beginning of any DL model which is named backbones. A backbone is a\nknown network trained in many other tasks before and demonstrates its\neffectiveness. In this paper, an overview of the existing backbones, e.g. VGGs,\nResNets, DenseNet, etc, is given with a detailed description. Also, a couple of\ncomputer vision tasks are discussed by providing a review of each task\nregarding the backbones used. In addition, a comparison in terms of performance\nis also provided, based on the backbone used for each task.",
    "descriptor": "",
    "authors": [
      "Omar Elharrouss",
      "Younes Akbari",
      "Noor Almaadeed",
      "Somaya Al-Maadeed"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08016"
  },
  {
    "id": "arXiv:2206.08021",
    "title": "Exploiting Global Semantic Similarities in Knowledge Graphs by  Relational Prototype Entities",
    "abstract": "Knowledge graph (KG) embedding aims at learning the latent representations\nfor entities and relations of a KG in continuous vector spaces. An empirical\nobservation is that the head (tail) entities connected by the same relation\noften share similar semantic attributes -- specifically, they often belong to\nthe same category -- no matter how far away they are from each other in the KG;\nthat is, they share global semantic similarities. However, many existing\nmethods derive KG embeddings based on the local information, which fail to\neffectively capture such global semantic similarities among entities. To\naddress this challenge, we propose a novel approach, which introduces a set of\nvirtual nodes called \\textit{\\textbf{relational prototype entities}} to\nrepresent the prototypes of the head and tail entities connected by the same\nrelations. By enforcing the entities' embeddings close to their associated\nprototypes' embeddings, our approach can effectively encourage the global\nsemantic similarities of entities -- that can be far away in the KG --\nconnected by the same relation. Experiments on the entity alignment and KG\ncompletion tasks demonstrate that our approach significantly outperforms recent\nstate-of-the-arts.",
    "descriptor": "\nComments: 11 pages, 6 figures, Knowledge Graph Embedding\n",
    "authors": [
      "Xueliang Wang",
      "Jiajun Chen",
      "Feng Wu",
      "Jie Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08021"
  },
  {
    "id": "arXiv:2206.08022",
    "title": "Partial Identifiability for Nonnegative Matrix Factorization",
    "abstract": "Given a nonnegative matrix factorization, $R$, and a factorization rank, $r$,\nExact nonnegative matrix factorization (Exact NMF) decomposes $R$ as the\nproduct of two nonnegative matrices, $C$ and $S$ with $r$ columns, such as $R =\nCS^\\top$. A central research topic in the literature is the conditions under\nwhich such a decomposition is unique/identifiable, up to trivial ambiguities.\nIn this paper, we focus on partial identifiability, that is, the uniqueness of\na subset of columns of $C$ and $S$. We start our investigations with the\ndata-based uniqueness (DBU) theorem from the chemometrics literature. The DBU\ntheorem analyzes all feasible solutions of Exact NMF, and relies on sparsity\nconditions on $C$ and $S$. We provide a mathematically rigorous theorem of a\nrecently published restricted version of the DBU theorem, relying only on\nsimple sparsity and algebraic conditions: it applies to a particular solution\nof Exact NMF (as opposed to all feasible solutions) and allows us to guarantee\nthe partial uniqueness of a single column of $C$ or $S$. Second, based on a\ngeometric interpretation of the restricted DBU theorem, we obtain a new partial\nidentifiability result. We prove it is stronger than the restricted DBU\ntheorem, given that a proper preprocessing on the Exact NMF is used. This\ngeometric interpretation also leads us to another partial identifiability\nresult in the case $r=3$. Third, we show how partial identifiability results\ncan be used sequentially to guarantee the identifiability of more columns of\n$C$ and $S$. We illustrate these results on several examples, including one\nfrom the chemometrics literature.",
    "descriptor": "\nComments: 26 pages, 6 figures, 7 examples\n",
    "authors": [
      "Nicolas Gillis",
      "R\u00f3bert Rajk\u00f3"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08022"
  },
  {
    "id": "arXiv:2206.08026",
    "title": "DeepFormableTag: End-to-end Generation and Recognition of Deformable  Fiducial Markers",
    "abstract": "Fiducial markers have been broadly used to identify objects or embed messages\nthat can be detected by a camera. Primarily, existing detection methods assume\nthat markers are printed on ideally planar surfaces. Markers often fail to be\nrecognized due to various imaging artifacts of optical/perspective distortion\nand motion blur. To overcome these limitations, we propose a novel deformable\nfiducial marker system that consists of three main parts: First, a fiducial\nmarker generator creates a set of free-form color patterns to encode\nsignificantly large-scale information in unique visual codes. Second, a\ndifferentiable image simulator creates a training dataset of photorealistic\nscene images with the deformed markers, being rendered during optimization in a\ndifferentiable manner. The rendered images include realistic shading with\nspecular reflection, optical distortion, defocus and motion blur, color\nalteration, imaging noise, and shape deformation of markers. Lastly, a trained\nmarker detector seeks the regions of interest and recognizes multiple marker\npatterns simultaneously via inverse deformation transformation. The deformable\nmarker creator and detector networks are jointly optimized via the\ndifferentiable photorealistic renderer in an end-to-end manner, allowing us to\nrobustly recognize a wide range of deformable markers with high accuracy. Our\ndeformable marker system is capable of decoding 36-bit messages successfully at\n~29 fps with severe shape deformation. Results validate that our system\nsignificantly outperforms the traditional and data-driven marker methods. Our\nlearning-based marker system opens up new interesting applications of fiducial\nmarkers, including cost-effective motion capture of the human body, active 3D\nscanning using our fiducial markers' array as structured light patterns, and\nrobust augmented reality rendering of virtual objects on dynamic surfaces.",
    "descriptor": "",
    "authors": [
      "Mustafa B. Yaldiz",
      "Andreas Meuleman",
      "Hyeonjoong Jang",
      "Hyunho Ha",
      "Min H. Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.08026"
  },
  {
    "id": "arXiv:2206.08029",
    "title": "DIALOG-22 RuATD Generated Text Detection",
    "abstract": "Text Generation Models (TGMs) succeed in creating text that matches human\nlanguage style reasonably well. Detectors that can distinguish between\nTGM-generated text and human-written ones play an important role in preventing\nabuse of TGM.\nIn this paper, we describe our pipeline for the two DIALOG-22 RuATD tasks:\ndetecting generated text (binary task) and classification of which model was\nused to generate text (multiclass task). We achieved 1st place on the binary\nclassification task with an accuracy score of 0.82995 on the private test set\nand 4th place on the multiclass classification task with an accuracy score of\n0.62856 on the private test set. We proposed an ensemble method of different\npre-trained models based on the attention mechanism.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Narek Maloyan",
      "Bulat Nutfullin",
      "Eugene Ilyushin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08029"
  },
  {
    "id": "arXiv:2206.08039",
    "title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis  Using Linguistic and Prosodic Contexts of Dialogue History",
    "abstract": "We propose an end-to-end empathetic dialogue speech synthesis (DSS) model\nthat considers both the linguistic and prosodic contexts of dialogue history.\nEmpathy is the active attempt by humans to get inside the interlocutor in\ndialogue, and empathetic DSS is a technology to implement this act in spoken\ndialogue systems. Our model is conditioned by the history of linguistic and\nprosody features for predicting appropriate dialogue context. As such, it can\nbe regarded as an extension of the conventional linguistic-feature-based\ndialogue history modeling. To train the empathetic DSS model effectively, we\ninvestigate 1) a self-supervised learning model pretrained with large speech\ncorpora, 2) a style-guided training using a prosody embedding of the current\nutterance to be predicted by the dialogue context embedding, 3) a cross-modal\nattention to combine text and speech modalities, and 4) a sentence-wise\nembedding to achieve fine-grained prosody modeling rather than utterance-wise\nmodeling. The evaluation results demonstrate that 1) simply considering\nprosodic contexts of the dialogue history does not improve the quality of\nspeech in empathetic DSS and 2) introducing style-guided training and\nsentence-wise embedding modeling achieves higher speech quality than that by\nthe conventional method.",
    "descriptor": "\nComments: 5 pages, 3 figures, Accepted for INTERSPEECH2022\n",
    "authors": [
      "Yuto Nishimura",
      "Yuki Saito",
      "Shinnosuke Takamichi",
      "Kentaro Tachibana",
      "Hiroshi Saruwatari"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08039"
  },
  {
    "id": "arXiv:2206.08046",
    "title": "An Open-Domain QA System for e-Governance",
    "abstract": "The paper presents an open-domain Question Answering system for Romanian,\nanswering COVID-19 related questions. The QA system pipeline involves automatic\nquestion processing, automatic query generation, web searching for the top 10\nmost relevant documents and answer extraction using a fine-tuned BERT model for\nExtractive QA, trained on a COVID-19 data set that we have manually created.\nThe paper will present the QA system and its integration with the Romanian\nlanguage technologies portal RELATE, the COVID-19 data set and different\nevaluations of the QA performance.",
    "descriptor": "\nComments: 8 pages, accepted to CLIB2022 in the main conference\n",
    "authors": [
      "Radu Ion",
      "Andrei-Marius Avram",
      "Vasile P\u0103i\u015f",
      "Maria Mitrofan",
      "Verginica Barbu Mititelu",
      "Elena Irimia",
      "Valentin Badea"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08046"
  },
  {
    "id": "arXiv:2206.08050",
    "title": "Time Interval-enhanced Graph Neural Network for Shared-account  Cross-domain Sequential Recommendation",
    "abstract": "Shared-account Cross-domain Sequential Recommendation (SCSR) task aims to\nrecommend the next item via leveraging the mixed user behaviors in multiple\ndomains. It is gaining immense research attention as more and more users tend\nto sign up on different platforms and share accounts with others to access\ndomain-specific services. Existing works on SCSR mainly rely on mining\nsequential patterns via Recurrent Neural Network (RNN)-based models, which\nsuffer from the following limitations: 1) RNN-based methods overwhelmingly\ntarget discovering sequential dependencies in single-user behaviors. They are\nnot expressive enough to capture the relationships among multiple entities in\nSCSR. 2) All existing methods bridge two domains via knowledge transfer in the\nlatent space, and ignore the explicit cross-domain graph structure. 3) None\nexisting studies consider the time interval information among items, which is\nessential in the sequential recommendation for characterizing different items\nand learning discriminative representations for them. In this work, we propose\na new graph-based solution, namely TiDA-GCN, to address the above challenges.\nSpecifically, we first link users and items in each domain as a graph. Then, we\ndevise a domain-aware graph convolution network to learn userspecific node\nrepresentations. To fully account for users' domainspecific preferences on\nitems, two effective attention mechanisms are further developed to selectively\nguide the message passing process. Moreover, to further enhance item- and\naccount-level representation learning, we incorporate the time interval into\nthe message passing, and design an account-aware self-attention module for\nlearning items' interactive characteristics. Experiments demonstrate the\nsuperiority of our proposed method from various aspects.",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Lei Guo",
      "Jinyu Zhang",
      "Li Tang",
      "Tong Chen",
      "Lei Zhu",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08050"
  },
  {
    "id": "arXiv:2206.08053",
    "title": "JU_NLP at HinglishEval: Quality Evaluation of the Low-Resource  Code-Mixed Hinglish Text",
    "abstract": "In this paper we describe a system submitted to the INLG 2022 Generation\nChallenge (GenChal) on Quality Evaluation of the Low-Resource Synthetically\nGenerated Code-Mixed Hinglish Text. We implement a Bi-LSTM-based neural network\nmodel to predict the Average rating score and Disagreement score of the\nsynthetic Hinglish dataset. In our models, we used word embeddings for English\nand Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score\nof 0.11, and mean squared error of 6.0 in the average rating score prediction\ntask. In the task of Disagreement score prediction, we achieve a F1 score of\n0.18, and mean squared error of 5.0.",
    "descriptor": "",
    "authors": [
      "Prantik Guha",
      "Rudra Dhar",
      "Dipankar Das"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.08053"
  },
  {
    "id": "arXiv:2206.08054",
    "title": "Generalized Leverage Scores: Geometric Interpretation and Applications",
    "abstract": "In problems involving matrix computations, the concept of leverage has found\na large number of applications. In particular, leverage scores, which relate\nthe columns of a matrix to the subspaces spanned by its leading singular\nvectors, are helpful in revealing column subsets to approximately factorize a\nmatrix with quality guarantees. As such, they provide a solid foundation for a\nvariety of machine-learning methods. In this paper we extend the definition of\nleverage scores to relate the columns of a matrix to arbitrary subsets of\nsingular vectors. We establish a precise connection between column and\nsingular-vector subsets, by relating the concepts of leverage scores and\nprincipal angles between subspaces. We employ this result to design\napproximation algorithms with provable guarantees for two well-known problems:\ngeneralized column subset selection and sparse canonical correlation analysis.\nWe run numerical experiments to provide further insight on the proposed\nmethods. The novel bounds we derive improve our understanding of fundamental\nconcepts in matrix approximations. In addition, our insights may serve as\nbuilding blocks for further contributions.",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Bruno Ordozgoiti",
      "Antonis Matakos",
      "Aristides Gionis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08054"
  },
  {
    "id": "arXiv:2206.08061",
    "title": "Active Nearest Neighbor Regression Through Delaunay Refinement",
    "abstract": "We introduce an algorithm for active function approximation based on nearest\nneighbor regression. Our Active Nearest Neighbor Regressor (ANNR) relies on the\nVoronoi-Delaunay framework from computational geometry to subdivide the space\ninto cells with constant estimated function value and select novel query points\nin a way that takes the geometry of the function graph into account. We\nconsider the recent state-of-the-art active function approximator called DEFER,\nwhich is based on incremental rectangular partitioning of the space, as the\nmain baseline. The ANNR addresses a number of limitations that arise from the\nspace subdivision strategy used in DEFER. We provide a computationally\nefficient implementation of our method, as well as theoretical halting\nguarantees. Empirical results show that ANNR outperforms the baseline for both\nclosed-form functions and real-world examples, such as gravitational wave\nparameter inference and exploration of the latent space of a generative model.",
    "descriptor": "\nComments: Accepted at the International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Alexander Kravberg",
      "Giovanni Luca Marchetti",
      "Vladislav Polianskii",
      "Anastasiia Varava",
      "Florian T. Pokorny",
      "Danica Kragic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.08061"
  },
  {
    "id": "arXiv:2206.08063",
    "title": "Towards Robust Ranker for Text Retrieval",
    "abstract": "A ranker plays an indispensable role in the de facto 'retrieval & rerank'\npipeline, but its training still lags behind -- learning from moderate\nnegatives or/and serving as an auxiliary module for a retriever. In this work,\nwe first identify two major barriers to a robust ranker, i.e., inherent label\nnoises caused by a well-trained retriever and non-ideal negatives sampled for a\nhigh-capable ranker. Thereby, we propose multiple retrievers as negative\ngenerators improve the ranker's robustness, where i) involving extensive\nout-of-distribution label noises renders the ranker against each noise\ndistribution, and ii) diverse hard negatives from a joint distribution are\nrelatively close to the ranker's negative distribution, leading to more\nchallenging thus effective training. To evaluate our robust ranker (dubbed\nR$^2$anker), we conduct experiments in various settings on the popular passage\nretrieval benchmark, including BM25-reranking, full-ranking, retriever\ndistillation, etc. The empirical results verify the new state-of-the-art\neffectiveness of our model.",
    "descriptor": "\nComments: 11 pages of main content, 4 tables, 3 figures\n",
    "authors": [
      "Yucheng Zhou",
      "Tao Shen",
      "Xiubo Geng",
      "Chongyang Tao",
      "Can Xu",
      "Guodong Long",
      "Binxing Jiao",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08063"
  },
  {
    "id": "arXiv:2206.08065",
    "title": "Neural tangent kernel analysis of shallow $\u03b1$-Stable ReLU neural  networks",
    "abstract": "There is a recent literature on large-width properties of Gaussian neural\nnetworks (NNs), i.e. NNs whose weights are distributed according to Gaussian\ndistributions. Two popular problems are: i) the study of the large-width\nbehaviour of NNs, which provided a characterization of the infinitely wide\nlimit of a rescaled NN in terms of a Gaussian process; ii) the study of the\nlarge-width training dynamics of NNs, which set forth an equivalence between\ntraining the rescaled NN and performing a kernel regression with a\ndeterministic kernel referred to as the neural tangent kernel (NTK). In this\npaper, we consider these problems for $\\alpha$-Stable NNs, which generalize\nGaussian NNs by assuming that the NN's weights are distributed as\n$\\alpha$-Stable distributions with $\\alpha\\in(0,2]$, i.e. distributions with\nheavy tails. For shallow $\\alpha$-Stable NNs with a ReLU activation function,\nwe show that if the NN's width goes to infinity then a rescaled NN converges\nweakly to an $\\alpha$-Stable process, i.e. a stochastic process with\n$\\alpha$-Stable finite-dimensional distributions. As a novelty with respect to\nthe Gaussian setting, in the $\\alpha$-Stable setting the choice of the\nactivation function affects the scaling of the NN, that is: to achieve the\ninfinitely wide $\\alpha$-Stable process, the ReLU function requires an\nadditional logarithmic scaling with respect to sub-linear functions. Then, our\nmain contribution is the NTK analysis of shallow $\\alpha$-Stable ReLU-NNs,\nwhich leads to an equivalence between training a rescaled NN and performing a\nkernel regression with an $(\\alpha/2)$-Stable random kernel. The randomness of\nsuch a kernel is a further novelty with respect to the Gaussian setting, that\nis: in the $\\alpha$-Stable setting the randomness of the NN at initialization\ndoes not vanish in the NTK analysis, thus inducing a distribution for the\nkernel of the underlying kernel regression.",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Stefano Favaro",
      "Sandra Fortini",
      "Stefano Peluchetti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08065"
  },
  {
    "id": "arXiv:2206.08067",
    "title": "Industrial Limitations on Academic Freedom in Computer Science",
    "abstract": "The field of computer science is perhaps uniquely connected with industry.\nFor example, our main publication outlets (i.e. conferences) are regularly\nsponsored by large technology companies, and much of our research funding is\neither directly or indirectly provided by industry. In turn, this places\npotential limitations on academic freedom, which is a profound ethical concern,\nyet curiously is not directly addressed within existing ethical codes. A field\nthat limits academic freedom presents the risk that the results of the work\nconducted within it cannot always be relied upon. In the context of a field\nthat is perhaps unique in both its connection to industry and impact on\nsociety, special measures are needed to address this problem. This paper\ndiscusses the range of protections that could be provided.",
    "descriptor": "\nComments: Accepted to Ethicomp 2022\n",
    "authors": [
      "Reuben Kirkham"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08067"
  },
  {
    "id": "arXiv:2206.08069",
    "title": "Data-Driven Abstraction-Based Control Synthesis",
    "abstract": "This paper studies formal synthesis of controllers for continuous-space\nsystems with unknown dynamics to satisfy requirements expressed as linear\ntemporal logic formulas. Formal abstraction-based synthesis schemes rely on a\nprecise mathematical model of the system to build a finite abstract model,\nwhich is then used to design a controller. The abstraction-based schemes are\nnot applicable when the dynamics of the system are unknown. We propose a\ndata-driven approach that computes the growth bound of the system using a\nfinite number of trajectories. The growth bound together with the sampled\ntrajectories are then used to construct the abstraction and synthesise a\ncontroller.\nOur approach casts the computation of the growth bound as a robust convex\noptimisation program (RCP). Since the unknown dynamics appear in the\noptimisation, we formulate a scenario convex program (SCP) corresponding to the\nRCP using a finite number of sampled trajectories. We establish a sample\ncomplexity result that gives a lower bound for the number of sampled\ntrajectories to guarantee the correctness of the growth bound computed from the\nSCP with a given confidence. We also provide a sample complexity result for the\nsatisfaction of the specification on the system in closed loop with the\ndesigned controller for a given confidence. Our results are founded on\nestimating a bound on the Lipschitz constant of the system and provide\nguarantees on satisfaction of both finite and infinite-horizon specifications.\nWe show that our data-driven approach can be readily used as a model-free\nabstraction refinement scheme by modifying the formulation of the growth bound\nand providing similar sample complexity results. The performance of our\napproach is shown on three case studies.",
    "descriptor": "",
    "authors": [
      "Milad Kazemi",
      "Rupak Majumdar",
      "Mahmoud Salamati",
      "Sadegh Soudjani",
      "Ben Wooding"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08069"
  },
  {
    "id": "arXiv:2206.08075",
    "title": "Exploring Collaborative Game Play with Robots to Encourage Good Hand  Hygiene Practises among Children",
    "abstract": "This paper presents the design, implementation, and evaluation of a novel\ncollaborative educational game titled \"Land of Hands\", involving children and a\ncustomized social robot that we designed (\\emph{HakshE}). Through this gaming\nplatform, we aim to teach proper hand hygiene practises to children and explore\nthe extent of interactions that take place between a pro-social robot and\nchildren in such a setting. We blended gamification with Computers as Social\nActors (CASA) paradigm to model the robot as a social actor or a fellow player\nin the game. The game was developed using Godot's 2D engine and Alice 3. In\nthis study, 32 participants played the game online through a video\nteleconferencing platform \\emph{Zoom}. To understand the influence a pro-social\nrobot's nudges has on children's interactions, we split our study into two\nconditions: With-Nudges and Without-Nudges. Detailed analysis of rubrics and\nvideo analyses of children's interactions show that our platform helped\nchildren learn good hand hygiene practises. We also found that using a\npro-social robot creates enjoyable interactions and greater social engagement\nbetween the children and the robot although learning itself wasn't influenced\nby the pro-sociality of the robot.",
    "descriptor": "\nComments: 8 pages, 9 figures, 31st IEEE International Conference on Robot & Human Interactive Communication (RO-MAN 2022)\n",
    "authors": [
      "Devasena Pasupuleti",
      "Sreejith Sasidharan",
      "Rajesh Sharma",
      "Gayathri Manikutty"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08075"
  },
  {
    "id": "arXiv:2206.08076",
    "title": "Learning Effect of Lay People in Gesture-Based Locomotion in Virtual  Reality",
    "abstract": "Locomotion in Virtual Reality (VR) is an important part of VR applications.\nMany scientists are enriching the community with different variations that\nenable locomotion in VR. Some of the most promising methods are gesture-based\nand do not require additional handheld hardware. Recent work focused mostly on\nuser preference and performance of the different locomotion techniques. This\nignores the learning effect that users go through while new methods are being\nexplored. In this work, it is investigated whether and how quickly users can\nadapt to a hand gesture-based locomotion system in VR. Four different\nlocomotion techniques are implemented and tested by participants. The goal of\nthis paper is twofold: First, it aims to encourage researchers to consider the\nlearning effect in their studies. Second, this study aims to provide insight\ninto the learning effect of users in gesture-based systems.",
    "descriptor": "",
    "authors": [
      "Alexander Sch\u00e4fer",
      "Gerd Reis",
      "Didier Stricker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08076"
  },
  {
    "id": "arXiv:2206.08077",
    "title": "Neural Scene Representation for Locomotion on Structured Terrain",
    "abstract": "We propose a learning-based method to reconstruct the local terrain for\nlocomotion with a mobile robot traversing urban environments. Using a stream of\ndepth measurements from the onboard cameras and the robot's trajectory, the\nalgorithm estimates the topography in the robot's vicinity. The raw\nmeasurements from these cameras are noisy and only provide partial and occluded\nobservations that in many cases do not show the terrain the robot stands on.\nTherefore, we propose a 3D reconstruction model that faithfully reconstructs\nthe scene, despite the noisy measurements and large amounts of missing data\ncoming from the blind spots of the camera arrangement. The model consists of a\n4D fully convolutional network on point clouds that learns the geometric priors\nto complete the scene from the context and an auto-regressive feedback to\nleverage spatio-temporal consistency and use evidence from the past. The\nnetwork can be solely trained with synthetic data, and due to extensive\naugmentation, it is robust in the real world, as shown in the validation on a\nquadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline\non the robot's onboard low-power computer using an efficient sparse tensor\nimplementation and show that the proposed method outperforms classical map\nrepresentations.",
    "descriptor": "",
    "authors": [
      "David Hoeller",
      "Nikita Rudin",
      "Christopher Choy",
      "Animashree Anandkumar",
      "Marco Hutter"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08077"
  },
  {
    "id": "arXiv:2206.08080",
    "title": "A Machine Learning-based Digital Twin for Electric Vehicle Battery  Modeling",
    "abstract": "The widespread adoption of Electric Vehicles (EVs) is limited by their\nreliance on batteries with presently low energy and power densities compared to\nliquid fuels and are subject to aging and performance deterioration over time.\nFor this reason, monitoring the battery State Of Charge (SOC) and State Of\nHealth (SOH) during the EV lifetime is a very relevant problem. This work\nproposes a battery digital twin structure designed to accurately reflect\nbattery dynamics at the run time. To ensure a high degree of correctness\nconcerning non-linear phenomena, the digital twin relies on data-driven models\ntrained on traces of battery evolution over time: a SOH model, repeatedly\nexecuted to estimate the degradation of maximum battery capacity, and a SOC\nmodel, retrained periodically to reflect the impact of aging. The proposed\ndigital twin structure will be exemplified on a public dataset to motivate its\nadoption and prove its effectiveness, with high accuracy and inference and\nretraining times compatible with onboard execution.",
    "descriptor": "\nComments: Accepted as a conference paper at the 2022 IEEE International Conference on Omni-Layer Intelligent Systems (COINS)\n",
    "authors": [
      "Khaled Sidahmed Sidahmed Alamin",
      "Yukai Chen",
      "Enrico Macii",
      "Massimo Poncino",
      "Sara Vinco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08080"
  },
  {
    "id": "arXiv:2206.08081",
    "title": "TransDrift: Modeling Word-Embedding Drift using Transformer",
    "abstract": "In modern NLP applications, word embeddings are a crucial backbone that can\nbe readily shared across a number of tasks. However as the text distributions\nchange and word semantics evolve over time, the downstream applications using\nthe embeddings can suffer if the word representations do not conform to the\ndata drift. Thus, maintaining word embeddings to be consistent with the\nunderlying data distribution is a key problem. In this work, we tackle this\nproblem and propose TransDrift, a transformer-based prediction model for word\nembeddings. Leveraging the flexibility of transformer, our model accurately\nlearns the dynamics of the embedding drift and predicts the future embedding.\nIn experiments, we compare with existing methods and show that our model makes\nsignificantly more accurate predictions of the word embedding than the\nbaselines. Crucially, by applying the predicted embeddings as a backbone for\ndownstream classification tasks, we show that our embeddings lead to superior\nperformance compared to the previous methods.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Nishtha Madaan",
      "Prateek Chaudhury",
      "Nishant Kumar",
      "Srikanta Bedathur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08081"
  },
  {
    "id": "arXiv:2206.08082",
    "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language  Models as a Demonstration Generator",
    "abstract": "Large-scale pre-trained language models (PLMs) are well-known for being\ncapable of solving a task simply by conditioning a few input-label pairs dubbed\ndemonstrations on a prompt without being explicitly tuned for the desired\ndownstream task. Such a process (i.e., in-context learning), however, naturally\nleads to high reliance on the demonstrations which are usually selected from\nexternal datasets. In this paper, we propose self-generated in-context learning\n(SG-ICL), which generates demonstrations for in-context learning from PLM\nitself to minimize the reliance on the external demonstration. We conduct\nexperiments on four different text classification tasks and show SG-ICL\nsignificantly outperforms zero-shot learning and is generally worth\napproximately 0.6 gold training samples. Moreover, our generated demonstrations\nshow more consistent performance with low variance compared to randomly\nselected demonstrations from the training dataset.",
    "descriptor": "\nComments: NAACL 2022 Workshop on Large-scale Pre-trained Language Models\n",
    "authors": [
      "Hyuhng Joon Kim",
      "Hyunsoo Cho",
      "Junyeob Kim",
      "Taeuk Kim",
      "Kang Min Yoo",
      "Sang-goo Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08082"
  },
  {
    "id": "arXiv:2206.08083",
    "title": "CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation  from Simulation to multiple Real-World Domains",
    "abstract": "Unsupervised Domain Adaptation demonstrates great potential to mitigate\ndomain shifts by transferring models from labeled source domains to unlabeled\ntarget domains. While Unsupervised Domain Adaptation has been applied to a wide\nvariety of complex vision tasks, only few works focus on lane detection for\nautonomous driving. This can be attributed to the lack of publicly available\ndatasets. To facilitate research in these directions, we propose CARLANE, a\n3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE\nencompasses the single-target datasets MoLane and TuLane and the multi-target\ndataset MuLane. These datasets are built from three different domains, which\ncover diverse scenes and contain a total of 163K unique images, 118K of which\nare annotated. In addition we evaluate and report systematic baselines,\nincluding our own method, which builds upon Prototypical Cross-domain\nSelf-supervised Learning. We find that false positive and false negative rates\nof the evaluated domain adaptation methods are high compared to those of fully\nsupervised baselines. This affirms the need for benchmarks such as CARLANE to\nfurther strengthen research in Unsupervised Domain Adaptation for lane\ndetection. CARLANE, all evaluated models and the corresponding implementations\nare publicly available at https://carlanebenchmark.github.io.",
    "descriptor": "\nComments: 19 pages, 10 figures, under review at Neural Information Processing Systems Track on Datasets and Benchmarks\n",
    "authors": [
      "Julian Gebele",
      "Bonifaz Stuhr",
      "Johann Haselberger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08083"
  },
  {
    "id": "arXiv:2206.08084",
    "title": "An Improved Normed-Deformable Convolution for Crowd Counting",
    "abstract": "In recent years, crowd counting has become an important issue in computer\nvision. In most methods, the density maps are generated by convolving with a\nGaussian kernel from the ground-truth dot maps which are marked around the\ncenter of human heads. Due to the fixed geometric structures in CNNs and\nindistinct head-scale information, the head features are obtained incompletely.\nDeformable convolution is proposed to exploit the scale-adaptive capabilities\nfor CNN features in the heads. By learning the coordinate offsets of the\nsampling points, it is tractable to improve the ability to adjust the receptive\nfield. However, the heads are not uniformly covered by the sampling points in\nthe deformable convolution, resulting in loss of head information. To handle\nthe non-uniformed sampling, an improved Normed-Deformable Convolution\n(\\textit{i.e.,}NDConv) implemented by Normed-Deformable loss\n(\\textit{i.e.,}NDloss) is proposed in this paper. The offsets of the sampling\npoints which are constrained by NDloss tend to be more even. Then, the features\nin the heads are obtained more completely, leading to better performance.\nEspecially, the proposed NDConv is a light-weight module which shares similar\ncomputation burden with Deformable Convolution. In the extensive experiments,\nour method outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech\nB, UCF\\_QNRF, and UCF\\_CC\\_50 dataset, achieving 61.4, 7.8, 91.2, and 167.2\nMAE, respectively. The code is available at\nhttps://github.com/bingshuangzhuzi/NDConv",
    "descriptor": "",
    "authors": [
      "Xin Zhong",
      "Zhaoyi Yan",
      "Jing Qin",
      "Wangmeng Zuo",
      "Weigang Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08084"
  },
  {
    "id": "arXiv:2206.08087",
    "title": "ALL-MASK: A Reconfigurable Logic Locking Method for Multicore  Architecture with Sequential-Instruction-Oriented Key",
    "abstract": "Intellectual property (IP) piracy has become a non-negligible problem as the\nintegrated circuit (IC) production supply chain is becoming increasingly\nglobalized and separated that enables attacks by potentially untrusted\nattackers. Logic locking is a widely adopted method to lock the circuit module\nwith a key and prevent hackers from cracking it. The key is the critical aspect\nof logic locking, but the existing works have overlooked three possible\nchallenges of the key: safety of key storage, easy key-attempt from interface\nand key-related overheads, bringing the further challenges of low error rate\nand small state space. In this work, the key is dynamically generated by\nutilizing the huge space of a CPU core, and the unlocking is performed\nimplicitly through the interconnection inside the chip. A novel low-cost logic\nreconfigurable gate is together proposed with ferroelectric FET (FeFET) to\nmitigate the reverse engineering and removal attack. Compared to the common\nlogic locking methods, our proposed approach is 19,945 times more time\nconsuming to traverse all the possible combinations in only 9-bit-key\ncondition. Furthermore, our technique let key length increases this complexity\nexponentially and ensure the logic obfuscation effect.",
    "descriptor": "\nComments: 15 pages, 17 figures\n",
    "authors": [
      "Jianfeng Wang",
      "Zhonghao Chen",
      "Jiahao Zhang",
      "Yixin Xu",
      "Tongguang Yu",
      "Enze Ye",
      "Ziheng Zheng",
      "Huazhong Yang",
      "Sumitha George",
      "Yongpan Liu",
      "Vijaykrishnan Narayanan",
      "Xueqing Li"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.08087"
  },
  {
    "id": "arXiv:2206.08088",
    "title": "Reinforcement Learning-enhanced Shared-account Cross-domain Sequential  Recommendation",
    "abstract": "Shared-account Cross-domain Sequential Recommendation (SCSR) is an emerging\nyet challenging task that simultaneously considers the shared-account and\ncross-domain characteristics in the sequential recommendation. Existing works\non SCSR are mainly based on Recurrent Neural Network (RNN) and Graph Neural\nNetwork (GNN) but they ignore the fact that although multiple users share a\nsingle account, it is mainly occupied by one user at a time. This observation\nmotivates us to learn a more accurate user-specific account representation by\nattentively focusing on its recent behaviors. Furthermore, though existing\nworks endow lower weights to irrelevant interactions, they may still dilute the\ndomain information and impede the cross-domain recommendation. To address the\nabove issues, we propose a reinforcement learning-based solution, namely\nRL-ISN, which consists of a basic cross-domain recommender and a reinforcement\nlearning-based domain filter. Specifically, to model the account representation\nin the shared-account scenario, the basic recommender first clusters users'\nmixed behaviors as latent users, and then leverages an attention model over\nthem to conduct user identification. To reduce the impact of irrelevant domain\ninformation, we formulate the domain filter as a hierarchical reinforcement\nlearning task, where a high-level task is utilized to decide whether to revise\nthe whole transferred sequence or not, and if it does, a low-level task is\nfurther performed to determine whether to remove each interaction within it or\nnot. To evaluate the performance of our solution, we conduct extensive\nexperiments on two real-world datasets, and the experimental results\ndemonstrate the superiority of our RL-ISN method compared with the\nstate-of-the-art recommendation methods.",
    "descriptor": "\nComments: 14 pages, 5 figures\n",
    "authors": [
      "Lei Guo",
      "Jinyu Zhang",
      "Tong Chen",
      "Xinhua Wang",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08088"
  },
  {
    "id": "arXiv:2206.08091",
    "title": "Unsupervised Space Partitioning for Nearest Neighbor Search",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) in high dimensional spaces is\ncrucial for many real-life applications (e.g., e-commerce, web, multimedia,\netc.) dealing with an abundance of data. In this paper, we propose an\nend-to-end learning framework that couples the partitioning (one key step of\nANNS) and learning-to-search steps using a custom loss function. A key\nadvantage of our proposed solution is that it does not require any expensive\npre-processing of the dataset, which is one of the key limitations of the\nstate-of-the-art approach. We achieve the above edge by formulating a\nmulti-objective custom loss function that does not need ground truth labels to\nquantify the quality of a given partition of the data space, making it entirely\nunsupervised. We also propose an ensembling technique by adding varying input\nweights to the loss function to train an ensemble of models to enhance the\nsearch quality. On several standard benchmarks for ANNS, we show that our\nmethod beats the state-of-the-art space partitioning method and the ubiquitous\nK-means clustering method while using fewer parameters and shorter offline\ntraining times. Without loss of generality, our unsupervised partitioning\napproach is shown as a promising alternative to many widely used clustering\nmethods like K-means clustering and DBSCAN.",
    "descriptor": "",
    "authors": [
      "Abrar Fahim",
      "Mohammed Eunus Ali",
      "Muhammad Aamir Cheema"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08091"
  },
  {
    "id": "arXiv:2206.08092",
    "title": "On the well-spread property and its relation to linear regression",
    "abstract": "We consider the robust linear regression model $\\boldsymbol{y} = X\\beta^* +\n\\boldsymbol{\\eta}$, where an adversary oblivious to the design $X \\in\n\\mathbb{R}^{n \\times d}$ may choose $\\boldsymbol{\\eta}$ to corrupt all but a\n(possibly vanishing) fraction of the observations $\\boldsymbol{y}$ in an\narbitrary way. Recent work [dLN+21, dNS21] has introduced efficient algorithms\nfor consistent recovery of the parameter vector. These algorithms crucially\nrely on the design matrix being well-spread (a matrix is well-spread if its\ncolumn span is far from any sparse vector).\nIn this paper, we show that there exists a family of design matrices lacking\nwell-spreadness such that consistent recovery of the parameter vector in the\nabove robust linear regression model is information-theoretically impossible.\nWe further investigate the average-case time complexity of certifying\nwell-spreadness of random matrices. We show that it is possible to efficiently\ncertify whether a given $n$-by-$d$ Gaussian matrix is well-spread if the number\nof observations is quadratic in the ambient dimension. We complement this\nresult by showing rigorous evidence -- in the form of a lower bound against\nlow-degree polynomials -- of the computational hardness of this same\ncertification problem when the number of observations is $o(d^2)$.",
    "descriptor": "\nComments: To appear in COLT 2022\n",
    "authors": [
      "Hongjie Chen",
      "Tommaso d'Orsi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08092"
  },
  {
    "id": "arXiv:2206.08093",
    "title": "Applications of Machine Learning to the Identification of Anomalous ER  Claims",
    "abstract": "Improper health insurance payments resulting from fraud and upcoding result\nin tens of billions of dollars in excess health care costs annually in the\nUnited States, motivating machine learning researchers to build anomaly\ndetection models for health insurance claims. This article describes two such\nstrategies specifically for ER claims. The first is an upcoding model based on\nseverity code distributions, stratified by hierarchical diagnosis code\nclusters. A statistically significant difference in mean upcoding anomaly\nscores is observed between free-standing ERs and acute care hospitals, with\nfree-standing ERs being more anomalous. The second model is a random forest\nthat minimizes improper payments by optimally sorting ER claims within review\nqueues. Depending on the percentage of claims reviewed, the random forest saved\n12% to 40% above a baseline approach that prioritized claims by billed amount.",
    "descriptor": "\nComments: 15 pages, 12 figures\n",
    "authors": [
      "Jesse B. Crawford",
      "Nicholas Petela"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08093"
  },
  {
    "id": "arXiv:2206.08094",
    "title": "Deep Neural Imputation: A Framework for Recovering Incomplete Brain  Recordings",
    "abstract": "Neuroscientists and neuroengineers have long relied on multielectrode neural\nrecordings to study the brain. However, in a typical experiment, many factors\ncorrupt neural recordings from individual electrodes, including electrical\nnoise, movement artifacts, and faulty manufacturing. Currently, common practice\nis to discard these corrupted recordings, reducing already limited data that is\ndifficult to collect. To address this challenge, we propose Deep Neural\nImputation (DNI), a framework to recover missing values from electrodes by\nlearning from data collected across spatial locations, days, and participants.\nWe explore our framework with a linear nearest-neighbor approach and two deep\ngenerative autoencoders, demonstrating DNI's flexibility. One deep autoencoder\nmodels participants individually, while the other extends this architecture to\nmodel many participants jointly. We evaluate our models across 12 human\nparticipants implanted with multielectrode intracranial electrocorticography\narrays; participants had no explicit task and behaved naturally across hundreds\nof recording hours. We show that DNI recovers not only time series but also\nfrequency content, and further establish DNI's practical value by recovering\nsignificant performance on a scientifically-relevant downstream neural decoding\ntask.",
    "descriptor": "",
    "authors": [
      "Sabera Talukder",
      "Jennifer J. Sun",
      "Matthew Leonard",
      "Bingni W. Brunton",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08094"
  },
  {
    "id": "arXiv:2206.08096",
    "title": "On-the-fly Adaptation of Patrolling Strategies in Changing Environments",
    "abstract": "We consider the problem of efficient patrolling strategy adaptation in a\nchanging environment where the topology of Defender's moves and the importance\nof guarded targets change unpredictably. The Defender must instantly switch to\na new strategy optimized for the new environment, not disrupting the ongoing\npatrolling task, and the new strategy must be computed promptly under all\ncircumstances. Since strategy switching may cause unintended security risks\ncompromising the achieved protection, our solution includes mechanisms for\ndetecting and mitigating this problem. The efficiency of our framework is\nevaluated experimentally.",
    "descriptor": "",
    "authors": [
      "Tom\u00e1\u0161 Br\u00e1zdil",
      "David Kla\u0161ka",
      "Anton\u00edn Ku\u010dera",
      "V\u00edt Musil",
      "Petr Novotn\u00fd",
      "Vojt\u011bch \u0158eh\u00e1k"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.08096"
  },
  {
    "id": "arXiv:2206.08101",
    "title": "Is Continual Learning Truly Learning Representations Continually?",
    "abstract": "Continual learning (CL) aims to learn from sequentially arriving tasks\nwithout forgetting previous tasks. Whereas CL algorithms have tried to achieve\nhigher average test accuracy across all the tasks learned so far, learning\ncontinuously useful representations is critical for successful generalization\nand downstream transfer. To measure representational quality, we re-train only\nthe output layers using a small balanced dataset for all the tasks, evaluating\nthe average accuracy without any biased predictions toward the current task. We\nalso test on several downstream tasks, measuring transfer learning accuracy of\nthe learned representations. By testing our new formalism on ImageNet-100 and\nImageNet-1000, we find that using more exemplar memory is the only option to\nmake a meaningful difference in learned representations, and most of the\nregularization- or distillation-based CL algorithms that use the exemplar\nmemory fail to learn continuously useful representations in class-incremental\nlearning. Surprisingly, unsupervised (or self-supervised) CL with sufficient\nmemory size can achieve comparable performance to the supervised counterparts.\nConsidering non-trivial labeling costs, we claim that finding more efficient\nunsupervised CL algorithms that minimally use exemplary memory would be the\nnext promising direction for CL research.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Sungmin Cha",
      "Dongsub Shim",
      "Hyunwoo Kim",
      "Moontae Lee",
      "Honglak Lee",
      "Taesup Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08101"
  },
  {
    "id": "arXiv:2206.08105",
    "title": "A Simple Baseline for Adversarial Domain Adaptation-based Unsupervised  Flood Forecasting",
    "abstract": "Flood disasters cause enormous social and economic losses. However, both\ntraditional physical models and learning-based flood forecasting models require\nmassive historical flood data to train the model parameters. When come to some\nnew site that does not have sufficient historical data, the model performance\nwill drop dramatically due to overfitting. This technical report presents a\nFlood Domain Adaptation Network (FloodDAN), a baseline of applying Unsupervised\nDomain Adaptation (UDA) to the flood forecasting problem. Specifically,\ntraining of FloodDAN includes two stages: in the first stage, we train a\nrainfall encoder and a prediction head to learn general transferable\nhydrological knowledge on large-scale source domain data; in the second stage,\nwe transfer the knowledge in the pretrained encoder into the rainfall encoder\nof target domain through adversarial domain alignment. During inference, we\nutilize the target domain rainfall encoder trained in the second stage and the\nprediction head trained in the first stage to get flood forecasting\npredictions. Experimental results on Tunxi and Changhua flood dataset show that\nFloodDAN can perform flood forecasting effectively with zero target domain\nsupervision. The performance of the FloodDAN is on par with supervised models\nthat uses 450-500 hours of supervision.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Delong Chen",
      "Ruizhi Zhou",
      "Yanling Pan",
      "Fan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08105"
  },
  {
    "id": "arXiv:2206.08107",
    "title": "Closed-Form Diffeomorphic Transformations for Time Series Alignment",
    "abstract": "Time series alignment methods call for highly expressive, differentiable and\ninvertible warping functions which preserve temporal topology, i.e\ndiffeomorphisms. Diffeomorphic warping functions can be generated from the\nintegration of velocity fields governed by an ordinary differential equation\n(ODE). Gradient-based optimization frameworks containing diffeomorphic\ntransformations require to calculate derivatives to the differential equation's\nsolution with respect to the model parameters, i.e. sensitivity analysis.\nUnfortunately, deep learning frameworks typically lack\nautomatic-differentiation-compatible sensitivity analysis methods; and implicit\nfunctions, such as the solution of ODE, require particular care. Current\nsolutions appeal to adjoint sensitivity methods, ad-hoc numerical solvers or\nResNet's Eulerian discretization. In this work, we present a closed-form\nexpression for the ODE solution and its gradient under continuous\npiecewise-affine (CPA) velocity functions. We present a highly optimized\nimplementation of the results on CPU and GPU. Furthermore, we conduct extensive\nexperiments on several datasets to validate the generalization ability of our\nmodel to unseen data for time-series joint alignment. Results show significant\nimprovements both in terms of efficiency and accuracy.",
    "descriptor": "\nComments: 37 pages, 24 figures, 4 tables. Accepted at International Conference on Machine Learning ICML 2022\n",
    "authors": [
      "I\u00f1igo Martinez",
      "Elisabeth Viles",
      "Igor G. Olaizola"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08107"
  },
  {
    "id": "arXiv:2206.08111",
    "title": "On Private Online Convex Optimization: Optimal Algorithms in  $\\ell_p$-Geometry and High Dimensional Contextual Bandits",
    "abstract": "Differentially private (DP) stochastic convex optimization (SCO) is\nubiquitous in trustworthy machine learning algorithm design. This paper studies\nthe DP-SCO problem with streaming data sampled from a distribution and arrives\nsequentially. We also consider the continual release model where parameters\nrelated to private information are updated and released upon each new data,\noften known as the online algorithms. Despite that numerous algorithms have\nbeen developed to achieve the optimal excess risks in different $\\ell_p$ norm\ngeometries, yet none of the existing ones can be adapted to the streaming and\ncontinual release setting. To address such a challenge as the online convex\noptimization with privacy protection, we propose a private variant of online\nFrank-Wolfe algorithm with recursive gradients for variance reduction to update\nand reveal the parameters upon each data. Combined with the adaptive\ndifferential privacy analysis, our online algorithm achieves in linear time the\noptimal excess risk when $1<p\\leq 2$ and the state-of-the-art excess risk\nmeeting the non-private lower ones when $2<p\\leq\\infty$. Our algorithm can also\nbe extended to the case $p=1$ to achieve nearly dimension-independent excess\nrisk. While previous variance reduction results on recursive gradient have\ntheoretical guarantee only in the independent and identically distributed\nsample setting, we establish such a guarantee in a non-stationary setting. To\ndemonstrate the virtues of our method, we design the first DP algorithm for\nhigh-dimensional generalized linear bandits with logarithmic regret.\nComparative experiments with a variety of DP-SCO and DP-Bandit algorithms\nexhibit the efficacy and utility of the proposed algorithms.",
    "descriptor": "\nComments: This is the extended version of the paper appeared in the 39th International Conference on Machine Learning (ICML 2022): Optimal Private Streaming SCO in $\\ell_p$-geometry with Applications in High Dimensional Online Decision Making\n",
    "authors": [
      "Yuxuan Han",
      "Zhicong Liang",
      "Zhipeng Liang",
      "Yang Wang",
      "Yuan Yao",
      "Jiheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08111"
  },
  {
    "id": "arXiv:2206.08119",
    "title": "Learning to Infer Structures of Network Games",
    "abstract": "Strategic interactions between a group of individuals or organisations can be\nmodelled as games played on networks, where a player's payoff depends not only\non their actions but also on those of their neighbours. Inferring the network\nstructure from observed game outcomes (equilibrium actions) is an important\nproblem with numerous potential applications in economics and social sciences.\nExisting methods mostly require the knowledge of the utility function\nassociated with the game, which is often unrealistic to obtain in real-world\nscenarios. We adopt a transformer-like architecture which correctly accounts\nfor the symmetries of the problem and learns a mapping from the equilibrium\nactions to the network structure of the game without explicit knowledge of the\nutility function. We test our method on three different types of network games\nusing both synthetic and real-world data, and demonstrate its effectiveness in\nnetwork structure inference and superior performance over existing methods.",
    "descriptor": "",
    "authors": [
      "Emanuele Rossi",
      "Federico Monti",
      "Yan Leng",
      "Michael M. Bronstein",
      "Xiaowen Dong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.08119"
  },
  {
    "id": "arXiv:2206.08124",
    "title": "Using adversarial images to improve outcomes of federated learning for  non-IID data",
    "abstract": "One of the important problems in federated learning is how to deal with\nunbalanced data. This contribution introduces a novel technique designed to\ndeal with label skewed non-IID data, using adversarial inputs, created by the\nI-FGSM method. Adversarial inputs guide the training process and allow the\nWeighted Federated Averaging to give more importance to clients with 'selected'\nlocal label distributions. Experimental results, gathered from image\nclassification tasks, for MNIST and CIFAR-10 datasets, are reported and\nanalyzed.",
    "descriptor": "",
    "authors": [
      "Anastasiya Danilenka",
      "Maria Ganzha",
      "Marcin Paprzycki",
      "Jacek Ma\u0144dziuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08124"
  },
  {
    "id": "arXiv:2206.08126",
    "title": "Channel Importance Matters in Few-Shot Image Classification",
    "abstract": "Few-Shot Learning (FSL) requires vision models to quickly adapt to brand-new\nclassification tasks with a shift in task distribution. Understanding the\ndifficulties posed by this task distribution shift is central to FSL. In this\npaper, we show that a simple channel-wise feature transformation may be the key\nto unraveling this secret from a channel perspective. When facing novel\nfew-shot tasks in the test-time datasets, this transformation can greatly\nimprove the generalization ability of learned image representations, while\nbeing agnostic to the choice of training algorithms and datasets. Through an\nin-depth analysis of this transformation, we find that the difficulty of\nrepresentation transfer in FSL stems from the severe channel bias problem of\nimage representations: channels may have different importance in different\ntasks, while convolutional neural networks are likely to be insensitive, or\nrespond incorrectly to such a shift. This points out a core problem of the\ngeneralization ability of modern vision systems and needs further attention in\nthe future.",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Xu Luo",
      "Jing Xu",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08126"
  },
  {
    "id": "arXiv:2206.08127",
    "title": "Random Access Concatenated Libraries and dd enable a short-latency  high-content website on an inexpensive shared server",
    "abstract": "Shared website host service providers impose strict limits on each user's\ndisk and computing resources. To minimize the load on these limited resources,\nlarge numbers of images or other records may be concatenated into a single file\nand then delivered with short latency independent of location within the file\nusing Linux utility dd. This solution and its performance are presented for (a)\na website which houses 8000+ genealogical reference books with more than 3.5\nmillion 250-Kbyte page images, (b) a website-based database of 86 million death\nrecords, and (c) a shared cluster computing application which utilizes 800\nneuroimaging databases, each with 1.5 million records. These widely disparate\napplications demonstrate both the efficacy and general utility of the solution.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Don Krieger"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2206.08127"
  },
  {
    "id": "arXiv:2206.08129",
    "title": "Trajectory-guided Control Prediction for End-to-end Autonomous Driving:  A Simple yet Strong Baseline",
    "abstract": "Current end-to-end autonomous driving methods either run a controller based\non a planned trajectory or perform control prediction directly, which have\nspanned two separately studied lines of research. Seeing their potential mutual\nbenefits to each other, this paper takes the initiative to explore the\ncombination of these two well-developed worlds. Specifically, our integrated\napproach has two branches for trajectory planning and direct control,\nrespectively. The trajectory branch predicts the future trajectory, while the\ncontrol branch involves a novel multi-step prediction scheme such that the\nrelationship between current actions and future states can be reasoned. The two\nbranches are connected so that the control branch receives corresponding\nguidance from the trajectory branch at each time step. The outputs from two\nbranches are then fused to achieve complementary advantages. Our results are\nevaluated in the closed-loop urban driving setting with challenging scenarios\nusing the CARLA simulator. Even with a monocular camera input, the proposed\napproach ranks $first$ on the official CARLA Leaderboard, outperforming other\ncomplex candidates with multiple sensors or fusion mechanisms by a large\nmargin. The source code and data will be made publicly available at\nhttps://github.com/OpenPerceptionX/TCP.",
    "descriptor": "\nComments: 15 pages, 7 figures\n",
    "authors": [
      "Penghao Wu",
      "Xiaosong Jia",
      "Li Chen",
      "Junchi Yan",
      "Hongyang Li",
      "Yu Qiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08129"
  },
  {
    "id": "arXiv:2206.08132",
    "title": "Adaptive versus Static Multi-oracle Algorithms, and Quantum Security of  a Split-key PRF",
    "abstract": "In the first part of the paper, we show a generic compiler that transforms\nany oracle algorithm that can query multiple oracles {\\em adaptively}, i.e.,\ncan decide on {\\em which} oracle to query at what point dependent on previous\noracle responses, into a {\\em static} algorithm that fixes these choices at the\nbeginning of the execution. Compared to naive ways of achieving this, our\ncompiler controls the blow-up in query complexity for each oracle {\\em\nindividually}, and causes a very mild blow-up only. In the second part of the\npaper, we use our compiler to show the security of the very efficient\nhash-based {\\em split-key PRF} proposed by Giacon, Heuer and Poettering\n(PKC~2018), in the {\\em quantum} random-oracle model. Using a split-key PRF as\nthe key-derivation function gives rise to a secure KEM combiner. Thus, our\nresult shows that the hash-based construction of Giacon {\\em et al.}\\ can be\nsafely used in the context of quantum attacks, for instance to combine a\nwell-established but only classically-secure KEM with a candidate KEM that is\nbelieved to be quantum-secure. Our security proof for the split-key PRF\ncrucially relies on our adaptive-to-static compiler, but we expect our compiler\nto be useful beyond this particular application. Indeed, we discuss a couple of\nother, known results from the literature that would have profitted from our\ncompiler, in that these works had to go though serious complications in oder to\ndeal with adaptivity.",
    "descriptor": "",
    "authors": [
      "Jelle Don",
      "Serge Fehr",
      "Yu-Hsuan Huang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.08132"
  },
  {
    "id": "arXiv:2206.08138",
    "title": "Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone  fine-tuning without episodic meta-learning dominates for few-shot learning  image classification",
    "abstract": "Although deep neural networks are capable of achieving performance superior\nto humans on various tasks, they are notorious for requiring large amounts of\ndata and computing resources, restricting their success to domains where such\nresources are available. Metalearning methods can address this problem by\ntransferring knowledge from related tasks, thus reducing the amount of data and\ncomputing resources needed to learn new tasks. We organize the MetaDL\ncompetition series, which provide opportunities for research groups all over\nthe world to create and experimentally assess new meta-(deep)learning solutions\nfor real problems. In this paper, authored collaboratively between the\ncompetition organizers and the top-ranked participants, we describe the design\nof the competition, the datasets, the best experimental results, as well as the\ntop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active\nteams who made it to the final phase (by outperforming the baseline), making\nover 100 code submissions during the feedback phase. The solutions of the top\nparticipants have been open-sourced. The lessons learned include that learning\ngood representations is essential for effective transfer learning.",
    "descriptor": "",
    "authors": [
      "Adrian El Baz",
      "Andr\u00e9 Carvalho",
      "Hong Chen",
      "Fabio Ferreira",
      "Henry Gouk",
      "Shell Hu",
      "Frank Hutter",
      "Zhengying Liu",
      "Felix Mohr",
      "Jan van Rijn",
      "Xin Wang",
      "Isabelle Guyon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.08138"
  },
  {
    "id": "arXiv:2206.08141",
    "title": "i-FlatCam: A 253 FPS, 91.49 $\u03bc$J/Frame Ultra-Compact Intelligent  Lensless Camera for Real-Time and Efficient Eye Tracking in VR/AR",
    "abstract": "We present a first-of-its-kind ultra-compact intelligent camera system,\ndubbed i-FlatCam, including a lensless camera with a computational (Comp.)\nchip. It highlights (1) a predict-then-focus eye tracking pipeline for boosted\nefficiency without compromising the accuracy, (2) a unified compression scheme\nfor single-chip processing and improved frame rate per second (FPS), and (3)\ndedicated intra-channel reuse design for depth-wise convolutional layers\n(DW-CONV) to increase utilization. i-FlatCam demonstrates the first eye\ntracking pipeline with a lensless camera and achieves 3.16 degrees of accuracy,\n253 FPS, 91.49 $\\mu$J/Frame, and 6.7mm x 8.9mm x 1.2mm camera form factor,\npaving the way for next-generation Augmented Reality (AR) and Virtual Reality\n(VR) devices.",
    "descriptor": "\nComments: Accepted by VLSI 2022\n",
    "authors": [
      "Yang Zhao",
      "Ziyun Li",
      "Yonggan Fu",
      "Yongan Zhang",
      "Chaojian Li",
      "Cheng Wan",
      "Haoran You",
      "Shang Wu",
      "Xu Ouyang",
      "Vivek Boominathan",
      "Ashok Veeraraghavan",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.08141"
  },
  {
    "id": "arXiv:2206.08144",
    "title": "A Contextual Combinatorial Semi-Bandit Approach to Network Bottleneck  Identification",
    "abstract": "Bottleneck identification is a challenging task in network analysis,\nespecially when the network is not fully specified. To address this task, we\ndevelop a unified online learning framework based on combinatorial semi-bandits\nthat performs bottleneck identification alongside learning the specifications\nof the underlying network. Within this framework, we adapt and investigate\nseveral combinatorial semi-bandit methods such as epsilon-greedy, LinUCB,\nBayesUCB, and Thompson Sampling. Our framework is able to employ contextual\ninformation in the form of contextual bandits. We evaluate our framework on the\nreal-world application of road networks and demonstrate its effectiveness in\ndifferent settings.",
    "descriptor": "",
    "authors": [
      "Fazeleh Hoseini",
      "Niklas \u00c5kerblom",
      "Morteza Haghir Chehreghani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08144"
  },
  {
    "id": "arXiv:2206.08149",
    "title": "A Truthful Owner-Assisted Scoring Mechanism",
    "abstract": "Alice (owner) has knowledge of the underlying quality of her items measured\nin grades. Given the noisy grades provided by an independent party, can Bob\n(appraiser) obtain accurate estimates of the ground-truth grades of the items\nby asking Alice a question about the grades? We address this when the payoff to\nAlice is additive convex utility over all her items. We establish that if Alice\nhas to truthfully answer the question so that her payoff is maximized, the\nquestion must be formulated as pairwise comparisons between her items. Next, we\nprove that if Alice is required to provide a ranking of her items, which is the\nmost fine-grained question via pairwise comparisons, she would be truthful. By\nincorporating the ground-truth ranking, we show that Bob can obtain an\nestimator with the optimal squared error in certain regimes based on any\npossible way of truthful information elicitation. Moreover, the estimated\ngrades are substantially more accurate than the raw grades when the number of\nitems is large and the raw grades are very noisy. Finally, we conclude the\npaper with several extensions and some refinements for practical\nconsiderations.",
    "descriptor": "\nComments: A (significantly) extended version of arXiv: 2110.14802\n",
    "authors": [
      "Weijie J. Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08149"
  },
  {
    "id": "arXiv:2206.08150",
    "title": "Self-Adaptive Label Augmentation for Semi-supervised Few-shot  Classification",
    "abstract": "Few-shot classification aims to learn a model that can generalize well to new\ntasks when only a few labeled samples are available. To make use of unlabeled\ndata that are more abundantly available in real applications, Ren et al.\n\\shortcite{ren2018meta} propose a semi-supervised few-shot classification\nmethod that assigns an appropriate label to each unlabeled sample by a manually\ndefined metric. However, the manually defined metric fails to capture the\nintrinsic property in data. In this paper, we propose a\n\\textbf{S}elf-\\textbf{A}daptive \\textbf{L}abel \\textbf{A}ugmentation approach,\ncalled \\textbf{SALA}, for semi-supervised few-shot classification. A major\nnovelty of SALA is the task-adaptive metric, which can learn the metric\nadaptively for different tasks in an end-to-end fashion. Another appealing\nfeature of SALA is a progressive neighbor selection strategy, which selects\nunlabeled data with high confidence progressively through the training phase.\nExperiments demonstrate that SALA outperforms several state-of-the-art methods\nfor semi-supervised few-shot classification on benchmark datasets.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Xueliang Wang",
      "Jianyu Cai",
      "Shuiwang Ji",
      "Houqiang Li",
      "Feng Wu",
      "Jie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08150"
  },
  {
    "id": "arXiv:2206.08152",
    "title": "Fault-Tolerant Collaborative Inference through the Edge-PRUNE Framework",
    "abstract": "Collaborative inference has received significant research interest in machine\nlearning as a vehicle for distributing computation load, reducing latency, as\nwell as addressing privacy preservation in communications. Recent collaborative\ninference frameworks have adopted dynamic inference methodologies such as\nearly-exit and run-time partitioning of neural networks. However, as machine\nlearning frameworks scale in the number of inference inputs, e.g., in\nsurveillance applications, fault tolerance related to device failure needs to\nbe considered. This paper presents the Edge-PRUNE distributed computing\nframework, built on a formally defined model of computation, which provides a\nflexible infrastructure for fault tolerant collaborative inference. The\nexperimental section of this work shows results on achievable inference time\nsavings by collaborative inference, presents fault tolerant system topologies\nand analyzes their cost in terms of execution time overhead.",
    "descriptor": "\nComments: Accepted to ICML 2022 Workshop on Dynamic Neural Networks (DyNN)\n",
    "authors": [
      "Jani Boutellier",
      "Bo Tan",
      "Jari Nurmi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.08152"
  },
  {
    "id": "arXiv:2206.08155",
    "title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language  Models",
    "abstract": "Video question answering (VideoQA) is a complex task that requires diverse\nmulti-modal data for training. Manual annotation of question and answers for\nvideos, however, is tedious and prohibits scalability. To tackle this problem,\nrecent methods consider zero-shot settings with no manual annotation of visual\nquestion-answer. In particular, a promising approach adapts frozen\nautoregressive language models pretrained on Web-scale text-only data to\nmulti-modal inputs. In contrast, we here build on frozen bidirectional language\nmodels (BiLM) and show that such an approach provides a stronger and cheaper\nalternative for zero-shot VideoQA. In particular, (i) we combine visual inputs\nwith the frozen BiLM using light trainable modules, (ii) we train such modules\nusing Web-scraped multi-modal data, and finally (iii) we perform zero-shot\nVideoQA inference through masked language modeling, where the masked text is\nthe answer to a given question. Our proposed approach, FrozenBiLM, outperforms\nthe state of the art in zero-shot VideoQA by a significant margin on a variety\nof datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,\nTGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in\nthe few-shot and fully-supervised setting. Our code and models will be made\npublicly available at https://antoyang.github.io/frozenbilm.html.",
    "descriptor": "\nComments: 23 pages; 5 figures\n",
    "authors": [
      "Antoine Yang",
      "Antoine Miech",
      "Josef Sivic",
      "Ivan Laptev",
      "Cordelia Schmid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08155"
  },
  {
    "id": "arXiv:2206.08158",
    "title": "Volumetric Supervised Contrastive Learning for Seismic Semantic  Segmentation",
    "abstract": "In seismic interpretation, pixel-level labels of various rock structures can\nbe time-consuming and expensive to obtain. As a result, there oftentimes exists\na non-trivial quantity of unlabeled data that is left unused simply because\ntraditional deep learning methods rely on access to fully labeled volumes. To\nrectify this problem, contrastive learning approaches have been proposed that\nuse a self-supervised methodology in order to learn useful representations from\nunlabeled data. However, traditional contrastive learning approaches are based\non assumptions from the domain of natural images that do not make use of\nseismic context. In order to incorporate this context within contrastive\nlearning, we propose a novel positive pair selection strategy based on the\nposition of slices within a seismic volume. We show that the learnt\nrepresentations from our method out-perform a state of the art contrastive\nlearning methodology in a semantic segmentation task.",
    "descriptor": "",
    "authors": [
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.08158"
  },
  {
    "id": "arXiv:2206.08164",
    "title": "Long Range Graph Benchmark",
    "abstract": "Graph Neural Networks (GNNs) that are based on the message passing (MP)\nparadigm exchange information between 1-hop neighbors to build node\nrepresentations at each layer. In principle, such networks are not able to\ncapture long-range interactions (LRI) that may be desired or necessary for\nlearning a given task on graphs. Recently, there has been an increasing\ninterest in development of Transformer-based methods for graphs that can\nconsider full node connectivity beyond the original sparse structure, thus\nenabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop\nmessage passing often fare better in several existing graph benchmarks when\ncombined with positional feature representations, among other innovations,\nhence limiting the perceived utility and ranking of Transformer-like\narchitectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5\ngraph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and\nPeptides-struct that arguably require LRI reasoning to achieve strong\nperformance in a given task. We benchmark both baseline GNNs and Graph\nTransformer networks to verify that the models which capture long-range\ndependencies perform significantly better on these tasks. Therefore, these\ndatasets are suitable for benchmarking and exploration of MP-GNNs and Graph\nTransformer architectures that are intended to capture LRI.",
    "descriptor": "\nComments: The benchmark is open-sourced at: this https URL\n",
    "authors": [
      "Vijay Prakash Dwivedi",
      "Ladislav Ramp\u00e1\u0161ek",
      "Mikhail Galkin",
      "Ali Parviz",
      "Guy Wolf",
      "Anh Tuan Luu",
      "Dominique Beaini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08164"
  },
  {
    "id": "arXiv:2206.08170",
    "title": "Adversarial Privacy Protection on Speech Enhancement",
    "abstract": "Speech is easily leaked imperceptibly, such as being recorded by mobile\nphones in different situations. Private content in speech may be maliciously\nextracted through speech enhancement technology. Speech enhancement technology\nhas developed rapidly along with deep neural networks (DNNs), but adversarial\nexamples can cause DNNs to fail. In this work, we propose an adversarial method\nto degrade speech enhancement systems. Experimental results show that generated\nadversarial examples can erase most content information in original examples or\nreplace it with target speech content through speech enhancement. The word\nerror rate (WER) between an enhanced original example and enhanced adversarial\nexample recognition result can reach 89.0%. WER of target attack between\nenhanced adversarial example and target example is low to 33.75% . Adversarial\nperturbation can bring the rate of change to the original example to more than\n1.4430. This work can prevent the malicious extraction of speech.",
    "descriptor": "\nComments: 5 pages, 6 figures\n",
    "authors": [
      "Mingyu Dong",
      "Diqun Yan",
      "Rangding Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08170"
  },
  {
    "id": "arXiv:2206.08171",
    "title": "K-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous  Driving in Various Weather Conditions",
    "abstract": "Unlike RGB cameras that use visible light bands (384$\\sim$769 THz) and Lidar\nthat use infrared bands (361$\\sim$331 THz), Radars use relatively longer\nwavelength radio bands (77$\\sim$81 GHz), resulting in robust measurements in\nadverse weathers. Unfortunately, existing Radar datasets only contain a\nrelatively small number of samples compared to the existing camera and Lidar\ndatasets. This may hinder the development of sophisticated data-driven deep\nlearning techniques for Radar-based perception. Moreover, most of the existing\nRadar datasets only provide 3D Radar tensor (3DRT) data that contain power\nmeasurements along the Doppler, range, and azimuth dimensions. As there is no\nelevation information, it is challenging to estimate the 3D bounding box of an\nobject from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel\nlarge-scale object detection dataset and benchmark that contains 35K frames of\n4D Radar tensor (4DRT) data with power measurements along the Doppler, range,\nazimuth, and elevation dimensions, together with carefully annotated 3D\nbounding box labels of objects on the roads. K-Radar includes challenging\ndriving conditions such as adverse weathers (fog, rain, and snow) on various\nroad structures (urban, suburban roads, alleyways, and highways). In addition\nto the 4DRT, we provide auxiliary measurements from carefully calibrated\nhigh-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide\n4DRT-based object detection baseline neural networks (baseline NNs) and show\nthat the height information is crucial for 3D object detection. And by\ncomparing the baseline NN with a similarly-structured Lidar-based neural\nnetwork, we demonstrate that 4D Radar is a more robust sensor for adverse\nweather conditions. All codes are available at\nhttps://github.com/kaist-avelab/k-radar.",
    "descriptor": "\nComments: 20 pages, 18 figures, 9 tables\n",
    "authors": [
      "Dong-Hee Paek",
      "Seung-Hyun Kong",
      "Kevin Tirta Wijaya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08171"
  },
  {
    "id": "arXiv:2206.08172",
    "title": "RefCrowd: Grounding the Target in Crowd with Referring Expressions",
    "abstract": "Crowd understanding has aroused the widespread interest in vision domain due\nto its important practical significance. Unfortunately, there is no effort to\nexplore crowd understanding in multi-modal domain that bridges natural language\nand computer vision. Referring expression comprehension (REF) is such a\nrepresentative multi-modal task. Current REF studies focus more on grounding\nthe target object from multiple distinctive categories in general scenarios. It\nis difficult to applied to complex real-world crowd understanding. To fill this\ngap, we propose a new challenging dataset, called RefCrowd, which towards\nlooking for the target person in crowd with referring expressions. It not only\nrequires to sufficiently mine the natural language information, but also\nrequires to carefully focus on subtle differences between the target and a\ncrowd of persons with similar appearance, so as to realize the fine-grained\nmapping from language to vision. Furthermore, we propose a Fine-grained\nMulti-modal Attribute Contrastive Network (FMAC) to deal with REF in crowd\nunderstanding. It first decomposes the intricate visual and language features\ninto attribute-aware multi-modal features, and then captures discriminative but\nrobustness fine-grained attribute features to effectively distinguish these\nsubtle differences between similar persons. The proposed method outperforms\nexisting state-of-the-art (SoTA) methods on our RefCrowd dataset and existing\nREF datasets. In addition, we implement an end-to-end REF toolbox for the\ndeeper research in multi-modal domain. Our dataset and code can be available\nat: \\url{https://qiuheqian.github.io/datasets/refcrowd/}.",
    "descriptor": "",
    "authors": [
      "Heqian Qiu",
      "Hongliang Li",
      "Taijin Zhao",
      "Lanxiao Wang",
      "Qingbo Wu",
      "Fanman Meng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08172"
  },
  {
    "id": "arXiv:2206.08175",
    "title": "Not All Lotteries Are Made Equal",
    "abstract": "The Lottery Ticket Hypothesis (LTH) states that for a reasonably sized neural\nnetwork, a sub-network within the same network yields no less performance than\nthe dense counterpart when trained from the same initialization. This work\ninvestigates the relation between model size and the ease of finding these\nsparse sub-networks. We show through experiments that, surprisingly, under a\nfinite budget, smaller models benefit more from Ticket Search (TS).",
    "descriptor": "\nComments: Accepted at ICML 2022 HAET Workshop\n",
    "authors": [
      "Surya Kant Sahu",
      "Sai Mitheran",
      "Somya Suhans Mahapatra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08175"
  },
  {
    "id": "arXiv:2206.08176",
    "title": "Level 2 Autonomous Driving on a Single Device: Diving into the Devils of  Openpilot",
    "abstract": "Equipped with a wide span of sensors, predominant autonomous driving\nsolutions are becoming more modular-oriented for safe system design. Though\nthese sensors have laid a solid foundation, most massive-production solutions\nup to date still fall into L2 phase. Among these, Comma.ai comes to our sight,\nclaiming one $999 aftermarket device mounted with a single camera and board\ninside owns the ability to handle L2 scenarios. Together with open-sourced\nsoftware of the entire system released by Comma.ai, the project is named\nOpenpilot. Is it possible? If so, how is it made possible? With curiosity in\nmind, we deep-dive into Openpilot and conclude that its key to success is the\nend-to-end system design instead of a conventional modular framework. The model\nis briefed as Supercombo, and it can predict the ego vehicle's future\ntrajectory and other road semantics on the fly from monocular input.\nUnfortunately, the training process and massive amount of data to make all\nthese work are not publicly available. To achieve an intensive investigation,\nwe try to reimplement the training details and test the pipeline on public\nbenchmarks. The refactored network proposed in this work is referred to as\nOP-Deepdive. For a fair comparison of our version to the original Supercombo,\nwe introduce a dual-model deployment scheme to test the driving performance in\nthe real world. Experimental results on nuScenes, Comma2k19, CARLA, and\nin-house realistic scenarios verify that a low-cost device can indeed achieve\nmost L2 functionalities and be on par with the original Supercombo model. In\nthis report, we would like to share our latest findings, shed some light on the\nnew perspective of end-to-end autonomous driving from an industrial\nproduct-level side, and potentially inspire the community to continue improving\nthe performance. Our code, benchmarks are at\nhttps://github.com/OpenPerceptionX/Openpilot-Deepdive.",
    "descriptor": "\nComments: Tech report. Project page: this https URL\n",
    "authors": [
      "Li Chen",
      "Tutian Tang",
      "Zhitian Cai",
      "Yang Li",
      "Penghao Wu",
      "Hongyang Li",
      "Jianping Shi",
      "Junchi Yan",
      "Yu Qiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08176"
  },
  {
    "id": "arXiv:2206.08181",
    "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural  Networks via Normalization",
    "abstract": "Graph Neural Networks (GNNs) have attracted much attention due to their\nability in learning representations from graph-structured data. Despite the\nsuccessful applications of GNNs in many domains, the optimization of GNNs is\nless well studied, and the performance on node classification heavily suffers\nfrom the long-tailed node degree distribution. This paper focuses on improving\nthe performance of GNNs via normalization.\nIn detail, by studying the long-tailed distribution of node degrees in the\ngraph, we propose a novel normalization method for GNNs, which is termed\nResNorm (\\textbf{Res}haping the long-tailed distribution into a normal-like\ndistribution via \\textbf{norm}alization). The $scale$ operation of ResNorm\nreshapes the node-wise standard deviation (NStd) distribution so as to improve\nthe accuracy of tail nodes (\\textit{i}.\\textit{e}., low-degree nodes). We\nprovide a theoretical interpretation and empirical evidence for understanding\nthe mechanism of the above $scale$. In addition to the long-tailed distribution\nissue, over-smoothing is also a fundamental issue plaguing the community. To\nthis end, we analyze the behavior of the standard shift and prove that the\nstandard shift serves as a preconditioner on the weight matrix, increasing the\nrisk of over-smoothing. With the over-smoothing issue in mind, we design a\n$shift$ operation for ResNorm that simulates the degree-specific parameter\nstrategy in a low-cost manner. Extensive experiments have validated the\neffectiveness of ResNorm on several node classification benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Langzhang Liang",
      "Zenglin Xu",
      "Zixing Song",
      "Irwin King",
      "Jieping Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08181"
  },
  {
    "id": "arXiv:2206.08182",
    "title": "Nucleus Segmentation and Analysis in Breast Cancer with the MIScnn  Framework",
    "abstract": "The NuCLS dataset contains over 220.000 annotations of cell nuclei in breast\ncancers. We show how to use these data to create a multi-rater model with the\nMIScnn Framework to automate the analysis of cell nuclei. For the model\ncreation, we use the widespread U-Net approach embedded in a pipeline. This\npipeline provides besides the high performance convolution neural network,\nseveral preprocessor techniques and a extended data exploration. The final\nmodel is tested in the evaluation phase using a wide variety of metrics with a\nsubsequent visualization. Finally, the results are compared and interpreted\nwith the results of the NuCLS study. As an outlook, indications are given which\nare important for the future development of models in the context of cell\nnuclei.",
    "descriptor": "",
    "authors": [
      "Adrian Pfleiderer",
      "Dominik M\u00fcller",
      "Frank Kramer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08182"
  },
  {
    "id": "arXiv:2206.08185",
    "title": "UAVs Beneath the Surface: Cooperative Autonomy for Subterranean Search  and Rescue in DARPA SubT",
    "abstract": "This paper presents a novel approach for autonomous cooperating UAVs in\nsearch and rescue operations in subterranean domains with complex topology. The\nproposed system was ranked second in the Virtual Track of the DARPA SubT Finals\nas part of the team CTU-CRAS-NORLAB. In contrast to the winning solution that\nwas developed specifically for the Virtual Track, the proposed solution also\nproved to be a robust system for deployment onboard physical UAVs flying in the\nextremely harsh and confined environment of the real-world competition. The\nproposed approach enables fully autonomous and decentralized deployment of a\nUAV team with seamless simulation-to-world transfer, and proves its advantage\nover less mobile UGV teams in the flyable space of diverse environments. The\nmain contributions of the paper are present in the mapping and navigation\npipelines. The mapping approach employs novel map representations -- SphereMap\nfor efficient risk-aware long-distance planning, FacetMap for surface coverage,\nand the compressed topological-volumetric LTVMap for allowing multi-robot\ncooperation under low-bandwidth communication. These representations are used\nin navigation together with novel methods for visibility-constrained informed\nsearch in a general 3D environment with no assumptions about the environment\nstructure, while balancing deep exploration with sensor-coverage exploitation.\nThe proposed solution also includes a visual-perception pipeline for on-board\ndetection and localization of objects of interest in four RGB stream at 5 Hz\neach without a dedicated GPU. Apart from participation in the DARPA SubT, the\nperformance of the UAV system is supported by extensive experimental\nverification in diverse environments with both qualitative and quantitative\nevaluation.",
    "descriptor": "\nComments: Submitted to Field Robotics Special Issue: DARPA Subterranean Challenge, Advancement and Lessons Learned from the Finals\n",
    "authors": [
      "Matej Petrlik",
      "Pavel Petracek",
      "Vit Kratky",
      "Tomas Musil",
      "Yurii Stasinchuk",
      "Matous Vrba",
      "Tomas Baca",
      "Daniel Hert",
      "Martin Pecka",
      "Tomas Svoboda",
      "Martin Saska"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08185"
  },
  {
    "id": "arXiv:2206.08186",
    "title": "Asymptotic Soft Cluster Pruning for Deep Neural Networks",
    "abstract": "Filter pruning method introduces structural sparsity by removing selected\nfilters and is thus particularly effective for reducing complexity. Previous\nworks empirically prune networks from the point of view that filter with\nsmaller norm contributes less to the final results. However, such criteria has\nbeen proven sensitive to the distribution of filters, and the accuracy may hard\nto recover since the capacity gap is fixed once pruned. In this paper, we\npropose a novel filter pruning method called Asymptotic Soft Cluster Pruning\n(ASCP), to identify the redundancy of network based on the similarity of\nfilters. Each filter from over-parameterized network is first distinguished by\nclustering, and then reconstructed to manually introduce redundancy into it.\nSeveral guidelines of clustering are proposed to better preserve feature\nextraction ability. After reconstruction, filters are allowed to be updated to\neliminate the effect caused by mistakenly selected. Besides, various decaying\nstrategies of the pruning rate are adopted to stabilize the pruning process and\nimprove the final performance as well. By gradually generating more identical\nfilters within each cluster, ASCP can remove them through channel addition\noperation with almost no accuracy drop. Extensive experiments on CIFAR-10 and\nImageNet datasets show that our method can achieve competitive results compared\nwith many state-of-the-art algorithms.",
    "descriptor": "",
    "authors": [
      "Tao Niu",
      "Yinglei Teng",
      "Panpan Zou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08186"
  },
  {
    "id": "arXiv:2206.08187",
    "title": "Approximating optimization problems in graphs with locational  uncertainty",
    "abstract": "Many combinatorial optimization problems can be formulated as the search for\na subgraph that satisfies certain properties and minimizes the total weight. We\nassume here that the vertices correspond to points in a metric space and can\ntake any position in given uncertainty sets. Then, the cost function to be\nminimized is the sum of the distances for the worst positions of the vertices\nin their uncertainty sets. We propose two types of polynomial-time\napproximation algorithms. The first one relies on solving a deterministic\ncounterpart of the problem where the uncertain distances are replaced with\nmaximum pairwise distances. We study in details the resulting approximation\nratio, which depends on the structure of the feasible subgraphs and whether the\nmetric space is Ptolemaic or not. The second algorithm is a fully-polynomial\ntime approximation scheme for the special case of $s-t$ paths.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2109.00389\n",
    "authors": [
      "Marin Bougeret",
      "J\u00e9r\u00e9my Omer",
      "Michael Poss"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08187"
  },
  {
    "id": "arXiv:2206.08189",
    "title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based  on Self-supervised Pre-training",
    "abstract": "Recent studies have shown that the benefits provided by self-supervised\npre-training and self-training (pseudo-labeling) are complementary.\nSemi-supervised fine-tuning strategies under the pre-training framework,\nhowever, remain insufficiently studied. Besides, modern semi-supervised speech\nrecognition algorithms either treat unlabeled data indiscriminately or filter\nout noisy samples with a confidence threshold. The dissimilarities among\ndifferent unlabeled data are often ignored. In this paper, we propose Censer, a\nsemi-supervised speech recognition algorithm based on self-supervised\npre-training to maximize the utilization of unlabeled data. The pre-training\nstage of Censer adopts wav2vec2.0 and the fine-tuning stage employs an improved\nsemi-supervised learning algorithm from slimIPL, which leverages unlabeled data\nprogressively according to their pseudo labels' qualities. We also incorporate\na temporal pseudo label pool and an exponential moving average to control the\npseudo labels' update frequency and to avoid model divergence. Experimental\nresults on Libri-Light and LibriSpeech datasets manifest our proposed method\nachieves better performance compared to existing approaches while being more\nunified.",
    "descriptor": "",
    "authors": [
      "Bowen Zhang",
      "Songjun Cao",
      "Xiaoming Zhang",
      "Yike Zhang",
      "Long Ma",
      "Takahiro Shinozaki"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08189"
  },
  {
    "id": "arXiv:2206.08194",
    "title": "Online Segmentation of LiDAR Sequences: Dataset and Algorithm",
    "abstract": "Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles,\ndriving the need for real-time processing of 3D point sequences. However, most\nLiDAR semantic segmentation datasets and algorithms split these acquisitions\ninto $360^\\circ$ frames, leading to acquisition latency that is incompatible\nwith realistic real-time applications and evaluations. We address this issue\nwith two key contributions. First, we introduce HelixNet, a $10$ billion point\ndataset with fine-grained labels, timestamps, and sensor rotation information\nthat allows an accurate assessment of real-time readiness of segmentation\nalgorithms. Second, we propose Helix4D, a compact and efficient spatio-temporal\ntransformer architecture specifically designed for rotating LiDAR point\nsequences. Helix4D operates on acquisition slices that correspond to a fraction\nof a full rotation of the sensor, significantly reducing the total latency. We\npresent an extensive benchmark of the performance and real-time readiness of\nseveral state-of-the-art models on HelixNet and SemanticKITTI. Helix4D reaches\naccuracy on par with the best segmentation algorithms with a reduction of more\nthan $5\\times$ in terms of latency and $50\\times$ in model size. Code and data\nare available at: https://romainloiseau.fr/helixnet",
    "descriptor": "\nComments: Code and data are available at: this https URL\n",
    "authors": [
      "Romain Loiseau",
      "Mathieu Aubry",
      "Lo\u00efc Landrieu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08194"
  },
  {
    "id": "arXiv:2206.08202",
    "title": "Token Spammers, Rug Pulls, and SniperBots: An Analysis of the Ecosystem  of Tokens in Ethereum and the Binance Smart Chain (BNB)",
    "abstract": "In this work, we perform a longitudinal analysis of the BNB Smart Chain and\nEthereum blockchain from their inception to March 2022. We study the ecosystem\nof the tokens and liquidity pools, highlighting analogies and differences\nbetween the two blockchains. We estimate the lifetime of the tokens,\ndiscovering that about 60% of them are active for less than one day. Moreover,\nwe find that 1% of addresses create an anomalous number of tokens (between 20%\nand 25%). We present an exit scam fraud and quantify its prevalence on both\nblockchains. We find that token spammers use short lifetime tokens as\ndisposable tokens to perpetrate these frauds serially. Finally, we present a\nnew kind of trader bot involved in these activities, and we detect their\npresence and quantify their activity in the exit scam operations.",
    "descriptor": "",
    "authors": [
      "Federico Cernera",
      "Massimo La Morgia",
      "Alessandro Mei",
      "Francesco Sassi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08202"
  },
  {
    "id": "arXiv:2206.08204",
    "title": "Inherent Inconsistencies of Feature Importance",
    "abstract": "The black-box nature of modern machine learning techniques invokes a\npractical and ethical need for explainability. Feature importance aims to meet\nthis need by assigning scores to features, so humans can understand their\ninfluence on predictions. Feature importance can be used to explain predictions\nunder different settings: of the entire sample space or a specific instance; of\nmodel behavior, or the dependencies in the data themselves. However, in most\ncases thus far, each of these settings was studied in isolation.\nWe attempt to develop a sound feature importance score framework by defining\na small set of desired properties. Surprisingly, we prove an inconsistency\ntheorem, showing that the expected properties cannot hold simultaneously. To\novercome this difficulty, we propose the novel notion of re-partitioning the\nfeature space into separable sets. Such sets are constructed to contain\nfeatures that exhibit inter-set independence with respect to the target\nvariable. We show that there exists a unique maximal partitioning into\nseparable sets. Moreover, assigning scores to separable sets, instead of single\nfeatures, unifies the results of commonly used feature importance scores and\nannihilates the inconsistencies we demonstrated.",
    "descriptor": "",
    "authors": [
      "Nimrod Harel",
      "Ran Gilad-Bachrach",
      "Uri Obolski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.08204"
  },
  {
    "id": "arXiv:2206.08206",
    "title": "Selective Multi-Scale Learning for Object Detection",
    "abstract": "Pyramidal networks are standard methods for multi-scale object detection.\nCurrent researches on feature pyramid networks usually adopt layer connections\nto collect features from certain levels of the feature hierarchy, and do not\nconsider the significant differences among them. We propose a better\narchitecture of feature pyramid networks, named selective multi-scale learning\n(SMSL), to address this issue. SMSL is efficient and general, which can be\nintegrated in both single-stage and two-stage detectors to boost detection\nperformance, with nearly no extra inference cost. RetinaNet combined with SMSL\nobtains 1.8\\% improvement in AP (from 39.1\\% to 40.9\\%) on COCO dataset. When\nintegrated with SMSL, two-stage detectors can get around 1.0\\% improvement in\nAP.",
    "descriptor": "\nComments: Accepted by ICANN2021\n",
    "authors": [
      "Junliang Chen",
      "Weizeng Lu",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08206"
  },
  {
    "id": "arXiv:2206.08213",
    "title": "A Closer Look at Smoothness in Domain Adversarial Training",
    "abstract": "Domain adversarial training has been ubiquitous for achieving invariant\nrepresentations and is used widely for various domain adaptation tasks. In\nrecent times, methods converging to smooth optima have shown improved\ngeneralization for supervised learning tasks like classification. In this work,\nwe analyze the effect of smoothness enhancing formulations on domain\nadversarial training, the objective of which is a combination of task loss (eg.\nclassification, regression, etc.) and adversarial terms. We find that\nconverging to a smooth minima with respect to (w.r.t.) task loss stabilizes the\nadversarial training leading to better performance on target domain. In\ncontrast to task loss, our analysis shows that converging to smooth minima\nw.r.t. adversarial loss leads to sub-optimal generalization on the target\ndomain. Based on the analysis, we introduce the Smooth Domain Adversarial\nTraining (SDAT) procedure, which effectively enhances the performance of\nexisting domain adversarial methods for both classification and object\ndetection tasks. Our analysis also provides insight into the extensive usage of\nSGD over Adam in the community for domain adversarial training.",
    "descriptor": "\nComments: ICML 2022. Code: this https URL\n",
    "authors": [
      "Harsh Rangwani",
      "Sumukh K Aithal",
      "Mayank Mishra",
      "Arihant Jain",
      "R. Venkatesh Babu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08213"
  },
  {
    "id": "arXiv:2206.08219",
    "title": "HaGRID - HAnd Gesture Recognition Image Dataset",
    "abstract": "In this paper, we introduce an enormous dataset HaGRID (HAnd Gesture\nRecognition Image Dataset) for hand gesture recognition (HGR) systems. This\ndataset contains 552,992 samples divided into 18 classes of gestures. The\nannotations consist of bounding boxes of hands with gesture labels and markups\nof leading hands. The proposed dataset allows for building HGR systems, which\ncan be used in video conferencing services, home automation systems, the\nautomotive sector, services for people with speech and hearing impairments,\netc. We are especially focused on interaction with devices to manage them. That\nis why all 18 chosen gestures are functional, familiar to the majority of\npeople, and may be an incentive to take some action. In addition, we used\ncrowdsourcing platforms to collect the dataset and took into account various\nparameters to ensure data diversity. We describe the challenges of using\nexisting HGR datasets for our task and provide a detailed overview of them.\nFurthermore, the baselines for the hand detection and gesture classification\ntasks are proposed.",
    "descriptor": "\nComments: 11 pages, 9 figures, open-source dataset for computer vision\n",
    "authors": [
      "Alexander Kapitanov",
      "Andrew Makhlyarchuk",
      "Karina Kvanchiani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08219"
  },
  {
    "id": "arXiv:2206.08222",
    "title": "Adapting Self-Supervised Vision Transformers by Probing  Attention-Conditioned Masking Consistency",
    "abstract": "Visual domain adaptation (DA) seeks to transfer trained models to unseen,\nunlabeled domains across distribution shift, but approaches typically focus on\nadapting convolutional neural network architectures initialized with supervised\nImageNet representations. In this work, we shift focus to adapting modern\narchitectures for object recognition -- the increasingly popular Vision\nTransformer (ViT) -- and modern pretraining based on self-supervised learning\n(SSL). Inspired by the design of recent SSL approaches based on learning from\npartial image inputs generated via masking or cropping -- either by learning to\npredict the missing pixels, or learning representational invariances to such\naugmentations -- we propose PACMAC, a simple two-stage adaptation algorithm for\nself-supervised ViTs. PACMAC first performs in-domain SSL on pooled source and\ntarget data to learn task-discriminative features, and then probes the model's\npredictive consistency across a set of partial target inputs generated via a\nnovel attention-conditioned masking strategy, to identify reliable candidates\nfor self-training. Our simple approach leads to consistent performance gains\nover competing methods that use ViTs and self-supervised initializations on\nstandard object recognition benchmarks. Code available at\nhttps://github.com/virajprabhu/PACMAC",
    "descriptor": "",
    "authors": [
      "Viraj Prabhu",
      "Sriram Yenamandra",
      "Aaditya Singh",
      "Judy Hoffman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08222"
  },
  {
    "id": "arXiv:2206.08223",
    "title": "Downlink Spectral Efficiency of Massive MIMO with Dual-Polarized  Antennas",
    "abstract": "This paper considers the downlink of a single-cell massive MIMO\n(multiple-input multiple-output) system with dual-polarized antennas at both\nthe base station and users. We consider a channel model that takes into account\nseveral practical aspects that arise when utilizing dual polarization, such as\nchannel cross-polar discrimination (XPD) and cross-polar receive and transmit\ncorrelations (XPC). We derive the statistical properties of the minimum mean\nsquared error (MMSE) channel estimator for this model. Using these estimates\nfor maximum ratio precoding, a rigorous closed-form downlink spectral\nefficiency (SE) expression is derived. We compare the SEs achieved in\ndual-polarized and uni-polarized setups numerically and evaluate the impact of\nXPD on the downlink SE.",
    "descriptor": "\nComments: Published at the 25th International ITG Workshop on Smart Antennas (WSA) 2021, 6 pages, 3 figures. arXiv admin note: substantial text overlap with arXiv:2202.10084\n",
    "authors": [
      "\u00d6zgecan \u00d6zdogan",
      "Emil Bj\u00f6rnson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08223"
  },
  {
    "id": "arXiv:2206.08224",
    "title": "Multi scale Feature Extraction and Fusion for Online Knowledge  Distillation",
    "abstract": "Online knowledge distillation conducts knowledge transfer among all student\nmodels to alleviate the reliance on pre-trained models. However, existing\nonline methods rely heavily on the prediction distributions and neglect the\nfurther exploration of the representational knowledge. In this paper, we\npropose a novel Multi-scale Feature Extraction and Fusion method (MFEF) for\nonline knowledge distillation, which comprises three key components:\nMulti-scale Feature Extraction, Dual-attention and Feature Fusion, towards\ngenerating more informative feature maps for distillation. The multiscale\nfeature extraction exploiting divide-and-concatenate in channel dimension is\nproposed to improve the multi-scale representation ability of feature maps. To\nobtain more accurate information, we design a dual-attention to strengthen the\nimportant channel and spatial regions adaptively. Moreover, we aggregate and\nfuse the former processed feature maps via feature fusion to assist the\ntraining of student models. Extensive experiments on CIF AR-10, CIF AR-100, and\nCINIC-10 show that MFEF transfers more beneficial representational knowledge\nfor distillation and outperforms alternative methods among various network\narchitectures",
    "descriptor": "\nComments: 12 pages, 3 figures\n",
    "authors": [
      "Panpan Zou",
      "Yinglei Teng",
      "Tao Niu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08224"
  },
  {
    "id": "arXiv:2206.08225",
    "title": "All the World's a (Hyper)Graph: A Data Drama",
    "abstract": "We introduce Hyperbard, a dataset of diverse relational data representations\nderived from Shakespeare's plays. Our representations range from simple graphs\ncapturing character co-occurrence in single scenes to hypergraphs encoding\ncomplex communication settings and character contributions as hyperedges with\nedge-specific node weights. By making multiple intuitive representations\nreadily available for experimentation, we facilitate rigorous representation\nrobustness checks in graph learning, graph mining, and network analysis,\nhighlighting the advantages and drawbacks of specific representations.\nLeveraging the data released in Hyperbard, we demonstrate that many solutions\nto popular graph mining problems are highly dependent on the representation\nchoice, thus calling current graph curation practices into question. As an\nhomage to our data source, and asserting that science can also be art, we\npresent all our points in the form of a play.",
    "descriptor": "",
    "authors": [
      "Corinna Coupette",
      "Jilles Vreeken",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2206.08225"
  },
  {
    "id": "arXiv:2206.08227",
    "title": "Delving into the Scale Variance Problem in Object Detection",
    "abstract": "Object detection has made substantial progress in the last decade, due to the\ncapability of convolution in extracting local context of objects. However, the\nscales of objects are diverse and current convolution can only process\nsingle-scale input. The capability of traditional convolution with a fixed\nreceptive field in dealing with such a scale variance problem, is thus limited.\nMulti-scale feature representation has been proven to be an effective way to\nmitigate the scale variance problem. Recent researches mainly adopt partial\nconnection with certain scales, or aggregate features from all scales and focus\non the global information across the scales. However, the information across\nspatial and depth dimensions is ignored. Inspired by this, we propose the\nmulti-scale convolution (MSConv) to handle this problem. Taking into\nconsideration scale, spatial and depth information at the same time, MSConv is\nable to process multi-scale input more comprehensively. MSConv is effective and\ncomputationally efficient, with only a small increase of computational cost.\nFor most of the single-stage object detectors, replacing the traditional\nconvolutions with MSConvs in the detection head can bring more than 2.5\\%\nimprovement in AP (on COCO 2017 dataset), with only 3\\% increase of FLOPs.\nMSConv is also flexible and effective for two-stage object detectors. When\nextended to the mainstream two-stage object detectors, MSConv can bring up to\n3.0\\% improvement in AP. Our best model under single-scale testing achieves\n48.9\\% AP on COCO 2017 \\textit{test-dev} split, which surpasses many\nstate-of-the-art methods.",
    "descriptor": "\nComments: Accepted by ICTAI2021\n",
    "authors": [
      "Junliang Chen",
      "Xiaodong Zhao",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08227"
  },
  {
    "id": "arXiv:2206.08229",
    "title": "Open-Set Recognition with Gradient-Based Representations",
    "abstract": "Neural networks for image classification tasks assume that any given image\nduring inference belongs to one of the training classes. This closed-set\nassumption is challenged in real-world applications where models may encounter\ninputs of unknown classes. Open-set recognition aims to solve this problem by\nrejecting unknown classes while classifying known classes correctly. In this\npaper, we propose to utilize gradient-based representations obtained from a\nknown classifier to train an unknown detector with instances of known classes\nonly. Gradients correspond to the amount of model updates required to properly\nrepresent a given sample, which we exploit to understand the model's capability\nto characterize inputs with its learned features. Our approach can be utilized\nwith any classifier trained in a supervised manner on known classes without the\nneed to model the distribution of unknown samples explicitly. We show that our\ngradient-based approach outperforms state-of-the-art methods by up to 11.6% in\nopen-set classification.",
    "descriptor": "\nComments: Published at IEEE International Conference on Image Processing (ICIP) 2021\n",
    "authors": [
      "Jinsol Lee",
      "Ghassan AlRegib"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08229"
  },
  {
    "id": "arXiv:2206.08232",
    "title": "Deep Learning Architecture for Automatic Essay Scoring",
    "abstract": "Automatic evaluation of essay (AES) and also called automatic essay scoring\nhas become a severe problem due to the rise of online learning and evaluation\nplatforms such as Coursera, Udemy, Khan academy, and so on. Researchers have\nrecently proposed many techniques for automatic evaluation. However, many of\nthese techniques use hand-crafted features and thus are limited from the\nfeature representation point of view. Deep learning has emerged as a new\nparadigm in machine learning which can exploit the vast data and identify the\nfeatures useful for essay evaluation. To this end, we propose a novel\narchitecture based on recurrent networks (RNN) and convolution neural network\n(CNN). In the proposed architecture, the multichannel convolutional layer\nlearns and captures the contextual features of the word n-gram from the word\nembedding vectors and the essential semantic concepts to form the feature\nvector at essay level using max-pooling operation. A variant of RNN called\nBi-gated recurrent unit (BGRU) is used to access both previous and subsequent\ncontextual representations. The experiment was carried out on eight data sets\navailable on Kaggle for the task of AES. The experimental results show that our\nproposed system achieves significantly higher grading accuracy than other deep\nlearning-based AES systems and also other state-of-the-art AES systems.",
    "descriptor": "",
    "authors": [
      "Tsegaye Misikir Tashu",
      "Chandresh Kumar Maurya",
      "Tomas Horvath"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08232"
  },
  {
    "id": "arXiv:2206.08233",
    "title": "Event-related data conditioning for acoustic event classification",
    "abstract": "Models based on diverse attention mechanisms have recently shined in tasks\nrelated to acoustic event classification (AEC). Among them, self-attention is\noften used in audio-only tasks to help the model recognize different acoustic\nevents. Self-attention relies on the similarity between time frames, and uses\nglobal information from the whole segment to highlight specific features within\na frame. In real life, information related to acoustic events will attenuate\nover time, which means the information within some frames around the event\ndeserves more attention than distant time global information that may be\nunrelated to the event. This paper shows that self-attention may over-enhance\ncertain segments of audio representations, and smooth out the boundaries\nbetween events representations and background noises. Hence, this paper\nproposes an event-related data conditioning (EDC) for AEC. EDC directly works\non spectrograms. The idea of EDC is to adaptively select the frame-related\nattention range based on acoustic features, and gather the event-related local\ninformation to represent the frame. Experiments show that: 1) compared with\nspectrogram-based data augmentation methods and trainable feature weighting and\nself-attention, EDC outperforms them in both the original-size mode and the\naugmented mode; 2) EDC effectively gathers event-related local information and\nenhances boundaries between events and backgrounds, improving the performance\nof AEC.",
    "descriptor": "\nComments: Accepted by INTERSPEECH 2022\n",
    "authors": [
      "Yuanbo Hou",
      "Dick Botteldooren"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08233"
  },
  {
    "id": "arXiv:2206.08236",
    "title": "Simple and Efficient Architectures for Semantic Segmentation",
    "abstract": "Though the state-of-the architectures for semantic segmentation, such as\nHRNet, demonstrate impressive accuracy, the complexity arising from their\nsalient design choices hinders a range of model acceleration tools, and further\nthey make use of operations that are inefficient on current hardware. This\npaper demonstrates that a simple encoder-decoder architecture with a\nResNet-like backbone and a small multi-scale head, performs on-par or better\nthan complex semantic segmentation architectures such as HRNet, FANet and\nDDRNets. Naively applying deep backbones designed for Image Classification to\nthe task of Semantic Segmentation leads to sub-par results, owing to a much\nsmaller effective receptive field of these backbones. Implicit among the\nvarious design choices put forth in works like HRNet, DDRNet, and FANet are\nnetworks with a large effective receptive field. It is natural to ask if a\nsimple encoder-decoder architecture would compare favorably if comprised of\nbackbones that have a larger effective receptive field, though without the use\nof inefficient operations like dilated convolutions. We show that with minor\nand inexpensive modifications to ResNets, enlarging the receptive field, very\nsimple and competitive baselines can be created for Semantic Segmentation. We\npresent a family of such simple architectures for desktop as well as mobile\ntargets, which match or exceed the performance of complex models on the\nCityscapes dataset. We hope that our work provides simple yet effective\nbaselines for practitioners to develop efficient semantic segmentation models.",
    "descriptor": "\nComments: To be presented at Efficient Deep Learning for Computer Vision Workshop at CVPR 2022\n",
    "authors": [
      "Dushyant Mehta",
      "Andrii Skliar",
      "Haitam Ben Yahia",
      "Shubhankar Borse",
      "Fatih Porikli",
      "Amirhossein Habibian",
      "Tijmen Blankevoort"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.08236"
  },
  {
    "id": "arXiv:2206.08237",
    "title": "Noisy Learning for Neural ODEs Acts as a Robustness Locus Widening",
    "abstract": "We investigate the problems and challenges of evaluating the robustness of\nDifferential Equation-based (DE) networks against synthetic distribution\nshifts. We propose a novel and simple accuracy metric which can be used to\nevaluate intrinsic robustness and to validate dataset corruption simulators. We\nalso propose methodology recommendations, destined for evaluating the many\nfaces of neural DEs' robustness and for comparing them with their discrete\ncounterparts rigorously. We then use this criteria to evaluate a cheap data\naugmentation technique as a reliable way for demonstrating the natural\nrobustness of neural ODEs against simulated image corruptions across multiple\ndatasets.",
    "descriptor": "\nComments: Accepted at ICLM 2022 Workshop \"PODS\"\n",
    "authors": [
      "Martin Gonzalez",
      "Hatem Hajri",
      "Loic Cantat",
      "Mihaly Petreczky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08237"
  },
  {
    "id": "arXiv:2206.08242",
    "title": "Catastrophic overfitting is a bug but also a feature",
    "abstract": "Despite clear computational advantages in building robust neural networks,\nadversarial training (AT) using single-step methods is unstable as it suffers\nfrom catastrophic overfitting (CO): Networks gain non-trivial robustness during\nthe first stages of adversarial training, but suddenly reach a breaking point\nwhere they quickly lose all robustness in just a few iterations. Although some\nworks have succeeded at preventing CO, the different mechanisms that lead to\nthis remarkable failure mode are still poorly understood. In this work,\nhowever, we find that the interplay between the structure of the data and the\ndynamics of AT plays a fundamental role in CO. Specifically, through active\ninterventions on typical datasets of natural images, we establish a causal link\nbetween the structure of the data and the onset of CO in single-step AT\nmethods. This new perspective provides important insights into the mechanisms\nthat lead to CO and paves the way towards a better understanding of the general\ndynamics of robust model construction. The code to reproduce the experiments of\nthis paper can be found at https://github.com/gortizji/co_features .",
    "descriptor": "\nComments: 24 pages, 19 figures, 1 table. Work partially presented at Adversarial Machine Learning Workshop at ICML 2022\n",
    "authors": [
      "Guillermo Ortiz-Jim\u00e9nez",
      "Pau de Jorge",
      "Amartya Sanyal",
      "Adel Bibi",
      "Puneet K. Dokania",
      "Pascal Frossard",
      "Gregory Rog\u00e9z",
      "Philip H.S. Torr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08242"
  },
  {
    "id": "arXiv:2206.08248",
    "title": "Twin-width and types",
    "abstract": "We study problems connected to first-order logic in graphs of bounded\ntwin-width. Inspired by the approach of Bonnet et al. [FOCS 2020], we introduce\na robust methodology of local types and describe their behavior in contraction\nsequences -- the decomposition notion underlying twin-width. We showcase the\napplicability of the methodology by proving the following two algorithmic\nresults. In both statements, we fix a first-order formula\n$\\varphi(x_1,\\ldots,x_k)$ and a constant $d$, and we assume that on input we\nare given a graph $G$ together with a contraction sequence of width at most\n$d$.\n(A) One can in time $O(n)$ construct a data structure that can answer the\nfollowing queries in time $O(\\log \\log n)$: given $w_1,\\ldots,w_k$, decide\nwhether $\\phi(w_1,\\ldots,w_k)$ holds in $G$.\n(B) After $O(n)$-time preprocessing, one can enumerate all tuples\n$w_1,\\ldots,w_k$ that satisfy $\\phi(x_1,\\ldots,x_k)$ in $G$ with $O(1)$ delay.\nIn the case of (A), the query time can be reduced to $O(1/\\varepsilon)$ at\nthe expense of increasing the construction time to $O(n^{1+\\varepsilon})$, for\nany fixed $\\varepsilon>0$. Finally, we also apply our tools to prove the\nfollowing statement, which shows optimal bounds on the VC density of set\nsystems that are first-order definable in graphs of bounded twin-width.\n(C) Let $G$ be a graph of twin-width $d$, $A$ be a subset of vertices of $G$,\nand $\\varphi(x_1,\\ldots,x_k,y_1,\\ldots,y_l)$ be a first-order formula. Then the\nnumber of different subsets of $A^k$ definable by $\\phi$ using $l$-tuples of\nvertices from $G$ as parameters, is bounded by $O(|A|^l)$.",
    "descriptor": "\nComments: 35 pages. Full version of an extended abstract to appear in the proceedings of ICALP 2022\n",
    "authors": [
      "Jakub Gajarsk\u00fd",
      "Micha\u0142 Pilipczuk",
      "Wojciech Przybyszewski",
      "Szymon Toru\u0144czyk"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Data Structures and Algorithms (cs.DS)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08248"
  },
  {
    "id": "arXiv:2206.08252",
    "title": "On the Surprising Behaviour of node2vec",
    "abstract": "Graph embedding techniques are a staple of modern graph learning research.\nWhen using embeddings for downstream tasks such as classification, information\nabout their stability and robustness, i.e., their susceptibility to sources of\nnoise, stochastic effects, or specific parameter choices, becomes increasingly\nimportant. As one of the most prominent graph embedding schemes, we focus on\nnode2vec and analyse its embedding quality from multiple perspectives. Our\nfindings indicate that embedding quality is unstable with respect to parameter\nchoices, and we propose strategies to remedy this in practice.",
    "descriptor": "\nComments: ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning (Camera-Ready Version)\n",
    "authors": [
      "Celia Hacker",
      "Bastian Rieck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08252"
  },
  {
    "id": "arXiv:2206.08255",
    "title": "Gradient-Based Adversarial and Out-of-Distribution Detection",
    "abstract": "We propose to utilize gradients for detecting adversarial and\nout-of-distribution samples. We introduce confounding labels -- labels that\ndiffer from normal labels seen during training -- in gradient generation to\nprobe the effective expressivity of neural networks. Gradients depict the\namount of change required for a model to properly represent given inputs,\nproviding insight into the representational power of the model established by\nnetwork architectural properties as well as training data. By introducing a\nlabel of different design, we remove the dependency on ground truth labels for\ngradient generation during inference. We show that our gradient-based approach\nallows for capturing the anomaly in inputs based on the effective expressivity\nof the models with no hyperparameter tuning or additional processing, and\noutperforms state-of-the-art methods for adversarial and out-of-distribution\ndetection.",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML) Workshop on New Frontiers in Adversarial Machine Learning, July 2022\n",
    "authors": [
      "Jinsol Lee",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08255"
  },
  {
    "id": "arXiv:2206.08257",
    "title": "Gradient Descent for Low-Rank Functions",
    "abstract": "Several recent empirical studies demonstrate that important machine learning\ntasks, e.g., training deep neural networks, exhibit low-rank structure, where\nthe loss function varies significantly in only a few directions of the input\nspace. In this paper, we leverage such low-rank structure to reduce the high\ncomputational cost of canonical gradient-based methods such as gradient descent\n(GD). Our proposed \\emph{Low-Rank Gradient Descent} (LRGD) algorithm finds an\n$\\epsilon$-approximate stationary point of a $p$-dimensional function by first\nidentifying $r \\leq p$ significant directions, and then estimating the true\n$p$-dimensional gradient at every iteration by computing directional\nderivatives only along those $r$ directions. We establish that the \"directional\noracle complexities\" of LRGD for strongly convex and non-convex objective\nfunctions are $\\mathcal{O}(r \\log(1/\\epsilon) + rp)$ and\n$\\mathcal{O}(r/\\epsilon^2 + rp)$, respectively. When $r \\ll p$, these\ncomplexities are smaller than the known complexities of $\\mathcal{O}(p\n\\log(1/\\epsilon))$ and $\\mathcal{O}(p/\\epsilon^2)$ of {\\gd} in the strongly\nconvex and non-convex settings, respectively. Thus, LRGD significantly reduces\nthe computational cost of gradient-based methods for sufficiently low-rank\nfunctions. In the course of our analysis, we also formally define and\ncharacterize the classes of exact and approximately low-rank functions.",
    "descriptor": "\nComments: 26 pages, 2 figures\n",
    "authors": [
      "Romain Cosson",
      "Ali Jadbabaie",
      "Anuran Makur",
      "Amirhossein Reisizadeh",
      "Devavrat Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.08257"
  },
  {
    "id": "arXiv:2206.08258",
    "title": "ProGNNosis: A Data-driven Model to Predict GNN Computation Time Using  Graph Metrics",
    "abstract": "Graph Neural Networks (GNN) show great promise in problems dealing with\ngraph-structured data. One of the unique points of GNNs is their flexibility to\nadapt to multiple problems, which not only leads to wide applicability, but\nalso poses important challenges when finding the best model or acceleration\ntechnique for a particular problem. An example of such challenges resides in\nthe fact that the accuracy or effectiveness of a GNN model or acceleration\ntechnique generally depends on the structure of the underlying graph. In this\npaper, in an attempt to address the problem of graph-dependent acceleration, we\npropose ProGNNosis, a data-driven model that can predict the GNN training time\nof a given GNN model running over a graph of arbitrary characteristics by\ninspecting the input graph metrics. Such prediction is made based on a\nregression that was previously trained offline using a diverse synthetic graph\ndataset. In practice, our method allows making informed decisions on which\ndesign to use for a specific problem. In the paper, the methodology to build\nProGNNosis is defined and applied for a specific use case, where it helps to\ndecide which graph representation is better. Our results show that ProGNNosis\nhelps achieve an average speedup of 1.22X over randomly selecting a graph\nrepresentation in multiple widely used GNN models such as GCN, GIN, GAT, or\nGraphSAGE.",
    "descriptor": "",
    "authors": [
      "Axel Wassington",
      "Sergi Abadal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08258"
  },
  {
    "id": "arXiv:2206.08259",
    "title": "CLEF. A Linked Open Data native system for Crowdsourcing",
    "abstract": "Collaborative data collection initiatives are increasingly becoming pivotal\nto cultural institutions and scholars, to boost the population of born-digital\narchives. For over a decade, organisations have been leveraging Semantic Web\ntechnologies to design their workflows, ensure data quality, and a means for\nsharing and reusing (Linked Data). Crucially, scholarly projects that leverage\ncultural heritage data to collaboratively develop new resources would benefit\nfrom agile solutions to simplify the Linked Data production workflow via\nuser-friendly interfaces. To date, only a few pioneers have abandoned legacy\ncataloguing and archiving systems to fully embrace the Linked Open Data (LOD)\nparadigm and manage their catalogues or research products through LOD-native\nmanagement systems. In this article we present Crowdsourcing Linked Entities\nvia web Form (CLEF), an agile LOD-native platform for collaborative data\ncollection, peer-review, and publication. We detail design choices as motivated\nby two case studies, from the Cultural Heritage and scholarly domains\nrespectively, and we discuss benefits of our solution in the light of prior\nworks. In particular, the strong focus on user-friendly interfaces for\nproducing FAIR data, the provenance-aware editorial process, and the\nintegration with consolidated data management workflows, distinguish CLEF as a\nnovel attempt to develop Linked Data platforms for cultural heritage.",
    "descriptor": "",
    "authors": [
      "Marilena Daquino",
      "Mari Wigham",
      "Enrico Daga",
      "Lucia Giagnolini",
      "Francesca Tomasi"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2206.08259"
  },
  {
    "id": "arXiv:2206.08260",
    "title": "Adversarial Robustness of Graph-based Anomaly Detection",
    "abstract": "Graph-based anomaly detection is becoming prevalent due to the powerful\nrepresentation abilities of graphs as well as recent advances in graph mining\ntechniques. These GAD tools, however, expose a new attacking surface,\nironically due to their unique advantage of being able to exploit the relations\namong data. That is, attackers now can manipulate those relations (i.e., the\nstructure of the graph) to allow target nodes to evade detection or degenerate\nthe classification performance of the detection. In this paper, we exploit this\nvulnerability by designing the structural poisoning attacks to a FeXtra-based\nGAD system termed OddBall as well as the black box attacks against GCN-based\nGAD systems by attacking the imbalanced lienarized GCN ( LGCN ). Specifically,\nwe formulate the attack against OddBall and LGCN as a one-level optimization\nproblem by incorporating different regression techniques, where the key\ntechnical challenge is to efficiently solve the problem in a discrete domain.\nWe propose a novel attack method termed BinarizedAttack based on gradient\ndescent. Comparing to prior arts, BinarizedAttack can better use the gradient\ninformation, making it particularly suitable for solving discrete optimization\nproblems, thus opening the door to studying a new type of attack against\nsecurity analytic tools that rely on graph data.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2106.09989\n",
    "authors": [
      "Yulin Zhu",
      "Yuni Lai",
      "Kaifa Zhao",
      "Xiapu Luo",
      "Mingquan Yuan",
      "Jian Ren",
      "Kai Zhou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.08260"
  },
  {
    "id": "arXiv:2206.08261",
    "title": "To Help or Disturb: Introduction of Crowdsourced WiFi to 5G Networks",
    "abstract": "After upgrading to 5G, a network operator still faces congestion when\nproviding the ubiquitous wireless service to the crowd. To meet users'\never-increasing demand, some other operators (e.g., Fon) have been developing\nanother crowdsourced WiFi network to combine many users' home WiFi access\npoints and provide enlarged WiFi coverage to them. While the 5G network\nexperiences negative network externality, the crowdsourced WiFi network helps\noffload traffic from 5G and its service coverage exhibits positive externality\nwith its subscription number. To our best knowledge, we are the first to\ninvestigate how these two heterogeneous networks of diverse network\nexternalities co-exist from an economic perspective. We propose a dynamic game\ntheoretic model to analyze the hybrid interaction among the 5G operator, the\ncrowdsourced WiFi operator, and users. Our user choice model with WiFi's\ncomplementarity for 5G allows users to choose both services, departing from the\ntraditional economics literature where a user chooses one over another\nalternative. Despite of non-convexity of the operators' pricing problems, we\nprove that the 5G operator facing severe congestion may purposely lower his\nprice to encourage users to add-on WiFi to offload, and he benefits from the\nintroduction of crowdsourced WiFi. However, 5G operator with mild congestion\ntends to charge users more and all the users' payoffs may decrease.",
    "descriptor": "",
    "authors": [
      "Shugang Hao",
      "Lingjie Duan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2206.08261"
  },
  {
    "id": "arXiv:2206.08263",
    "title": "'John ate 5 apples' != 'John ate some apples': Self-Supervised  Paraphrase Quality Detection for Algebraic Word Problems",
    "abstract": "This paper introduces the novel task of scoring paraphrases for Algebraic\nWord Problems (AWP) and presents a self-supervised method for doing so. In the\ncurrent online pedagogical setting, paraphrasing these problems is helpful for\nacademicians to generate multiple syntactically diverse questions for\nassessments. It also helps induce variation to ensure that the student has\nunderstood the problem instead of just memorizing it or using unfair means to\nsolve it. The current state-of-the-art paraphrase generation models often\ncannot effectively paraphrase word problems, losing a critical piece of\ninformation (such as numbers or units) which renders the question unsolvable.\nThere is a need for paraphrase scoring methods in the context of AWP to enable\nthe training of good paraphrasers. Thus, we propose ParaQD, a self-supervised\nparaphrase quality detection method using novel data augmentations that can\nlearn latent representations to separate a high-quality paraphrase of an\nalgebraic question from a poor one by a wide margin. Through extensive\nexperimentation, we demonstrate that our method outperforms existing\nstate-of-the-art self-supervised methods by up to 32% while also demonstrating\nimpressive zero-shot performance.",
    "descriptor": "\nComments: Accepted as a full paper at ECML-PKDD'22, 26 Pages\n",
    "authors": [
      "Rishabh Gupta",
      "Venktesh V",
      "Mukesh Mohania",
      "Vikram Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08263"
  },
  {
    "id": "arXiv:2206.08264",
    "title": "Towards the Generation of Musical Explanations with GPT-3",
    "abstract": "Open AI's language model, GPT-3, has shown great potential for many NLP\ntasks, with applications in many different domains. In this work we carry out a\nfirst study on GPT-3's capability to communicate musical decisions through\ntextual explanations when prompted with a textual representation of a piece of\nmusic. Enabling a dialogue in human-AI music partnerships is an important step\ntowards more engaging and creative human-AI interactions. Our results show that\nGPT-3 lacks the necessary intelligence to really understand musical decisions.\nA major barrier to reach a better performance is the lack of data that includes\nexplanations of the creative process carried out by artists for musical pieces.\nWe believe such a resource would aid the understanding and collaboration with\nAI music systems.",
    "descriptor": "",
    "authors": [
      "Stephen James Krol",
      "Maria Teresa Llano",
      "Jon McCormack"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08264"
  },
  {
    "id": "arXiv:2206.08266",
    "title": "ANGLEr: A Next-Generation Natural Language Exploratory Framework",
    "abstract": "Natural language processing is used for solving a wide variety of problems.\nSome scholars and interest groups working with language resources are not well\nversed in programming, so there is a need for a good graphical framework that\nallows users to quickly design and test natural language processing pipelines\nwithout the need for programming. The existing frameworks do not satisfy all\nthe requirements for such a tool. We, therefore, propose a new framework that\nprovides a simple way for its users to build language processing pipelines. It\nalso allows a simple programming language agnostic way for adding new modules,\nwhich will help the adoption by natural language processing developers and\nresearchers. The main parts of the proposed framework consist of (a) a\npluggable Docker-based architecture, (b) a general data model, and (c) APIs\ndescription along with the graphical user interface. The proposed design is\nbeing used for implementation of a new natural language processing framework,\ncalled ANGLEr.",
    "descriptor": "",
    "authors": [
      "Timotej Knez",
      "Marko Bajec",
      "Slavko \u017ditnik"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08266"
  },
  {
    "id": "arXiv:2206.08267",
    "title": "Ratatouille: A tool for Novel Recipe Generation",
    "abstract": "Due to availability of a large amount of cooking recipes online, there is a\ngrowing interest in using this as data to create novel recipes. Novel Recipe\nGeneration is a problem in the field of Natural Language Processing in which\nour main interest is to generate realistic, novel cooking recipes. To come up\nwith such novel recipes, we trained various Deep Learning models such as LSTMs\nand GPT-2 with a large amount of recipe data. We present Ratatouille\n(https://cosylab.iiitd.edu.in/ratatouille2/), a web based application to\ngenerate novel recipes.",
    "descriptor": "\nComments: 4 pages, 5 figures, 38th IEEE International Conference on Data Engineering, DECOR Workshop\n",
    "authors": [
      "Mansi Goel",
      "Pallab Chakraborty",
      "Vijay Ponnaganti",
      "Minnet Khan",
      "Sritanaya Tatipamala",
      "Aakanksha Saini",
      "Ganesh Bagler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08267"
  },
  {
    "id": "arXiv:2206.08269",
    "title": "Learning with little mixing",
    "abstract": "We study square loss in a realizable time-series framework with martingale\ndifference noise. Our main result is a fast rate excess risk bound which shows\nthat whenever a trajectory hypercontractivity condition holds, the risk of the\nleast-squares estimator on dependent data matches the iid rate order-wise after\na burn-in time. In comparison, many existing results in learning from dependent\ndata have rates where the effective sample size is deflated by a factor of the\nmixing-time of the underlying process, even after the burn-in time.\nFurthermore, our results allow the covariate process to exhibit long range\ncorrelations which are substantially weaker than geometric ergodicity. We call\nthis phenomenon learning with little mixing, and present several examples for\nwhen it occurs: bounded function classes for which the $L^2$ and\n$L^{2+\\epsilon}$ norms are equivalent, ergodic finite state Markov chains,\nvarious parametric models, and a broad family of infinite dimensional\n$\\ell^2(\\mathbb{N})$ ellipsoids. By instantiating our main result to system\nidentification of nonlinear dynamics with generalized linear model transitions,\nwe obtain a nearly minimax optimal excess risk bound after only a polynomial\nburn-in time.",
    "descriptor": "",
    "authors": [
      "Ingvar Ziemann",
      "Stephen Tu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08269"
  },
  {
    "id": "arXiv:2206.08270",
    "title": "The Road to a Successful HRI: AI, Trust and ethicS-TRAITS",
    "abstract": "The aim of this workshop is to foster the exchange of insights on past and\nongoing research towards effective and long-lasting collaborations between\nhumans and robots. This workshop will provide a forum for representatives from\nacademia and industry communities to analyse the different aspects of HRI that\nimpact on its success. We particularly focus on AI techniques required to\nimplement autonomous and proactive interactions, on the factors that enhance,\nundermine, or recover humans' acceptance and trust in robots, and on the\npotential ethical and legal concerns related to the deployment of such robots\nin human-centred environments.\nWebsite: https://sites.google.com/view/traits-hri-2022",
    "descriptor": "\nComments: TRAITS Workshop Proceedings including 7 articles\n",
    "authors": [
      "Alessandra Rossi",
      "Antonio Andriella",
      "Silvia Rossi",
      "Anouk van Maris"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08270"
  },
  {
    "id": "arXiv:2206.08275",
    "title": "Rank the triplets: A ranking-based multiple instance learning framework  for detecting HPV infection in head and neck cancers using routine H&E images",
    "abstract": "The aetiology of head and neck squamous cell carcinoma (HNSCC) involves\nmultiple carcinogens such as alcohol, tobacco and infection with human\npapillomavirus (HPV). As the HPV infection influences the prognosis, treatment\nand survival of patients with HNSCC, it is important to determine the HPV\nstatus of these tumours. In this paper, we propose a novel triplet-ranking loss\nfunction and a multiple instance learning pipeline for HPV status prediction.\nThis achieves a new state-of-the-art performance in HPV detection using only\nthe routine H&E stained WSIs on two HNSCC cohorts. Furthermore, a comprehensive\ntumour microenvironment profiling was performed, which characterised the unique\npatterns between HPV+/- HNSCC from genomic, immunology and cellular\nperspectives. Positive correlations of the proposed score with different\nsubtypes of T cells (e.g. T cells follicular helper, CD8+ T cells), and\nnegative correlations with macrophages and connective cells (e.g. fibroblast)\nwere identified, which is in line with clinical findings. Unique gene\nexpression profiles were also identified with respect to HPV infection status,\nand is in line with existing findings.",
    "descriptor": "",
    "authors": [
      "Ruoyu Wang",
      "Syed Ali Khurram",
      "Amina Asif",
      "Lawrence Young",
      "Nasir Rajpoot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08275"
  },
  {
    "id": "arXiv:2206.08278",
    "title": "Securing Automotive Architectures with Named Data Networking",
    "abstract": "As in-vehicle communication becomes more complex, the automotive community is\nexploring various architectural options such as centralized and zonal\narchitectures for their numerous benefits. Zonal architecture reduces the\nwiring cost by physically locating related operations and ECUs near their\nintended functions and the number of physical ECUs through function\nconsolidation. Centralized architectures consolidate the number of ECUs into\nfew, powerful compute units. Common characteristics of these architectures\ninclude the need for high-bandwidth communication and security, which have been\nelusive with standard automotive architectures. Further, as automotive\ncommunication technologies evolve, it is also likely that multiple link-layer\ntechnologies such as CAN and Automotive Ethernet will co-exist. These\nalternative architectures promise to integrate these diverse sets of\ntechnologies. However, architectures that allow such co-existence have not been\nadequately explored.\nIn this work we explore a new network architecture called Named Data\nNetworking (NDN) to achieve multiple goals: provide a foundational security\ninfrastructure and bridge different link layer protocols such as CAN, LIN, and\nautomotive Ethernet into a unified communication system.\nWe created a proof-of-concept bench-top testbed using CAN HATS and Raspberry\nPIs that replay real traffic over CAN and Ethernet to demonstrate how NDN can\nprovide a secure, high-speed bridge between different automotive link layers.\nWe also show how NDN can support communication between centralized or zonal\nhigh-power compute components. Security is achieved through digitally signing\nall Data packets between these components, preventing unauthorized ECUs from\ninjecting arbitrary data into the network. We also demonstrate NDN's ability to\nprevent DoS and replay attacks between different network segments connected\nthrough NDN.",
    "descriptor": "",
    "authors": [
      "Zachariah Threet",
      "Christos Papadopoulos",
      "William Lambert",
      "Proyash Podder",
      "Spiros Thanasoulas",
      "Alex Afanasyev",
      "Sheikh Ghafoor",
      "Susmit Shannigrahi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.08278"
  },
  {
    "id": "arXiv:2206.08287",
    "title": "Alternative models: Critical examination of disability definitions in  the development of artificial intelligence technologies",
    "abstract": "Disabled people are subject to a wide variety of complex decision-making\nprocesses in diverse areas such as healthcare, employment, and government\npolicy. These contexts, which are already often opaque to the people they\naffect and lack adequate representation of disabled perspectives, are rapidly\nadopting artificial intelligence (AI) technologies for data analytics to inform\ndecision making, creating an increased risk of harm due to inappropriate or\ninequitable algorithms. This article presents a framework for critically\nexamining AI data analytics technologies through a disability lens and\ninvestigates how the definition of disability chosen by the designers of an AI\ntechnology affects its impact on disabled subjects of analysis. We consider\nthree conceptual models of disability: the medical model, the social model, and\nthe relational model; and show how AI technologies designed under each of these\nmodels differ so significantly as to be incompatible with and contradictory to\none another. Through a discussion of common use cases for AI analytics in\nhealthcare and government disability benefits, we illustrate specific\nconsiderations and decision points in the technology design process that affect\npower dynamics and inclusion in these settings and help determine their\norientation towards marginalisation or support. The framework we present can\nserve as a foundation for in-depth critical examination of AI technologies and\nthe development of a design praxis for disability-related AI analytics.",
    "descriptor": "\nComments: 25 pages, 1 figure, 2 tables. Keywords: artificial intelligence; critical disability studies; information and communication technologies; data analytics; data science\n",
    "authors": [
      "Denis Newman-Griffis",
      "Jessica Sage Rauchberg",
      "Rahaf Alharbi",
      "Louise Hickman",
      "Harry Hochheiser"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.08287"
  },
  {
    "id": "arXiv:2206.08288",
    "title": "Balancing Cost and Quality: An Exploration of Human-in-the-loop  Frameworks for Automated Short Answer Scoring",
    "abstract": "Short answer scoring (SAS) is the task of grading short text written by a\nlearner. In recent years, deep-learning-based approaches have substantially\nimproved the performance of SAS models, but how to guarantee high-quality\npredictions still remains a critical issue when applying such models to the\neducation field. Towards guaranteeing high-quality predictions, we present the\nfirst study of exploring the use of human-in-the-loop framework for minimizing\nthe grading cost while guaranteeing the grading quality by allowing a SAS model\nto share the grading task with a human grader. Specifically, by introducing a\nconfidence estimation method for indicating the reliability of the model\npredictions, one can guarantee the scoring quality by utilizing only\npredictions with high reliability for the scoring results and casting\npredictions with low reliability to human graders. In our experiments, we\ninvestigate the feasibility of the proposed framework using multiple confidence\nestimation methods and multiple SAS datasets. We find that our\nhuman-in-the-loop framework allows automatic scoring models and human graders\nto achieve the target scoring quality.",
    "descriptor": "\nComments: 12pages, To be published in proceedings of AIED2022\n",
    "authors": [
      "Hiroaki Funayama",
      "Tasuku Sato",
      "Yuichiroh Matsubayashi",
      "Tomoya Mizumoto",
      "Jun Suzuki",
      "Kentaro Inui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08288"
  },
  {
    "id": "arXiv:2206.08289",
    "title": "Switchable Representation Learning Framework with Self-compatibility",
    "abstract": "Real-world visual search systems involve deployments on multiple platforms\nwith different computing and storage resources. Deploying a unified model that\nsuits the minimal-constrain platforms leads to limited accuracy. It is expected\nto deploy models with different capacities adapting to the resource\nconstraints, which requires features extracted by these models to be aligned in\nthe metric space. The method to achieve feature alignments is called\n\"compatible learning\". Existing research mainly focuses on the one-to-one\ncompatible paradigm, which is limited in learning compatibility among multiple\nmodels. We propose a Switchable representation learning Framework with\nSelf-Compatibility (SFSC). SFSC generates a series of compatible sub-models\nwith different capacities through one training process. The optimization of\nsub-models faces gradients conflict, and we mitigate it from the perspective of\nthe magnitude and direction. We adjust the priorities of sub-models dynamically\nthrough uncertainty estimation to co-optimize sub-models properly. Besides, the\ngradients with conflicting directions are projected to avoid mutual\ninterference. SFSC achieves state-of-art performance on the evaluated dataset.",
    "descriptor": "",
    "authors": [
      "Shengsen Wu",
      "Yan Bai",
      "Yihang Lou",
      "Xiongkun Linghu",
      "Jianzhong He",
      "Tao Bai",
      "Ling-Yu Duan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08289"
  },
  {
    "id": "arXiv:2206.08292",
    "title": "Closed-loop Position Control of a Pediatric Soft Robotic Wearable Device  for Upper Extremity Assistance",
    "abstract": "This work focuses on closed-loop control based on proprioceptive feedback for\na pneumatically-actuated soft wearable device aimed at future support of infant\nreaching tasks. The device comprises two soft pneumatic actuators (one\ntextile-based and one silicone-casted) actively controlling two\ndegrees-of-freedom per arm (shoulder adduction/abduction and elbow\nflexion/extension, respectively). Inertial measurement units (IMUs) attached to\nthe wearable device provide real-time joint angle feedback. Device kinematics\nanalysis is informed by anthropometric data from infants (arm lengths) reported\nin the literature. Range of motion and muscle co-activation patterns in infant\nreaching are considered to derive desired trajectories for the device's\nend-effector. Then, a proportional-derivative controller is developed to\nregulate the pressure inside the actuators and in turn move the arm along\ndesired setpoints within the reachable workspace. Experimental results on\ntracking desired arm trajectories using an engineered mannequin are presented,\ndemonstrating that the proposed controller can help guide the mannequin's wrist\nto the desired setpoints.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Caio Mucchiani",
      "Zhichao Liu",
      "Ipsita Sahin",
      "Jared Dube",
      "Linh Vu",
      "Elena Kokkoni",
      "Konstantinos Karydis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08292"
  },
  {
    "id": "arXiv:2206.08297",
    "title": "GoodBye WaveNet -- A Language Model for Raw Audio with Context of 1/2  Million Samples",
    "abstract": "Modeling long-term dependencies for audio signals is a particularly\nchallenging problem, as even small-time scales yield on the order of a hundred\nthousand samples. With the recent advent of Transformers, neural architectures\nbecame good at modeling dependencies over longer time scales, but they suffered\nfrom quadratic constraints to scale them. We propose a generative\nauto-regressive architecture that can model audio waveforms over quite a large\ncontext, greater than 500,000 samples. Our work is adapted to learn time\ndependencies by learning a latent representation by a CNN front-end, and then\nlearning dependencies over these representations using Transformer encoders,\nfully trained end-to-end: thereby allowing to learn representations as it deems\nfit for the next sample. Unlike previous works that compared different time\nscales to show improvement, we use a standard dataset, with the same number of\nparameters/context to show improvements. We achieve a state-of-the-art\nperformance as compared to other approaches such as Wavenet, SaSHMI, and\nSample-RNN on a standard dataset for modeling long-term structure. This work\ngives very exciting direction for the field, given improvements in context\nmodeling that can be scaled with more data, as well as potentially better\nresults by using billions/trillions of parameters.",
    "descriptor": "\nComments: 12 pages, 1 figure. Technical Report at Stanford University. Ongoing Work\n",
    "authors": [
      "Prateek Verma"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08297"
  },
  {
    "id": "arXiv:2206.08301",
    "title": "Deinsum: Practically I/O Optimal Multilinear Algebra",
    "abstract": "Multilinear algebra kernel performance on modern massively-parallel systems\nis determined mainly by data movement. However, deriving data movement-optimal\ndistributed schedules for programs with many high-dimensional inputs is a\nnotoriously hard problem. State-of-the-art libraries rely on heuristics and\noften fall back to suboptimal tensor folding and BLAS calls. We present\nDeinsum, an automated framework for distributed multilinear algebra\ncomputations expressed in Einstein notation, based on rigorous mathematical\ntools to address this problem. Our framework automatically derives data\nmovement-optimal tiling and generates corresponding distributed schedules,\nfurther optimizing the performance of local computations by increasing their\narithmetic intensity. To show the benefits of our approach, we test it on two\nimportant tensor kernel classes: Matricized Tensor Times Khatri-Rao Products\nand Tensor Times Matrix chains. We show performance results and scaling on the\nPiz Daint supercomputer, with up to 19x speedup over state-of-the-art solutions\non 512 nodes.",
    "descriptor": "",
    "authors": [
      "Alexandros Nikolaos Ziogas",
      "Grzegorz Kwasniewski",
      "Tal Ben-Nun",
      "Timo Schneider",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2206.08301"
  },
  {
    "id": "arXiv:2206.08303",
    "title": "On Scaled Methods for Saddle Point Problems",
    "abstract": "Methods with adaptive scaling of different features play a key role in\nsolving saddle point problems, primarily due to Adam's popularity for solving\nadversarial machine learning problems, including GANS training. This paper\ncarries out a theoretical analysis of the following scaling techniques for\nsolving SPPs: the well-known Adam and RmsProp scaling and the newer AdaHessian\nand OASIS based on Hutchison approximation. We use the Extra Gradient and its\nimproved version with negative momentum as the basic method. Experimental\nstudies on GANs show good applicability not only for Adam, but also for other\nless popular methods.",
    "descriptor": "\nComments: 51 pages, 2 algorithms with 4 options for each, 12 figures, 5 tables, 2 theorems\n",
    "authors": [
      "Aleksandr Beznosikov",
      "Aibek Alanov",
      "Dmitry Kovalev",
      "Martin Tak\u00e1\u010d",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.08303"
  },
  {
    "id": "arXiv:2206.08304",
    "title": "Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey",
    "abstract": "Adversarial attacks in deep learning models, especially for safety-critical\nsystems, are gaining more and more attention in recent years, due to the lack\nof trust in the security and robustness of AI models. Yet the more primitive\nadversarial attacks might be physically infeasible or require some resources\nthat are hard to access like the training data, which motivated the emergence\nof patch attacks. In this survey, we provide a comprehensive overview to cover\nexisting techniques of adversarial patch attacks, aiming to help interested\nresearchers quickly catch up with the progress in this field. We also discuss\nexisting techniques for developing detection and defences against adversarial\npatches, aiming to help the community better understand this field and its\napplications in the real world.",
    "descriptor": "\nComments: A. Sharma and Y. Bian share equal contribution\n",
    "authors": [
      "Abhijith Sharma",
      "Yijun Bian",
      "Phil Munz",
      "Apurva Narayan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.08304"
  },
  {
    "id": "arXiv:2206.08306",
    "title": "A Comprehensive Eco-Driving Strategy for Connected and Autonomous  Vehicles (CAVs) with Microscopic Traffic Simulation Testing Evaluation",
    "abstract": "In this paper, a comprehensive Eco-Driving strategy for CAVs is presented. In\nthis setup, multiple driving modes calculate speed profiles ideal for their own\nset of constraints simultaneously to save fuel as much as possible, while a\nHigh Level (HL) controller ensures smooth transitions between the driving modes\nfor Eco-Driving. This Eco-Driving deterministic controller for an ego CAV was\nequipped with Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V)\nalgorithms. Simulation results are used to show that the HL controller ensures\nsignificant fuel economy improvement as compared to baseline driving modes with\nno collisions between the ego CAV and traffic vehicles while the driving mode\nof the ego CAV was set correctly under changing constraints.",
    "descriptor": "",
    "authors": [
      "Ozgenur Kavas-Torris",
      "Levent Guvenc"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08306"
  },
  {
    "id": "arXiv:2206.08307",
    "title": "Sharper Convergence Guarantees for Asynchronous SGD for Distributed and  Federated Learning",
    "abstract": "We study the asynchronous stochastic gradient descent algorithm for\ndistributed training over $n$ workers which have varying computation and\ncommunication frequency over time. In this algorithm, workers compute\nstochastic gradients in parallel at their own pace and return those to the\nserver without any synchronization. Existing convergence rates of this\nalgorithm for non-convex smooth objectives depend on the maximum gradient delay\n$\\tau_{\\max}$ and show that an $\\epsilon$-stationary point is reached after\n$\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{\\max}\\epsilon^{-1}\\right)$\niterations, where $\\sigma$ denotes the variance of stochastic gradients.\nIn this work (i) we obtain a tighter convergence rate of\n$\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+\n\\sqrt{\\tau_{\\max}\\tau_{avg}}\\epsilon^{-1}\\right)$ without any change in the\nalgorithm where $\\tau_{avg}$ is the average delay, which can be significantly\nsmaller than $\\tau_{\\max}$. We also provide (ii) a simple delay-adaptive\nlearning rate scheme, under which asynchronous SGD achieves a convergence rate\nof $\\mathcal{O}\\!\\left(\\sigma^2\\epsilon^{-2}+ \\tau_{avg}\\epsilon^{-1}\\right)$,\nand does not require any extra hyperparameter tuning nor extra communications.\nOur result allows to show for the first time that asynchronous SGD is always\nfaster than mini-batch SGD. In addition, (iii) we consider the case of\nheterogeneous functions motivated by federated learning applications and\nimprove the convergence rate by proving a weaker dependence on the maximum\ndelay compared to prior works. In particular, we show that the heterogeneity\nterm in convergence rate is only affected by the average delay within each\nworker.",
    "descriptor": "",
    "authors": [
      "Anastasia Koloskova",
      "Sebastian U. Stich",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.08307"
  },
  {
    "id": "arXiv:2206.08309",
    "title": "Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use  Case",
    "abstract": "In recent years, deep generative models have attracted increasing interest\ndue to their capacity to model complex distributions. Among those models,\nvariational autoencoders have gained popularity as they have proven both to be\ncomputationally efficient and yield impressive results in multiple fields.\nFollowing this breakthrough, extensive research has been done in order to\nimprove the original publication, resulting in a variety of different VAE\nmodels in response to different tasks. In this paper we present Pythae, a\nversatile open-source Python library providing both a unified implementation\nand a dedicated framework allowing straightforward, reproducible and reliable\nuse of generative autoencoder models. We then propose to use this library to\nperform a case study benchmark where we present and compare 19 generative\nautoencoder models representative of some of the main improvements on\ndownstream tasks such as image reconstruction, generation, classification,\nclustering and interpolation. The open-source library can be found at\nhttps://github.com/clementchadebec/benchmark_VAE.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Chadebec",
      "Louis J. Vincent",
      "St\u00e9phanie Allassonni\u00e8re"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08309"
  },
  {
    "id": "arXiv:2206.08311",
    "title": "Continuous-Time Modeling of Counterfactual Outcomes Using Neural  Controlled Differential Equations",
    "abstract": "Estimating counterfactual outcomes over time has the potential to unlock\npersonalized healthcare by assisting decision-makers to answer ''what-iF''\nquestions. Existing causal inference approaches typically consider regular,\ndiscrete-time intervals between observations and treatment decisions and hence\nare unable to naturally model irregularly sampled data, which is the common\nsetting in practice. To handle arbitrary observation patterns, we interpret the\ndata as samples from an underlying continuous-time process and propose to model\nits latent trajectory explicitly using the mathematics of controlled\ndifferential equations. This leads to a new approach, the Treatment Effect\nNeural Controlled Differential Equation (TE-CDE), that allows the potential\noutcomes to be evaluated at any time point. In addition, adversarial training\nis used to adjust for time-dependent confounding which is critical in\nlongitudinal settings and is an added challenge not encountered in conventional\ntime-series. To assess solutions to this problem, we propose a controllable\nsimulation environment based on a model of tumor growth for a range of\nscenarios with irregular sampling reflective of a variety of clinical\nscenarios. TE-CDE consistently outperforms existing approaches in all simulated\nscenarios with irregular sampling.",
    "descriptor": "\nComments: Presented at the International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Nabeel Seedat",
      "Fergus Imrie",
      "Alexis Bellot",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08311"
  },
  {
    "id": "arXiv:2206.08312",
    "title": "SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning",
    "abstract": "We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio\nrendering for 3D environments. Given a 3D mesh of a real-world environment,\nSoundSpaces can generate highly realistic acoustics for arbitrary sounds\ncaptured from arbitrary microphone locations. Together with existing 3D visual\nassets, it supports an array of audio-visual research tasks, such as\naudio-visual navigation, mapping, source localization and separation, and\nacoustic matching. Compared to existing resources, SoundSpaces 2.0 has the\nadvantages of allowing continuous spatial sampling, generalization to novel\nenvironments, and configurable microphone and material properties. To our best\nknowledge, this is the first geometry-based acoustic simulation that offers\nhigh fidelity and realism while also being fast enough to use for embodied\nlearning. We showcase the simulator's properties and benchmark its performance\nagainst real-world audio measurements. In addition, through two downstream\ntasks covering embodied navigation and far-field automatic speech recognition,\nhighlighting sim2real performance for the latter. SoundSpaces 2.0 is publicly\navailable to facilitate wider research for perceptual systems that can both see\nand hear.",
    "descriptor": "\nComments: Website: this https URL\n",
    "authors": [
      "Changan Chen",
      "Carl Schissler",
      "Sanchit Garg",
      "Philip Kobernik",
      "Alexander Clegg",
      "Paul Calamia",
      "Dhruv Batra",
      "Philip W Robinson",
      "Kristen Grauman"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08312"
  },
  {
    "id": "arXiv:2206.08316",
    "title": "Boosting the Adversarial Transferability of Surrogate Model with Dark  Knowledge",
    "abstract": "Deep neural networks (DNNs) for image classification are known to be\nvulnerable to adversarial examples. And, the adversarial examples have\ntransferability, which means an adversarial example for a DNN model can fool\nanother black-box model with a non-trivial probability. This gave birth of the\ntransfer-based adversarial attack where the adversarial examples generated by a\npretrained or known model (called surrogate model) are used to conduct\nblack-box attack. There are some work on how to generate the adversarial\nexamples from a given surrogate model to achieve better transferability.\nHowever, training a special surrogate model to generate adversarial examples\nwith better transferability is relatively under-explored. In this paper, we\npropose a method of training a surrogate model with abundant dark knowledge to\nboost the adversarial transferability of the adversarial examples generated by\nthe surrogate model. This trained surrogate model is named dark surrogate model\n(DSM), and the proposed method to train DSM consists of two key components: a\nteacher model extracting dark knowledge and providing soft labels, and the\nmixing augmentation skill which enhances the dark knowledge of training data.\nExtensive experiments have been conducted to show that the proposed method can\nsubstantially improve the adversarial transferability of surrogate model across\ndifferent architectures of surrogate model and optimizers for generating\nadversarial examples. We also show that the proposed method can be applied to\nother scenarios of transfer-based attack that contain dark knowledge, like face\nverification.",
    "descriptor": "\nComments: 26 pages, 5 figures\n",
    "authors": [
      "Dingcheng Yang",
      "Zihao Xiao",
      "Wenjian Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08316"
  },
  {
    "id": "arXiv:2206.08317",
    "title": "Paraformer: Fast and Accurate Parallel Transformer for  Non-autoregressive End-to-End Speech Recognition",
    "abstract": "Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.",
    "descriptor": "\nComments: 5 pages, 3 figures, accepted by InterSpeech2022\n",
    "authors": [
      "Zhifu Gao",
      "Shiliang Zhang",
      "Ian McLoughlin",
      "Zhijie Yan"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.08317"
  },
  {
    "id": "arXiv:2206.08318",
    "title": "Planning and Formulations in Pursuit-Evasion: Keep-away Games and Their  Strategies",
    "abstract": "We study a pursuit-evasion problem which can be viewed as an extension of the\nkeep-away game. In the game, pursuer(s) will attempt to intersect or catch the\nevader, while the evader can visit a fixed set of locations, which we denote as\nthe anchors. These anchors may or may not be stationary. When the velocity of\nthe pursuers is limited and considered low compared to the evaders, we are\ninterested in whether a winning strategy exists for the pursuers or the\nevaders, or the game will draw. When the anchors are stationary, we show an\nalgorithm that can help answer the above question.\nThe primary motivation for this study is to explore the boundaries between\nkinematic and dynamic constraints. In particular, whether the solution of the\nkinematic problem can be used to speed up the search for the problems with\ndynamic constraints and how to discretize the problem to utilize such relations\nbest. In this work, we show that a geometric branch-and-bound type of approach\ncan be used to solve the stationary anchor problem, and the approach and the\nsolution can be extended to solve the dynamic problem where the pursuers have\ndynamic constraints, including velocity and acceleration bounds.",
    "descriptor": "",
    "authors": [
      "Weifu Wang",
      "Ping Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08318"
  },
  {
    "id": "arXiv:2206.08321",
    "title": "Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for  End-to-End Visual Robotic Manipulation Learning",
    "abstract": "End-to-end learning for visual robotic manipulation is known to suffer from\nsample inefficiency, requiring a large number of demonstrations. The spatial\nroto-translation equivariance, or the SE(3)-equivariance can be exploited to\nimprove the sample efficiency for learning robotic manipulation. In this paper,\nwe present fully end-to-end SE(3)-equivariant models for visual robotic\nmanipulation from a point cloud input. By utilizing the representation theory\nof the Lie group, we construct novel SE(3)-equivariant energy-based models that\nallow highly sample efficient end-to-end learning. We show that our models can\nlearn from scratch without prior knowledge yet is highly sample efficient (~10\ndemonstrations are enough). Furthermore, we show that the trained models can\ngeneralize to tasks with (i) previously unseen target object poses, (ii)\npreviously unseen target object instances of the category, and (iii) previously\nunseen visual distractors. We experiment with 6-DoF robotic manipulation tasks\nto validate our models' sample efficiency and generalizability. Codes are\navailable at: https://github.com/tomato1mule/edf",
    "descriptor": "",
    "authors": [
      "Hyunwoo Ryu",
      "Jeong-Hoon Lee",
      "Hong-in Lee",
      "Jongeun Choi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.08321"
  },
  {
    "id": "arXiv:2206.08324",
    "title": "Modeling, robust control synthesis and worst-case analysis for an  on-orbit servicing mission with large flexible spacecraft",
    "abstract": "This paper outlines a complete methodology for modeling an on-orbit servicing\nmission scenario and designing a feedback control system for the attitude\ndynamics that is guaranteed to robustly meet pointing requirements, despite\nmodel uncertainties as well as large inertia and flexibility changes throughout\nthe mission scenario. A model of the uncertain plant was derived, which fully\ncaptures the dynamics and couplings between all subsystems as well as the\ndecoupled/coupled configurations of the chaser/target system in a single linear\nfractional representation (LFR). In addition, a new approach is proposed to\nmodel and analyze a closed-loop kinematic chain formed by the chaser and the\ntarget spacecraft through the chaser's robotic arm, which uses two local\nspring-damper systems with uncertain damping and stiffness. This approach\noffers the possibility to model the dynamical behaviour of a docking mechanism\nwith dynamic stiffness and damping. The controller was designed by taking into\naccount all the interactions between subsystems and uncertainties as well as\nthe time-varying and coupled flexible dynamics. Lastly, the robust stability\nand worst-case performances were assessed by means of a structured singular\nvalue analysis.",
    "descriptor": "",
    "authors": [
      "Ricardo Rodrigues",
      "Valentin Preda",
      "Francesco Sanfedino",
      "Daniel Alazard"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.08324"
  },
  {
    "id": "arXiv:2206.08325",
    "title": "Characteristics of Harmful Text: Towards Rigorous Benchmarking of  Language Models",
    "abstract": "Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.",
    "descriptor": "\nComments: 9 pages plus appendix\n",
    "authors": [
      "Maribeth Rauh",
      "John Mellor",
      "Jonathan Uesato",
      "Po-Sen Huang",
      "Johannes Welbl",
      "Laura Weidinger",
      "Sumanth Dathathri",
      "Amelia Glaese",
      "Geoffrey Irving",
      "Iason Gabriel",
      "William Isaac",
      "Lisa Anne Hendricks"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.08325"
  },
  {
    "id": "arXiv:2206.08330",
    "title": "Compressed-VFL: Communication-Efficient Learning with Vertically  Partitioned Data",
    "abstract": "We propose Compressed Vertical Federated Learning (C-VFL) for\ncommunication-efficient training on vertically partitioned data. In C-VFL, a\nserver and multiple parties collaboratively train a model on their respective\nfeatures utilizing several local iterations and sharing compressed intermediate\nresults periodically. Our work provides the first theoretical analysis of the\neffect message compression has on distributed training over vertically\npartitioned data. We prove convergence of non-convex objectives at a rate of\n$O(\\frac{1}{\\sqrt{T}})$ when the compression error is bounded over the course\nof training. We provide specific requirements for convergence with common\ncompression techniques, such as quantization and top-$k$ sparsification.\nFinally, we experimentally show compression can reduce communication by over\n$90\\%$ without a significant decrease in accuracy over VFL without compression.",
    "descriptor": "",
    "authors": [
      "Timothy Castiglia",
      "Anirban Das",
      "Shiqiang Wang",
      "Stacy Patterson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08330"
  },
  {
    "id": "arXiv:2206.08332",
    "title": "BYOL-Explore: Exploration by Bootstrapped Prediction",
    "abstract": "We present BYOL-Explore, a conceptually simple yet general approach for\ncuriosity-driven exploration in visually-complex environments. BYOL-Explore\nlearns a world representation, the world dynamics, and an exploration policy\nall-together by optimizing a single prediction loss in the latent space with no\nadditional auxiliary objective. We show that BYOL-Explore is effective in\nDM-HARD-8, a challenging partially-observable continuous-action\nhard-exploration benchmark with visually-rich 3-D environments. On this\nbenchmark, we solve the majority of the tasks purely through augmenting the\nextrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could\nonly get off the ground with human demonstrations. As further evidence of the\ngenerality of BYOL-Explore, we show that it achieves superhuman performance on\nthe ten hardest exploration games in Atari while having a much simpler design\nthan other competitive agents.",
    "descriptor": "",
    "authors": [
      "Zhaohan Daniel Guo",
      "Shantanu Thakoor",
      "Miruna P\u00eeslar",
      "Bernardo Avila Pires",
      "Florent Altch\u00e9",
      "Corentin Tallec",
      "Alaa Saade",
      "Daniele Calandriello",
      "Jean-Bastien Grill",
      "Yunhao Tang",
      "Michal Valko",
      "R\u00e9mi Munos",
      "Mohammad Gheshlaghi Azar",
      "Bilal Piot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08332"
  },
  {
    "id": "arXiv:2206.08337",
    "title": "Planning through Workspace Constraint Satisfaction and Optimization",
    "abstract": "In this work, we present a workspace-based planning framework, which though\nusing redundant workspace key-points to represent robot states, can take\nadvantage of the interpretable geometric information to derive good quality\ncollision-free paths for even complex robots. Using workspace geometries, we\nfirst find collision-free piece-wise linear paths for each key point so that at\nthe endpoints of each segment, the distance constraints are satisfied among the\nkey points. Using these piece-wise linear paths as initial conditions, we can\nperform optimization steps to quickly find paths that satisfy various\nconstraints and piece together all segments to obtain a valid path. We show\nthat these adjusted paths are unlikely to create a collision, and the proposed\napproach is fast and can produce good quality results.",
    "descriptor": "",
    "authors": [
      "Weifu Wang",
      "Ping Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08337"
  },
  {
    "id": "arXiv:2206.08339",
    "title": "iBoot: Image-bootstrapped Self-Supervised Video Representation Learning",
    "abstract": "Learning visual representations through self-supervision is an extremely\nchallenging task as the network needs to sieve relevant patterns from spurious\ndistractors without the active guidance provided by supervision. This is\nachieved through heavy data augmentation, large-scale datasets and prohibitive\namounts of compute. Video self-supervised learning (SSL) suffers from added\nchallenges: video datasets are typically not as large as image datasets,\ncompute is an order of magnitude larger, and the amount of spurious patterns\nthe optimizer has to sieve through is multiplied several fold. Thus, directly\nlearning self-supervised representations from video data might result in\nsub-optimal performance. To address this, we propose to utilize a strong\nimage-based model, pre-trained with self- or language supervision, in a video\nrepresentation learning framework, enabling the model to learn strong spatial\nand temporal information without relying on the video labeled data. To this\nend, we modify the typical video-based SSL design and objective to encourage\nthe video encoder to \\textit{subsume} the semantic content of an image-based\nmodel trained on a general domain. The proposed algorithm is shown to learn\nmuch more efficiently (i.e. in less epochs and with a smaller batch) and\nresults in a new state-of-the-art performance on standard downstream tasks\namong single-modality SSL methods.",
    "descriptor": "",
    "authors": [
      "Fatemeh Saleh",
      "Fuwen Tan",
      "Adrian Bulat",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08339"
  },
  {
    "id": "arXiv:2206.08343",
    "title": "Realistic One-shot Mesh-based Head Avatars",
    "abstract": "We present a system for realistic one-shot mesh-based human head avatars\ncreation, ROME for short. Using a single photograph, our model estimates a\nperson-specific head mesh and the associated neural texture, which encodes both\nlocal photometric and geometric details. The resulting avatars are rigged and\ncan be rendered using a neural network, which is trained alongside the mesh and\ntexture estimators on a dataset of in-the-wild videos. In the experiments, we\nobserve that our system performs competitively both in terms of head geometry\nrecovery and the quality of renders, especially for the cross-person\nreenactment. See results https://samsunglabs.github.io/rome/",
    "descriptor": "",
    "authors": [
      "Taras Khakhulin",
      "Vanessa Sklyarova",
      "Victor Lempitsky",
      "Egor Zakharov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2206.08343"
  },
  {
    "id": "arXiv:2206.08345",
    "title": "Real-World Single Image Super-Resolution Under Rainy Condition",
    "abstract": "Image super-resolution is an important research area in computer vision that\nhas a wide variety of applications including surveillance, medical imaging etc.\nReal-world signal image super-resolution has become very popular now-a-days due\nto its real-time application. There are still a lot of scopes to improve\nreal-world single image super-resolution specially during challenging weather\nscenarios. In this paper, we have proposed a new algorithm to perform\nreal-world single image super-resolution during rainy condition. Our proposed\nmethod can mitigate the influence of rainy conditions during image\nsuper-resolution. Our experiment results show that our proposed algorithm can\nperform image super-resolution decreasing the negative effects of the rain.",
    "descriptor": "",
    "authors": [
      "Mohammad Shahab Uddin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.08345"
  },
  {
    "id": "arXiv:2206.08347",
    "title": "Beyond Supervised vs. Unsupervised: Representative Benchmarking and  Analysis of Image Representation Learning",
    "abstract": "By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.",
    "descriptor": "\nComments: CVPR 2022, project page: this https URL\n",
    "authors": [
      "Matthew Gwilliam",
      "Abhinav Shrivastava"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08347"
  },
  {
    "id": "arXiv:2206.08349",
    "title": "Know your audience: specializing grounded language models with the game  of Dixit",
    "abstract": "Effective communication requires adapting to the idiosyncratic common ground\nshared with each communicative partner. We study a particularly challenging\ninstantiation of this problem: the popular game Dixit. We formulate a round of\nDixit as a multi-agent image reference game where a (trained) speaker model is\nrewarded for describing a target image such that one (pretrained) listener\nmodel can correctly identify it from a pool of distractors, but another\nlistener cannot. To adapt to this setting, the speaker must exploit differences\nin the common ground it shares with the different listeners. We show that\nfinetuning an attention-based adapter between a CLIP vision encoder and a large\nlanguage model in this contrastive, multi-agent setting gives rise to\ncontext-dependent natural language specialization from rewards only, without\ndirect supervision. In a series of controlled experiments, we show that the\nspeaker can adapt according to the idiosyncratic strengths and weaknesses of\nvarious pairs of different listeners. Furthermore, we show zero-shot transfer\nof the speaker's specialization to unseen real-world data. Our experiments\noffer a step towards adaptive communication in complex multi-partner settings\nand highlight the interesting research challenges posed by games like Dixit. We\nhope that our work will inspire creative new approaches to adapting pretrained\nmodels.",
    "descriptor": "\nComments: 27 pages, 9 figures\n",
    "authors": [
      "Aaditya K. Singh",
      "David Ding",
      "Andrew Saxe",
      "Felix Hill",
      "Andrew K. Lampinen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.08349"
  },
  {
    "id": "arXiv:2206.08353",
    "title": "Towards Understanding How Machines Can Learn Causal Overhypotheses",
    "abstract": "Recent work in machine learning and cognitive science has suggested that\nunderstanding causal information is essential to the development of\nintelligence. The extensive literature in cognitive science using the ``blicket\ndetector'' environment shows that children are adept at many kinds of causal\ninference and learning. We propose to adapt that environment for machine\nlearning agents. One of the key challenges for current machine learning\nalgorithms is modeling and understanding causal overhypotheses: transferable\nabstract hypotheses about sets of causal relationships. In contrast, even young\nchildren spontaneously learn and use causal overhypotheses. In this work, we\npresent a new benchmark -- a flexible environment which allows for the\nevaluation of existing techniques under variable causal overhypotheses -- and\ndemonstrate that many existing state-of-the-art methods have trouble\ngeneralizing in this environment. The code and resources for this benchmark are\navailable at https://github.com/CannyLab/casual_overhypotheses.",
    "descriptor": "",
    "authors": [
      "Eliza Kosoy",
      "David M. Chan",
      "Adrian Liu",
      "Jasmine Collins",
      "Bryanna Kaufmann",
      "Sandy Han Huang",
      "Jessica B. Hamrick",
      "John Canny",
      "Nan Rosemary Ke",
      "Alison Gopnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08353"
  },
  {
    "id": "arXiv:2206.08355",
    "title": "FWD: Real-time Novel View Synthesis with Forward Warping and Depth",
    "abstract": "Novel view synthesis (NVS) is a challenging task requiring systems to\ngenerate photorealistic images of scenes from new viewpoints, where both\nquality and speed are important for applications. Previous image-based\nrendering (IBR) methods are fast, but have poor quality when input views are\nsparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give\nimpressive results but are not real-time. In our paper, we propose a\ngeneralizable NVS method with sparse inputs, called FWD, which gives\nhigh-quality synthesis in real-time. With explicit depth and differentiable\nrendering, it achieves competitive results to the SOTA methods with 130-1000x\nspeedup and better perceptual quality. If available, we can seamlessly\nintegrate sensor depth during either training or inference to improve image\nquality while retaining real-time speed. With the growing prevalence of depths\nsensors, we hope that methods making use of depth will become increasingly\nuseful.",
    "descriptor": "\nComments: Project website this https URL 2022 with some modifications on MVSNeRF results\n",
    "authors": [
      "Ang Cao",
      "Chris Rockwell",
      "Justin Johnson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08355"
  },
  {
    "id": "arXiv:2206.08356",
    "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos",
    "abstract": "Transformer-based architectures have become competitive across a variety of\nvisual domains, most notably images and videos. While prior work has studied\nthese modalities in isolation, having a common architecture suggests that one\ncan train a single unified model for multiple visual modalities. Prior attempts\nat unified modeling typically use architectures tailored for vision tasks, or\nobtain worse performance compared to single modality models. In this work, we\nshow that masked autoencoding can be used to train a simple Vision Transformer\non images and videos, without requiring any labeled data. This single model\nlearns visual representations that are comparable to or better than\nsingle-modality representations on both image and video benchmarks, while using\na much simpler architecture. In particular, our single pretrained model can be\nfinetuned to achieve 86.5% on ImageNet and 75.3% on the challenging Something\nSomething-v2 video benchmark. Furthermore, this model can be learned by\ndropping 90% of the image and 95% of the video patches, enabling extremely fast\ntraining.",
    "descriptor": "",
    "authors": [
      "Rohit Girdhar",
      "Alaaeldin El-Nouby",
      "Mannat Singh",
      "Kalyan Vasudev Alwala",
      "Armand Joulin",
      "Ishan Misra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08356"
  },
  {
    "id": "arXiv:2206.08357",
    "title": "Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing",
    "abstract": "Existing GAN inversion and editing methods work well for aligned objects with\na clean background, such as portraits and animal faces, but often struggle for\nmore difficult categories with complex scene layouts and object occlusions,\nsuch as cars, animals, and outdoor images. We propose a new method to invert\nand edit such complex images in the latent space of GANs, such as StyleGAN2.\nOur key idea is to explore inversion with a collection of layers, spatially\nadapting the inversion process to the difficulty of the image. We learn to\npredict the \"invertibility\" of different image segments and project each\nsegment into a latent layer. Easier regions can be inverted into an earlier\nlayer in the generator's latent space, while more challenging regions can be\ninverted into a later feature space. Experiments show that our method obtains\nbetter inversion results compared to the recent approaches on complex\ncategories, while maintaining downstream editability. Please refer to our\nproject page at https://www.cs.cmu.edu/~SAMInversion.",
    "descriptor": "\nComments: CVPR 2022. Github: this https URL Website: this https URL\n",
    "authors": [
      "Gaurav Parmar",
      "Yijun Li",
      "Jingwan Lu",
      "Richard Zhang",
      "Jun-Yan Zhu",
      "Krishna Kumar Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08357"
  },
  {
    "id": "arXiv:2206.08358",
    "title": "MixGen: A New Multi-Modal Data Augmentation",
    "abstract": "Data augmentation is a necessity to enhance data efficiency in deep learning.\nFor vision-language pre-training, data is only augmented either for images or\nfor text in previous works. In this paper, we present MixGen: a joint data\naugmentation for vision-language representation learning to further improve\ndata efficiency. It generates new image-text pairs with semantic relationships\npreserved by interpolating images and concatenating text. It's simple, and can\nbe plug-and-played into existing pipelines. We evaluate MixGen on four\narchitectures, including CLIP, ViLT, ALBEF and TCL, across five downstream\nvision-language tasks to show its versatility and effectiveness. For example,\nadding MixGen in ALBEF pre-training leads to absolute performance improvements\non downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%\non Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual\nreasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),\nand visual entailment (+0.4% on SNLI-VE).",
    "descriptor": "",
    "authors": [
      "Xiaoshuai Hao",
      "Yi Zhu",
      "Srikar Appalaraju",
      "Aston Zhang",
      "Wanqian Zhang",
      "Bo Li",
      "Mu Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08358"
  },
  {
    "id": "arXiv:2206.08361",
    "title": "Controllable 3D Face Synthesis with Conditional Generative Occupancy  Fields",
    "abstract": "Capitalizing on the recent advances in image generation models, existing\ncontrollable face image synthesis methods are able to generate high-fidelity\nimages with some levels of controllability, e.g., controlling the shapes,\nexpressions, textures, and poses of the generated face images. However, these\nmethods focus on 2D image generative models, which are prone to producing\ninconsistent face images under large expression and pose changes. In this\npaper, we propose a new NeRF-based conditional 3D face synthesis framework,\nwhich enables 3D controllability over the generated face images by imposing\nexplicit 3D conditions from 3D face priors. At its core is a conditional\nGenerative Occupancy Field (cGOF) that effectively enforces the shape of the\ngenerated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve\naccurate control over fine-grained 3D face shapes of the synthesized image, we\nadditionally incorporate a 3D landmark loss as well as a volume warping loss\ninto our synthesis algorithm. Experiments validate the effectiveness of the\nproposed method, which is able to generate high-fidelity face images and shows\nmore precise 3D controllability than state-of-the-art 2D-based controllable\nface synthesis methods. Find code and demo at\nhttps://keqiangsun.github.io/projects/cgof.",
    "descriptor": "",
    "authors": [
      "Keqiang Sun",
      "Shangzhe Wu",
      "Zhaoyang Huang",
      "Ning Zhang",
      "Quan Wang",
      "HongSheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08361"
  },
  {
    "id": "arXiv:2206.08362",
    "title": "Unified Fourier-based Kernel and Nonlinearity Design for Equivariant  Networks on Homogeneous Spaces",
    "abstract": "We introduce a unified framework for group equivariant networks on\nhomogeneous spaces derived from a Fourier perspective. We address the case of\nfeature fields being tensor valued before and after a convolutional layer. We\npresent a unified derivation of kernels via the Fourier domain by taking\nadvantage of the sparsity of Fourier coefficients of the lifted feature fields.\nThe sparsity emerges when the stabilizer subgroup of the homogeneous space is a\ncompact Lie group. We further introduce an activation method via an elementwise\nnonlinearity on the regular representation after lifting and projecting back to\nthe field through an equivariant convolution. We show that other methods\ntreating features as the Fourier coefficients in the stabilizer subgroup are\nspecial cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show\nstate-of-the-art performance in spherical vector field regression, point cloud\nclassification, and molecular completion.",
    "descriptor": "\nComments: Accepted at ICML2022 Thirty-ninth International Conference on Machine Learning\n",
    "authors": [
      "Yinshuang Xu",
      "Jiahui Lei",
      "Edgar Dobriban",
      "Kostas Daniilidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08362"
  },
  {
    "id": "arXiv:2206.08363",
    "title": "Benchmarking Heterogeneous Treatment Effect Models through the Lens of  Interpretability",
    "abstract": "Estimating personalized effects of treatments is a complex, yet pervasive\nproblem. To tackle it, recent developments in the machine learning (ML)\nliterature on heterogeneous treatment effect estimation gave rise to many\nsophisticated, but opaque, tools: due to their flexibility, modularity and\nability to learn constrained representations, neural networks in particular\nhave become central to this literature. Unfortunately, the assets of such black\nboxes come at a cost: models typically involve countless nontrivial operations,\nmaking it difficult to understand what they have learned. Yet, understanding\nthese models can be crucial -- in a medical context, for example, discovered\nknowledge on treatment effect heterogeneity could inform treatment prescription\nin clinical practice. In this work, we therefore use post-hoc feature\nimportance methods to identify features that influence the model's predictions.\nThis allows us to evaluate treatment effect estimators along a new and\nimportant dimension that has been overlooked in previous work: We construct a\nbenchmarking environment to empirically investigate the ability of personalized\ntreatment effect models to identify predictive covariates -- covariates that\ndetermine differential responses to treatment. Our benchmarking environment\nthen enables us to provide new insight into the strengths and weaknesses of\ndifferent types of treatment effects models as we modulate different challenges\nspecific to treatment effect estimation -- e.g. the ratio of prognostic to\npredictive information, the possible nonlinearity of potential outcomes and the\npresence and type of confounding.",
    "descriptor": "",
    "authors": [
      "Jonathan Crabb\u00e9",
      "Alicia Curth",
      "Ioana Bica",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08363"
  },
  {
    "id": "arXiv:2206.08364",
    "title": "Interaction-Grounded Learning with Action-inclusive Feedback",
    "abstract": "Consider the problem setting of Interaction-Grounded Learning (IGL), in which\na learner's goal is to optimally interact with the environment with no explicit\nreward to ground its policies. The agent observes a context vector, takes an\naction, and receives a feedback vector, using this information to effectively\noptimize a policy with respect to a latent reward function. Prior analyzed\napproaches fail when the feedback vector contains the action, which\nsignificantly limits IGL's success in many potential scenarios such as\nBrain-computer interface (BCI) or Human-computer interface (HCI) applications.\nWe address this by creating an algorithm and analysis which allows IGL to work\neven when the feedback vector contains the action, encoded in any fashion. We\nprovide theoretical guarantees and large-scale experiments based on supervised\ndatasets to demonstrate the effectiveness of the new approach.",
    "descriptor": "",
    "authors": [
      "Tengyang Xie",
      "Akanksha Saran",
      "Dylan J. Foster",
      "Lekan Molu",
      "Ida Momennejad",
      "Nan Jiang",
      "Paul Mineiro",
      "John Langford"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08364"
  },
  {
    "id": "arXiv:2206.08365",
    "title": "Virtual Correspondence: Humans as a Cue for Extreme-View Geometry",
    "abstract": "Recovering the spatial layout of the cameras and the geometry of the scene\nfrom extreme-view images is a longstanding challenge in computer vision.\nPrevailing 3D reconstruction algorithms often adopt the image matching paradigm\nand presume that a portion of the scene is co-visible across images, yielding\npoor performance when there is little overlap among inputs. In contrast, humans\ncan associate visible parts in one image to the corresponding invisible\ncomponents in another image via prior knowledge of the shapes. Inspired by this\nfact, we present a novel concept called virtual correspondences (VCs). VCs are\na pair of pixels from two images whose camera rays intersect in 3D. Similar to\nclassic correspondences, VCs conform with epipolar geometry; unlike classic\ncorrespondences, VCs do not need to be co-visible across views. Therefore VCs\ncan be established and exploited even if images do not overlap. We introduce a\nmethod to find virtual correspondences based on humans in the scene. We\nshowcase how VCs can be seamlessly integrated with classic bundle adjustment to\nrecover camera poses across extreme views. Experiments show that our method\nsignificantly outperforms state-of-the-art camera pose estimation methods in\nchallenging scenarios and is comparable in the traditional densely captured\nsetup. Our approach also unleashes the potential of multiple downstream tasks\nsuch as scene reconstruction from multi-view stereo and novel view synthesis in\nextreme-view scenarios.",
    "descriptor": "\nComments: CVPR 2022. Project page: this https URL\n",
    "authors": [
      "Wei-Chiu Ma",
      "Anqi Joyce Yang",
      "Shenlong Wang",
      "Raquel Urtasun",
      "Antonio Torralba"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.08365"
  },
  {
    "id": "arXiv:2206.08366",
    "title": "Scalable First-Order Bayesian Optimization via Structured Automatic  Differentiation",
    "abstract": "Bayesian Optimization (BO) has shown great promise for the global\noptimization of functions that are expensive to evaluate, but despite many\nsuccesses, standard approaches can struggle in high dimensions. To improve the\nperformance of BO, prior work suggested incorporating gradient information into\na Gaussian process surrogate of the objective, giving rise to kernel matrices\nof size $nd \\times nd$ for $n$ observations in $d$ dimensions. Na\\\"ively\nmultiplying with (resp. inverting) these matrices requires\n$\\mathcal{O}(n^2d^2)$ (resp. $\\mathcal{O}(n^3d^3$)) operations, which becomes\ninfeasible for moderate dimensions and sample sizes. Here, we observe that a\nwide range of kernels gives rise to structured matrices, enabling an exact\n$\\mathcal{O}(n^2d)$ matrix-vector multiply for gradient observations and\n$\\mathcal{O}(n^2d^2)$ for Hessian observations. Beyond canonical kernel\nclasses, we derive a programmatic approach to leveraging this type of structure\nfor transformations and combinations of the discussed kernel classes, which\nconstitutes a structure-aware automatic differentiation algorithm. Our methods\napply to virtually all canonical kernels and automatically extend to complex\nkernels, like the neural network, radial basis function network, and spectral\nmixture kernels without any additional derivations, enabling flexible,\nproblem-dependent modeling while scaling first-order BO to high $d$.",
    "descriptor": "",
    "authors": [
      "Sebastian Ament",
      "Carla Gomes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Mathematical Software (cs.MS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08366"
  },
  {
    "id": "arXiv:2206.08367",
    "title": "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain  Adaptation",
    "abstract": "Adapting to a continuously evolving environment is a safety-critical\nchallenge inevitably faced by all autonomous driving systems. Existing image\nand video driving datasets, however, fall short of capturing the mutable nature\nof the real world. In this paper, we introduce the largest multi-task synthetic\ndataset for autonomous driving, SHIFT. It presents discrete and continuous\nshifts in cloudiness, rain and fog intensity, time of day, and vehicle and\npedestrian density. Featuring a comprehensive sensor suite and annotations for\nseveral mainstream perception tasks, SHIFT allows investigating the degradation\nof a perception system performance at increasing levels of domain shift,\nfostering the development of continuous adaptation strategies to mitigate this\nproblem and assess model robustness and generality. Our dataset and benchmark\ntoolkit are publicly available at www.vis.xyz/shift.",
    "descriptor": "\nComments: Published at IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2022\n",
    "authors": [
      "Tao Sun",
      "Mattia Segu",
      "Janis Postels",
      "Yuxuan Wang",
      "Luc Van Gool",
      "Bernt Schiele",
      "Federico Tombari",
      "Fisher Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08367"
  },
  {
    "id": "arXiv:2206.08368",
    "title": "Unbiased 4D: Monocular 4D Reconstruction with a Neural Deformation Model",
    "abstract": "Capturing general deforming scenes is crucial for many computer graphics and\nvision applications, and it is especially challenging when only a monocular RGB\nvideo of the scene is available. Competing methods assume dense point tracks,\n3D templates, large-scale training datasets, or only capture small-scale\ndeformations. In contrast to those, our method, Ub4D, makes none of these\nassumptions while outperforming the previous state of the art in challenging\nscenarios. Our technique includes two new, in the context of non-rigid 3D\nreconstruction, components, i.e., 1) A coordinate-based and implicit neural\nrepresentation for non-rigid scenes, which enables an unbiased reconstruction\nof dynamic scenes, and 2) A novel dynamic scene flow loss, which enables the\nreconstruction of larger deformations. Results on our new dataset, which will\nbe made publicly available, demonstrate the clear improvement over the state of\nthe art in terms of surface reconstruction accuracy and robustness to large\ndeformations. Visit the project page https://4dqv.mpi-inf.mpg.de/Ub4D/.",
    "descriptor": "\nComments: 26 pages, 17 figures, 8 tables\n",
    "authors": [
      "Erik C.M. Johnson",
      "Marc Habermann",
      "Soshi Shimada",
      "Vladislav Golyanik",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08368"
  },
  {
    "id": "arXiv:2206.07732",
    "title": "A model predictive control (MPC)-integrated multiphase immersed boundary  (IB) framework for simulating wave energy converters (WECs)",
    "abstract": "In this work, we present a novel MPC-integrated multiphase IB framework that\ncan compute the optimal energy-maximizing control force on-the-fly by\ndynamically interacting with a high-fidelity numerical wave tank (NWT). Due to\nthe requirement of solving a constrained optimization problem at each time step\nof the IB simulation, the MPC algorithm utilizes a low-dimensional dynamical\nmodel of the device that is based on the linear potential theory (LPT). The\nmultiphase IB solver, on the other hand, is based on the high-dimensional\nfictitious domain Brinkman penalization (FD/BP) method, which fully-resolves\nthe hydrodynamic nonlinearities associated with the wave-structure interaction\n(WSI). A time-series forecasting auto-regressive model is implemented that\npredicts wave heights to estimate the future wave excitation/Froude- Krylov\nforces for the MPC algorithm. Moreover, we also experiment with non-linear\nFroude-Krylov (NLFK) forces for the first time in an MPC formulation. Under\nvarying sea conditions, the predictions of the MPC-integrated multiphase IB\nsolver are compared to the widely popular LPT-based solvers. Overall, six\nWSI/MPC solver combinations are compared for a heaving vertical cylinder. We\nalso determine the pathway of energy transfer from the waves to the power\ntake-off (PTO) system and verify the relationships using IB simulations.\nAdditionally, three different sea states are simulated within the IB simulation\nto test the adaptive capability of MPC for WECs. MPC is demonstrated to adapt\nto changing sea conditions and find the optimal solution for each sea state.",
    "descriptor": "",
    "authors": [
      "Kaustubh Khedkar",
      "Amneet Pal Singh Bhalla"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07732"
  },
  {
    "id": "arXiv:2206.07760",
    "title": "Multiscale methods for signal selection in single-cell data",
    "abstract": "Analysis of single-cell transcriptomics often relies on clustering cells and\nthen performing differential gene expression (DGE) to identify genes that vary\nbetween these clusters. These discrete analyses successfully determine cell\ntypes and markers; however, continuous variation within and between cell types\nmay not be detected. We propose three topologically-motivated mathematical\nmethods for unsupervised feature selection that consider discrete and\ncontinuous transcriptional patterns on an equal footing across multiple scales\nsimultaneously. Eigenscores ($\\mathrm{eig}_i$) rank signals or genes based on\ntheir correspondence to low-frequency intrinsic patterning in the data using\nthe spectral decomposition of the graph Laplacian. The multiscale Laplacian\nscore (MLS) is an unsupervised method for locating relevant scales in data and\nselecting the genes that are coherently expressed at these respective scales.\nThe persistent Rayleigh quotient (PRQ) takes data equipped with a filtration,\nallowing separation of genes with different roles in a bifurcation process\n(e.g. pseudo-time). We demonstrate the utility of these techniques by applying\nthem to published single-cell transcriptomics data sets. The methods validate\npreviously identified genes and detect additional genes with coherent\nexpression patterns. By studying the interaction between gene signals and the\ngeometry of the underlying space, the three methods give multidimensional\nrankings of the genes and visualisation of relationships between them.",
    "descriptor": "\nComments: 26 pages, 13 figures, 2 tables\n",
    "authors": [
      "Renee S. Hoekzema",
      "Lewis Marsh",
      "Otto Sumray",
      "Xin Lu",
      "Helen M. Byrne",
      "Heather A. Harrington"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Social and Information Networks (cs.SI)",
      "Algebraic Topology (math.AT)",
      "Spectral Theory (math.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07760"
  },
  {
    "id": "arXiv:2206.07769",
    "title": "HyperImpute: Generalized Iterative Imputation with Automatic Model  Selection",
    "abstract": "Consider the problem of imputing missing values in a dataset. One the one\nhand, conventional approaches using iterative imputation benefit from the\nsimplicity and customizability of learning conditional distributions directly,\nbut suffer from the practical requirement for appropriate model specification\nof each and every variable. On the other hand, recent methods using deep\ngenerative modeling benefit from the capacity and efficiency of learning with\nneural network function approximators, but are often difficult to optimize and\nrely on stronger data assumptions. In this work, we study an approach that\nmarries the advantages of both: We propose *HyperImpute*, a generalized\niterative imputation framework for adaptively and automatically configuring\ncolumn-wise models and their hyperparameters. Practically, we provide a\nconcrete implementation with out-of-the-box learners, optimizers, simulators,\nand extensible interfaces. Empirically, we investigate this framework via\ncomprehensive experiments and sensitivities on a variety of public datasets,\nand demonstrate its ability to generate accurate imputations relative to a\nstrong suite of benchmarks. Contrary to recent work, we believe our findings\nconstitute a strong defense of the iterative imputation paradigm.",
    "descriptor": "",
    "authors": [
      "Daniel Jarrett",
      "Bogdan Cebere",
      "Tennison Liu",
      "Alicia Curth",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07769"
  },
  {
    "id": "arXiv:2206.07778",
    "title": "Frequency Response and Eddy Current Power Loss in Magneto-Mechanical  Transmitters",
    "abstract": "Magneto-mechanical transmitters offer a compact and low-power solution for\nthe generation of ultra-low frequency (ULF) magnetic signals for through-ground\nand through-seawater communications. Resonant arrays of smaller\nmagneto-mechanical transmitters are particularly interesting in this context as\nthe physical scaling laws allow for the increase of operating frequency and\nreduce the power requirements for ULF signal generation. In this work, we\nintroduce a generalized model for accurate prediction of frequency and mode\nshape in generalized magneto-mechanical resonator arrays (MMRAs) that accounts\nfor near-field magnetic interactions as well as magnetically induced\nnonlinearity. Using experiments, we demonstrate that our predictive capability\nis significantly improved compared against simplified dipole approximations. We\nadditionally model the eddy current losses internal to the array and find that\nthey are in agreement with experimental observations.",
    "descriptor": "\nComments: 12 pages, 8 figures, 6 tables\n",
    "authors": [
      "Jiheng Jing",
      "Sameh Tawfick",
      "Gaurav Bahl"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.07778"
  },
  {
    "id": "arXiv:2206.07780",
    "title": "A machine learning approach to predicting pore pressure response in  liquefiable sands under cyclic loading",
    "abstract": "Shear stress history controls the pore pressure response in liquefiable\nsoils. The excess pore pressure does not increase under cyclic loading when\nshear stress amplitude is lower than the peak prior amplitude -- the shielding\neffect. Many sophisticated constitutive models fail to capture the shielding\neffect observed in the cyclic liquefaction experiments. We develop a\ndata-driven machine learning model based on the LSTM neural network to capture\nthe liquefaction response of soils under cyclic loading. The LSTM model is\ntrained on 12 laboratory cyclic simple shear tests on Nevada sand in loose and\ndense conditions subjected to different cyclic simple shear loading conditions.\nThe LSTM model features include the relative density of soil and the previous\nstress history to predict the pore water pressure response. The LSTM model\nsuccessfully replicates the pore pressure response for three cyclic simple test\nresults considering the shielding and density effects.",
    "descriptor": "",
    "authors": [
      "Yongjin Choi",
      "Krishna Kumar"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07780"
  },
  {
    "id": "arXiv:2206.07786",
    "title": "Federated Data Analytics: A Study on Linear Models",
    "abstract": "As edge devices become increasingly powerful, data analytics are gradually\nmoving from a centralized to a decentralized regime where edge compute\nresources are exploited to process more of the data locally. This regime of\nanalytics is coined as federated data analytics (FDA). In spite of the recent\nsuccess stories of FDA, most literature focuses exclusively on deep neural\nnetworks. In this work, we take a step back to develop an FDA treatment for one\nof the most fundamental statistical models: linear regression. Our treatment is\nbuilt upon hierarchical modeling that allows borrowing strength across multiple\ngroups. To this end, we propose two federated hierarchical model structures\nthat provide a shared representation across devices to facilitate information\nsharing. Notably, our proposed frameworks are capable of providing uncertainty\nquantification, variable selection, hypothesis testing and fast adaptation to\nnew unseen data. We validate our methods on a range of real-life applications\nincluding condition monitoring for aircraft engines. The results show that our\nFDA treatment for linear models can serve as a competing benchmark model for\nfuture development of federated algorithms.",
    "descriptor": "",
    "authors": [
      "Xubo Yue",
      "Raed Al Kontar",
      "Ana Mar\u00eda Estrada G\u00f3mez"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07786"
  },
  {
    "id": "arXiv:2206.07788",
    "title": "Experimental Evaluation of Multi-operator RIS-assisted Links in Indoor  Environment",
    "abstract": "In this work, we present reconfigurable intelligent surface (RIS)-assisted\noptimization of the multiple links in the same indoor environment. Multiple\nRISs from different operators can co-exists and handle independent robust\ncommunication links in the same indoor environment. We investigated the key\nperformance metrics with the help of two simultaneously operating RIS-empowered\nrobust communication links at different center frequencies in the same indoor\nenvironment. We found with the help of bit error rate (BER) and error vector\nmagnitude (EVM) measurements that two operators can co-exist in the same RF\nenvironment without seriously impacting quality of service of users.",
    "descriptor": "\nComments: 4 pages, 8 figures\n",
    "authors": [
      "Mir Lodro",
      "Jean-Baptiste Gros",
      "Steve Greedy",
      "Geoffroy Lerosey",
      "Anas Al Rawi",
      "Gabriele Gradoni"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.07788"
  },
  {
    "id": "arXiv:2206.07824",
    "title": "Large-Scale Differentiable Causal Discovery of Factor Graphs",
    "abstract": "A common theme in causal inference is learning causal relationships between\nobserved variables, also known as causal discovery. This is usually a daunting\ntask, given the large number of candidate causal graphs and the combinatorial\nnature of the search space. Perhaps for this reason, most research has so far\nfocused on relatively small causal graphs, with up to hundreds of nodes.\nHowever, recent advances in fields like biology enable generating experimental\ndata sets with thousands of interventions followed by rich profiling of\nthousands of variables, raising the opportunity and urgent need for large\ncausal graph models. Here, we introduce the notion of factor directed acyclic\ngraphs (f-DAGs) as a way to restrict the search space to non-linear low-rank\ncausal interaction models. Combining this novel structural assumption with\nrecent advances that bridge the gap between causal discovery and continuous\noptimization, we achieve causal discovery on thousands of variables.\nAdditionally, as a model for the impact of statistical noise on this estimation\nprocedure, we study a model of edge perturbations of the f-DAG skeleton based\non random graphs and quantify the effect of such perturbations on the f-DAG\nrank. This theoretical analysis suggests that the set of candidate f-DAGs is\nmuch smaller than the whole DAG space and thus more statistically robust in the\nhigh-dimensional regime where the underlying skeleton is hard to assess. We\npropose Differentiable Causal Discovery of Factor Graphs (DCD-FG), a scalable\nimplementation of f-DAG constrained causal discovery for high-dimensional\ninterventional data. DCD-FG uses a Gaussian non-linear low-rank structural\nequation model and shows significant improvements compared to state-of-the-art\nmethods in both simulations as well as a recent large-scale single-cell RNA\nsequencing data set with hundreds of genetic interventions.",
    "descriptor": "\nComments: 29 pages, 8 figures\n",
    "authors": [
      "Romain Lopez",
      "Jan-Christian H\u00fctter",
      "Jonathan K. Pritchard",
      "Aviv Regev"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Genomics (q-bio.GN)"
    ],
    "url": "https://arxiv.org/abs/2206.07824"
  },
  {
    "id": "arXiv:2206.07849",
    "title": "Spiraling and Folding: The Topological View",
    "abstract": "For every $n$, we construct two curves in the plane that intersect at least\n$n$ times and do not form spirals. The construction is in three stages: we\nfirst exhibit closed curves on the torus that do not form double spirals, then\narcs on the torus that do not form spirals, and finally pairs of planar arcs\nthat do not form spirals. These curves provide a counterexample to a proof of\nPach and T\\'{o}th concerning string graphs.",
    "descriptor": "\nComments: 23 pages, 18 figures\n",
    "authors": [
      "Jan Kyn\u010dl",
      "Marcus Schaefer",
      "Eric Sedgwick",
      "Daniel \u0160tefankovi\u010d"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.07849"
  },
  {
    "id": "arXiv:2206.07851",
    "title": "Conformal prediction set for time-series",
    "abstract": "When building either prediction intervals for regression (with real-valued\nresponse) or prediction sets for classification (with categorical responses),\nuncertainty quantification is essential to studying complex machine learning\nmethods. In this paper, we develop Ensemble Regularized Adaptive Prediction Set\n(ERAPS) to construct prediction sets for time-series (with categorical\nresponses), based on the prior work of [Xu and Xie, 2021]. In particular, we\nallow unknown dependencies to exist within features and responses that arrive\nin sequence. Method-wise, ERAPS is a distribution-free and ensemble-based\nframework that is applicable for arbitrary classifiers. Theoretically, we bound\nthe coverage gap without assuming data exchangeability and show asymptotic set\nconvergence. Empirically, we demonstrate valid marginal and conditional\ncoverage by ERAPS, which also tends to yield smaller prediction sets than\ncompeting methods.",
    "descriptor": "\nComments: Strongly accepted by the Workshop on Distribution-Free Uncertainty Quantification at ICML 2022\n",
    "authors": [
      "Chen Xu",
      "Yao Xie"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.07851"
  },
  {
    "id": "arXiv:2206.07852",
    "title": "Performance analysis of coreset selection for quantum implementation of  K-Means clustering algorithm",
    "abstract": "Quantum computing is anticipated to offer immense computational capabilities\nwhich could provide efficient solutions to many data science problems. However,\nthe current generation of quantum devices are small and noisy, which makes it\ndifficult to process large data sets relevant for practical problems. Coreset\nselection aims to circumvent this problem by reducing the size of input data\nwithout compromising the accuracy. Recent work has shown that coreset selection\ncan help to implement quantum K-Means clustering problem. However, the impact\nof coreset selection on the performance of quantum K-Means clustering has not\nbeen explored. In this work, we compare the relative performance of two coreset\ntechniques (BFL16 and ONESHOT), and the size of coreset construction in each\ncase, with respect to a variety of data sets and layout the advantages and\nlimitations of coreset selection in implementing quantum algorithms. We also\ninvestigated the effect of depolarisation quantum noise and bit-flip error, and\nimplemented the Quantum AutoEncoder technique for surpassing the noise effect.\nOur work provides useful insights for future implementation of data science\nalgorithms on near-term quantum devices where problem size has been reduced by\ncoreset selection.",
    "descriptor": "\nComments: 18 pages, 16 figures\n",
    "authors": [
      "Fanzhe Qu",
      "Sarah M. Erfani",
      "Muhammad Usman"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07852"
  },
  {
    "id": "arXiv:2206.07857",
    "title": "The Scattering Transform Network with Generalized Morse Wavelets and Its  Application to Music Genre Classification",
    "abstract": "We propose to use the Generalized Morse Wavelets (GMWs) instead of\ncommonly-used Morlet (or Gabor) wavelets in the Scattering Transform Network\n(STN), which we call the GMW-STN, for signal classification problems. The GMWs\nform a parameterized family of truly analytic wavelets while the Morlet\nwavelets are only approximately analytic. The analyticity of underlying wavelet\nfilters in the STN is particularly important for nonstationary oscillatory\nsignals such as music signals because it improves interpretability of the STN\nrepresentations by providing multiscale amplitude and phase (and consequently\nfrequency) information of input signals. We demonstrate the superiority of the\nGMW-STN over the conventional STN in music genre classification using the\nso-called GTZAN database. Moreover, we show the performance improvement of the\nGMW-STN by increasing its number of layers to three over the typical two-layer\nSTN.}",
    "descriptor": "",
    "authors": [
      "Wai Ho Chak",
      "Naoki Saito",
      "David Weber"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07857"
  },
  {
    "id": "arXiv:2206.07885",
    "title": "Quantum Circuit Optimization and Transpilation via Parameterized Circuit  Instantiation",
    "abstract": "Parameterized circuit instantiation is a common technique encountered in the\ngeneration of circuits for a large class of hybrid quantum-classical\nalgorithms. Despite being supported by popular quantum compilation\ninfrastructures such as IBM Qiskit and Google Cirq, instantiation has not been\nextensively considered in the context of circuit compilation and optimization\npipelines. In this work, we describe algorithms to apply instantiation during\ntwo common compilation steps: circuit optimization and gate-set transpilation.\nWhen placed in a compilation workflow, our circuit optimization algorithm\nproduces circuits with an average of 13% fewer gates than other optimizing\ncompilers. Our gate-set transpilation algorithm can target any gate-set, even\nsets with multiple two-qubit gates, and produces circuits with an average of\n12% fewer two-qubit gates than other compilers. Overall, we show how\ninstantiation can be incorporated into a compiler workflow to improve circuit\nquality and enhance portability, all while maintaining a reasonably low compile\ntime overhead.",
    "descriptor": "",
    "authors": [
      "Ed Younis",
      "Costin Iancu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2206.07885"
  },
  {
    "id": "arXiv:2206.07917",
    "title": "To Dereverb Or Not to Dereverb? Perceptual Studies On Real-Time  Dereverberation Targets",
    "abstract": "In real life, room effect, also known as room reverberation, and the present\nbackground noise degrade the quality of speech. Recently, deep learning-based\nspeech enhancement approaches have shown a lot of promise and surpassed\ntraditional denoising and dereverberation methods. It is also well established\nthat these state-of-the-art denoising algorithms significantly improve the\nquality of speech as perceived by human listeners. But the role of\ndereverberation on subjective (perceived) speech quality, and whether the\nadditional artifacts introduced by dereverberation cause more harm than good\nare still unclear. In this paper, we attempt to answer these questions by\nevaluating a state of the art speech enhancement system in a comprehensive\nsubjective evaluation study for different choices of dereverberation targets.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Jean-Marc Valin",
      "Ritwik Giri",
      "Shrikant Venkataramani",
      "Umut Isik",
      "Arvindh Krishnaswamy"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.07917"
  },
  {
    "id": "arXiv:2206.07931",
    "title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised  Learning and Its Application to Children's ASR",
    "abstract": "Self-supervised learning (SSL) in the pretraining stage using un-annotated\nspeech data has been successful in low-resource automatic speech recognition\n(ASR) tasks. However, models trained through SSL are biased to the pretraining\ndata which is usually different from the data used in finetuning tasks, causing\na domain shifting problem, and thus resulting in limited knowledge transfer. We\npropose a novel framework, domain responsible adaptation and finetuning\n(DRAFT), to reduce domain shifting in pretrained speech models through an\nadditional adaptation stage. In DRAFT, residual adapters (RAs) are inserted in\nthe pretrained model to learn domain-related information with the same SSL loss\nas the pretraining stage. Only RA parameters are updated during the adaptation\nstage. DRAFT is agnostic to the type of SSL method used and is evaluated with\nthree widely used approaches: APC, Wav2vec2.0, and HuBERT. On two child ASR\ntasks (OGI and MyST databases), using SSL models trained with un-annotated\nadult speech data (Librispeech), relative WER improvements of up to 19.7% are\nobserved when compared to the pretrained models without adaptation. Additional\nexperiments examined the potential of cross knowledge transfer between the two\ndatasets and the results are promising, showing a broader usage of the proposed\nDRAFT framework.",
    "descriptor": "\nComments: Accepted to Interspeech 2022\n",
    "authors": [
      "Ruchao Fan",
      "Abeer Alwan"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.07931"
  },
  {
    "id": "arXiv:2206.07936",
    "title": "Universality of regularized regression estimators in high dimensions",
    "abstract": "The Convex Gaussian Min-Max Theorem (CGMT) has emerged as a prominent\ntheoretical tool for analyzing the precise stochastic behavior of various\nstatistical estimators in the so-called high dimensional proportional regime,\nwhere the sample size and the signal dimension are of the same order. However,\na well recognized limitation of the existing CGMT machinery rests in its\nstringent requirement on the exact Gaussianity of the design matrix, therefore\nrendering the obtained precise high dimensional asymptotics largely a specific\nGaussian theory in various important statistical models.\nThis paper provides a structural universality framework for a broad class of\nregularized regression estimators that is particularly compatible with the CGMT\nmachinery. In particular, we show that with a good enough $\\ell_\\infty$ bound\nfor the regression estimator $\\hat{\\mu}_A$, any `structural property' that can\nbe detected via the CGMT for $\\hat{\\mu}_G$ (under a standard Gaussian design\n$G$) also holds for $\\hat{\\mu}_A$ under a general design $A$ with independent\nentries. As a proof of concept, we demonstrate our new universality framework\nin three key examples of regularized regression estimators: the Ridge, Lasso\nand regularized robust regression estimators, where new universality properties\nof risk asymptotics and/or distributions of regression estimators and other\nrelated quantities are proved. As a major statistical implication of the Lasso\nuniversality results, we validate inference procedures using the\ndegrees-of-freedom adjusted debiased Lasso under general design and error\ndistributions. We also provide a counterexample, showing that universality\nproperties for regularized regression estimators do not extend to general\nisotropic designs.",
    "descriptor": "",
    "authors": [
      "Qiyang Han",
      "Yandi Shen"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2206.07936"
  },
  {
    "id": "arXiv:2206.07944",
    "title": "Distributed Online Learning Algorithm With Differential Privacy Strategy  for Convex Nondecomposable Global Objectives",
    "abstract": "In this paper, we deal with a general distributed constrained online learning\nproblem with privacy over time-varying networks, where a class of\nnondecomposable objective functions are considered. Under this setting, each\nnode only controls a part of the global decision variable, and the goal of all\nnodes is to collaboratively minimize the global objective over a time horizon\n$T$ while guarantees the security of the transmitted information. For such\nproblems, we first design a novel generic algorithm framework, named as DPSDA,\nof differentially private distributed online learning using the Laplace\nmechanism and the stochastic variants of dual averaging method. Then, we\npropose two algorithms, named as DPSDA-C and DPSDA-PS, under this framework.\nTheoretical results show that both algorithms attain an expected regret upper\nbound in $\\mathcal{O}( \\sqrt{T} )$ when the objective function is convex, which\nmatches the best utility achievable by cutting-edge algorithms. Finally,\nnumerical experiment results on both real-world and randomly generated datasets\nverify the effectiveness of our algorithms.",
    "descriptor": "\nComments: 16 pages, 6 figures\n",
    "authors": [
      "Huqiang Cheng",
      "Xiaofeng Liao",
      "Huaqing Li"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07944"
  },
  {
    "id": "arXiv:2206.07976",
    "title": "Cyclocopula Technique to Study the Relationship Between Two  Cyclostationary Time Series with Fractional Brownian Motion Errors",
    "abstract": "Detection of the relationship between two time series is so important in\nenvironmental and hydrological studies. Several parametric and non-parametric\napproaches can be applied to detect relationships. These techniques are usually\nsensitive to stationarity assumptions. In this research, a new copula-based\nmethod is introduced to detect the relationship between two cylostationary time\nseries with fractional Brownian motion (fBm) errors. The numerical studies\nverify the performance of the introduced approach.",
    "descriptor": "\nComments: 16 pages, t tables\n",
    "authors": [
      "Mohammadreza Mahmoudi",
      "Amir Mosavi"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07976"
  },
  {
    "id": "arXiv:2206.08002",
    "title": "The convergent Indian buffet process",
    "abstract": "We propose a new Bayesian nonparametric prior for latent feature models,\nwhich we call the convergent Indian buffet process (CIBP). We show that under\nthe CIBP, the number of latent features is distributed as a Poisson\ndistribution with the mean monotonically increasing but converging to a certain\nvalue as the number of objects goes to infinity. That is, the expected number\nof features is bounded above even when the number of objects goes to infinity,\nunlike the standard Indian buffet process under which the expected number of\nfeatures increases with the number of objects. We provide two alternative\nrepresentations of the CIBP based on a hierarchical distribution and a\ncompletely random measure, respectively, which are of independent interest. The\nproposed CIBP is assessed on a high-dimensional sparse factor model.",
    "descriptor": "",
    "authors": [
      "Ilsang Ohn"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08002"
  },
  {
    "id": "arXiv:2206.08011",
    "title": "Hardness prediction of age-hardening aluminum alloy based on ensemble  learning",
    "abstract": "With the rapid development of artificial intelligence, the combination of\nmaterial database and machine learning has driven the progress of material\ninformatics. Because aluminum alloy is widely used in many fields, so it is\nsignificant to predict the properties of aluminum alloy. In this thesis, the\ndata of Al-Cu-Mg-X (X: Zn, Zr, etc.) alloy are used to input the composition,\naging conditions (time and temperature) and predict its hardness. An ensemble\nlearning solution based on automatic machine learning and an attention\nmechanism introduced into the secondary learner of deep neural network are\nproposed respectively. The experimental results show that selecting the correct\nsecondary learner can further improve the prediction accuracy of the model.\nThis manuscript introduces the attention mechanism to improve the secondary\nlearner based on deep neural network, and obtains a fusion model with better\nperformance. The R-Square of the best model is 0.9697 and the MAE is 3.4518HV.",
    "descriptor": "",
    "authors": [
      "Zuo Houchen",
      "Jiang Yongquan",
      "Yang Yan",
      "Liu Baoying",
      "Hu Jie"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08011"
  },
  {
    "id": "arXiv:2206.08019",
    "title": "Multi-View Imputation and Cross-Attention Network Based on Incomplete  Longitudinal and Multi-Modal Data for Alzheimer's Disease Prediction",
    "abstract": "Longitudinal variations and complementary information inherent in\nlongitudinal and multi-modal data play an important role in Alzheimer's disease\n(AD) prediction, particularly in identifying subjects with mild cognitive\nimpairment who are about to have AD. However, longitudinal and multi-modal data\nmay have missing data, which hinders the effective application of these data.\nAdditionally, previous longitudinal studies require existing longitudinal data\nto achieve prediction, but AD prediction is expected to be conducted at\npatients' baseline visit (BL) in clinical practice. Thus, we proposed a\nmulti-view imputation and cross-attention network (MCNet) to integrate data\nimputation and AD prediction in a unified framework and achieve accurate AD\nprediction. First, a multi-view imputation method combined with adversarial\nlearning, which can handle a wide range of missing data situations and reduce\nimputation errors, was presented. Second, two cross-attention blocks were\nintroduced to exploit the potential associations in longitudinal and\nmulti-modal data. Finally, a multi-task learning model was built for data\nimputation, longitudinal classification, and AD prediction tasks. When the\nmodel was properly trained, the disease progression information learned from\nlongitudinal data can be leveraged by BL data to improve AD prediction. The\nproposed method was tested on two independent testing sets and single-model\ndata at BL to verify its effectiveness and flexibility on AD prediction.\nResults showed that MCNet outperformed several state-of-the-art methods.\nMoreover, the interpretability of MCNet was presented. Thus, our MCNet is a\ntool with a great application potential in longitudinal and multi-modal data\nanalysis for AD prediction. Codes are available at\nhttps://github.com/Meiyan88/MCNET.",
    "descriptor": "",
    "authors": [
      "Meiyan Huang",
      "Tao Wang",
      "Xiumei Chen",
      "Xiaoling Zhang",
      "Shuoling Zhou",
      "Qianjin Feng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08019"
  },
  {
    "id": "arXiv:2206.08023",
    "title": "AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile  Medical Image Segmentation",
    "abstract": "Despite the considerable progress in automatic abdominal multi-organ\nsegmentation from CT/MRI scans in recent years, a comprehensive evaluation of\nthe models' capabilities is hampered by the lack of a large-scale benchmark\nfrom diverse clinical scenarios. Constraint by the high cost of collecting and\nlabeling 3D medical data, most of the deep learning models to date are driven\nby datasets with a limited number of organs of interest or samples, which still\nlimits the power of modern deep models and makes it difficult to provide a\nfully comprehensive and fair estimate of various methods. To mitigate the\nlimitations, we present AMOS, a large-scale, diverse, clinical dataset for\nabdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected\nfrom multi-center, multi-vendor, multi-modality, multi-phase, multi-disease\npatients, each with voxel-level annotations of 15 abdominal organs, providing\nchallenging examples and test-bed for studying robust segmentation algorithms\nunder diverse targets and scenarios. We further benchmark several\nstate-of-the-art medical segmentation models to evaluate the status of the\nexisting methods on this new challenging dataset. We have made our datasets,\nbenchmark servers, and baselines publicly available, and hope to inspire future\nresearch. Information can be found at https://amos22.grand-challenge.org.",
    "descriptor": "",
    "authors": [
      "Yuanfeng Ji",
      "Haotian Bai",
      "Jie Yang",
      "Chongjian Ge",
      "Ye Zhu",
      "Ruimao Zhang",
      "Zhen Li",
      "Lingyan Zhang",
      "Wanling Ma",
      "Xiang Wan",
      "Ping Luo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08023"
  },
  {
    "id": "arXiv:2206.08048",
    "title": "Automated analysis of continuum fields from atomistic simulations using  statistical machine learning",
    "abstract": "Atomistic simulations of the molecular dynamics/statics kind are regularly\nused to study small scale plasticity. Contemporary simulations are performed\nwith tens to hundreds of millions of atoms, with snapshots of these\nconfigurations written out at regular intervals for further analysis. Continuum\nscale constitutive models for material behavior can benefit from information on\nthe atomic scale, in particular in terms of the deformation mechanisms, the\naccommodation of the total strain and partitioning of stress and strain fields\nin individual grains. In this work we develop a methodology using statistical\ndata mining and machine learning algorithms to automate the analysis of\ncontinuum field variables in atomistic simulations. We focus on three important\nfield variables: total strain, elastic strain and microrotation. Our results\nshow that the elastic strain in individual grains exhibits a unimodal\nlog-normal distribution, whilst the total strain and microrotation fields\nevidence a multimodal distribution. The peaks in the distribution of total\nstrain are identified with a Gaussian mixture model and methods to circumvent\noverfitting problems are presented. Subsequently, we evaluate the identified\npeaks in terms of deformation mechanisms in a grain, which e.g., helps to\nquantify the strain for which individual deformation mechanisms are\nresponsible. The overall statistics of the distributions over all grains are an\nimportant input for higher scale models, which ultimately also helps to be able\nto quantitatively discuss the implications for information transfer to\nphenomenological models.",
    "descriptor": "\nComments: 17 pages, 8 Figures in the main paper, 44 pages, 7 figures in supplementary material\n",
    "authors": [
      "Aruna Prakash",
      "Stefan Sandfeld"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08048"
  },
  {
    "id": "arXiv:2206.08051",
    "title": "Voronoi Density Estimator for High-Dimensional Data: Computation,  Compactification and Convergence",
    "abstract": "The Voronoi Density Estimator (VDE) is an established density estimation\ntechnique that adapts to the local geometry of data. However, its applicability\nhas been so far limited to problems in two and three dimensions. This is\nbecause Voronoi cells rapidly increase in complexity as dimensions grow, making\nthe necessary explicit computations infeasible. We define a variant of the VDE\ndeemed Compactified Voronoi Density Estimator (CVDE), suitable for higher\ndimensions. We propose computationally efficient algorithms for numerical\napproximation of the CVDE and formally prove convergence of the estimated\ndensity to the original one. We implement and empirically validate the CVDE\nthrough a comparison with the Kernel Density Estimator (KDE). Our results\nindicate that the CVDE outperforms the KDE on sound and image data.",
    "descriptor": "\nComments: Accepted at the Conference on Uncertainty in Artificial Intelligence (UAI) 2022\n",
    "authors": [
      "Vladislav Polianskii",
      "Giovanni Luca Marchetti",
      "Alexander Kravberg",
      "Anastasiia Varava",
      "Florian T. Pokorny",
      "Danica Kragic"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.08051"
  },
  {
    "id": "arXiv:2206.08058",
    "title": "Nonwords Pronunciation Classification in Language Development Tests for  Preschool Children",
    "abstract": "This work aims to automatically evaluate whether the language development of\nchildren is age-appropriate. Validated speech and language tests are used for\nthis purpose to test the auditory memory. In this work, the task is to\ndetermine whether spoken nonwords have been uttered correctly. We compare\ndifferent approaches that are motivated to model specific language structures:\nLow-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated\nembeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR\nacoustic model). Each of the approaches provides input for VGG-like 5-layer CNN\nclassifiers. We also examine the adaptation per nonword. The evaluation of the\nproposed systems was performed using recordings from different kindergartens of\nspoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model\nphonetic information; wav2vec2.0 is trained on grapheme labels, our ASR\nacoustic model features contain (sub-)phonetic information. We found that the\nmore granular the phonetic modeling is, the higher are the achieved recognition\nrates. The best system trained on ASR acoustic model features with VTLN\nachieved an accuracy of 89.4% and an area under the ROC (Receiver Operating\nCharacteristic) curve (AUC) of 0.923. This corresponds to an improvement in\naccuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.",
    "descriptor": "\nComments: Accepted to Interspeech 2022\n",
    "authors": [
      "Ilja Baumann",
      "Dominik Wagner",
      "Sebastian Bayerl",
      "Tobias Bocklet"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.08058"
  },
  {
    "id": "arXiv:2206.08078",
    "title": "U-PET: MRI-based Dementia Detection with Joint Generation of Synthetic  FDG-PET Images",
    "abstract": "Alzheimer's disease (AD) is the most common cause of dementia. An early\ndetection is crucial for slowing down the disease and mitigating risks related\nto the progression. While the combination of MRI and FDG-PET is the best\nimage-based tool for diagnosis, FDG-PET is not always available. The reliable\ndetection of Alzheimer's disease with only MRI could be beneficial, especially\nin regions where FDG-PET might not be affordable for all patients. To this end,\nwe propose a multi-task method based on U-Net that takes T1-weighted MR images\nas an input to generate synthetic FDG-PET images and classifies the dementia\nprogression of the patient into cognitive normal (CN), cognitive impairment\n(MCI), and AD. The attention gates used in both task heads can visualize the\nmost relevant parts of the brain, guiding the examiner and adding\ninterpretability. Results show the successful generation of synthetic FDG-PET\nimages and a performance increase in disease classification over the naive\nsingle-task baseline.",
    "descriptor": "",
    "authors": [
      "Marcel Kollovieh",
      "Matthias Keicher",
      "Stephan Wunderlich",
      "Hendrik Burwinkel",
      "Thomas Wendler",
      "Nassir Navab"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08078"
  },
  {
    "id": "arXiv:2206.08100",
    "title": "DeepJSCC-Q: Constellation Constrained Deep Joint Source-Channel Coding",
    "abstract": "Recent works have shown that modern machine learning techniques can provide\nan alternative approach to the long-standing joint source-channel coding (JSCC)\nproblem. Very promising initial results, superior to popular digital schemes\nthat utilize separate source and channel codes, have been demonstrated for\nwireless image and video transmission using deep neural networks (DNNs).\nHowever, end-to-end training of such schemes requires a differentiable channel\ninput representation; hence, prior works have assumed that any complex value\ncan be transmitted over the channel. This can prevent the application of these\ncodes in scenarios where the hardware or protocol can only admit certain sets\nof channel inputs, prescribed by a digital constellation. Herein, we propose\nDeepJSCC-Q, an end-to-end optimized JSCC solution for wireless image\ntransmission using a finite channel input alphabet. We show that DeepJSCC-Q can\nachieve similar performance to prior works that allow any complex valued\nchannel input, especially when high modulation orders are available, and that\nthe performance asymptotically approaches that of unconstrained channel input\nas the modulation order increases. Importantly, DeepJSCC-Q preserves the\ngraceful degradation of image quality in unpredictable channel conditions, a\ndesirable property for deployment in mobile systems with rapidly changing\nchannel conditions.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2111.13042\n",
    "authors": [
      "Tze-Yang Tung",
      "David Burth Kurka",
      "Mikolaj Jankowski",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08100"
  },
  {
    "id": "arXiv:2206.08137",
    "title": "Large-scale, multi-centre, multi-disease validation of an AI clinical  tool for cine CMR analysis",
    "abstract": "INTRODUCTION: Artificial intelligence (AI) has the potential to facilitate\nthe automation of CMR analysis for biomarker extraction. However, most AI\nalgorithms are trained on a specific input domain (e.g., single scanner vendor\nor hospital-tailored imaging protocol) and lack the robustness to perform\noptimally when applied to CMR data from other input domains. METHODS: Our\nproposed framework consists of an AI-based algorithm for biventricular\nsegmentation of short-axis images, followed by a post-analysis quality control\nto detect erroneous results. The segmentation algorithm was trained on a large\ndataset of clinical CMR scans from two NHS hospitals (n=2793) and validated on\nadditional cases from this dataset (n=441) and on five external datasets\n(n=6808). The validation data included CMR scans of patients with a range of\ndiseases acquired at 12 different centres using CMR scanners from all major\nvendors. RESULTS: Our method yielded median Dice scores over 87%, translating\ninto median absolute errors in cardiac biomarkers within the range of\ninter-observer variability: <8.4mL (left ventricle), <9.2mL (right ventricle),\n<13.3g (left ventricular mass), and <5.9% (ejection fraction) across all\ndatasets. Stratification of cases according to phenotypes of cardiac disease\nand scanner vendors showed good agreement. CONCLUSIONS: We show that our\nproposed tool, which combines a state-of-the-art AI algorithm trained on a\nlarge-scale multi-domain CMR dataset with a post-analysis quality control,\nallows us to robustly deal with routine clinical data from multiple centres,\nvendors, and cardiac diseases. This is a fundamental step for the clinical\ntranslation of AI algorithms. Moreover, our method yields a range of additional\nbiomarkers of cardiac function (filling and ejection rates, regional wall\nmotion, and strain) at no extra computational cost.",
    "descriptor": "\nComments: 35 pages, 9 figures, to be submitted to JACC: Cardiovascular Imaging, Bram Ruijsink and Esther Puyol-Ant\\'on are shared last authors\n",
    "authors": [
      "Jorge Mariscal-Harana",
      "Clint Asher",
      "Vittoria Vergani",
      "Maleeha Rizvi",
      "Louise Keehn",
      "Raymond J. Kim",
      "Robert M. Judd",
      "Steffen E. Petersen",
      "Reza Razavi",
      "Andrew King",
      "Bram Ruijsink",
      "Esther Puyol-Ant\u00f3n"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2206.08137"
  },
  {
    "id": "arXiv:2206.08139",
    "title": "Cold Dynamics in Cellular Automata: a Tutorial",
    "abstract": "This tutorial is about cellular automata that exhibit 'cold dynamics'. By\nthis we mean zero entropy, stabilization of all orbits, trivial asymptotic\ndynamics, etc. These are purely transient irreversible dynamics, but they\ncapture many examples from the literature. A rich zoo of properties is\npresented and discussed: nilpotency and asymptotic, generic or mu-variants,\nunique ergodicity, convergence, bounded-changeness, freezingness. They all\ncorrespond to the 'cold dynamics' paradigm in some way, and we study their\nlinks and differences by various examples and results from the literature.\nBesides dynamical considerations, we also focus on computational aspects: we\nshow how such 'cold cellular automata' can still compute under their dynamical\nconstraint, and what are their computational limitation.",
    "descriptor": "\nComments: Natural Computing, Springer Verlag, 2022\n",
    "authors": [
      "Guillaume Theyssier"
    ],
    "subjectives": [
      "Cellular Automata and Lattice Gases (nlin.CG)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2206.08139"
  },
  {
    "id": "arXiv:2206.08169",
    "title": "Deterministic and Random Perturbations of the Kepler Problem",
    "abstract": "We investigate perturbations in the Kepler problem. We offer an overview of\nthe dynamical system using Newtonian, Lagrangian and Hamiltonian Mechanics to\nbuild a foundation for analyzing perturbations. We consider the effects of a\ndeterministic perturbation in the form of a first order relativistic correction\nwhich change bounded orbits from standard to precessing ellipses. We also\nconsider the effects of stochastic perturbations with certain potentials and\nevaluate the analytical results of mean exit times using Monte Carlo\nsimulations.",
    "descriptor": "\nComments: 51 pages, 13 figures. Honors thesis submitted to Department of Mathematics of the College of Staten Island City University of New York in partial fulfillment of the requirements for the degree of Bachelor of Science in Mathematics with Honors. Signatures not included due to privacy concerns\n",
    "authors": [
      "Jesse Dimino"
    ],
    "subjectives": [
      "Classical Physics (physics.class-ph)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08169"
  },
  {
    "id": "arXiv:2206.08174",
    "title": "Strategies to Improve Robustness of Target Speech Extraction to  Enrollment Variations",
    "abstract": "Target speech extraction is a technique to extract the target speaker's voice\nfrom mixture signals using a pre-recorded enrollment utterance that\ncharacterize the voice characteristics of the target speaker. One major\ndifficulty of target speech extraction lies in handling variability in\n``intra-speaker'' characteristics, i.e., characteristics mismatch between\ntarget speech and an enrollment utterance. While most conventional approaches\nfocus on improving {\\it average performance} given a set of enrollment\nutterances, here we propose to guarantee the {\\it worst performance}, which we\nbelieve is of great practical importance. In this work, we propose an\nevaluation metric called worst-enrollment source-to-distortion ratio (SDR) to\nquantitatively measure the robustness towards enrollment variations. We also\nintroduce a novel training scheme that aims at directly optimizing the\nworst-case performance by focusing on training with difficult enrollment cases\nwhere extraction does not perform well. In addition, we investigate the\neffectiveness of auxiliary speaker identification loss (SI-loss) as another way\nto improve robustness over enrollments. Experimental validation reveals the\neffectiveness of both worst-enrollment target training and SI-loss training to\nimprove robustness against enrollment variations, by increasing speaker\ndiscriminability.",
    "descriptor": "\nComments: 5 pages, 2 figures, 3 tables Submitted to Interspeech 2022\n",
    "authors": [
      "Hiroshi Sato",
      "Tsubasa Ochiai",
      "Marc Delcroix",
      "Keisuke Kinoshita",
      "Takafumi Moriya",
      "Naoki Makishima",
      "Mana Ihori",
      "Tomohiro Tanaka",
      "Ryo Masumura"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.08174"
  },
  {
    "id": "arXiv:2206.08178",
    "title": "User Engagement and Churn in Mobile Health Applications",
    "abstract": "Mobile health apps are revolutionizing the healthcare ecosystem by improving\ncommunication, efficiency, and quality of service. In low- and middle-income\ncountries, they also play a unique role as a source of information about health\noutcomes and behaviors of patients and healthcare workers, while providing a\nsuitable channel to deliver both personalized and collective policy\ninterventions. We propose a framework to study user engagement with mobile\nhealth, focusing on healthcare workers and digital health apps designed to\nsupport them in resource-poor settings. The behavioral logs produced by these\napps can be transformed into daily time series characterizing each user's\nactivity. We use probabilistic and survival analysis to build multiple\npersonalized measures of meaningful engagement, which could serve to tailor\ncontent and digital interventions suiting each health worker's specific needs.\nSpecial attention is given to the problem of detecting churn, understood as a\nmarker of complete disengagement. We discuss the application of our methods to\nthe Indian and Ethiopian users of the Safe Delivery App, a capacity-building\ntool for skilled birth attendants. This work represents an important step\ntowards a full characterization of user engagement in mobile health\napplications, which can significantly enhance the abilities of health workers\nand, ultimately, save lives.",
    "descriptor": "\nComments: Accepted at KDD 2022 Health Day, will be appear in the KDD2022 proceedings as a full paper\n",
    "authors": [
      "Babaniyi Yusuf Olaniyi",
      "Ana Fern\u00e1ndez del R\u00edo",
      "\u00c1frica Peri\u00e1\u00f1ez",
      "Lauren Bellhouse"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.08178"
  },
  {
    "id": "arXiv:2206.08195",
    "title": "On a High-Precision Method for Studying Attractors of Dynamical Systems  and Systems of Explosive Type",
    "abstract": "The author of this article considers a numerical method that uses\nhigh-precision calculations to construct approximations to attractors of\ndynamical systems of chaotic type with a quadratic right-hand side, as well as\nto find the vertical asymptotes of solutions of systems of explosive type. A\nspecial case of such systems is the population explosion model. A theorem on\nthe existence of asymptotes is proved. The extension of the numerical method\nfor piecewise smooth systems is described using the Chua system as an example,\nas well as systems with hysteresis.",
    "descriptor": "\nComments: Mathematics, 10:8 (2022), 1207\n",
    "authors": [
      "Alexander N. Pchelintsev"
    ],
    "subjectives": [
      "Chaotic Dynamics (nlin.CD)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.08195"
  },
  {
    "id": "arXiv:2206.08199",
    "title": "MAGIC: Microlensing Analysis Guided by Intelligent Computation",
    "abstract": "The modeling of binary microlensing light curves via the standard\nsampling-based method can be challenging, because of the time-consuming light\ncurve computation and the pathological likelihood landscape in the\nhigh-dimensional parameter space. In this work, we present MAGIC, which is a\nmachine learning framework to efficiently and accurately infer the microlensing\nparameters of binary events with realistic data quality. In MAGIC, binary\nmicrolensing parameters are divided into two groups and inferred separately\nwith different neural networks. The key feature of MAGIC is the introduction of\nneural controlled differential equation, which provides the capability to\nhandle light curves with irregular sampling and large data gaps. Based on\nsimulated light curves, we show that MAGIC can achieve fractional uncertainties\nof a few percent on the binary mass ratio and separation. We also test MAGIC on\na real microlensing event. MAGIC is able to locate the degenerate solutions\neven when large data gaps are introduced. As irregular samplings are common in\nastronomical surveys, our method also has implications to other studies that\ninvolve time series.",
    "descriptor": "\nComments: 20 pages, 15 figures, code available at this https URL . An earlier version of this work is accepted to the ICML 2022 Workshop on Machine Learning for Astrophysics at this https URL\n",
    "authors": [
      "Haimeng Zhao",
      "Wei Zhu"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2206.08199"
  },
  {
    "id": "arXiv:2206.08201",
    "title": "Learning Physics between Digital Twins with Low-Fidelity Models and  Physics-Informed Gaussian Processes",
    "abstract": "A digital twin is a computer model that represents an individual, for\nexample, a component, a patient or a process. In many situations, we want to\ngain knowledge about an individual from its data while incorporating imperfect\nphysical knowledge and also learn from data from other individuals. In this\npaper, we introduce and demonstrate a fully Bayesian methodology for learning\nbetween digital twins in a setting where the physical parameters of each\nindividual are of interest. For each individual, the methodology is based on\nBayesian calibration with model discrepancy. Through the discrepancy, modelled\nas a Gaussian process, the imperfect low-fidelity physical model is accounted\nfor. Using ideas from Bayesian hierarchical models, a joint probabilistic model\nof digital twins is constructed by connecting them through a new level in the\nhierarchy. For the physical parameters, the methodology can be seen as using a\nprior distribution in the individual model that is the posterior of the\ncorresponding hyperparameter in the joint model. For learning the imperfect\nphysics between individuals two approaches are introduced, one that assumes the\nsame discrepancy for all individuals and one that can be seen as using a prior\nlearned from all individuals for the parameters of the Gaussian processes\nrepresenting the discrepancies. Based on recent advances related to\nphysics-informed priors, Hamiltonian Monte Carlo methods and using these for\ninverse problems we set up an inference methodology that allows our approach to\nbe computational feasible also for physical models based on partial\ndifferential equations and individual data that are not aligned. The\nmethodology is demonstrated in two synthetic case studies, a toy example\npreviously used in the literature extended to more individuals and an example\nbased on a cardiovascular differential equation model relevant for the\ntreatment of hypertension.",
    "descriptor": "\nComments: 17 pages, 12 figures\n",
    "authors": [
      "Michail Spitieris",
      "Ingelin Steinsland"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.08201"
  },
  {
    "id": "arXiv:2206.08220",
    "title": "Functional Output Regression with Infimal Convolution: Exploring the  Huber and $\u03b5$-insensitive Losses",
    "abstract": "The focus of the paper is functional output regression (FOR) with convoluted\nlosses. While most existing work consider the square loss setting, we leverage\nextensions of the Huber and the $\\epsilon$-insensitive loss (induced by infimal\nconvolution) and propose a flexible framework capable of handling various forms\nof outliers and sparsity in the FOR family. We derive computationally tractable\nalgorithms relying on duality to tackle the resulting tasks in the context of\nvector-valued reproducing kernel Hilbert spaces. The efficiency of the approach\nis demonstrated and contrasted with the classical squared loss setting on both\nsynthetic and real-world benchmarks.",
    "descriptor": "\nComments: 24 pages, ICML 2022\n",
    "authors": [
      "Alex Lambert",
      "Dimitri Bouche",
      "Zoltan Szabo",
      "Florence d'Alch\u00e9-Buc"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08220"
  },
  {
    "id": "arXiv:2206.08262",
    "title": "Attention-wise masked graph contrastive learning for predicting  molecular property",
    "abstract": "Accurate and efficient prediction of the molecular properties of drugs is one\nof the fundamental problems in drug research and development. Recent\nadvancements in representation learning have been shown to greatly improve the\nperformance of molecular property prediction. However, due to limited labeled\ndata, supervised learning-based molecular representation algorithms can only\nsearch limited chemical space, which results in poor generalizability. In this\nwork, we proposed a self-supervised representation learning framework for\nlarge-scale unlabeled molecules. We developed a novel molecular graph\naugmentation strategy, referred to as attention-wise graph mask, to generate\nchallenging positive sample for contrastive learning. We adopted the graph\nattention network (GAT) as the molecular graph encoder, and leveraged the\nlearned attention scores as masking guidance to generate molecular augmentation\ngraphs. By minimization of the contrastive loss between original graph and\nmasked graph, our model can capture important molecular structure and\nhigher-order semantic information. Extensive experiments showed that our\nattention-wise graph mask contrastive learning exhibit state-of-the-art\nperformance in a couple of downstream molecular property prediction tasks.",
    "descriptor": "",
    "authors": [
      "Hui Liu",
      "Yibiao Huang",
      "Xuejun Liu",
      "Lei Deng"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08262"
  },
  {
    "id": "arXiv:2206.08265",
    "title": "Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order  Denoising Score Matching",
    "abstract": "Score-based generative models have excellent performance in terms of\ngeneration quality and likelihood. They model the data distribution by matching\na parameterized score network with first-order data score functions. The score\nnetwork can be used to define an ODE (\"score-based diffusion ODE\") for exact\nlikelihood evaluation. However, the relationship between the likelihood of the\nODE and the score matching objective is unclear. In this work, we prove that\nmatching the first-order score is not sufficient to maximize the likelihood of\nthe ODE, by showing a gap between the maximum likelihood and score matching\nobjectives. To fill up this gap, we show that the negative likelihood of the\nODE can be bounded by controlling the first, second, and third-order score\nmatching errors; and we further present a novel high-order denoising score\nmatching method to enable maximum likelihood training of score-based diffusion\nODEs. Our algorithm guarantees that the higher-order matching error is bounded\nby the training error and the lower-order errors. We empirically observe that\nby high-order score matching, score-based diffusion ODEs achieve better\nlikelihood on both synthetic data and CIFAR-10, while retaining the high\ngeneration quality.",
    "descriptor": "\nComments: Accepted in ICML 2022\n",
    "authors": [
      "Cheng Lu",
      "Kaiwen Zheng",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08265"
  },
  {
    "id": "arXiv:2206.08272",
    "title": "Longitudinal detection of new MS lesions using Deep Learning",
    "abstract": "The detection of new multiple sclerosis (MS) lesions is an important marker\nof the evolution of the disease. The applicability of learning-based methods\ncould automate this task efficiently. However, the lack of annotated\nlongitudinal data with new-appearing lesions is a limiting factor for the\ntraining of robust and generalizing models. In this work, we describe a\ndeep-learning-based pipeline addressing the challenging task of detecting and\nsegmenting new MS lesions. First, we propose to use transfer-learning from a\nmodel trained on a segmentation task using single time-points. Therefore, we\nexploit knowledge from an easier task and for which more annotated datasets are\navailable. Second, we propose a data synthesis strategy to generate realistic\nlongitudinal time-points with new lesions using single time-point scans. In\nthis way, we pretrain our detection model on large synthetic annotated\ndatasets. Finally, we use a data-augmentation technique designed to simulate\ndata diversity in MRI. By doing that, we increase the size of the available\nsmall annotated longitudinal datasets. Our ablation study showed that each\ncontribution lead to an enhancement of the segmentation accuracy. Using the\nproposed pipeline, we obtained the best score for the segmentation and the\ndetection of new MS lesions in the MSSEG2 MICCAI challenge.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Reda Abdellah Kamraoui",
      "Boris Mansencal",
      "Jos\u00e9 V Manjon",
      "Pierrick Coup\u00e9"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08272"
  },
  {
    "id": "arXiv:2206.08273",
    "title": "Concentration of Data Encoding in Parameterized Quantum Circuits",
    "abstract": "Variational quantum algorithms have been acknowledged as a leading strategy\nto realize near-term quantum advantages in meaningful tasks, including machine\nlearning and combinatorial optimization. When applied to tasks involving\nclassical data, such algorithms generally begin with quantum circuits for data\nencoding and then train quantum neural networks (QNNs) to minimize target\nfunctions. Although QNNs have been widely studied to improve these algorithms'\nperformance on practical tasks, there is a gap in systematically understanding\nthe influence of data encoding on the eventual performance. In this paper, we\nmake progress in filling this gap by considering the common data encoding\nstrategies based on parameterized quantum circuits. We prove that, under\nreasonable assumptions, the distance between the average encoded state and the\nmaximally mixed state could be explicitly upper-bounded with respect to the\nwidth and depth of the encoding circuit. This result in particular implies that\nthe average encoded state will concentrate on the maximally mixed state at an\nexponential speed on depth. Such concentration seriously limits the\ncapabilities of quantum classifiers, and strictly restricts the\ndistinguishability of encoded states from a quantum information perspective. We\nfurther support our findings by numerically verifying these results on both\nsynthetic and public data sets. Our results highlight the significance of\nquantum data encoding in machine learning tasks and may shed light on future\nencoding strategies.",
    "descriptor": "\nComments: 26 pages including appendix\n",
    "authors": [
      "Guangxi Li",
      "Ruilin Ye",
      "Xuanqiang Zhao",
      "Xin Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08273"
  },
  {
    "id": "arXiv:2206.08277",
    "title": "A machine-generated catalogue of Charon's craters and implications for  the Kuiper belt",
    "abstract": "In this paper we investigate Charon's craters size distribution using a deep\nlearning model. This is motivated by the recent results of Singer et al. (2019)\nwho, using manual cataloging, found a change in the size distribution slope of\ncraters smaller than 12 km in diameter, translating into a paucity of small\nKuiper Belt objects. These results were corroborated by Robbins and Singer\n(2021), but opposed by Morbidelli et al. (2021), necessitating an independent\nreview. Our MaskRCNN-based ensemble of models was trained on Lunar, Mercurian,\nand Martian crater catalogues and both optical and digital elevation images. We\nuse a robust image augmentation scheme to force the model to generalize and\ntransfer-learn into icy objects. With no prior bias or exposure to Charon, our\nmodel find best fit slopes of q =-1.47+-0.33 for craters smaller than 10 km,\nand q =-2.91+-0.51 for craters larger than 15 km. These values indicate a clear\nchange in slope around 15 km as suggested by Singer et al. (2019) and thus\nindependently confirm their conclusions. Our slopes however are both slightly\nflatter than those found more recently by Robbins and Singer (2021). Our\ntrained models and relevant codes are available online on\ngithub.com/malidib/ACID .",
    "descriptor": "\nComments: 16 pages, 2 figures, accepted for publication in Icarus\n",
    "authors": [
      "Mohamad Ali-Dib"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08277"
  },
  {
    "id": "arXiv:2206.08290",
    "title": "Multi-path fading and interference mitigation with Reconfigurable  Intelligent Surfaces",
    "abstract": "We exploit multi-path fading propagation to improve both the\nsignal-to-interference-plus-noise-ratio and the stability of wireless\ncommunications within electromagnetic environments that support rich multipath\npropagation. Quasi-passive propagation control with multiple binary\nreconfigurable intelligent surfaces is adopted to control the stationary waves\nsupported by a metallic cavity hosting a software-defined radio link. Results\nare demonstrated in terms of the error vector magnitude minimization of a\nquadrature phase-shift modulation scheme under no-line-of-sight conditions. It\nis found that the magnitude of fluctuation of received symbols is reduced to a\nstable constellation by increasing the number of individual surfaces, or\nelements, thus demonstrating channel hardening. By using a second\nsoftware-defined radio device as a jammer, we demonstrate the ability of the\nRIS to mitigate the co-channel interference by channel hardening. Results are\nof particular interest in smart radio environments for mobile network\narchitectures beyond 5G.",
    "descriptor": "\nComments: 13 Pages, submitted to IEEE Access\n",
    "authors": [
      "Jean Baptiste Gros",
      "Geoffroy Lerosey",
      "Fabrice Lemoult",
      "Mir Lodro",
      "Steve Greedy",
      "Gabriele Gradoni"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2206.08290"
  },
  {
    "id": "arXiv:2206.08298",
    "title": "Video Capsule Endoscopy Classification using Focal Modulation Guided  Convolutional Neural Network",
    "abstract": "Video capsule endoscopy is a hot topic in computer vision and medicine. Deep\nlearning can have a positive impact on the future of video capsule endoscopy\ntechnology. It can improve the anomaly detection rate, reduce physicians' time\nfor screening, and aid in real-world clinical analysis. CADx classification\nsystem for video capsule endoscopy has shown a great promise for further\nimprovement. For example, detection of cancerous polyp and bleeding can lead to\nswift medical response and improve the survival rate of the patients. To this\nend, an automated CADx system must have high throughput and decent accuracy. In\nthis paper, we propose FocalConvNet, a focal modulation network integrated with\nlightweight convolutional layers for the classification of small bowel\nanatomical landmarks and luminal findings. FocalConvNet leverages focal\nmodulation to attain global context and allows global-local spatial\ninteractions throughout the forward pass. Moreover, the convolutional block\nwith its intrinsic inductive/learning bias and capacity to extract hierarchical\nfeatures allows our FocalConvNet to achieve favourable results with high\nthroughput. We compare our FocalConvNet with other SOTA on Kvasir-Capsule, a\nlarge-scale VCE dataset with 44,228 frames with 13 classes of different\nanomalies. Our proposed method achieves the weighted F1-score, recall and MCC}\nof 0.6734, 0.6373 and 0.2974, respectively outperforming other SOTA\nmethodologies. Furthermore, we report the highest throughput of 148.02\nimages/second rate to establish the potential of FocalConvNet in a real-time\nclinical environment. The code of the proposed FocalConvNet is available at\nhttps://github.com/NoviceMAn-prog/FocalConvNet.",
    "descriptor": "",
    "authors": [
      "Abhishek Srivastava",
      "Nikhil Kumar Tomar",
      "Ulas Bagci",
      "Debesh Jha"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08298"
  },
  {
    "id": "arXiv:2206.08308",
    "title": "Deepfake histological images for enhancing digital pathology",
    "abstract": "An optical microscopic examination of thinly cut stained tissue on glass\nslides prepared from a FFPE tissue blocks is the gold standard for tissue\ndiagnostics. In addition, the diagnostic abilities and expertise of any\npathologist is dependent on their direct experience with common as well as\nrarer variant morphologies. Recently, deep learning approaches have been used\nto successfully show a high level of accuracy for such tasks. However,\nobtaining expert-level annotated images is an expensive and time-consuming task\nand artificially synthesized histological images can prove greatly beneficial.\nHere, we present an approach to not only generate histological images that\nreproduce the diagnostic morphologic features of common disease but also\nprovide a user ability to generate new and rare morphologies. Our approach\ninvolves developing a generative adversarial network model that synthesizes\npathology images constrained by class labels. We investigated the ability of\nthis framework in synthesizing realistic prostate and colon tissue images and\nassessed the utility of these images in augmenting diagnostic ability of\nmachine learning methods as well as their usability by a panel of experienced\nanatomic pathologists. Synthetic data generated by our framework performed\nsimilar to real data in training a deep learning model for diagnosis.\nPathologists were not able to distinguish between real and synthetic images and\nshowed a similar level of inter-observer agreement for prostate cancer grading.\nWe extended the approach to significantly more complex images from colon\nbiopsies and showed that the complex microenvironment in such tissues can also\nbe reproduced. Finally, we present the ability for a user to generate deepfake\nhistological images via a simple markup of sematic labels.",
    "descriptor": "",
    "authors": [
      "Kianoush Falahkheirkhah",
      "Saumya Tiwari",
      "Kevin Yeh",
      "Sounak Gupta",
      "Loren Herrera-Hernandez",
      "Michael R. McCarthy",
      "Rafael E. Jimenez",
      "John C. Cheville",
      "Rohit Bhargava"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08308"
  },
  {
    "id": "arXiv:2206.08336",
    "title": "Constrained Submodular Optimization for Vaccine Design",
    "abstract": "Advances in machine learning have enabled the prediction of immune system\nresponses to prophylactic and therapeutic vaccines. However, the engineering\ntask of designing vaccines remains a challenge. In particular, the genetic\nvariability of the human immune system makes it difficult to design peptide\nvaccines that provide widespread immunity in vaccinated populations. We\nintroduce a framework for evaluating and designing peptide vaccines that uses\nprobabilistic machine learning models, and demonstrate its ability to produce\ndesigns for a SARS-CoV-2 vaccine that outperform previous designs. We provide a\ntheoretical analysis of the approximability, scalability, and complexity of our\nframework.",
    "descriptor": "\nComments: 20 pages, 5 figures\n",
    "authors": [
      "Zheng Dai",
      "David Gifford"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.08336"
  },
  {
    "id": "arXiv:2206.08342",
    "title": "An Optimal Product-State Approximation for 2-Local Quantum Hamiltonians  with Positive Terms",
    "abstract": "We resolve the approximability of the maximum energy of the Quantum Max Cut\n(QMC) problem using product states. A classical 0.498-approximation, using a\nbasic semidefinite programming relaxation, is known for QMC, paralleling the\ncelebrated 0.878-approximation for classical Max Cut. For Max Cut, improving\nthe 0.878-approximation is Unique-Games-hard (UG-hard), and one might expect\nthat improving the 0.498-approximation is UG-hard for QMC. In contrast, we give\na classical 1/2-approximation for QMC that is unconditionally optimal, since\nsimple examples exhibit a gap of 1/2 between the energies of an optimal product\nstate and general quantum state. Our result relies on a new nonlinear monogamy\nof entanglement inequality on a triangle that is derived from the second level\nof the quantum Lasserre hierarchy. This inequality also applies to the quantum\nHeisenberg model, and our results generalize to instances of Max 2-Local\nHamiltonian where each term is positive and has no 1-local parts. Finally, we\ngive further evidence that product states are essential for approximations of\n2-Local Hamiltonian.",
    "descriptor": "\nComments: 40 pages; presented at QIP 2022\n",
    "authors": [
      "Ojas Parekh",
      "Kevin Thompson"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.08342"
  },
  {
    "id": "arXiv:1807.05443",
    "title": "Exact Algorithms and Lower Bounds for Stable Instances of Euclidean  k-Means",
    "abstract": "Comments: 28 pages, 2 figures",
    "descriptor": "\nComments: 28 pages, 2 figures\n",
    "authors": [
      "Zachary Friggstad",
      "Kamyar Khodamoradi",
      "Mohammad R. Salavatipour"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1807.05443"
  },
  {
    "id": "arXiv:2002.09564",
    "title": "Towards Robust and Reproducible Active Learning Using Neural Networks",
    "abstract": "Comments: Accepted at CVPR 2022; Improved figures and plots for better readability",
    "descriptor": "\nComments: Accepted at CVPR 2022; Improved figures and plots for better readability\n",
    "authors": [
      "Prateek Munjal",
      "Nasir Hayat",
      "Munawar Hayat",
      "Jamshid Sourati",
      "Shadab Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.09564"
  },
  {
    "id": "arXiv:2003.08024",
    "title": "Face Anti-Spoofing by Learning Polarization Cues in a Real-World  Scenario",
    "abstract": "Comments: 14pages,8figures",
    "descriptor": "\nComments: 14pages,8figures\n",
    "authors": [
      "Yu Tian",
      "Kunbo Zhang",
      "Leyuan Wang",
      "Zhenan Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.08024"
  },
  {
    "id": "arXiv:2004.04331",
    "title": "Robust Linear Precoder Design for 3D Massive MIMO Downlink with A  Posteriori Channel Model",
    "abstract": "Comments: 29 pages, 6 figures",
    "descriptor": "\nComments: 29 pages, 6 figures\n",
    "authors": [
      "An-An Lu",
      "Xiqi Gao",
      "Chengshan Xiao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2004.04331"
  },
  {
    "id": "arXiv:2004.10240",
    "title": "Deep Learning for Time Series Forecasting: Tutorial and Literature  Survey",
    "abstract": "Comments: 33 pages, 6 figures",
    "descriptor": "\nComments: 33 pages, 6 figures\n",
    "authors": [
      "Konstantinos Benidis",
      "Syama Sundar Rangapuram",
      "Valentin Flunkert",
      "Yuyang Wang",
      "Danielle Maddix",
      "Caner Turkmen",
      "Jan Gasthaus",
      "Michael Bohlke-Schneider",
      "David Salinas",
      "Lorenzo Stella",
      "Francois-Xavier Aubet",
      "Laurent Callot",
      "Tim Januschowski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2004.10240"
  },
  {
    "id": "arXiv:2005.05048",
    "title": "A Light Signalling Approach to Node Grouping for Massive MIMO IoT  Networks",
    "abstract": "A Light Signalling Approach to Node Grouping for Massive MIMO IoT  Networks",
    "descriptor": "",
    "authors": [
      "Emma Fitzgerald",
      "Micha\u0142 Pi\u00f3ro",
      "Harsh Tataria",
      "Gilles Callebaut",
      "Sara Gunnarsson",
      "Liesbet Van der Perre"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2005.05048"
  },
  {
    "id": "arXiv:2008.02027",
    "title": "Learning to Denoise Historical Music",
    "abstract": "Comments: ISMIR 2020",
    "descriptor": "\nComments: ISMIR 2020\n",
    "authors": [
      "Yunpeng Li",
      "Beat Gfeller",
      "Marco Tagliasacchi",
      "Dominik Roblek"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.02027"
  },
  {
    "id": "arXiv:2008.10086",
    "title": "Learning Models of Individual Behavior in Chess",
    "abstract": "Comments: 12 pages, 11 figures, 5 tables, Published in the Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2022), Code this https URL",
    "descriptor": "\nComments: 12 pages, 11 figures, 5 tables, Published in the Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2022), Code this https URL\n",
    "authors": [
      "Reid McIlroy-Young",
      "Russell Wang",
      "Siddhartha Sen",
      "Jon Kleinberg",
      "Ashton Anderson"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.10086"
  },
  {
    "id": "arXiv:2009.08535",
    "title": "Numerical Testing of a New Positivity-Preserving Interpolation Algorithm",
    "abstract": "Comments: 57 pages, 15 figures",
    "descriptor": "\nComments: 57 pages, 15 figures\n",
    "authors": [
      "T.A.J. Ouermi",
      "Robert M. Kirby",
      "Martin Berzins"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2009.08535"
  },
  {
    "id": "arXiv:2009.10749",
    "title": "Fourier Analysis-based Iterative Combinatorial Auctions",
    "abstract": "Fourier Analysis-based Iterative Combinatorial Auctions",
    "descriptor": "",
    "authors": [
      "Jakob Weissteiner",
      "Chris Wendler",
      "Sven Seuken",
      "Ben Lubin",
      "Markus P\u00fcschel"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2009.10749"
  },
  {
    "id": "arXiv:2010.15088",
    "title": "Finite-Time Convergence Rates of Decentralized Stochastic Approximation  with Applications in Multi-Agent and Multi-Task Learning",
    "abstract": "Finite-Time Convergence Rates of Decentralized Stochastic Approximation  with Applications in Multi-Agent and Multi-Task Learning",
    "descriptor": "",
    "authors": [
      "Sihan Zeng",
      "Thinh T. Doan",
      "Justin Romberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.15088"
  },
  {
    "id": "arXiv:2011.04740",
    "title": "Applying Machine Learning to Crowd-sourced Data from Earthquake  Detective",
    "abstract": "Comments: Updated version of the paper presented at AI for Earth Sciences Workshop, NeurIPS 2020",
    "descriptor": "\nComments: Updated version of the paper presented at AI for Earth Sciences Workshop, NeurIPS 2020\n",
    "authors": [
      "Omkar Ranadive",
      "Suzan van der Lee",
      "Vivian Tang",
      "Kevin Chao"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.04740"
  },
  {
    "id": "arXiv:2012.03476",
    "title": "NCGNN: Node-Level Capsule Graph Neural Network for Semisupervised  Classification",
    "abstract": "Comments: accepted by TNNLS",
    "descriptor": "\nComments: accepted by TNNLS\n",
    "authors": [
      "Rui Yang",
      "Wenrui Dai",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.03476"
  },
  {
    "id": "arXiv:2101.03328",
    "title": "Persistent Homology of Fractional Gaussian Noise",
    "abstract": "Comments: 17 pages, 12 figures, matched to the published version",
    "descriptor": "\nComments: 17 pages, 12 figures, matched to the published version\n",
    "authors": [
      "H. Masoomy",
      "B. Askari",
      "M. N. Najafi",
      "S. M. S. Movahed"
    ],
    "subjectives": [
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2101.03328"
  },
  {
    "id": "arXiv:2102.03401",
    "title": "Federated Learning on the Road: Autonomous Controller Design for  Connected and Autonomous Vehicles",
    "abstract": "Comments: 30 pages, 6 figures",
    "descriptor": "\nComments: 30 pages, 6 figures\n",
    "authors": [
      "Tengchan Zeng",
      "Omid Semiari",
      "Mingzhe Chen",
      "Walid Saad",
      "Mehdi Bennis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.03401"
  },
  {
    "id": "arXiv:2102.04811",
    "title": "Broader terms curriculum mapping: Using natural language processing and  visual-supported communication to create representative program planning  experiences",
    "abstract": "Comments: v3 removed date and undertitle",
    "descriptor": "\nComments: v3 removed date and undertitle\n",
    "authors": [
      "Rog\u00e9rio Duarte",
      "\u00c2ngela Lacerda Nobre",
      "Fernando Pimentel",
      "Marc Jacquinet"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.04811"
  },
  {
    "id": "arXiv:2102.10271",
    "title": "Meta-Learning Dynamics Forecasting Using Task Inference",
    "abstract": "Meta-Learning Dynamics Forecasting Using Task Inference",
    "descriptor": "",
    "authors": [
      "Rui Wang",
      "Robin Walters",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.10271"
  },
  {
    "id": "arXiv:2102.11686",
    "title": "New Characterizations of Strategy-Proofness under Single-Peakedness",
    "abstract": "New Characterizations of Strategy-Proofness under Single-Peakedness",
    "descriptor": "",
    "authors": [
      "Andrew Jennings",
      "Rida Laraki",
      "Clemens Puppe",
      "Estelle Varloot"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2102.11686"
  },
  {
    "id": "arXiv:2102.12528",
    "title": "Preserved central model for faster bidirectional compression in  distributed settings",
    "abstract": "Comments: Bidirectional compression for Federated Learning: 59 pages, 5 theorems, published at NeurIPS 2021",
    "descriptor": "\nComments: Bidirectional compression for Federated Learning: 59 pages, 5 theorems, published at NeurIPS 2021\n",
    "authors": [
      "Constantin Philippenko",
      "Aymeric Dieuleveut"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2102.12528"
  },
  {
    "id": "arXiv:2103.06261",
    "title": "A Tree-based Model Averaging Approach for Personalized Treatment Effect  Estimation from Heterogeneous Data Sources",
    "abstract": "Comments: Accepted at ICML 2022. Previously titled \"A Tree-based Federated Learning Approach for Personalized Treatment Effect Estimation from Heterogeneous Data Sources\"",
    "descriptor": "\nComments: Accepted at ICML 2022. Previously titled \"A Tree-based Federated Learning Approach for Personalized Treatment Effect Estimation from Heterogeneous Data Sources\"\n",
    "authors": [
      "Xiaoqing Tan",
      "Chung-Chou H. Chang",
      "Ling Zhou",
      "Lu Tang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2103.06261"
  },
  {
    "id": "arXiv:2104.06404",
    "title": "Pointly-Supervised Instance Segmentation",
    "abstract": "Comments: CVPR 2022, Oral. Project page: this https URL",
    "descriptor": "\nComments: CVPR 2022, Oral. Project page: this https URL\n",
    "authors": [
      "Bowen Cheng",
      "Omkar Parkhi",
      "Alexander Kirillov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.06404"
  },
  {
    "id": "arXiv:2104.07651",
    "title": "mlf-core: a framework for deterministic machine learning",
    "abstract": "Comments: this https URL and this https URL",
    "descriptor": "\nComments: this https URL and this https URL\n",
    "authors": [
      "Lukas Heumos",
      "Philipp Ehmele",
      "Luis Kuhn Cuellar",
      "Kevin Menden",
      "Edmund Miller",
      "Steffen Lemke",
      "Gisela Gabernet",
      "Sven Nahnsen"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.07651"
  },
  {
    "id": "arXiv:2105.05758",
    "title": "DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell  Morphology with Deep multiple instance learning",
    "abstract": "DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell  Morphology with Deep multiple instance learning",
    "descriptor": "",
    "authors": [
      "M.Sadegh Saberian",
      "Kathleen P. Moriarty",
      "Andrea D. Olmstead",
      "Christian Hallgrimson",
      "Fran\u00e7ois Jean",
      "Ivan R. Nabi",
      "Maxwell W. Libbrecht",
      "Ghassan Hamarneh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2105.05758"
  },
  {
    "id": "arXiv:2105.05987",
    "title": "Two Influence Maximization Games on Graphs Made Temporal",
    "abstract": "Comments: Accepted to IJCAI 2021",
    "descriptor": "\nComments: Accepted to IJCAI 2021\n",
    "authors": [
      "Niclas Boehmer",
      "Vincent Froese",
      "Julia Henkel",
      "Yvonne Lasars",
      "Rolf Niedermeier",
      "Malte Renken"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2105.05987"
  },
  {
    "id": "arXiv:2105.07372",
    "title": "An accelerated expectation-maximization algorithm for multi-reference  alignment",
    "abstract": "An accelerated expectation-maximization algorithm for multi-reference  alignment",
    "descriptor": "",
    "authors": [
      "Noam Janco",
      "Tamir Bendory"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.07372"
  },
  {
    "id": "arXiv:2106.03027",
    "title": "Model Zoo: A Growing \"Brain\" That Learns Continually",
    "abstract": "Model Zoo: A Growing \"Brain\" That Learns Continually",
    "descriptor": "",
    "authors": [
      "Rahul Ramesh",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03027"
  },
  {
    "id": "arXiv:2106.11825",
    "title": "Beyond 5G URLLC Evolution: New Service Modes and Practical  Considerations",
    "abstract": "Comments: The manuscript is undergoing extensive review",
    "descriptor": "\nComments: The manuscript is undergoing extensive review\n",
    "authors": [
      "Hirley Alves",
      "Gweon Do Jo",
      "JaeSheung Shin",
      "Choongil Yeh",
      "Nurul Huda Mahmood",
      "Carlos Lima",
      "Chanho Yoon",
      "Nandana Rahatheva",
      "Ok-Sun Park",
      "Seokki Kim",
      "Eunah Kim",
      "Ville Niemel\u00e4",
      "Hyeon Woo Lee",
      "Ari Pouttu",
      "Hyun Kyu Chung",
      "Matti Latva-aho"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.11825"
  },
  {
    "id": "arXiv:2106.13973",
    "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models",
    "abstract": "Comments: 4 pages, 3 tables, 1 figure",
    "descriptor": "\nComments: 4 pages, 3 tables, 1 figure\n",
    "authors": [
      "Priyam Basu",
      "Tiasa Singha Roy",
      "Rakshit Naidu",
      "Zumrut Muftuoglu",
      "Sahib Singh",
      "Fatemehsadat Mireshghallah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.13973"
  },
  {
    "id": "arXiv:2106.14587",
    "title": "Topos and Stacks of Deep Neural Networks",
    "abstract": "Comments: 151 pages, 12 figures",
    "descriptor": "\nComments: 151 pages, 12 figures\n",
    "authors": [
      "Jean-Claude Belfiore",
      "Daniel Bennequin"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.14587"
  },
  {
    "id": "arXiv:2106.14832",
    "title": "The role of reciprocity in human-robot social influence",
    "abstract": "Comments: Article published on iScience. See this https URL Cite as: Zonca, J., Folso, A., & Sciutti, A. (2021). The role of reciprocity in human-robot social influence. iScience, 24(12), 103424. 10.1016/j.isci.2021.103424",
    "descriptor": "\nComments: Article published on iScience. See this https URL Cite as: Zonca, J., Folso, A., & Sciutti, A. (2021). The role of reciprocity in human-robot social influence. iScience, 24(12), 103424. 10.1016/j.isci.2021.103424\n",
    "authors": [
      "Joshua Zonca",
      "Anna Folso",
      "Alessandra Sciutti"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.14832"
  },
  {
    "id": "arXiv:2107.00110",
    "title": "Classical Planning in Deep Latent Space",
    "abstract": "Comments: Accepted in Journal of Artificial Intelligence Research (JAIR)",
    "descriptor": "\nComments: Accepted in Journal of Artificial Intelligence Research (JAIR)\n",
    "authors": [
      "Masataro Asai",
      "Hiroshi Kajino",
      "Alex Fukunaga",
      "Christian Muise"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.00110"
  },
  {
    "id": "arXiv:2107.00472",
    "title": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets",
    "abstract": "Comments: 35 pages, 11 figures",
    "descriptor": "\nComments: 35 pages, 11 figures\n",
    "authors": [
      "Baojian Zhou",
      "Yifan Sun"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.00472"
  },
  {
    "id": "arXiv:2107.00679",
    "title": "Efficient Attribute-Based Smart Contract Access Control Enhanced by  Reputation Assessment",
    "abstract": "Comments: major revision needed",
    "descriptor": "\nComments: major revision needed\n",
    "authors": [
      "Yang Liu",
      "Terry Guo",
      "Zhe Chen",
      "Xueying Jiang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2107.00679"
  },
  {
    "id": "arXiv:2107.08943",
    "title": "OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set  Unlabeled Data",
    "abstract": "Comments: Code is available at this https URL",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Jongjin Park",
      "Sukmin Yun",
      "Jongheon Jeong",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.08943"
  },
  {
    "id": "arXiv:2107.11630",
    "title": "Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them",
    "abstract": "Comments: ICML 2022 (Long Talk)",
    "descriptor": "\nComments: ICML 2022 (Long Talk)\n",
    "authors": [
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.11630"
  },
  {
    "id": "arXiv:2107.13473",
    "title": "The Portiloop: a deep learning-based open science tool for closed-loop  brain stimulation",
    "abstract": "Comments: 10 pages, 6 Figures (+ Supplementary: 10 pages, 13 Figures), journal paper. Open source code at this https URL",
    "descriptor": "\nComments: 10 pages, 6 Figures (+ Supplementary: 10 pages, 13 Figures), journal paper. Open source code at this https URL\n",
    "authors": [
      "Nicolas Valenchon",
      "Yann Bouteiller",
      "Hugo R. Jourde",
      "Xavier L'Heureux",
      "Milo Sobral",
      "Emily B.J. Coffey",
      "Giovanni Beltrame"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2107.13473"
  },
  {
    "id": "arXiv:2108.02390",
    "title": "Fuzzy Logic Based Logical Query Answering on Knowledge Graphs",
    "abstract": "Comments: AAAI'2022",
    "descriptor": "\nComments: AAAI'2022\n",
    "authors": [
      "Xuelu Chen",
      "Ziniu Hu",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.02390"
  },
  {
    "id": "arXiv:2108.02888",
    "title": "Out-of-Domain Generalization from a Single Source: An Uncertainty  Quantification Approach",
    "abstract": "Comments: 13 pages, 11 figures, accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv admin note: substantial text overlap with arXiv:2003.13216",
    "descriptor": "\nComments: 13 pages, 11 figures, accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv admin note: substantial text overlap with arXiv:2003.13216\n",
    "authors": [
      "Xi Peng",
      "Fengchun Qiao",
      "Long Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.02888"
  },
  {
    "id": "arXiv:2109.01904",
    "title": "Estimating Categorical Counterfactuals via Deep Twin Networks",
    "abstract": "Comments: 8 pages + appendix",
    "descriptor": "\nComments: 8 pages + appendix\n",
    "authors": [
      "Athanasios Vlontzos",
      "Bernhard Kainz",
      "Ciaran M. Gilligan-Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.01904"
  },
  {
    "id": "arXiv:2109.02490",
    "title": "Learning Interpretable Representations of Entanglement in Quantum Optics  Experiments using Deep Generative Models",
    "abstract": "Comments: Published in Nature Machine Intelligence this https URL",
    "descriptor": "\nComments: Published in Nature Machine Intelligence this https URL\n",
    "authors": [
      "Daniel Flam-Shepherd",
      "Tony Wu",
      "Xuemei Gu",
      "Alba Cervera-Lierta",
      "Mario Krenn",
      "Alan Aspuru-Guzik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2109.02490"
  },
  {
    "id": "arXiv:2109.03867",
    "title": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "abstract": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "descriptor": "",
    "authors": [
      "Emanuele Sansone"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.03867"
  },
  {
    "id": "arXiv:2109.04033",
    "title": "New Versions of Gradient Temporal Difference Learning",
    "abstract": "New Versions of Gradient Temporal Difference Learning",
    "descriptor": "",
    "authors": [
      "Donghwan Lee",
      "Han-Dong Lim",
      "Jihoon Park",
      "Okyong Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.04033"
  },
  {
    "id": "arXiv:2109.09818",
    "title": "Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context  of Melanoma Classification",
    "abstract": "Comments: 9 pages, accepted to ICML 2022",
    "descriptor": "\nComments: 9 pages, accepted to ICML 2022\n",
    "authors": [
      "Peter J. Bevan",
      "Amir Atapour-Abarghouei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.09818"
  },
  {
    "id": "arXiv:2109.09960",
    "title": "Mutual Consistency Learning for Semi-supervised Medical Image  Segmentation",
    "abstract": "Mutual Consistency Learning for Semi-supervised Medical Image  Segmentation",
    "descriptor": "",
    "authors": [
      "Yicheng Wu",
      "Zongyuan Ge",
      "Donghao Zhang",
      "Minfeng Xu",
      "Lei Zhang",
      "Yong Xia",
      "Jianfei Cai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.09960"
  },
  {
    "id": "arXiv:2109.10949",
    "title": "Recursive Feasibility Guided Optimal Parameter Adaptation of  Differential Convex Optimization Policies for Safety-Critical Systems",
    "abstract": "Comments: This paper was accepted to and presented at ICRA 2022. Details will be out soon",
    "descriptor": "\nComments: This paper was accepted to and presented at ICRA 2022. Details will be out soon\n",
    "authors": [
      "Hardik Parwana",
      "Dimitra Panagou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.10949"
  },
  {
    "id": "arXiv:2109.10964",
    "title": "Multi-Objective Bayesian Optimization over High-Dimensional Search  Spaces",
    "abstract": "Comments: To appear at UAI 2022. 24 pages",
    "descriptor": "\nComments: To appear at UAI 2022. 24 pages\n",
    "authors": [
      "Samuel Daulton",
      "David Eriksson",
      "Maximilian Balandat",
      "Eytan Bakshy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.10964"
  },
  {
    "id": "arXiv:2109.15117",
    "title": "Monotone-Value Neural Networks: Exploiting Preference Monotonicity in  Combinatorial Assignment",
    "abstract": "Monotone-Value Neural Networks: Exploiting Preference Monotonicity in  Combinatorial Assignment",
    "descriptor": "",
    "authors": [
      "Jakob Weissteiner",
      "Jakob Heiss",
      "Julien Siems",
      "Sven Seuken"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2109.15117"
  },
  {
    "id": "arXiv:2110.01359",
    "title": "CENN: Conservative energy method based on neural networks with  subdomains for solving variational problems involving heterogeneous and  complex geometries",
    "abstract": "Comments: 38 pages, 22 figures, 1 graphical abstract",
    "descriptor": "\nComments: 38 pages, 22 figures, 1 graphical abstract\n",
    "authors": [
      "Yizheng Wang",
      "Jia Sun",
      "Wei Li",
      "Zaiyuan Lu",
      "Yinghua Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01359"
  },
  {
    "id": "arXiv:2110.03210",
    "title": "Universality of Winning Tickets: A Renormalization Group Perspective",
    "abstract": "Comments: 16 pages, 3 figures, 8 tables",
    "descriptor": "\nComments: 16 pages, 3 figures, 8 tables\n",
    "authors": [
      "William T. Redman",
      "Tianlong Chen",
      "Zhangyang Wang",
      "Akshunna S. Dogra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ],
    "url": "https://arxiv.org/abs/2110.03210"
  },
  {
    "id": "arXiv:2110.05722",
    "title": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs",
    "abstract": "Comments: 13 pages, 22 figures, accepted by SC 22",
    "descriptor": "\nComments: 13 pages, 22 figures, accepted by SC 22\n",
    "authors": [
      "Xiaohui Wang",
      "Yang Wei",
      "Ying Xiong",
      "Guyue Huang",
      "Xian Qian",
      "Yufei Ding",
      "Mingxuan Wang",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2110.05722"
  },
  {
    "id": "arXiv:2110.06892",
    "title": "TAG: Toward Accurate Social Media Content Tagging with a Concept Graph",
    "abstract": "Comments: Accepted by ACM SIGKDD 2022",
    "descriptor": "\nComments: Accepted by ACM SIGKDD 2022\n",
    "authors": [
      "Jiuding Yang",
      "Weidong Guo",
      "Bang Liu",
      "Yakun Yu",
      "Chaoyue Wang",
      "Jinwen Luo",
      "Linglong Kong",
      "Di Niu",
      "Zhen Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06892"
  },
  {
    "id": "arXiv:2110.06904",
    "title": "Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks",
    "abstract": "Comments: 18 pages",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Shawn Shan",
      "Arjun Nitin Bhagoji",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06904"
  },
  {
    "id": "arXiv:2110.07798",
    "title": "Dynamics of Cross-Platform Attention to Retracted Papers",
    "abstract": "Dynamics of Cross-Platform Attention to Retracted Papers",
    "descriptor": "",
    "authors": [
      "Hao Peng",
      "Daniel M. Romero",
      "Em\u0151ke-\u00c1gnes Horv\u00e1t"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Digital Libraries (cs.DL)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07798"
  },
  {
    "id": "arXiv:2110.08449",
    "title": "Adversarial Attacks on Gaussian Process Bandits",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Eric Han",
      "Jonathan Scarlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08449"
  },
  {
    "id": "arXiv:2110.09993",
    "title": "A Unified and Refined Convergence Analysis for Non-Convex Decentralized  Learning",
    "abstract": "A Unified and Refined Convergence Analysis for Non-Convex Decentralized  Learning",
    "descriptor": "",
    "authors": [
      "Sulaiman A. Alghunaim",
      "Kun Yuan"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.09993"
  },
  {
    "id": "arXiv:2110.10498",
    "title": "Differential Privacy in Multi-Party Resource Sharing",
    "abstract": "Differential Privacy in Multi-Party Resource Sharing",
    "descriptor": "",
    "authors": [
      "Utku Karaca",
      "S. Ilker Birbil",
      "Sinan Yildirim",
      "Nursen Aydin",
      "Gizem Mullaoglu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.10498"
  },
  {
    "id": "arXiv:2110.13939",
    "title": "CausalAF: Causal Autoregressive Flow for Safety-Critical Driving  Scenario Generation",
    "abstract": "Comments: 11 pages, under review",
    "descriptor": "\nComments: 11 pages, under review\n",
    "authors": [
      "Wenhao Ding",
      "Haohong Lin",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13939"
  },
  {
    "id": "arXiv:2110.14868",
    "title": "An Asymptotic Test for Conditional Independence using Analytic Kernel  Embeddings",
    "abstract": "An Asymptotic Test for Conditional Independence using Analytic Kernel  Embeddings",
    "descriptor": "",
    "authors": [
      "Meyer Scetbon",
      "Laurent Meunier",
      "Yaniv Romano"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14868"
  },
  {
    "id": "arXiv:2111.02212",
    "title": "A swarm intelligence-based robust solution for Virtual Reference  Feedback Tuning",
    "abstract": "Comments: 33 pages, 9 figures, journal",
    "descriptor": "\nComments: 33 pages, 9 figures, journal\n",
    "authors": [
      "L. V. Fiorio",
      "C. L. Remes",
      "Y. R. de Novaes"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2111.02212"
  },
  {
    "id": "arXiv:2111.02363",
    "title": "Deep Learning-based Non-Intrusive Multi-Objective Speech Assessment  Model with Cross-Domain Features",
    "abstract": "Deep Learning-based Non-Intrusive Multi-Objective Speech Assessment  Model with Cross-Domain Features",
    "descriptor": "",
    "authors": [
      "Ryandhimas E. Zezario",
      "Szu-Wei Fu",
      "Fei Chen",
      "Chiou-Shann Fuh",
      "Hsin-Min Wang",
      "Yu Tsao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2111.02363"
  },
  {
    "id": "arXiv:2111.02926",
    "title": "OpenFWI: Large-Scale Multi-Structural Benchmark Datasets for Seismic  Full Waveform Inversion",
    "abstract": "OpenFWI: Large-Scale Multi-Structural Benchmark Datasets for Seismic  Full Waveform Inversion",
    "descriptor": "",
    "authors": [
      "Chengyuan Deng",
      "Shihang Feng",
      "Hanchen Wang",
      "Xitong Zhang",
      "Peng Jin",
      "Yinan Feng",
      "Qili Zeng",
      "Yinpeng Chen",
      "Youzuo Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2111.02926"
  },
  {
    "id": "arXiv:2111.05486",
    "title": "Multi-Agent Learning for Iterative Dominance Elimination: Formal  Barriers and New Algorithms",
    "abstract": "Multi-Agent Learning for Iterative Dominance Elimination: Formal  Barriers and New Algorithms",
    "descriptor": "",
    "authors": [
      "Jibang Wu",
      "Haifeng Xu",
      "Fan Yao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2111.05486"
  },
  {
    "id": "arXiv:2111.06784",
    "title": "A Minimax Learning Approach to Off-Policy Evaluation in Confounded  Partially Observable Markov Decision Processes",
    "abstract": "A Minimax Learning Approach to Off-Policy Evaluation in Confounded  Partially Observable Markov Decision Processes",
    "descriptor": "",
    "authors": [
      "Chengchun Shi",
      "Masatoshi Uehara",
      "Jiawei Huang",
      "Nan Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.06784"
  },
  {
    "id": "arXiv:2111.08005",
    "title": "Solving Inverse Problems in Medical Imaging with Score-Based Generative  Models",
    "abstract": "Comments: Published at ICLR 2022",
    "descriptor": "\nComments: Published at ICLR 2022\n",
    "authors": [
      "Yang Song",
      "Liyue Shen",
      "Lei Xing",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.08005"
  },
  {
    "id": "arXiv:2111.09298",
    "title": "SeCGAN: Parallel Conditional Generative Adversarial Networks for Face  Editing via Semantic Consistency",
    "abstract": "Comments: Accepted by AI for Content Creation (AI4CC) workshop at CVPR 2022",
    "descriptor": "\nComments: Accepted by AI for Content Creation (AI4CC) workshop at CVPR 2022\n",
    "authors": [
      "Jiaze Sun",
      "Binod Bhattarai",
      "Zhixiang Chen",
      "Tae-Kyun Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.09298"
  },
  {
    "id": "arXiv:2111.09561",
    "title": "Adversarial attacks on voter model dynamics in complex networks",
    "abstract": "Comments: 7 pages, 5 figures",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Katsumi Chiyomaru",
      "Kazuhiro Takemoto"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2111.09561"
  },
  {
    "id": "arXiv:2111.14946",
    "title": "Verifying Transactional Consistency of MongoDB",
    "abstract": "Comments: v0.2, update with proof of correctness. 17 pages(16 pages excluding reference), 8 algorithms, 5 tables and 2 figures",
    "descriptor": "\nComments: v0.2, update with proof of correctness. 17 pages(16 pages excluding reference), 8 algorithms, 5 tables and 2 figures\n",
    "authors": [
      "Hongrong Ouyang",
      "Hengfeng Wei",
      "Yu Huang",
      "Haixiang Li",
      "Anqun Pan"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2111.14946"
  },
  {
    "id": "arXiv:2112.01527",
    "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
    "abstract": "Comments: CVPR 2022. Project page/code/models: this https URL",
    "descriptor": "\nComments: CVPR 2022. Project page/code/models: this https URL\n",
    "authors": [
      "Bowen Cheng",
      "Ishan Misra",
      "Alexander G. Schwing",
      "Alexander Kirillov",
      "Rohit Girdhar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.01527"
  },
  {
    "id": "arXiv:2112.02573",
    "title": "Generalized hybrid momentum maps and reduction by symmetries of forced  mechanical systems with inelastic collisions",
    "abstract": "Comments: Preprint submitted to a Journal. Comments Welcome!",
    "descriptor": "\nComments: Preprint submitted to a Journal. Comments Welcome!\n",
    "authors": [
      "Leonardo J. Colombo",
      "Manuel de Le\u00f3n",
      "Mar\u00eda Emma Eyrea Iraz\u00fa",
      "Asier L\u00f3pez-Gord\u00f3n"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Symplectic Geometry (math.SG)"
    ],
    "url": "https://arxiv.org/abs/2112.02573"
  },
  {
    "id": "arXiv:2112.03626",
    "title": "Phase transitions in nonparametric regressions: a curse of exploiting  higher degree smoothness assumptions in finite samples",
    "abstract": "Comments: 4 Tables",
    "descriptor": "\nComments: 4 Tables\n",
    "authors": [
      "Ying Zhu"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.03626"
  },
  {
    "id": "arXiv:2112.03750",
    "title": "Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth  with RGB Fusion in Challenging Environments",
    "abstract": "Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth  with RGB Fusion in Challenging Environments",
    "descriptor": "",
    "authors": [
      "HyunJun Jung",
      "Nikolas Brasch",
      "Ales Leonardis",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.03750"
  },
  {
    "id": "arXiv:2112.04145",
    "title": "A Review for Deep Reinforcement Learning in Atari:Benchmarks,  Challenges, and Solutions",
    "abstract": "Comments: preliminary work, preprint",
    "descriptor": "\nComments: preliminary work, preprint\n",
    "authors": [
      "Jiajun Fan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.04145"
  },
  {
    "id": "arXiv:2112.06362",
    "title": "Scheduling Servers with Stochastic Bilinear Rewards",
    "abstract": "Scheduling Servers with Stochastic Bilinear Rewards",
    "descriptor": "",
    "authors": [
      "Jung-hun Kim",
      "Milan Vojnovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2112.06362"
  },
  {
    "id": "arXiv:2112.07344",
    "title": "SCORE: Approximating Curvature Information under Self-Concordant  Regularization",
    "abstract": "Comments: 21 pages, 12 figures",
    "descriptor": "\nComments: 21 pages, 12 figures\n",
    "authors": [
      "Adeyemi D. Adeoye",
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2112.07344"
  },
  {
    "id": "arXiv:2112.07794",
    "title": "Review of Factor Graphs for Robust GNSS Applications",
    "abstract": "Review of Factor Graphs for Robust GNSS Applications",
    "descriptor": "",
    "authors": [
      "Shounak Das",
      "Ryan Watson",
      "Jason Gross"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2112.07794"
  },
  {
    "id": "arXiv:2112.08693",
    "title": "Helmholtz equation and non-singular boundary elements applied to  multi-disciplinary physical problems",
    "abstract": "Helmholtz equation and non-singular boundary elements applied to  multi-disciplinary physical problems",
    "descriptor": "",
    "authors": [
      "Evert Klaseboer",
      "Qiang Sun"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Classical Physics (physics.class-ph)",
      "Computational Physics (physics.comp-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2112.08693"
  },
  {
    "id": "arXiv:2112.09822",
    "title": "Multimeasurement Generative Models",
    "abstract": "Comments: Our code is publicly available at this https URL",
    "descriptor": "\nComments: Our code is publicly available at this https URL\n",
    "authors": [
      "Saeed Saremi",
      "Rupesh Kumar Srivastava"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.09822"
  },
  {
    "id": "arXiv:2112.11790",
    "title": "BEVDet: High-performance Multi-camera 3D Object Detection in  Bird-Eye-View",
    "abstract": "Comments: Multi-camera 3D Object Detection",
    "descriptor": "\nComments: Multi-camera 3D Object Detection\n",
    "authors": [
      "Junjie Huang",
      "Guan Huang",
      "Zheng Zhu",
      "Yun Ye",
      "Dalong Du"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.11790"
  },
  {
    "id": "arXiv:2112.13720",
    "title": "Computationally Efficient Approximations for Matrix-based Renyi's  Entropy",
    "abstract": "Computationally Efficient Approximations for Matrix-based Renyi's  Entropy",
    "descriptor": "",
    "authors": [
      "Tieliang Gong",
      "Yuxin Dong",
      "Shujian Yu",
      "Bo Dong"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.13720"
  },
  {
    "id": "arXiv:2201.00378",
    "title": "Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring  Platforms",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Pau Ferrer-Cid",
      "Jose M. Barcelo-Ordinas",
      "Jorge Garcia-Vidal"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2201.00378"
  },
  {
    "id": "arXiv:2201.02115",
    "title": "The dynamics of representation learning in shallow, non-linear  autoencoders",
    "abstract": "The dynamics of representation learning in shallow, non-linear  autoencoders",
    "descriptor": "",
    "authors": [
      "Maria Refinetti",
      "Sebastian Goldt"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.02115"
  },
  {
    "id": "arXiv:2201.02135",
    "title": "Deep Reinforcement Learning, a textbook",
    "abstract": "Deep Reinforcement Learning, a textbook",
    "descriptor": "",
    "authors": [
      "Aske Plaat"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.02135"
  },
  {
    "id": "arXiv:2201.03016",
    "title": "Learning from Synthetic InSAR with Vision Transformers: The case of  volcanic unrest detection",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication\n",
    "authors": [
      "Nikolaos Ioannis Bountos",
      "Dimitrios Michail",
      "Ioannis Papoutsis"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.03016"
  },
  {
    "id": "arXiv:2201.05077",
    "title": "Black-box Safety Analysis and Retraining of DNNs based on Feature  Extraction and Clustering",
    "abstract": "Comments: 41 pages, 12 figures, 15 tables",
    "descriptor": "\nComments: 41 pages, 12 figures, 15 tables\n",
    "authors": [
      "Mohammed Oualid Attaoui",
      "Hazem Fahmy",
      "Fabrizio Pastore",
      "Lionel Briand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.05077"
  },
  {
    "id": "arXiv:2201.05544",
    "title": "BandMaxSAT: A Local Search MaxSAT Solver with Multi-armed Bandit",
    "abstract": "Comments: Accepted by IJCAI 2022",
    "descriptor": "\nComments: Accepted by IJCAI 2022\n",
    "authors": [
      "Jiongzhi Zheng",
      "Kun He",
      "Jianrong Zhou",
      "Yan Jin",
      "Chu-Min Li",
      "Felip Manya"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.05544"
  },
  {
    "id": "arXiv:2201.06346",
    "title": "Can We Find Neurons that Cause Unrealistic Images in Deep Generative  Networks?",
    "abstract": "Comments: Accepted at IJCAI-2022",
    "descriptor": "\nComments: Accepted at IJCAI-2022\n",
    "authors": [
      "Hwanil Choi",
      "Wonjoon Chang",
      "Jaesik Choi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.06346"
  },
  {
    "id": "arXiv:2201.07296",
    "title": "Convergence of Policy Gradient for Entropy Regularized MDPs with Neural  Network Approximation in the Mean-Field Regime",
    "abstract": "Convergence of Policy Gradient for Entropy Regularized MDPs with Neural  Network Approximation in the Mean-Field Regime",
    "descriptor": "",
    "authors": [
      "Bekzhan Kerimkulov",
      "James-Michael Leahy",
      "David \u0160i\u0161ka",
      "Lukasz Szpruch"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.07296"
  },
  {
    "id": "arXiv:2201.07386",
    "title": "An Optimization Framework for General Rate Splitting for General  Multicast",
    "abstract": "Comments: 30 pages, 9 figures, submitted to Trans. Wireless Commun, and under minor revision",
    "descriptor": "\nComments: 30 pages, 9 figures, submitted to Trans. Wireless Commun, and under minor revision\n",
    "authors": [
      "Lingzhi Zhao",
      "Ying Cui",
      "Sheng Yang",
      "Shlomo Shamai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2201.07386"
  },
  {
    "id": "arXiv:2201.11969",
    "title": "Approximately Equivariant Networks for Imperfectly Symmetric Dynamics",
    "abstract": "Approximately Equivariant Networks for Imperfectly Symmetric Dynamics",
    "descriptor": "",
    "authors": [
      "Rui Wang",
      "Robin Walters",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11969"
  },
  {
    "id": "arXiv:2201.12018",
    "title": "Transfer Learning In Differential Privacy's Hybrid-Model",
    "abstract": "Transfer Learning In Differential Privacy's Hybrid-Model",
    "descriptor": "",
    "authors": [
      "Refael Kohen",
      "Or Sheffet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2201.12018"
  },
  {
    "id": "arXiv:2201.12740",
    "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term  Series Forecasting",
    "abstract": "Comments: Accepted by The 39th International Conference on Machine Learning (ICML 2022)",
    "descriptor": "\nComments: Accepted by The 39th International Conference on Machine Learning (ICML 2022)\n",
    "authors": [
      "Tian Zhou",
      "Ziqing Ma",
      "Qingsong Wen",
      "Xue Wang",
      "Liang Sun",
      "Rong Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.12740"
  },
  {
    "id": "arXiv:2201.12987",
    "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention  Mechanism",
    "abstract": "Comments: Accepted to ICML 2022",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Siqi Miao",
      "Miaoyuan Liu",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12987"
  },
  {
    "id": "arXiv:2201.13117",
    "title": "Continual Repeated Annealed Flow Transport Monte Carlo",
    "abstract": "Comments: 21 pages, 6 figures Published at International Conference on Machine Learning (ICML) 2022",
    "descriptor": "\nComments: 21 pages, 6 figures Published at International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Alexander G. D. G. Matthews",
      "Michael Arbel",
      "Danilo J. Rezende",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Lattice (hep-lat)"
    ],
    "url": "https://arxiv.org/abs/2201.13117"
  },
  {
    "id": "arXiv:2202.00187",
    "title": "Deep Reference Priors: What is the best way to pretrain a model?",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Yansong Gao",
      "Rahul Ramesh",
      "Pratik Chaudhari"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.00187"
  },
  {
    "id": "arXiv:2202.00240",
    "title": "A Criterion for Decoding on the BSC",
    "abstract": "Comments: Removed the section on Krawtchouk polynomials, as these results were already known. Added references for Krawtchouk polynomials and for Proposition 20, and reorganized some parts of the paper",
    "descriptor": "\nComments: Removed the section on Krawtchouk polynomials, as these results were already known. Added references for Krawtchouk polynomials and for Proposition 20, and reorganized some parts of the paper\n",
    "authors": [
      "Anup Rao",
      "Oscar Sprumont"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2202.00240"
  },
  {
    "id": "arXiv:2202.02794",
    "title": "Active Learning on a Budget: Opposite Strategies Suit High and Low  Budgets",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Guy Hacohen",
      "Avihu Dekel",
      "Daphna Weinshall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02794"
  },
  {
    "id": "arXiv:2202.02916",
    "title": "Dataset Condensation with Contrastive Signals",
    "abstract": "Comments: Accepted at ICML 2022",
    "descriptor": "\nComments: Accepted at ICML 2022\n",
    "authors": [
      "Saehyung Lee",
      "Sanghyuk Chun",
      "Sangwon Jung",
      "Sangdoo Yun",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.02916"
  },
  {
    "id": "arXiv:2202.03093",
    "title": "Effective Variable Depth Local Search for the Budgeted Maximum Coverage  Problem",
    "abstract": "Effective Variable Depth Local Search for the Budgeted Maximum Coverage  Problem",
    "descriptor": "",
    "authors": [
      "Jianrong Zhou",
      "Jiongzhi Zheng",
      "Kun He"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2202.03093"
  },
  {
    "id": "arXiv:2202.04985",
    "title": "Generalization Bounds via Convex Analysis",
    "abstract": "Generalization Bounds via Convex Analysis",
    "descriptor": "",
    "authors": [
      "Gergely Neu",
      "G\u00e1bor Lugosi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04985"
  },
  {
    "id": "arXiv:2202.06207",
    "title": "Performance of Downlink and Uplink Integrated Sensing and Communications  (ISAC) Systems",
    "abstract": "Comments: 8 figures",
    "descriptor": "\nComments: 8 figures\n",
    "authors": [
      "Chongjun Ouyang",
      "Yuanwei Liu",
      "Hongwen Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.06207"
  },
  {
    "id": "arXiv:2202.06258",
    "title": "Flowformer: Linearizing Transformers with Conservation Flows",
    "abstract": "Flowformer: Linearizing Transformers with Conservation Flows",
    "descriptor": "",
    "authors": [
      "Haixu Wu",
      "Jialong Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.06258"
  },
  {
    "id": "arXiv:2202.06317",
    "title": "Off-Policy Evaluation for Large Action Spaces via Embeddings",
    "abstract": "Comments: accepted at ICML2022",
    "descriptor": "\nComments: accepted at ICML2022\n",
    "authors": [
      "Yuta Saito",
      "Thorsten Joachims"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.06317"
  },
  {
    "id": "arXiv:2202.08835",
    "title": "General Cyclical Training of Neural Networks",
    "abstract": "Comments: Position paper",
    "descriptor": "\nComments: Position paper\n",
    "authors": [
      "Leslie N. Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.08835"
  },
  {
    "id": "arXiv:2202.08978",
    "title": "Cyclical Focal Loss",
    "abstract": "Cyclical Focal Loss",
    "descriptor": "",
    "authors": [
      "Leslie N. Smith"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08978"
  },
  {
    "id": "arXiv:2202.09792",
    "title": "Hierarchical Interpretation of Neural Text Classification",
    "abstract": "Comments: Under review of Computational Linguistics",
    "descriptor": "\nComments: Under review of Computational Linguistics\n",
    "authors": [
      "Hanqi Yan",
      "Lin Gui",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2202.09792"
  },
  {
    "id": "arXiv:2202.09889",
    "title": "Memorize to Generalize: on the Necessity of Interpolation in High  Dimensional Linear Regression",
    "abstract": "Comments: 32 pages; accepted to the 35th Annual Conference on Learning Theory (COLT) 2022",
    "descriptor": "\nComments: 32 pages; accepted to the 35th Annual Conference on Learning Theory (COLT) 2022\n",
    "authors": [
      "Chen Cheng",
      "John Duchi",
      "Rohith Kuditipudi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2202.09889"
  },
  {
    "id": "arXiv:2202.10103",
    "title": "Robustness and Accuracy Could Be Reconcilable by (Proper) Definition",
    "abstract": "Comments: ICML 2022",
    "descriptor": "\nComments: ICML 2022\n",
    "authors": [
      "Tianyu Pang",
      "Min Lin",
      "Xiao Yang",
      "Jun Zhu",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.10103"
  },
  {
    "id": "arXiv:2202.10994",
    "title": "An accelerated proximal gradient method for multiobjective optimization",
    "abstract": "An accelerated proximal gradient method for multiobjective optimization",
    "descriptor": "",
    "authors": [
      "Hiroki Tanabe",
      "Ellen H. Fukuda",
      "Nobuo Yamashita"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.10994"
  },
  {
    "id": "arXiv:2202.12230",
    "title": "Sample Efficiency of Data Augmentation Consistency Regularization",
    "abstract": "Sample Efficiency of Data Augmentation Consistency Regularization",
    "descriptor": "",
    "authors": [
      "Shuo Yang",
      "Yijun Dong",
      "Rachel Ward",
      "Inderjit S. Dhillon",
      "Sujay Sanghavi",
      "Qi Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12230"
  },
  {
    "id": "arXiv:2202.12636",
    "title": "Learning Multi-Task Gaussian Process Over Heterogeneous Input Domains",
    "abstract": "Comments: 12 pages, 9 figures, 4 tables, preprint under review",
    "descriptor": "\nComments: 12 pages, 9 figures, 4 tables, preprint under review\n",
    "authors": [
      "Haitao Liu",
      "Kai Wu",
      "Yew-Soon Ong",
      "Chao Bian",
      "Xiaomo Jiang",
      "Xiaofang Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12636"
  },
  {
    "id": "arXiv:2202.13110",
    "title": "Optimal-er Auctions through Attention",
    "abstract": "Optimal-er Auctions through Attention",
    "descriptor": "",
    "authors": [
      "Dmitry Ivanov",
      "Iskander Safiulin",
      "Igor Filippov",
      "Ksenia Balabaeva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2202.13110"
  },
  {
    "id": "arXiv:2203.00573",
    "title": "Contrasting random and learned features in deep Bayesian linear  regression",
    "abstract": "Comments: 35 pages, 7 figures. v2: minor typos corrected and references added; published in PRE",
    "descriptor": "\nComments: 35 pages, 7 figures. v2: minor typos corrected and references added; published in PRE\n",
    "authors": [
      "Jacob A. Zavatone-Veth",
      "William L. Tong",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.00573"
  },
  {
    "id": "arXiv:2203.01255",
    "title": "Low-Degree Multicalibration",
    "abstract": "Comments: Appears at COLT'22",
    "descriptor": "\nComments: Appears at COLT'22\n",
    "authors": [
      "Parikshit Gopalan",
      "Michael P. Kim",
      "Mihir Singhal",
      "Shengjia Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2203.01255"
  },
  {
    "id": "arXiv:2203.01324",
    "title": "Exploring Smoothness and Class-Separation for Semi-supervised Medical  Image Segmentation",
    "abstract": "Comments: Accepted by MICCAI 2022",
    "descriptor": "\nComments: Accepted by MICCAI 2022\n",
    "authors": [
      "Yicheng Wu",
      "Zhonghua Wu",
      "Qianyi Wu",
      "Zongyuan Ge",
      "Jianfei Cai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.01324"
  },
  {
    "id": "arXiv:2203.02149",
    "title": "HDNet: High-resolution Dual-domain Learning for Spectral Compressive  Imaging",
    "abstract": "Comments: CVPR 2022",
    "descriptor": "\nComments: CVPR 2022\n",
    "authors": [
      "Xiaowan Hu",
      "Yuanhao Cai",
      "Jing Lin",
      "Haoqian Wang",
      "Xin Yuan",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.02149"
  },
  {
    "id": "arXiv:2203.02654",
    "title": "A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation  Protocol for Segment-level Video Copy Detection",
    "abstract": "Comments: Accepted by CVPR 2022. Codes are all publicly available at this https URL",
    "descriptor": "\nComments: Accepted by CVPR 2022. Codes are all publicly available at this https URL\n",
    "authors": [
      "Sifeng He",
      "Xudong Yang",
      "Chen Jiang",
      "Gang Liang",
      "Wei Zhang",
      "Tan Pan",
      "Qing Wang",
      "Furong Xu",
      "Chunguang Li",
      "Jingxiong Liu",
      "Hui Xu",
      "Kaiming Huang",
      "Yuan Cheng",
      "Feng Qian",
      "Xiaobo Zhang",
      "Lei Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.02654"
  },
  {
    "id": "arXiv:2203.04845",
    "title": "Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction",
    "abstract": "Comments: A novel state-of-the-art Transformer-based method for hyperspectral image reconstruction",
    "descriptor": "\nComments: A novel state-of-the-art Transformer-based method for hyperspectral image reconstruction\n",
    "authors": [
      "Jing Lin",
      "Yuanhao Cai",
      "Xiaowan Hu",
      "Haoqian Wang",
      "Xin Yuan",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04845"
  },
  {
    "id": "arXiv:2203.04854",
    "title": "High order corrected trapezoidal rules for a class of singular integrals",
    "abstract": "Comments: 38 pages, 8 figures, 2 appendices",
    "descriptor": "\nComments: 38 pages, 8 figures, 2 appendices\n",
    "authors": [
      "Federico Izzo",
      "Olof Runborg",
      "Richard Tsai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2203.04854"
  },
  {
    "id": "arXiv:2203.05383",
    "title": "KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset  of Stuttering",
    "abstract": "Comments: Accepted at LREC 2022 Conference on Language Resources and Evaluation",
    "descriptor": "\nComments: Accepted at LREC 2022 Conference on Language Resources and Evaluation\n",
    "authors": [
      "Sebastian P. Bayerl",
      "Alexander Wolff von Gudenberg",
      "Florian H\u00f6nig",
      "Elmar N\u00f6th",
      "Korbinian Riedhammer"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.05383"
  },
  {
    "id": "arXiv:2203.08957",
    "title": "Risk-Averse No-Regret Learning in Online Convex Games",
    "abstract": "Risk-Averse No-Regret Learning in Online Convex Games",
    "descriptor": "",
    "authors": [
      "Zifan Wang",
      "Yi Shen",
      "Michael M. Zavlanos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.08957"
  },
  {
    "id": "arXiv:2203.09103",
    "title": "Knowledge Graph-Enabled Text-Based Automatic Personality Prediction",
    "abstract": "Comments: This is a preprint of an article published in \"Computational Intelligence and Neuroscience\"",
    "descriptor": "\nComments: This is a preprint of an article published in \"Computational Intelligence and Neuroscience\"\n",
    "authors": [
      "Majid Ramezani",
      "Mohammad-Reza Feizi-Derakhshi",
      "Mohammad-Ali Balafar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.09103"
  },
  {
    "id": "arXiv:2203.09948",
    "title": "Neural Enhanced Belief Propagation for Data Association in Multiobject  Tracking",
    "abstract": "Neural Enhanced Belief Propagation for Data Association in Multiobject  Tracking",
    "descriptor": "",
    "authors": [
      "Mingchao Liang",
      "Florian Meyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.09948"
  },
  {
    "id": "arXiv:2203.10180",
    "title": "Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous  Precision Drone Landing with a Gimbal-Mounted Camera",
    "abstract": "Comments: License change to non-commercial",
    "descriptor": "\nComments: License change to non-commercial\n",
    "authors": [
      "Joshua Springer",
      "Marcel Kyas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.10180"
  },
  {
    "id": "arXiv:2203.10384",
    "title": "Data Smells: Categories, Causes and Consequences, and Detection of  Suspicious Data in AI-based Systems",
    "abstract": "Data Smells: Categories, Causes and Consequences, and Detection of  Suspicious Data in AI-based Systems",
    "descriptor": "",
    "authors": [
      "Harald Foidl",
      "Michael Felderer",
      "Rudolf Ramler"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.10384"
  },
  {
    "id": "arXiv:2203.10916",
    "title": "DBSOP: An Efficient Heuristic for Speedy MCMC Sampling on Polytopes",
    "abstract": "Comments: 13 pages, typos corrected, references added",
    "descriptor": "\nComments: 13 pages, typos corrected, references added\n",
    "authors": [
      "Christos Karras",
      "Aristeidis Karras"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2203.10916"
  },
  {
    "id": "arXiv:2203.12587",
    "title": "Bubble Prediction of Non-Fungible Tokens (NFTs): An Empirical  Investigation",
    "abstract": "Bubble Prediction of Non-Fungible Tokens (NFTs): An Empirical  Investigation",
    "descriptor": "",
    "authors": [
      "Kensuke Ito",
      "Kyohei Shibano",
      "Gento Mogi"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.12587"
  },
  {
    "id": "arXiv:2203.12922",
    "title": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of  Stationary Policies",
    "abstract": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of  Stationary Policies",
    "descriptor": "",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12922"
  },
  {
    "id": "arXiv:2203.13574",
    "title": "Embedding Recurrent Layers with Dual-Path Strategy in a Variant of  Convolutional Network for Speaker-Independent Speech Separation",
    "abstract": "Comments: Accepted by Interspeech 2022",
    "descriptor": "\nComments: Accepted by Interspeech 2022\n",
    "authors": [
      "Xue Yang",
      "Changchun Bao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2203.13574"
  },
  {
    "id": "arXiv:2203.14757",
    "title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly  Voice Agent",
    "abstract": "Comments: 5 pages, 2 figures, Accepted for INTERSPEECH2022, project page: this http URL",
    "descriptor": "\nComments: 5 pages, 2 figures, Accepted for INTERSPEECH2022, project page: this http URL\n",
    "authors": [
      "Yuki Saito",
      "Yuto Nishimura",
      "Shinnosuke Takamichi",
      "Kentaro Tachibana",
      "Hiroshi Saruwatari"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.14757"
  },
  {
    "id": "arXiv:2203.16618",
    "title": "End-to-end Document Recognition and Understanding with Dessurt",
    "abstract": "End-to-end Document Recognition and Understanding with Dessurt",
    "descriptor": "",
    "authors": [
      "Brian Davis",
      "Bryan Morse",
      "Bryan Price",
      "Chris Tensmeyer",
      "Curtis Wigington",
      "Vlad Morariu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.16618"
  },
  {
    "id": "arXiv:2203.17003",
    "title": "Equivariant Diffusion for Molecule Generation in 3D",
    "abstract": "Comments: Accepted at International Conference on Machine Learning (ICML) 2022",
    "descriptor": "\nComments: Accepted at International Conference on Machine Learning (ICML) 2022\n",
    "authors": [
      "Emiel Hoogeboom",
      "Victor Garcia Satorras",
      "Cl\u00e9ment Vignac",
      "Max Welling"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.17003"
  },
  {
    "id": "arXiv:2203.17054",
    "title": "BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2112.11790",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.11790\n",
    "authors": [
      "Junjie Huang",
      "Guan Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.17054"
  },
  {
    "id": "arXiv:2204.03196",
    "title": "A Framework for Following Temporal Logic Instructions with Unknown  Causal Dependencies",
    "abstract": "Comments: Accepted at IJCNN 2022 (Oral)",
    "descriptor": "\nComments: Accepted at IJCNN 2022 (Oral)\n",
    "authors": [
      "Duo Xu",
      "Faramarz Fekri"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.03196"
  },
  {
    "id": "arXiv:2204.03417",
    "title": "Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0",
    "abstract": "Comments: Accepted at Interspeech 2022",
    "descriptor": "\nComments: Accepted at Interspeech 2022\n",
    "authors": [
      "Sebastian P. Bayerl",
      "Dominik Wagner",
      "Elmar N\u00f6th",
      "Korbinian Riedhammer"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.03417"
  },
  {
    "id": "arXiv:2204.06924",
    "title": "Handling sign language transcription system with the computer-friendly  numerical multilabels",
    "abstract": "Comments: 6 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 6 pages, 3 figures, 3 tables\n",
    "authors": [
      "Sylwia Majchrowska",
      "Marta Plantykow",
      "Milena Olech"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.06924"
  },
  {
    "id": "arXiv:2204.07402",
    "title": "BYOL for Audio: Exploring Pre-trained General-purpose Audio  Representations",
    "abstract": "Comments: 15 pages, 6 figures, and 15 tables. Under the review process",
    "descriptor": "\nComments: 15 pages, 6 figures, and 15 tables. Under the review process\n",
    "authors": [
      "Daisuke Niizumi",
      "Daiki Takeuchi",
      "Yasunori Ohishi",
      "Noboru Harada",
      "Kunio Kashino"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.07402"
  },
  {
    "id": "arXiv:2204.09610",
    "title": "MEDFORD: A human and machine readable metadata markup language",
    "abstract": "Comments: 10 pages, no figures",
    "descriptor": "\nComments: 10 pages, no figures\n",
    "authors": [
      "Polina Shpilker",
      "John Freeman",
      "Hailey McKelvie",
      "Jill Ashey",
      "Jay-Miguel Fonticella",
      "Hollie Putnam",
      "Jane Greenberg",
      "Lenore J. Cowen",
      "Alva Couch",
      "Noah M. Daniels"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Databases (cs.DB)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2204.09610"
  },
  {
    "id": "arXiv:2204.11307",
    "title": "A Comprehensive Test Pattern Generation Approach Exploiting SAT Attack  for Logic Locking",
    "abstract": "Comments: 10 pages, 8 figures",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Yadi Zhong",
      "Ujjwal Guin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.11307"
  },
  {
    "id": "arXiv:2204.11526",
    "title": "Faculty Distillation with Optimal Transport",
    "abstract": "Faculty Distillation with Optimal Transport",
    "descriptor": "",
    "authors": [
      "Su Lu",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.11526"
  },
  {
    "id": "arXiv:2204.13650",
    "title": "Unlocking High-Accuracy Differentially Private Image Classification  through Scale",
    "abstract": "Unlocking High-Accuracy Differentially Private Image Classification  through Scale",
    "descriptor": "",
    "authors": [
      "Soham De",
      "Leonard Berrada",
      "Jamie Hayes",
      "Samuel L. Smith",
      "Borja Balle"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.13650"
  },
  {
    "id": "arXiv:2204.13731",
    "title": "An Intriguing Property of Geophysics Inversion",
    "abstract": "An Intriguing Property of Geophysics Inversion",
    "descriptor": "",
    "authors": [
      "Yinan Feng",
      "Yinpeng Chen",
      "Shihang Feng",
      "Peng Jin",
      "Zicheng Liu",
      "Youzuo Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.13731"
  },
  {
    "id": "arXiv:2204.14028",
    "title": "Quantum Computing for Power Flow Algorithms: Testing on real Quantum  Computers",
    "abstract": "Comments: 8 pages, 6 figures, 4 tables, Accepted for Presentation in 11th Bulk Power Systems Dynamics and Control Symposium, July 25-30, 2022, Banff, Canada",
    "descriptor": "\nComments: 8 pages, 6 figures, 4 tables, Accepted for Presentation in 11th Bulk Power Systems Dynamics and Control Symposium, July 25-30, 2022, Banff, Canada\n",
    "authors": [
      "Brynjar S\u00e6varsson",
      "Spyros Chatzivasileiadis",
      "Hj\u00f6rtur J\u00f3hannsson",
      "Jacob \u00d8stergaard"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.14028"
  },
  {
    "id": "arXiv:2205.03569",
    "title": "Representation Learning for Compressed Video Action Recognition via  Attentive Cross-modal Interaction with Motion Enhancement",
    "abstract": "Comments: Accepted to IJCAI 2022",
    "descriptor": "\nComments: Accepted to IJCAI 2022\n",
    "authors": [
      "Bing Li",
      "Jiaxin Chen",
      "Dongming Zhang",
      "Xiuguo Bao",
      "Di Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.03569"
  },
  {
    "id": "arXiv:2205.03811",
    "title": "Data-Free Adversarial Knowledge Distillation for Graph Neural Networks",
    "abstract": "Comments: Accepted by IJCAI 2022. arXiv admin note: text overlap with arXiv:2011.14779, arXiv:1912.11006 by other authors",
    "descriptor": "\nComments: Accepted by IJCAI 2022. arXiv admin note: text overlap with arXiv:2011.14779, arXiv:1912.11006 by other authors\n",
    "authors": [
      "Yuanxin Zhuang",
      "Lingjuan Lyu",
      "Chuan Shi",
      "Carl Yang",
      "Lichao Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.03811"
  },
  {
    "id": "arXiv:2205.04066",
    "title": "Multi-level Consistency Learning for Semi-supervised Domain Adaptation",
    "abstract": "Comments: IJCAI 2022",
    "descriptor": "\nComments: IJCAI 2022\n",
    "authors": [
      "Zizheng Yan",
      "Yushuang Wu",
      "Guanbin Li",
      "Yipeng Qin",
      "Xiaoguang Han",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.04066"
  },
  {
    "id": "arXiv:2205.05664",
    "title": "Process, Bias and Temperature Scalable CMOS Analog Computing Circuits  for Machine Learning",
    "abstract": "Comments: 14 Pages, 15 Figures, 4 Tables. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: 14 Pages, 15 Figures, 4 Tables. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Pratik Kumar",
      "Ankita Nandi",
      "Shantanu Chakrabartty",
      "Chetan Singh Thakur"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.05664"
  },
  {
    "id": "arXiv:2205.05715",
    "title": "Causal discovery under a confounder blanket",
    "abstract": "Comments: To be presented at the 38th Conference on Uncertainty in Artificial Intelligence",
    "descriptor": "\nComments: To be presented at the 38th Conference on Uncertainty in Artificial Intelligence\n",
    "authors": [
      "David S. Watson",
      "Ricardo Silva"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.05715"
  },
  {
    "id": "arXiv:2205.06073",
    "title": "Consensus Capacity of Noisy Broadcast Channels",
    "abstract": "Consensus Capacity of Noisy Broadcast Channels",
    "descriptor": "",
    "authors": [
      "Neha Sangwan",
      "Varun Narayanan",
      "Vinod M. Prabhakaran"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.06073"
  },
  {
    "id": "arXiv:2205.06296",
    "title": "Integrating User and Item Reviews in Deep Cooperative Neural Networks  for Movie Ranking Prediction",
    "abstract": "Comments: 14 pages, typos corrected, references added",
    "descriptor": "\nComments: 14 pages, typos corrected, references added\n",
    "authors": [
      "Aristeidis Karras",
      "Christos Karras"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06296"
  },
  {
    "id": "arXiv:2205.07471",
    "title": "Adaptive Convolutional Dictionary Network for CT Metal Artifact  Reduction",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Hong Wang",
      "Yuexiang Li",
      "Deyu Meng",
      "Yefeng Zheng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.07471"
  },
  {
    "id": "arXiv:2205.07649",
    "title": "Generalizing to Evolving Domains with Latent Structure-Aware Sequential  Autoencoder",
    "abstract": "Comments: ICML 2022, code is available at this https URL",
    "descriptor": "\nComments: ICML 2022, code is available at this https URL\n",
    "authors": [
      "Tiexin Qin",
      "Shiqi Wang",
      "Haoliang Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.07649"
  },
  {
    "id": "arXiv:2205.09148",
    "title": "DDXPlus: A New Dataset For Automatic Medical Diagnosis",
    "abstract": "Comments: Submitted to NeurIPS 2022 Datasets and Benchmarks Track",
    "descriptor": "\nComments: Submitted to NeurIPS 2022 Datasets and Benchmarks Track\n",
    "authors": [
      "Arsene Fansi Tchango",
      "Rishab Goel",
      "Zhi Wen",
      "Julien Martel",
      "Joumana Ghosn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09148"
  },
  {
    "id": "arXiv:2205.10102",
    "title": "Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral  Compressive Imaging",
    "abstract": "Comments: The first Transformer-based deep unfolding method for spectral compressive imaging",
    "descriptor": "\nComments: The first Transformer-based deep unfolding method for spectral compressive imaging\n",
    "authors": [
      "Yuanhao Cai",
      "Jing Lin",
      "Haoqian Wang",
      "Xin Yuan",
      "Henghui Ding",
      "Yulun Zhang",
      "Radu Timofte",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.10102"
  },
  {
    "id": "arXiv:2205.10195",
    "title": "Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video  Restoration",
    "abstract": "Comments: ICML 2022; The first sequence-to-sequence model for video restoration",
    "descriptor": "\nComments: ICML 2022; The first sequence-to-sequence model for video restoration\n",
    "authors": [
      "Jing Lin",
      "Xiaowan Hu",
      "Yuanhao Cai",
      "Haoqian Wang",
      "Youliang Yan",
      "Xueyi Zou",
      "Yulun Zhang",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.10195"
  },
  {
    "id": "arXiv:2205.12897",
    "title": "Cryptocurrency Giveaway Scam with YouTube Live Stream",
    "abstract": "Cryptocurrency Giveaway Scam with YouTube Live Stream",
    "descriptor": "",
    "authors": [
      "Iman Vakilinia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.12897"
  },
  {
    "id": "arXiv:2205.13205",
    "title": "$O(N^2)$ Universal Antisymmetry in Fermionic Neural Networks",
    "abstract": "Comments: ICML 2022 AI for Science Workshop",
    "descriptor": "\nComments: ICML 2022 AI for Science Workshop\n",
    "authors": [
      "Tianyu Pang",
      "Shuicheng Yan",
      "Min Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.13205"
  },
  {
    "id": "arXiv:2205.13542",
    "title": "BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View  Representation",
    "abstract": "Comments: The first two authors contributed equally to this work. Project page: this https URL",
    "descriptor": "\nComments: The first two authors contributed equally to this work. Project page: this https URL\n",
    "authors": [
      "Zhijian Liu",
      "Haotian Tang",
      "Alexander Amini",
      "Xinyu Yang",
      "Huizi Mao",
      "Daniela Rus",
      "Song Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13542"
  },
  {
    "id": "arXiv:2205.13574",
    "title": "Pruning has a disparate impact on model accuracy",
    "abstract": "Pruning has a disparate impact on model accuracy",
    "descriptor": "",
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto",
      "Jung-Eun Kim",
      "Rakshit Naidu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13574"
  },
  {
    "id": "arXiv:2206.02051",
    "title": "Fast and Accurate Error Simulation for CNNs against Soft Errors",
    "abstract": "Comments: Accepted for publication in IEEE Transactions on Computers",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Computers\n",
    "authors": [
      "Cristiana Bolchini",
      "Luca Cassano",
      "Antonio Miele",
      "Alessandro Toschi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.02051"
  },
  {
    "id": "arXiv:2206.02796",
    "title": "Mixed Graph Contrastive Network for Semi-Supervised Node Classification",
    "abstract": "Mixed Graph Contrastive Network for Semi-Supervised Node Classification",
    "descriptor": "",
    "authors": [
      "Xihong Yang",
      "Yue Liu",
      "Sihang Zhou",
      "Xinwang Liu",
      "En Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02796"
  },
  {
    "id": "arXiv:2206.03192",
    "title": "Generalized Data Distribution Iteration",
    "abstract": "Comments: update the appendix. arXiv admin note: substantial text overlap with arXiv:2112.04145",
    "descriptor": "\nComments: update the appendix. arXiv admin note: substantial text overlap with arXiv:2112.04145\n",
    "authors": [
      "Jiajun Fan",
      "Changnan Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.03192"
  },
  {
    "id": "arXiv:2206.03563",
    "title": "Two Ways of Understanding Social Dynamics: Analyzing the Predictability  of Emergence of Objects in Reddit r/place Dependent on Locality in Space and  Time",
    "abstract": "Two Ways of Understanding Social Dynamics: Analyzing the Predictability  of Emergence of Objects in Reddit r/place Dependent on Locality in Space and  Time",
    "descriptor": "",
    "authors": [
      "Alyssa M Adams",
      "Javier Fernandez",
      "Olaf Witkowski"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Social and Information Networks (cs.SI)",
      "Cellular Automata and Lattice Gases (nlin.CG)"
    ],
    "url": "https://arxiv.org/abs/2206.03563"
  },
  {
    "id": "arXiv:2206.03734",
    "title": "On gradient descent training under data augmentation with on-line noisy  copies",
    "abstract": "On gradient descent training under data augmentation with on-line noisy  copies",
    "descriptor": "",
    "authors": [
      "Katsuyuki Hagiwara"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03734"
  },
  {
    "id": "arXiv:2206.03838",
    "title": "Efficient reversible data hiding via two layers of double-peak embedding",
    "abstract": "Efficient reversible data hiding via two layers of double-peak embedding",
    "descriptor": "",
    "authors": [
      "Fuhu Wu",
      "Jian Sun",
      "Shun Zhang",
      "Zhili Chen",
      "Hong Zhong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.03838"
  },
  {
    "id": "arXiv:2206.04617",
    "title": "Autonomous Precision Drone Landing with Fiducial Markers and a  Gimbal-Mounted Camera for Active Tracking",
    "abstract": "Comments: License change to non-commercial",
    "descriptor": "\nComments: License change to non-commercial\n",
    "authors": [
      "Joshua Springer",
      "Marcel Kyas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2206.04617"
  },
  {
    "id": "arXiv:2206.05075",
    "title": "Diffeomorphic Counterfactuals with Generative Models",
    "abstract": "Diffeomorphic Counterfactuals with Generative Models",
    "descriptor": "",
    "authors": [
      "Ann-Kathrin Dombrowski",
      "Jan E. Gerken",
      "Klaus-Robert M\u00fcller",
      "Pan Kessel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05075"
  },
  {
    "id": "arXiv:2206.05391",
    "title": "Feature Selection using e-values",
    "abstract": "Comments: accepted in ICML-2022",
    "descriptor": "\nComments: accepted in ICML-2022\n",
    "authors": [
      "Subhabrata Majumdar",
      "Snigdhansu Chatterjee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.05391"
  },
  {
    "id": "arXiv:2206.05404",
    "title": "Squeeze All: Novel Estimator and Self-Normalized Bound for Linear  Contextual Bandits",
    "abstract": "Comments: 27 pages including Appendix",
    "descriptor": "\nComments: 27 pages including Appendix\n",
    "authors": [
      "Wonyoung Kim",
      "Min-hwan Oh",
      "Myunghee Cho Paik"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05404"
  },
  {
    "id": "arXiv:2206.05490",
    "title": "Discovery and density estimation of latent confounders in Bayesian  networks with evidence lower bound",
    "abstract": "Discovery and density estimation of latent confounders in Bayesian  networks with evidence lower bound",
    "descriptor": "",
    "authors": [
      "Kiattikun Chobtham",
      "Anthony C. Constantinou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05490"
  },
  {
    "id": "arXiv:2206.05807",
    "title": "Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for  Simultaneous Speech Translation",
    "abstract": "Comments: AutoSimTrans Workshop @ NAACL2022",
    "descriptor": "\nComments: AutoSimTrans Workshop @ NAACL2022\n",
    "authors": [
      "Sara Papi",
      "Marco Gaido",
      "Matteo Negri",
      "Marco Turchi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.05807"
  },
  {
    "id": "arXiv:2206.06023",
    "title": "Virtual embeddings and self-consistency for self-supervised learning",
    "abstract": "Virtual embeddings and self-consistency for self-supervised learning",
    "descriptor": "",
    "authors": [
      "Tariq Bdair",
      "Hossam Abdelhamid",
      "Nassir Navab",
      "Shadi Albarqouni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.06023"
  },
  {
    "id": "arXiv:2206.06556",
    "title": "F3 Hand: A Versatile Robot Hand Inspired by Human Thumb and Index  Fingers",
    "abstract": "Comments: 8 pages. Accepted at IEEE RO-MAN 2022. An accompanying video is available at this https URL",
    "descriptor": "\nComments: 8 pages. Accepted at IEEE RO-MAN 2022. An accompanying video is available at this https URL\n",
    "authors": [
      "Naoki Fukaya",
      "Avinash Ummadisingu",
      "Guilherme Maeda",
      "Shin-ichi Maeda"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.06556"
  },
  {
    "id": "arXiv:2206.06795",
    "title": "The Dynamics of Riemannian Robbins-Monro Algorithms",
    "abstract": "The Dynamics of Riemannian Robbins-Monro Algorithms",
    "descriptor": "",
    "authors": [
      "Mohammad Reza Karimi",
      "Ya-Ping Hsieh",
      "Panayotis Mertikopoulos",
      "Andreas Krause"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2206.06795"
  },
  {
    "id": "arXiv:2206.06829",
    "title": "Efficient Decoder-free Object Detection with Transformers",
    "abstract": "Comments: Update metadata, 10 pages",
    "descriptor": "\nComments: Update metadata, 10 pages\n",
    "authors": [
      "Peixian Chen",
      "Mengdan Zhang",
      "Yunhang Shen",
      "Kekai Sheng",
      "Yuting Gao",
      "Xing Sun",
      "Ke Li",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06829"
  },
  {
    "id": "arXiv:2206.06872",
    "title": "On Provably Robust Meta-Bayesian Optimization",
    "abstract": "Comments: Accepted to 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022), Extended version with proofs and additional experimental details and results, 31 pages",
    "descriptor": "\nComments: Accepted to 38th Conference on Uncertainty in Artificial Intelligence (UAI 2022), Extended version with proofs and additional experimental details and results, 31 pages\n",
    "authors": [
      "Zhongxiang Dai",
      "Yizhou Chen",
      "Haibin Yu",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.06872"
  },
  {
    "id": "arXiv:2206.06924",
    "title": "The Maximum Linear Arrangement Problem for trees under projectivity and  planarity",
    "abstract": "The Maximum Linear Arrangement Problem for trees under projectivity and  planarity",
    "descriptor": "",
    "authors": [
      "Llu\u00eds Alemany-Puig",
      "Juan Luis Esteban",
      "Ramon Ferrer-i-Cancho"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computation and Language (cs.CL)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.06924"
  },
  {
    "id": "arXiv:2206.07137",
    "title": "Prioritized Training on Points that are Learnable, Worth Learning, and  Not Yet Learnt",
    "abstract": "Comments: ICML 2022 (Follow up to arXiv:2107.02565)",
    "descriptor": "\nComments: ICML 2022 (Follow up to arXiv:2107.02565)\n",
    "authors": [
      "S\u00f6ren Mindermann",
      "Jan Brauner",
      "Muhammed Razzak",
      "Mrinank Sharma",
      "Andreas Kirsch",
      "Winnie Xu",
      "Benedikt H\u00f6ltgen",
      "Aidan N. Gomez",
      "Adrien Morisot",
      "Sebastian Farquhar",
      "Yarin Gal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07137"
  },
  {
    "id": "arXiv:2206.07286",
    "title": "Faster Decomposition of Weighted Graphs into Cliques using Fisher's  Inequality",
    "abstract": "Comments: 11 pages, 7 figures",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Shweta Jain",
      "Yosuke Mizutani",
      "Blair Sullivan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2206.07286"
  },
  {
    "id": "arXiv:2206.07298",
    "title": "S$^2$-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for  Real-time Semantic Segmentation",
    "abstract": "S$^2$-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for  Real-time Semantic Segmentation",
    "descriptor": "",
    "authors": [
      "Mohammed A. M. Elhassan",
      "Chenhui Yang",
      "Chenxi Huang",
      "Tewodros Legesse Munea",
      "Xin Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07298"
  },
  {
    "id": "arXiv:2206.07351",
    "title": "RecBole 2.0: Towards a More Up-to-Date Recommendation Library",
    "abstract": "Comments: A new version of recommendation toolkit -- RecBole",
    "descriptor": "\nComments: A new version of recommendation toolkit -- RecBole\n",
    "authors": [
      "Wayne Xin Zhao",
      "Yupeng Hou",
      "Xingyu Pan",
      "Chen Yang",
      "Zeyu Zhang",
      "Zihan Lin",
      "Jingsen Zhang",
      "Shuqing Bian",
      "Jiakai Tang",
      "Wenqi Sun",
      "Yushuo Chen",
      "Lanling Xu",
      "Gaowei Zhang",
      "Zhen Tian",
      "Changxin Tian",
      "Shanlei Mu",
      "Xinyan Fan",
      "Xu Chen",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2206.07351"
  },
  {
    "id": "arXiv:2206.07359",
    "title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for  Emotion Recognition in Conversation",
    "abstract": "Comments: Accepted by INTERSPEECH 2022",
    "descriptor": "\nComments: Accepted by INTERSPEECH 2022\n",
    "authors": [
      "Joosung Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07359"
  },
  {
    "id": "arXiv:2206.07364",
    "title": "Seeking Common Ground While Reserving Differences: Multiple Anatomy  Collaborative Framework for Undersampled MRI Reconstruction",
    "abstract": "Comments: submitted to an IEEE journal",
    "descriptor": "\nComments: submitted to an IEEE journal\n",
    "authors": [
      "Jiangpeng Yan",
      "Chenghui Yu",
      "Hanbo Chen",
      "Zhe Xu",
      "Junzhou Huang",
      "Xiu Li",
      "Jianhua Yao"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07364"
  },
  {
    "id": "arXiv:2206.07481",
    "title": "A Survey of Detection Methods for Die Attachment and Wire Bonding  Defects in Integrated Circuit Manufacturing",
    "abstract": "Comments: 13 pages, 9 figures, 8 tables",
    "descriptor": "\nComments: 13 pages, 9 figures, 8 tables\n",
    "authors": [
      "Lamia Alam",
      "Nasser Kehtarnavaz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07481"
  },
  {
    "id": "arXiv:2206.07492",
    "title": "Preliminary study on the impact of EEG density on TMS-EEG classification  in Alzheimer's disease",
    "abstract": "Comments: 4 pages, 4 figures, accepted to the 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 11-15 July 2022, Glasgow, Scotland, UK",
    "descriptor": "\nComments: 4 pages, 4 figures, accepted to the 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 11-15 July 2022, Glasgow, Scotland, UK\n",
    "authors": [
      "Alexandra-Maria Tautan",
      "Elias Casula",
      "Ilaria Borghi",
      "Michele Maiella",
      "Sonia Bonni",
      "Marilena Minei",
      "Martina Assogna",
      "Bogdan Ionescu",
      "Giacomo Koch",
      "Emiliano Santarnecchi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07492"
  }
]