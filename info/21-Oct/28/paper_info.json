[
  {
    "id": "arXiv:2110.13935",
    "title": "Frequency Centric Defense Mechanisms against Adversarial Examples",
    "abstract": "Adversarial example (AE) aims at fooling a Convolution Neural Network by\nintroducing small perturbations in the input image.The proposed work uses the\nmagnitude and phase of the Fourier Spectrum and the entropy of the image to\ndefend against AE. We demonstrate the defense in two ways: by training an\nadversarial detector and denoising the adversarial effect. Experiments were\nconducted on the low-resolution CIFAR-10 and high-resolution ImageNet datasets.\nThe adversarial detector has 99% accuracy for FGSM and PGD attacks on the\nCIFAR-10 dataset. However, the detection accuracy falls to 50% for\nsophisticated DeepFool and Carlini & Wagner attacks on ImageNet. We overcome\nthe limitation by using autoencoder and show that 70% of AEs are correctly\nclassified after denoising.",
    "descriptor": "\nComments: AdvM '21: Proceedings of the 1st International Workshop on Adversarial Learning for Multimedia, at ACM Multimedia '21\n",
    "authors": [
      "Sanket B. Shah",
      "Param Raval",
      "Harin Khakhi",
      "Mehul S. Raval"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.13935"
  },
  {
    "id": "arXiv:2110.13937",
    "title": "Provably Robust Model-Centric Explanations for Critical Decision-Making",
    "abstract": "We recommend using a model-centric, Boolean Satisfiability (SAT) formalism to\nobtain useful explanations of trained model behavior, different and\ncomplementary to what can be gleaned from LIME and SHAP, popular data-centric\nexplanation tools in Artificial Intelligence (AI). We compare and contrast\nthese methods, and show that data-centric methods may yield brittle\nexplanations of limited practical utility. The model-centric framework,\nhowever, can offer actionable insights into risks of using AI models in\npractice. For critical applications of AI, split-second decision making is best\ninformed by robust explanations that are invariant to properties of data, the\ncapability offered by model-centric frameworks.",
    "descriptor": "\nComments: 8 pages, 9 figures\n",
    "authors": [
      "Cecilia G. Morales",
      "Nicholas Gisolfi",
      "Robert Edman",
      "James K. Miller",
      "Artur Dubrawski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.13937"
  },
  {
    "id": "arXiv:2110.13939",
    "title": "CausalAF: Causal Autoregressive Flow for Goal-Directed Safety-Critical  Scenes Generation",
    "abstract": "Goal-directed generation, aiming for solving downstream tasks by generating\ndiverse data, has a potentially wide range of applications in the real world.\nPrevious works tend to formulate goal-directed generation as a purely\ndata-driven problem, which directly searches or approximates the distribution\nof samples satisfying the goal. However, the generation ability of preexisting\nwork is heavily restricted by inefficient sampling, especially for sparse goals\nthat rarely show up in off-the-shelf datasets. For instance, generating\nsafety-critical traffic scenes with the goal of increasing the risk of\ncollision is critical to evaluate autonomous vehicles, but the rareness of such\nscenes is the biggest resistance. In this paper, we integrate causality as a\nprior into the safety-critical scene generation process and propose a\nflow-based generative framework - Causal Autoregressive Flow (CausalAF).\nCausalAF encourages the generative model to uncover and follow the causal\nrelationship among generated objects via novel causal masking operations\ninstead of searching the sample only from observational data. By learning the\ncause-and-effect mechanism of how the generated scene achieves the goal rather\nthan just learning correlations from data, CausalAF significantly improves the\nlearning efficiency. Extensive experiments on three heterogeneous traffic\nscenes illustrate that CausalAF requires much fewer optimization resources to\neffectively generate goal-directed scenes for safety evaluation tasks.",
    "descriptor": "\nComments: 17 pages. Under review of CleaR 2022\n",
    "authors": [
      "Wenhao Ding",
      "Haohong Lin",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13939"
  },
  {
    "id": "arXiv:2110.13941",
    "title": "Rapid IoT Device Identification at the Edge",
    "abstract": "Consumer Internet of Things (IoT) devices are increasingly common in everyday\nhomes, from smart speakers to security cameras. Along with their benefits come\npotential privacy and security threats. To limit these threats we must\nimplement solutions to filter IoT traffic at the edge. To this end the\nidentification of the IoT device is the first natural step.\nIn this paper we demonstrate a novel method of rapid IoT device\nidentification that uses neural networks trained on device DNS traffic that can\nbe captured from a DNS server on the local network. The method identifies\ndevices by fitting a model to the first seconds of DNS second-level-domain\ntraffic following their first connection. Since security and privacy threat\ndetection often operate at a device specific level, rapid identification allows\nthese strategies to be implemented immediately. Through a total of 51,000\nrigorous automated experiments, we classify 30 consumer IoT devices from 27\ndifferent manufacturers with 82% and 93% accuracy for product type and device\nmanufacturers respectively.",
    "descriptor": "",
    "authors": [
      "Oliver Thompson",
      "Anna Maria Mandalari",
      "Hamed Haddadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.13941"
  },
  {
    "id": "arXiv:2110.13947",
    "title": "Collaborative Uncertainty in Multi-Agent Trajectory Forecasting",
    "abstract": "Uncertainty modeling is critical in trajectory forecasting systems for both\ninterpretation and safety reasons. To better predict the future trajectories of\nmultiple agents, recent works have introduced interaction modules to capture\ninteractions among agents. This approach leads to correlations among the\npredicted trajectories. However, the uncertainty brought by such correlations\nis neglected. To fill this gap, we propose a novel concept, collaborative\nuncertainty(CU), which models the uncertainty resulting from the interaction\nmodule. We build a general CU-based framework to make a prediction model to\nlearn the future trajectory and the corresponding uncertainty. The CU-based\nframework is integrated as a plugin module to current state-of-the-art (SOTA)\nsystems and deployed in two special cases based on multivariate Gaussian and\nLaplace distributions. In each case, we conduct extensive experiments on two\nsynthetic datasets and two public, large-scale benchmarks of trajectory\nforecasting. The results are promising: 1) The results of synthetic datasets\nshow that CU-based framework allows the model to appropriately approximate the\nground-truth distribution. 2) The results of trajectory forecasting benchmarks\ndemonstrate that the CU-based framework steadily helps SOTA systems improve\ntheir performances. Especially, the proposed CU-based framework helps VectorNet\nimprove by 57cm regarding Final Displacement Error on nuScenes dataset. 3) The\nvisualization results of CU illustrate that the value of CU is highly related\nto the amount of the interactive information among agents.",
    "descriptor": "\nComments: This paper has been accepted by NeurIPS 2021\n",
    "authors": [
      "Bohan Tang",
      "Yiqi Zhong",
      "Ulrich Neumann",
      "Gang Wang",
      "Ya Zhang",
      "Siheng Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13947"
  },
  {
    "id": "arXiv:2110.13948",
    "title": "Boosted CVaR Classification",
    "abstract": "Many modern machine learning tasks require models with high tail performance,\ni.e. high performance over the worst-off samples in the dataset. This problem\nhas been widely studied in fields such as algorithmic fairness, class\nimbalance, and risk-sensitive decision making. A popular approach to maximize\nthe model's tail performance is to minimize the CVaR (Conditional Value at\nRisk) loss, which computes the average risk over the tails of the loss.\nHowever, for classification tasks where models are evaluated by the zero-one\nloss, we show that if the classifiers are deterministic, then the minimizer of\nthe average zero-one loss also minimizes the CVaR zero-one loss, suggesting\nthat CVaR loss minimization is not helpful without additional assumptions. We\ncircumvent this negative result by minimizing the CVaR loss over randomized\nclassifiers, for which the minimizers of the average zero-one loss and the CVaR\nzero-one loss are no longer the same, so minimizing the latter can lead to\nbetter tail performance. To learn such randomized classifiers, we propose the\nBoosted CVaR Classification framework which is motivated by a direct\nrelationship between CVaR and a classical boosting algorithm called LPBoost.\nBased on this framework, we design an algorithm called $\\alpha$-AdaLPBoost. We\nempirically evaluate our proposed algorithm on four benchmark datasets and show\nthat it achieves higher tail performance than deterministic model training\nmethods.",
    "descriptor": "\nComments: NeurIPS 2021. 16 pages, 4 figures\n",
    "authors": [
      "Runtian Zhai",
      "Chen Dan",
      "Arun Sai Suggala",
      "Zico Kolter",
      "Pradeep Ravikumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.13948"
  },
  {
    "id": "arXiv:2110.13950",
    "title": "Can't Fool Me: Adversarially Robust Transformer for Video Understanding",
    "abstract": "Deep neural networks have been shown to perform poorly on adversarial\nexamples. To address this, several techniques have been proposed to increase\nrobustness of a model for image classification tasks. However, in video\nunderstanding tasks, developing adversarially robust models is still\nunexplored. In this paper, we aim to bridge this gap. We first show that simple\nextensions of image based adversarially robust models slightly improve the\nworst-case performance. Further, we propose a temporal attention regularization\nscheme in Transformer to improve the robustness of attention modules to\nadversarial examples. We illustrate using a large-scale video data set\nYouTube-8M that the final model (A-ART) achieves close to non-adversarial\nperformance on its adversarial example set. We achieve 91% GAP on adversarial\nexamples, whereas baseline Transformer and simple adversarial extensions\nachieve 72.9% and 82% respectively, showing significant improvement in\nrobustness over the state-of-the-art.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2103.10043\n",
    "authors": [
      "Divya Choudhary",
      "Palash Goyal",
      "Saurabh Sahu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.13950"
  },
  {
    "id": "arXiv:2110.13953",
    "title": "On sensitivity of meta-learning to support data",
    "abstract": "Meta-learning algorithms are widely used for few-shot learning. For example,\nimage recognition systems that readily adapt to unseen classes after seeing\nonly a few labeled examples. Despite their success, we show that modern\nmeta-learning algorithms are extremely sensitive to the data used for\nadaptation, i.e. support data. In particular, we demonstrate the existence of\n(unaltered, in-distribution, natural) images that, when used for adaptation,\nyield accuracy as low as 4\\% or as high as 95\\% on standard few-shot image\nclassification benchmarks. We explain our empirical findings in terms of class\nmargins, which in turn suggests that robust and safe meta-learning requires\nlarger margins than supervised learning.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Mayank Agarwal",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13953"
  },
  {
    "id": "arXiv:2110.13957",
    "title": "Unbiased Graph Embedding with Biased Graph Observations",
    "abstract": "Graph embedding techniques have been increasingly employed in real-world\nmachine learning tasks on graph-structured data, such as social recommendations\nand protein structure modeling. Since the generation of a graph is inevitably\naffected by some sensitive node attributes (such as gender and age of users in\na social network), the learned graph representations can inherit such sensitive\ninformation and introduce undesirable biases in downstream tasks. Most existing\nworks on debiasing graph representations add ad-hoc constraints on the learned\nembeddings to restrict their distributions, which however compromise the\nutility of resulting graph representations in downstream tasks.\nIn this paper, we propose a principled new way for obtaining unbiased\nrepresentations by learning from an underlying bias-free graph that is not\ninfluenced by sensitive attributes. Based on this new perspective, we propose\ntwo complementary methods for uncovering such an underlying graph with the goal\nof introducing minimum impact on the utility of learned representations in\ndownstream tasks. Both our theoretical justification and extensive experiment\ncomparisons against state-of-the-art solutions demonstrate the effectiveness of\nour proposed methods.",
    "descriptor": "",
    "authors": [
      "Nan Wang",
      "Lu Lin",
      "Jundong Li",
      "Hongning Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.13957"
  },
  {
    "id": "arXiv:2110.13967",
    "title": "Evaluating Serverless Architecture for Big Data Enterprise Applications",
    "abstract": "In this paper, we investigate serverless computing for performing large scale\ndata processing with cloudnative primitives.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Aimer Bhat",
      "Madhumonti Roy",
      "Heeki Park"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.13967"
  },
  {
    "id": "arXiv:2110.13968",
    "title": "On the Effects of Data Distortion on Model Analysis and Training",
    "abstract": "Data modification can introduce artificial information. It is often assumed\nthat the resulting artefacts are detrimental to training, whilst being\nnegligible when analysing models. We investigate these assumptions and conclude\nthat in some cases they are unfounded and lead to incorrect results.\nSpecifically, we show current shape bias identification methods and occlusion\nrobustness measures are biased and propose a fairer alternative for the latter.\nSubsequently, through a series of experiments we seek to correct and strengthen\nthe community's perception of how distorting data affects learning. Based on\nour empirical results we argue that the impact of the artefacts must be\nunderstood and exploited rather than eliminated.",
    "descriptor": "",
    "authors": [
      "Antonia Marcu",
      "Adam Pr\u00fcgel-Bennett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13968"
  },
  {
    "id": "arXiv:2110.13970",
    "title": "Rademacher Random Projections with Tensor Networks",
    "abstract": "Random projection (RP) have recently emerged as popular techniques in\nthemachine learning community for their ability in reducing the dimension of\nveryhigh-dimensional tensors. Following the work in [29], we consider a\ntensorizedrandom projection relying on Tensor Train (TT) decomposition where\neach elementof the core tensors is drawn from a Rademacher distribution. Our\ntheoreticalresults reveal that the Gaussian low-rank tensor represented in\ncompressed formin TT format in [29] can be replaced by a TT tensor with core\nelements drawnfrom a Rademacher distribution with the same embedding size.\nExperiments onsynthetic data demonstrate that tensorized Rademacher RP can\noutperform thetensorized Gaussian RP studied in [29]. In addition, we show both\ntheoreticallyand experimentally, that the tensorized RP in the Matrix Product\nOperator (MPO)format proposed in [5] for performing SVD on large matrices is\nnot a Johnson-Lindenstrauss transform (JLT) and therefore not a well-suited\nrandom projectionmap",
    "descriptor": "",
    "authors": [
      "Beheshteh T. Rakhshan",
      "Guillaume Rabusseau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.13970"
  },
  {
    "id": "arXiv:2110.13971",
    "title": "Diachronic Text Mining Investigation of Therapeutic Candidates for  COVID-19",
    "abstract": "Diachronic text mining has frequently been applied to long-term linguistic\nsurveys of word meaning and usage shifts over time. In this paper we apply\nshort-term diachronic text mining to a rapidly growing corpus of scientific\npublications on COVID-19 captured in the CORD-19 dataset in order to identify\nco-occurrences and analyze the behavior of potential candidate treatments. We\nused a data set associated with a COVID-19 drug re-purposing study from Oak\nRidge National Laboratory. This study identified existing candidate coronavirus\ntreatments, including drugs and approved compounds, which had been analyzed and\nranked according to their potential for blocking the ability of the SARS-COV-2\nvirus to invade human cells. We investigated the occurrence of these candidates\nin temporal instances of the CORD-19 corpus. We found that at least 25% of the\nidentified terms occurred in temporal instances of the corpus to the extent\nthat their frequency and contextual dynamics could be evaluated. We identified\nthree classes of behaviors: those where frequency and contextual shifts were\nsmall and positively correlated; those where there was no correlation between\nfrequency and contextual changes; and those where there was a negative\ncorrelation between frequency and contextual shift. We speculate that the\nlatter two patterns are indicative that a target candidate therapeutics is\nundergoing active evaluation. The patterns we detected demonstrate the\npotential benefits of using diachronic text mining techniques with a large\ndynamic text corpus to track drug-repurposing activities across international\nclinical and laboratory settings.",
    "descriptor": "",
    "authors": [
      "James Powell",
      "Kari Sentz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Other Quantitative Biology (q-bio.OT)"
    ],
    "url": "https://arxiv.org/abs/2110.13971"
  },
  {
    "id": "arXiv:2110.13972",
    "title": "Video-based fully automatic assessment of open surgery suturing skills",
    "abstract": "The goal of this study was to develop new reliable open surgery suturing\nsimulation system for training medical students in situation where resources\nare limited or in the domestic setup. Namely, we developed an algorithm for\ntools and hands localization as well as identifying the interactions between\nthem based on simple webcam video data, calculating motion metrics for\nassessment of surgical skill. Twenty-five participants performed multiple\nsuturing tasks using our simulator. The YOLO network has been modified to a\nmulti-task network, for the purpose of tool localization and tool-hand\ninteraction detection. This was accomplished by splitting the YOLO detection\nheads so that they supported both tasks with minimal addition to computer\nrun-time. Furthermore, based on the outcome of the system, motion metrics were\ncalculated. These metrics included traditional metrics such as time and path\nlength as well as new metrics assessing the technique participants use for\nholding the tools. The dual-task network performance was similar to that of two\nnetworks, while computational load was only slightly bigger than one network.\nIn addition, the motion metrics showed significant differences between experts\nand novices. While video capture is an essential part of minimally invasive\nsurgery, it is not an integral component of open surgery. Thus, new algorithms,\nfocusing on the unique challenges open surgery videos present, are required. In\nthis study, a dual-task network was developed to solve both a localization task\nand a hand-tool interaction task. The dual network may be easily expanded to a\nmulti-task network, which may be useful for images with multiple layers and for\nevaluating the interaction between these different layers.",
    "descriptor": "\nComments: 12 pages, 5 figures, submitted to IJCARS\n",
    "authors": [
      "Adam Goldbraikh",
      "Anne-Lise D'Angelo",
      "Carla M. Pugh",
      "Shlomi Laufer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13972"
  },
  {
    "id": "arXiv:2110.13973",
    "title": "The Value of Information When Deciding What to Learn",
    "abstract": "All sequential decision-making agents explore so as to acquire knowledge\nabout a particular target. It is often the responsibility of the agent designer\nto construct this target which, in rich and complex environments, constitutes a\nonerous burden; without full knowledge of the environment itself, a designer\nmay forge a sub-optimal learning target that poorly balances the amount of\ninformation an agent must acquire to identify the target against the target's\nassociated performance shortfall. While recent work has developed a connection\nbetween learning targets and rate-distortion theory to address this challenge\nand empower agents that decide what to learn in an automated fashion, the\nproposed algorithm does not optimally tackle the equally important challenge of\nefficient information acquisition. In this work, building upon the seminal\ndesign principle of information-directed sampling (Russo & Van Roy, 2014), we\naddress this shortcoming directly to couple optimal information acquisition\nwith the optimal design of learning targets. Along the way, we offer new\ninsights into learning targets from the literature on rate-distortion theory\nbefore turning to empirical results that confirm the value of information when\ndeciding what to learn.",
    "descriptor": "\nComments: Accepted to Neural Information Processing Systems (NeurIPS) 2021\n",
    "authors": [
      "Dilip Arumugam",
      "Benjamin Van Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.13973"
  },
  {
    "id": "arXiv:2110.13974",
    "title": "Global sensitivity analysis of rare event probabilities",
    "abstract": "By their very nature, rare event probabilities are expensive to compute; they\nare also delicate to estimate as their value strongly depends on distributional\nassumptions on the model parameters. Hence, understanding the sensitivity of\nthe computed rare event probabilities to the hyper-parameters that define the\ndistribution law of the model parameters is crucial. We show that by (i)\naccelerating the calculation of rare event probabilities through subset\nsimulation and (ii) approximating the resulting probabilities through a\npolynomial chaos expansion, the global sensitivity of such problems can be\nanalyzed through a double-loop sampling approach. The resulting method is\nconceptually simple and computationally efficient; its performance is\nillustrated on a subsurface flow application and on an analytical example.",
    "descriptor": "",
    "authors": [
      "Michael Merritt",
      "Alen Alexanderian",
      "Pierre Gremaud"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.13974"
  },
  {
    "id": "arXiv:2110.13980",
    "title": "Adversarial Attacks and Defenses for Social Network Text Processing  Applications: Techniques, Challenges and Future Research Directions",
    "abstract": "The growing use of social media has led to the development of several Machine\nLearning (ML) and Natural Language Processing(NLP) tools to process the\nunprecedented amount of social media content to make actionable decisions.\nHowever, these MLand NLP algorithms have been widely shown to be vulnerable to\nadversarial attacks. These vulnerabilities allow adversaries to launch a\ndiversified set of adversarial attacks on these algorithms in different\napplications of social media text processing. In this paper, we provide a\ncomprehensive review of the main approaches for adversarial attacks and\ndefenses in the context of social media applications with a particular focus on\nkey challenges and future research directions. In detail, we cover literature\non six key applications, namely (i) rumors detection, (ii) satires detection,\n(iii) clickbait & spams identification, (iv) hate speech detection,\n(v)misinformation detection, and (vi) sentiment analysis. We then highlight the\nconcurrent and anticipated future research questions and provide\nrecommendations and directions for future work.",
    "descriptor": "\nComments: 21 pages, 6 figures, 10 tables\n",
    "authors": [
      "Izzat Alsmadi",
      "Kashif Ahmad",
      "Mahmoud Nazzal",
      "Firoj Alam",
      "Ala Al-Fuqaha",
      "Abdallah Khreishah",
      "Abdulelah Algosaibi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.13980"
  },
  {
    "id": "arXiv:2110.13981",
    "title": "CHIP: CHannel Independence-based Pruning for Compact Neural Networks",
    "abstract": "Filter pruning has been widely used for neural network compression because of\nits enabled practical acceleration. To date, most of the existing filter\npruning works explore the importance of filters via using intra-channel\ninformation. In this paper, starting from an inter-channel perspective, we\npropose to perform efficient filter pruning using Channel Independence, a\nmetric that measures the correlations among different feature maps. The less\nindependent feature map is interpreted as containing less useful\ninformation$/$knowledge, and hence its corresponding filter can be pruned\nwithout affecting model capacity. We systematically investigate the\nquantification metric, measuring scheme and sensitiveness$/$reliability of\nchannel independence in the context of filter pruning. Our evaluation results\nfor different models on various datasets show the superior performance of our\napproach. Notably, on CIFAR-10 dataset our solution can bring $0.75\\%$ and\n$0.94\\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,\nrespectively, and meanwhile the model size and FLOPs are reduced by $42.8\\%$\nand $47.4\\%$ (for ResNet-56) and $48.3\\%$ and $52.1\\%$ (for ResNet-110),\nrespectively. On ImageNet dataset, our approach can achieve $40.8\\%$ and\n$44.8\\%$ storage and computation reductions, respectively, with $0.15\\%$\naccuracy increase over the baseline ResNet-50 model. The code is available at\nhttps://github.com/Eclipsess/CHIP_NeurIPS2021.",
    "descriptor": "\nComments: This paper is accepted by NeurIPS 2021. Key words: Model Compression, Filter Pruning, CNN, Channel Pruning, Deep Learning\n",
    "authors": [
      "Yang Sui",
      "Miao Yin",
      "Yi Xie",
      "Huy Phan",
      "Saman Zonouz",
      "Bo Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13981"
  },
  {
    "id": "arXiv:2110.13985",
    "title": "Combining Recurrent, Convolutional, and Continuous-time Models with  Linear State-Space Layers",
    "abstract": "Recurrent neural networks (RNNs), temporal convolutions, and neural\ndifferential equations (NDEs) are popular families of deep learning models for\ntime-series data, each with unique strengths and tradeoffs in modeling power\nand computational efficiency. We introduce a simple sequence model inspired by\ncontrol systems that generalizes these approaches while addressing their\nshortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$\nby simply simulating a linear continuous-time state-space representation\n$\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are\nclosely related to the three aforementioned families of models and inherit\ntheir strengths. For example, they generalize convolutions to continuous-time,\nexplain common RNN heuristics, and share features of NDEs such as time-scale\nadaptation. We then incorporate and generalize recent theory on continuous-time\nmemorization to introduce a trainable subset of structured matrices $A$ that\nendow LSSLs with long-range memory. Empirically, stacking LSSL layers into a\nsimple deep neural network obtains state-of-the-art results across time series\nbenchmarks for long dependencies in sequential image classification, real-world\nhealthcare regression tasks, and speech. On a difficult speech classification\ntask with length-16000 sequences, LSSL outperforms prior approaches by 24\naccuracy points, and even outperforms baselines that use hand-crafted features\non 100x shorter sequences.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Albert Gu",
      "Isys Johnson",
      "Karan Goel",
      "Khaled Saab",
      "Tri Dao",
      "Atri Rudra",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.13985"
  },
  {
    "id": "arXiv:2110.13986",
    "title": "Fair Sequential Selection Using Supervised Learning Models",
    "abstract": "We consider a selection problem where sequentially arrived applicants apply\nfor a limited number of positions/jobs. At each time step, a decision maker\naccepts or rejects the given applicant using a pre-trained supervised learning\nmodel until all the vacant positions are filled. In this paper, we discuss\nwhether the fairness notions (e.g., equal opportunity, statistical parity,\netc.) that are commonly used in classification problems are suitable for the\nsequential selection problems. In particular, we show that even with a\npre-trained model that satisfies the common fairness notions, the selection\noutcomes may still be biased against certain demographic groups. This\nobservation implies that the fairness notions used in classification problems\nare not suitable for a selection problem where the applicants compete for a\nlimited number of positions. We introduce a new fairness notion, ``Equal\nSelection (ES),'' suitable for sequential selection problems and propose a\npost-processing approach to satisfy the ES fairness notion. We also consider a\nsetting where the applicants have privacy concerns, and the decision maker only\nhas access to the noisy version of sensitive attributes. In this setting, we\ncan show that the perfect ES fairness can still be attained under certain\nconditions.",
    "descriptor": "\nComments: This paper has been accepted for publication in the 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Mohammad Mahdi Khalili",
      "Xueru Zhang",
      "Mahed Abroshan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.13986"
  },
  {
    "id": "arXiv:2110.13987",
    "title": "Learning Collaborative Policies to Solve NP-hard Routing Problems",
    "abstract": "Recently, deep reinforcement learning (DRL) frameworks have shown potential\nfor solving NP-hard routing problems such as the traveling salesman problem\n(TSP) without problem-specific expert knowledge. Although DRL can be used to\nsolve complex problems, DRL frameworks still struggle to compete with\nstate-of-the-art heuristics showing a substantial performance gap. This paper\nproposes a novel hierarchical problem-solving strategy, termed learning\ncollaborative policies (LCP), which can effectively find the near-optimum\nsolution using two iterative DRL policies: the seeder and reviser. The seeder\ngenerates as diversified candidate solutions as possible (seeds) while being\ndedicated to exploring over the full combinatorial action space (i.e., sequence\nof assignment action). To this end, we train the seeder's policy using a simple\nyet effective entropy regularization reward to encourage the seeder to find\ndiverse solutions. On the other hand, the reviser modifies each candidate\nsolution generated by the seeder; it partitions the full trajectory into\nsub-tours and simultaneously revises each sub-tour to minimize its traveling\ndistance. Thus, the reviser is trained to improve the candidate solution's\nquality, focusing on the reduced solution space (which is beneficial for\nexploitation). Extensive experiments demonstrate that the proposed two-policies\ncollaboration scheme improves over single-policy DRL framework on various\nNP-hard routing problems, including TSP, prize collecting TSP (PCTSP), and\ncapacitated vehicle routing problem (CVRP).",
    "descriptor": "\nComments: NeurIPS 2021, 23 pages, 8 figures\n",
    "authors": [
      "Minsu Kim",
      "Jinkyoo Park",
      "Joungho Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.13987"
  },
  {
    "id": "arXiv:2110.13989",
    "title": "Revisiting Batch Normalization",
    "abstract": "Batch normalization (BN) is comprised of a normalization component followed\nby an affine transformation and has become essential for training deep neural\nnetworks. Standard initialization of each BN in a network sets the affine\ntransformation scale and shift to 1 and 0, respectively. However, after\ntraining we have observed that these parameters do not alter much from their\ninitialization. Furthermore, we have noticed that the normalization process can\nstill yield overly large values, which is undesirable for training. We revisit\nthe BN formulation and present a new initialization method and update approach\nfor BN to address the aforementioned issues. Experimental results using the\nproposed alterations to BN show statistically significant performance gains in\na variety of scenarios. The approach can be used with existing implementations\nat no additional computational cost. We also present a new online BN-based\ninput data normalization technique to alleviate the need for other offline or\nfixed methods. Source code is available at\nhttps://github.com/osu-cvl/revisiting-bn.",
    "descriptor": "",
    "authors": [
      "Jim Davis",
      "Logan Frank"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.13989"
  },
  {
    "id": "arXiv:2110.13992",
    "title": "Leveraging Local Temporal Information for Multimodal Scene  Classification",
    "abstract": "Robust video scene classification models should capture the spatial\n(pixel-wise) and temporal (frame-wise) characteristics of a video effectively.\nTransformer models with self-attention which are designed to get contextualized\nrepresentations for individual tokens given a sequence of tokens, are becoming\nincreasingly popular in many computer vision tasks. However, the use of\nTransformer based models for video understanding is still relatively\nunexplored. Moreover, these models fail to exploit the strong temporal\nrelationships between the neighboring video frames to get potent frame-level\nrepresentations. In this paper, we propose a novel self-attention block that\nleverages both local and global temporal relationships between the video frames\nto obtain better contextualized representations for the individual frames. This\nenables the model to understand the video at various granularities. We\nillustrate the performance of our models on the large scale YoutTube-8M data\nset on the task of video categorization and further analyze the results to\nshowcase improvement.",
    "descriptor": "",
    "authors": [
      "Saurabh Sahu",
      "Palash Goyal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13992"
  },
  {
    "id": "arXiv:2110.13995",
    "title": "Software Engineering Meets Systems Engineering: Conceptual Modeling  Applied to Engineering Operations",
    "abstract": "Models are fundamentally crucial to many scientific fields, including\nsoftware engineering, systems engineering, enterprise modeling, and business\nmodeling. This paper focuses on diagrammatic conceptual modeling, as opposed to\nmathematical or computational models, wherein a conceptual model is a\ntranslation of reality processes into an abstract mechanism that has similar\nstructure and parallel events of the external processes. Although various\nmodeling approaches exist, including UML (Unified Modeling Language) in\nsoftware engineering and its dialect, SysML (System Modeling Language), in\nsystems engineering, several difficulties arise in such models, including the\nproblem of model multiplicity that is related to the lack an integrated view of\nstructure and behavior. This paper generalizes conceptual modeling to be\napplied in organizations at large. According to authorities, the so-called\norganization theory portrays organizations as machine-like systems. As a\nmachine, an organization coordinates its parts to transform inputs into\noutputs. Therefore, we synthesize the notion of an organization as a machine\nand apply a new modeling methodology called thinging machine (TM) to real\nengineering operations. The results show the viability of the TM methodology\nserving as a foundation for high-level modelling of systems.",
    "descriptor": "\nComments: 14 pages, 10 figures\n",
    "authors": [
      "Sabah Al-Fedaghi",
      "Mahdi Modhaffar"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.13995"
  },
  {
    "id": "arXiv:2110.13996",
    "title": "Controllable Data Augmentation Through Deep Relighting",
    "abstract": "At the heart of the success of deep learning is the quality of the data.\nThrough data augmentation, one can train models with better generalization\ncapabilities and thus achieve greater results in their field of interest. In\nthis work, we explore how to augment a varied set of image datasets through\nrelighting so as to improve the ability of existing models to be invariant to\nillumination changes, namely for learned descriptors. We develop a tool, based\non an encoder-decoder network, that is able to quickly generate multiple\nvariations of the illumination of various input scenes whilst also allowing the\nuser to define parameters such as the angle of incidence and intensity. We\ndemonstrate that by training models on datasets that have been augmented with\nour pipeline, it is possible to achieve higher performance on localization\nbenchmarks.",
    "descriptor": "",
    "authors": [
      "George Chogovadze",
      "R\u00e9mi Pautrat",
      "Marc Pollefeys"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13996"
  },
  {
    "id": "arXiv:2110.13998",
    "title": "Efficient Learning and Decoding of the Continuous-Time Hidden Markov  Model for Disease Progression Modeling",
    "abstract": "The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to\nmodeling disease progression due to its ability to describe noisy observations\narriving irregularly in time. However, the lack of an efficient parameter\nlearning algorithm for CT-HMM restricts its use to very small models or\nrequires unrealistic constraints on the state transitions. In this paper, we\npresent the first complete characterization of efficient EM-based learning\nmethods for CT-HMM models, as well as the first solution to decoding the\noptimal state transition sequence and the corresponding state dwelling time. We\nshow that EM-based learning consists of two challenges: the estimation of\nposterior state probabilities and the computation of end-state conditioned\nstatistics. We solve the first challenge by reformulating the estimation\nproblem as an equivalent discrete time-inhomogeneous hidden Markov model. The\nsecond challenge is addressed by adapting three distinct approaches from the\ncontinuous time Markov chain (CTMC) literature to the CT-HMM domain.\nAdditionally, we further improve the efficiency of the most efficient method by\na factor of the number of states. Then, for decoding, we incorporate a\nstate-of-the-art method from the (CTMC) literature, and extend the end-state\nconditioned optimal state sequence decoding to the CT-HMM case with the\ncomputation of the expected state dwelling time. We demonstrate the use of\nCT-HMMs with more than 100 states to visualize and predict disease progression\nusing a glaucoma dataset and an Alzheimer's disease dataset, and to decode and\nvisualize the most probable state transition trajectory for individuals on the\nglaucoma dataset, which helps to identify progressing phenotypes in a\ncomprehensive way. Finally, we apply the CT-HMM modeling and decoding strategy\nto investigate the progression of language acquisition and development.",
    "descriptor": "",
    "authors": [
      "Yu-Ying Liu",
      "Alexander Moreno",
      "Maxwell A. Xu",
      "Shuang Li",
      "Jena C. McDaniel",
      "Nancy C. Brady",
      "Agata Rozga",
      "Fuxin Li",
      "Le Song",
      "James M. Rehg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.13998"
  },
  {
    "id": "arXiv:2110.13999",
    "title": "Exploring the Role of Machine Learning in Scientific Workflows:  Opportunities and Challenges",
    "abstract": "In this survey, we discuss the challenges of executing scientific workflows\nas well as existing Machine Learning (ML) techniques to alleviate those\nchallenges. We provide the context and motivation for applying ML to each step\nof the execution of these workflows. Furthermore, we provide recommendations on\nhow to extend ML techniques to unresolved challenges in the execution of\nscientific workflows. Moreover, we discuss the possibility of using ML\ntechniques for in-situ operations. We explore the challenges of in-situ\nworkflows and provide suggestions for improving the performance of their\nexecution using ML techniques.",
    "descriptor": "",
    "authors": [
      "Azita Nouri",
      "Philip E. Davis",
      "Pradeep Subedi",
      "Manish Parashar"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.13999"
  },
  {
    "id": "arXiv:2110.14000",
    "title": "Towards Hyperparameter-free Policy Selection for Offline Reinforcement  Learning",
    "abstract": "How to select between policies and value functions produced by different\ntraining algorithms in offline reinforcement learning (RL) -- which is crucial\nfor hyperpa-rameter tuning -- is an important open question. Existing\napproaches based on off-policy evaluation (OPE) often require additional\nfunction approximation and hence hyperparameters, creating a chicken-and-egg\nsituation. In this paper, we design hyperparameter-free algorithms for policy\nselection based on BVFT [XJ21], a recent theoretical advance in value-function\nselection, and demonstrate their effectiveness in discrete-action benchmarks\nsuch as Atari. To address performance degradation due to poor critics in\ncontinuous-action domains, we further combine BVFT with OPE to get the best of\nboth worlds, and obtain a hyperparameter-tuning method for Q-function based OPE\nwith theoretical guarantees as a side product.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Siyuan Zhang",
      "Nan Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14000"
  },
  {
    "id": "arXiv:2110.14001",
    "title": "SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event  Data",
    "abstract": "We study the problem of inferring heterogeneous treatment effects from\ntime-to-event data. While both the related problems of (i) estimating treatment\neffects for binary or continuous outcomes and (ii) predicting survival outcomes\nhave been well studied in the recent machine learning literature, their\ncombination -- albeit of high practical relevance -- has received considerably\nless attention. With the ultimate goal of reliably estimating the effects of\ntreatments on instantaneous risk and survival probabilities, we focus on the\nproblem of learning (discrete-time) treatment-specific conditional hazard\nfunctions. We find that unique challenges arise in this context due to a\nvariety of covariate shift issues that go beyond a mere combination of\nwell-studied confounding and censoring biases. We theoretically analyse their\neffects by adapting recent generalization bounds from domain adaptation and\ntreatment effect estimation to our setting and discuss implications for model\ndesign. We use the resulting insights to propose a novel deep learning method\nfor treatment-specific hazard estimation based on balancing representations. We\ninvestigate performance across a range of experimental settings and empirically\nconfirm that our method outperforms baselines by addressing covariate shifts\nfrom various sources.",
    "descriptor": "\nComments: To Appear in the Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Alicia Curth",
      "Changhee Lee",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14001"
  },
  {
    "id": "arXiv:2110.14002",
    "title": "CARMS: Categorical-Antithetic-REINFORCE Multi-Sample Gradient Estimator",
    "abstract": "Accurately backpropagating the gradient through categorical variables is a\nchallenging task that arises in various domains, such as training discrete\nlatent variable models. To this end, we propose CARMS, an unbiased estimator\nfor categorical random variables based on multiple mutually negatively\ncorrelated (jointly antithetic) samples. CARMS combines REINFORCE with copula\nbased sampling to avoid duplicate samples and reduce its variance, while\nkeeping the estimator unbiased using importance sampling. It generalizes both\nthe ARMS antithetic estimator for binary variables, which is CARMS for two\ncategories, as well as LOORF/VarGrad, the leave-one-out REINFORCE estimator,\nwhich is CARMS with independent samples. We evaluate CARMS on several benchmark\ndatasets on a generative modeling task, as well as a structured output\nprediction task, and find it to outperform competing methods including a strong\nself-control baseline. The code is publicly available.",
    "descriptor": "",
    "authors": [
      "Alek Dimitriev",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14002"
  },
  {
    "id": "arXiv:2110.14003",
    "title": "Connected greedy colourings of perfect graphs and other classes: the  good, the bad and the ugly",
    "abstract": "The Grundy number of a graph is the maximum number of colours used by the\n``First-Fit'' greedy colouring algorithm over all vertex orderings. Given a\nvertex ordering $\\sigma= v_1,\\dots,v_n$, the ``First-Fit'' greedy colouring\nalgorithm colours the vertices in the order of $\\sigma$ by assigning to each\nvertex the smallest colour unused in its neighbourhood.\nBy restricting this procedure to vertex orderings that are connected, we\nobtain {\\em connected greedy colourings}. For some graphs, all connected greedy\ncolourings use exactly $\\chi(G)$ colours; they are called {\\em good graphs}. On\nthe opposite, some graphs do not admit any connected greedy colouring using\nonly $\\chi(G)$ colours; they are called {\\em ugly graphs}.\nWe show that no perfect graph is ugly. We also give simple proofs of this\nfact for subclasses of perfect graphs (block graphs, comparability graphs), and\nshow that no $K_4$-minor free graph is ugly.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Laurent Beaudou",
      "Caroline Brosse",
      "Oscar Defrain",
      "Florent Foucaud",
      "Aur\u00e9lie Lagoutte",
      "Vincent Limouzy",
      "Lucas Pastor"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.14003"
  },
  {
    "id": "arXiv:2110.14007",
    "title": "TOD: Tensor-based Outlier Detection",
    "abstract": "To scale outlier detection (OD) to large-scale, high-dimensional datasets, we\npropose TOD, a novel system that abstracts OD algorithms into basic tensor\noperations for efficient GPU acceleration. To make TOD highly efficient in both\ntime and space, we leverage recent advances in deep learning infrastructure in\nboth hardware and software. To deploy large OD applications on GPUs with\nlimited memory, we introduce two key techniques. First, provable quantization\naccelerates OD computation and reduces the memory requirement by performing\nspecific OD computations in lower precision while provably guaranteeing no\naccuracy loss. Second, to exploit the aggregated compute resources and memory\ncapacity of multiple GPUs, we introduce automatic batching, which decomposes OD\ncomputations into small batches that can be executed on multiple GPUs in\nparallel.\nTOD supports a comprehensive set of OD algorithms and utility functions.\nExtensive evaluation on both real and synthetic OD datasets shows that TOD is\non average 11.9X faster than the state-of-the-art comprehensive OD system PyOD,\nand takes less than an hour to detect outliers within a million samples. TOD\nenables straightforward integration for additional OD algorithms and provides a\nunified framework for combining classical OD algorithms with deep learning\nmethods. These combinations result in an infinite number of OD methods, many of\nwhich are novel and can be easily prototyped in TOD.",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Yue Zhao",
      "George H. Chen",
      "Zhihao Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.14007"
  },
  {
    "id": "arXiv:2110.14010",
    "title": "MisConv: Convolutional Neural Networks for Missing Data",
    "abstract": "Processing of missing data by modern neural networks, such as CNNs, remains a\nfundamental, yet unsolved challenge, which naturally arises in many practical\napplications, like image inpainting or autonomous vehicles and robots. While\nimputation-based techniques are still one of the most popular solutions, they\nfrequently introduce unreliable information to the data and do not take into\naccount the uncertainty of estimation, which may be destructive for a machine\nlearning model. In this paper, we present MisConv, a general mechanism, for\nadapting various CNN architectures to process incomplete images. By modeling\nthe distribution of missing values by the Mixture of Factor Analyzers, we cover\nthe spectrum of possible replacements and find an analytical formula for the\nexpected value of convolution operator applied to the incomplete image. The\nwhole framework is realized by matrix operations, which makes MisConv extremely\nefficient in practice. Experiments performed on various image processing tasks\ndemonstrate that MisConv achieves superior or comparable performance to the\nstate-of-the-art methods.",
    "descriptor": "\nComments: Accepted for publication at WACV 2022 Conference\n",
    "authors": [
      "Marcin Przewi\u0119\u017alikowski",
      "Marek \u015amieja",
      "\u0141ukasz Struski",
      "Jacek Tabor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14010"
  },
  {
    "id": "arXiv:2110.14011",
    "title": "Cluster-and-Conquer: A Framework For Time-Series Forecasting",
    "abstract": "We propose a three-stage framework for forecasting high-dimensional\ntime-series data. Our method first estimates parameters for each univariate\ntime series. Next, we use these parameters to cluster the time series. These\nclusters can be viewed as multivariate time series, for which we then compute\nparameters. The forecasted values of a single time series can depend on the\nhistory of other time series in the same cluster, accounting for intra-cluster\nsimilarity while minimizing potential noise in predictions by ignoring\ninter-cluster effects. Our framework -- which we refer to as\n\"cluster-and-conquer\" -- is highly general, allowing for any time-series\nforecasting and clustering method to be used in each step. It is\ncomputationally efficient and embarrassingly parallel. We motivate our\nframework with a theoretical analysis in an idealized mixed linear regression\nsetting, where we provide guarantees on the quality of the estimates. We\naccompany these guarantees with experimental results that demonstrate the\nadvantages of our framework: when instantiated with simple linear\nautoregressive models, we are able to achieve state-of-the-art results on\nseveral benchmark datasets, sometimes outperforming deep-learning-based\napproaches.",
    "descriptor": "\nComments: 25 pages, 3 figures\n",
    "authors": [
      "Reese Pathak",
      "Rajat Sen",
      "Nikhil Rao",
      "N. Benjamin Erichson",
      "Michael I. Jordan",
      "Inderjit S. Dhillon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14011"
  },
  {
    "id": "arXiv:2110.14019",
    "title": "Reliable and Trustworthy Machine Learning for Health Using Dataset Shift  Detection",
    "abstract": "Unpredictable ML model behavior on unseen data, especially in the health\ndomain, raises serious concerns about its safety as repercussions for mistakes\ncan be fatal. In this paper, we explore the feasibility of using\nstate-of-the-art out-of-distribution detectors for reliable and trustworthy\ndiagnostic predictions. We select publicly available deep learning models\nrelating to various health conditions (e.g., skin cancer, lung sound, and\nParkinson's disease) using various input data types (e.g., image, audio, and\nmotion data). We demonstrate that these models show unreasonable predictions on\nout-of-distribution datasets. We show that Mahalanobis distance- and Gram\nmatrices-based out-of-distribution detection methods are able to detect\nout-of-distribution data with high accuracy for the health models that operate\non different modalities. We then translate the out-of-distribution score into a\nhuman interpretable CONFIDENCE SCORE to investigate its effect on the users'\ninteraction with health ML applications. Our user study shows that the\n\\textsc{confidence score} helped the participants only trust the results with a\nhigh score to make a medical decision and disregard results with a low score.\nThrough this work, we demonstrate that dataset shift is a critical piece of\ninformation for high-stake ML applications, such as medical diagnosis and\nhealthcare, to provide reliable and trustworthy predictions to the users.",
    "descriptor": "\nComments: Neu\n",
    "authors": [
      "Chunjong Park",
      "Anas Awadalla",
      "Tadayoshi Kohno",
      "Shwetak Patel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14019"
  },
  {
    "id": "arXiv:2110.14020",
    "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning",
    "abstract": "Learning to act from observational data without active environmental\ninteraction is a well-known challenge in Reinforcement Learning (RL). Recent\napproaches involve constraints on the learned policy or conservative updates,\npreventing strong deviations from the state-action distribution of the dataset.\nAlthough these methods are evaluated using non-linear function approximation,\ntheoretical justifications are mostly limited to the tabular or linear cases.\nGiven the impressive results of deep reinforcement learning, we argue for a\nneed to more clearly understand the challenges in this setting.\nIn the vein of Held & Hein's classic 1963 experiment, we propose the \"tandem\nlearning\" experimental paradigm which facilitates our empirical analysis of the\ndifficulties in offline reinforcement learning. We identify function\napproximation in conjunction with fixed data distributions as the strongest\nfactors, thereby extending but also challenging hypotheses stated in past work.\nOur results provide relevant insights for offline deep reinforcement learning,\nwhile also shedding new light on phenomena observed in the online case of\nlearning control.",
    "descriptor": "\nComments: Accepted paper at NeurIPS 2021\n",
    "authors": [
      "Georg Ostrovski",
      "Pablo Samuel Castro",
      "Will Dabney"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14020"
  },
  {
    "id": "arXiv:2110.14030",
    "title": "Improving Local Effectiveness for Global robust training",
    "abstract": "Despite its popularity, deep neural networks are easily fooled. To alleviate\nthis deficiency, researchers are actively developing new training strategies,\nwhich encourage models that are robust to small input perturbations. Several\nsuccessful robust training methods have been proposed. However, many of them\nrely on strong adversaries, which can be prohibitively expensive to generate\nwhen the input dimension is high and the model structure is complicated. We\nadopt a new perspective on robustness and propose a novel training algorithm\nthat allows a more effective use of adversaries. Our method improves the model\nrobustness at each local ball centered around an adversary and then, by\ncombining these local balls through a global term, achieves overall robustness.\nWe demonstrate that, by maximizing the use of adversaries via focusing on local\nballs, we achieve high robust accuracy with weak adversaries. Specifically, our\nmethod reaches a similar robust accuracy level to the state of the art\napproaches trained on strong adversaries on MNIST, CIFAR-10 and CIFAR-100. As a\nresult, the overall training time is reduced. Furthermore, when trained with\nstrong adversaries, our method matches with the current state of the art on\nMNIST and outperforms them on CIFAR-10 and CIFAR-100.",
    "descriptor": "",
    "authors": [
      "Jingyue Lu",
      "M. Pawan Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14030"
  },
  {
    "id": "arXiv:2110.14031",
    "title": "Surrogate Regret Bounds for Polyhedral Losses",
    "abstract": "Surrogate risk minimization is an ubiquitous paradigm in supervised machine\nlearning, wherein a target problem is solved by minimizing a surrogate loss on\na dataset. Surrogate regret bounds, also called excess risk bounds, are a\ncommon tool to prove generalization rates for surrogate risk minimization.\nWhile surrogate regret bounds have been developed for certain classes of loss\nfunctions, such as proper losses, general results are relatively sparse. We\nprovide two general results. The first gives a linear surrogate regret bound\nfor any polyhedral (piecewise-linear and convex) surrogate, meaning that\nsurrogate generalization rates translate directly to target rates. The second\nshows that for sufficiently non-polyhedral surrogates, the regret bound is a\nsquare root, meaning fast surrogate generalization rates translate to slow\nrates for the target. Together, these results suggest polyhedral surrogates are\noptimal in many cases.",
    "descriptor": "\nComments: Appears in NeurIPS 2021\n",
    "authors": [
      "Rafael Frongillo",
      "Bo Waggoner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14031"
  },
  {
    "id": "arXiv:2110.14032",
    "title": "MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the  Edge",
    "abstract": "Recently, a new trend of exploring sparsity for accelerating neural network\ntraining has emerged, embracing the paradigm of training on the edge. This\npaper proposes a novel Memory-Economic Sparse Training (MEST) framework\ntargeting for accurate and fast execution on edge devices. The proposed MEST\nframework consists of enhancements by Elastic Mutation (EM) and Soft Memory\nBound (&S) that ensure superior accuracy at high sparsity ratios. Different\nfrom the existing works for sparse training, this current work reveals the\nimportance of sparsity schemes on the performance of sparse training in terms\nof accuracy as well as training speed on real edge devices. On top of that, the\npaper proposes to employ data efficiency for further acceleration of sparse\ntraining. Our results suggest that unforgettable examples can be identified\nin-situ even during the dynamic exploration of sparsity masks in the sparse\ntraining process, and therefore can be removed for further training speedup on\nedge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our\nMEST increases Top-1 accuracy significantly on ImageNet when using the same\nunstructured sparsity scheme. Systematical evaluation on accuracy, training\nspeed, and memory footprint are conducted, where the proposed MEST framework\nconsistently outperforms representative SOTA works. A reviewer strongly against\nour work based on his false assumptions and misunderstandings. On top of the\nprevious submission, we employ data efficiency for further acceleration of\nsparse training. And we explore the impact of model sparsity, sparsity schemes,\nand sparse training algorithms on the number of removable training examples.\nOur codes are publicly available at: https://github.com/boone891214/MEST.",
    "descriptor": "\nComments: NeurIPS 2021 Spotlight Paper\n",
    "authors": [
      "Geng Yuan",
      "Xiaolong Ma",
      "Wei Niu",
      "Zhengang Li",
      "Zhenglun Kong",
      "Ning Liu",
      "Yifan Gong",
      "Zheng Zhan",
      "Chaoyang He",
      "Qing Jin",
      "Siyue Wang",
      "Minghai Qin",
      "Bin Ren",
      "Yanzhi Wang",
      "Sijia Liu",
      "Xue Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.14032"
  },
  {
    "id": "arXiv:2110.14037",
    "title": "Revisiting the Performance of iALS on Item Recommendation Benchmarks",
    "abstract": "Matrix factorization learned by implicit alternating least squares (iALS) is\na popular baseline in recommender system research publications. iALS is known\nto be one of the most computationally efficient and scalable collaborative\nfiltering methods. However, recent studies suggest that its prediction quality\nis not competitive with the current state of the art, in particular\nautoencoders and other item-based collaborative filtering methods. In this\nwork, we revisit the iALS algorithm and present a bag of tricks that we found\nuseful when applying iALS. We revisit four well-studied benchmarks where iALS\nwas reported to perform poorly and show that with proper tuning, iALS is highly\ncompetitive and outperforms any method on at least half of the comparisons. We\nhope that these high quality results together with iALS's known scalability\nspark new interest in applying and further improving this decade old technique.",
    "descriptor": "",
    "authors": [
      "Steffen Rendle",
      "Walid Krichene",
      "Li Zhang",
      "Yehuda Koren"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14037"
  },
  {
    "id": "arXiv:2110.14038",
    "title": "Robustness of Graph Neural Networks at Scale",
    "abstract": "Graph Neural Networks (GNNs) are increasingly important given their\npopularity and the diversity of applications. Yet, existing studies of their\nvulnerability to adversarial attacks rely on relatively small graphs. We\naddress this gap and study how to attack and defend GNNs at scale. We propose\ntwo sparsity-aware first-order optimization attacks that maintain an efficient\nrepresentation despite optimizing over a number of parameters which is\nquadratic in the number of nodes. We show that common surrogate losses are not\nwell-suited for global attacks on GNNs. Our alternatives can double the attack\nstrength. Moreover, to improve GNNs' reliability we design a robust aggregation\nfunction, Soft Median, resulting in an effective defense at all scales. We\nevaluate our attacks and defense with standard GNNs on graphs more than 100\ntimes larger compared to previous work. We even scale one order of magnitude\nfurther by extending our techniques to a scalable GNN.",
    "descriptor": "\nComments: 39 pages, 22 figures, 17 tables NeurIPS 2021\n",
    "authors": [
      "Simon Geisler",
      "Tobias Schmidt",
      "Hakan \u015eirin",
      "Daniel Z\u00fcgner",
      "Aleksandar Bojchevski",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14038"
  },
  {
    "id": "arXiv:2110.14040",
    "title": "Finding the Best Partitioning Policy for Efficient Verification of  Autonomous Systems at Runtime",
    "abstract": "The autonomous systems need to decide how to react to the changes at runtime\nefficiently. The ability to rigorously analyze the environment and the system\ntogether is theoretically possible by the model-driven approaches; however, the\nmodel size and timing limitations are two significant obstacles against such an\nautonomous decision-making process. To tackle this issue, the incremental\napproximation technique can be used to partition the model and only verify a\npartition if it is affected by the change. This paper proposes a policy-based\nanalysis approach that finds the best partitioning policy among a set of\navailable policies based on two proposed metrics, namely Balancing and\nVariation. The metrics quantitatively evaluate the generated components from\nthe incremental approximation scheme according to their size and frequency. We\ninvestigate the validity of the approach both theoretically and experimentally\nvia a case study on energy harvesting systems. The results confirm the\neffectiveness of the proposed approach.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Melika Dastranj",
      "Mehran Alidoost Nia",
      "Mehdi Kargahi"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.14040"
  },
  {
    "id": "arXiv:2110.14042",
    "title": "A Cost-Effective, Scalable, and Portable IoT Data Infrastructure for  Indoor Environment Sensing",
    "abstract": "The vast number of facility management systems, home automation systems, and\nthe ever-increasing number of Internet of Things (IoT) devices are in constant\nneed of environmental monitoring. Indoor environment data can be utilized to\nimprove indoor facilities and better occupants' working and living experience,\nhowever, such data are scarce because many existing facility monitoring\ntechnologies are expensive and proprietary for certain building systems, such\nas building automation systems, energy management systems, and maintenance\nsystems. In this work, the authors designed and prototyped a cost-effective,\ndistributed, scalable, and portable indoor environmental data collection\nsystem, Building Data Lite (BDL). BDL is based on Raspberry Pi computers and\nmultiple changeable arrays of sensors, such as sensors of temperature,\nhumidity, light, motion, sound, vibration, and multiple types of gases. The\nsystem includes a distributed sensing network and a centralized server. The\nserver provides a web-based graphical user interface that enables users to\naccess the collected data over the Internet. To evaluate the BDL system's\nfunctionality, cost-effectiveness, scalability, and portability, the research\nteam conducted a case study in an affordable housing community where the system\nprototype is deployed to 12 households. The case study results indicate that\nthe system is functioning as designed, costs about \\$3500 to sense 48 building\nzones (about \\$73 per zone) and provides 12 types of indoor environment data,\nis easy to scale up, and is fully portable.",
    "descriptor": "\nComments: 32 pages, 15 figures\n",
    "authors": [
      "Sheik Anik",
      "Xinghua Gao",
      "Na Meng",
      "Philip Agee",
      "Andrew McCoy"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14042"
  },
  {
    "id": "arXiv:2110.14043",
    "title": "Fragment-Based Test Generation For Web Apps",
    "abstract": "Automated model-based test generation presents a viable alternative to the\ncostly manual test creation currently employed for regression testing of web\napps. However, existing model inference techniques rely on threshold-based\nwhole-page comparison to establish state equivalence, which cannot reliably\nidentify near-duplicate web pages in modern web apps. Consequently, existing\ntechniques produce inadequate models for dynamic web apps, and fragile test\noracles, rendering the generated regression test suites ineffective. We propose\na model-based test generation technique, FRAGGEN, that eliminates the need for\nthresholds, by employing a novel state abstraction based on page fragmentation\nto establish state equivalence. FRAGGEN also uses fine-grained page fragment\nanalysis to diversify state exploration and generate reliable test oracles. Our\nevaluation shows that FRAGGEN improves the precision and recall of the inferred\nmodels by 70% and 62% respectively, and generates reliable regression test\nsuites with test actions that have nearly 100% success rate on the same version\nof the web app even if the execution environment is varied. The test oracles\ngenerated by FRAGGEN can detect 98.7% of the visible changes in web pages while\nbeing highly tolerant to web app dynamism, making them suitable for regression\ntesting.",
    "descriptor": "\nComments: 12 pages with 11 figures and 8 tables. Under Review at IEEE Transactions on Software Engineering (IEEE TSE)\n",
    "authors": [
      "Rahulkrishna Yandrapally",
      "Ali Mesbah"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.14043"
  },
  {
    "id": "arXiv:2110.14044",
    "title": "iALS++: Speeding up Matrix Factorization with Subspace Optimization",
    "abstract": "iALS is a popular algorithm for learning matrix factorization models from\nimplicit feedback with alternating least squares. This algorithm was invented\nover a decade ago but still shows competitive quality compared to recent\napproaches like VAE, EASE, SLIM, or NCF. Due to a computational trick that\navoids negative sampling, iALS is very efficient especially for large item\ncatalogues. However, iALS does not scale well with large embedding dimensions,\nd, due to its cubic runtime dependency on d. Coordinate descent variations,\niCD, have been proposed to lower the complexity to quadratic in d. In this\nwork, we show that iCD approaches are not well suited for modern processors and\ncan be an order of magnitude slower than a careful iALS implementation for\nsmall to mid scale embedding sizes (d ~ 100) and only perform better than iALS\non large embeddings d ~ 1000. We propose a new solver iALS++ that combines the\nadvantages of iALS in terms of vector processing with a low computational\ncomplexity as in iCD. iALS++ is an order of magnitude faster than iCD both for\nsmall and large embedding dimensions. It can solve benchmark problems like\nMovielens 20M or Million Song Dataset even for 1000 dimensional embedding\nvectors in a few minutes.",
    "descriptor": "",
    "authors": [
      "Steffen Rendle",
      "Walid Krichene",
      "Li Zhang",
      "Yehuda Koren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.14044"
  },
  {
    "id": "arXiv:2110.14045",
    "title": "The Computational Complexity of Finding Arithmetic Expressions With and  Without Parentheses",
    "abstract": "We show NP-completeness for various problems about the existence of\narithmetic expression trees. When given a set of operations, inputs, and a\ntarget value does there exist an expression tree with those inputs and\noperations that evaluates to the target? We consider the variations where the\nstructure of the tree is also given and the variation where no parentheses are\nallowed in the expression.",
    "descriptor": "\nComments: 14 pages, 1 figure\n",
    "authors": [
      "Jayson Lynch",
      "Weng"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2110.14045"
  },
  {
    "id": "arXiv:2110.14048",
    "title": "Conflict-Averse Gradient Descent for Multi-task Learning",
    "abstract": "The goal of multi-task learning is to enable more efficient learning than\nsingle task learning by sharing model structures for a diverse set of tasks. A\nstandard multi-task learning objective is to minimize the average loss across\nall tasks. While straightforward, using this objective often results in much\nworse final performance for each task than learning them independently. A major\nchallenge in optimizing a multi-task model is the conflicting gradients, where\ngradients of different task objectives are not well aligned so that following\nthe average gradient direction can be detrimental to specific tasks'\nperformance. Previous work has proposed several heuristics to manipulate the\ntask gradients for mitigating this problem. But most of them lack convergence\nguarantee and/or could converge to any Pareto-stationary point. In this paper,\nwe introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the\naverage loss function, while leveraging the worst local improvement of\nindividual tasks to regularize the algorithm trajectory. CAGrad balances the\nobjectives automatically and still provably converges to a minimum over the\naverage loss. It includes the regular gradient descent (GD) and the multiple\ngradient descent algorithm (MGDA) in the multi-objective optimization (MOO)\nliterature as special cases. On a series of challenging multi-task supervised\nlearning and reinforcement learning tasks, CAGrad achieves improved performance\nover prior state-of-the-art multi-objective gradient manipulation methods.",
    "descriptor": "\nComments: 20 pages, 6 figures, Conference on Neural Information Processing Systems, 2021\n",
    "authors": [
      "Bo Liu",
      "Xingchao Liu",
      "Xiaojie Jin",
      "Peter Stone",
      "Qiang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14048"
  },
  {
    "id": "arXiv:2110.14049",
    "title": "Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for  Machine Learning",
    "abstract": "Data Shapley has recently been proposed as a principled framework to quantify\nthe contribution of individual datum in machine learning. It can effectively\nidentify helpful or harmful data points for a learning algorithm. In this\npaper, we propose Beta Shapley, which is a substantial generalization of Data\nShapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the\nShapley value, which is not critical for machine learning settings. Beta\nShapley unifies several popular data valuation methods and includes data\nShapley as a special case. Moreover, we prove that Beta Shapley has several\ndesirable statistical properties and propose efficient algorithms to estimate\nit. We demonstrate that Beta Shapley outperforms state-of-the-art data\nvaluation methods on several downstream ML tasks such as: 1) detecting\nmislabeled training data; 2) learning with subsamples; and 3) identifying\npoints whose addition or removal have the largest positive or negative impact\non the model.",
    "descriptor": "\nComments: 24 pages, 13 figures\n",
    "authors": [
      "Yongchan Kwon",
      "James Zou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14049"
  },
  {
    "id": "arXiv:2110.14051",
    "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution  Detection: Solutions and Future Challenges",
    "abstract": "Machine learning models often encounter samples that are diverged from the\ntraining distribution. Failure to recognize an out-of-distribution (OOD)\nsample, and consequently assign that sample to an in-class label significantly\ncompromises the reliability of a model. The problem has gained significant\nattention due to its importance for safety deploying models in open-world\nsettings. Detecting OOD samples is challenging due to the intractability of\nmodeling all possible unknown distributions. To date, several research domains\ntackle the problem of detecting unfamiliar samples, including anomaly\ndetection, novelty detection, one-class learning, open set recognition, and\nout-of-distribution detection. Despite having similar and shared concepts,\nout-of-distribution, open-set, and anomaly detection have been investigated\nindependently. Accordingly, these research avenues have not cross-pollinated,\ncreating research barriers. While some surveys intend to provide an overview of\nthese approaches, they seem to only focus on a specific domain without\nexamining the relationship between different domains. This survey aims to\nprovide a cross-domain and comprehensive review of numerous eminent works in\nrespective areas while identifying their commonalities. Researchers can benefit\nfrom the overview of research advances in different fields and develop future\nmethodology synergistically. Furthermore, to the best of our knowledge, while\nthere are surveys in anomaly detection or one-class learning, there is no\ncomprehensive or up-to-date survey on out-of-distribution detection, which our\nsurvey covers extensively. Finally, having a unified cross-domain perspective,\nwe discuss and shed light on future lines of research, intending to bring these\nfields closer together.",
    "descriptor": "",
    "authors": [
      "Mohammadreza Salehi",
      "Hossein Mirzaei",
      "Dan Hendrycks",
      "Yixuan Li",
      "Mohammad Hossein Rohban",
      "Mohammad Sabokrou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14051"
  },
  {
    "id": "arXiv:2110.14053",
    "title": "Improving SAT Solving with Graph Neural Networks",
    "abstract": "Propositional satisfiability (SAT) is an NP-complete problem that impacts\nmany research fields, such as planning, verification, and security. Despite the\nremarkable success of modern SAT solvers, scalability still remains a\nchallenge. Main stream modern SAT solvers are based on the Conflict-Driven\nClause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers\nby improving its variable branching heuristics through predictions generated by\nGraph Neural Networks (GNNs). However, so far this approach either has not made\nsolving more effective, or has required frequent online accesses to substantial\nGPU resources. Aiming to make GNN improvements practical, this paper proposes\nan approach called NeuroComb, which builds on two insights: (1) predictions of\nimportant variables and clauses can be combined with dynamic branching into a\nmore effective hybrid branching strategy, and (2) it is sufficient to query the\nneural model only once for the predictions before the SAT solving starts.\nImplemented as an enhancement to the classic MiniSat solver, NeuroComb allowed\nit to solve 18.5% more problems on the recent SATCOMP-2020 competition problem\nset. NeuroComb is therefore a practical approach to improving SAT solving\nthrough modern machine learning.",
    "descriptor": "",
    "authors": [
      "Wenxi Wang",
      "Yang Hu",
      "Mohit Tiwari",
      "Sarfraz Khurshid",
      "Kenneth McMillan",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14053"
  },
  {
    "id": "arXiv:2110.14055",
    "title": "Polynomial-Spline Neural Networks with Exact Integrals",
    "abstract": "Using neural networks to solve variational problems, and other scientific\nmachine learning tasks, has been limited by a lack of consistency and an\ninability to exactly integrate expressions involving neural network\narchitectures. We address these limitations by formulating a novel neural\nnetwork architecture that combines a polynomial mixture-of-experts model with\nfree knot B1-spline basis functions. Effectively, our architecture performs\npiecewise polynomial approximation on each cell of a trainable partition of\nunity. Our architecture exhibits both $h$- and $p$- refinement for regression\nproblems at the convergence rates expected from approximation theory, allowing\nfor consistency in solving variational problems. Moreover, this architecture,\nits moments, and its partial derivatives can all be integrated exactly,\nobviating a reliance on sampling or quadrature and enabling error-free\ncomputation of variational forms. We demonstrate the success of our network on\na range of regression and variational problems that illustrate the consistency\nand exact integrability of our network architecture.",
    "descriptor": "\nComments: 8 pages + 4 pages Technical Appendix. Contact authors regarding supplementary multimedia and code appendices\n",
    "authors": [
      "Jonas A. Actor",
      "Andy Huang",
      "Nathaniel Trask"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14055"
  },
  {
    "id": "arXiv:2110.14056",
    "title": "How to transfer algorithmic reasoning knowledge to learn new algorithms?",
    "abstract": "Learning to execute algorithms is a fundamental problem that has been widely\nstudied. Prior work~\\cite{veli19neural} has shown that to enable systematic\ngeneralisation on graph algorithms it is critical to have access to the\nintermediate steps of the program/algorithm. In many reasoning tasks, where\nalgorithmic-style reasoning is important, we only have access to the input and\noutput examples. Thus, inspired by the success of pre-training on similar tasks\nor data in Natural Language Processing (NLP) and Computer Vision, we set out to\nstudy how we can transfer algorithmic reasoning knowledge. Specifically, we\ninvestigate how we can use algorithms for which we have access to the execution\ntrace to learn to solve similar tasks for which we do not. We investigate two\nmajor classes of graph algorithms, parallel algorithms such as breadth-first\nsearch and Bellman-Ford and sequential greedy algorithms such as Prim and\nDijkstra. Due to the fundamental differences between algorithmic reasoning\nknowledge and feature extractors such as used in Computer Vision or NLP, we\nhypothesise that standard transfer techniques will not be sufficient to achieve\nsystematic generalisation. To investigate this empirically we create a dataset\nincluding 9 algorithms and 3 different graph types. We validate this\nempirically and show how instead multi-task learning can be used to achieve the\ntransfer of algorithmic reasoning knowledge.",
    "descriptor": "",
    "authors": [
      "Louis-Pascal A. C. Xhonneux",
      "Andreea Deac",
      "Petar Velickovic",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14056"
  },
  {
    "id": "arXiv:2110.14057",
    "title": "Meta-learning with an Adaptive Task Scheduler",
    "abstract": "To benefit the learning of a new task, meta-learning has been proposed to\ntransfer a well-generalized meta-model learned from various meta-training\ntasks. Existing meta-learning algorithms randomly sample meta-training tasks\nwith a uniform probability, under the assumption that tasks are of equal\nimportance. However, it is likely that tasks are detrimental with noise or\nimbalanced given a limited number of meta-training tasks. To prevent the\nmeta-model from being corrupted by such detrimental tasks or dominated by tasks\nin the majority, in this paper, we propose an adaptive task scheduler (ATS) for\nthe meta-training process. In ATS, for the first time, we design a neural\nscheduler to decide which meta-training tasks to use next by predicting the\nprobability being sampled for each candidate task, and train the scheduler to\noptimize the generalization capacity of the meta-model to unseen tasks. We\nidentify two meta-model-related factors as the input of the neural scheduler,\nwhich characterize the difficulty of a candidate task to the meta-model.\nTheoretically, we show that a scheduler taking the two factors into account\nimproves the meta-training loss and also the optimization landscape. Under the\nsetting of meta-learning with noise and limited budgets, ATS improves the\nperformance on both miniImageNet and a real-world drug discovery benchmark by\nup to 13% and 18%, respectively, compared to state-of-the-art task schedulers.",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Huaxiu Yao",
      "Yu Wang",
      "Ying Wei",
      "Peilin Zhao",
      "Mehrdad Mahdavi",
      "Defu Lian",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14057"
  },
  {
    "id": "arXiv:2110.14060",
    "title": "Argo Scholar: Interactive Visual Exploration of Literature in Browsers",
    "abstract": "Discovering and making sense of relevant research literature is fundamental\nto becoming knowledgeable in any scientific discipline. Visualization can aid\nthis process; however, existing tools' adoption and impact have often been\nconstrained, such as by their reliance on small curated paper datasets that\nquickly become outdated or a lack of support for personalized exploration. We\nintroduce Argo Scholar, an open-source, web-based visualization tool for\ninteractive exploration of literature and easy sharing of exploration results.\nArgo Scholar queries and visualizes Semantic Scholar's live data of almost 200\nmillion papers, enabling users to generate personalized literature exploration\nresults in real-time through flexible, incremental exploration, a common and\neffective method for researchers to discover relevant work. Our tool allows\nusers to easily share their literature exploration results as a URL or\nweb-embedded IFrame application. Argo Scholar is open-sourced and available at\nhttps://poloclub.github.io/argo-scholar/.",
    "descriptor": "\nComments: IEEE VIS 2021\n",
    "authors": [
      "Kevin Li",
      "Haoyang Yang",
      "Anish Upadhayay",
      "Zhiyan Zhou",
      "Jon Saad-Falcon",
      "Duen Horng Chau"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.14060"
  },
  {
    "id": "arXiv:2110.14064",
    "title": "How will electric vehicles affect traffic congestion and energy  consumption: an integrated modelling approach",
    "abstract": "This paper explores the impact of electric vehicles (EVs) on traffic\ncongestion and energy consumption by proposing an integrated bi-level framework\ncomprising of: a) a dynamic micro-scale traffic simulation suitable for\nmodelling current and hypothetical traffic and charging demand scenarios and b)\na queue model for capturing the impact of fast charging station use, informed\nby traffic flows, travel distances, availability of charging infrastructure and\nestimated vehicle battery state of charge. To the best of our knowledge, this\npaper represents the first integrated analysis of potential traffic congestion\nand energy infrastructure impacts linked to EV uptake, based on real traffic\nflows and the placement and design of existing fast-charging infrastructure.\nResults showcase that the integrated queue-energy-transport modelling framework\ncan predict correctly the limitations of the EV infrastructure as well as the\ntraffic congestion evolution. The modelling approach identifies concrete pain\npoints to be addressed in both traffic and energy management and planning. The\ncode for this project can be found at :\nhttps://github.com/Future-Mobility-Lab/EV-charging-impact",
    "descriptor": "",
    "authors": [
      "Artur Grigorev",
      "Tuo Mao",
      "Adam Berry",
      "Joachim Tan",
      "Loki Purushothaman",
      "Adriana-Simona Mihaita"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14064"
  },
  {
    "id": "arXiv:2110.14066",
    "title": "Model Reduction of Swing Equations with Physics Informed PDE",
    "abstract": "This manuscript is the first step towards building a robust and efficient\nmodel reduction methodology to capture transient dynamics in a transmission\nlevel electric power system. Such dynamics is normally modeled on\nseconds-to-tens-of-seconds time scales by the so-called swing equations, which\nare ordinary differential equations defined on a spatially discrete model of\nthe power grid. We suggest, following Seymlyen (1974) and Thorpe, Seyler and\nPhadke (1999), to map the swing equations onto a linear, inhomogeneous Partial\nDifferential Equation (PDE) of parabolic type in two space and one time\ndimensions with time-independent coefficients and properly defined boundary\nconditions. The continuous two-dimensional spatial domain is defined by a\ngeographical map of the area served by the power grid, and associated with the\nPDE coefficients derived from smoothed graph-Laplacian of susceptances, machine\ninertia and damping. Inhomogeneous source terms represent spatially distributed\ninjection/consumption of power. We illustrate our method on PanTaGruEl\n(Pan-European Transmission Grid and ELectricity generation model). We show\nthat, when properly coarse-grained, i.e. with the PDE coefficients and source\nterms extracted from a spatial convolution procedure of the respective discrete\ncoefficients in the swing equations, the resulting PDE reproduces faithfully\nand efficiently the original swing dynamics. We finally discuss future\nextensions of this work, where the presented PDE-based reduced modeling will\ninitialize a physics-informed machine learning approach for real-time modeling,\n$n-1$ feasibility assessment and transient stability analysis of power systems.",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "Laurent Pagnier",
      "Michael Chertkov",
      "Julian Fritzsch",
      "Philippe Jacquod"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.14066"
  },
  {
    "id": "arXiv:2110.14068",
    "title": "Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are  Found within Randomly Initialized Networks",
    "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to adversarial\nattacks, i.e., an imperceptible perturbation to the input can mislead DNNs\ntrained on clean images into making erroneous predictions. To tackle this,\nadversarial training is currently the most effective defense method, by\naugmenting the training set with adversarial samples generated on the fly.\nInterestingly, we discover for the first time that there exist subnetworks with\ninborn robustness, matching or surpassing the robust accuracy of the\nadversarially trained networks with comparable model sizes, within randomly\ninitialized networks without any model training, indicating that adversarial\ntraining on model weights is not indispensable towards adversarial robustness.\nWe name such subnetworks Robust Scratch Tickets (RSTs), which are also by\nnature efficient. Distinct from the popular lottery ticket hypothesis, neither\nthe original dense networks nor the identified RSTs need to be trained. To\nvalidate and understand this fascinating finding, we further conduct extensive\nexperiments to study the existence and properties of RSTs under different\nmodels, datasets, sparsity patterns, and attacks, drawing insights regarding\nthe relationship between DNNs' robustness and their\ninitialization/overparameterization. Furthermore, we identify the poor\nadversarial transferability between RSTs of different sparsity ratios drawn\nfrom the same randomly initialized dense network, and propose a Random RST\nSwitch (R2S) technique, which randomly switches between different RSTs, as a\nnovel defense method built on top of RSTs. We believe our findings about RSTs\nhave opened up a new perspective to study model robustness and extend the\nlottery ticket hypothesis.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Yonggan Fu",
      "Qixuan Yu",
      "Yang Zhang",
      "Shang Wu",
      "Xu Ouyang",
      "David Cox",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14068"
  },
  {
    "id": "arXiv:2110.14074",
    "title": "Fault-Tolerant Federated Reinforcement Learning with Theoretical  Guarantee",
    "abstract": "The growing literature of Federated Learning (FL) has recently inspired\nFederated Reinforcement Learning (FRL) to encourage multiple agents to\nfederatively build a better decision-making policy without sharing raw\ntrajectories. Despite its promising applications, existing works on FRL fail to\nI) provide theoretical analysis on its convergence, and II) account for random\nsystem failures and adversarial attacks. Towards this end, we propose the first\nFRL framework the convergence of which is guaranteed and tolerant to less than\nhalf of the participating agents being random system failures or adversarial\nattackers. We prove that the sample efficiency of the proposed framework is\nguaranteed to improve with the number of agents and is able to account for such\npotential failures or attacks. All theoretical results are empirically verified\non various RL benchmark tasks.",
    "descriptor": "\nComments: Accepted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Extended version with proofs and additional experimental details and results, 27 pages\n",
    "authors": [
      "Flint Xiaofeng Fan",
      "Yining Ma",
      "Zhongxiang Dai",
      "Wei Jing",
      "Cheston Tan",
      "Bryan Kian Hsiang Low"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14074"
  },
  {
    "id": "arXiv:2110.14076",
    "title": "CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud  Registration",
    "abstract": "We study the problem of extracting correspondences between a pair of point\nclouds for registration. For correspondence retrieval, existing works benefit\nfrom matching sparse keypoints detected from dense points but usually struggle\nto guarantee their repeatability. To address this issue, we present CoFiNet -\nCoarse-to-Fine Network which extracts hierarchical correspondences from coarse\nto fine without keypoint detection. On a coarse scale and guided by a weighting\nscheme, our model firstly learns to match down-sampled nodes whose vicinity\npoints share more overlap, which significantly shrinks the search space of a\nconsecutive stage. On a finer scale, node proposals are consecutively expanded\nto patches that consist of groups of points together with associated\ndescriptors. Point correspondences are then refined from the overlap areas of\ncorresponding patches, by a density-adaptive matching module capable to deal\nwith varying point density. Extensive evaluation of CoFiNet on both indoor and\noutdoor standard benchmarks shows our superiority over existing methods.\nEspecially on 3DLoMatch where point clouds share less overlap, CoFiNet\nsignificantly outperforms state-of-the-art approaches by at least 5% on\nRegistration Recall, with at most two-third of their parameters.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Hao Yu",
      "Fu Li",
      "Mahdi Saleh",
      "Benjamin Busam",
      "Slobodan Ilic"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14076"
  },
  {
    "id": "arXiv:2110.14078",
    "title": "Adversarial Online Learning with Variable Plays in the Pursuit-Evasion  Game: Theoretical Foundations and Application in Connected and Automated  Vehicle Cybersecurity",
    "abstract": "We extend the adversarial/non-stochastic multi-play multi-armed bandit\n(MPMAB) to the case where the number of arms to play is variable. The work is\nmotivated by the fact that the resources allocated to scan different critical\nlocations in an interconnected transportation system change dynamically over\ntime and depending on the environment. By modeling the malicious hacker and the\nintrusion monitoring system as the attacker and the defender, respectively, we\nformulate the problem for the two players as a sequential pursuit-evasion game.\nWe derive the condition under which a Nash equilibrium of the strategic game\nexists. For the defender side, we provide an exponential-weighted based\nalgorithm with sublinear pseudo-regret. We further extend our model to\nheterogeneous rewards for both players, and obtain lower and upper bounds on\nthe average reward for the attacker. We provide numerical experiments to\ndemonstrate the effectiveness of a variable-arm play.",
    "descriptor": "\nComments: Published in IEEE Access. DOI: 10.1109/ACCESS.2021.3120700\n",
    "authors": [
      "Yiyang Wang",
      "Neda Masoud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14078"
  },
  {
    "id": "arXiv:2110.14081",
    "title": "A Controlled Experiment of Different Code Representations for  Learning-Based Bug Repair",
    "abstract": "Training a deep learning model on source code has gained significant traction\nrecently. Since such models reason about vectors of numbers, source code needs\nto be converted to a code representation and then will be transformed into\nvectors. Numerous approaches have been proposed to represent source code, from\nsequences of tokens to abstract syntax trees. However, there is no systematic\nstudy to understand the effect of code representation on learning performance.\nThrough a controlled experiment, we examine the impact of various code\nrepresentations on model accuracy and usefulness in learning-based program\nrepair. We train 21 different models, including 14 different homogeneous code\nrepresentations, four mixed representations for the buggy and fixed code, and\nthree different embeddings. We also conduct a user study to qualitatively\nevaluate the usefulness of inferred fixes in different code representations.\nOur results highlight the importance of code representation and its impact on\nlearning and usefulness. Our findings indicate that (1) while code abstractions\nhelp the learning process, they can adversely impact the usefulness of inferred\nfixes from a developer's point of view; this emphasizes the need to look at the\npatches generated from the practitioner's perspective, which is often neglected\nin the literature, (2) mixed representations can outperform homogeneous code\nrepresentations, (3) bug type can affect the effectiveness of different code\nrepresentations; although current techniques use a single code representation\nfor all bug types, there is no single best code representation applicable to\nall bug types.",
    "descriptor": "",
    "authors": [
      "Marjane Namavar",
      "Noor Nashid",
      "Ali Mesbah"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.14081"
  },
  {
    "id": "arXiv:2110.14090",
    "title": "Teardown and feasibility study of IronKey -- the most secure USB Flash  drive",
    "abstract": "There are many solutions for protecting user data on USB Flash drives.\nHowever, the family of IronKey devices was designed with the highest security\nexpectations. They are definitely standing above others by being certified to\nFIPS 140-2 Level 3 and also claimed as certified by NATO for Top-Secret use.\nMany encrypted USB drives had been evaluated and found insecure, however, no\npublic research on IronKey devices was made. This feasibility study fills the\ngap by looking inside the IronKey family of devices. As a result the users of\nthe IronKey devices could be assured about the real level of the security\nprotection they get. Several generations of devices from IronKey family and\ncompetitors are teared down, their hardware solutions discussed and evaluated\nfor possible attacks. Some potential flaws are exposed and those findings are\nlikely to stimulate further research into specific solutions aimed to protect\nuser data.",
    "descriptor": "\nComments: 16 pages, 73 figures, Hardwear.IO conference 28-29 October 2021, Hague, Netherlands\n",
    "authors": [
      "Sergei Skorobogatov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.14090"
  },
  {
    "id": "arXiv:2110.14091",
    "title": "Connect-the-Dots: Bridging Semantics between Words and Definitions via  Aligning Word Sense Inventories",
    "abstract": "Word Sense Disambiguation (WSD) aims to automatically identify the exact\nmeaning of one word according to its context. Existing supervised models\nstruggle to make correct predictions on rare word senses due to limited\ntraining data and can only select the best definition sentence from one\npredefined word sense inventory (e.g., WordNet). To address the data sparsity\nproblem and generalize the model to be independent of one predefined inventory,\nwe propose a gloss alignment algorithm that can align definition sentences\n(glosses) with the same meaning from different sense inventories to collect\nrich lexical knowledge. We then train a model to identify semantic equivalence\nbetween a target word in context and one of its glosses using these aligned\ninventories, which exhibits strong transfer capability to many WSD tasks.\nExperiments on benchmark datasets show that the proposed method improves\npredictions on both frequent and rare word senses, outperforming prior work by\n1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on\nWiC Task also indicates that our method can better capture word meanings in\ncontext.",
    "descriptor": "\nComments: 11 pages. Accepted to the main conference of EMNLP 2021 (long paper)\n",
    "authors": [
      "Wenlin Yao",
      "Xiaoman Pan",
      "Lifeng Jin",
      "Jianshu Chen",
      "Dian Yu",
      "Dong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14091"
  },
  {
    "id": "arXiv:2110.14092",
    "title": "BioGrad: Biologically Plausible Gradient-Based Learning for Spiking  Neural Networks",
    "abstract": "Spiking neural networks (SNN) are delivering energy-efficient, massively\nparallel, and low-latency solutions to AI problems, facilitated by the emerging\nneuromorphic chips. To harness these computational benefits, SNN need to be\ntrained by learning algorithms that adhere to brain-inspired neuromorphic\nprinciples, namely event-based, local, and online computations. Yet, the\nstate-of-the-art SNN training algorithms are based on backprop that does not\nfollow the above principles. Due to its limited biological plausibility, the\napplication of backprop to SNN requires non-local feedback pathways for\ntransmitting continuous-valued errors, and relies on gradients from future\ntimesteps. The introduction of biologically plausible modifications to backprop\nhas helped overcome several of its limitations, but limits the degree to which\nbackprop is approximated, which hinders its performance. We propose a\nbiologically plausible gradient-based learning algorithm for SNN that is\nfunctionally equivalent to backprop, while adhering to all three neuromorphic\nprinciples. We introduced multi-compartment spiking neurons with local\neligibility traces to compute the gradients required for learning, and a\nperiodic \"sleep\" phase to further improve the approximation to backprop during\nwhich a local Hebbian rule aligns the feedback and feedforward weights. Our\nmethod achieved the same level of performance as backprop with multi-layer\nfully connected SNN on MNIST (98.13%) and the event-based N-MNIST (97.59%)\ndatasets. We deployed our learning algorithm on Intel's Loihi to train a\n1-hidden-layer network for MNIST, and obtained 93.32% test accuracy while\nconsuming 400 times less energy per training sample than BioGrad on GPU. Our\nwork shows that optimal learning is feasible in neuromorphic computing, and\nfurther pursuing its biological plausibility can better capture the benefits of\nthis emerging computing paradigm.",
    "descriptor": "\nComments: 14 pages, 6 figures\n",
    "authors": [
      "Guangzhi Tang",
      "Neelesh Kumar",
      "Ioannis Polykretis",
      "Konstantinos P. Michmizos"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.14092"
  },
  {
    "id": "arXiv:2110.14094",
    "title": "Learning-Augmented $k$-means Clustering",
    "abstract": "$k$-means clustering is a well-studied problem due to its wide applicability.\nUnfortunately, there exist strong theoretical limits on the performance of any\nalgorithm for the $k$-means problem on worst-case inputs. To overcome this\nbarrier, we consider a scenario where \"advice\" is provided to help perform\nclustering. Specifically, we consider the $k$-means problem augmented with a\npredictor that, given any point, returns its cluster label in an approximately\noptimal clustering up to some, possibly adversarial, error. We present an\nalgorithm whose performance improves along with the accuracy of the predictor,\neven though na\\\"{i}vely following the accurate predictor can still lead to a\nhigh clustering cost. Thus if the predictor is sufficiently accurate, we can\nretrieve a close to optimal clustering with nearly optimal runtime, breaking\nknown computational barriers for algorithms that do not have access to such\nadvice. We evaluate our algorithms on real datasets and show significant\nimprovements in the quality of clustering.",
    "descriptor": "",
    "authors": [
      "Jon Ergun",
      "Zhili Feng",
      "Sandeep Silwal",
      "David P. Woodruff",
      "Samson Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14094"
  },
  {
    "id": "arXiv:2110.14096",
    "title": "Towards Robust Bisimulation Metric Learning",
    "abstract": "Learned representations in deep reinforcement learning (DRL) have to extract\ntask-relevant information from complex observations, balancing between\nrobustness to distraction and informativeness to the policy. Such stable and\nrich representations, often learned via modern function approximation\ntechniques, can enable practical application of the policy improvement theorem,\neven in high-dimensional continuous state-action spaces. Bisimulation metrics\noffer one solution to this representation learning problem, by collapsing\nfunctionally similar states together in representation space, which promotes\ninvariance to noise and distractors. In this work, we generalize value function\napproximation bounds for on-policy bisimulation metrics to non-optimal policies\nand approximate environment dynamics. Our theoretical results help us identify\nembedding pathologies that may occur in practical use. In particular, we find\nthat these issues stem from an underconstrained dynamics model and an unstable\ndependence of the embedding norm on the reward signal in environments with\nsparse rewards. Further, we propose a set of practical remedies: (i) a norm\nconstraint on the representation space, and (ii) an extension of prior\napproaches with intrinsic rewards and latent space regularization. Finally, we\nprovide evidence that the resulting method is not only more robust to sparse\nreward functions, but also able to solve challenging continuous control tasks\nwith observational distractions, where prior methods fail.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Mete Kemertas",
      "Tristan Aumentado-Armstrong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14096"
  },
  {
    "id": "arXiv:2110.14097",
    "title": "Automated Evaluation of Web Site Accessibility Using A Dynamic  Accessibility Measurement Crawler",
    "abstract": "Achieving accessibility compliance is extremely important for many government\nagencies and businesses who wish to improve services for their consumers. With\nthe growing reliance on dynamic web applications many organizations are finding\nit difficult to implement accessibility standards, often due to the inability\nof current automated testing tools to test the stateful environments created by\ndynamic web applications. In this paper, we present mathematical foundations\nand theory for the Demodocus framework and prototype, and outline its approach\nto using web science, web crawling,and accessibility testing to automatically\nnavigate and test interactive content for accessibility. Our approach simulates\nthe page interactions of users with and without disabilities, and compares\ngraphs of reachable states from these simulations to determine both the\naccessibility and the difficulty of content access for these different users.",
    "descriptor": "\nComments: This version has been removed by arXiv administrators as the submitter did not have the authority to grant the license applied at the time of submission\n",
    "authors": [
      "Trevor Bostic",
      "Jeffrey Stanley",
      "John Higgins",
      "Daniel Chudnov",
      "Justin Brunelle",
      "Brittany Tracy"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.14097"
  },
  {
    "id": "arXiv:2110.14098",
    "title": "Provable Lifelong Learning of Representations",
    "abstract": "In lifelong learning, the tasks (or classes) to be learned arrive\nsequentially over time in arbitrary order. During training, knowledge from\nprevious tasks can be captured and transferred to subsequent ones to improve\nsample efficiency. We consider the setting where all target tasks can be\nrepresented in the span of a small number of unknown linear or nonlinear\nfeatures of the input data. We propose a provable lifelong learning algorithm\nthat maintains and refines the internal feature representation. We prove that\nfor any desired accuracy on all tasks, the dimension of the representation\nremains close to that of the underlying representation. The resulting sample\ncomplexity improves significantly on existing bounds. In the setting of linear\nfeatures, our algorithm is provably efficient and the sample complexity for\ninput dimension $d$, $m$ tasks with $k$ features up to error $\\epsilon$ is\n$\\tilde{O}(dk^{1.5}/\\epsilon+km/\\epsilon)$. We also prove a matching lower\nbound for any lifelong learning algorithm that uses a single task learner as a\nblack box. Finally, we complement our analysis with an empirical study.",
    "descriptor": "\nComments: Working paper (30 pages, 6 figures)\n",
    "authors": [
      "Xinyuan Cao",
      "Weiyang Liu",
      "Santosh S. Vempala"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14098"
  },
  {
    "id": "arXiv:2110.14109",
    "title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic  Objectives with Skewed Hessian Spectrums",
    "abstract": "Learning rate schedulers have been widely adopted in training deep neural\nnetworks. Despite their practical importance, there is a discrepancy between\nits practice and its theoretical analysis. For instance, it is not known what\nschedules of SGD achieve best convergence, even for simple problems such as\noptimizing quadratic objectives. So far, step decay has been one of the\nstrongest candidates under this setup, which is proved to be nearly optimal\nwith a $\\cO(\\log T)$ gap. However, according to our analysis, this gap turns\nout to be $\\Omega(\\log T)$ in a wide range of settings, which throws the\nschedule optimality problem into an open question again. Towards answering this\nreopened question, in this paper, we propose Eigencurve, the first family of\nlearning rate schedules that can achieve minimax optimal convergence rates (up\nto a constant) for SGD on quadratic objectives when the eigenvalue distribution\nof the underlying Hessian matrix is skewed. The condition is quite common in\npractice. Experimental results show that Eigencurve can significantly\noutperform step decay in image classification tasks on CIFAR-10, especially\nwhen the number of epochs is small. Moreover, the theory inspires two simple\nlearning rate schedulers for practical applications that can approximate\nEigencurve. For some problems, the optimal shape of the proposed schedulers\nresembles that of cosine decay, which sheds light to the success of cosine\ndecay for such situations. For other situations, the proposed schedulers are\nsuperior to cosine decay.",
    "descriptor": "",
    "authors": [
      "Rui Pan",
      "Haishan Ye",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.14109"
  },
  {
    "id": "arXiv:2110.14110",
    "title": "Mining frequency-based sequential trajectory co-clusters",
    "abstract": "Co-clustering is a specific type of clustering that addresses the problem of\nfinding groups of objects without necessarily considering all attributes. This\ntechnique has shown to have more consistent results in high-dimensional sparse\ndata than traditional clustering. In trajectory co-clustering, the methods\nfound in the literature have two main limitations: first, the space and time\ndimensions have to be constrained by user-defined thresholds; second, elements\n(trajectory points) are clustered ignoring the trajectory sequence, assuming\nthat the points are independent among them. To address the limitations above,\nwe propose a new trajectory co-clustering method for mining semantic trajectory\nco-clusters. It simultaneously clusters the trajectories and their elements\ntaking into account the order in which they appear. This new method uses the\nelement frequency to identify candidate co-clusters. Besides, it uses an\nobjective cost function that automatically drives the co-clustering process,\navoiding the need for constraining dimensions. We evaluate the proposed\napproach using real-world a publicly available dataset. The experimental\nresults show that our proposal finds frequent and meaningful contiguous\nsequences revealing mobility patterns, thereby the most relevant elements.",
    "descriptor": "\nComments: 10 pages, 2 tables, and 1 algorithm\n",
    "authors": [
      "Yuri Santos",
      "J\u00f4nata Tyska",
      "Vania Bogorny"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14110"
  },
  {
    "id": "arXiv:2110.14112",
    "title": "A Bayesian Receiver with Improved Complexity-Reliability Trade-off in  Massive MIMO Systems",
    "abstract": "The stringent requirements on reliability and processing delay in the\nfifth-generation ($5$G) cellular networks introduce considerable challenges in\nthe design of massive multiple-input-multiple-output (M-MIMO) receivers. The\ntwo main components of an M-MIMO receiver are a detector and a decoder. To\nimprove the trade-off between reliability and complexity, a Bayesian concept\nhas been considered as a promising approach that enhances classical detectors,\ne.g. minimum-mean-square-error detector. This work proposes an iterative M-MIMO\ndetector based on a Bayesian framework, a parallel interference cancellation\nscheme, and a decision statistics combining concept. We then develop a high\nperformance M-MIMO receiver, integrating the proposed detector with a low\ncomplexity sequential decoding for polar codes. Simulation results of the\nproposed detector show a significant performance gain compared to other low\ncomplexity detectors. Furthermore, the proposed M-MIMO receiver with sequential\ndecoding ensures one order magnitude lower complexity compared to a receiver\nwith stack successive cancellation decoding for polar codes from the 5G New\nRadio standard.",
    "descriptor": "",
    "authors": [
      "Alva Kosasih",
      "Vera Miloslavskaya",
      "Wibowo Hardjawana",
      "Changyang She",
      "Chao-Kai Wen",
      "Branka Vucetic"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14112"
  },
  {
    "id": "arXiv:2110.14118",
    "title": "Object-Aware Regularization for Addressing Causal Confusion in Imitation  Learning",
    "abstract": "Behavioral cloning has proven to be effective for learning sequential\ndecision-making policies from expert demonstrations. However, behavioral\ncloning often suffers from the causal confusion problem where a policy relies\non the noticeable effect of expert actions due to the strong correlation but\nnot the cause we desire. This paper presents Object-aware REgularizatiOn\n(OREO), a simple technique that regularizes an imitation policy in an\nobject-aware manner. Our main idea is to encourage a policy to uniformly attend\nto all semantic objects, in order to prevent the policy from exploiting\nnuisance variables strongly correlated with expert actions. To this end, we\nintroduce a two-stage approach: (a) we extract semantic objects from images by\nutilizing discrete codes from a vector-quantized variational autoencoder, and\n(b) we randomly drop the units that share the same discrete code together,\ni.e., masking out semantic objects. Our experiments demonstrate that OREO\nsignificantly improves the performance of behavioral cloning, outperforming\nvarious other regularization and causality-based methods on a variety of Atari\nenvironments and a self-driving CARLA environment. We also show that our method\neven outperforms inverse reinforcement learning methods trained with a\nconsiderable amount of environment interaction.",
    "descriptor": "\nComments: NeurIPS 2021. First two authors contributed equally\n",
    "authors": [
      "Jongjin Park",
      "Younggyo Seo",
      "Chang Liu",
      "Li Zhao",
      "Tao Qin",
      "Jinwoo Shin",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14118"
  },
  {
    "id": "arXiv:2110.14120",
    "title": "ScaleCert: Scalable Certified Defense against Adversarial Patches with  Sparse Superficial Layers",
    "abstract": "Adversarial patch attacks that craft the pixels in a confined region of the\ninput images show their powerful attack effectiveness in physical environments\neven with noises or deformations. Existing certified defenses towards\nadversarial patch attacks work well on small images like MNIST and CIFAR-10\ndatasets, but achieve very poor certified accuracy on higher-resolution images\nlike ImageNet. It is urgent to design both robust and effective defenses\nagainst such a practical and harmful attack in industry-level larger images. In\nthis work, we propose the certified defense methodology that achieves high\nprovable robustness for high-resolution images and largely improves the\npracticality for real adoption of the certified defense. The basic insight of\nour work is that the adversarial patch intends to leverage localized\nsuperficial important neurons (SIN) to manipulate the prediction results.\nHence, we leverage the SIN-based DNN compression techniques to significantly\nimprove the certified accuracy, by reducing the adversarial region searching\noverhead and filtering the prediction noises. Our experimental results show\nthat the certified accuracy is increased from 36.3% (the state-of-the-art\ncertified detection) to 60.4% on the ImageNet dataset, largely pushing the\ncertified defenses for practical use.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Husheng Han",
      "Kaidi Xu",
      "Xing Hu",
      "Xiaobing Chen",
      "Ling Liang",
      "Zidong Du",
      "Qi Guo",
      "Yanzhi Wang",
      "Yunji Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14120"
  },
  {
    "id": "arXiv:2110.14123",
    "title": "Newtonian Mechanics Based Transient Stability PART V: Inner-group  Machine",
    "abstract": "This paper analyzes the mechanisms of the inner-group machine. It is first\nclarified that the inner-group machine is created from the difference between\nthe equivalent system and the original system. The inner-group machine\nstability is analyzed based on the machine paradigms. In particular, strict\ncorrelation between the inner-group machine trajectory and the inner-group\nmachine transient energy conversion is established through the I-CR system\nmodeling. Then, the transient characteristics of the inner-group machine are\nanalyzed. It is clarified that the inner-group motions might be inseparable or\nseparable, and the inner-group machine DLP will occur later than the EDLP and\nIDLP. Simulation results show that the severity of the original system cannot\nbe simply evaluated through its equivalent system once any inner-group motion\nbecomes fierce.",
    "descriptor": "\nComments: Index Terms: Transient stability, transient energy, equal area criterion, individual machine, inner-group motion. This paper contains 11 pages and 23 figures\n",
    "authors": [
      "Songyan Wang",
      "Jilai Yu",
      "Aoife Foley",
      "Jingrui Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14123"
  },
  {
    "id": "arXiv:2110.14124",
    "title": "An improved multiobjective evolutionary algorithm based on decomposition  and adaptive multi-reference points",
    "abstract": "Many real-world optimization problems such as engineering design are finally\nmodeled as a multiobjective optimization problem (MOP) which must be solved to\nget a set of trade-offs. Multiobjective evolutionary algorithm based on\ndecomposition (MOEA/D) has been regarded as a very promising approach for\nsolving MOPs, which offers a general algorithmic framework of evolutionary\nmultiobjective optimization. Recent studies have shown that MOEA/D with\nuniformly distributed weight vectors is well-suited to MOPs with regular Pareto\noptimal front, but its performance in terms of diversity deteriorates on MOPs\nwith irregular Pareto optimal front such as highly nonlinear and convex. In\nthis way, the solution set obtained by the algorithm can not provide more\nreasonable choices for decision makers. In order to efficiently overcome this\nshortcoming, in this paper, we propose an improved MOEA/D algorithm by virtue\nof the well-known Pascoletti-Serafini scalarization method and a new strategy\nof multi-reference points. Specifically, this strategy consists of the setting\nand adaptation of reference points generated by the techniques of equidistant\npartition and projection. For performance assessment, the proposed algorithm is\ncompared with existing four state-of-the-art multiobjective evolutionary\nalgorithms on both benchmark test problems with various types of Pareto optimal\nfronts and two real-world MOPs including the hatch cover design and the rocket\ninjector design in engineering optimization. Experimental results reveal that\nthe proposed algorithm is better than that of the other compared algorithms in\ndiversity.",
    "descriptor": "",
    "authors": [
      "Wang Chen",
      "Jian Chen",
      "Weitian Wu",
      "Xinmin Yang",
      "Hui Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.14124"
  },
  {
    "id": "arXiv:2110.14131",
    "title": "Temporal Knowledge Distillation for On-device Audio Classification",
    "abstract": "Improving the performance of on-device audio classification models remains a\nchallenge given the computational limits of the mobile environment. Many\nstudies leverage knowledge distillation to boost predictive performance by\ntransferring the knowledge from large models to on-device models. However, most\nlack the essence of the temporal information which is crucial to audio\nclassification tasks, or similar architecture is often required. In this paper,\nwe propose a new knowledge distillation method designed to incorporate the\ntemporal knowledge embedded in attention weights of large models to on-device\nmodels. Our distillation method is applicable to various types of\narchitectures, including the non-attention-based architectures such as CNNs or\nRNNs, without any architectural change during inference. Through extensive\nexperiments on both an audio event detection dataset and a noisy keyword\nspotting dataset, we show that our proposed method improves the predictive\nperformance across diverse on-device architectures.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Kwanghee Choi",
      "Martin Kersner",
      "Jacob Morton",
      "Buru Chang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14131"
  },
  {
    "id": "arXiv:2110.14137",
    "title": "Interactive Affordance Learning through Manipulation Relationship Graph  Construction",
    "abstract": "In this paper, we propose Manipulation Relationship Graph (MRG), a novel\naffordance representation which captures the underlying manipulation\nrelationships of an arbitrary scene. To construct such graph from raw visual\nobservation, a deep nerual network named AffRel is introduced. It consists of a\nAttribute and Context module, which guide the relationship learning at instance\nand subgraph level respectively. We quantitatively validate our method on a\nnovel manipulation relationship dataset named SMRD dataset. To evaluate the\nperformance of proposed model and representation, both visual and physical\nexperiments are conducted. Overall, AffRel along with MRG outperforms all\nbaselines, achieving the success rate of 88.89% on task relationship\nrecognition (TRR) and 73.33% on task completion (TC). It demonstrates its\nsuperior capability to reason about the affordance in an interactive way for\nthe purpose of robotic manipulation.",
    "descriptor": "",
    "authors": [
      "Chao Tang",
      "Jingwen Yu",
      "Weinan Chen",
      "Hong Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14137"
  },
  {
    "id": "arXiv:2110.14138",
    "title": "A Linear Bayesian Learning Receiver Scheme for Massive MIMO Systems",
    "abstract": "Much stringent reliability and processing latency requirements in\nultra-reliable-low-latency-communication (URLLC) traffic make the design of\nlinear massive multiple-input-multiple-output (M-MIMO) receivers becomes very\nchallenging. Recently, Bayesian concept has been used to increase the detection\nreliability in minimum-mean-square-error (MMSE) linear receivers. However, the\nlatency processing time is a major concern due to the exponential complexity of\nmatrix inversion operations in MMSE schemes. This paper proposes an iterative\nM-MIMO receiver that is developed by using a Bayesian concept and a parallel\ninterference cancellation (PIC) scheme, referred to as a linear Bayesian\nlearning (LBL) receiver. PIC has a linear complexity as it uses a combination\nof maximum ratio combining (MRC) and decision statistic combining (DSC) schemes\nto avoid matrix inversion operations. Simulation results show that the\nbit-error-rate (BER) and latency processing performances of the proposed\nreceiver outperform the ones of MMSE and best Bayesian-based receivers by\nminimum $2$ dB and $19$ times for various M-MIMO system configurations.",
    "descriptor": "",
    "authors": [
      "Alva Kosasih",
      "Wibowo Hardjawana",
      "Branka Vucetic",
      "Chao-Kai Wen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14138"
  },
  {
    "id": "arXiv:2110.14143",
    "title": "SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language  Navigation",
    "abstract": "Natural language instructions for visual navigation often use scene\ndescriptions (e.g., \"bedroom\") and object references (e.g., \"green chairs\") to\nprovide a breadcrumb trail to a goal location. This work presents a\ntransformer-based vision-and-language navigation (VLN) agent that uses two\ndifferent visual encoders -- a scene classification network and an object\ndetector -- which produce features that match these two distinct types of\nvisual cues. In our method, scene features contribute high-level contextual\ninformation that supports object-level processing. With this design, our model\nis able to use vision-and-language pretraining (i.e., learning the alignment\nbetween images and text from large-scale web data) to substantially improve\nperformance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks.\nSpecifically, our approach leads to improvements of 1.8% absolute in SPL on R2R\nand 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for\nnavigation instructions that contain six or more object references, which\nfurther suggests that our approach is better able to use object features and\nalign them to references in the instructions.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Abhinav Moudgil",
      "Arjun Majumdar",
      "Harsh Agrawal",
      "Stefan Lee",
      "Dhruv Batra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14143"
  },
  {
    "id": "arXiv:2110.14147",
    "title": "Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis",
    "abstract": "Transferring human motion from a source to a target person poses great\npotential in computer vision and graphics applications. A crucial step is to\nmanipulate sequential future motion while retaining the appearance\ncharacteristic.Previous work has either relied on crafted 3D human models or\ntrained a separate model specifically for each target person, which is not\nscalable in practice.This work studies a more general setting, in which we aim\nto learn a \\emph{single} model to parsimoniously transfer motion from a source\nvideo to any target person given only one image of the person, named as\nCollaborative Parsing-Flow Network (CPF-Net). The paucity of information\nregarding the target person makes the task particularly challenging to\nfaithfully preserve the appearance in varying designated poses.To address this\nissue, CPF-Net integrates the structured human parsing and appearance flow to\nguide the realistic foreground synthesis which is merged into the background by\na spatio-temporal fusion module.In particular, CPF-Net decouples the problem\ninto stages of human parsing sequence generation, foreground sequence\ngeneration and final video generation. The human parsing generation stage\ncaptures both the pose and the body structure of the target. The appearance\nflow is beneficial to keep details in synthesized frames. The integration of\nhuman parsing and appearance flow effectively guides the generation of video\nframes with realistic appearance. Finally, the dedicated designed fusion\nnetwork ensure the temporal coherence. We further collect a large set of human\ndancing videos to push forward this research field. Both quantitative and\nqualitative results show our method substantially improves over previous\napproaches and is able to generate appealing and photo-realistic target videos\ngiven any input person image. All source code and dataset will be released at\nhttps://github.com/xiezhy6/CPF-Net.",
    "descriptor": "\nComments: TIP 2021\n",
    "authors": [
      "Bowen Wu",
      "Zhenyu Xie",
      "Xiaodan Liang",
      "Yubei Xiao",
      "Haoye Dong",
      "Liang Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14147"
  },
  {
    "id": "arXiv:2110.14149",
    "title": "Diversity Matters When Learning From Ensembles",
    "abstract": "Deep ensembles excel in large-scale image classification tasks both in terms\nof prediction accuracy and calibration. Despite being simple to train, the\ncomputation and memory cost of deep ensembles limits their practicability.\nWhile some recent works propose to distill an ensemble model into a single\nmodel to reduce such costs, there is still a performance gap between the\nensemble and distilled models. We propose a simple approach for reducing this\ngap, i.e., making the distilled performance close to the full ensemble. Our key\nassumption is that a distilled model should absorb as much function diversity\ninside the ensemble as possible. We first empirically show that the typical\ndistillation procedure does not effectively transfer such diversity, especially\nfor complex models that achieve near-zero training error. To fix this, we\npropose a perturbation strategy for distillation that reveals diversity by\nseeking inputs for which ensemble member outputs disagree. We empirically show\nthat a model distilled with such perturbed samples indeed exhibits enhanced\ndiversity, leading to improved performance.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Giung Nam",
      "Jongmin Yoon",
      "Yoonho Lee",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14149"
  },
  {
    "id": "arXiv:2110.14150",
    "title": "Training Wasserstein GANs without gradient penalties",
    "abstract": "We propose a stable method to train Wasserstein generative adversarial\nnetworks. In order to enhance stability, we consider two objective functions\nusing the $c$-transform based on Kantorovich duality which arises in the theory\nof optimal transport. We experimentally show that this algorithm can\neffectively enforce the Lipschitz constraint on the discriminator while other\nstandard methods fail to do so. As a consequence, our method yields an accurate\nestimation for the optimal discriminator and also for the Wasserstein distance\nbetween the true distribution and the generated one. Our method requires no\ngradient penalties nor corresponding hyperparameter tuning and is\ncomputationally more efficient than other methods. At the same time, it yields\ncompetitive generators of synthetic images based on the MNIST, F-MNIST, and\nCIFAR-10 datasets.",
    "descriptor": "",
    "authors": [
      "Dohyun Kwon",
      "Yeoneung Kim",
      "Guido Mont\u00fafar",
      "Insoon Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14150"
  },
  {
    "id": "arXiv:2110.14153",
    "title": "Differentially Private Federated Bayesian Optimization with Distributed  Exploration",
    "abstract": "Bayesian optimization (BO) has recently been extended to the federated\nlearning (FL) setting by the federated Thompson sampling (FTS) algorithm, which\nhas promising applications such as federated hyperparameter tuning. However,\nFTS is not equipped with a rigorous privacy guarantee which is an important\nconsideration in FL. Recent works have incorporated differential privacy (DP)\ninto the training of deep neural networks through a general framework for\nadding DP to iterative algorithms. Following this general DP framework, our\nwork here integrates DP into FTS to preserve user-level privacy. We also\nleverage the ability of this general DP framework to handle different parameter\nvectors, as well as the technique of local modeling for BO, to further improve\nthe utility of our algorithm through distributed exploration (DE). The\nresulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed\nwith theoretical guarantees for both the privacy and utility and is amenable to\ninteresting theoretical insights about the privacy-utility trade-off. We also\nuse real-world experiments to show that DP-FTS-DE achieves high utility\n(competitive performance) with a strong privacy guarantee (small privacy loss)\nand induces a trade-off between privacy and utility.",
    "descriptor": "\nComments: Accepted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Extended version with proofs and additional experimental details and results, 29 pages\n",
    "authors": [
      "Zhongxiang Dai",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.14153"
  },
  {
    "id": "arXiv:2110.14157",
    "title": "Dream to Explore: Adaptive Simulations for Autonomous Systems",
    "abstract": "One's ability to learn a generative model of the world without supervision\ndepends on the extent to which one can construct abstract knowledge\nrepresentations that generalize across experiences. To this end, capturing an\naccurate statistical structure from observational data provides useful\ninductive biases that can be transferred to novel environments. Here, we tackle\nthe problem of learning to control dynamical systems by applying Bayesian\nnonparametric methods, which is applied to solve visual servoing tasks. This is\naccomplished by first learning a state space representation, then inferring\nenvironmental dynamics and improving the policies through imagined future\ntrajectories. Bayesian nonparametric models provide automatic model adaptation,\nwhich not only combats underfitting and overfitting, but also allows the\nmodel's unbounded dimension to be both flexible and computationally tractable.\nBy employing Gaussian processes to discover latent world dynamics, we mitigate\ncommon data efficiency issues observed in reinforcement learning and avoid\nintroducing explicit model bias by describing the system's dynamics. Our\nalgorithm jointly learns a world model and policy by optimizing a variational\nlower bound of a log-likelihood with respect to the expected free energy\nminimization objective function. Finally, we compare the performance of our\nmodel with the state-of-the-art alternatives for continuous control tasks in\nsimulated environments.",
    "descriptor": "",
    "authors": [
      "Zahra Sheikhbahaee",
      "Dongshu Luo",
      "Blake VanBerlo",
      "S. Alex Yun",
      "Adam Safron",
      "Jesse Hoey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14157"
  },
  {
    "id": "arXiv:2110.14160",
    "title": "Identifying the key components in ResNet-50 for diabetic retinopathy  grading from fundus images: a systematic investigation",
    "abstract": "Although deep learning based diabetic retinopathy (DR) classification methods\ntypically benefit from well-designed architectures of convolutional neural\nnetworks, the training setting also has a non-negligible impact on the\nprediction performance. The training setting includes various interdependent\ncomponents, such as objective function, data sampling strategy and data\naugmentation approach. To identify the key components in a standard deep\nlearning framework (ResNet-50) for DR grading, we systematically analyze the\nimpact of several major components. Extensive experiments are conducted on a\npublicly-available dataset EyePACS. We demonstrate that (1) the ResNet-50\nframework for DR grading is sensitive to input resolution, objective function,\nand composition of data augmentation, (2) using mean square error as the loss\nfunction can effectively improve the performance with respect to a\ntask-specific evaluation metric, namely the quadratically-weighted Kappa, (3)\nutilizing eye pairs boosts the performance of DR grading and (4) using data\nresampling to address the problem of imbalanced data distribution in EyePACS\nhurts the performance. Based on these observations and an optimal combination\nof the investigated components, our framework, without any specialized network\ndesign, achieves the state-of-the-art result (0.8631 for Kappa) on the EyePACS\ntest set (a total of 42670 fundus images) with only image-level labels. Our\ncodes and pre-trained model are available at\nhttps://github.com/YijinHuang/pytorch-classification",
    "descriptor": "",
    "authors": [
      "Yijin Huang",
      "Li Lin",
      "Pujin Cheng",
      "Junyan Lyu",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.14160"
  },
  {
    "id": "arXiv:2110.14162",
    "title": "Stubbifier: Debloating Dynamic Server-Side JavaScript Applications",
    "abstract": "JavaScript is an increasingly popular language for server-side development,\nthanks in part to the Node.js runtime environment and its vast ecosystem of\nmodules. With the Node.js package manager npm, users are able to easily include\nexternal modules as dependencies in their projects. However, npm installs\nmodules with all of their functionality, even if only a fraction is needed,\nwhich causes an undue increase in code size. Eliminating this unused\nfunctionality from distributions is desirable, but the sound analysis required\nto find unused code is difficult due to JavaScript's extreme dynamicity.\nWe present a fully automatic technique that identifies unused code by\nconstructing static or dynamic call graphs from the application's tests, and\nreplacing code deemed unreachable with either file- or function-level stubs. If\na stub is called, it will fetch and execute the original code on-demand, thus\nrelaxing the requirement that the call graph be sound. The technique also\nprovides an optional guarded execution mode to guard application against\ninjection vulnerabilities in untested code that resulted from stub expansion.\nThis technique is implemented in an open source tool called Stubbifier, which\nsupports the ECMAScript 2019 standard. In an empirical evaluation on 15 Node.js\napplications and 75 clients of these applications, Stubbifier reduced\napplication size by 56% on average while incurring only minor performance\noverhead. The evaluation also shows that Stubbifier's guarded execution mode is\ncapable of preventing several known injection vulnerabilities that are\nmanifested in stubbed-out code. Finally, Stubbifier can work alongside\nbundlers, popular JavaScript tools for bundling an application with its\ndependencies. For the considered subject applications, we measured an average\nsize reduction of 37% in bundled distributions.",
    "descriptor": "\nComments: 29 pages. This work has been submitted to the Journal on Empirical Software Engineering\n",
    "authors": [
      "Alexi Turcotte",
      "Ellen Arteca",
      "Ashish Mishra",
      "Saba Alimadadi",
      "Frank Tip"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.14162"
  },
  {
    "id": "arXiv:2110.14163",
    "title": "Does the Data Induce Capacity Control in Deep Learning?",
    "abstract": "This paper studies how the dataset may be the cause of the anomalous\ngeneralization performance of deep networks. We show that the data correlation\nmatrix of typical classification datasets has an eigenspectrum where, after a\nsharp initial drop, a large number of small eigenvalues are distributed\nuniformly over an exponentially large range. This structure is mirrored in a\nnetwork trained on this data: we show that the Hessian and the Fisher\nInformation Matrix (FIM) have eigenvalues that are spread uniformly over\nexponentially large ranges. We call such eigenspectra \"sloppy\" because sets of\nweights corresponding to small eigenvalues can be changed by large magnitudes\nwithout affecting the loss. Networks trained on atypical, non-sloppy synthetic\ndata do not share these traits. We show how this structure in the data can give\nto non-vacuous PAC-Bayes generalization bounds analytically; we also construct\ndata-distribution dependent priors that lead to accurate bounds using numerical\noptimization.",
    "descriptor": "",
    "authors": [
      "Yang Rubing",
      "Mao Jialin",
      "Chaudhari Pratik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14163"
  },
  {
    "id": "arXiv:2110.14164",
    "title": "Don't read, just look: Main content extraction from web pages using  visually apparent features",
    "abstract": "The extraction of main content provides only primary informative blocks by\nremoving a web page's minor areas like navigation menu, ads, and site\ntemplates. It has various applications: information retrieval, search engine\noptimization, and browser reader mode. We tested the existing four main content\nextraction methods (Firefox Readability.js, Chrome DOM Distiller, Web2Text, and\nBoilernet) in web pages datasets of two English datasets from the global\nwebsites and seven non-English datasets from seven local regions each. It shows\nthat the performance decreases by up to 40% in non-English datasets over\nEnglish datasets. This paper proposes a multilingual main content extraction\nmethod that uses visually apparent features such as the elements' positions,\nsize, and distances from the centers of the browser window and the web\ndocument. These are based on the authors' intention: the elements' placement\nand appearance in web pages have constraints because of humans' narrow central\nvision. Hence, our method, Grid-Center-Expand (GCE), finds the closest leaf\nnode to the centroid of the web page from which minor areas have been removed.\nFor the main content, the leaf node repeatedly ascends to the parent node of\nthe DOM tree until this node fits one of the following conditions: <article>\ntag, containing specific attributes, or sudden width change. In the non-English\ndatasets, our method performs better than up to 13% over Boilernet, especially\n56% in the Japan dataset and 7% in the English dataset. Therefore, our method\nperforms well regardless of the regional and linguistic characteristics of the\nweb page. In addition, we create DNN models using Google's TabNet with GCE's\nfeatures. The best of our models has similar performance to Boilernet and\nWeb2text in all datasets. Accordingly, we show that these features can be\nuseful to machine learning models for extracting main content.",
    "descriptor": "",
    "authors": [
      "Geunseong Jung",
      "Sungjae Han",
      "Hansung Kim",
      "Jaehyuk Cha"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.14164"
  },
  {
    "id": "arXiv:2110.14166",
    "title": "A Self-adaptive Weighted Differential Evolution Approach for Large-scale  Feature Selection",
    "abstract": "Recently, many evolutionary computation methods have been developed to solve\nthe feature selection problem. However, the studies focused mainly on\nsmall-scale issues, resulting in stagnation issues in local optima and\nnumerical instability when dealing with large-scale feature selection dilemmas.\nTo address these challenges, this paper proposes a novel weighted differential\nevolution algorithm based on self-adaptive mechanism, named SaWDE, to solve\nlarge-scale feature selection. First, a multi-population mechanism is adopted\nto enhance the diversity of the population. Then, we propose a new\nself-adaptive mechanism that selects several strategies from a strategy pool to\ncapture the diverse characteristics of the datasets from the historical\ninformation. Finally, a weighted model is designed to identify the important\nfeatures, which enables our model to generate the most suitable\nfeature-selection solution. We demonstrate the effectiveness of our algorithm\non twelve large-scale datasets. The performance of SaWDE is superior compared\nto six non-EC algorithms and six other EC algorithms, on both training and test\ndatasets and on subset size, indicating that our algorithm is a favorable tool\nto solve the large-scale feature selection problem. Moreover, we have\nexperimented SaWDE with six EC algorithms on twelve higher-dimensional data,\nwhich demonstrates that SaWDE is more robust and efficient compared to those\nstate-of-the-art methods. SaWDE source code is available on Github at\nhttps://github.com/wangxb96/SaWDE.",
    "descriptor": "",
    "authors": [
      "Xubin Wang",
      "Yunhe Wang",
      "Ka-Chun Wong",
      "Xiangtao Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.14166"
  },
  {
    "id": "arXiv:2110.14168",
    "title": "Training Verifiers to Solve Math Word Problems",
    "abstract": "State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
    "descriptor": "",
    "authors": [
      "Karl Cobbe",
      "Vineet Kosaraju",
      "Mohammad Bavarian",
      "Jacob Hilton",
      "Reiichiro Nakano",
      "Christopher Hesse",
      "John Schulman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14168"
  },
  {
    "id": "arXiv:2110.14169",
    "title": "Control of a Floating Wind Turbine on a Novel Actuated Platform",
    "abstract": "Designing a floating offshore wind turbine (FOWT) controller requires solving\nengineering challenges not found for fixed-bottom turbines. This paper applies\nseveral methods from the growing body of FOWT control literature to the 10-MW\nUltraflexible Smart FLoating Offshore Wind Turbine (USFLOWT) baseline\ngenerator-speed controller. USFLOWT aims to reduce capital expenses using the\nlightweight SpiderFLOAT platform, a novel smart floating substructure with\nbuilt-in distributed actuators for direct platform tilt and heave control. In\nthis work, the USFLOWT baseline controller is improved through detuning and\nparallel compensation with both blade pitch and generator torque. The\nSpiderFLOAT platform additionally allows motion compensation through\ndistributed platform actuators. Two proposed SpiderFLOAT actuator types are\nconsidered for active platform control: a low-bandwidth actuator that uses\nvariable floater ballast to bring a heeling platform to a steady-state upright\nposition, and a high-bandwidth actuator that dynamically changes the\nsubstructure geometry to actively reject transient platform motion. Each\ncontrol approach is tested for USFLOWT using the open-source\naero-hydro-servo-elastic wind turbine simulation tool OpenFAST. Performance\nresults for each approach are compared across a range of above-rated wind\nspeeds, and promising combined approaches are further evaluated to recommend\nfuture multi-parameter optimization pathways.",
    "descriptor": "\nComments: 9 pages (incl. leading disclaimer), 9 figures. This work has been submitted to the IEEE for possible publication in the American Control Conference, Atlanta, GA, 2022\n",
    "authors": [
      "David Stockhouse",
      "Mandar Phadnis",
      "Elenya Grant",
      "Kathryn Johnson",
      "Rick Damiani",
      "Lucy Pao"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14169"
  },
  {
    "id": "arXiv:2110.14170",
    "title": "Standing on the Shoulders of Predecessors: Meta-Knowledge Transfer for  Knowledge Graphs",
    "abstract": "Knowledge graphs (KGs) have become widespread, and various knowledge graphs\nare constructed incessantly to support many in-KG and out-of-KG applications.\nDuring the construction of KGs, although new KGs may contain new entities with\nrespect to constructed KGs, some entity-independent knowledge can be\ntransferred from constructed KGs to new KGs. We call such knowledge\nmeta-knowledge, and refer to the problem of transferring meta-knowledge from\nconstructed (source) KGs to new (target) KGs to improve the performance of\ntasks on target KGs as meta-knowledge transfer for knowledge graphs. However,\nthere is no available general framework that can tackle meta-knowledge transfer\nfor both in-KG and out-of-KG tasks uniformly. Therefore, in this paper, we\npropose a framework, MorsE, which means conducting Meta-Learning for\nMeta-Knowledge Transfer via Knowledge Graph Embedding. MorsE represents the\nmeta-knowledge via Knowledge Graph Embedding and learns the meta-knowledge by\nMeta-Learning. Specifically, MorsE uses an entity initializer and a Graph\nNeural Network (GNN) modulator to entity-independently obtain entity embeddings\ngiven a KG and is trained following the meta-learning setting to gain the\nability of effectively obtaining embeddings. Experimental results on\nmeta-knowledge transfer for both in-KG and out-of-KG tasks show that MorsE is\nable to learn and transfer meta-knowledge between KGs effectively, and\noutperforms existing state-of-the-art models.",
    "descriptor": "",
    "authors": [
      "Mingyang Chen",
      "Wen Zhang",
      "Yushan Zhu",
      "Hongting Zhou",
      "Zonggang Yuan",
      "Changliang Xu",
      "Huajun Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14170"
  },
  {
    "id": "arXiv:2110.14171",
    "title": "Diversity Enhanced Active Learning with Strictly Proper Scoring Rules",
    "abstract": "We study acquisition functions for active learning (AL) for text\nclassification. The Expected Loss Reduction (ELR) method focuses on a Bayesian\nestimate of the reduction in classification error, recently updated with Mean\nObjective Cost of Uncertainty (MOCU). We convert the ELR framework to estimate\nthe increase in (strictly proper) scores like log probability or negative mean\nsquare error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS). We\nalso prove convergence results borrowing techniques used with MOCU. In order to\nallow better experimentation with the new acquisition functions, we develop a\ncomplementary batch AL algorithm, which encourages diversity in the vector of\nexpected changes in scores for unlabelled data. To allow high performance text\nclassifiers, we combine ensembling and dynamic validation set construction on\npretrained language models. Extensive experimental evaluation then explores how\nthese different acquisition functions perform. The results show that the use of\nmean square error and log probability with BEMPS yields robust acquisition\nfunctions, which consistently outperform the others tested.",
    "descriptor": "",
    "authors": [
      "Wei Tan",
      "Lan Du",
      "Wray Buntine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14171"
  },
  {
    "id": "arXiv:2110.14180",
    "title": "AeCoM: Design, Modeling and Control of a Novel Aerial Continuum  Manipulator",
    "abstract": "Aerial robotic systems has raised emerging interests among researchers. In\nthis paper, a novel aerial manipulation system: a flying continuum robotic\nmanipulator (AeCoM) is first proposed, to the best of authors' knowledge. In\nthe perspective of design, a lightweight tendon-driven continuum robotic arm\n(in 3D-printed material) is delicately coupled with a quadrotor. To address the\nproblem of kinematics inaccuracy due to different tip loading, we introduce an\nattitude sensor (IMU) to assist in PCC (Piecewise Constant Curvature)\nconfiguration. To deal with frequent and complex aerial manipulation tasks, we\ndeploy a tension-based closed-loop control method, which is used to avoid\ntendon-slacking in manipulating the shape of the continuum arm. Distinct from\nthe conventional aerial rigid manipulators, the proposed system achieve more\nrelative payload capability and motion dexterity. The system's experimental\nresults validate the performance of tendon-slacking avoidance, kinematics\naccuracy with different tip loading, and tip positioning accuracy for aerial\ngrasping. The comparison with conventional aerial manipulators, indicates that\nthe proposed manipulator has better manipulation performance and more potential\napplications in the cluttered environment.",
    "descriptor": "",
    "authors": [
      "Rui Peng",
      "Zehao Wang",
      "Peng Lu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14180"
  },
  {
    "id": "arXiv:2110.14182",
    "title": "Evidential Softmax for Sparse Multimodal Distributions in Deep  Generative Models",
    "abstract": "Many applications of generative models rely on the marginalization of their\nhigh-dimensional output probability distributions. Normalization functions that\nyield sparse probability distributions can make exact marginalization more\ncomputationally tractable. However, sparse normalization functions usually\nrequire alternative loss functions for training since the log-likelihood is\nundefined for sparse probability distributions. Furthermore, many sparse\nnormalization functions often collapse the multimodality of distributions. In\nthis work, we present $\\textit{ev-softmax}$, a sparse normalization function\nthat preserves the multimodality of probability distributions. We derive its\nproperties, including its gradient in closed-form, and introduce a continuous\nfamily of approximations to $\\textit{ev-softmax}$ that have full support and\ncan be trained with probabilistic loss functions such as negative\nlog-likelihood and Kullback-Leibler divergence. We evaluate our method on a\nvariety of generative models, including variational autoencoders and\nauto-regressive architectures. Our method outperforms existing dense and sparse\nnormalization techniques in distributional accuracy. We demonstrate that\n$\\textit{ev-softmax}$ successfully reduces the dimensionality of probability\ndistributions while maintaining multimodality.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021. Code is available at this https URL\n",
    "authors": [
      "Phil Chen",
      "Masha Itkina",
      "Ransalu Senanayake",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14182"
  },
  {
    "id": "arXiv:2110.14183",
    "title": "(Im)balance in the Representation of News? An Extensive Study on a  Decade Long Dataset from India",
    "abstract": "(Im)balance in the representation of news has always been a topic of debate\nin political circles.\nThe concept of balance has often been discussed and studied in the context of\nthe social responsibility theory and the prestige press in the USA. While\nvarious qualitative, as well as quantitative measures of balance, have been\nsuggested in the literature, a comprehensive analysis of all these measures\nacross a large dataset of the post-truth era comprising different popular news\nmedia houses and over a sufficiently long temporal scale in a non-US democratic\nsetting is lacking. We use this concept of balance to measure and understand\nthe evolution of imbalance in Indian media on various journalistic metrics on a\nmonth-by-month basis. For this study, we amass a huge dataset of over four\nmillion political articles from India for 9+ years and analyze the extent and\nquality of coverage given to issues and political parties in the context of\ncontemporary influential events for three leading newspapers. We use several\nstate-of-the-art NLP tools to effectively understand political polarization (if\nany) manifesting in these articles over time. We find that two out of the three\nnews outlets are more strongly clustered in their imbalance metrics. We also\nobserve that only a few locations are extensively covered across all the news\noutlets and the situation is only slightly getting better for one of the three\nnews outlets. Cloze tests show that the changing landscape of events get\nreflected in all the news outlets with border and terrorism issues dominating\nin around 2010 while economic aspects like unemployment, GST, demonetization,\netc. became more dominant in the period 2014 -- 2018. Further, cloze tests\nclearly portray the changing popularity profile of the political parties over\ntime.",
    "descriptor": "\nComments: 14 pages, submitted to IEEE TCSS\n",
    "authors": [
      "Souvic Chakraborty",
      "Pawan Goyal",
      "Animesh Mukherjee"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.14183"
  },
  {
    "id": "arXiv:2110.14184",
    "title": "OpeNPDN: A Neural-network-based Framework for Power Delivery Network  Synthesis",
    "abstract": "Power delivery network (PDN) design is a nontrivial, time-intensive, and\niterative task. Correct PDN design must account for considerations related to\npower bumps, currents, blockages, and signal congestion distribution patterns.\nThis work proposes a machine learning-based methodology that employs a set of\npredefined PDN templates. At the floorplan stage, coarse estimates of current,\ncongestion, macro/blockages, and C4 bump distributions are used to synthesize a\ngrid for early design. At the placement stage, the grid is incrementally\nrefined based on more accurate and fine-grained distributions of current and\ncongestion. At each stage, a convolutional neural network (CNN) selects an\nappropriate PDN template for each region on the chip, building a\nsafe-by-construction PDN that meets IR drop and electromigration (EM)\nspecifications. The CNN is initially trained using a large\nsynthetically-created dataset, following which transfer learning is leveraged\nto bridge the gap between real-circuit data (with a limited dataset size) and\nsynthetically-generated data. On average, the optimization of the PDN frees\nthousands of routing tracks in congestion-critical regions, when compared to a\nglobally uniform PDN, while staying within the IR drop and EM limits.",
    "descriptor": "\nComments: 14 pages, 20 figures, is currently under review at IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\n",
    "authors": [
      "Vidya A. Chhabria",
      "Sachin S. Sapatnekar"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14184"
  },
  {
    "id": "arXiv:2110.14185",
    "title": "Massive MIMO NOMA with Wavelet Pulse Shaping to Minimize Undesired  Channel Interference",
    "abstract": "In this article, wavelet OFDM based non-orthogonal-multiple-access (NOMA)\ncombined with massive MIMO system for 6G networks is proposed. For mMIMO\ntransmissions, the proposed system could enhance the performance by utilizing\nwavelets to compensate for channel impairments on the transmitted signal.\nPerformance measures include spectral efficiency, symbol error rate (SER), and\npeak to average ratio (PAPR). Simulation results prove that the proposed system\noutperforms the conventional OFDM based NOMA systems.",
    "descriptor": "\nComments: 5 pages, 8 figures, IEEE Transaction on Vehicular Technology (Submitted)\n",
    "authors": [
      "Muneeb Ahmad",
      "Soo Young Shin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14185"
  },
  {
    "id": "arXiv:2110.14187",
    "title": "An Experimental Study of Permanently Stored Learned Clauses",
    "abstract": "Modern CDCL SAT solvers learn clauses rapidly, and an important heuristic is\nthe clause deletion scheme. Most current solvers have two (or more) stores of\nclauses. One has ``valuable'' clauses which are never deleted. Most learned\nclauses are added to the other, with an aggressive deletion strategy to\nrestrict its size. Recent solvers in the MapleSAT family, have comparatively\ncomplex deletion scheme, and perform well. Many solvers store only binary\nclauses permanently, but MapleLCMDistChronoBT stores clauses with small LBD\npermanently. We report an experimental study of the permanent clause store in\nMapleLCMDistChronoBT. We observe that this store can get quite large, but\nseveral methods for limiting its size reduced performance. We also show that\nalternate size and LBD based criteria improve performance, while still having\nlarge permanent stores. In particular, saving clauses up to size 8, and adding\nsmall numbers of high-centrality clauses, both improved performance, with the\nbest improvement using both methods.",
    "descriptor": "",
    "authors": [
      "Sima Jamali",
      "David Mitchell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.14187"
  },
  {
    "id": "arXiv:2110.14188",
    "title": "RoMA: Robust Model Adaptation for Offline Model-based Optimization",
    "abstract": "We consider the problem of searching an input maximizing a black-box\nobjective function given a static dataset of input-output queries. A popular\napproach to solving this problem is maintaining a proxy model, e.g., a deep\nneural network (DNN), that approximates the true objective function. Here, the\nmain challenge is how to avoid adversarially optimized inputs during the\nsearch, i.e., the inputs where the DNN highly overestimates the true objective\nfunction. To handle the issue, we propose a new framework, coined robust model\nadaptation (RoMA), based on gradient-based optimization of inputs over the DNN.\nSpecifically, it consists of two steps: (a) a pre-training strategy to robustly\ntrain the proxy model and (b) a novel adaptation procedure of the proxy model\nto have robust estimates for a specific set of candidate solutions. At a high\nlevel, our scheme utilizes the local smoothness prior to overcome the\nbrittleness of the DNN. Experiments under various tasks show the effectiveness\nof RoMA compared with previous methods, obtaining state-of-the-art results,\ne.g., RoMA outperforms all at 4 out of 6 tasks and achieves runner-up results\nat the remaining tasks.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Sihyun Yu",
      "Sungsoo Ahn",
      "Le Song",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14188"
  },
  {
    "id": "arXiv:2110.14189",
    "title": "Robust Contrastive Learning Using Negative Samples with Diminished  Semantics",
    "abstract": "Unsupervised learning has recently made exceptional progress because of the\ndevelopment of more effective contrastive learning methods. However, CNNs are\nprone to depend on low-level features that humans deem non-semantic. This\ndependency has been conjectured to induce a lack of robustness to image\nperturbations or domain shift. In this paper, we show that by generating\ncarefully designed negative samples, contrastive learning can learn more robust\nrepresentations with less dependence on such features. Contrastive learning\nutilizes positive pairs that preserve semantic information while perturbing\nsuperficial features in the training images. Similarly, we propose to generate\nnegative samples in a reversed way, where only the superfluous instead of the\nsemantic features are preserved. We develop two methods, texture-based and\npatch-based augmentations, to generate negative samples. These samples achieve\nbetter generalization, especially under out-of-domain settings. We also analyze\nour method and the generated texture-based samples, showing that texture\nfeatures are indispensable in classifying particular ImageNet classes and\nespecially finer classes. We also show that model bias favors texture and shape\nfeatures differently under different test settings. Our code, trained models,\nand ImageNet-Texture dataset can be found at\nhttps://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.",
    "descriptor": "\nComments: Published as a conference paper at NeurIPS 2021\n",
    "authors": [
      "Songwei Ge",
      "Shlok Mishra",
      "Haohan Wang",
      "Chun-Liang Li",
      "David Jacobs"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14189"
  },
  {
    "id": "arXiv:2110.14191",
    "title": "Mixed Supervised Object Detection by Transferring Mask Prior and  Semantic Similarity",
    "abstract": "Object detection has achieved promising success, but requires large-scale\nfully-annotated data, which is time-consuming and labor-extensive. Therefore,\nwe consider object detection with mixed supervision, which learns novel object\ncategories using weak annotations with the help of full annotations of existing\nbase object categories. Previous works using mixed supervision mainly learn the\nclass-agnostic objectness from fully-annotated categories, which can be\ntransferred to upgrade the weak annotations to pseudo full annotations for\nnovel categories. In this paper, we further transfer mask prior and semantic\nsimilarity to bridge the gap between novel categories and base categories.\nSpecifically, the ability of using mask prior to help detect objects is learned\nfrom base categories and transferred to novel categories. Moreover, the\nsemantic similarity between objects learned from base categories is transferred\nto denoise the pseudo full annotations for novel categories. Experimental\nresults on three benchmark datasets demonstrate the effectiveness of our method\nover existing methods. Codes are available at\nhttps://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection.",
    "descriptor": "\nComments: accepted by NeurIPS2021\n",
    "authors": [
      "Yan Liu",
      "Zhijie Zhang",
      "Li Niu",
      "Junjie Chen",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14191"
  },
  {
    "id": "arXiv:2110.14193",
    "title": "Smooth head tracking for virtual reality applications",
    "abstract": "In this work, we propose a new head-tracking solution for human-machine\nreal-time interaction with virtual 3D environments. This solution leverages\nRGBD data to compute virtual camera pose according to the movements of the\nuser's head. The process starts with the extraction of a set of facial features\nfrom the images delivered by the sensor. Such features are matched against\ntheir respective counterparts in a reference image for the computation of the\ncurrent head pose. Afterwards, a prediction approach is used to guess the most\nlikely next head move (final pose). Pythagorean Hodograph interpolation is then\nadapted to determine the path and local frames taken between the two poses. The\nresult is a smooth head trajectory that serves as an input to set the camera in\nvirtual scenes according to the user's gaze. The resulting motion model has the\nadvantage of being: continuous in time, it adapts to any frame rate of\nrendering; it is ergonomic, as it frees the user from wearing tracking markers;\nit is smooth and free from rendering jerks; and it is also torsion and\ncurvature minimizing as it produces a path with minimum bending energy.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Abdenour Amamra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.14193"
  },
  {
    "id": "arXiv:2110.14195",
    "title": "Dual-Mode Synchronization Predictive Control of Robotic Manipulator",
    "abstract": "To reduce the contour error of the end-effector of a robotic manipulator\nduring trajectory tracking, a dual-mode synchronization predictive control is\nproposed. Firstly, the dynamic model of n-DoF robotic manipulator is\ndiscretized by using the Taylor expansion method, and the mapping relationship\nbetween the joint error in the joint space and the contour error in task space\nis constructed. Secondly, the synchronization error and the tracking error in\nthe joint space are defined, and the coupling error of joints is derived\nthrough the coupling coefficient . Thirdly, a dual-mode synchronization\npredictive control is proposed, and the stability of the proposed control\nsystem is guaranteed using constraint set conditions. Finally, numerical\nsimulation and experimental results are shown to prove the effectiveness of the\nproposed control strategy.",
    "descriptor": "",
    "authors": [
      "Zhu Dachang",
      "Cui Aodong",
      "Du Baolin",
      "Zhu Puchen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14195"
  },
  {
    "id": "arXiv:2110.14196",
    "title": "From Image to Imuge: Immunized Image Generation",
    "abstract": "We introduce Imuge, an image tamper resilient generative scheme for image\nself-recovery. The traditional manner of concealing image content within the\nimage are inflexible and fragile to diverse digital attack, i.e. image cropping\nand JPEG compression. To address this issue, we jointly train a U-Net backboned\nencoder, a tamper localization network and a decoder for image recovery. Given\nan original image, the encoder produces a visually indistinguishable immunized\nimage. At the recipient's side, the verifying network localizes the malicious\nmodifications, and the original content can be approximately recovered by the\ndecoder, despite the presence of the attacks. Several strategies are proposed\nto boost the training efficiency. We demonstrate that our method can recover\nthe details of the tampered regions with a high quality despite the presence of\nvarious kinds of attacks. Comprehensive ablation studies are conducted to\nvalidate our network designs.",
    "descriptor": "\nComments: Accepted as Poster at ACMMM 2021. Authors are from Fudan University, Simon Fraser University and NVIDIA, China\n",
    "authors": [
      "Qichao Ying",
      "Zhenxing Qian",
      "Hang Zhou",
      "Haisheng Xu",
      "Xinpeng Zhang",
      "Siyi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14196"
  },
  {
    "id": "arXiv:2110.14197",
    "title": "Encoder-Decoder Networks for Analyzing Thermal and Power Delivery  Networks",
    "abstract": "Power delivery network (PDN) analysis and thermal analysis are\ncomputationally expensive tasks that are essential for successful IC design.\nAlgorithmically, both these analyses have similar computational structure and\ncomplexity as they involve the solution to a partial differential equation of\nthe same form. This paper converts these analyses into image-to-image and\nsequence-to-sequence translation tasks, which allows leveraging a class of\nmachine learning models with an encoder-decoder-based generative (EDGe)\narchitecture to address the time-intensive nature of these tasks. For PDN\nanalysis, we propose two networks: (i) IREDGe: a full-chip static and dynamic\nIR drop predictor and (ii) EMEDGe: electromigration (EM) hotspot classifier\nbased on input power, power grid distribution, and power pad distribution\npatterns. For thermal analysis, we propose ThermEDGe, a full-chip static and\ndynamic temperature estimator based on input power distribution patterns for\nthermal analysis. These networks are transferable across designs synthesized\nwithin the same technology and packing solution. The networks predict on-chip\nIR drop, EM hotspot locations, and temperature in milliseconds with negligibly\nsmall errors against commercial tools requiring several hours.",
    "descriptor": "\nComments: 26 pages, 17 figures, Submitted to TODAES for review. arXiv admin note: text overlap with arXiv:2009.09009\n",
    "authors": [
      "Vidya A. Chhabria",
      "Vipul Ahuja",
      "Ashwath Prabhu",
      "Nikhil Patil",
      "Palkesh Jain",
      "Sachin S. Sapatnekar"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14197"
  },
  {
    "id": "arXiv:2110.14199",
    "title": "Arbitrarily Fast Switched Distributed Stabilization of Partially Unknown  Interconnected Multiagent Systems: A Proactive Cyber Defense Perspective",
    "abstract": "A design framework recently has been developed to stabilize interconnected\nmultiagent systems in a distributed manner, and systematically capture the\narchitectural aspect of cyber-physical systems. Such a control theoretic\nframework, however, results in a stabilization protocol which is passive with\nrespect to the cyber attacks and conservative regarding the guaranteed level of\nresiliency. We treat the control layer topology and stabilization gains as the\ndegrees of freedom, and develop a mixed control and cybersecurity design\nframework to address the above concerns. From a control perspective, despite\nthe agent layer modeling uncertainties and perturbations, we propose a new\nstep-by-step procedure to design a set of control sublayers for an arbitrarily\nfast switching of the control layer topology. From a proactive cyber defense\nperspective, we propose a satisfiability modulo theory formulation to obtain a\nset of control sublayer structures with security considerations, and offer a\nfrequent and fast mutation of these sublayers such that the control layer\ntopology will remain unpredictable for the adversaries. We prove the robust\ninput-to-state stability of the two-layer interconnected multiagent system, and\nvalidate the proposed ideas in simulation.",
    "descriptor": "",
    "authors": [
      "Vahid Rezaei",
      "Jafar Haadi Jafarian",
      "Douglas C. Sicker"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Cryptography and Security (cs.CR)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.14199"
  },
  {
    "id": "arXiv:2110.14200",
    "title": "Denoised Non-Local Neural Network for Semantic Segmentation",
    "abstract": "The non-local network has become a widely used technique for semantic\nsegmentation, which computes an attention map to measure the relationships of\neach pixel pair. However, most of the current popular non-local models tend to\nignore the phenomenon that the calculated attention map appears to be very\nnoisy, containing inter-class and intra-class inconsistencies, which lowers the\naccuracy and reliability of the non-local methods. In this paper, we\nfiguratively denote these inconsistencies as attention noises and explore the\nsolutions to denoise them. Specifically, we inventively propose a Denoised\nNon-Local Network (Denoised NL), which consists of two primary modules, i.e.,\nthe Global Rectifying (GR) block and the Local Retention (LR) block, to\neliminate the inter-class and intra-class noises respectively. First, GR adopts\nthe class-level predictions to capture a binary map to distinguish whether the\nselected two pixels belong to the same category. Second, LR captures the\nignored local dependencies and further uses them to rectify the unwanted\nhollows in the attention map. The experimental results on two challenging\nsemantic segmentation datasets demonstrate the superior performance of our\nmodel. Without any external training data, our proposed Denoised NL can achieve\nthe state-of-the-art performance of 83.5\\% and 46.69\\% mIoU on Cityscapes and\nADE20K, respectively.",
    "descriptor": "\nComments: submitted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)\n",
    "authors": [
      "Qi Song",
      "Jie Li",
      "Hao Guo",
      "Rui Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14200"
  },
  {
    "id": "arXiv:2110.14202",
    "title": "Revisit Multimodal Meta-Learning through the Lens of Multi-Task Learning",
    "abstract": "Multimodal meta-learning is a recent problem that extends conventional\nfew-shot meta-learning by generalizing its setup to diverse multimodal task\ndistributions. This setup makes a step towards mimicking how humans make use of\na diverse set of prior skills to learn new skills. Previous work has achieved\nencouraging performance. In particular, in spite of the diversity of the\nmultimodal tasks, previous work claims that a single meta-learner trained on a\nmultimodal distribution can sometimes outperform multiple specialized\nmeta-learners trained on individual unimodal distributions. The improvement is\nattributed to knowledge transfer between different modes of task distributions.\nHowever, there is no deep investigation to verify and understand the knowledge\ntransfer between multimodal tasks. Our work makes two contributions to\nmultimodal meta-learning. First, we propose a method to quantify knowledge\ntransfer between tasks of different modes at a micro-level. Our quantitative,\ntask-level analysis is inspired by the recent transference idea from multi-task\nlearning. Second, inspired by hard parameter sharing in multi-task learning and\na new interpretation of related work, we propose a new multimodal meta-learner\nthat outperforms existing work by considerable margins. While the major focus\nis on multimodal meta-learning, our work also attempts to shed light on task\ninteraction in conventional meta-learning. The code for this project is\navailable at https://miladabd.github.io/KML.",
    "descriptor": "\nComments: Accepted in 35th Conference on Neural Information Processing Systems (NeurIPS 2021); 27 pages\n",
    "authors": [
      "Milad Abdollahzadeh",
      "Touba Malekzadeh",
      "Ngai-Man Cheung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14202"
  },
  {
    "id": "arXiv:2110.14203",
    "title": "Syllabic Quantity Patterns as Rhythmic Features for Latin Authorship  Attribution",
    "abstract": "It is well known that, within the Latin production of written text, peculiar\nmetric schemes were followed not only in poetic compositions, but also in many\nprose works. Such metric patterns were based on so-called syllabic quantity,\ni.e., on the length of the involved syllables, and there is substantial\nevidence suggesting that certain authors had a preference for certain metric\npatterns over others. In this research we investigate the possibility to employ\nsyllabic quantity as a base for deriving rhythmic features for the task of\ncomputational authorship attribution of Latin prose texts. We test the impact\nof these features on the authorship attribution task when combined with other\ntopic-agnostic features. Our experiments, carried out on three different\ndatasets, using two different machine learning methods, show that rhythmic\nfeatures based on syllabic quantity are beneficial in discriminating among\nLatin prose authors.",
    "descriptor": "",
    "authors": [
      "Silvia Corbara",
      "Alejandro Moreo",
      "Fabrizio Sebastiani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14203"
  },
  {
    "id": "arXiv:2110.14205",
    "title": "FedPrune: Towards Inclusive Federated Learning",
    "abstract": "Federated learning (FL) is a distributed learning technique that trains a\nshared model over distributed data in a privacy-preserving manner.\nUnfortunately, FL's performance degrades when there is (i) variability in\nclient characteristics in terms of computational and memory resources (system\nheterogeneity) and (ii) non-IID data distribution across clients (statistical\nheterogeneity). For example, slow clients get dropped in FL schemes, such as\nFederated Averaging (FedAvg), which not only limits overall learning but also\nbiases results towards fast clients. We propose FedPrune; a system that tackles\nthis challenge by pruning the global model for slow clients based on their\ndevice characteristics. By doing so, slow clients can train a small model\nquickly and participate in FL which increases test accuracy as well as\nfairness. By using insights from Central Limit Theorem, FedPrune incorporates a\nnew aggregation technique that achieves robust performance over non-IID data.\nExperimental evaluation shows that Fed- Prune provides robust convergence and\nbetter fairness compared to Federated Averaging.",
    "descriptor": "",
    "authors": [
      "Muhammad Tahir Munir",
      "Muhammad Mustansar Saeed",
      "Mahad Ali",
      "Zafar Ayyub Qazi",
      "Ihsan Ayyub Qazi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14205"
  },
  {
    "id": "arXiv:2110.14207",
    "title": "How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New  Reasoning Challenge for AI",
    "abstract": "Many real-world problems require the combined application of multiple\nreasoning abilities employing suitable abstractions, commonsense knowledge, and\ncreative synthesis of problem-solving strategies. To help advance AI systems\ntowards such capabilities, we propose a new reasoning challenge, namely Fermi\nProblems (FPs), which are questions whose answers can only be approximately\nestimated because their precise computation is either impractical or\nimpossible. For example, \"How much would the sea level rise if all ice in the\nworld melted?\" FPs are commonly used in quizzes and interviews to bring out and\nevaluate the creative reasoning abilities of humans. To do the same for AI\nsystems, we present two datasets: 1) A collection of 1k real-world FPs sourced\nfrom quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate\ncomplexity to serve as a sandbox for the harder real-world challenge. In\naddition to question answer pairs, the datasets contain detailed solutions in\nthe form of an executable program and supporting facts, helping in supervision\nand evaluation of intermediate steps. We demonstrate that even extensively\nfine-tuned large scale language models perform poorly on these datasets, on\naverage making estimates that are off by two orders of magnitude. Our\ncontribution is thus the crystallization of several unsolved AI problems into a\nsingle, new challenge that we hope will spur further advances in building\nsystems that can reason.",
    "descriptor": "\nComments: Accepted for publication at EMNLP 2021, 11 pages, 5 tables, 4 figures\n",
    "authors": [
      "Ashwin Kalyan",
      "Abhinav Kumar",
      "Arjun Chandrasekaran",
      "Ashish Sabharwal",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14207"
  },
  {
    "id": "arXiv:2110.14209",
    "title": "Distributionally Robust Day-ahead Scheduling for Power-traffic Network  Considering Multiple Uncertainties under a Potential Game Framework",
    "abstract": "Widespread utilization of electric vehicles (EVs) incurs more uncertainties\nand impacts on the scheduling of the power-transportation coupled network. This\npaper investigates optimal power scheduling for a power-transportation coupled\nnetwork in the day-ahead energy market considering multiple uncertainties\nrelated to photovoltaic (PV) generation and the traffic demand of vehicles.\nSpecifically, the EV drivers choose the lowest-cost routes in response to\nelectricity prices and the power operators maximize their operating profits.\nFurthermore, we show the interactions between the power system and EV users\nfrom a potential game-theoretic perspective, and the scheduling problem of the\nequilibrium of power-transportation coupled network can be interpreted by a\ntwo-stage distributionally robust optimization (DRO) model. In addition,\nuncertainties of PV generation and traffic demand are established as\nscenario-based ambiguity sets combined with the historical distribution\ninformation, respectively. A combination of the duality theory and the Benders\ndecomposition is developed to solve the adaptive DRO model. Simulation results\ndemonstrate the effectiveness and applicability of the proposed approach.",
    "descriptor": "",
    "authors": [
      "Haoran Deng",
      "Bo Yang",
      "Jiaxin Cao",
      "Chao Ning",
      "Cailian Chen",
      "Xinping Guan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14209"
  },
  {
    "id": "arXiv:2110.14213",
    "title": "Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning  of 3D Pose",
    "abstract": "We study the problem of learning to estimate the 3D object pose from a few\nlabelled examples and a collection of unlabelled data. Our main contribution is\na learning framework, neural view synthesis and matching, that can transfer the\n3D pose annotation from the labelled to unlabelled images reliably, despite\nunseen 3D views and nuisance variations such as the object shape, texture,\nillumination or scene context. In our approach, objects are represented as 3D\ncuboid meshes composed of feature vectors at each mesh vertex. The model is\ninitialized from a few labelled images and is subsequently used to synthesize\nfeature representations of unseen 3D views. The synthesized views are matched\nwith the feature representations of unlabelled images to generate pseudo-labels\nof the 3D pose. The pseudo-labelled data is, in turn, used to train the feature\nextractor such that the features at each mesh vertex are more invariant across\nvarying 3D views of the object. Our model is trained in an EM-type manner\nalternating between increasing the 3D pose invariance of the feature extractor\nand annotating unlabelled data through neural view synthesis and matching. We\ndemonstrate the effectiveness of the proposed semi-supervised learning\nframework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We find\nthat our approach outperforms all baselines by a wide margin, particularly in\nan extreme few-shot setting where only 7 annotated images are given.\nRemarkably, we observe that our model also achieves an exceptional robustness\nin out-of-distribution scenarios that involve partial occlusion.",
    "descriptor": "\nComments: NeurIPS 2021; Code is available under this https URL\n",
    "authors": [
      "Angtian Wang",
      "Shenxiao Mei",
      "Alan Yuille",
      "Adam Kortylewski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14213"
  },
  {
    "id": "arXiv:2110.14215",
    "title": "Beyond Classification: Knowledge Distillation using Multi-Object  Impressions",
    "abstract": "Knowledge Distillation (KD) utilizes training data as a transfer set to\ntransfer knowledge from a complex network (Teacher) to a smaller network\n(Student). Several works have recently identified many scenarios where the\ntraining data may not be available due to data privacy or sensitivity concerns\nand have proposed solutions under this restrictive constraint for the\nclassification task. Unlike existing works, we, for the first time, solve a\nmuch more challenging problem, i.e., \"KD for object detection with zero\nknowledge about the training data and its statistics\". Our proposed approach\nprepares pseudo-targets and synthesizes corresponding samples (termed as\n\"Multi-Object Impressions\"), using only the pretrained Faster RCNN Teacher\nnetwork. We use this pseudo-dataset as a transfer set to conduct zero-shot KD\nfor object detection. We demonstrate the efficacy of our proposed method\nthrough several ablations and extensive experiments on benchmark datasets like\nKITTI, Pascal and COCO. Our approach with no training samples, achieves a\nrespectable mAP of 64.2% and 55.5% on the student with same and half capacity\nwhile performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI.",
    "descriptor": "\nComments: Accepted in BMVC 2021\n",
    "authors": [
      "Gaurav Kumar Nayak",
      "Monish Keswani",
      "Sharan Seshadri",
      "Anirban Chakraborty"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14215"
  },
  {
    "id": "arXiv:2110.14216",
    "title": "What Do We Mean by Generalization in Federated Learning?",
    "abstract": "Federated learning data is drawn from a distribution of distributions:\nclients are drawn from a meta-distribution, and their data are drawn from local\ndata distributions. Thus generalization studies in federated learning should\nseparate performance gaps from unseen client data (out-of-sample gap) from\nperformance gaps from unseen client distributions (participation gap). In this\nwork, we propose a framework for disentangling these performance gaps. Using\nthis framework, we observe and explain differences in behavior across natural\nand synthetic federated datasets, indicating that dataset synthesis strategy\ncan be important for realistic simulations of generalization in federated\nlearning. We propose a semantic synthesis strategy that enables realistic\nsimulation without naturally-partitioned data. Informed by our findings, we\ncall out community suggestions for future federated learning works.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021 Workshop on New Frontiers in Federated Learning. Code repository see this https URL\n",
    "authors": [
      "Honglin Yuan",
      "Warren Morningstar",
      "Lin Ning",
      "Karan Singhal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14216"
  },
  {
    "id": "arXiv:2110.14217",
    "title": "Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects",
    "abstract": "The ability to grasp and manipulate transparent objects is a major challenge\nfor robots. Existing depth cameras have difficulty detecting, localizing, and\ninferring the geometry of such objects. We propose using neural radiance fields\n(NeRF) to detect, localize, and infer the geometry of transparent objects with\nsufficient accuracy to find and grasp them securely. We leverage NeRF's\nview-independent learned density, place lights to increase specular\nreflections, and perform a transparency-aware depth-rendering that we feed into\nthe Dex-Net grasp planner. We show how additional lights create specular\nreflections that improve the quality of the depth map, and test a setup for a\nrobot workcell equipped with an array of cameras to perform transparent object\nmanipulation. We also create synthetic and real datasets of transparent objects\nin real-world settings, including singulated objects, cluttered tables, and the\ntop rack of a dishwasher. In each setting we show that NeRF and Dex-Net are\nable to reliably compute robust grasps on transparent objects, achieving 90%\nand 100% grasp success rates in physical experiments on an ABB YuMi, on objects\nwhere baseline methods fail.",
    "descriptor": "\nComments: 11 pages, 9 figures, to be published in the Conference on Robot Learning (CoRL) 2021\n",
    "authors": [
      "Jeffrey Ichnowski",
      "Yahav Avigal",
      "Justin Kerr",
      "Ken Goldberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14217"
  },
  {
    "id": "arXiv:2110.14221",
    "title": "Learning Diverse Policies in MOBA Games via Macro-Goals",
    "abstract": "Recently, many researchers have made successful progress in building the AI\nsystems for MOBA-game-playing with deep reinforcement learning, such as on Dota\n2 and Honor of Kings. Even though these AI systems have achieved or even\nexceeded human-level performance, they still suffer from the lack of policy\ndiversity. In this paper, we propose a novel Macro-Goals Guided framework,\ncalled MGG, to learn diverse policies in MOBA games. MGG abstracts strategies\nas macro-goals from human demonstrations and trains a Meta-Controller to\npredict these macro-goals. To enhance policy diversity, MGG samples macro-goals\nfrom the Meta-Controller prediction and guides the training process towards\nthese goals. Experimental results on the typical MOBA game Honor of Kings\ndemonstrate that MGG can execute diverse policies in different matches and\nlineups, and also outperform the state-of-the-art methods over 102 heroes.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Yiming Gao",
      "Bei Shi",
      "Xueying Du",
      "Liang Wang",
      "Guangwei Chen",
      "Zhenjie Lian",
      "Fuhao Qiu",
      "Guoan Han",
      "Weixuan Wang",
      "Deheng Ye",
      "Qiang Fu",
      "Wei Yang",
      "Lanxiao Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14221"
  },
  {
    "id": "arXiv:2110.14222",
    "title": "Sample Selection for Fair and Robust Training",
    "abstract": "Fairness and robustness are critical elements of Trustworthy AI that need to\nbe addressed together. Fairness is about learning an unbiased model while\nrobustness is about learning from corrupted data, and it is known that\naddressing only one of them may have an adverse affect on the other. In this\nwork, we propose a sample selection-based algorithm for fair and robust\ntraining. To this end, we formulate a combinatorial optimization problem for\nthe unbiased selection of samples in the presence of data corruption. Observing\nthat solving this optimization problem is strongly NP-hard, we propose a greedy\nalgorithm that is efficient and effective in practice. Experiments show that\nour algorithm obtains fairness and robustness that are better than or\ncomparable to the state-of-the-art technique, both on synthetic and benchmark\nreal datasets. Moreover, unlike other fair and robust training baselines, our\nalgorithm can be used by only modifying the sampling step in batch selection\nwithout changing the training algorithm or leveraging additional clean data.",
    "descriptor": "\nComments: Accepted to 35th Conference on Neural Information Processing Systems (NeurIPS), 2021\n",
    "authors": [
      "Yuji Roh",
      "Kangwook Lee",
      "Steven Euijong Whang",
      "Changho Suh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14222"
  },
  {
    "id": "arXiv:2110.14223",
    "title": "RRNet: Relational Reasoning Network with Parallel Multi-scale Attention  for Salient Object Detection in Optical Remote Sensing Images",
    "abstract": "Salient object detection (SOD) for optical remote sensing images (RSIs) aims\nat locating and extracting visually distinctive objects/regions from the\noptical RSIs. Despite some saliency models were proposed to solve the intrinsic\nproblem of optical RSIs (such as complex background and scale-variant objects),\nthe accuracy and completeness are still unsatisfactory. To this end, we propose\na relational reasoning network with parallel multi-scale attention for SOD in\noptical RSIs in this paper. The relational reasoning module that integrates the\nspatial and the channel dimensions is designed to infer the semantic\nrelationship by utilizing high-level encoder features, thereby promoting the\ngeneration of more complete detection results. The parallel multi-scale\nattention module is proposed to effectively restore the detail information and\naddress the scale variation of salient objects by using the low-level features\nrefined by multi-scale attention. Extensive experiments on two datasets\ndemonstrate that our proposed RRNet outperforms the existing state-of-the-art\nSOD competitors both qualitatively and quantitatively.",
    "descriptor": "\nComments: 11 pages, 9 figures, Accepted by IEEE Transactions on Geoscience and Remote Sensing 2021, project: this https URL\n",
    "authors": [
      "Runmin Cong",
      "Yumo Zhang",
      "Leyuan Fang",
      "Jun Li",
      "Chunjie Zhang",
      "Yao Zhao",
      "Sam Kwong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14223"
  },
  {
    "id": "arXiv:2110.14224",
    "title": "SOAR: Minimizing Network Utilization with Bounded In-network Computing",
    "abstract": "In-network computing via smart networking devices is a recent trend for\nmodern datacenter networks. State-of-the-art switches with near line rate\ncomputing and aggregation capabilities are developed to enable, e.g.,\nacceleration and better utilization for modern applications like big data\nanalytics, and large-scale distributed and federated machine learning. We\nformulate and study the problem of activating a limited number of in-network\ncomputing devices within a network, aiming at reducing the overall network\nutilization for a given workload. Such limitations on the number of in-network\ncomputing elements per workload arise, e.g., in incremental upgrades of network\ninfrastructure, and are also due to requiring specialized middleboxes, or\nFPGAs, that should support heterogeneous workloads, and multiple tenants.\nWe present an optimal and efficient algorithm for placing such devices in\ntree networks with arbitrary link rates, and further evaluate our proposed\nsolution in various scenarios and for various tasks. Our results show that\nhaving merely a small fraction of network devices support in-network\naggregation can lead to a significant reduction in network utilization.\nFurthermore, we show that various intuitive strategies for performing such\nplacements exhibit significantly inferior performance compared to our solution,\nfor varying workloads, tasks, and link rates.",
    "descriptor": "",
    "authors": [
      "Raz Segal",
      "Chen Avin",
      "Gabriel Scalosub"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14224"
  },
  {
    "id": "arXiv:2110.14225",
    "title": "The Finite Cell Method with Least Squares Stabilized Nitsche Boundary  Conditions",
    "abstract": "We apply the recently developed least squares stabilized symmetric Nitsche\nmethod for enforcement of Dirichlet boundary conditions to the finite cell\nmethod. The least squares stabilized Nitsche method in combination with finite\ncell stabilization leads to a symmetric positive definite stiffness matrix and\nrelies only on elementwise stabilization, which does not lead to additional\nfill in. We prove a priori error estimates and bounds on the condition numbers.",
    "descriptor": "",
    "authors": [
      "Karl Larsson",
      "Stefan Kollmannsberger",
      "Ernst Rank",
      "Mats G. Larson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14225"
  },
  {
    "id": "arXiv:2110.14227",
    "title": "Emoji-based Co-attention Network for Microblog Sentiment Analysis",
    "abstract": "Emojis are widely used in online social networks to express emotions,\nattitudes, and opinions. As emotional-oriented characters, emojis can be\nmodeled as important features of emotions towards the recipient or subject for\nsentiment analysis. However, existing methods mainly take emojis as heuristic\ninformation that fails to resolve the problem of ambiguity noise. Recent\nresearches have utilized emojis as an independent input to classify text\nsentiment but they ignore the emotional impact of the interaction between text\nand emojis. It results that the emotional semantics of emojis cannot be fully\nexplored. In this paper, we propose an emoji-based co-attention network that\nlearns the mutual emotional semantics between text and emojis on microblogs.\nOur model adopts the co-attention mechanism based on bidirectional long\nshort-term memory incorporating the text and emojis, and integrates a\nsqueeze-and-excitation block in a convolutional neural network classifier to\nincrease its sensitivity to emotional semantic features. Experimental results\nshow that the proposed method can significantly outperform several baselines\nfor sentiment analysis on short texts of social media.",
    "descriptor": "",
    "authors": [
      "Xiaowei Yuan",
      "Jingyuan Hu",
      "Xiaodan Zhang",
      "Honglei Lv",
      "Hao Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14227"
  },
  {
    "id": "arXiv:2110.14228",
    "title": "Usability Inspection: Novice Crowd Inspectors versus Expert",
    "abstract": "Objective: This research study aims to investigate the use of novice crowd\ninspectors for usability inspection with respect to time spent and the cost\nincurred. This study compares the results of the novice crowd usability\ninspection guided by a single expert's heuristic usability inspection (novice\ncrowd usability inspection henceforth) with the expert heuristic usability\ninspection. Background: Traditional usability evaluation methods are time\nconsuming and expensive. Crowdsourcing has emerged as a cost effective and\nquick means of software usability evaluation. Method: In this regard, we\ndesigned an experiment to evaluate the usability of two websites and a web\ndashboard. Results: The results of the experiment show that novice crowd\nusability inspection guided by a single expert's heuristic usability\ninspection: a). Finds the same usability issues (w.r.t. content & quantity) as\nexpert heuristic usability inspection. b). Is cost effective than expert\nheuristic usability inspection employing less time duration. Conclusion: Based\non the findings of this research study, we can conclude that the novice crowd\nusability inspection guided by a single expert's heuristic usability inspection\nand expert heuristic usability inspection, on average, gives the same results\nin terms of issues identified.",
    "descriptor": "",
    "authors": [
      "Muhammad Nasir",
      "Naveed Ikram",
      "Zakia Jalil"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.14228"
  },
  {
    "id": "arXiv:2110.14230",
    "title": "Systematic definition and classification of data anomalies in DBMS  (English Version)",
    "abstract": "There is no unified definition of Data anomalies, which refers to the\nspecific data operation mode that may violate the consistency of the database.\nKnown data anomalies include Dirty Write, Dirty Read, Non-repeatable Read,\nPhantom, Read Skew and Write Skew, etc. In order to improve the efficiency of\nconcurrency control algorithms, data anomalies are also used to define the\nisolation levels, because the weak isolation level can improve the efficiency\nof transaction processing systems. This paper systematically studies the data\nanomalies and the corresponding isolation levels. We report twenty-two new data\nanomalies that other papers have not reported, and all data anomalies are\nclassified miraculously. Based on the classification of data anomalies, two new\nisolation levels systems with different granularity are proposed, which reveals\nthe rule of defining isolation levels based on data anomalies and makes the\ncognition of data anomalies and isolation levels more concise.",
    "descriptor": "",
    "authors": [
      "Li Hai-Xiang",
      "Li Xiao-Yan",
      "Liu Chang",
      "Du Xiao-Yong",
      "Lu Wei",
      "Pan An-Qun"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.14230"
  },
  {
    "id": "arXiv:2110.14237",
    "title": "Learning Graph Cellular Automata",
    "abstract": "Cellular automata (CA) are a class of computational models that exhibit rich\ndynamics emerging from the local interaction of cells arranged in a regular\nlattice. In this work we focus on a generalised version of typical CA, called\ngraph cellular automata (GCA), in which the lattice structure is replaced by an\narbitrary graph. In particular, we extend previous work that used convolutional\nneural networks to learn the transition rule of conventional CA and we use\ngraph neural networks to learn a variety of transition rules for GCA. First, we\npresent a general-purpose architecture for learning GCA, and we show that it\ncan represent any arbitrary GCA with finite and discrete state space. Then, we\ntest our approach on three different tasks: 1) learning the transition rule of\na GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of\nflocking agents; 3) learning a rule that converges to a desired target state.",
    "descriptor": "\nComments: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Daniele Grattarola",
      "Lorenzo Livi",
      "Cesare Alippi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14237"
  },
  {
    "id": "arXiv:2110.14238",
    "title": "History Determinism vs. Good for Gameness in Quantitative Automata",
    "abstract": "Automata models between determinism and nondeterminism/alternations can\nretain some of the algorithmic properties of deterministic automata while\nenjoying some of the expressiveness and succinctness of nondeterminism. We\nstudy three closely related such models -- history determinism, good for\ngameness and determinisability by pruning -- on quantitative automata. While in\nthe Boolean setting, history determinism and good for gameness coincide, we\nshow that this is no longer the case in the quantitative setting: good for\ngameness is broader than history determinism, and coincides with a relaxed\nversion of it, defined with respect to thresholds. We further identify criteria\nin which history determinism, which is generally broader than determinisability\nby pruning, coincides with it, which we then apply to typical quantitative\nautomata types. As a key application of good for games and history\ndeterministic automata is synthesis, we clarify the relationship between the\ntwo notions and various quantitative synthesis problems. We show that\ngood-for-games automata are central for \"global\" (classical) synthesis, while\n\"local\" (good-enough) synthesis reduces to deciding whether a nondeterministic\nautomaton is history deterministic.",
    "descriptor": "\nComments: Accepted for publication at FSTTCS 2021\n",
    "authors": [
      "Udi Boker",
      "Karoliina Lehtinen"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.14238"
  },
  {
    "id": "arXiv:2110.14239",
    "title": "Sharding and HTTP/2 Connection Reuse Revisited: Why Are There Still  Redundant Connections?",
    "abstract": "HTTP/2 and HTTP/3 avoid concurrent connections but instead multiplex requests\nover a single connection. Besides enabling new features, this reduces overhead\nand enables fair bandwidth sharing. Redundant connections should hence be a\nstory of the past with HTTP/2. However, they still exist, potentially hindering\ninnovation and performance. Thus, we measure their spread and analyze their\ncauses in this paper. We find that 36% - 72% of the 6.24M HTTP Archive and 78%\nof the Alexa Top 100k websites cause Chromium-based webbrowsers to open\nsuperfluous connections. We mainly attribute these to domain sharding, despite\nHTTP/2 efforts to revert it, and DNS load balancing, but also the Fetch\nStandard.",
    "descriptor": "\nComments: Accepted at the ACM Internet Measurement Conference 2021 (IMC'21)\n",
    "authors": [
      "Constantin Sander",
      "Leo Bl\u00f6cher",
      "Klaus Wehrle",
      "Jan R\u00fcth"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.14239"
  },
  {
    "id": "arXiv:2110.14240",
    "title": "2nd Place Solution for VisDA 2021 Challenge -- Universally Domain  Adaptive Image Recognition",
    "abstract": "The Visual Domain Adaptation (VisDA) 2021 Challenge calls for unsupervised\ndomain adaptation (UDA) methods that can deal with both input distribution\nshift and label set variance between the source and target domains. In this\nreport, we introduce a universal domain adaptation (UniDA) method by\naggregating several popular feature extraction and domain adaptation schemes.\nFirst, we utilize VOLO, a Transformer-based architecture with state-of-the-art\nperformance in several visual tasks, as the backbone to extract effective\nfeature representations. Second, we modify the open-set classifier of OVANet to\nrecognize the unknown class with competitive accuracy and robustness. As shown\nin the leaderboard, our proposed UniDA method ranks the 2nd place with 48.56%\nACC and 70.72% AUROC in the VisDA 2021 Challenge.",
    "descriptor": "",
    "authors": [
      "Haojin Liao",
      "Xiaolin Song",
      "Sicheng Zhao",
      "Shanghang Zhang",
      "Xiangyu Yue",
      "Xingxu Yao",
      "Yueming Zhang",
      "Tengfei Xing",
      "Pengfei Xu",
      "Qiang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14240"
  },
  {
    "id": "arXiv:2110.14241",
    "title": "Dynamic population-based meta-learning for multi-agent communication  with natural language",
    "abstract": "In this work, our goal is to train agents that can coordinate with seen,\nunseen as well as human partners in a multi-agent communication environment\ninvolving natural language. Previous work using a single set of agents has\nshown great progress in generalizing to known partners, however it struggles\nwhen coordinating with unfamiliar agents. To mitigate that, recent work\nexplored the use of population-based approaches, where multiple agents interact\nwith each other with the goal of learning more generic protocols. These\nmethods, while able to result in good coordination between unseen partners,\nstill only achieve so in cases of simple languages, thus failing to adapt to\nhuman partners using natural language. We attribute this to the use of static\npopulations and instead propose a dynamic population-based meta-learning\napproach that builds such a population in an iterative manner. We perform a\nholistic evaluation of our method on two different referential games, and show\nthat our agents outperform all prior work when communicating with seen partners\nand humans. Furthermore, we analyze the natural language generation skills of\nour agents, where we find that our agents also outperform strong baselines.\nFinally, we test the robustness of our agents when communicating with\nout-of-population agents and carefully test the importance of each component of\nour method through ablation studies.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Abhinav Gupta",
      "Marc Lanctot",
      "Angeliki Lazaridou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.14241"
  },
  {
    "id": "arXiv:2110.14242",
    "title": "Tight FPT Approximation for Constrained k-Center and k-Supplier",
    "abstract": "In this work, we study a range of constrained versions of the $k$-supplier\nand $k$-center problems such as: capacitated, fault-tolerant, fair, etc. These\nproblems fall under a broad framework of constrained clustering. A unified\nframework for constrained clustering was proposed by Ding and Xu [SODA 2015] in\ncontext of the $k$-median and $k$-means objectives. In this work, we extend\nthis framework to the $k$-supplier and $k$-center objectives. This unified\nframework allows us to obtain results simultaneously for the following\nconstrained versions of the $k$-supplier problem: $r$-gather, $r$-capacity,\nbalanced, chromatic, fault-tolerant, strongly private, $\\ell$-diversity, and\nfair $k$-supplier problems, with and without outliers. We obtain the following\nresults: We give $3$ and $2$ approximation algorithms for the constrained\n$k$-supplier and $k$-center problems, respectively, with $\\mathsf{FPT}$ running\ntime $k^{O(k)} \\cdot n^{O(1)}$, where $n = |C \\cup L|$. Moreover, these\napproximation guarantees are tight; that is, for any constant $\\epsilon>0$, no\nalgorithm can achieve $(3-\\epsilon)$ and $(2-\\epsilon)$ approximation\nguarantees for the constrained $k$-supplier and $k$-center problems in\n$\\mathsf{FPT}$ time, assuming $\\mathsf{FPT} \\neq \\mathsf{W}[2]$. Furthermore,\nwe study these constrained problems in outlier setting. Our algorithm gives $3$\nand $2$ approximation guarantees for the constrained outlier $k$-supplier and\n$k$-center problems, respectively, with $\\mathsf{FPT}$ running time\n$(k+m)^{O(k)} \\cdot n^{O(1)}$, where $n = |C \\cup L|$ and $m$ is the number of\noutliers.",
    "descriptor": "",
    "authors": [
      "Dishant Goyal",
      "Ragesh Jaiswal"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14242"
  },
  {
    "id": "arXiv:2110.14243",
    "title": "Online Selective Classification with Limited Feedback",
    "abstract": "Motivated by applications to resource-limited and safety-critical domains, we\nstudy selective classification in the online learning model, wherein a\npredictor may abstain from classifying an instance. For example, this may model\nan adaptive decision to invoke more resources on this instance. Two salient\naspects of the setting we consider are that the data may be non-realisable, due\nto which abstention may be a valid long-term action, and that feedback is only\nreceived when the learner abstains, which models the fact that reliable labels\nare only available when the resource intensive processing is invoked.\nWithin this framework, we explore strategies that make few mistakes, while\nnot abstaining too many times more than the best-in-hindsight error-free\nclassifier from a given class. That is, the one that makes no mistakes, while\nabstaining the fewest number of times. We construct simple versioning-based\nschemes for any $\\mu \\in (0,1],$ that make most $T^\\mu$ mistakes while\nincurring \\smash{$\\tilde{O}(T^{1-\\mu})$} excess abstention against adaptive\nadversaries. We further show that this dependence on $T$ is tight, and provide\nillustrative experiments on realistic datasets.",
    "descriptor": "\nComments: To appear at NeurIPS 2021\n",
    "authors": [
      "Aditya Gangrade",
      "Anil Kag",
      "Ashok Cutkosky",
      "Venkatesh Saligrama"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14243"
  },
  {
    "id": "arXiv:2110.14245",
    "title": "Capturing Dynamics of Information Diffusion in SNS: A Survey of  Methodology and Techniques",
    "abstract": "Studying information diffusion in SNS (Social Networks Service) has\nremarkable significance in both academia and industry. Theoretically, it boosts\nthe development of other subjects such as statistics, sociology, and data\nmining. Practically, diffusion modeling provides fundamental support for many\ndownstream applications (\\textit{e.g.}, public opinion monitoring, rumor source\nidentification, and viral marketing.) Tremendous efforts have been devoted to\nthis area to understand and quantify information diffusion dynamics. This\nsurvey investigates and summarizes the emerging distinguished works in\ndiffusion modeling. We first put forward a unified information diffusion\nconcept in terms of three components: information, user decision, and social\nvectors, followed by a detailed introduction of the methodologies for diffusion\nmodeling. And then, a new taxonomy adopting hybrid philosophy (\\textit{i.e.,}\ngranularity and techniques) is proposed, and we made a series of comparative\nstudies on elementary diffusion models under our taxonomy from the aspects of\nassumptions, methods, and pros and cons. We further summarized representative\ndiffusion modeling in special scenarios and significant downstream tasks based\non these elementary models. Finally, open issues in this field following the\nmethodology of diffusion modeling are discussed.",
    "descriptor": "\nComments: Author version, with 50 pages, 6 figures, 16 tables, and 5 algorithms\n",
    "authors": [
      "Huacheng Li",
      "Chunhe Xia",
      "Tianbo Wang",
      "Sheng Wen",
      "Chao Chen",
      "Yang Xiang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.14245"
  },
  {
    "id": "arXiv:2110.14248",
    "title": "Learning Domain Invariant Representations in Goal-conditioned Block MDPs",
    "abstract": "Deep Reinforcement Learning (RL) is successful in solving many complex Markov\nDecision Processes (MDPs) problems. However, agents often face unanticipated\nenvironmental changes after deployment in the real world. These changes are\noften spurious and unrelated to the underlying problem, such as background\nshifts for visual input agents. Unfortunately, deep RL policies are usually\nsensitive to these changes and fail to act robustly against them. This\nresembles the problem of domain generalization in supervised learning. In this\nwork, we study this problem for goal-conditioned RL agents. We propose a\ntheoretical framework in the Block MDP setting that characterizes the\ngeneralizability of goal-conditioned policies to new environments. Under this\nframework, we develop a practical method PA-SkewFit that enhances domain\ngeneralization. The empirical evaluation shows that our goal-conditioned RL\nagent can perform well in various unseen test environments, improving by 50%\nover baselines.",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Beining Han",
      "Chongyi Zheng",
      "Harris Chan",
      "Keiran Paster",
      "Michael R. Zhang",
      "Jimmy Ba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14248"
  },
  {
    "id": "arXiv:2110.14254",
    "title": "Multilayer Lookahead: a Nested Version of Lookahead",
    "abstract": "In recent years, SGD and its variants have become the standard tool to train\nDeep Neural Networks. In this paper, we focus on the recently proposed variant\nLookahead, which improves upon SGD in a wide range of applications. Following\nthis success, we study an extension of this algorithm, the \\emph{Multilayer\nLookahead} optimizer, which recursively wraps Lookahead around itself. We prove\nthe convergence of Multilayer Lookahead with two layers to a stationary point\nof smooth non-convex functions with $O(\\frac{1}{\\sqrt{T}})$ rate. We also\njustify the improved generalization of both Lookahead over SGD, and of\nMultilayer Lookahead over Lookahead, by showing how they amplify the implicit\nregularization effect of SGD. We empirically verify our results and show that\nMultilayer Lookahead outperforms Lookahead on CIFAR-10 and CIFAR-100\nclassification tasks, and on GANs training on the MNIST dataset.",
    "descriptor": "",
    "authors": [
      "Denys Pushkin",
      "Luis Barba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14254"
  },
  {
    "id": "arXiv:2110.14256",
    "title": "Cascaded Classifier for Pareto-Optimal Accuracy-Cost Trade-Off Using  off-the-Shelf ANNs",
    "abstract": "Machine-learning classifiers provide high quality of service in\nclassification tasks. Research now targets cost reduction measured in terms of\naverage processing time or energy per solution. Revisiting the concept of\ncascaded classifiers, we present a first of its kind analysis of optimal\npass-on criteria between the classifier stages. Based on this analysis, we\nderive a methodology to maximize accuracy and efficiency of cascaded\nclassifiers. On the one hand, our methodology allows cost reduction of 1.32x\nwhile preserving reference classifier's accuracy. On the other hand, it allows\nto scale cost over two orders while gracefully degrading accuracy. Thereby, the\nfinal classifier stage sets the top accuracy. Hence, the multi-stage\nrealization can be employed to optimize any state-of-the-art classifier.",
    "descriptor": "\nComments: The final accepted publication was presented on the 7th International Conference on Machine Learning, Optimization, Data Science (LOD), October 4 - 8, 2021 in Grasmere, Lake District, England\n",
    "authors": [
      "Cecilia Latotzke",
      "Johnson Loh",
      "Tobias Gemmeke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14256"
  },
  {
    "id": "arXiv:2110.14264",
    "title": "Kalman-Like Filter under Binary Sensors",
    "abstract": "This paper is concerned with the linear/nonlinear Kalman-like filtering\nproblem under binary sensors. Since innovation represents new information in\nthe sensor measurement and serves to correct the prediction for the Kalman-like\nfilter (KLF), a novel uncertain measurement model is proposed such that the\ninnovation generated from binary sensor can be captured. When considering\nlinear dynamic systems, a conservative estimation error covariance with\nadjustable parameters is constructed by matrix inequality, and then an optimal\nfilter gain is derived by minimizing its trace. Meanwhile, the optimal\nselection criterion of an adjustable parameter is developed by minimizing the\nupper bound of the conservative estimation error covariance. When considering\nnonlinear dynamic systems, a conservative estimation error covariance with\nadjustable parameters is also constructed via unscented transform and matrix\ninequalities. Then, following the idea of designing KLF in linear dynamic\nsystems, the nonlinear filter gain and the optimal adjustable parameter are\ndesigned. Finally, $O_2$ content estimation and nonlinear numerical system are\nemployed to show the effectiveness and advantages of the proposed methods.",
    "descriptor": "\nComments: 8 pages, 6 figures\n",
    "authors": [
      "Zhongyao Hu",
      "Bo Chen",
      "Yuchen Zhang",
      "Li Yu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14264"
  },
  {
    "id": "arXiv:2110.14266",
    "title": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical  Reasoning",
    "abstract": "State-of-the-art approaches to reasoning and question answering over\nknowledge graphs (KGs) usually scale with the number of edges and can only be\napplied effectively on small instance-dependent subgraphs. In this paper, we\naddress this issue by showing that multi-hop and more complex logical reasoning\ncan be accomplished separately without losing expressive power. Motivated by\nthis insight, we propose an approach to multi-hop reasoning that scales\nlinearly with the number of relation types in the graph, which is usually\nsignificantly smaller than the number of edges or nodes. This produces a set of\ncandidate solutions that can be provably refined to recover the solution to the\noriginal problem. Our experiments on knowledge-based question answering show\nthat our approach solves the multi-hop MetaQA dataset, achieves a new\nstate-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude\nmore scalable than competitive approaches, and can achieve compositional\ngeneralization out of the training distribution.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Mattia Atzeni",
      "Jasmina Bogojeska",
      "Andreas Loukas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.14266"
  },
  {
    "id": "arXiv:2110.14270",
    "title": "Counterfactual Shapley Additive Explanations",
    "abstract": "Feature attributions are a common paradigm for model explanations due to\ntheir simplicity in assigning a single numeric score for each input feature to\na model. In the actionable recourse setting, wherein the goal of the\nexplanations is to improve outcomes for model consumers, it is often unclear\nhow feature attributions should be correctly used. With this work, we aim to\nstrengthen and clarify the link between actionable recourse and feature\nattributions. Concretely, we propose a variant of SHAP, CoSHAP, that uses\ncounterfactual generation techniques to produce a background dataset for use\nwithin the marginal (a.k.a. interventional) Shapley value framework. We\nmotivate the need within the actionable recourse setting for careful\nconsideration of background datasets when using Shapley values for feature\nattributions, alongside the requirement for monotonicity, with numerous\nsynthetic examples. Moreover, we demonstrate the efficacy of CoSHAP by\nproposing and justifying a quantitative score for feature attributions,\ncounterfactual-ability, showing that as measured by this metric, CoSHAP is\nsuperior to existing methods when evaluated on public datasets using monotone\ntree ensembles.",
    "descriptor": "\nComments: Accepted to XAI-FIN (Workshop on Explainable AI in Finance) at ICAIF '21 (this https URL)\n",
    "authors": [
      "Emanuele Albini",
      "Jason Long",
      "Danial Dervovic",
      "Daniele Magazzeni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14270"
  },
  {
    "id": "arXiv:2110.14271",
    "title": "Mechanisms for Trading Durable Goods",
    "abstract": "We consider trading indivisible and easily transferable \\emph{durable goods},\nwhich are goods that an agent can receive, use, and trade again for a different\ngood. This is often the case with books that can be read and later exchanged\nfor unread ones. Other examples of such easily transferable durable goods\ninclude puzzles, video games and baby clothes.\nWe introduce a model for the exchange of easily transferable durable goods.\nIn our model, each agent owns a set of items and demands a different set of\nitems. An agent is interested in receiving as many items as possible from his\ndemand set. We consider mechanisms that exchange items in cycles in which each\nparticipating agent receives an item that he demands and gives an item that he\nowns. We aim to develop mechanisms that have the following properties: they are\n\\emph{efficient}, in the sense that they maximize the total number of items\nthat agents receive from their demand set, they are \\emph{strategyproof} (i.e.,\nit is in the agents' best interest to report their preferences truthfully) and\nthey run in \\emph{polynomial time}.\nOne challenge in developing mechanisms for our setting is that the supply and\ndemand sets of the agents are updated after a trade cycle is executed. This\nmakes constructing strategyproof mechanisms in our model significantly\ndifferent from previous works, both technically and conceptually and requires\ndeveloping new tools and techniques. We prove that simultaneously satisfying\nall desired properties is impossible and thus focus on studying the tradeoffs\nbetween these properties. To this end, we provide both approximation algorithms\nand impossibility results.",
    "descriptor": "\nComments: WINE'21\n",
    "authors": [
      "Sigal Oren",
      "Oren Roth"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.14271"
  },
  {
    "id": "arXiv:2110.14273",
    "title": "Deep Learning For Prominence Detection In Children's Read Speech",
    "abstract": "The detection of perceived prominence in speech has attracted approaches\nranging from the design of linguistic knowledge-based acoustic features to the\nautomatic feature learning from suprasegmental attributes such as pitch and\nintensity contours. We present here, in contrast, a system that operates\ndirectly on segmented speech waveforms to learn features relevant to prominent\nword detection for children's oral fluency assessment. The chosen CRNN\n(convolutional recurrent neural network) framework, incorporating both\nword-level features and sequence information, is found to benefit from the\nperceptually motivated SincNet filters as the first convolutional layer. We\nfurther explore the benefits of the linguistic association between the prosodic\nevents of phrase boundary and prominence with different multi-task\narchitectures. Matching the previously reported performance on the same dataset\nof a random forest ensemble predictor trained on carefully chosen hand-crafted\nacoustic features, we evaluate further the possibly complementary information\nfrom hand-crafted acoustic and pre-trained lexical features.",
    "descriptor": "\nComments: Under review at ICASSP 2022. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Mithilesh Vaidya",
      "Kamini Sabu",
      "Preeti Rao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14273"
  },
  {
    "id": "arXiv:2110.14277",
    "title": "Hybrid consensus for multi-agent systems with time-driven jumps",
    "abstract": "In this paper, the behavior of scalar multi-agent systems over networks\nsubject to time-driven jumps. Assuming that all agents communicate through\ndistinct communication digraphs at jump and flow times, the asymptotic\nmulti-consensus behavior of the hybrid network is explicitly characterized. The\nhybrid multi-consensus is shown to be associated with a suitable partition that\nis almost equitable for both the jump and flow communication digraphs. In doing\nso, no assumption on the underlying digraphs is introduced. Finally, the\ncoupling rules making the multi-consensus subspace attractive are established.\nSeveral simulation examples illustrate the theoretical results.",
    "descriptor": "\nComments: Nonlinear Analysis: Hybrid Systems, Elsevier, In press\n",
    "authors": [
      "Andrea Cristofaro",
      "Mattia Mattioni"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14277"
  },
  {
    "id": "arXiv:2110.14283",
    "title": "How Important is Importance Sampling for Deep Budgeted Training?",
    "abstract": "Long iterative training processes for Deep Neural Networks (DNNs) are\ncommonly required to achieve state-of-the-art performance in many computer\nvision tasks. Importance sampling approaches might play a key role in budgeted\ntraining regimes, i.e. when limiting the number of training iterations. These\napproaches aim at dynamically estimating the importance of each sample to focus\non the most relevant and speed up convergence. This work explores this paradigm\nand how a budget constraint interacts with importance sampling approaches and\ndata augmentation techniques. We show that under budget restrictions,\nimportance sampling approaches do not provide a consistent improvement over\nuniform sampling. We suggest that, given a specific budget, the best course of\naction is to disregard the importance and introduce adequate data augmentation;\ne.g. when reducing the budget to a 30% in CIFAR-10/100, RICAP data augmentation\nmaintains accuracy, while importance sampling does not. We conclude from our\nwork that DNNs under budget restrictions benefit greatly from variety in the\ntraining set and that finding the right samples to train on is not the most\neffective strategy when balancing high performance with low computational\nrequirements. Source code available at https://git.io/JKHa3 .",
    "descriptor": "\nComments: British Machine Vision Conference (BMVC) 2021, oral presentation\n",
    "authors": [
      "Eric Arazo",
      "Diego Ortego",
      "Paul Albert",
      "Noel E. O'Connor",
      "Kevin McGuinness"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14283"
  },
  {
    "id": "arXiv:2110.14284",
    "title": "MIRA: Multihop Relation Prediction in Temporal Knowledge Graphs",
    "abstract": "In knowledge graph reasoning, we observe a trend to analyze temporal data\nevolving over time. The additional temporal dimension is attached to facts in a\nknowledge base resulting in quadruples between entities such as (Nintendo,\nreleased, Super Mario, Sep-13-1985), where the relation between two entities is\nassociated to a specific time interval or point in time. Multi-hop reasoning on\ninferred subgraphs connecting entities within a knowledge graph can be\nformulated as a reinforcement learning task where the agent sequentially\nperforms inference upon the explored subgraph. The task in this work is to\ninfer the predicate between a subject and an object entity, i.e., (subject, ?,\nobject, time), being valid at a certain timestamp or time interval. Given query\nentities, our agent starts to gather temporal relevant information about the\nneighborhood of the subject and object. The encoding of information about the\nexplored graph structures is referred to as fingerprints. Subsequently, we use\nthe two fingerprints as input to a Q-Network. Our agent decides sequentially\nwhich relational type needs to be explored next expanding the local subgraphs\nof the query entities in order to find promising paths between them. The\nevaluation shows that the proposed method not only yields results being in line\nwith state-of-the-art embedding algorithms for temporal Knowledge Graphs (tKG),\nbut we also gain information about the relevant structures between subjects and\nobjects.",
    "descriptor": "",
    "authors": [
      "Christian M.M. Frey",
      "Yunpu Ma",
      "Matthias Schubert"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14284"
  },
  {
    "id": "arXiv:2110.14286",
    "title": "TopicNet: Semantic Graph-Guided Topic Discovery",
    "abstract": "Existing deep hierarchical topic models are able to extract semantically\nmeaningful topics from a text corpus in an unsupervised manner and\nautomatically organize them into a topic hierarchy. However, it is unclear how\nto incorporate prior beliefs such as knowledge graph to guide the learning of\nthe topic hierarchy. To address this issue, we introduce TopicNet as a deep\nhierarchical topic model that can inject prior structural knowledge as an\ninductive bias to influence learning. TopicNet represents each topic as a\nGaussian-distributed embedding vector, projects the topics of all layers into a\nshared embedding space, and explores both the symmetric and asymmetric\nsimilarities between Gaussian embedding vectors to incorporate prior semantic\nhierarchies. With an auto-encoding variational inference network, the model\nparameters are optimized by minimizing the evidence lower bound and a\nregularization term via stochastic gradient descent. Experiments on widely used\nbenchmarks show that TopicNet outperforms related deep topic models on\ndiscovering deeper interpretable topics and mining better\ndocument~representations.",
    "descriptor": "",
    "authors": [
      "Zhibin Duan",
      "Yishi Xu",
      "Bo Chen",
      "Dongsheng Wang",
      "Chaojie Wang",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.14286"
  },
  {
    "id": "arXiv:2110.14287",
    "title": "CG2A: Conceptual Graphs Generation Algorithm",
    "abstract": "Conceptual Graphs (CGs) are a formalism to represent knowledge. However\nproducing a CG database is complex. To the best of our knowledge, existing\nmethods do not fully use the expressivity of CGs. It is particularly\ntroublesome as it is necessary to have CG databases to test and validate\nalgorithms running on CGs. This paper proposes CG2A, an algorithm to build\nsynthetic CGs exploiting most of their expressivity. CG2A takes as input\nconstraints that constitute ontological knowledge including a vocabulary and a\nset of CGs with some label variables, called $\\gamma$-CGs, as components of the\ngenerated CGs. Extensions also enable the automatic generation of the set of\n$\\gamma$-CGs and vocabulary to ease the database generation and increase\nvariability.",
    "descriptor": "",
    "authors": [
      "Adam Faci",
      "Marie-Jeanne Lesot",
      "Claire Laudy"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.14287"
  },
  {
    "id": "arXiv:2110.14292",
    "title": "Arbitrarily high-order methods for Poisson problems",
    "abstract": "In this paper we are concerned with energy-conserving methods for Poisson\nproblems, which are effectively solved by defining a suitable generalization of\nHBVMs, a class of energy-conserving methods for Hamiltonian problems. The\nactual implementation of the methods is fully discussed, with a particular\nemphasis on the conservation of Casimirs. Some numerical tests are reported, in\norder to assess the theoretical findings.",
    "descriptor": "\nComments: 29 pages, 5 figures, 3 tables\n",
    "authors": [
      "Pierluigi Amodio",
      "Luigi Brugnano",
      "Felice Iavernaro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14292"
  },
  {
    "id": "arXiv:2110.14295",
    "title": "A Subgame Perfect Equilibrium Reinforcement Learning Approach to  Time-inconsistent Problems",
    "abstract": "In this paper, we establish a subgame perfect equilibrium reinforcement\nlearning (SPERL) framework for time-inconsistent (TIC) problems. In the context\nof RL, TIC problems are known to face two main challenges: the non-existence of\nnatural recursive relationships between value functions at different time\npoints and the violation of Bellman's principle of optimality that raises\nquestions on the applicability of standard policy iteration algorithms for\nunprovable policy improvement theorems. We adapt an extended dynamic\nprogramming theory and propose a new class of algorithms, called backward\npolicy iteration (BPI), that solves SPERL and addresses both challenges. To\ndemonstrate the practical usage of BPI as a training framework, we adapt\nstandard RL simulation methods and derive two BPI-based training algorithms. We\nexamine our derived training frameworks on a mean-variance portfolio selection\nproblem and evaluate some performance metrics including convergence and model\nidentifiability.",
    "descriptor": "",
    "authors": [
      "Nixie S. Lesmana",
      "Chi Seng Pun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.14295"
  },
  {
    "id": "arXiv:2110.14296",
    "title": "Learning Stable Deep Dynamics Models for Partially Observed or Delayed  Dynamical Systems",
    "abstract": "Learning how complex dynamical systems evolve over time is a key challenge in\nsystem identification. For safety critical systems, it is often crucial that\nthe learned model is guaranteed to converge to some equilibrium point. To this\nend, neural ODEs regularized with neural Lyapunov functions are a promising\napproach when states are fully observed. For practical applications however,\npartial observations are the norm. As we will demonstrate, initialization of\nunobserved augmented states can become a key problem for neural ODEs. To\nalleviate this issue, we propose to augment the system's state with its\nhistory. Inspired by state augmentation in discrete-time systems, we thus\nobtain neural delay differential equations. Based on classical time delay\nstability analysis, we then show how to ensure stability of the learned models,\nand theoretically analyze our approach. Our experiments demonstrate its\napplicability to stable system identification of partially observed systems and\nlearning a stabilizing feedback policy in delayed feedback control.",
    "descriptor": "\nComments: Published at NeurIPS 2021\n",
    "authors": [
      "Andreas Schlaginhaufen",
      "Philippe Wenk",
      "Andreas Krause",
      "Florian D\u00f6rfler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14296"
  },
  {
    "id": "arXiv:2110.14297",
    "title": "Revisiting Sanity Checks for Saliency Maps",
    "abstract": "Saliency methods are a popular approach for model debugging and\nexplainability. However, in the absence of ground-truth data for what the\ncorrect maps should be, evaluating and comparing different approaches remains a\nlong-standing challenge. The sanity checks methodology of Adebayo et al\n[Neurips 2018] has sought to address this challenge. They argue that some\npopular saliency methods should not be used for explainability purposes since\nthe maps they produce are not sensitive to the underlying model that is to be\nexplained. Through a causal re-framing of their objective, we argue that their\nempirical evaluation does not fully establish these conclusions, due to a form\nof confounding introduced by the tasks they evaluate on. Through various\nexperiments on simple custom tasks we demonstrate that some of their\nconclusions may indeed be artifacts of the tasks more than a criticism of the\nsaliency methods themselves. More broadly, our work challenges the utility of\nthe sanity check methodology, and further highlights that saliency map\nevaluation beyond ad-hoc visual examination remains a fundamental challenge.",
    "descriptor": "",
    "authors": [
      "Gal Yona",
      "Daniel Greenfeld"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.14297"
  },
  {
    "id": "arXiv:2110.14300",
    "title": "Multi-Agent Reinforcement Learning for Active Voltage Control on Power  Distribution Networks",
    "abstract": "This paper presents a problem in power networks that creates an exciting and\nyet challenging real-world scenario for application of multi-agent\nreinforcement learning (MARL). The emerging trend of decarbonisation is placing\nexcessive stress on power distribution networks. Active voltage control is seen\nas a promising solution to relieve power congestion and improve voltage quality\nwithout extra hardware investment, taking advantage of the controllable\napparatuses in the network, such as roof-top photovoltaics (PVs) and static var\ncompensators (SVCs). These controllable apparatuses appear in a vast number and\nare distributed in a wide geographic area, making MARL a natural candidate.\nThis paper formulates the active voltage control problem in the framework of\nDec-POMDP and establishes an open-source environment. It aims to bridge the gap\nbetween the power community and the MARL community and be a drive force towards\nreal-world applications of MARL algorithms. Finally, we analyse the special\ncharacteristics of the active voltage control problems that cause challenges\nfor state-of-the-art MARL approaches, and summarise the potential directions.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Jianhong Wang",
      "Wangkun Xu",
      "Yunjie Gu",
      "Wenbin Song",
      "Tim C. Green"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.14300"
  },
  {
    "id": "arXiv:2110.14301",
    "title": "From Complexity Measurement to Holistic Quality Evaluation for  Automotive Software Development",
    "abstract": "In recent years, the role and the importance of software in the automotive\ndomain have changed dramatically. Being able to systematically evaluate and\nmanage software quality is becoming even more crucial. In practice, however, we\nstill find a largely static approach for measuring software quality based on a\npredefined list of complexity metrics with static thresholds to fulfill. We\npropose using a more flexible framework instead, which systematically derives\nmeasures and evaluation rules based on the goals and context of a development\nproject.",
    "descriptor": "\nComments: Position Paper, 21 pages, 1 figure, and 3 tables\n",
    "authors": [
      "Jens Heidrich",
      "Michael Kl\u00e4s",
      "Andreas Morgenstern",
      "Pablo Oliveira Antonino",
      "Adam Trendowicz",
      "Jochen Quante",
      "Thomas Grundler"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.14301"
  },
  {
    "id": "arXiv:2110.14307",
    "title": "RF-Based Human Activity Recognition Using Signal Adapted Convolutional  Neural Network",
    "abstract": "Human Activity Recognition (HAR) plays a critical role in a wide range of\nreal-world applications, and it is traditionally achieved via wearable sensing.\nRecently, to avoid the burden and discomfort caused by wearable devices,\ndevice-free approaches exploiting RF signals arise as a promising alternative\nfor HAR. Most of the latest device-free approaches require training a large\ndeep neural network model in either time or frequency domain, entailing\nextensive storage to contain the model and intensive computations to infer\nactivities. Consequently, even with some major advances on device-free HAR,\ncurrent device-free approaches are still far from practical in real-world\nscenarios where the computation and storage resources possessed by, for\nexample, edge devices, are limited. Therefore, we introduce HAR-SAnet which is\na novel RF-based HAR framework. It adopts an original signal adapted\nconvolutional neural network architecture: instead of feeding the handcraft\nfeatures of RF signals into a classifier, HAR-SAnet fuses them adaptively from\nboth time and frequency domains to design an end-to-end neural network model.\nWe apply point-wise grouped convolution and depth-wise separable convolutions\nto confine the model scale and to speed up the inference execution time. The\nexperiment results show that the recognition accuracy of HAR-SAnet outperforms\nstate-of-the-art algorithms and systems.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Zhe Chen",
      "Chao Cai",
      "Tianyue Zheng",
      "Jun Luo",
      "Jie Xiong",
      "Xin Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14307"
  },
  {
    "id": "arXiv:2110.14308",
    "title": "Token Games and History-Deterministic Quantitative-Automata",
    "abstract": "A nondeterministic (quantitative) automaton is history deterministic if its\nnondeterminism can be resolved by only considering the prefix of the word read\nso far. Due to their good compositional properties, history deterministic\nautomata are useful in solving games and synthesis problems. Deciding whether\nor not a given nondeterministic automaton is history deterministic (the HDness\nproblem) is generally a difficult task, which might involve an exponential\nprocedure, or even be undecidable, for example for pushdown automata. Token\ngames provide a PTime solution to the HDness problem of B\\\"uchi and coB\\\"uchi\nautomata, and it is conjectured that 2-token games characterize HDness for all\n!-regular automata. We extend token games to the quantitative setting and\nanalyze their potential to help deciding HDness for quantitative automata. In\nparticular, we show that 1-token games characterize HDness for all quantitative\n(and Boolean) automata on finite words, as well as discounted-sum (DSum)\nautomata on infinite words, and that 2-token games characterize HDness of\nLimInf and LimSup automata. Using these characterizations, we provide solutions\nto the HDness problem of Inf and Sup automata on finite words in PTime, for\nDSum automata on finite and infinite words in NP$\\cap$co-NP, for LimSup\nautomata in quasipolynomial time, and for LimInf automata in exponential time,\nwhere the latter two are only polynomial for automata with a fixed number of\nweights.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Udi Boker",
      "Karoliina Lehtinen"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.14308"
  },
  {
    "id": "arXiv:2110.14309",
    "title": "Inferring the Class Conditional Response Map for Weakly Supervised  Semantic Segmentation",
    "abstract": "Image-level weakly supervised semantic segmentation (WSSS) relies on class\nactivation maps (CAMs) for pseudo labels generation. As CAMs only highlight the\nmost discriminative regions of objects, the generated pseudo labels are usually\nunsatisfactory to serve directly as supervision. To solve this, most existing\napproaches follow a multi-training pipeline to refine CAMs for better\npseudo-labels, which includes: 1) re-training the classification model to\ngenerate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training\na semantic segmentation model with the obtained pseudo labels. However, this\nmulti-training pipeline requires complicated adjustment and additional time. To\naddress this, we propose a class-conditional inference strategy and an\nactivation aware mask refinement loss function to generate better pseudo labels\nwithout re-training the classifier. The class conditional inference-time\napproach is presented to separately and iteratively reveal the classification\nnetwork's hidden object activation to generate more complete response maps.\nFurther, our activation aware mask refinement loss function introduces a novel\nway to exploit saliency maps during segmentation training and refine the\nforeground object masks without suppressing background objects. Our method\nachieves superior WSSS results without requiring re-training of the classifier.",
    "descriptor": "",
    "authors": [
      "Weixuan Sun",
      "Jing Zhang",
      "Nick Barnes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14309"
  },
  {
    "id": "arXiv:2110.14312",
    "title": "A Visualization Authoring Model for Post-WIMP Interfaces",
    "abstract": "Besides the ability to utilize visualizations, the process of creating and\nauthoring them is of equal importance. However, for visualization environments\nbeyond the desktop, like multi-display or immersive analytics environments,\nthis process is often decoupled from the place where the visualization is\nactually used. This separation makes it hard for authors, developers, or users\nof such systems to understand, what consequences different choices they made\nwill have for the created visualizations. We present an extended visualization\nauthoring model for Post-WIMP interfaces, which support designers by a more\nseamless approach of developing and utilizing visualizations. With it, our\nemphasis is on the iterative nature of creating and configuring visualizations,\nthe existence of multiple views in the same system, and requirements for the\ndata analysis process.",
    "descriptor": "\nComments: This work has been accepted for the IEEE VIS 2021 Poster Track. (this http URL)\n",
    "authors": [
      "Marc Satkowski",
      "Weizhou Luo",
      "Raimund Dachselt"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.14312"
  },
  {
    "id": "arXiv:2110.14322",
    "title": "Node-wise Localization of Graph Neural Networks",
    "abstract": "Graph neural networks (GNNs) emerge as a powerful family of representation\nlearning models on graphs. To derive node representations, they utilize a\nglobal model that recursively aggregates information from the neighboring\nnodes. However, different nodes reside at different parts of the graph in\ndifferent local contexts, making their distributions vary across the graph.\nIdeally, how a node receives its neighborhood information should be a function\nof its local context, to diverge from the global GNN model shared by all nodes.\nTo utilize node locality without overfitting, we propose a node-wise\nlocalization of GNNs by accounting for both global and local aspects of the\ngraph. Globally, all nodes on the graph depend on an underlying global GNN to\nencode the general patterns across the graph; locally, each node is localized\ninto a unique model as a function of the global model and its local context.\nFinally, we conduct extensive experiments on four benchmark graphs, and\nconsistently obtain promising performance surpassing the state-of-the-art GNNs.",
    "descriptor": "",
    "authors": [
      "Zemin Liu",
      "Yuan Fang",
      "Chenghao Liu",
      "Steven C.H. Hoi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.14322"
  },
  {
    "id": "arXiv:2110.14325",
    "title": "Unified Resource Allocation Framework for the Edge Intelligence-Enabled  Metaverse",
    "abstract": "Dubbed as the next-generation Internet, the metaverse is a virtual world that\nallows users to interact with each other or objects in real-time using their\navatars. The metaverse is envisioned to support novel ecosystems of service\nprovision in an immersive environment brought about by an intersection of the\nvirtual and physical worlds. The native AI systems in metaverse will\npersonalized user experience over time and shape the experience in a scalable,\nseamless, and synchronous way. However, the metaverse is characterized by\ndiverse resource types amid a highly dynamic demand environment. In this paper,\nwe propose the case study of virtual education in the metaverse and address the\nunified resource allocation problem amid stochastic user demand. We propose a\nstochastic optimal resource allocation scheme (SORAS) based on stochastic\ninteger programming with the objective of minimizing the cost of the virtual\nservice provider. The simulation results show that SORAS can minimize the cost\nof the virtual service provider while accounting for the users' demands\nuncertainty.",
    "descriptor": "\nComments: 6 pages, 10 figures\n",
    "authors": [
      "Wei Chong Ng",
      "Wei Yang Bryan Lim",
      "Jer Shyuan Ng",
      "Zehui Xiong",
      "Dusit Niyato",
      "Chunyan Miao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.14325"
  },
  {
    "id": "arXiv:2110.14330",
    "title": "Multi-frequency image completion via a biologically-inspired  sub-Riemannian model with frequency and phase",
    "abstract": "We present a novel cortically-inspired image completion algorithm. It uses a\nfive dimensional sub-Riemannian cortical geometry modelling the orientation,\nspatial frequency and phase selective behavior of the cells in the visual\ncortex. The algorithm extracts the orientation, frequency and phase information\nexisting in a given two dimensional corrupted input image via a Gabor transform\nand represent those values in terms of cortical cell output responses in the\nmodel geometry. Then it performs completion via a diffusion concentrated in a\nneighbourhood along the neural connections within the model geometry. The\ndiffusion models the activity propagation integrating orientation, frequency\nand phase features along the neural connections. Finally, the algorithm\ntransforms back the diffused and completed output responses back to the two\ndimensional image plane.",
    "descriptor": "",
    "authors": [
      "Emre Baspinar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2110.14330"
  },
  {
    "id": "arXiv:2110.14331",
    "title": "GACAN: Graph Attention-Convolution-Attention Networks for Traffic  Forecasting Based on Multi-granularity Time Series",
    "abstract": "Traffic forecasting is an integral part of intelligent transportation systems\n(ITS). Achieving a high prediction accuracy is a challenging task due to a high\nlevel of dynamics and complex spatial-temporal dependency of road networks. For\nthis task, we propose Graph Attention-Convolution-Attention Networks (GACAN).\nThe model uses a novel Att-Conv-Att (ACA) block which contains two graph\nattention layers and one spectral-based GCN layer sandwiched in between. The\ngraph attention layers are meant to capture temporal features while the\nspectral-based GCN layer is meant to capture spatial features. The main novelty\nof the model is the integration of time series of four different time\ngranularities: the original time series, together with hourly, daily, and\nweekly time series. Unlike previous work that used multi-granularity time\nseries by handling every time series separately, GACAN combines the outcome of\nprocessing all time series after each graph attention layer. Thus, the effects\nof different time granularities are integrated throughout the model. We perform\na series of experiments on three real-world datasets. The experimental results\nverify the advantage of using multi-granularity time series and that the\nproposed GACAN model outperforms the state-of-the-art baselines.",
    "descriptor": "\nComments: This paper has been published in the IJCNN 2021 (this https URL)\n",
    "authors": [
      "Sikai Zhang",
      "Hong Zheng",
      "Hongyi Su",
      "Bo Yan",
      "Jiamou Liu",
      "Song Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14331"
  },
  {
    "id": "arXiv:2110.14335",
    "title": "Optimal Importance Sampling via Stochastic Optimal Control for  Stochastic Reaction Networks",
    "abstract": "We explore the efficient estimation of statistical quantities, particularly\nrare event probabilities, for stochastic reaction networks. To this end, we\npropose a novel importance sampling (IS) approach to improve the efficiency of\nMonte Carlo (MC) estimators when based on an approximate tau-leap scheme. The\ncrucial step in IS is choosing an appropriate change of measure for achieving\nsubstantial variance reduction. Based on an original connection between finding\nthe optimal IS parameters within a class of probability measures and a\nstochastic optimal control (SOC) formulation, we propose an automated approach\nto obtain an efficient path-dependent measure change. The optimal IS parameters\nare obtained by solving a variance minimization problem. We derive an\nassociated backward equation solved by these optimal parameters. Given the\nchallenge of analytically solving this backward equation, we propose a\nnumerical dynamic programming algorithm to approximate the optimal control\nparameters. In the one-dimensional case, our numerical results show that the\nvariance of our proposed estimator decays at a rate of $\\mathcal{O}(\\Delta t)$\nfor a step size of $\\Delta t$, compared to $\\mathcal{O}(1)$ for a standard MC\nestimator. For a given prescribed error tolerance, $\\text{TOL}$, this implies\nan improvement in the computational complexity to become\n$\\mathcal{O}(\\text{TOL}^{-2})$ instead of $\\mathcal{O}(\\text{TOL}^{-3})$ when\nusing a standard MC estimator. To mitigate the curse of dimensionality issue\ncaused by solving the backward equation in the multi-dimensional case, we\npropose an alternative learning-based method that approximates the value\nfunction using a neural network, the parameters of which are determined via a\nstochastic optimization algorithm. Our numerical experiments demonstrate that\nour learning-based IS approach substantially reduces the variance of the MC\nestimator.",
    "descriptor": "",
    "authors": [
      "Chiheb Ben Hammouda",
      "Nadhir Ben Rached",
      "Ra\u00fal Tempone",
      "Sophia Wiechert"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Quantitative Methods (q-bio.QM)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.14335"
  },
  {
    "id": "arXiv:2110.14336",
    "title": "Feature and Label Embedding Spaces Matter in Addressing Image Classifier  Bias",
    "abstract": "This paper strives to address image classifier bias, with a focus on both\nfeature and label embedding spaces. Previous works have shown that spurious\ncorrelations from protected attributes, such as age, gender, or skin tone, can\ncause adverse decisions. To balance potential harms, there is a growing need to\nidentify and mitigate image classifier bias. First, we identify in the feature\nspace a bias direction. We compute class prototypes of each protected attribute\nvalue for every class, and reveal an existing subspace that captures the\nmaximum variance of the bias. Second, we mitigate biases by mapping image\ninputs to label embedding spaces. Each value of the protected attribute has its\nprojection head where classes are embedded through a latent vector\nrepresentation rather than a common one-hot encoding. Once trained, we further\nreduce in the feature space the bias effect by removing its direction.\nEvaluation on biased image datasets, for multi-class, multi-label and binary\nclassifications, shows the effectiveness of tackling both feature and label\nembedding spaces in improving the fairness of the classifier predictions, while\npreserving classification performance.",
    "descriptor": "\nComments: Accepted at British Machine Vision Conference (BMVC) 2021\n",
    "authors": [
      "William Thong",
      "Cees G. M. Snoek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14336"
  },
  {
    "id": "arXiv:2110.14340",
    "title": "JACC: An OpenACC Runtime Framework with Kernel-Level and Multi-GPU  Parallelization",
    "abstract": "The rapid development in computing technology has paved the way for\ndirective-based programming models towards a principal role in maintaining\nsoftware portability of performance-critical applications. Efforts on such\nmodels involve a least engineering cost for enabling computational acceleration\non multiple architectures while programmers are only required to add meta\ninformation upon sequential code. Optimizations for obtaining the best possible\nefficiency, however, are often challenging. The insertions of directives by the\nprogrammer can lead to side-effects that limit the available compiler\noptimization possible, which could result in performance degradation. This is\nexacerbated when targeting multi-GPU systems, as pragmas do not automatically\nadapt to such systems, and require expensive and time consuming code adjustment\nby programmers.\nThis paper introduces JACC, an OpenACC runtime framework which enables the\ndynamic extension of OpenACC programs by serving as a transparent layer between\nthe program and the compiler. We add a versatile code-translation method for\nmulti-device utilization by which manually-optimized applications can be\ndistributed automatically while keeping original code structure and\nparallelism. We show in some cases nearly linear scaling on the part of kernel\nexecution with the NVIDIA V100 GPUs. While adaptively using multi-GPUs, the\nresulting performance improvements amortize the latency of GPU-to-GPU\ncommunications.",
    "descriptor": "\nComments: Extended version of a paper to appear in: Proceedings of the 28th IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC), December 17-18, 2021\n",
    "authors": [
      "Kazuaki Matsumura",
      "Simon Garcia De Gonzalo",
      "Antonio J. Pe\u00f1a"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.14340"
  },
  {
    "id": "arXiv:2110.14341",
    "title": "Active-LATHE: An Active Learning Algorithm for Boosting the Error  Exponent for Learning Homogeneous Ising Trees",
    "abstract": "The Chow-Liu algorithm (IEEE Trans.~Inform.~Theory, 1968) has been a mainstay\nfor the learning of tree-structured graphical models from i.i.d.\\ sampled data\nvectors. Its theoretical properties have been well-studied and are\nwell-understood. In this paper, we focus on the class of trees that are\narguably even more fundamental, namely {\\em homogeneous} trees in which each\npair of nodes that forms an edge has the same correlation $\\rho$. We ask\nwhether we are able to further reduce the error probability of learning the\nstructure of the homogeneous tree model when {\\em active learning} or {\\em\nactive sampling of nodes or variables} is allowed. Our figure of merit is the\n{\\em error exponent}, which quantifies the exponential rate of decay of the\nerror probability with an increasing number of data samples. At first sight, an\nimprovement in the error exponent seems impossible, as all the edges are\nstatistically identical. We design and analyze an algorithm Active Learning\nAlgorithm for Trees with Homogeneous Edge (Active-LATHE), which surprisingly\nboosts the error exponent by at least 40\\% when $\\rho$ is at least $0.8$. For\nall other values of $\\rho$, we also observe commensurate, but more modest,\nimprovements in the error exponent. Our analysis hinges on judiciously\nexploiting the minute but detectable statistical variation of the samples to\nallocate more data to parts of the graph in which we are less confident of\nbeing correct.",
    "descriptor": "",
    "authors": [
      "Fengzhuo Zhang",
      "Anshoo Tandon",
      "Vincent Y. F. Vincent"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14341"
  },
  {
    "id": "arXiv:2110.14343",
    "title": "Comprehensive learning particle swarm optimization enabled modeling  framework for multi-step-ahead influenza prediction",
    "abstract": "Epidemics of influenza are major public health concerns. Since influenza\nprediction always relies on the weekly clinical or laboratory surveillance\ndata, typically the weekly Influenza-like illness (ILI) rate series, accurate\nmulti-step-ahead influenza predictions using ILI series is of great importance,\nespecially, to the potential coming influenza outbreaks. This study proposes\nComprehensive Learning Particle Swarm Optimization based Machine Learning\n(CLPSO-ML) framework incorporating support vector regression (SVR) and\nmultilayer perceptron (MLP) for multi-step-ahead influenza prediction. A\ncomprehensive examination and comparison of the performance and potential of\nthree commonly used multi-step-ahead prediction modeling strategies, including\niterated strategy, direct strategy and multiple-input multiple-output (MIMO)\nstrategy, was conducted using the weekly ILI rate series from both the Southern\nand Northern China. The results show that: (1) The MIMO strategy achieves the\nbest multi-step-ahead prediction, and is potentially more adaptive for longer\nhorizon; (2) The iterated strategy demonstrates special potentials for deriving\nthe least time difference between the occurrence of the predicted peak value\nand the true peak value of an influenza outbreak; (3) For ILI in the Northern\nChina, SVR model implemented with MIMO strategy performs best, and SVR with\niterated strategy also shows remarkable performance especially during outbreak\nperiods; while for ILI in the Southern China, both SVR and MLP models with MIMO\nstrategy have competitive prediction performance",
    "descriptor": "",
    "authors": [
      "Siyue Yang",
      "Yukun Bao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.14343"
  },
  {
    "id": "arXiv:2110.14347",
    "title": "CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera  Parameters",
    "abstract": "Perceiving 3D information is of paramount importance in many applications of\ncomputer vision. Recent advances in monocular depth estimation have shown that\ngaining such knowledge from a single camera input is possible by training deep\nneural networks to predict inverse depth and pose, without the necessity of\nground truth data. The majority of such approaches, however, require camera\nparameters to be fed explicitly during training. As a result, image sequences\nfrom wild cannot be used during training. While there exist methods which also\npredict camera intrinsics, their performance is not on par with novel methods\ntaking camera parameters as input. In this work, we propose a method for\nimplicit estimation of pinhole camera intrinsics along with depth and pose, by\nlearning from monocular image sequences alone. In addition, by utilizing\nefficient sub-pixel convolutions, we show that high fidelity depth estimates\ncan be obtained. We also embed pixel-wise uncertainty estimation into the\nframework, to emphasize the possible applicability of this work in practical\ndomain. Finally, we demonstrate the possibility of accurate prediction of depth\ninformation without prior knowledge of camera intrinsics, while outperforming\nthe existing state-of-the-art approaches on KITTI benchmark.",
    "descriptor": "\nComments: Accepted to BMVC 2021\n",
    "authors": [
      "Sai Shyam Chanduri",
      "Zeeshan Khan Suri",
      "Igor Vozniak",
      "Christian M\u00fcller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14347"
  },
  {
    "id": "arXiv:2110.14350",
    "title": "Enhancing Reinforcement Learning with discrete interfaces to learn the  Dyck Language",
    "abstract": "Even though most interfaces in the real world are discrete, no efficient way\nexists to train neural networks to make use of them, yet. We enhance an\nInteraction Network (a Reinforcement Learning architecture) with discrete\ninterfaces and train it on the generalized Dyck language. This task requires an\nunderstanding of hierarchical structures to solve, and has long proven\ndifficult for neural networks. We provide the first solution based on learning\nto use discrete data structures. We encountered unexpected anomalous behavior\nduring training, and utilized pre-training based on execution traces to\novercome them. The resulting model is very small and fast, and generalizes to\nsequences that are an entire order of magnitude longer than the training data.",
    "descriptor": "",
    "authors": [
      "Florian Dietz",
      "Dietrich Klakow"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14350"
  },
  {
    "id": "arXiv:2110.14354",
    "title": "MixSeq: Connecting Macroscopic Time Series Forecasting with Microscopic  Time Series Data",
    "abstract": "Time series forecasting is widely used in business intelligence, e.g.,\nforecast stock market price, sales, and help the analysis of data trend. Most\ntime series of interest are macroscopic time series that are aggregated from\nmicroscopic data. However, instead of directly modeling the macroscopic time\nseries, rare literature studied the forecasting of macroscopic time series by\nleveraging data on the microscopic level. In this paper, we assume that the\nmicroscopic time series follow some unknown mixture probabilistic\ndistributions. We theoretically show that as we identify the ground truth\nlatent mixture components, the estimation of time series from each component\ncould be improved because of lower variance, thus benefitting the estimation of\nmacroscopic time series as well. Inspired by the power of Seq2seq and its\nvariants on the modeling of time series data, we propose Mixture of Seq2seq\n(MixSeq), an end2end mixture model to cluster microscopic time series, where\nall the components come from a family of Seq2seq models parameterized by\ndifferent parameters. Extensive experiments on both synthetic and real-world\ndata show the superiority of our approach.",
    "descriptor": "\nComments: 15 pages, 2 figures, NeurIPS 2021\n",
    "authors": [
      "Zhibo Zhu",
      "Ziqi Liu",
      "Ge Jin",
      "Zhiqiang Zhang",
      "Lei Chen",
      "Jun Zhou",
      "Jianyong Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14354"
  },
  {
    "id": "arXiv:2110.14355",
    "title": "Transfer learning with causal counterfactual reasoning in Decision  Transformers",
    "abstract": "The ability to adapt to changes in environmental contingencies is an\nimportant challenge in reinforcement learning. Indeed, transferring previously\nacquired knowledge to environments with unseen structural properties can\ngreatly enhance the flexibility and efficiency by which novel optimal policies\nmay be constructed. In this work, we study the problem of transfer learning\nunder changes in the environment dynamics. In this study, we apply causal\nreasoning in the offline reinforcement learning setting to transfer a learned\npolicy to new environments. Specifically, we use the Decision Transformer (DT)\narchitecture to distill a new policy on the new environment. The DT is trained\non data collected by performing policy rollouts on factual and counterfactual\nsimulations from the source environment. We show that this mechanism can\nbootstrap a successful policy on the target environment while retaining most of\nthe reward.",
    "descriptor": "",
    "authors": [
      "Ayman Boustati",
      "Hana Chockler",
      "Daniel C. McNamee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14355"
  },
  {
    "id": "arXiv:2110.14357",
    "title": "Binarized ResNet: Enabling Automatic Modulation Classification at the  resource-constrained Edge",
    "abstract": "In this paper, we propose a ResNet based neural architecture to solve the\nproblem of Automatic Modulation Classification. We showed that our architecture\noutperforms the state-of-the-art (SOTA) architectures. We further propose to\nbinarize the network to deploy it in the Edge network where the devices are\nresource-constrained i.e. have limited memory and computing power. Instead of\nsimple binarization, rotated binarization is applied to the network which helps\nto close the significant performance gap between the real and the binarized\nnetwork. Because of the immense representation capability or the real network,\nits rotated binarized version achieves $85.33\\%$ accuracy compared to $95.76\\%$\naccuracy of the proposed real network with $2.33$ and $16$ times lesser\ncomputing power than two of the SOTA architectures, MCNet and RMLResNet\nrespectively, and approximately $16$ times less memory than both. The\nperformance can be improved further to $87.74\\%$ by taking an ensemble of four\nsuch rotated binarized networks.",
    "descriptor": "",
    "authors": [
      "Nitin Priyadarshini Shankar",
      "Deepsayan Sadhukhan",
      "Nancy Nayak",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14357"
  },
  {
    "id": "arXiv:2110.14363",
    "title": "VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using  Vector Quantization",
    "abstract": "Most state-of-the-art Graph Neural Networks (GNNs) can be defined as a form\nof graph convolution which can be realized by message passing between direct\nneighbors or beyond. To scale such GNNs to large graphs, various neighbor-,\nlayer-, or subgraph-sampling techniques are proposed to alleviate the \"neighbor\nexplosion\" problem by considering only a small subset of messages passed to the\nnodes in a mini-batch. However, sampling-based methods are difficult to apply\nto GNNs that utilize many-hops-away or global context each layer, show unstable\nperformance for different tasks and datasets, and do not speed up model\ninference. We propose a principled and fundamentally different approach,\nVQ-GNN, a universal framework to scale up any convolution-based GNNs using\nVector Quantization (VQ) without compromising the performance. In contrast to\nsampling-based techniques, our approach can effectively preserve all the\nmessages passed to a mini-batch of nodes by learning and updating a small\nnumber of quantized reference vectors of global node representations, using VQ\nwithin each GNN layer. Our framework avoids the \"neighbor explosion\" problem of\nGNNs using quantized representations combined with a low-rank version of the\ngraph convolution matrix. We show that such a compact low-rank version of the\ngigantic convolution matrix is sufficient both theoretically and\nexperimentally. In company with VQ, we design a novel approximated message\npassing algorithm and a nontrivial back-propagation rule for our framework.\nExperiments on various types of GNN backbones demonstrate the scalability and\ncompetitive performance of our framework on large-graph node classification and\nlink prediction benchmarks.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Mucong Ding",
      "Kezhi Kong",
      "Jingling Li",
      "Chen Zhu",
      "John P Dickerson",
      "Furong Huang",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14363"
  },
  {
    "id": "arXiv:2110.14369",
    "title": "ConAM: Confidence Attention Module for Convolutional Neural Networks",
    "abstract": "The so-called ``attention'' is an efficient mechanism to improve the\nperformance of convolutional neural networks. It uses contextual information to\nrecalibrate the input to strengthen the propagation of informative features.\nHowever, the majority of the attention mechanisms only consider either local or\nglobal contextual information, which is singular to extract features. Moreover,\nmany existing mechanisms directly use the contextual information to recalibrate\nthe input, which unilaterally enhances the propagation of the informative\nfeatures, but does not suppress the useless ones. This paper proposes a new\nattention mechanism module based on the correlation between local and global\ncontextual information and we name this correlation as confidence. The novel\nattention mechanism extracts the local and global contextual information\nsimultaneously, and calculates the confidence between them, then uses this\nconfidence to recalibrate the input pixels. The extraction of local and global\ncontextual information increases the diversity of features. The recalibration\nwith confidence suppresses useless information while enhancing the informative\none with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and\nexplore the performance of our method's components by sufficient ablation\nstudies. Finally, we compare our method with a various state-of-the-art\nconvolutional neural networks and the results show that our method completely\nsurpasses these models. We implement ConAM with the Python library, Pytorch,\nand the code and models will be publicly available.",
    "descriptor": "",
    "authors": [
      "Yu Xue",
      "Ziming Yuan",
      "Ferrante Neri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14369"
  },
  {
    "id": "arXiv:2110.14373",
    "title": "Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition",
    "abstract": "Decomposing a scene into its shape, reflectance and illumination is a\nfundamental problem in computer vision and graphics. Neural approaches such as\nNeRF have achieved remarkable success in view synthesis, but do not explicitly\nperform decomposition and instead operate exclusively on radiance (the product\nof reflectance and illumination). Extensions to NeRF, such as NeRD, can perform\ndecomposition but struggle to accurately recover detailed illumination, thereby\nsignificantly limiting realism. We propose a novel reflectance decomposition\nnetwork that can estimate shape, BRDF, and per-image illumination given a set\nof object images captured under varying illumination. Our key technique is a\nnovel illumination integration network called Neural-PIL that replaces a costly\nillumination integral operation in the rendering with a simple network query.\nIn addition, we also learn deep low-dimensional priors on BRDF and illumination\nrepresentations using novel smooth manifold auto-encoders. Our decompositions\ncan result in considerably better BRDF and light estimates enabling more\naccurate novel view-synthesis and relighting compared to prior art. Project\npage: https://markboss.me/publication/2021-neural-pil/",
    "descriptor": "\nComments: Project page: this https URL Video: this https URL - Accepted at NeurIPS 2021\n",
    "authors": [
      "Mark Boss",
      "Varun Jampani",
      "Raphael Braun",
      "Ce Liu",
      "Jonathan T. Barron",
      "Hendrik P.A. Lensch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14373"
  },
  {
    "id": "arXiv:2110.14375",
    "title": "Perceptual Score: What Data Modalities Does Your Model Perceive?",
    "abstract": "Machine learning advances in the last decade have relied significantly on\nlarge-scale datasets that continue to grow in size. Increasingly, those\ndatasets also contain different data modalities. However, large multi-modal\ndatasets are hard to annotate, and annotations may contain biases that we are\noften unaware of. Deep-net-based classifiers, in turn, are prone to exploit\nthose biases and to find shortcuts. To study and quantify this concern, we\nintroduce the perceptual score, a metric that assesses the degree to which a\nmodel relies on the different subsets of the input features, i.e., modalities.\nUsing the perceptual score, we find a surprisingly consistent trend across four\npopular datasets: recent, more accurate state-of-the-art multi-modal models for\nvisual question-answering or visual dialog tend to perceive the visual data\nless than their predecessors. This trend is concerning as answers are hence\nincreasingly inferred from textual cues only. Using the perceptual score also\nhelps to analyze model biases by decomposing the score into data subset\ncontributions. We hope to spur a discussion on the perceptiveness of\nmulti-modal models and also hope to encourage the community working on\nmulti-modal classifiers to start quantifying perceptiveness via the proposed\nperceptual score.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Itai Gat",
      "Idan Schwartz",
      "Alexander Schwing"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.14375"
  },
  {
    "id": "arXiv:2110.14377",
    "title": "Node Dependent Local Smoothing for Scalable Graph Learning",
    "abstract": "Recent works reveal that feature or label smoothing lies at the core of Graph\nNeural Networks (GNNs). Concretely, they show feature smoothing combined with\nsimple linear regression achieves comparable performance with the carefully\ndesigned GNNs, and a simple MLP model with label smoothing of its prediction\ncan outperform the vanilla GCN. Though an interesting finding, smoothing has\nnot been well understood, especially regarding how to control the extent of\nsmoothness. Intuitively, too small or too large smoothing iterations may cause\nunder-smoothing or over-smoothing and can lead to sub-optimal performance.\nMoreover, the extent of smoothness is node-specific, depending on its degree\nand local structure. To this end, we propose a novel algorithm called\nnode-dependent local smoothing (NDLS), which aims to control the smoothness of\nevery node by setting a node-specific smoothing iteration. Specifically, NDLS\ncomputes influence scores based on the adjacency matrix and selects the\niteration number by setting a threshold on the scores. Once selected, the\niteration number can be applied to both feature smoothing and label smoothing.\nExperimental results demonstrate that NDLS enjoys high accuracy --\nstate-of-the-art performance on node classifications tasks, flexibility -- can\nbe incorporated with any models, scalability and efficiency -- can support\nlarge scale graphs with fast training.",
    "descriptor": "\nComments: 19 pages, 5 figures\n",
    "authors": [
      "Wentao Zhang",
      "Mingyu Yang",
      "Zeang Sheng",
      "Yang Li",
      "Wen Ouyang",
      "Yangyu Tao",
      "Zhi Yang",
      "Bin Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14377"
  },
  {
    "id": "arXiv:2110.14378",
    "title": "WenLan 2.0: Make AI Imagine via a Multimodal Foundation Model",
    "abstract": "The fundamental goal of artificial intelligence (AI) is to mimic the core\ncognitive activities of human including perception, memory, and reasoning.\nAlthough tremendous success has been achieved in various AI research fields\n(e.g., computer vision and natural language processing), the majority of\nexisting works only focus on acquiring single cognitive ability (e.g., image\nclassification, reading comprehension, or visual commonsense reasoning). To\novercome this limitation and take a solid step to artificial general\nintelligence (AGI), we develop a novel foundation model pre-trained with huge\nmultimodal (visual and textual) data, which is able to be quickly adapted for a\nbroad class of downstream cognitive tasks. Such a model is fundamentally\ndifferent from the multimodal foundation models recently proposed in the\nliterature that typically make strong semantic correlation assumption and\nexpect exact alignment between image and text modalities in their pre-training\ndata, which is often hard to satisfy in practice thus limiting their\ngeneralization abilities. To resolve this issue, we propose to pre-train our\nfoundation model by self-supervised learning with weak semantic correlation\ndata crawled from the Internet and show that state-of-the-art results can be\nobtained on a wide range of downstream tasks (both single-modal and\ncross-modal). Particularly, with novel model-interpretability tools developed\nin this work, we demonstrate that strong imagination ability (even with hints\nof commonsense) is now possessed by our foundation model. We believe our work\nmakes a transformative stride towards AGI and will have broad impact on various\nAI+ fields (e.g., neuroscience and healthcare).",
    "descriptor": "",
    "authors": [
      "Nanyi Fei",
      "Zhiwu Lu",
      "Yizhao Gao",
      "Guoxing Yang",
      "Yuqi Huo",
      "Jingyuan Wen",
      "Haoyu Lu",
      "Ruihua Song",
      "Xin Gao",
      "Tao Xiang",
      "Hao Sun",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14378"
  },
  {
    "id": "arXiv:2110.14381",
    "title": "Temporal-attentive Covariance Pooling Networks for Video Recognition",
    "abstract": "For video recognition task, a global representation summarizing the whole\ncontents of the video snippets plays an important role for the final\nperformance. However, existing video architectures usually generate it by using\na simple, global average pooling (GAP) method, which has limited ability to\ncapture complex dynamics of videos. For image recognition task, there exist\nevidences showing that covariance pooling has stronger representation ability\nthan GAP. Unfortunately, such plain covariance pooling used in image\nrecognition is an orderless representative, which cannot model spatio-temporal\nstructure inherent in videos. Therefore, this paper proposes a\nTemporal-attentive Covariance Pooling(TCP), inserted at the end of deep\narchitectures, to produce powerful video representations. Specifically, our TCP\nfirst develops a temporal attention module to adaptively calibrate\nspatio-temporal features for the succeeding covariance pooling, approximatively\nproducing attentive covariance representations. Then, a temporal covariance\npooling performs temporal pooling of the attentive covariance representations\nto characterize both intra-frame correlations and inter-frame\ncross-correlations of the calibrated features. As such, the proposed TCP can\ncapture complex temporal dynamics. Finally, a fast matrix power normalization\nis introduced to exploit geometry of covariance representations. Note that our\nTCP is model-agnostic and can be flexibly integrated into any video\narchitectures, resulting in TCPNet for effective video recognition. The\nextensive experiments on six benchmarks using various video architectures show\nour TCPNet is clearly superior to its counterparts, while having strong\ngeneralization\nability.$\\href{https://github.com/ZilinGao/Temporal-attentive-Covariance-Pooling-Networks-for-Video-Recognition}{\\textit{The\nsource code is publicly available.}}$",
    "descriptor": "\nComments: Accepted to NeurIPS 2021; Project page: $\\href{this https URL}{\\textit{link}}$\n",
    "authors": [
      "Zilin Gao",
      "Qilong Wang",
      "Bingbing Zhang",
      "Qinghua Hu",
      "Peihua Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14381"
  },
  {
    "id": "arXiv:2110.14383",
    "title": "Traffic Forecasting on Traffic Moving Snippets",
    "abstract": "Advances in traffic forecasting technology can greatly impact urban mobility.\nIn the traffic4cast competition, the task of short-term traffic prediction is\ntackled in unprecedented detail, with traffic volume and speed information\navailable at 5 minute intervals and high spatial resolution. To improve\ngeneralization to unknown cities, as required in the 2021 extended challenge,\nwe propose to predict small quadratic city sections, rather than processing a\nfull-city-raster at once. At test time, breaking down the test data into\nspatially-cropped overlapping snippets improves stability and robustness of the\nfinal predictions, since multiple patches covering one cell can be processed\nindependently. With the performance on the traffic4cast test data and further\nexperiments on a validation set it is shown that patch-wise prediction indeed\nimproves accuracy. Further advantages can be gained with a Unet++ architecture\nand with an increasing number of patches per sample processed at test time. We\nconclude that our snippet-based method, combined with other successful network\narchitectures proposed in the competition, can leverage performance, in\nparticular on unseen cities. All source code is available at\nhttps://github.com/NinaWie/NeurIPS2021-traffic4cast.",
    "descriptor": "",
    "authors": [
      "Nina Wiedemann",
      "Martin Raubal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.14383"
  },
  {
    "id": "arXiv:2110.14389",
    "title": "Charon: Load-Aware Load-Balancing in P4",
    "abstract": "Load-Balancers play an important role in data centers as they distribute\nnetwork flows across application servers and guarantee per-connection\nconsistency. It is hard however to make fair load balancing decisions so that\nall resources are efficiently occupied yet not overloaded. Tracking connection\nstates allows to infer server load states and make informed decisions, but at\nthe cost of additional memory space consumption. This makes it hard to\nimplement on programmable hardware, which has constrained memory but offers\nline-rate performance. This paper presents Charon, a stateless load-aware load\nbalancer that has line-rate performance implemented in P4-NetFPGA. Charon\npassively collects load states from application servers and employs the\npower-of-2-choices scheme to make data-driven load balancing decisions and\nimprove resource utilization. Perconnection consistency is preserved\nstatelessly by encoding server ID in a covert channel. The prototype design and\nimplementation details are described in this paper. Simulation results show\nperformance gains in terms of load distribution fairness, quality of service,\nthroughput and processing latency.",
    "descriptor": "",
    "authors": [
      "Carmine Rizzi",
      "Zhiyuan Yao",
      "Yoann Desmouceaux",
      "Mark Townsley",
      "Thomas Heide Clausen"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.14389"
  },
  {
    "id": "arXiv:2110.14392",
    "title": "Taylor Swift: Taylor Driven Temporal Modeling for Swift Future Frame  Prediction",
    "abstract": "While recurrent neural networks (RNNs) demonstrate outstanding capabilities\nin future video frame prediction, they model dynamics in a discrete time space\nand sequentially go through all frames until the desired future temporal step\nis reached. RNNs are therefore prone to accumulate the error as the number of\nfuture frames increases. In contrast, partial differential equations (PDEs)\nmodel physical phenomena like dynamics in continuous time space, however,\ncurrent PDE-based approaches discretize the PDEs using e.g., the forward Euler\nmethod. In this work, we therefore propose to approximate the motion in a video\nby a continuous function using the Taylor series. To this end, we introduce\nTayloSwiftNet, a novel convolutional neural network that learns to estimate the\nhigher order terms of the Taylor series for a given input video. TayloSwiftNet\ncan swiftly predict any desired future frame in just one forward pass and\nchange the temporal resolution on-the-fly. The experimental results on various\ndatasets demonstrate the superiority of our model.",
    "descriptor": "",
    "authors": [
      "Mohammad Saber Pourheydari",
      "Mohsen Fayyaz",
      "Emad Bahrami",
      "Mehdi Noroozi",
      "Juergen Gall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14392"
  },
  {
    "id": "arXiv:2110.14395",
    "title": "Evaluating Robot Posture Control and Balance by Comparison to Human  Subjects using Human Likeness Measures",
    "abstract": "Posture control and balance are basic requirements for a humanoid robot\nperforming motor tasks like walking and interacting with the environment. For\nthis reason, posture control is one of the elements taken into account when\nevaluating the performance of humanoids. In this work, we describe and analyze\na performance indicator based on the comparison between the body sway of a\nrobot standing on a moving surface and the one of healthy subjects performing\nthe same experiment. This approach is here oriented to the evaluation of human\nlikeness. The measure is tested with three human-inspired humanoid posture\ncontrol systems, the independent channel (IC), the disturbance identification\nand compensation (DEC), and the eigenmovement (EM) control. The potential and\nthe limitations connected with such human-inspired humanoid control mechanisms\nare then discussed.",
    "descriptor": "\nComments: Presented at ROBOVIS 2021\n",
    "authors": [
      "Vittorio Lippi",
      "Christoph Maurer",
      "Thomas Mergner"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14395"
  },
  {
    "id": "arXiv:2110.14396",
    "title": "Multi-fidelity data fusion through parameter space reduction with  applications to automotive engineering",
    "abstract": "Multi-fidelity models are of great importance due to their capability of\nfusing information coming from different simulations and sensors. Gaussian\nprocesses are employed for nonparametric regression in a Bayesian setting. They\ngeneralize linear regression embedding the inputs in a latent manifold inside\nan infinite-dimensional reproducing kernel Hilbert space. We can augment the\ninputs with the observations of low-fidelity models in order to learn a more\nexpressive latent manifold and thus increment the model's accuracy. This can be\nrealized recursively with a chain of Gaussian processes with incrementally\nhigher fidelity. We would like to extend these multi-fidelity model\nrealizations to case studies affected by a high-dimensional input space but\nwith low intrinsic dimensionality. In these cases physical supported or purely\nnumerical low-order models are still affected by the curse of dimensionality\nwhen queried for responses. When the model's gradient information is provided,\nthe existence of an active subspace, or a nonlinear transformation of the input\nparameter space, can be exploited to design low-fidelity response surfaces and\nthus enable Gaussian process multi-fidelity regression, without the need to\nperform new simulations. This is particularly useful in the case of data\nscarcity. In this work, we present a new multi-fidelity approach involving\nactive subspaces and nonlinear level-set learning method. We test the proposed\nnumerical method on two different high-dimensional benchmarks, and on a more\ncomplex car aerodynamics problem. We show how a low intrinsic dimensionality\nbias can increase the accuracy of Gaussian process response surfaces.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2010.08349\n",
    "authors": [
      "Francesco Romor",
      "Marco Tezzele",
      "Markus Mrosek",
      "Carsten Othmer",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14396"
  },
  {
    "id": "arXiv:2110.14397",
    "title": "A Preliminary Case Study of Planning With Complex Transitions: Plotting",
    "abstract": "Plotting is a tile-matching puzzle video game published by Taito in 1989. Its\nobjective is to reduce a given grid of coloured blocks down to a goal number or\nfewer. This is achieved by the avatar character repeatedly shooting the block\nit holds into the grid. Plotting is an example of a planning problem: given a\nmodel of the environment, a planning problem asks us to find a sequence of\nactions that can lead from an initial state of the environment to a given goal\nstate while respecting some constraints. The key difficulty in modelling\nPlotting is in capturing the way the puzzle state changes after each shot. A\nsingle shot can affect multiple tiles directly, and the grid is affected by\ngravity so numerous other tiles can be affected indirectly. We present and\nevaluate a constraint model of the Plotting problem that captures this\ncomplexity. We also discuss the difficulties and inefficiencies of modelling\nPlotting in PDDL, the standard language used for input to specialised AI\nplanners. We conclude by arguing that AI planning could benefit from a richer\nmodelling language.",
    "descriptor": "\nComments: Presented in the 20th workshop on Constraint Modelling and Reformulation (October 25th, 2021). The workshop is integrated in the The 27th International Conference on Principles and Practice of Constraint Programming, CP2021\n",
    "authors": [
      "Jordi Coll",
      "Joan Espasa",
      "Ian Miguel",
      "Mateu Villaret"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.14397"
  },
  {
    "id": "arXiv:2110.14398",
    "title": "Can Linguistic Distance help Language Classification? Assessing  Hawrami-Zaza and Kurmanji-Sorani",
    "abstract": "To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a\nlanguage have been discussed and debated for a while among linguists active in\nstudying Iranian languages. The question of whether those languages/dialects\nbelong to the Kurdish language or if they are independent descendants of\nIranian languages was answered by MacKenzie (1961). However, a majority of\npeople who speak the dialects are against that answer. Their disapproval mainly\nseems to be based on the sociological, cultural, and historical relationship\namong the speakers of the dialects. While the case of Hawrami and Zaza has\nremained unexplored and under-examined, an almost unanimous agreement exists\nabout the classification of Kurmanji and Sorani as Kurdish dialects. The\nrelated studies to address the mentioned cases are primarily qualitative.\nHowever, computational linguistics could approach the question from a\nquantitative perspective. In this research, we look into three questions from a\nlinguistic distance point of view. First, how similar or dissimilar Hawrami and\nZaza are, considering no common geographical coexistence between the two.\nSecond, what about Kurmanji and Sorani that have geographical overlap. Finally,\nwhat is the distance among all these dialects, pair by pair? We base our\ncomputation on phonetic presentations of these dialects (languages), and we\ncalculate various linguistic distances among the pairs. We analyze the data and\ndiscuss the results to conclude.",
    "descriptor": "\nComments: This paper is based on an abstract that was submitted to and presented under the same title at the 5 the International Conference on Kurdish Linguistics at the University of Graz, 25 September 2021\n",
    "authors": [
      "Hossein Hassani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14398"
  },
  {
    "id": "arXiv:2110.14402",
    "title": "Learning where to learn: Gradient sparsity in meta and continual  learning",
    "abstract": "Finding neural network weights that generalize well from small datasets is\ndifficult. A promising approach is to learn a weight initialization such that a\nsmall number of weight changes results in low generalization error. We show\nthat this form of meta-learning can be improved by letting the learning\nalgorithm decide which weights to change, i.e., by learning where to learn. We\nfind that patterned sparsity emerges from this process, with the pattern of\nsparsity varying on a problem-by-problem basis. This selective sparsity results\nin better generalization and less interference in a range of few-shot and\ncontinual learning problems. Moreover, we find that sparse learning also\nemerges in a more expressive model where learning rates are meta-learned. Our\nresults shed light on an ongoing debate on whether meta-learning can discover\nadaptable features and suggest that learning by sparse gradient descent is a\npowerful inductive bias for meta-learning systems.",
    "descriptor": "\nComments: Published at NeurIPS 2021\n",
    "authors": [
      "Johannes von Oswald",
      "Dominic Zhao",
      "Seijin Kobayashi",
      "Simon Schug",
      "Massimo Caccia",
      "Nicolas Zucchet",
      "Jo\u00e3o Sacramento"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.14402"
  },
  {
    "id": "arXiv:2110.14404",
    "title": "Separating Content and Style for Unsupervised Image-to-Image Translation",
    "abstract": "Unsupervised image-to-image translation aims to learn the mapping between two\nvisual domains with unpaired samples. Existing works focus on disentangling\ndomain-invariant content code and domain-specific style code individually for\nmultimodal purposes. However, less attention has been paid to interpreting and\nmanipulating the translated image. In this paper, we propose to separate the\ncontent code and style code simultaneously in a unified framework. Based on the\ncorrelation between the latent features and the high-level domain-invariant\ntasks, the proposed framework demonstrates superior performance in multimodal\ntranslation, interpretability and manipulation of the translated image.\nExperimental results show that the proposed approach outperforms the existing\nunsupervised image translation methods in terms of visual quality and\ndiversity.",
    "descriptor": "\nComments: Accepted by BMVC2021\n",
    "authors": [
      "Yunfei Liu",
      "Haofei Wang",
      "Yang Yue",
      "Feng Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14404"
  },
  {
    "id": "arXiv:2110.14413",
    "title": "Localized Super Resolution for Foreground Images using U-Net and MR-CNN",
    "abstract": "Images play a vital role in understanding data through visual representation.\nIt gives a clear representation of the object in context. But if this image is\nnot clear it might not be of much use. Thus, the topic of Image Super\nResolution arose and many researchers have been working towards applying\nComputer Vision and Deep Learning Techniques to increase the quality of images.\nOne of the applications of Super Resolution is to increase the quality of\nPortrait Images. Portrait Images are images which mainly focus on capturing the\nessence of the main object in the frame, where the object in context is\nhighlighted whereas the background is occluded. When performing Super\nResolution the model tries to increase the overall resolution of the image. But\nin portrait images the foreground resolution is more important than that of the\nbackground. In this paper, the performance of a Convolutional Neural Network\n(CNN) architecture known as U-Net for Super Resolution combined with Mask\nRegion Based CNN (MR-CNN) for foreground super resolution is analysed. This\nanalysis is carried out based on Localized Super Resolution i.e. We pass the LR\nImages to a pre-trained Image Segmentation model (MR-CNN) and perform super\nresolution inference on the foreground or Segmented Images and compute the\nStructural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR)\nmetrics for comparisons.",
    "descriptor": "\nComments: Submitted for peer review and acceptance at ICRTAC 2021\n",
    "authors": [
      "Umashankar Kumaravelan",
      "Nivedita M"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.14413"
  },
  {
    "id": "arXiv:2110.14416",
    "title": "Transformers Generalize DeepSets and Can be Extended to Graphs and  Hypergraphs",
    "abstract": "We present a generalization of Transformers to any-order permutation\ninvariant data (sets, graphs, and hypergraphs). We begin by observing that\nTransformers generalize DeepSets, or first-order (set-input) permutation\ninvariant MLPs. Then, based on recently characterized higher-order invariant\nMLPs, we extend the concept of self-attention to higher orders and propose\nhigher-order Transformers for order-$k$ data ($k=2$ for graphs and $k>2$ for\nhypergraphs). Unfortunately, higher-order Transformers turn out to have\nprohibitive complexity $\\mathcal{O}(n^{2k})$ to the number of input nodes $n$.\nTo address this problem, we present sparse higher-order Transformers that have\nquadratic complexity to the number of input hyperedges, and further adopt the\nkernel attention approach to reduce the complexity to linear. In particular, we\nshow that the sparse second-order Transformers with kernel attention are\ntheoretically more expressive than message passing operations while having an\nasymptotically identical complexity. Our models achieve significant performance\nimprovement over invariant MLPs and message-passing graph neural networks in\nlarge-scale graph regression and set-to-(hyper)graph prediction tasks. Our\nimplementation is available at https://github.com/jw9730/hot.",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "Jinwoo Kim",
      "Saeyoon Oh",
      "Seunghoon Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14416"
  },
  {
    "id": "arXiv:2110.14419",
    "title": "Towards a Theory of Justice for Artificial Intelligence",
    "abstract": "This paper explores the relationship between artificial intelligence and\nprinciples of distributive justice. Drawing upon the political philosophy of\nJohn Rawls, it holds that the basic structure of society should be understood\nas a composite of socio-technical systems, and that the operation of these\nsystems is increasingly shaped and influenced by AI. As a consequence,\negalitarian norms of justice apply to the technology when it is deployed in\nthese contexts. These norms entail that the relevant AI systems must meet a\ncertain standard of public justification, support citizens rights, and promote\nsubstantively fair outcomes -- something that requires specific attention be\npaid to the impact they have on the worst-off members of society.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Iason Gabriel"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.14419"
  },
  {
    "id": "arXiv:2110.14422",
    "title": "Zero-shot Voice Conversion via Self-supervised Prosody Representation  Learning",
    "abstract": "Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an\nattractive topic due to its usefulness in real use-case scenarios. Recent work\nin this area made progress with disentanglement methods that separate utterance\ncontent and speaker characteristics. Although crucial, extracting disentangled\nprosody characteristics for unseen speakers remains an open issue. In this\npaper, we propose a novel self-supervised approach to effectively learn the\nprosody characteristics. Then, we use the learned prosodic representations to\ntrain our VC model for zero-shot conversion. Our evaluation demonstrates that\nwe can efficiently extract disentangled prosody representation. Moreover, we\nshow improved performance compared to the state-of-the-art zero-shot VC models.",
    "descriptor": "",
    "authors": [
      "Shijun Wang",
      "Dimche Kostadinov",
      "Damian Borth"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14422"
  },
  {
    "id": "arXiv:2110.14425",
    "title": "Generalizing AUC Optimization to Multiclass Classification for Audio  Segmentation With Limited Training Data",
    "abstract": "Area under the ROC curve (AUC) optimisation techniques developed for neural\nnetworks have recently demonstrated their capabilities in different audio and\nspeech related tasks. However, due to its intrinsic nature, AUC optimisation\nhas focused only on binary tasks so far. In this paper, we introduce an\nextension to the AUC optimisation framework so that it can be easily applied to\nan arbitrary number of classes, aiming to overcome the issues derived from\ntraining data limitations in deep learning solutions. Building upon the\nmulticlass definitions of the AUC metric found in the literature, we define two\nnew training objectives using a one-versus-one and a one-versus-rest approach.\nIn order to demonstrate its potential, we apply them in an audio segmentation\ntask with limited training data that aims to differentiate 3 classes:\nforeground music, background music and no music. Experimental results show that\nour proposal can improve the performance of audio segmentation systems\nsignificantly compared to traditional training criteria such as cross entropy.",
    "descriptor": "",
    "authors": [
      "Pablo Gimeno",
      "Victoria Mingote",
      "Alfonso Ortega",
      "Antonio Miguel",
      "Eduardo Lleida"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14425"
  },
  {
    "id": "arXiv:2110.14429",
    "title": "Numerical simulation of multiscale fault systems with rate- and  state-dependent friction",
    "abstract": "We consider the deformation of a geological structure with non-intersecting\nfaults that can be represented by a layered system of viscoelastic bodies\nsatisfying rate- and state-depending friction conditions along the common\ninterfaces. We derive a mathematical model that contains classical Dieterich-\nand Ruina-type friction as special cases and accounts for possibly large\ntangential displacements. Semi-discretization in time by a Newmark scheme leads\nto a coupled system of non-smooth, convex minimization problems for rate and\nstate to be solved in each time step. Additional spatial discretization by a\nmortar method and piecewise constant finite elements allows for the decoupling\nof rate and state by a fixed point iteration and efficient algebraic solution\nof the rate problem by truncated non-smooth Newton methods. Numerical\nexperiments with a spring slider and a layered multiscale system illustrate the\nbehavior of our model as well as the efficiency and reliability of the\nnumerical solver.",
    "descriptor": "",
    "authors": [
      "Carsten Gr\u00e4ser",
      "Ralf Kornhuber",
      "Joscha Podlesny"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14429"
  },
  {
    "id": "arXiv:2110.14430",
    "title": "Adversarial Neuron Pruning Purifies Backdoored Deep Models",
    "abstract": "As deep neural networks (DNNs) are growing larger, their requirements for\ncomputational resources become huge, which makes outsourcing training more\npopular. Training in a third-party platform, however, may introduce potential\nrisks that a malicious trainer will return backdoored DNNs, which behave\nnormally on clean samples but output targeted misclassifications whenever a\ntrigger appears at the test time. Without any knowledge of the trigger, it is\ndifficult to distinguish or recover benign DNNs from backdoored ones. In this\npaper, we first identify an unexpected sensitivity of backdoored DNNs, that is,\nthey are much easier to collapse and tend to predict the target label on clean\nsamples when their neurons are adversarially perturbed. Based on these\nobservations, we propose a novel model repairing method, termed Adversarial\nNeuron Pruning (ANP), which prunes some sensitive neurons to purify the\ninjected backdoor. Experiments show, even with only an extremely small amount\nof clean data (e.g., 1%), ANP effectively removes the injected backdoor without\ncausing obvious performance degradation.",
    "descriptor": "\nComments: To appear in NeurIPS 2021\n",
    "authors": [
      "Dongxian Wu",
      "Yisen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14430"
  },
  {
    "id": "arXiv:2110.14432",
    "title": "Iterative Teaching by Label Synthesis",
    "abstract": "In this paper, we consider the problem of iterative machine teaching, where a\nteacher provides examples sequentially based on the current iterative learner.\nIn contrast to previous methods that have to scan over the entire pool and\nselect teaching examples from it in each iteration, we propose a label\nsynthesis teaching framework where the teacher randomly selects input teaching\nexamples (e.g., images) and then synthesizes suitable outputs (e.g., labels)\nfor them. We show that this framework can avoid costly example selection while\nstill provably achieving exponential teachability. We propose multiple novel\nteaching algorithms in this framework. Finally, we empirically demonstrate the\nvalue of our framework.",
    "descriptor": "\nComments: NeurIPS 2021 Spotlight (26 pages, 20 figures)\n",
    "authors": [
      "Weiyang Liu",
      "Zhen Liu",
      "Hanchen Wang",
      "Liam Paull",
      "Bernhard Sch\u00f6lkopf",
      "Adrian Weller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14432"
  },
  {
    "id": "arXiv:2110.14434",
    "title": "Nonnegative Tucker Decomposition with Beta-divergence for Music  Structure Analysis of audio signals",
    "abstract": "Nonnegative Tucker Decomposition (NTD), a tensor decomposition model, has\nreceived increased interest in the recent years because of its ability to\nblindly extract meaningful patterns in tensor data. Nevertheless, existing\nalgorithms to compute NTD are mostly designed for the Euclidean loss. On the\nother hand, NTD has recently proven to be a powerful tool in Music Information\nRetrieval. This work proposes a Multiplicative Updates algorithm to compute NTD\nwith the beta-divergence loss, often considered a better loss for audio\nprocessing. We notably show how to implement efficiently the multiplicative\nrules using tensor algebra, a naive approach being intractable. Finally, we\nshow on a Music Structure Analysis task that unsupervised NTD fitted with\nbeta-divergence loss outperforms earlier results obtained with the Euclidean\nloss.",
    "descriptor": "\nComments: 4 pages, 2 figures, 1 table, 1 algorithm, submitted to ICASSP 2022\n",
    "authors": [
      "Axel Marmoret",
      "Florian Voorwinden",
      "Valentin Leplat",
      "J\u00e9r\u00e9my E. Cohen",
      "Fr\u00e9d\u00e9ric Bimbot"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14434"
  },
  {
    "id": "arXiv:2110.14437",
    "title": "Exploring single-song autoencoding schemes for audio-based music  structure analysis",
    "abstract": "The ability of deep neural networks to learn complex data relations and\nrepresentations is established nowadays, but it generally relies on large sets\nof training data. This work explores a \"piece-specific\" autoencoding scheme, in\nwhich a low-dimensional autoencoder is trained to learn a latent/compressed\nrepresentation specific to a given song, which can then be used to infer the\nsong structure. Such a model does not rely on supervision nor annotations,\nwhich are well-known to be tedious to collect and often ambiguous in Music\nStructure Analysis. We report that the proposed unsupervised auto-encoding\nscheme achieves the level of performance of supervised state-of-the-art methods\nwith 3 seconds tolerance when using a Log Mel spectrogram representation on the\nRWC-Pop dataset.",
    "descriptor": "\nComments: 4 pages, 4 figures, 2 tables, submitted to ICASSP 2022\n",
    "authors": [
      "Axel Marmoret",
      "J\u00e9r\u00e9my E. Cohen",
      "Fr\u00e9d\u00e9ric Bimbot"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14437"
  },
  {
    "id": "arXiv:2110.14438",
    "title": "Classification of Automorphisms for the Decoding of Polar Codes",
    "abstract": "This paper proposes new polar code design principles for the low-latency\nautomorphism ensemble (AE) decoding. Our proposal permits to design a polar\ncode with the desired automorphism group (if possible) while assuring the\ndecreasing monomial property. Moreover, we prove that some automorphisms are\nredundant under AE decoding, and we propose a new automorphisms classification\nbased on equivalence classes. Finally, we propose an automorphism selection\nheuristic based on drawing only one element of each class; we show that this\nmethod enhances the block error rate (BLER) performance of short polar codes\neven with a limited number of automorphisms.",
    "descriptor": "\nComments: 7 pages, 5 figures, to be submitted in IEEE ICC 2022\n",
    "authors": [
      "Charles Pillet",
      "Valerio Bioglio",
      "Ingmar Land"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.14438"
  },
  {
    "id": "arXiv:2110.14439",
    "title": "Revisiting Discriminator in GAN Compression: A Generator-discriminator  Cooperative Compression Scheme",
    "abstract": "Recently, a series of algorithms have been explored for GAN compression,\nwhich aims to reduce tremendous computational overhead and memory usages when\ndeploying GANs on resource-constrained edge devices. However, most of the\nexisting GAN compression work only focuses on how to compress the generator,\nwhile fails to take the discriminator into account. In this work, we revisit\nthe role of discriminator in GAN compression and design a novel\ngenerator-discriminator cooperative compression scheme for GAN compression,\ntermed GCC. Within GCC, a selective activation discriminator automatically\nselects and activates convolutional channels according to a local capacity\nconstraint and a global coordination constraint, which help maintain the Nash\nequilibrium with the lightweight generator during the adversarial training and\navoid mode collapse. The original generator and discriminator are also\noptimized from scratch, to play as a teacher model to progressively refine the\npruned generator and the selective activation discriminator. A novel online\ncollaborative distillation scheme is designed to take full advantage of the\nintermediate feature of the teacher generator and discriminator to further\nboost the performance of the lightweight generator. Extensive experiments on\nvarious GAN-based generation tasks demonstrate the effectiveness and\ngeneralization of GCC. Among them, GCC contributes to reducing 80%\ncomputational costs while maintains comparable performance in image translation\ntasks. Our code and models are available at \\url{https://github.com/SJLeo/GCC}.",
    "descriptor": "\nComments: Accepted by NeurIPS2021 (The 35th Conference on Neural Information Processing Systems)\n",
    "authors": [
      "ShaoJie Li",
      "Jie Wu",
      "Xuefeng Xiao",
      "Fei Chao",
      "Xudong Mao",
      "Rongrong Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14439"
  },
  {
    "id": "arXiv:2110.14446",
    "title": "Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and  Strong Simple Methods",
    "abstract": "Many widely used datasets for graph machine learning tasks have generally\nbeen homophilous, where nodes with similar labels connect to each other.\nRecently, new Graph Neural Networks (GNNs) have been developed that move beyond\nthe homophily regime; however, their evaluation has often been conducted on\nsmall graphs with limited application domains. We collect and introduce diverse\nnon-homophilous datasets from a variety of application areas that have up to\n384x more nodes and 1398x more edges than prior datasets. We further show that\nexisting scalable graph learning and graph minibatching techniques lead to\nperformance degradation on these non-homophilous datasets, thus highlighting\nthe need for further work on scalable non-homophilous methods. To address these\nconcerns, we introduce LINKX -- a strong simple method that admits\nstraightforward minibatch training and inference. Extensive experimental\nresults with representative simple methods and GNNs across our proposed\ndatasets show that LINKX achieves state-of-the-art performance for learning on\nnon-homophilous graphs. Our codes and data are available at\nhttps://github.com/CUAI/Non-Homophily-Large-Scale.",
    "descriptor": "\nComments: Published at NeurIPS 2021\n",
    "authors": [
      "Derek Lim",
      "Felix Hohne",
      "Xiuyu Li",
      "Sijia Linda Huang",
      "Vaishnavi Gupta",
      "Omkar Bhalerao",
      "Ser-Nam Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14446"
  },
  {
    "id": "arXiv:2110.14450",
    "title": "Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph  Embedding",
    "abstract": "Knowledge graph embedding models learn the representations of entities and\nrelations in the knowledge graphs for predicting missing links (relations)\nbetween entities. Their effectiveness are deeply affected by the ability of\nmodeling and inferring different relation patterns such as symmetry, asymmetry,\ninversion, composition and transitivity. Although existing models are already\nable to model many of these relations patterns, transitivity, a very common\nrelation pattern, is still not been fully supported. In this paper, we first\ntheoretically show that the transitive relations can be modeled with\nprojections. We then propose the Rot-Pro model which combines the projection\nand relational rotation together. We prove that Rot-Pro can infer all the above\nrelation patterns. Experimental results show that the proposed Rot-Pro model\neffectively learns the transitivity pattern and achieves the state-of-the-art\nresults on the link prediction task in the datasets containing transitive\nrelations.",
    "descriptor": "\nComments: 10 pages, 6 figures, to be published in NeurIPS 2021\n",
    "authors": [
      "Tengwei Song",
      "Jie Luo",
      "Lei Huang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14450"
  },
  {
    "id": "arXiv:2110.14451",
    "title": "Validation Methods for Energy Time Series Scenarios from Deep Generative  Models",
    "abstract": "The design and operation of modern energy systems are heavily influenced by\ntime-dependent and uncertain parameters, e.g., renewable electricity\ngeneration, load-demand, and electricity prices. These are typically\nrepresented by a set of discrete realizations known as scenarios. A popular\nscenario generation approach uses deep generative models (DGM) that allow\nscenario generation without prior assumptions about the data distribution.\nHowever, the validation of generated scenarios is difficult, and a\ncomprehensive discussion about appropriate validation methods is currently\nlacking. To start this discussion, we provide a critical assessment of the\ncurrently used validation methods in the energy scenario generation literature.\nIn particular, we assess validation methods based on probability density,\nauto-correlation, and power spectral density. Furthermore, we propose using the\nmultifractal detrended fluctuation analysis (MFDFA) as an additional validation\nmethod for non-trivial features like peaks, bursts, and plateaus. As\nrepresentative examples, we train generative adversarial networks (GANs),\nWasserstein GANs (WGANs), and variational autoencoders (VAEs) on two renewable\npower generation time series (photovoltaic and wind from Germany in 2013 to\n2015) and an intra-day electricity price time series form the European Energy\nExchange in 2017 to 2019. We apply the four validation methods to both the\nhistorical and the generated data and discuss the interpretation of validation\nresults as well as common mistakes, pitfalls, and limitations of the validation\nmethods. Our assessment shows that no single method sufficiently characterizes\na scenario but ideally validation should include multiple methods and be\ninterpreted carefully in the context of scenarios over short time periods.",
    "descriptor": "\nComments: 16 pages, 8 figures, 1 table\n",
    "authors": [
      "Eike Cramer",
      "Leonardo Rydin Gorj\u00e3o",
      "Alexander Mitsos",
      "Benjamin Sch\u00e4fer",
      "Dirk Witthaut",
      "Manuel Dahmen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14451"
  },
  {
    "id": "arXiv:2110.14455",
    "title": "CBIR using Pre-Trained Neural Networks",
    "abstract": "Much of the recent research work in image retrieval, has been focused around\nusing Neural Networks as the core component. Many of the papers in other domain\nhave shown that training multiple models, and then combining their outcomes,\nprovide good results. This is since, a single Neural Network model, may not\nextract sufficient information from the input. In this paper, we aim to follow\na different approach. Instead of the using a single model, we use a pretrained\nInception V3 model, and extract activation of its last fully connected layer,\nwhich forms a low dimensional representation of the image. This feature matrix,\nis then divided into branches and separate feature extraction is done for each\nbranch, to obtain multiple features flattened into a vector. Such individual\nvectors are then combined, to get a single combined feature. We make use of\nCUB200-2011 Dataset, which comprises of 200 birds classes to train the model\non. We achieved a training accuracy of 99.46% and validation accuracy of 84.56%\nfor the same. On further use of 3 branched global descriptors, we improve the\nvalidation accuracy to 88.89%. For this, we made use of MS-RMAC feature\nextraction method.",
    "descriptor": "",
    "authors": [
      "Agnel Lazar Alappat",
      "Prajwal Nakhate",
      "Sagar Suman",
      "Ambarish Chandurkar",
      "Varad Pimpalkhute",
      "Tapan Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14455"
  },
  {
    "id": "arXiv:2110.14457",
    "title": "Direct then Diffuse: Incremental Unsupervised Skill Discovery for State  Covering and Goal Reaching",
    "abstract": "Learning meaningful behaviors in the absence of reward is a difficult problem\nin reinforcement learning. A desirable and challenging unsupervised objective\nis to learn a set of diverse skills that provide a thorough coverage of the\nstate space while being directed, i.e., reliably reaching distinct regions of\nthe environment. In this paper, we build on the mutual information framework\nfor skill discovery and introduce UPSIDE, which addresses the\ncoverage-directedness trade-off in the following ways: 1) We design policies\nwith a decoupled structure of a directed skill, trained to reach a specific\nregion, followed by a diffusing part that induces a local coverage. 2) We\noptimize policies by maximizing their number under the constraint that each of\nthem reaches distinct regions of the environment (i.e., they are sufficiently\ndiscriminable) and prove that this serves as a lower bound to the original\nmutual information objective. 3) Finally, we compose the learned directed\nskills into a growing tree that adaptively covers the environment. We\nillustrate in several navigation and control environments how the skills\nlearned by UPSIDE solve sparse-reward downstream tasks better than existing\nbaselines.",
    "descriptor": "",
    "authors": [
      "Pierre-Alexandre Kamienny",
      "Jean Tarbouriech",
      "Alessandro Lazaric",
      "Ludovic Denoyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14457"
  },
  {
    "id": "arXiv:2110.14459",
    "title": "Accelerating Gradient-based Meta Learner",
    "abstract": "Meta Learning has been in focus in recent years due to the meta-learner\nmodel's ability to adapt well and generalize to new tasks, thus, reducing both\nthe time and data requirements for learning. However, a major drawback of meta\nlearner is that, to reach to a state from where learning new tasks becomes\nfeasible with less data, it requires a large number of iterations and a lot of\ntime. We address this issue by proposing various acceleration techniques to\nspeed up meta learning algorithms such as MAML (Model Agnostic Meta Learning).\nWe present 3.73X acceleration on a well known RNN optimizer based meta learner\nproposed in literature [11]. We introduce a novel method of training tasks in\nclusters, which not only accelerates the meta learning process but also\nimproves model accuracy performance.\nKeywords: Meta learning, RNN optimizer, AGI, Performance optimization",
    "descriptor": "",
    "authors": [
      "Varad Pimpalkhute",
      "Amey Pandit",
      "Mayank Mishra",
      "Rekha Singhal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.14459"
  },
  {
    "id": "arXiv:2110.14460",
    "title": "Interaction Maxima in Distributed Systems",
    "abstract": "In this paper we study the maximum degree of interaction which may emerge in\ndistributed systems. It is assumed that a distributed system is represented by\na graph of nodes interacting over edges. Each node has some amount of data. The\nintensity of interaction over an edge is proportional to the product of the\namounts of data in each node at either end of the edge. The maximum sum of\ninteractions over the edges is searched for. This model can be extended to\nother interacting entities. For bipartite graphs and odd-length cycles we prove\nthat the greatest degree of interaction emerge when the whole data is\nconcentrated in an arbitrary pair of neighbors. Equal partitioning of the load\nis shown to be optimum for complete graphs. Finally, we show that in general\ngraphs for maximum interaction the data should be distributed equally between\nthe nodes of the largest clique in the graph. We also present in this context a\nresult of Motzkin and Straus from 1965 for the maximal interaction objective.",
    "descriptor": "\nComments: 10 pages, 1 figure\n",
    "authors": [
      "Thomas Robertazzi",
      "Maciej Drozdowski"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.14460"
  },
  {
    "id": "arXiv:2110.14461",
    "title": "Hand gesture detection in the hand movement test for the early diagnosis  of dementia",
    "abstract": "Collecting hands data is important for many cognitive studies, especially for\nsenior participants who has no IT background. For example, alternating hand\nmovements and imitation of gestures are formal cognitive assessment in the\nearly detection of dementia. During data collection process, one of the key\nsteps is to detect whether the participants is following the instruction\ncorrectly to do the correct gestures. Meanwhile, re-searchers found a lot of\nproblems in TAS Test hand movement data collection process, where is\nchallenging to detect similar gestures and guarantee the quality of the\ncollect-ed images. We have implemented a hand gesture detector to detect the\ngestures per-formed in the hand movement tests, which enables us to monitor if\nthe participants are following the instructions correctly. In this research, we\nhave processed 20,000 images collected from TAS Test and labelled 6,450 images\nto detect different hand poses in the hand movement tests. This paper has the\nfollowing three contributions. Firstly, we compared the performance of\ndifferent network structures for hand poses detection. Secondly, we introduced\na transformer block in the state of art network and increased the\nclassification performance of the similar gestures. Thirdly, we have created\ntwo datasets and included 20 percent of blurred images in the dataset to\ninvestigate how different network structures were impacted by noisy data, then\nwe proposed a novel net-work to increase the detection accuracy to mediate the\ninfluence of the noisy data.",
    "descriptor": "",
    "authors": [
      "Guan Huang",
      "Son N. Tran",
      "Quan Bai",
      "Jane Alty"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14461"
  },
  {
    "id": "arXiv:2110.14462",
    "title": "Superconvergence of the MINI mixed finite element discretization of the  Stokes problem: An experimental study in 3D",
    "abstract": "Stokes flows are a type of fluid flow where convective forces are small in\ncomparison with viscous forces, and momentum transport is entirely due to\nviscous diffusion. Besides being routinely used as benchmark test cases in\nnumerical fluid dynamics, Stokes flows are relevant in several applications in\nscience and engineering including porous media flow, biological flows,\nmicrofluidics, microrobotics, and hydrodynamic lubrication. The present study\nconcerns the discretization of the equations of motion of Stokes flows in three\ndimensions utilizing the MINI mixed finite element, focusing on the\nsuperconvergence of the method which was investigated with numerical\nexperiments using five purpose-made benchmark test cases with analytical\nsolution. Despite the fact that the MINI element is only linearly convergent\naccording to standard mixed finite element theory, a recent theoretical\ndevelopment proves that, for structured meshes in two dimensions, the pressure\nsuperconverges with order 1.5, as well as the linear part of the computed\nvelocity with respect to the piecewise-linear nodal interpolation of the exact\nvelocity. The numerical experiments documented herein suggest a more general\nvalidity of the superconvergence in pressure, possibly to unstructured\ntetrahedral meshes and even up to quadratic convergence which was observed with\none test problem, thereby indicating that there is scope to further extend the\navailable theoretical results on convergence.",
    "descriptor": "",
    "authors": [
      "Andrea Cioncolini",
      "Daniele Boffi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.14462"
  },
  {
    "id": "arXiv:2110.14464",
    "title": "Learning from demonstrations with SACR2: Soft Actor-Critic with Reward  Relabeling",
    "abstract": "During recent years, deep reinforcement learning (DRL) has made successful\nincursions into complex decision-making applications such as robotics,\nautonomous driving or video games. However, a well-known caveat of DRL\nalgorithms is their inefficiency, requiring huge amounts of data to converge.\nOff-policy algorithms tend to be more sample-efficient, and can additionally\nbenefit from any off-policy data stored in the replay buffer. Expert\ndemonstrations are a popular source for such data: the agent is exposed to\nsuccessful states and actions early on, which can accelerate the learning\nprocess and improve performance. In the past, multiple ideas have been proposed\nto make good use of the demonstrations in the buffer, such as pretraining on\ndemonstrations only or minimizing additional cost functions. We carry on a\nstudy to evaluate several of these ideas in isolation, to see which of them\nhave the most significant impact. We also present a new method, based on a\nreward bonus given to demonstrations and successful episodes. First, we give a\nreward bonus to the transitions coming from demonstrations to encourage the\nagent to match the demonstrated behaviour. Then, upon collecting a successful\nepisode, we relabel its transitions with the same bonus before adding them to\nthe replay buffer, encouraging the agent to also match its previous successes.\nThe base algorithm for our experiments is the popular Soft Actor-Critic (SAC),\na state-of-the-art off-policy algorithm for continuous action spaces. Our\nexperiments focus on robotics, specifically on a reaching task for a robotic\narm in simulation. We show that our method SACR2 based on reward relabeling\nimproves the performance on this task, even in the absence of demonstrations.",
    "descriptor": "\nComments: Presented at Deep RL Workshop, NeurIPS 2021\n",
    "authors": [
      "Jesus Bujalance Martin",
      "Rapha\u00ebl Chekroun",
      "Fabien Moutarde"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14464"
  },
  {
    "id": "arXiv:2110.14468",
    "title": "DESTA: A Framework for Safe Reinforcement Learning with Markov Games of  Intervention",
    "abstract": "Exploring in an unknown system can place an agent in dangerous situations,\nexposing to potentially catastrophic hazards. Many current approaches for\ntackling safe learning in reinforcement learning (RL) lead to a trade-off\nbetween safe exploration and fulfilling the task. Though these methods possibly\nincur fewer safety violations, they often also lead to reduced task\nperformance. In this paper, we take the first step in introducing a generation\nof RL solvers that learn to minimise safety violations while maximising the\ntask reward to the extend that can be tolerated by safe policies. Our approach\nuses a new two-player framework for safe RL called Distributive Exploration\nSafety Training Algorithm (DESTA). The core of DESTA is a novel game between\ntwo RL agents: SAFETY AGENT that is delegated the task of minimising safety\nviolations and TASK AGENT whose goal is to maximise the reward set by the\nenvironment task. SAFETY AGENT can selectively take control of the system at\nany given point to prevent safety violations while TASK AGENT is free to\nexecute its actions at all other states. This framework enables SAFETY AGENT to\nlearn to take actions that minimise future safety violations (during and after\ntraining) by performing safe actions at certain states while TASK AGENT\nperforms actions that maximise the task performance everywhere else. We\ndemonstrate DESTA's ability to tackle challenging tasks and compare against\nstate-of-the-art RL methods in Safety Gym Benchmarks which simulate real-world\nphysical systems and OpenAI's Lunar Lander.",
    "descriptor": "",
    "authors": [
      "David Mguni",
      "Joel Jennings",
      "Taher Jafferjee",
      "Aivar Sootla",
      "Yaodong Yang",
      "Changmin Yu",
      "Usman Islam",
      "Ziyan Wang",
      "Jun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14468"
  },
  {
    "id": "arXiv:2110.14491",
    "title": "Training Lightweight CNNs for Human-Nanodrone Proximity Interaction from  Small Datasets using Background Randomization",
    "abstract": "We consider the task of visually estimating the pose of a human from images\nacquired by a nearby nano-drone; in this context, we propose a data\naugmentation approach based on synthetic background substitution to learn a\nlightweight CNN model from a small real-world training set. Experimental\nresults on data from two different labs proves that the approach improves\ngeneralization to unseen environments.",
    "descriptor": "",
    "authors": [
      "Marco Ferri",
      "Dario Mantegazza",
      "Elia Cereda",
      "Nicky Zimmerman",
      "Luca M. Gambardella",
      "Daniele Palossi",
      "J\u00e9r\u00f4me Guzzi",
      "Alessandro Giusti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14491"
  },
  {
    "id": "arXiv:2110.14495",
    "title": "Convergence of Restricted Additive Schwarz with impedance transmission  conditions for discretised Helmholtz problems",
    "abstract": "The Restricted Additive Schwarz method with impedance transmission\nconditions, also known as the Optimised Restricted Additive Schwarz (ORAS)\nmethod, is a simple overlapping one-level parallel domain decomposition method,\nwhich has been successfully used as an iterative solver and as a preconditioner\nfor discretized Helmholtz boundary-value problems. In this paper, we give, for\nthe first time, a convergence analysis for ORAS as an iterative solver -- and\nalso as a preconditioner -- for nodal finite element Helmholtz systems of any\npolynomial order. The analysis starts by showing (for general domain\ndecompositions) that ORAS as an unconventional finite element approximation of\na classical parallel iterative Schwarz method, formulated at the PDE\n(non-discrete) level. This non-discrete Schwarz method was recently analysed in\n[Gong, Gander, Graham, Lafontaine, Spence, arXiv 2106.05218], and the present\npaper gives a corresponding discrete version of this analysis. In particular,\nfor domain decompositions in strips in 2-d, we show that, when the mesh size is\nsmall enough, ORAS inherits the convergence properties of the Schwarz method,\nindependent of polynomial order. The proof relies on characterising the ORAS\niteration in terms of discrete `impedance-to-impedance maps', which we prove\n(via a novel weighted finite-element error analysis) converge as $h\\rightarrow\n0$ in the operator norm to their non-discrete counterparts.",
    "descriptor": "\nComments: 34 pages, 2 figures\n",
    "authors": [
      "Shihua Gong",
      "Ivan G. Graham",
      "Euan A. Spence"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14495"
  },
  {
    "id": "arXiv:2110.14498",
    "title": "Structural Parameterizations of Budgeted Graph Coloring",
    "abstract": "We introduce a variant of the graph coloring problem, which we denote as {\\sc\nBudgeted Coloring Problem} (\\bcp). Given a graph $G$, an integer $c$ and an\nordered list of integers $\\{b_1, b_2, \\ldots, b_c\\}$, \\bcp asks whether there\nexists a proper coloring of $G$ where the $i$-th color is used to color at most\n$b_i$ many vertices. This problem generalizes two well-studied graph coloring\nproblems, {\\sc Bounded Coloring Problem} (\\bocp) and {\\sc Equitable Coloring\nProblem} (\\ecp) and as in the case of other coloring problems, it is \\nph even\nfor constant values of $c$. So we study \\bcp under the paradigm of\nparameterized complexity. \\begin{itemize}\n\\item We show that \\bcp is \\fpt (fixed-parameter tractable) parameterized by\nthe vertex cover size. This generalizes a similar result for \\ecp and\nimmediately extends to the \\bocp, which was earlier not known.\n\\item We show that \\bcp is polynomial time solvable for cluster graphs\ngeneralizing a similar result for \\ecp. However, we show that \\bcp is \\fpt, but\nunlikely to have polynomial kernel, when parameterized by the deletion distance\nto clique, contrasting the linear kernel for \\ecp for the same parameter.\n\\item While the \\bocp is known to be polynomial time solvable on split\ngraphs, we show that \\bcp is \\nph on split graphs. As \\bocp is hard on\nbipartite graphs when $c>3$, the result follows for \\bcp as well. We provide a\ndichotomy result by showing that \\bcp is polynomial time solvable on bipartite\ngraphs when $c=2$. We also show that \\bcp is \\nph on co-cluster graphs,\ncontrasting the polynomial time algorithm for \\ecp and \\bocp. \\end{itemize}\nFinally we present an $\\mathcal{O}^*(2^{|V(G)|})$ algorithm for the \\bcp,\ngeneralizing the known algorithm with a similar bound for the standard\nchromatic number.",
    "descriptor": "\nComments: 18 Pages\n",
    "authors": [
      "Susobhan Bandopadhyay",
      "Suman Banerjee",
      "Aritra Banik",
      "Venkatesh Raman"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14498"
  },
  {
    "id": "arXiv:2110.14503",
    "title": "Simple data balancing achieves competitive worst-group-accuracy",
    "abstract": "We study the problem of learning classifiers that perform well across (known\nor unknown) groups of data. After observing that common worst-group-accuracy\ndatasets suffer from substantial imbalances, we set out to compare\nstate-of-the-art methods to simple balancing of classes and groups by either\nsubsampling or reweighting data. Our results show that these data balancing\nbaselines achieve state-of-the-art-accuracy, while being faster to train and\nrequiring no additional hyper-parameters. In addition, we highlight that access\nto group information is most critical for model selection purposes, and not so\nmuch during training. All in all, our findings beg closer examination of\nbenchmarks and methods for research in worst-group-accuracy optimization.",
    "descriptor": "",
    "authors": [
      "Badr Youbi Idrissi",
      "Martin Arjovsky",
      "Mohammad Pezeshki",
      "David Lopez-Paz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.14503"
  },
  {
    "id": "arXiv:2110.14508",
    "title": "Finding Regions of Heterogeneity in Decision-Making via Expected  Conditional Covariance",
    "abstract": "Individuals often make different decisions when faced with the same context,\ndue to personal preferences and background. For instance, judges may vary in\ntheir leniency towards certain drug-related offenses, and doctors may vary in\ntheir preference for how to start treatment for certain types of patients. With\nthese examples in mind, we present an algorithm for identifying types of\ncontexts (e.g., types of cases or patients) with high inter-decision-maker\ndisagreement. We formalize this as a causal inference problem, seeking a region\nwhere the assignment of decision-maker has a large causal effect on the\ndecision. Our algorithm finds such a region by maximizing an empirical\nobjective, and we give a generalization bound for its performance. In a\nsemi-synthetic experiment, we show that our algorithm recovers the correct\nregion of heterogeneity accurately compared to baselines. Finally, we apply our\nalgorithm to real-world healthcare datasets, recovering variation that aligns\nwith existing clinical knowledge.",
    "descriptor": "\nComments: To appear in NeurIPS 2021\n",
    "authors": [
      "Justin Lim",
      "Christina X Ji",
      "Michael Oberst",
      "Saul Blecker",
      "Leora Horwitz",
      "David Sontag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14508"
  },
  {
    "id": "arXiv:2110.14509",
    "title": "Deep Transfer Learning for Multi-source Entity Linkage via Domain  Adaptation",
    "abstract": "Multi-source entity linkage focuses on integrating knowledge from multiple\nsources by linking the records that represent the same real world entity. This\nis critical in high-impact applications such as data cleaning and user\nstitching. The state-of-the-art entity linkage pipelines mainly depend on\nsupervised learning that requires abundant amounts of training data. However,\ncollecting well-labeled training data becomes expensive when the data from many\nsources arrives incrementally over time. Moreover, the trained models can\neasily overfit to specific data sources, and thus fail to generalize to new\nsources due to significant differences in data and label distributions. To\naddress these challenges, we present AdaMEL, a deep transfer learning framework\nthat learns generic high-level knowledge to perform multi-source entity\nlinkage. AdaMEL models the attribute importance that is used to match entities\nthrough an attribute-level self-attention mechanism, and leverages the massive\nunlabeled data from new data sources through domain adaptation to make it\ngeneric and data-source agnostic. In addition, AdaMEL is capable of\nincorporating an additional set of labeled data to more accurately integrate\ndata sources with different attribute importance. Extensive experiments show\nthat our framework achieves state-of-the-art results with 8.21% improvement on\naverage over methods based on supervised learning. Besides, it is more stable\nin handling different sets of data sources in less runtime.",
    "descriptor": "",
    "authors": [
      "Di Jin",
      "Bunyamin Sisman",
      "Hao Wei",
      "Xin Luna Dong",
      "Danai Koutra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.14509"
  },
  {
    "id": "arXiv:2110.14513",
    "title": "Neural Analysis and Synthesis: Reconstructing Speech from  Self-Supervised Representations",
    "abstract": "We present a neural analysis and synthesis (NANSY) framework that can\nmanipulate voice, pitch, and speed of an arbitrary speech signal. Most of the\nprevious works have focused on using information bottleneck to disentangle\nanalysis features for controllable synthesis, which usually results in poor\nreconstruction quality. We address this issue by proposing a novel training\nstrategy based on information perturbation. The idea is to perturb information\nin the original input signal (e.g., formant, pitch, and frequency response),\nthereby letting synthesis networks selectively take essential attributes to\nreconstruct the input signal. Because NANSY does not need any bottleneck\nstructures, it enjoys both high reconstruction quality and controllability.\nFurthermore, NANSY does not require any labels associated with speech data such\nas text and speaker information, but rather uses a new set of analysis\nfeatures, i.e., wav2vec feature and newly proposed pitch feature, Yingram,\nwhich allows for fully self-supervised training. Taking advantage of fully\nself-supervised training, NANSY can be easily extended to a multilingual\nsetting by simply training it with a multilingual dataset. The experiments show\nthat NANSY can achieve significant improvement in performance in several\napplications such as zero-shot voice conversion, pitch shift, and time-scale\nmodification.",
    "descriptor": "\nComments: Neural Information Processing Systems (NeurIPS) 2021\n",
    "authors": [
      "Hyeong-Seok Choi",
      "Juheon Lee",
      "Wansoo Kim",
      "Jie Hwan Lee",
      "Hoon Heo",
      "Kyogu Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.14513"
  },
  {
    "id": "arXiv:2110.14514",
    "title": "Streaming Generalized Canonical Polyadic Tensor Decompositions",
    "abstract": "In this paper, we develop a method which we call OnlineGCP for computing the\nGeneralized Canonical Polyadic (GCP) tensor decomposition of streaming data.\nGCP differs from traditional canonical polyadic (CP) tensor decompositions as\nit allows for arbitrary objective functions which the CP model attempts to\nminimize. This approach can provide better fits and more interpretable models\nwhen the observed tensor data is strongly non-Gaussian. In the streaming case,\ntensor data is gradually observed over time and the algorithm must\nincrementally update a GCP factorization with limited access to prior data. In\nthis work, we extend the GCP formalism to the streaming context by deriving a\nGCP optimization problem to be solved as new tensor data is observed, formulate\na tunable history term to balance reconstruction of recently observed data with\ndata observed in the past, develop a scalable solution strategy based on\nsegregated solves using stochastic gradient descent methods, describe a\nsoftware implementation that provides performance and portability to\ncontemporary CPU and GPU architectures and integrates with Matlab for enhanced\nuseability, and demonstrate the utility and performance of the approach and\nsoftware on several synthetic and real tensor data sets.",
    "descriptor": "",
    "authors": [
      "Eric Phipps",
      "Nick Johnson",
      "Tamara G. Kolda"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2110.14514"
  },
  {
    "id": "arXiv:2110.14516",
    "title": "Self-Contained Kinematic Calibration of a Novel Whole-Body Artificial  Skin for Human-Robot Collaboration",
    "abstract": "In this paper, we present an accelerometer-based kinematic calibration\nalgorithm to accurately estimate the pose of multiple sensor units distributed\nalong a robot body. Our approach is self-contained, can be used on any robot\nprovided with a Denavit-Hartenberg kinematic model, and on any skin equipped\nwith Inertial Measurement Units (IMUs). To validate the proposed method, we\nfirst conduct extensive experimentation in simulation and demonstrate a sub-cm\npositional error from ground truth data --an improvement of six times with\nrespect to prior work; subsequently, we then perform a real-world evaluation on\na seven degrees-of-freedom collaborative platform. For this purpose, we\nadditionally introduce a novel design for a stand-alone artificial skin\nequipped with an IMU for use with the proposed algorithm and a proximity sensor\nfor sensing distance to nearby objects. In conclusion, in this work, we\ndemonstrate seamless integration between a novel hardware design, an accurate\ncalibration method, and preliminary work on applications: the high positional\naccuracy effectively enables to locate distributed proximity data and allows\nfor a distributed avoidance controller to safely avoid obstacles and people\nwithout the need of additional sensing.",
    "descriptor": "",
    "authors": [
      "Kandai Watanabe",
      "Matthew Strong",
      "Mary West",
      "Caleb Escobedo",
      "Ander Aramburu",
      "Krishna Chaitanya Kodur",
      "Alessandro Roncone"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14516"
  },
  {
    "id": "arXiv:2110.14518",
    "title": "NIDA-CLIFGAN: Natural Infrastructure Damage Assessment through Efficient  Classification Combining Contrastive Learning, Information Fusion and  Generative Adversarial Networks",
    "abstract": "During natural disasters, aircraft and satellites are used to survey the\nimpacted regions. Usually human experts are needed to manually label the\ndegrees of the building damage so that proper humanitarian assistance and\ndisaster response (HADR) can be achieved, which is labor-intensive and\ntime-consuming. Expecting human labeling of major disasters over a wide area\ngravely slows down the HADR efforts. It is thus of crucial interest to take\nadvantage of the cutting-edge Artificial Intelligence and Machine Learning\ntechniques to speed up the natural infrastructure damage assessment process to\nachieve effective HADR. Accordingly, the paper demonstrates a systematic effort\nto achieve efficient building damage classification. First, two novel\ngenerative adversarial nets (GANs) are designed to augment data used to train\nthe deep-learning-based classifier. Second, a contrastive learning based method\nusing novel data structures is developed to achieve great performance. Third,\nby using information fusion, the classifier is effectively trained with very\nfew training data samples for transfer learning. All the classifiers are small\nenough to be loaded in a smart phone or simple laptop for first responders.\nBased on the available overhead imagery dataset, results demonstrate data and\ncomputational efficiency with 10% of the collected data combined with a GAN\nreducing the time of computation from roughly half a day to about 1 hour with\nroughly similar classification performances.",
    "descriptor": "",
    "authors": [
      "Jie Wei",
      "Zhigang Zhu",
      "Erik Blasch",
      "Bilal Abdulrahman",
      "Billy Davila",
      "Shuoxin Liu",
      "Jed Magracia",
      "Ling Fang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14518"
  },
  {
    "id": "arXiv:2110.14521",
    "title": "Active clustering for labeling training data",
    "abstract": "Gathering training data is a key step of any supervised learning task, and it\nis both critical and expensive. Critical, because the quantity and quality of\nthe training data has a high impact on the performance of the learned function.\nExpensive, because most practical cases rely on humans-in-the-loop to label the\ndata. The process of determining the correct labels is much more expensive than\ncomparing two items to see whether they belong to the same class. Thus\nmotivated, we propose a setting for training data gathering where the human\nexperts perform the comparatively cheap task of answering pairwise queries, and\nthe computer groups the items into classes (which can be labeled cheaply at the\nvery end of the process). Given the items, we consider two random models for\nthe classes: one where the set partition they form is drawn uniformly, the\nother one where each item chooses its class independently following a fixed\ndistribution. In the first model, we characterize the algorithms that minimize\nthe average number of queries required to cluster the items and analyze their\ncomplexity. In the second model, we analyze a specific algorithm family,\npropose as a conjecture that they reach the minimum average number of queries\nand compare their performance to a random approach. We also propose solutions\nto handle errors or inconsistencies in the experts' answers.",
    "descriptor": "\nComments: Accepted at Neurips 2021. The main part is 14 pages long, the rest is an appendix containing the long version of the proofs\n",
    "authors": [
      "Quentin Lutz",
      "\u00c9lie de Panafieu",
      "Alex Scott",
      "Maya Stein"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.14521"
  },
  {
    "id": "arXiv:2110.14524",
    "title": "Model based Multi-agent Reinforcement Learning with Tensor  Decompositions",
    "abstract": "A challenge in multi-agent reinforcement learning is to be able to generalize\nover intractable state-action spaces. Inspired from Tesseract [Mahajan et al.,\n2021], this position paper investigates generalisation in state-action space\nover unexplored state-action pairs by modelling the transition and reward\nfunctions as tensors of low CP-rank. Initial experiments on synthetic MDPs show\nthat using tensor decompositions in a model-based reinforcement learning\nalgorithm can lead to much faster convergence if the true transition and reward\nfunctions are indeed of low rank.",
    "descriptor": "",
    "authors": [
      "Pascal Van Der Vaart",
      "Anuj Mahajan",
      "Shimon Whiteson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.14524"
  },
  {
    "id": "arXiv:2110.14528",
    "title": "On Competitive Permutations for Set Cover by Intervals",
    "abstract": "We revisit the problem of computing an optimal partial cover of points by\nintervals. We show that the greedy algorithm computes a permutation $\\Pi =\n\\pi_1, \\pi_2,\\ldots$ of the intervals that is $3/4$-competitive for any prefix\nof $k$ intervals. That is, for any $k$, the intervals $\\pi_1 \\cup \\cdots \\cup\n\\pi_k$ covers at least $3/4$-fraction of the points covered by the optimal\nsolution using $k$ intervals.\nWe also provide an approximation algorithm that, in $O(n + m/\\varepsilon)$\ntime, computes a cover by $(1+\\varepsilon)k$ intervals that is as good as the\noptimal solution using $k$ intervals, where $n$ is the number of input points,\nand $m$ is the number of intervals (we assume here the input is presorted).\nFinally, we show a counter example illustrating that the optimal solutions\nfor set cover do not have the diminishing return property -- that is, the\nmarginal benefit from using more sets is not monotonically decreasing.\nFortunately, the diminishing returns does hold for intervals.",
    "descriptor": "",
    "authors": [
      "Sariel Har-Peled",
      "Jiaqi Cheng"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14528"
  },
  {
    "id": "arXiv:2110.14529",
    "title": "HSVI fo zs-POSGs using Concavity, Convexity and Lipschitz Properties",
    "abstract": "Dynamic programming and heuristic search are at the core of state-of-the-art\nsolvers for sequential decision-making problems. In partially observable or\ncollaborative settings (\\eg, POMDPs and Dec-POMDPs), this requires introducing\nan appropriate statistic that induces a fully observable problem as well as\nbounding (convex) approximators of the optimal value function. This approach\nhas succeeded in some subclasses of 2-player zero-sum partially observable\nstochastic games (zs-POSGs) as well, but failed in the general case despite\nknown concavity and convexity properties, which only led to heuristic\nalgorithms with poor convergence guarantees. We overcome this issue, leveraging\non these properties to derive bounding approximators and efficient update and\nselection operators, before deriving a prototypical solver inspired by HSVI\nthat provably converges to an $\\epsilon$-optimal solution in finite time, and\nwhich we empirically evaluate. This opens the door to a novel family of\npromising approaches complementing those relying on linear programming or\niterative methods.",
    "descriptor": "\nComments: 37 pages, 4 figures, 4 tables, 3 algorithms\n",
    "authors": [
      "Aur\u00e9lien Delage",
      "Olivier Buffet",
      "Jilles Dibangoye"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14529"
  },
  {
    "id": "arXiv:2110.14532",
    "title": "FacTeR-Check: Semi-automated fact-checking through Semantic Similarity  and Natural Language Inference",
    "abstract": "Our society produces and shares overwhelming amounts of information through\nthe Online Social Networks (OSNs). Within this environment, misinformation and\ndisinformation have proliferated, becoming a public safety concern on every\ncountry. Allowing the public and professionals to efficiently find reliable\nevidence about the factual veracity of a claim is crucial to mitigate this\nharmful spread. To this end, we propose FacTeR-Check, a multilingual\narchitecture for semi-automated fact-checking that can be used for either the\ngeneral public but also useful for fact-checking organisations. FacTeR-Check\nenables retrieving fact-checked information, unchecked claims verification and\ntracking dangerous information over social media. This architectures involves\nseveral modules developed to evaluate semantic similarity, to calculate natural\nlanguage inference and to retrieve information from Online Social Networks. The\nunion of all these modules builds a semi-automated fact-checking tool able of\nverifying new claims, to extract related evidence, and to track the evolution\nof a hoax on a OSN. While individual modules are validated on related\nbenchmarks (mainly MSTS and SICK), the complete architecture is validated using\na new dataset called NLI19-SP that is publicly released with COVID-19 related\nhoaxes and tweets from Spanish social media. Our results show state-of-the-art\nperformance on the individual benchmarks, as well as producing useful analysis\nof the evolution over time of 61 different hoaxes.",
    "descriptor": "",
    "authors": [
      "Alejandro Mart\u00edn",
      "Javier Huertas-Tato",
      "\u00c1lvaro Huertas-Garc\u00eda",
      "Guillermo Villar-Rodr\u00edguez",
      "David Camacho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14532"
  },
  {
    "id": "arXiv:2110.14535",
    "title": "Comparing Heuristics, Constraint Optimization, and Reinforcement  Learning for an Industrial 2D Packing Problem",
    "abstract": "Cutting and Packing problems are occurring in different industries with a\ndirect impact on the revenue of businesses. Generally, the goal in Cutting and\nPacking is to assign a set of smaller objects to a set of larger objects. To\nsolve Cutting and Packing problems, practitioners can resort to heuristic and\nexact methodologies. Lately, machine learning is increasingly used for solving\nsuch problems. This paper considers a 2D packing problem from the furniture\nindustry, where a set of wooden workpieces must be assigned to different\nmodules of a trolley in the most space-saving way. We present an experimental\nsetup to compare heuristics, constraint optimization, and deep reinforcement\nlearning for the given problem. The used methodologies and their results get\ncollated in terms of their solution quality and runtime. In the given use case\na greedy heuristic produces optimal results and outperforms the other\napproaches in terms of runtime. Constraint optimization also produces optimal\nresults but requires more time to perform. The deep reinforcement learning\napproach did not always produce optimal or even feasible solutions. While we\nassume this could be remedied with more training, considering the good results\nwith the heuristic, deep reinforcement learning seems to be a bad fit for the\ngiven use case.",
    "descriptor": "",
    "authors": [
      "Stefan B\u00f6hm",
      "Martin Neumayer",
      "Oliver Kramer",
      "Alexander Schiendorfer",
      "Alois Knoll"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14535"
  },
  {
    "id": "arXiv:2110.14538",
    "title": "Reinforcement Learning in Factored Action Spaces using Tensor  Decompositions",
    "abstract": "We present an extended abstract for the previously published work TESSERACT\n[Mahajan et al., 2021], which proposes a novel solution for Reinforcement\nLearning (RL) in large, factored action spaces using tensor decompositions. The\ngoal of this abstract is twofold: (1) To garner greater interest amongst the\ntensor research community for creating methods and analysis for approximate RL,\n(2) To elucidate the generalised setting of factored action spaces where tensor\ndecompositions can be used. We use cooperative multi-agent reinforcement\nlearning scenario as the exemplary setting where the action space is naturally\nfactored across agents and learning becomes intractable without resorting to\napproximation on the underlying hypothesis space for candidate solutions.",
    "descriptor": "",
    "authors": [
      "Anuj Mahajan",
      "Mikayel Samvelyan",
      "Lei Mao",
      "Viktor Makoviychuk",
      "Animesh Garg",
      "Jean Kossaifi",
      "Shimon Whiteson",
      "Yuke Zhu",
      "Animashree Anandkumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.14538"
  },
  {
    "id": "arXiv:2110.14541",
    "title": "Deep Reinforcement Learning for Simultaneous Sensing and Channel Access  in Cognitive Networks",
    "abstract": "We consider the problem of dynamic spectrum access (DSA) in cognitive\nwireless networks, where only partial observations are available to the users\ndue to narrowband sensing and transmissions. The cognitive network consists of\nprimary users (PUs) and a secondary user (SU), which operate in a time\nduplexing regime. The traffic pattern for each PU is assumed to be unknown to\nthe SU and is modeled as a finite-memory Markov chain. Since observations are\npartial, then both channel sensing and access actions affect the throughput.\nThe objective is to maximize the SU's long-term throughput. To achieve this\ngoal, we develop a novel algorithm that learns both access and sensing policies\nvia deep Q-learning, dubbed Double Deep Q-network for Sensing and Access\n(DDQSA). To the best of our knowledge, this is the first paper that solves both\nsensing and access policies for DSA via deep Q-learning. Second, we analyze the\noptimal policy theoretically to validate the performance of DDQSA. Although the\ngeneral DSA problem is P-SPACE hard, we derive the optimal policy explicitly\nfor a common model of a cyclic user dynamics. Our results show that DDQSA\nlearns a policy that implements both sensing and channel access, and\nsignificantly outperforms existing approaches.",
    "descriptor": "\nComments: 27 pages, 6 figures\n",
    "authors": [
      "Yoel Bokobza",
      "Ron Dabora",
      "Kobi Cohen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14541"
  },
  {
    "id": "arXiv:2110.14544",
    "title": "Power Minimization of Downlink Spectrum Slicing for eMBB and URLLC Users",
    "abstract": "5G technology allows the presence of heterogeneous services in the same\nphysical network. On the radio access network (RAN), the spectrum slicing of\nthe shared radio resources is a critical task to guarantee the performance of\neach service. In this paper, we analyze a downlink communication in which a\nbase station (BS) should serve two types of traffic, enhanced mobile broadband\n(eMBB) and ultra-reliable low-latency communication (URLLC), respectively. Due\nto the nature of low-latency traffic, the BS knows the channel state\ninformation (CSI) of the eMBB users only. In this setting, we study the power\nminimization problem employing orthogonal multiple access (OMA) and\nnon-orthogonal multiple access (NOMA) schemes. We analyze the impact of\nresource sharing, showing that the knowledge of eMBB CSI can be used also in\nresource allocation for URLLC users. Based on this analysis, we propose two\nalgorithms: a feasible and a block coordinated descent approach (BCD). We show\nthat the BCD is optimal for the URLLC power allocation. The numerical results\nshow that NOMA leads to a lower power consumption compared to OMA, except when\nthe URLLC user is very close to the BS. For the last case, the optimal approach\ndepends on the channel condition of the eMBB user. In any case, even when the\nOMA paradigm attains the best performance, the gap with NOMA is negligible,\nproving the NOMA capacity in exploiting the shared resources to reduce the\npower consumption in every condition.",
    "descriptor": "\nComments: This work will be submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Fabio Saggese",
      "Marco Moretti",
      "Petar Popovski"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14544"
  },
  {
    "id": "arXiv:2110.14545",
    "title": "Performance prediction of massively parallel computation by Bayesian  inference",
    "abstract": "A performance prediction method for massively parallel computation is\nproposed. The method is based on performance modeling and Bayesian inference to\npredict elapsed time T as a function of the number of used nodes P (T=T(P)).\nThe focus is on extrapolation for larger values of P from the perspective of\napplication researchers. The proposed method has several improvements over the\nmethod developed in a previous paper, and application to real-symmetric\ngeneralized eigenvalue problem shows promising prediction results. The method\nis generalizable and applicable to many other computations.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Hisashi Kohashi",
      "Harumichi Iwamoto",
      "Takeshi Fukaya",
      "Yusaku Yamamoto",
      "Takeo Hoshi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14545"
  },
  {
    "id": "arXiv:2110.14548",
    "title": "Stability estimates for radial basis function methods applied to  time-dependent hyperbolic PDEs",
    "abstract": "We derive stability estimates for three commonly used radial basis function\n(RBF) methods to solve hyperbolic time-dependent PDEs: the RBF generated finite\ndifference (RBF-FD) method, the RBF partition of unity method (RBF-PUM) and\nKansa's (global) RBF method. We give the estimates in the discrete\n$\\ell_2$-norm intrinsic to each of the three methods.The results show that\nKansa's method and RBF-PUM can be $\\ell_2$-stable in time under a sufficiently\nlarge oversampling of the discretized system of equations. On the other hand,\nthe RBF-FD method is not $\\ell_2$-stable by construction, no matter how large\nthe oversampling is. We show that this is due to the jumps (discontinuities) in\nthe RBF-FD cardinal basis functions. We also provide a stabilization of the\nRBF-FD method that penalizes the spurious jumps. Numerical experiments show an\nagreement with our theoretical observations.",
    "descriptor": "",
    "authors": [
      "Igor Tominec",
      "Murtazo Nazarov",
      "Elisabeth Larsson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14548"
  },
  {
    "id": "arXiv:2110.14553",
    "title": "GenURL: A General Framework for Unsupervised Representation Learning",
    "abstract": "Recently unsupervised representation learning (URL) has achieved remarkable\nprogress in various scenarios. However, most methods are specifically designed\nbased on specific data characters or task assumptions. Based on the manifold\nassumption, we regard most URL problems as an embedding problem that seeks an\noptimal low-dimensional representation of the given high-dimensional data. We\nsplit the embedding process into two steps, data structural modeling and\nlow-dimensional embedding, and propose a general similarity-based framework\ncalled GenURL. Specifically, we provide a general method to model data\nstructures by adaptively combining graph distances on the feature space and\npredefined graphs, then propose robust loss functions to learn the\nlow-dimensional embedding. Combining with a specific pretext task, we can adapt\nGenURL to various URL tasks in a unified manner and achieve state-of-the-art\nperformance, including self-supervised visual representation learning,\nunsupervised knowledge distillation, graph embeddings, and dimension reduction.\nMoreover, ablation studies of loss functions and basic hyper-parameter settings\nin GenURL illustrate the data characters of various tasks.",
    "descriptor": "\nComments: Preprint with 11 pages, 6 figures\n",
    "authors": [
      "Siyuan Li",
      "Zelin Zang",
      "Di Wu",
      "Zhiyuan Chen",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14553"
  },
  {
    "id": "arXiv:2110.14555",
    "title": "V-Learning -- A Simple, Efficient, Decentralized Algorithm for  Multiagent RL",
    "abstract": "A major challenge of multiagent reinforcement learning (MARL) is the curse of\nmultiagents, where the size of the joint action space scales exponentially with\nthe number of agents. This remains to be a bottleneck for designing efficient\nMARL algorithms even in a basic scenario with finitely many states and actions.\nThis paper resolves this challenge for the model of episodic Markov games. We\ndesign a new class of fully decentralized algorithms -- V-learning, which\nprovably learns Nash equilibria (in the two-player zero-sum setting),\ncorrelated equilibria and coarse correlated equilibria (in the multiplayer\ngeneral-sum setting) in a number of samples that only scales with\n$\\max_{i\\in[m]} A_i$, where $A_i$ is the number of actions for the $i^{\\rm th}$\nplayer. This is in sharp contrast to the size of the joint action space which\nis $\\prod_{i=1}^m A_i$. V-learning (in its basic form) is a new class of\nsingle-agent RL algorithms that convert any adversarial bandit algorithm with\nsuitable regret guarantees into a RL algorithm. Similar to the classical\nQ-learning algorithm, it performs incremental updates to the value functions.\nDifferent from Q-learning, it only maintains the estimates of V-values instead\nof Q-values. This key difference allows V-learning to achieve the claimed\nguarantees in the MARL setting by simply letting all agents run V-learning\nindependently.",
    "descriptor": "\nComments: This is the journal version of arXiv:2006.12007, with new results on (1) finding CE and CCE in the multiplayer general-sum setting, (2) monotonic techniques that allow V-learning to output Markov policies in a subset of settings, and (3) decoupling V-learning with the adversarial bandit subroutine\n",
    "authors": [
      "Chi Jin",
      "Qinghua Liu",
      "Yuanhao Wang",
      "Tiancheng Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14555"
  },
  {
    "id": "arXiv:2110.14560",
    "title": "Bugs in Quantum Computing Platforms: An Empirical Study",
    "abstract": "The interest in quantum computing is growing, and with it, the importance of\nsoftware platforms to develop quantum programs. Ensuring the correctness of\nsuch platforms is important, and it requires a thorough understanding of the\nbugs they typically suffer from. To address this need, this paper presents the\nfirst in-depth study of bugs in quantum computing platforms. We gather and\ninspect a set of 223 real-world bugs from 18 open-source quantum computing\nplatforms. Our study shows that a significant fraction of these bugs (39.9%)\nare quantum-specific, calling for dedicated approaches to prevent and find\nthem. The bugs are spread across various components, but quantum-specific bugs\noccur particularly often in components that represent, compile, and optimize\nquantum programming abstractions. Many quantum-specific bugs manifest through\nunexpected outputs, rather than more obvious signs of misbehavior, such as\ncrashes. Finally, we present a hierarchy of recurrent bug patterns, including\nten novel, quantum-specific patterns. Our findings not only show the importance\nand prevalence bugs in quantum computing platforms, but they help developers to\navoid common mistakes and tool builders to tackle the challenge of preventing,\nfinding, and fixing these bugs.",
    "descriptor": "",
    "authors": [
      "Matteo Paltenghi",
      "Michael Pradel"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.14560"
  },
  {
    "id": "arXiv:2110.14565",
    "title": "DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with  Prototypical Representations",
    "abstract": "Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as\nDreamer, learn the world model by reconstructing the image observations. Hence,\nthey often fail to discard task-irrelevant details and struggle to handle\nvisual distractions. To address this issue, previous work has proposed to\ncontrastively learn the world model, but the performance tends to be inferior\nin the absence of distractions. In this paper, we seek to enhance robustness to\ndistractions for MBRL agents. Specifically, we consider incorporating\nprototypical representations, which have yielded more accurate and robust\nresults than contrastive approaches in computer vision. However, it remains\nelusive how prototypical representations can benefit temporal dynamics learning\nin MBRL, since they treat each image independently without capturing temporal\nstructures. To this end, we propose to learn the prototypes from the recurrent\nstates of the world model, thereby distilling temporal structures from past\nobservations and actions into the prototypes. The resulting model, DreamerPro,\nsuccessfully combines Dreamer with prototypes, making large performance gains\non the DeepMind Control suite both in the standard setting and when there are\ncomplex background distractions. Code available at\nhttps://github.com/fdeng18/dreamer-pro .",
    "descriptor": "",
    "authors": [
      "Fei Deng",
      "Ingook Jang",
      "Sungjin Ahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14565"
  },
  {
    "id": "arXiv:2110.14566",
    "title": "IndoNLI: A Natural Language Inference Dataset for Indonesian",
    "abstract": "We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We\nadapt the data collection protocol for MNLI and collect nearly 18K sentence\npairs annotated by crowd workers and experts. The expert-annotated data is used\nexclusively as a test set. It is designed to provide a challenging test-bed for\nIndonesian NLI by explicitly incorporating various linguistic phenomena such as\nnumerical reasoning, structural changes, idioms, or temporal and spatial\nreasoning. Experiment results show that XLM-R outperforms other pre-trained\nmodels in our data. The best performance on the expert-annotated data is still\nfar below human performance (13.4% accuracy gap), suggesting that this test set\nis especially challenging. Furthermore, our analysis shows that our\nexpert-annotated data is more diverse and contains fewer annotation artifacts\nthan the crowd-annotated data. We hope this dataset can help accelerate\nprogress in Indonesian NLP research.",
    "descriptor": "\nComments: Accepted at EMNLP 2021 main conference\n",
    "authors": [
      "Rahmad Mahendra",
      "Alham Fikri Aji",
      "Samuel Louvan",
      "Fahrurrozi Rahman",
      "Clara Vania"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.14566"
  },
  {
    "id": "arXiv:2110.14573",
    "title": "Autonomous Exploration Development Environment and the Planning  Algorithms",
    "abstract": "Autonomous Exploration Development Environment is an open-source repository\nreleased to facilitate the development of high-level planning algorithms and\nintegration of complete autonomous navigation systems. The repository contains\nrepresentative simulation environment models, fundamental navigation modules,\ne.g., local planner, terrain traversability analysis, waypoint following, and\nvisualization tools. Together with two of our high-level planner releases --\nTARE planner for exploration and FAR planner for route planning, we detail\nusage of the three open-source repositories and share experiences in the\nintegration of autonomous navigation systems. We use DARPA Subterranean\nChallenge as a use case where the repositories together form the main\nnavigation system of the CMU-OSU Team. In the end, we discuss a few potential\nuse cases in extended applications.",
    "descriptor": "",
    "authors": [
      "Chao Cao",
      "Hongbiao Zhu",
      "Fan Yang",
      "Yukun Xia",
      "Howie Choset",
      "Jean Oh",
      "Ji Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.14573"
  },
  {
    "id": "arXiv:2110.14577",
    "title": "A Geometric Perspective towards Neural Calibration via Sensitivity  Decomposition",
    "abstract": "It is well known that vision classification models suffer from poor\ncalibration in the face of data distribution shifts. In this paper, we take a\ngeometric approach to this problem. We propose Geometric Sensitivity\nDecomposition (GSD) which decomposes the norm of a sample feature embedding and\nthe angular similarity to a target classifier into an instance-dependent and an\ninstance-independent component. The instance-dependent component captures the\nsensitive information about changes in the input while the instance-independent\ncomponent represents the insensitive information serving solely to minimize the\nloss on the training dataset. Inspired by the decomposition, we analytically\nderive a simple extension to current softmax-linear models, which learns to\ndisentangle the two components during training. On several common vision\nmodels, the disentangled model outperforms other calibration methods on\nstandard calibration metrics in the face of out-of-distribution (OOD) data and\ncorruption with significantly less complexity. Specifically, we surpass the\ncurrent state of the art by 30.8% relative improvement on corrupted CIFAR100 in\nExpected Calibration Error. Code available at\nhttps://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021 as a spotlight paper\n",
    "authors": [
      "Junjiao Tian",
      "Dylan Yung",
      "Yen-Chang Hsu",
      "Zsolt Kira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14577"
  },
  {
    "id": "arXiv:2110.14578",
    "title": "Spatio-Temporal Federated Learning for Massive Wireless Edge Networks",
    "abstract": "This paper presents a novel approach to conduct highly efficient federated\nlearning (FL) over a massive wireless edge network, where an edge server and\nnumerous mobile devices (clients) jointly learn a global model without\ntransporting the huge amount of data collected by the mobile devices to the\nedge server. The proposed FL approach is referred to as spatio-temporal FL\n(STFL), which jointly exploits the spatial and temporal correlations between\nthe learning updates from different mobile devices scheduled to join STFL in\nvarious training epochs. The STFL model not only represents the realistic\nintermittent learning behavior from the edge server to the mobile devices due\nto data delivery outage, but also features a mechanism of compensating loss\nlearning updates in order to mitigate the impacts of intermittent learning. An\nanalytical framework of STFL is proposed and employed to study the learning\ncapability of STFL via its convergence performance. In particular, we have\nassessed the impact of data delivery outage, intermittent learning mitigation,\nand statistical heterogeneity of datasets on the convergence performance of\nSTFL. The results provide crucial insights into the design and analysis of STFL\nbased wireless networks.",
    "descriptor": "\nComments: 3 figures, conference\n",
    "authors": [
      "Chun-Hung Liu",
      "Kai-Ten Feng",
      "Lu Wei",
      "Yu Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14578"
  },
  {
    "id": "arXiv:2110.14579",
    "title": "Bi-fidelity stochastic collocation methods for epidemic transport models  with uncertainties",
    "abstract": "Uncertainty in data is certainly one of the main problems in epidemiology, as\nshown by the recent COVID-19 pandemic. The need for efficient methods capable\nof quantifying uncertainty in the mathematical model is essential in order to\nproduce realistic scenarios of the spread of infection. In this paper, we\nintroduce a bi-fidelity approach to quantify uncertainty in spatially dependent\nepidemic models. The approach is based on evaluating a high-fidelity model on a\nsmall number of samples properly selected from a large number of evaluations of\na low-fidelity model. In particular, we will consider the class of multiscale\ntransport models recently introduced in Bertaglia, Boscheri, Dimarco &\nPareschi, Math. Biosci. Eng. (2021) and Boscheri, Dimarco & Pareschi, Math.\nMod. Meth. App. Scie. (2021) as the high-fidelity reference and use simple\ntwo-velocity discrete models for low-fidelity evaluations. Both models share\nthe same diffusive behavior and are solved with ad-hoc asymptotic-preserving\nnumerical discretizations. A series of numerical experiments confirm the\nvalidity of the approach.",
    "descriptor": "",
    "authors": [
      "Giulia Bertaglia",
      "Liu Liu",
      "Lorenzo Pareschi",
      "Xueyu Zhu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2110.14579"
  },
  {
    "id": "arXiv:2110.14583",
    "title": "Deep learning via message passing algorithms based on belief propagation",
    "abstract": "Message-passing algorithms based on the Belief Propagation (BP) equations\nconstitute a well-known distributed computational scheme. It is exact on\ntree-like graphical models and has also proven to be effective in many problems\ndefined on graphs with loops (from inference to optimization, from signal\nprocessing to clustering). The BP-based scheme is fundamentally different from\nstochastic gradient descent (SGD), on which the current success of deep\nnetworks is based. In this paper, we present and adapt to mini-batch training\non GPUs a family of BP-based message-passing algorithms with a reinforcement\nfield that biases distributions towards locally entropic solutions. These\nalgorithms are capable of training multi-layer neural networks with discrete\nweights and activations with performance comparable to SGD-inspired heuristics\n(BinaryNet) and are naturally well-adapted to continual learning. Furthermore,\nusing these algorithms to estimate the marginals of the weights allows us to\nmake approximate Bayesian predictions that have higher accuracy than point-wise\nsolutions.",
    "descriptor": "",
    "authors": [
      "Carlo Lucibello",
      "Fabrizio Pittorino",
      "Gabriele Perugini",
      "Riccardo Zecchina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14583"
  },
  {
    "id": "arXiv:2110.14587",
    "title": "Boundary Guided Context Aggregation for Semantic Segmentation",
    "abstract": "The recent studies on semantic segmentation are starting to notice the\nsignificance of the boundary information, where most approaches see boundaries\nas the supplement of semantic details. However, simply combing boundaries and\nthe mainstream features cannot ensure a holistic improvement of semantics\nmodeling. In contrast to the previous studies, we exploit boundary as a\nsignificant guidance for context aggregation to promote the overall semantic\nunderstanding of an image. To this end, we propose a Boundary guided Context\nAggregation Network (BCANet), where a Multi-Scale Boundary extractor (MSB)\nborrowing the backbone features at multiple scales is specifically designed for\naccurate boundary detection. Based on which, a Boundary guided Context\nAggregation module (BCA) improved from Non-local network is further proposed to\ncapture long-range dependencies between the pixels in the boundary regions and\nthe ones inside the objects. By aggregating the context information along the\nboundaries, the inner pixels of the same category achieve mutual gains and\ntherefore the intra-class consistency is enhanced. We conduct extensive\nexperiments on the Cityscapes and ADE20K databases, and comparable results are\nachieved with the state-of-the-art methods, clearly demonstrating the\neffectiveness of the proposed one.",
    "descriptor": "\nComments: Accepted at BMVC'2021\n",
    "authors": [
      "Haoxiang Ma",
      "Hongyu Yang",
      "Di Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14587"
  },
  {
    "id": "arXiv:2110.14588",
    "title": "Fuzzy Generative Adversarial Networks",
    "abstract": "Generative Adversarial Networks (GANs) are well-known tools for data\ngeneration and semi-supervised classification. GANs, with less labeled data,\noutperform Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs)\nin classification across various tasks, this shows promise for developing GANs\ncapable of trespassing into the domain of semi-supervised regression. However,\ndeveloping GANs for regression introduce two major challenges: (1) inherent\ninstability in the GAN formulation and (2) performing regression and achieving\nstability simultaneously. This paper introduces techniques that show\nimprovement in the GANs' regression capability through mean absolute error\n(MAE) and mean squared error (MSE). We bake a differentiable fuzzy logic system\nat multiple locations in a GAN because fuzzy logic systems have demonstrated\nhigh efficacy in classification and regression settings. The fuzzy logic takes\nthe output of either or both the generator and the discriminator to either or\nboth predict the output, $y$, and evaluate the generator's performance. We\noutline the results of applying the fuzzy logic system to CGAN and summarize\neach approach's efficacy. This paper shows that adding a fuzzy logic layer can\nenhance GAN's ability to perform regression; the most desirable injection\nlocation is problem-specific, and we show this through experiments over various\ndatasets. Besides, we demonstrate empirically that the fuzzy-infused GAN is\ncompetitive with DNNs.",
    "descriptor": "",
    "authors": [
      "Ryan Nguyen",
      "Shubhendu Kumar Singh",
      "Rahul Rai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14588"
  },
  {
    "id": "arXiv:2110.14590",
    "title": "TMBuD: A dataset for urban scene building detection",
    "abstract": "Building recognition and 3D reconstruction of human made structures in urban\nscenarios has become an interesting and actual topic in the image processing\ndomain. For this research topic the Computer Vision and Augmented Reality areas\nintersect for creating a better understanding of the urban scenario for various\ntopics. In this paper we aim to introduce a dataset solution, the TMBuD, that\nis better fitted for image processing on human made structures for urban scene\nscenarios. The proposed dataset will allow proper evaluation of salient edges\nand semantic segmentation of images focusing on the street view perspective of\nbuildings. The images that form our dataset offer various street view\nperspectives of buildings from urban scenarios, which allows for evaluating\ncomplex algorithms. The dataset features 160 images of buildings from\nTimisoara, Romania, with a resolution of 768 x 1024 pixels each.",
    "descriptor": "",
    "authors": [
      "Orhei Ciprian",
      "Vert Silviu",
      "Mocofan Muguras",
      "Vasiu Radu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14590"
  },
  {
    "id": "arXiv:2110.14596",
    "title": "Efficient and Secure TSA for the Tangle",
    "abstract": "The Tangle is the data structure used to store transactions in the IOTA\ncryptocurrency. In the Tangle, each block has two parents. As a result, the\nblocks do not form a chain, but a directed acyclic graph. In traditional\nBlockchain, a new block is appended to the heaviest chain in case of fork. In\nthe Tangle, the parent selection is done by the Tip Selection Algorithm (TSA).\nIn this paper, we make some important observations about the security of\nexisting TSAs. We then propose a new TSA that has low complexity and is more\nsecure than previous TSAs.",
    "descriptor": "",
    "authors": [
      "Quentin Bramas"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computational Complexity (cs.CC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.14596"
  },
  {
    "id": "arXiv:2110.14597",
    "title": "Evaluating Deep Learning Models and Adversarial Attacks on  Accelerometer-Based Gesture Authentication",
    "abstract": "Gesture-based authentication has emerged as a non-intrusive, effective means\nof authenticating users on mobile devices. Typically, such authentication\ntechniques have relied on classical machine learning techniques, but recently,\ndeep learning techniques have been applied this problem. Although prior\nresearch has shown that deep learning models are vulnerable to adversarial\nattacks, relatively little research has been done in the adversarial domain for\nbehavioral biometrics. In this research, we collect tri-axial accelerometer\ngesture data (TAGD) from 46 users and perform classification experiments with\nboth classical machine learning and deep learning models. Specifically, we\ntrain and test support vector machines (SVM) and convolutional neural networks\n(CNN). We then consider a realistic adversarial attack, where we assume the\nattacker has access to real users' TAGD data, but not the authentication model.\nWe use a deep convolutional generative adversarial network (DC-GAN) to create\nadversarial samples, and we show that our deep learning model is surprisingly\nrobust to such an attack scenario.",
    "descriptor": "",
    "authors": [
      "Elliu Huang",
      "Fabio Di Troia",
      "Mark Stamp"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14597"
  },
  {
    "id": "arXiv:2110.14607",
    "title": "Average-Case Subset Balancing Problems",
    "abstract": "Given a set of $n$ input integers, the Equal Subset Sum problem asks us to\nfind two distinct subsets with the same sum. In this paper we present an\nalgorithm that runs in time $O^*(3^{0.387n})$ in the~average case,\nsignificantly improving over the $O^*(3^{0.488n})$ running time of the best\nknown worst-case algorithm and the Meet-in-the-Middle benchmark of\n$O^*(3^{0.5n})$.\nOur algorithm generalizes to a number of related problems, such as the\n``Generalized Equal Subset Sum'' problem, which asks us to assign a coefficient\n$c_i$ from a set $C$ to each input number $x_i$ such that $\\sum_{i} c_i x_i =\n0$. Our algorithm for the average-case version of this problem runs in~time\n$|C|^{(0.5-c_0/|C|)n}$ for some positive constant $c_0$, whenever $C=\\{0, \\pm\n1, \\dots, \\pm d\\}$ or $\\{\\pm 1, \\dots, \\pm d\\}$ for some positive integer $d$\n(with $O^*(|C|^{0.45n})$ when $|C|<10$). Our results extend to the~problem of\nfinding ``nearly balanced'' solutions in which the target is a not-too-large\nnonzero offset $\\tau$.\nOur approach relies on new structural results that characterize the\nprobability that $\\sum_{i} c_i x_i$ $=\\tau$ has a solution $c \\in C^n$ when\n$x_i$'s are chosen randomly; these results may be of independent interest. Our\nalgorithm is inspired by the ``representation technique'' introduced by\nHowgrave-Graham and Joux. This requires several new ideas to overcome\npreprocessing hurdles that arise in the representation framework, as well as a\nnovel application of dynamic programming in the solution recovery phase of the\nalgorithm.",
    "descriptor": "\nComments: 44 pages, 5 figures\n",
    "authors": [
      "Xi Chen",
      "Yaonan Jin",
      "Tim Randolph",
      "Rocco A. Servedio"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14607"
  },
  {
    "id": "arXiv:2110.14608",
    "title": "Minimum Probability of Error of List M-ary Hypothesis Testing",
    "abstract": "We study a variation of Bayesian M-ary hypothesis testing in which the test\noutputs a list of L candidates out of the M possible upon processing the\nobservation. We study the minimum error probability of list hypothesis testing,\nwhere an error is defined as the event where the true hypothesis is not in the\nlist output by the test. We derive two exact expressions of the minimum\nprobability or error. The first is expressed as the error probability of a\ncertain non-Bayesian binary hypothesis test, and is reminiscent of the\nmeta-converse bound. The second, is expressed as the tail probability of the\nlikelihood ratio between the two distributions involved in the aforementioned\nnon-Bayesian binary hypothesis test.",
    "descriptor": "\nComments: 11 pages, submitted to Information and Inference: a journal of the IMA\n",
    "authors": [
      "Ehsan Asadi Kangarshahi",
      "Albert Guillen i Fabregas"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.14608"
  },
  {
    "id": "arXiv:2110.14613",
    "title": "International Workshop on Continual Semi-Supervised Learning:  Introduction, Benchmarks and Baselines",
    "abstract": "The aim of this paper is to formalize a new continual semi-supervised\nlearning (CSSL) paradigm, proposed to the attention of the machine learning\ncommunity via the IJCAI 2021 International Workshop on Continual\nSemi-Supervised Learning (CSSL-IJCAI), with the aim of raising field awareness\nabout this problem and mobilizing its effort in this direction. After a formal\ndefinition of continual semi-supervised learning and the appropriate training\nand testing protocols, the paper introduces two new benchmarks specifically\ndesigned to assess CSSL on two important computer vision tasks: activity\nrecognition and crowd counting. We describe the Continual Activity Recognition\n(CAR) and Continual Crowd Counting (CCC) challenges built upon those\nbenchmarks, the baseline models proposed for the challenges, and describe a\nsimple CSSL baseline which consists in applying batch self-training in temporal\nsessions, for a limited number of rounds. The results show that learning from\nunlabelled data streams is extremely challenging, and stimulate the search for\nmethods that can encode the dynamics of the data stream.",
    "descriptor": "",
    "authors": [
      "Ajmal Shahbaz",
      "Salman Khan",
      "Mohammad Asiful Hossain",
      "Vincenzo Lomonaco",
      "Kevin Cannons",
      "Zhan Xu",
      "Fabio Cuzzolin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14613"
  },
  {
    "id": "arXiv:2110.14615",
    "title": "Play to Grade: Testing Coding Games as Classifying Markov Decision  Process",
    "abstract": "Contemporary coding education often presents students with the task of\ndeveloping programs that have user interaction and complex dynamic systems,\nsuch as mouse based games. While pedagogically compelling, there are no\ncontemporary autonomous methods for providing feedback. Notably, interactive\nprograms are impossible to grade by traditional unit tests. In this paper we\nformalize the challenge of providing feedback to interactive programs as a task\nof classifying Markov Decision Processes (MDPs). Each student's program fully\nspecifies an MDP where the agent needs to operate and decide, under reasonable\ngeneralization, if the dynamics and reward model of the input MDP should be\ncategorized as correct or broken. We demonstrate that by designing a\ncooperative objective between an agent and an autoregressive model, we can use\nthe agent to sample differential trajectories from the input MDP that allows a\nclassifier to determine membership: Play to Grade. Our method enables an\nautomatic feedback system for interactive code assignments. We release a\ndataset of 711,274 anonymized student submissions to a single assignment with\nhand-coded bug labels to support future research.",
    "descriptor": "\nComments: NeurIPS 2021, 16 pages, 7 figures\n",
    "authors": [
      "Allen Nie",
      "Emma Brunskill",
      "Chris Piech"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14615"
  },
  {
    "id": "arXiv:2110.14621",
    "title": "Fairer LP-based Online Allocation",
    "abstract": "In this paper, we consider a Linear Program (LP)-based online resource\nallocation problem where a decision maker accepts or rejects incoming customer\nrequests irrevocably in order to maximize expected revenue given limited\nresources. At each time, a new order/customer/bid is revealed with a request of\nsome resource(s) and a reward. We consider a stochastic setting where all the\norders are i.i.d. sampled from an unknown distribution. Such formulation\ncontains many classic applications such as the canonical (quantity-based)\nnetwork revenue management problem and the Adwords problem. Specifically, we\nstudy the objective of providing fairness guarantees while maintaining low\nregret. Our definition of fairness is that a fair online algorithm should treat\nsimilar agents/customers similarly and the decision made for similar\nindividuals should be consistent over time. We define a fair offline solution\nas the analytic center of the offline optimal solution set, and propose a fair\nalgorithm that uses an interior-point LP solver and dynamically detects unfair\nresource spending. Our algorithm can control cumulative unfairness (the\ncumulative deviation from the online solutions to the offline fair solution) on\nthe scale of order $O(\\log(T))$, while maintaining the regret to be bounded\nwith dependency on $T$. Our approach do not formulate the fairness requirement\nas a constrain in the optimization instance, and instead we address the problem\nfrom the perspective of algorithm design. We get the desirable fairness\nguarantee without imposing any fairness constraint, and our regret result is\nstrong for the reason that we evaluate the regret by comparing to the original\nobjective value.",
    "descriptor": "\nComments: 45 pages. arXiv admin note: text overlap with arXiv:2101.11092\n",
    "authors": [
      "Guanting Chen",
      "Xiaocheng Li",
      "Yinyu Ye"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.14621"
  },
  {
    "id": "arXiv:2110.14626",
    "title": "Scalable Bayesian Network Structure Learning with Splines",
    "abstract": "A Bayesian Network (BN) is a probabilistic graphical model consisting of a\ndirected acyclic graph (DAG), where each node is a random variable represented\nas a function of its parents. We present a novel approach capable of learning\nthe global DAG structure of a BN and modelling linear and non-linear local\nrelationships between variables. We achieve this by a combination of feature\nselection to reduce the search space for local relationships, and extending the\nwidely used score-and-search approach to support modelling relationships\nbetween variables as Multivariate Adaptive Regression Splines (MARS). MARS are\npolynomial regression models represented as piecewise spline functions - this\nlets us model non-linear relationships without the risk of overfitting that a\nsingle polynomial regression model would bring. The combination allows us to\nlearn relationships in all bnlearn benchmark instances within minutes and\nenables us to scale to networks of over a thousand nodes",
    "descriptor": "",
    "authors": [
      "Charupriya Sharma",
      "Peter van Beek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14626"
  },
  {
    "id": "arXiv:2110.14629",
    "title": "The Importance of Open Data Policy to Tackle Pandemic in Latin America",
    "abstract": "Open Data Policies can provide transparency, impulse innovation and\ncitizenship participation. Access to the right data in right time can produce\nhuge benefits to population. But, in Latin America there is not enough interest\nfrom governments to promote and use properly. By the other hand, global\npandemic has caused many damages in different levels, i.e. Economy, Public\nHealth, Education, etc. The paper opens a discussion about the importance of\nOpen Data Policy to mitigate the impact of Covid-19 and overpass this problem.",
    "descriptor": "",
    "authors": [
      "Josimar Chire"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.14629"
  },
  {
    "id": "arXiv:2110.14631",
    "title": "Reed-Muller Codes Achieve Capacity on BMS Channels",
    "abstract": "This paper considers the performance of long Reed-Muller (RM) codes\ntransmitted over binary memoryless symmetric (BMS) channels under bitwise\nmaximum-a-posteriori decoding. Its main result is that the family of binary RM\ncodes achieves capacity on any BMS channel with respect to bit-error rate. This\nresolves a long-standing open problem that connects information theory and\nerror-correcting codes. In contrast with the earlier result for the binary\nerasure channel, the new proof does not rely on hypercontractivity. Instead, it\ncombines a nesting property of RM codes with new information inequalities\nrelating the generalized extrinsic information transfer function and the\nextrinsic minimum mean-squared error.",
    "descriptor": "\nComments: To be submitted to the IEEE Transactions on Information Theory\n",
    "authors": [
      "Galen Reeves",
      "Henry D. Pfister"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.14631"
  },
  {
    "id": "arXiv:2110.14632",
    "title": "Heterogeneous Effects of Software Patches in a Multiplayer Online Battle  Arena Game",
    "abstract": "The popularity of online gaming has grown dramatically, driven in part by\nstreaming and the billion-dollar e-sports industry. Online games regularly\nupdate their software to fix bugs, add functionality that improve the game's\nlook and feel, and change the game mechanics to keep the games fun and\nchallenging. An open question, however, is the impact of these changes on\nplayer performance and game balance, as well as how players adapt to these\nsudden changes. To address these questions, we use causal inference to measure\nthe impact of software patches to League of Legends, a popular team-based\nmultiplayer online game. We show that game patches have substantially different\nimpacts on players depending on their skill level and whether they take breaks\nbetween games. We find that the gap between good and bad players increases\nafter a patch, despite efforts to make gameplay more equal. Moreover, longer\nbetween-game breaks tend to improve player performance after patches. Overall,\nour results highlight the utility of causal inference, and specifically\nheterogeneous treatment effect estimation, as a tool to quantify the complex\nmechanisms of game balance and its interplay with players' performance.",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Yuzi He",
      "Christopher Tran",
      "Julie Jiang",
      "Keith Burghardt",
      "Emilio Ferrara",
      "Elena Zheleva",
      "Kristina Lerman"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.14632"
  },
  {
    "id": "arXiv:2110.14633",
    "title": "Similarity and Matching of Neural Network Representations",
    "abstract": "We employ a toolset -- dubbed Dr. Frankenstein -- to analyse the similarity\nof representations in deep neural networks. With this toolset, we aim to match\nthe activations on given layers of two trained neural networks by joining them\nwith a stitching layer. We demonstrate that the inner representations emerging\nin deep convolutional neural networks with the same architecture but different\ninitializations can be matched with a surprisingly high degree of accuracy even\nwith a single, affine stitching layer. We choose the stitching layer from\nseveral possible classes of linear transformations and investigate their\nperformance and properties. The task of matching representations is closely\nrelated to notions of similarity. Using this toolset, we also provide a novel\nviewpoint on the current line of research regarding similarity indices of\nneural network representations: the perspective of the performance on a task.",
    "descriptor": "\nComments: To appear in the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Adri\u00e1n Csisz\u00e1rik",
      "P\u00e9ter K\u0151r\u00f6si-Szab\u00f3",
      "\u00c1kos K. Matszangosz",
      "Gergely Papp",
      "D\u00e1niel Varga"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14633"
  },
  {
    "id": "arXiv:2110.13911",
    "title": "Modeling Category-Selective Cortical Regions with Topographic  Variational Autoencoders",
    "abstract": "Category-selectivity in the brain describes the observation that certain\nspatially localized areas of the cerebral cortex tend to respond robustly and\nselectively to stimuli from specific limited categories. One of the most well\nknown examples of category-selectivity is the Fusiform Face Area (FFA), an area\nof the inferior temporal cortex in primates which responds preferentially to\nimages of faces when compared with objects or other generic stimuli. In this\nwork, we leverage the newly introduced Topographic Variational Autoencoder to\nmodel of the emergence of such localized category-selectivity in an\nunsupervised manner. Experimentally, we demonstrate our model yields spatially\ndense neural clusters selective to faces, bodies, and places through visualized\nmaps of Cohen's d metric. We compare our model with related supervised\napproaches, namely the TDANN, and discuss both theoretical and empirical\nsimilarities. Finally, we show preliminary results suggesting that our model\nyields a nested spatial hierarchy of increasingly abstract categories,\nanalogous to observations from the human ventral temporal cortex.",
    "descriptor": "",
    "authors": [
      "T. Anderson Keller",
      "Qinghe Gao",
      "Max Welling"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13911"
  },
  {
    "id": "arXiv:2110.13969",
    "title": "Nonparametric Matrix Estimation with One-Sided Covariates",
    "abstract": "Consider the task of matrix estimation in which a dataset $X \\in\n\\mathbb{R}^{n\\times m}$ is observed with sparsity $p$, and we would like to\nestimate $\\mathbb{E}[X]$, where $\\mathbb{E}[X_{ui}] = f(\\alpha_u, \\beta_i)$ for\nsome Holder smooth function $f$. We consider the setting where the row\ncovariates $\\alpha$ are unobserved yet the column covariates $\\beta$ are\nobserved. We provide an algorithm and accompanying analysis which shows that\nour algorithm improves upon naively estimating each row separately when the\nnumber of rows is not too small. Furthermore when the matrix is moderately\nproportioned, our algorithm achieves the minimax optimal nonparametric rate of\nan oracle algorithm that knows the row covariates. In simulated experiments we\nshow our algorithm outperforms other baselines in low data regimes.",
    "descriptor": "",
    "authors": [
      "Christina Lee Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13969"
  },
  {
    "id": "arXiv:2110.13976",
    "title": "Biological learning in key-value memory networks",
    "abstract": "In neuroscience, classical Hopfield networks are the standard biologically\nplausible model of long-term memory, relying on Hebbian plasticity for storage\nand attractor dynamics for recall. In contrast, memory-augmented neural\nnetworks in machine learning commonly use a key-value mechanism to store and\nread out memories in a single step. Such augmented networks achieve impressive\nfeats of memory compared to traditional variants, yet their biological\nrelevance is unclear. We propose an implementation of basic key-value memory\nthat stores inputs using a combination of biologically plausible three-factor\nplasticity rules. The same rules are recovered when network parameters are\nmeta-learned. Our network performs on par with classical Hopfield networks on\nautoassociative memory tasks and can be naturally extended to continual recall,\nheteroassociative memory, and sequence learning. Our results suggest a\ncompelling alternative to the classical Hopfield network as a model of\nbiological long-term memory.",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Danil Tyulmankov",
      "Ching Fang",
      "Annapurna Vadaparty",
      "Guangyu Robert Yang"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.13976"
  },
  {
    "id": "arXiv:2110.14012",
    "title": "Graph Posterior Network: Bayesian Predictive Uncertainty for Node  Classification",
    "abstract": "The interdependence between nodes in graphs is key to improve class\npredictions on nodes and utilized in approaches like Label Propagation (LP) or\nin Graph Neural Networks (GNN). Nonetheless, uncertainty estimation for\nnon-independent node-level predictions is under-explored. In this work, we\nexplore uncertainty quantification for node classification in three ways: (1)\nWe derive three axioms explicitly characterizing the expected predictive\nuncertainty behavior in homophilic attributed graphs. (2) We propose a new\nmodel Graph Posterior Network (GPN) which explicitly performs Bayesian\nposterior updates for predictions on interdependent nodes. GPN provably obeys\nthe proposed axioms. (3) We extensively evaluate GPN and a strong set of\nbaselines on semi-supervised node classification including detection of\nanomalous features, and detection of left-out classes. GPN outperforms existing\napproaches for uncertainty estimation in the experiments.",
    "descriptor": "\nComments: Neurips 2021\n",
    "authors": [
      "Maximilian Stadler",
      "Bertrand Charpentier",
      "Simon Geisler",
      "Daniel Z\u00fcgner",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14012"
  },
  {
    "id": "arXiv:2110.14013",
    "title": "Deep Integrated Pipeline of Segmentation Leading to Classification for  Automated Detection of Breast Cancer from Breast Ultrasound Images",
    "abstract": "Breast cancer has become a symbol of tremendous concern in the modern world,\nas it is one of the major causes of cancer mortality worldwide. In this\nconcern, many people are frequently screening for breast cancer in order to be\nidentified early and avert mortality from the disease by receiving treatment.\nBreast Ultrasonography Images are frequently utilized by doctors to diagnose\nbreast cancer at an early stage. However, the complex artifacts and heavily\nnoised Breast Ultrasonography Images make detecting Breast Cancer a tough\nchallenge. Furthermore, the ever-increasing number of patients being screened\nfor Breast Cancer necessitates the use of automated Computer Aided Technology\nfor high accuracy diagnosis at a cheap cost and in a short period of time. The\ncurrent progress of Artificial Intelligence (AI) in the fields of Medical Image\nAnalysis and Health Care is a boon to humanity. In this study, we have proposed\na compact integrated automated pipelining framework which integrates\nultrasonography image preprocessing with Simple Linear Iterative Clustering\n(SLIC) to tackle the complex artifact of Breast Ultrasonography Images\ncomplementing semantic segmentation with Modified U-Net leading to Breast Tumor\nclassification with robust feature extraction using a transfer learning\napproach with pretrained VGG 16 model and densely connected neural network\narchitecture. The proposed automated pipeline can be effectively implemented to\nassist medical practitioners in making more accurate and timely diagnoses of\nbreast cancer.",
    "descriptor": "\nComments: Submitted To Journal (Under Review)\n",
    "authors": [
      "Muhammad Sakib Khan Inan",
      "Fahim Irfan Alam",
      "Rizwan Hasan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14013"
  },
  {
    "id": "arXiv:2110.14014",
    "title": "Measuring and Modeling Neighborhoods",
    "abstract": "With the availability of granular geographical data, social scientists are\nincreasingly interested in examining how residential neighborhoods are formed\nand how they influence attitudes and behavior. To facilitate such studies, we\ndevelop an easy-to-use online survey instrument that allows respondents to draw\ntheir neighborhoods on a map. We then propose a statistical model to analyze\nhow the characteristics of respondents, relevant local areas, and their\ninteractions shape subjective neighborhoods. The model also generates\nout-of-sample predictions of one's neighborhood given these observed\ncharacteristics. We illustrate the proposed methodology by conducting a survey\namong registered voters in Miami, New York City, and Phoenix. We find that\nacross these cities voters are more likely to include same-race and co-partisan\ncensus blocks in their neighborhoods. Net of other factors, White respondents\nare 6.1 to 16.9 percentage points more likely to include in their neighborhoods\na census block composed entirely of White residents compared to one with no\nWhite residents. Similarly, Democratic and Republican respondents are 8.6 to\n19.2 percentage points more likely to include an entirely co-partisan census\nblock compared to one consisting entirely of out-partisans. Co-partisanship\nexhibits a similar, independent, influence. We also show that our model\nprovides more accurate out-of-sample predictions than the standard\ndistance-based measures of neighborhoods. Open-source software is available for\nimplementing the proposed methodology.",
    "descriptor": "\nComments: 21 pages, 6 figures, and appendices\n",
    "authors": [
      "Cory McCartan",
      "Jacob R. Brown",
      "Kosuke Imai"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.14014"
  },
  {
    "id": "arXiv:2110.14022",
    "title": "Intelligent Meta-Imagers: From Compressed to Learned Sensing",
    "abstract": "Computational meta-imagers synergize metamaterial hardware with advanced\nsignal processing approaches such as compressed sensing. Recent advances in\nartificial intelligence (AI) are gradually reshaping the landscape of\nmeta-imaging. Most recent works use AI for data analysis, but some also use it\nto program the physical meta-hardware. The role of \"intelligence\" in the\nmeasurement process and its implications for critical metrics like latency are\noften not immediately clear. Here, we comprehensively review the evolution of\ncomputational meta-imaging from the earliest frequency-diverse compressive\nsystems to modern programmable intelligent meta-imagers. We introduce a clear\ntaxonomy in terms of the flow of task-relevant information that has direct\nlinks to information theory: compressive meta-imagers indiscriminately acquire\nall scene information in a task-agnostic measurement process that aims at a\nnear-isometric embedding; intelligent meta-imagers highlight task-relevant\ninformation in a task-aware measurement process that is purposefully\nnon-isometric. We provide explicit design tutorials for the integration of\nprogrammable meta-atoms as trainable physical weights into an intelligent\nend-to-end sensing pipeline. This merging of the physical world of metamaterial\nengineering and the digital world of AI enables the remarkable latency gains of\nintelligent meta-imagers. We further outline emerging opportunities for\ncognitive meta-imagers with reverberation-enhanced resolution and we point out\nhow the meta-imaging community can reap recent advances in the vibrant field of\nmetamaterial wave processors to reach the holy grail of low-energy ultra-fast\nall-analog intelligent meta-sensors.",
    "descriptor": "\nComments: 59 pages including 8 figures\n",
    "authors": [
      "Chlo\u00e9 Saigre-Tardif",
      "Rashid Faqiri",
      "Hanting Zhao",
      "Lianlin Li",
      "Philipp del Hougne"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2110.14022"
  },
  {
    "id": "arXiv:2110.14034",
    "title": "r-local sensing: Improved algorithm and applications",
    "abstract": "The unlabeled sensing problem is to solve a noisy linear system of equations\nunder unknown permutation of the measurements. We study a particular case of\nthe problem where the permutations are restricted to be r-local, i.e. the\npermutation matrix is block diagonal with r x r blocks. Assuming a Gaussian\nmeasurement matrix, we argue that the r-local permutation model is more\nchallenging compared to a recent sparse permutation model. We propose a\nproximal alternating minimization algorithm for the general unlabeled sensing\nproblem that provably converges to a first order stationary point. Applied to\nthe r-local model, we show that the resulting algorithm is efficient. We\nvalidate the algorithm on synthetic and real datasets. We also formulate the\n1-d unassigned distance geometry problem as an unlabeled sensing problem with a\nstructured measurement matrix.",
    "descriptor": "",
    "authors": [
      "Ahmed Ali Abbasi",
      "Abiy Tasissa",
      "Shuchin Aeron"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.14034"
  },
  {
    "id": "arXiv:2110.14047",
    "title": "Bounding Distance to Unsafe Sets with Convex Optimization",
    "abstract": "This work proposes an algorithm to bound the minimum distance between points\non trajectories of a dynamical system and points on an unsafe set. Prior work\non certifying safety of trajectories includes barrier and density methods,\nwhich do not provide a margin of proximity to the unsafe set in terms of\ndistance. The distance estimation problem is relaxed to a Monge-Kantorovich\ntype optimal transport problem based on existing occupation-measure methods of\npeak estimation. Specialized programs may be developed for polyhedral norm\ndistances (e.g. L1 and Linfinity) and for scenarios where a shape is traveling\nalong trajectories (e.g. rigid body motion). The distance estimation problem\nwill be correlatively sparse when the distance objective is separable.",
    "descriptor": "\nComments: 23 pages, 17 figures\n",
    "authors": [
      "Jared Miller",
      "Mario Sznaier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14047"
  },
  {
    "id": "arXiv:2110.14072",
    "title": "Efficient solvers for Armijo's backtracking problem",
    "abstract": "Backtracking is an inexact line search procedure that selects the first value\nin a sequence $x_0, x_0\\beta, x_0\\beta^2...$ that satisfies $g(x)\\leq 0$ on\n$\\mathbb{R}_+$ with $g(x)\\leq 0$ iff $x\\leq x^*$. This procedure is widely used\nin descent direction optimization algorithms with Armijo-type conditions. It\nboth returns an estimate in $(\\beta x^*,x^*]$ and enjoys an upper-bound $\\lceil\n\\log_{\\beta} \\epsilon/x_0 \\rceil$ on the number of function evaluations to\nterminate, with $\\epsilon$ a lower bound on $x^*$. The basic bracketing\nmechanism employed in several root-searching methods is adapted here for the\npurpose of performing inexact line searches, leading to a new class of inexact\nline search procedures. The traditional bisection algorithm for root-searching\nis transposed into a very simple method that completes the same inexact line\nsearch in at most $\\lceil \\log_2 \\log_{\\beta} \\epsilon/x_0 \\rceil$ function\nevaluations. A recent bracketing algorithm for root-searching which presents\nboth minmax function evaluation cost (as the bisection algorithm) and\nsuperlinear convergence is also transposed, asymptotically requiring $\\sim \\log\n\\log \\log \\epsilon/x_0 $ function evaluations for sufficiently smooth\nfunctions. Other bracketing algorithms for root-searching can be adapted in the\nsame way. Numerical experiments suggest time savings of 50\\% to 80\\% in each\ncall to the inexact search procedure.",
    "descriptor": "\nComments: Keywords: inexact line search, Armijo-type methods, backtracking, bracketing algorithms, geometric bisection\n",
    "authors": [
      "Ivo Fagundes David de Oliveira",
      "Ricardo Hiroshi Caldeira Takahashi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14072"
  },
  {
    "id": "arXiv:2110.14088",
    "title": "Shear-Driven Flow in an Elliptical Enclosure Generated by an Inner  Rotating Circular Cylinder",
    "abstract": "Shear-driven flow between a rotating cylinder and a stationary elliptical\nenclosure is studied in this paper. The two-dimensional time-dependent Navier\nStokes equations are solved using a meshless method where interpolations are\ndone with Polyharmonic Spline Radial Basis Functions (PHS-RBF). The fluid flow\nis analyzed for various aspect ratios of the ellipse and eccentric placements\nof the inner cylinder. Contour plots of vorticity with streamlines, plots of\nnon-dimensional torque, and the angle of eye of the primary vortex are\npresented in the paper for Reynolds numbers between 200 to 2000. Formation of\nMoffat like vortices in the wide-gap region of the model is observed and some\nbenchmark data are provided for various cases that are simulated.",
    "descriptor": "",
    "authors": [
      "Akash Unnikrishnan",
      "Shantanu Shahane",
      "Vinod Narayan",
      "Surya Pratap Vanka"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14088"
  },
  {
    "id": "arXiv:2110.14099",
    "title": "Tight Concentrations and Confidence Sequences from the Regret of  Universal Portfolio",
    "abstract": "A classic problem in statistics is the estimation of the expectation of\nrandom variables from samples. This gives rise to the tightly connected\nproblems of deriving concentration inequalities and confidence sequences, that\nis confidence intervals that hold uniformly over time. Jun and Orabona\n[COLT'19] have shown how to easily convert the regret guarantee of an online\nbetting algorithm into a time-uniform concentration inequality. Here, we show\nthat we can go even further: We show that the regret of a minimax betting\nalgorithm gives rise to a new implicit empirical time-uniform concentration. In\nparticular, we use a new data-dependent regret guarantee of the universal\nportfolio algorithm. We then show how to invert the new concentration in two\ndifferent ways: in an exact way with a numerical algorithm and symbolically in\nan approximate way. Finally, we show empirically that our algorithms have\nstate-of-the-art performance in terms of the width of the confidence sequences\nup to a moderately large amount of samples. In particular, our numerically\nobtained confidence sequences are never vacuous, even with a single sample.",
    "descriptor": "",
    "authors": [
      "Francesco Orabona",
      "Kwang-Sung Jun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2110.14099"
  },
  {
    "id": "arXiv:2110.14115",
    "title": "Metrics of research impact in astronomy: Predicting later impact from  metrics measured 10-15 years after the PhD",
    "abstract": "This paper calibrates how metrics derivable from the SAO/NASA Astrophysics\nData System can be used to estimate the future impact of astronomy research\ncareers and thereby to inform decisions on resource allocation such as job\nhires and tenure decisions. Three metrics are used, citations of refereed\npapers, citations of all publications normalized by the numbers of co-authors,\nand citations of all first-author papers. Each is individually calibrated as an\nimpact predictor in the book Kormendy (2020), \"Metrics of Research Impact in\nAstronomy\" (Publ Astron Soc Pac, San Francisco). How this is done is reviewed\nin the first half of this paper. Then, I show that averaging results from three\nmetrics produces more accurate predictions. Average prediction machines are\nconstructed for different cohorts of 1990-2007 PhDs and used to postdict 2017\nimpact from metrics measured 10, 12, and 15 years after the PhD. The time span\nover which prediction is made ranges from 0 years for 2007 PhDs to 17 years for\n1990 PhDs using metrics measured 10 years after the PhD. Calibration is based\non perceived 2017 impact as voted by 22 experienced astronomers for 510 faculty\nmembers at 17 highly-ranked university astronomy departments world-wide.\nPrediction machinery reproduces voted impact estimates with an RMS uncertainty\nof 1/8 of the dynamic range for people in the study sample. The aim of this\nwork is to lend some of the rigor that is normally used in scientific research\nto the difficult and subjective job of judging people's careers.",
    "descriptor": "\nComments: 11 pages, 8 postscript figures, 5 tables accepted for publication in Proceedings of the National Academy of Sciences\n",
    "authors": [
      "John Kormendy"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2110.14115"
  },
  {
    "id": "arXiv:2110.14116",
    "title": "Data-driven decomposition of brain dynamics with principal component  analysis in different types of head impacts",
    "abstract": "Strain and strain rate are effective traumatic brain injury predictors.\nKinematics-based models estimating these metrics suffer from significant\ndifferent distributions of both kinematics and the injury metrics across head\nimpact types. To address this, previous studies focus on the kinematics but not\nthe injury metrics. We have previously shown the kinematic features vary\nlargely across head impact types, resulting in different patterns of brain\ndeformation. This study analyzes the spatial distribution of brain deformation\nand applies principal component analysis (PCA) to extract the representative\npatterns of injury metrics (maximum principal strain (MPS), MPS rate (MPSR) and\nMPSXMPSR) in four impact types (simulation, football, mixed martial arts and\ncar crashes). We apply PCA to decompose the patterns of the injury metrics for\nall impacts in each impact type, and investigate the distributions among brain\nregions using the first principal component (PC1). Furthermore, we developed a\ndeep learning head model (DLHM) to predict PC1 and then inverse-transform to\npredict for all brain elements. PC1 explained >80% variance on the datasets.\nBased on PC1 coefficients, the corpus callosum and midbrain exhibit high\nvariance on all datasets. We found MPSXMPSR the most sensitive metric on which\nthe top 5% of severe impacts further deviates from the mean and there is a\nhigher variance among the severe impacts. Finally, the DLHM reached mean\nabsolute errors of <0.018 for MPS, <3.7 (1/s) for MPSR and <1.1 (1/s) for\nMPSXMPSR, much smaller than the injury thresholds. The brain injury metric in a\ndataset can be decomposed into mean components and PC1 with high explained\nvariance. The brain dynamics decomposition enables better interpretation of the\npatterns in brain injury metrics and the sensitivity of brain injury metrics\nacross impact types. The decomposition also reduces the dimensionality of DLHM.",
    "descriptor": "",
    "authors": [
      "Xianghao Zhan",
      "Yuzhe Liu",
      "Nicholas J. Cecchi",
      "Olivier Gevaert",
      "Michael M. Zeineh",
      "Gerald A. Grant",
      "David B. Camarillo"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2110.14116"
  },
  {
    "id": "arXiv:2110.14121",
    "title": "On Computing the Hyperparameter of Extreme Learning Machines: Algorithm  and Application to Computational PDEs, and Comparison with Classical and  High-Order Finite Elements",
    "abstract": "We consider the use of extreme learning machines (ELM) for computational\npartial differential equations (PDE). In ELM the hidden-layer coefficients in\nthe neural network are assigned to random values generated on $[-R_m,R_m]$ and\nfixed, where $R_m$ is a user-provided constant, and the output-layer\ncoefficients are trained by a linear or nonlinear least squares computation. We\npresent a method for computing the optimal value of $R_m$ based on the\ndifferential evolution algorithm. The presented method enables us to illuminate\nthe characteristics of the optimal $R_m$ for two types of ELM configurations:\n(i) Single-Rm-ELM, in which a single $R_m$ is used for generating the random\ncoefficients in all the hidden layers, and (ii) Multi-Rm-ELM, in which multiple\n$R_m$ constants are involved with each used for generating the random\ncoefficients of a different hidden layer. We adopt the optimal $R_m$ from this\nmethod and also incorporate other improvements into the ELM implementation. In\nparticular, here we compute all the differential operators involving the output\nfields of the last hidden layer by a forward-mode auto-differentiation, as\nopposed to the reverse-mode auto-differentiation in a previous work. These\nimprovements significantly reduce the network training time and enhance the ELM\nperformance. We systematically compare the computational performance of the\ncurrent improved ELM with that of the finite element method (FEM), both the\nclassical second-order FEM and the high-order FEM with Lagrange elements of\nhigher degrees, for solving a number of linear and nonlinear PDEs. It is shown\nthat the current improved ELM far outperforms the classical FEM. Its\ncomputational performance is comparable to that of the high-order FEM for\nsmaller problem sizes, and for larger problem sizes the ELM markedly\noutperforms the high-order FEM.",
    "descriptor": "\nComments: 46 pages, 35 figures\n",
    "authors": [
      "Suchuan Dong",
      "Jielin Yang"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2110.14121"
  },
  {
    "id": "arXiv:2110.14122",
    "title": "Data-Driven Representations for Testing Independence: Modeling, Analysis  and Connection with Mutual Information Estimation",
    "abstract": "This work addresses testing the independence of two continuous and\nfinite-dimensional random variables from the design of a data-driven partition.\nThe empirical log-likelihood statistic is adopted to approximate the sufficient\nstatistics of an oracle test against independence (that knows the two\nhypotheses). It is shown that approximating the sufficient statistics of the\noracle test offers a learning criterion for designing a data-driven partition\nthat connects with the problem of mutual information estimation. Applying these\nideas in the context of a data-dependent tree-structured partition (TSP), we\nderive conditions on the TSP's parameters to achieve a strongly consistent\ndistribution-free test of independence over the family of probabilities\nequipped with a density. Complementing this result, we present finite-length\nresults that show our TSP scheme's capacity to detect the scenario of\nindependence structurally with the data-driven partition as well as new\nsampling complexity bounds for this detection. Finally, some experimental\nanalyses provide evidence regarding our scheme's advantage for testing\nindependence compared with some strategies that do not use data-driven\nrepresentations.",
    "descriptor": "",
    "authors": [
      "Mauricio E. Gonzalez",
      "Jorge F. Silva",
      "Miguel Videla",
      "Marcos E. Orchard"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14122"
  },
  {
    "id": "arXiv:2110.14127",
    "title": "Constrained Optimization Involving Nonconvex $\\ell_p$ Norms: Optimality  Conditions, Algorithm and Convergence",
    "abstract": "This paper investigates the optimality conditions for characterizing the\nlocal minimizers of the constrained optimization problems involving an $\\ell_p$\nnorm ($0<p<1$) of the variables, which may appear in either the objective or\nthe constraint. This kind of problems have strong applicability to a wide range\nof areas since usually the $\\ell_p$ norm can promote sparse solutions. However,\nthe nonsmooth and non-Lipschtiz nature of the $\\ell_p$ norm often cause these\nproblems difficult to analyze and solve. We provide the calculation of the\nsubgradients of the $\\ell_p$ norm and the normal cones of the $\\ell_p$ ball.\nFor both problems, we derive the first-order necessary conditions under various\nconstraint qualifications. We also derive the sequential optimality conditions\nfor both problems and study the conditions under which these conditions imply\nthe first-order necessary conditions. We point out that the sequential\noptimality conditions can be easily satisfied for iteratively reweighted\nalgorithms and show that the global convergence can be easily derived using\nsequential optimality conditions.",
    "descriptor": "",
    "authors": [
      "Hao Wang",
      "Yining Gao",
      "Jiashan Wang",
      "Hongying Liu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14127"
  },
  {
    "id": "arXiv:2110.14139",
    "title": "Closing the Gap Between Time-Domain Multi-Channel Speech Enhancement on  Real and Simulation Conditions",
    "abstract": "The deep learning based time-domain models, e.g. Conv-TasNet, have shown\ngreat potential in both single-channel and multi-channel speech enhancement.\nHowever, many experiments on the time-domain speech enhancement model are done\nin simulated conditions, and it is not well studied whether the good\nperformance can generalize to real-world scenarios. In this paper, we aim to\nprovide an insightful investigation of applying multi-channel Conv-TasNet based\nspeech enhancement to both simulation and real data. Our preliminary\nexperiments show a large performance gap between the two conditions in terms of\nthe ASR performance. Several approaches are applied to close this gap,\nincluding the integration of multi-channel Conv-TasNet into the beamforming\nmodel with various strategies, and the joint training of speech enhancement and\nspeech recognition models. Our experiments on the CHiME-4 corpus show that our\nproposed approaches can greatly reduce the speech recognition performance\ndiscrepancy between simulation and real data, while preserving the strong\nspeech enhancement capability in the frontend.",
    "descriptor": "\nComments: 5 pages, 3 figures, accepted by IEEE WASPAA 2021\n",
    "authors": [
      "Wangyou Zhang",
      "Jing Shi",
      "Chenda Li",
      "Shinji Watanabe",
      "Yanmin Qian"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.14139"
  },
  {
    "id": "arXiv:2110.14142",
    "title": "Separating Long-Form Speech with Group-Wise Permutation Invariant  Training",
    "abstract": "Multi-talker conversational speech processing has drawn many interests for\nvarious applications such as meeting transcription. Speech separation is often\nrequired to handle overlapped speech that is commonly observed in conversation.\nAlthough the existing utterancelevel permutation invariant training-based\ncontinuous speech separation approach has proven to be effective in various\nconditions, it lacks the ability to leverage the long-span relationship of\nutterances and is computationally inefficient due to the highly overlapped\nsliding windows. To overcome these drawbacks, we propose a novel training\nscheme named Group-PIT, which allows direct training of the speech separation\nmodels on the long-form speech with a low computational cost for label\nassignment. Two different speech separation approaches with Group-PIT are\nexplored, including direct long-span speech separation and short-span speech\nseparation with long-span tracking. The experiments on the simulated\nmeeting-style data demonstrate the effectiveness of our proposed approaches,\nespecially in dealing with a very long speech input.",
    "descriptor": "\nComments: 5 pages, 3 figures, 3 tables, submitted to IEEE ICASSP 2022\n",
    "authors": [
      "Wangyou Zhang",
      "Zhuo Chen",
      "Naoyuki Kanda",
      "Shujie Liu",
      "Jinyu Li",
      "Sefik Emre Eskimez",
      "Takuya Yoshioka",
      "Xiong Xiao",
      "Zhong Meng",
      "Yanmin Qian",
      "Furu Wei"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.14142"
  },
  {
    "id": "arXiv:2110.14144",
    "title": "Physically Explainable CNN for SAR Image Classification",
    "abstract": "Integrating the special electromagnetic characteristics of Synthetic Aperture\nRadar (SAR) in deep neural networks is essential in order to enhance the\nexplainability and physics awareness of deep learning. In this paper, we\nfirstly propose a novel physics guided and injected neural network for SAR\nimage classification, which is mainly guided by explainable physics models and\ncan be learned with very limited labeled data. The proposed framework comprises\nthree parts: (1) generating physics guided signals using existing explainable\nmodels, (2) learning physics-aware features with physics guided network, and\n(3) injecting the physics-aware features adaptively to the conventional\nclassification deep learning model for prediction. The prior knowledge,\nphysical scattering characteristic of SAR in this paper, is injected into the\ndeep neural network in the form of physics-aware features which is more\nconducive to understanding the semantic labels of SAR image patches. A hybrid\nImage-Physics SAR dataset format is proposed, and both Sentinel-1 and Gaofen-3\nSAR data are taken for evaluation. The experimental results show that our\nproposed method substantially improve the classification performance compared\nwith the counterpart data-driven CNN. Moreover, the guidance of explainable\nphysics signals leads to explainability of physics-aware features and the\nphysics consistency of features are also preserved in the predictions. We deem\nthe proposed method would promote the development of physically explainable\ndeep learning in SAR image interpretation field.",
    "descriptor": "",
    "authors": [
      "Zhongling Huang",
      "Xiwen Yao",
      "Corneliu Octavian Dumitru",
      "Mihai Datcu",
      "Junwei Han"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14144"
  },
  {
    "id": "arXiv:2110.14148",
    "title": "Uniform Concentration Bounds toward a Unified Framework for Robust  Clustering",
    "abstract": "Recent advances in center-based clustering continue to improve upon the\ndrawbacks of Lloyd's celebrated $k$-means algorithm over $60$ years after its\nintroduction. Various methods seek to address poor local minima, sensitivity to\noutliers, and data that are not well-suited to Euclidean measures of fit, but\nmany are supported largely empirically. Moreover, combining such approaches in\na piecemeal manner can result in ad hoc methods, and the limited theoretical\nresults supporting each individual contribution may no longer hold. Toward\naddressing these issues in a principled way, this paper proposes a cohesive\nrobust framework for center-based clustering under a general class of\ndissimilarity measures. In particular, we present a rigorous theoretical\ntreatment within a Median-of-Means (MoM) estimation framework, showing that it\nsubsumes several popular $k$-means variants. In addition to unifying existing\nmethods, we derive uniform concentration bounds that complete their analyses,\nand bridge these results to the MoM framework via Dudley's chaining arguments.\nImportantly, we neither require any assumptions on the distribution of the\noutlying observations nor on the relative number of observations $n$ to\nfeatures $p$. We establish strong consistency and an error rate of\n$O(n^{-1/2})$ under mild conditions, surpassing the best-known results in the\nliterature. The methods are empirically validated thoroughly on real and\nsynthetic datasets.",
    "descriptor": "\nComments: To appear (spotlight) in the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS), 2021\n",
    "authors": [
      "Debolina Paul",
      "Saptarshi Chakraborty",
      "Swagatam Das",
      "Jason Xu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2110.14148"
  },
  {
    "id": "arXiv:2110.14174",
    "title": "On the Dynamics of the Tavis-Cummings Model",
    "abstract": "The purpose of this paper is to present a comprehensive study of the\nTavis-Cummings model from a system-theoretic perspective. A typical form of the\nTavis-Cummings model is composed of an ensemble of non-interacting two-level\nsystems that are all coupled to a common cavity resonator. A Tavis-Cummings\nsystem exhibits superradiance phenomenon, for example an ensemble of $N$\ntwo-level atoms act collectively as a giant atom which decays $N$ times as fast\nas an individual atom. An associated quantum linear passive system is proposed,\nwhose canonical form reveals typical features of the Tavis-Cummings model,\nincluding $\\sqrt{N}$- scaling, dark states, bright states, single-excitation\nsuperradiant and subradiant states. The passivity of this linear system is\nrelated to the vacuum Rabi mode splitting phenomenon in a Tavis-Cummings\nsystem. On the basis of the linear model, an analytic form is presented for the\nsteady-state output single-photon state of the Tavis-Cummings model driven by a\nsingle-photon state. Master equations are used to study the excitation\nproperties of the Tavis-Cummings model in the multi-excitation scenario.\nFinally, in terms of the transition matrix for a linear time-varying system, a\ncomputational framework is proposed for calculating the state of the\nTavis-Cummings model, which is applicable to the multi-excitation case.",
    "descriptor": "\nComments: 41 pages, 9 figures\n",
    "authors": [
      "Zhiyuan Dong",
      "Guofeng Zhang",
      "Ai-Guo Wu",
      "Re-Bing Wu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14174"
  },
  {
    "id": "arXiv:2110.14177",
    "title": "Federated Linear Contextual Bandits",
    "abstract": "This paper presents a novel federated linear contextual bandits model, where\nindividual clients face different $K$-armed stochastic bandits coupled through\ncommon global parameters. By leveraging the geometric structure of the linear\nrewards, a collaborative algorithm called Fed-PE is proposed to cope with the\nheterogeneity across clients without exchanging local feature vectors or raw\ndata. Fed-PE relies on a novel multi-client G-optimal design, and achieves\nnear-optimal regrets for both disjoint and shared parameter cases with\nlogarithmic communication costs. In addition, a new concept called\ncollinearly-dependent policies is introduced, based on which a tight minimax\nregret lower bound for the disjoint parameter case is derived. Experiments\ndemonstrate the effectiveness of the proposed algorithms on both synthetic and\nreal-world datasets.",
    "descriptor": "",
    "authors": [
      "Ruiquan Huang",
      "Weiqiang Wu",
      "Jing Yang",
      "Cong Shen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14177"
  },
  {
    "id": "arXiv:2110.14181",
    "title": "QU-net++: Image Quality Detection Framework for Segmentation of 3D  Medical Image Stacks",
    "abstract": "Automated segmentation of pathological regions of interest has been shown to\naid prognosis and follow up treatment. However, accurate pathological\nsegmentations require high quality of annotated data that can be both cost and\ntime intensive to generate. In this work, we propose an automated two-step\nmethod that evaluates the quality of medical images from 3D image stacks using\na U-net++ model, such that images that can aid further training of the U-net++\nmodel can be detected based on the disagreement in segmentations produced from\nthe final two layers. Images thus detected can then be used to further fine\ntune the U-net++ model for semantic segmentation. The proposed QU-net++ model\nisolates around 10\\% of images per 3D stack and can scale across imaging\nmodalities to segment cysts in OCT images and ground glass opacity in Lung CT\nimages with Dice cores in the range 0.56-0.72. Thus, the proposed method can be\napplied for multi-modal binary segmentation of pathology.",
    "descriptor": "\nComments: 5 pages, 7 figures, 1 Table\n",
    "authors": [
      "Sohini Roychowdhury"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14181"
  },
  {
    "id": "arXiv:2110.14206",
    "title": "The Quantum Approximate Optimization Algorithm at High Depth for MaxCut  on Large-Girth Regular Graphs and the Sherrington-Kirkpatrick Model",
    "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) finds approximate\nsolutions to combinatorial optimization problems. Its performance monotonically\nimproves with its depth $p$. We apply the QAOA to MaxCut on large-girth\n$D$-regular graphs. We give an iterative formula to evaluate performance for\nany $D$ at any depth $p$. Looking at random $D$-regular graphs, at optimal\nparameters and as $D$ goes to infinity, we find that the $p=11$ QAOA beats all\nclassical algorithms (known to the authors) that are free of unproven\nconjectures. While the iterative formula for these $D$-regular graphs is\nderived by looking at a single tree subgraph, we prove that it also gives the\nensemble-averaged performance of the QAOA on the Sherrington-Kirkpatrick (SK)\nmodel. Our iteration is a compact procedure, but its computational complexity\ngrows as $O(p^2 4^p)$. This iteration is more efficient than the previous\nprocedure for analyzing QAOA performance on the SK model, and we are able to\nnumerically go to $p=20$. Encouraged by our findings, we make the optimistic\nconjecture that the QAOA, as $p$ goes to infinity, will achieve the Parisi\nvalue. We analyze the performance of the quantum algorithm, but one needs to\nrun it on a quantum computer to produce a string with the guaranteed\nperformance.",
    "descriptor": "\nComments: 33 pages, 4 figures, 5 tables\n",
    "authors": [
      "Joao Basso",
      "Edward Farhi",
      "Kunal Marwaha",
      "Benjamin Villalonga",
      "Leo Zhou"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14206"
  },
  {
    "id": "arXiv:2110.14211",
    "title": "Beamforming Feedback-based Model-driven Angle of Departure Estimation  Toward Firmware-Agnostic WiFi Sensing",
    "abstract": "This paper proves that the angle of departure (AoD) estimation using the\nmultiple signal classification (MUSIC) with only WiFi control frames for\nbeamforming feedback (BFF), defined in IEEE 802.11ac/ax, is possible. Although\nchannel state information (CSI) enables model-driven AoD estimation, most\nBFF-based sensing techniques are data-driven because they only contain the\nright singular vectors of CSI and subcarrier-averaged stream gain.\nSpecifically, we find that right singular vectors with a subcarrier-averaged\nstream gain of zero have the same role as the noise subspace vectors in the\nCSI-based MUSIC algorithm. Numerical evaluations confirm that the proposed\nBFF-based MUSIC successfully estimates the AoDs and gains for all propagation\npaths. Meanwhile, this result implies a potential privacy risk; a malicious\nsniffer can carry out AoD estimation only with unencrypted BFF frames.",
    "descriptor": "\nComments: Submitted to IEEE wireless communications letter\n",
    "authors": [
      "Sohei Itahara",
      "Takayuki Nishio",
      "Koji Yamamoto"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.14211"
  },
  {
    "id": "arXiv:2110.14258",
    "title": "Scattering and uniform in time error estimates for splitting method in  NLS",
    "abstract": "We consider the nonlinear Schr{\\\"o}dinger equation with a defocusing\nnonlinearity which is mass-(super)critical and energy-subcritical. We prove\nuniform in time error estimates for the Lie-Trotter time splitting\ndiscretization. This uniformity in time is obtained thanks to a vectorfield\nwhich provides time decay estimates for the exact and numerical solutions. This\nvectorfield is classical in scattering theory, and requires several technical\nmodifications compared to previous error estimates for splitting methods.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "R\u00e9mi Carles",
      "Chunmei Su"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14258"
  },
  {
    "id": "arXiv:2110.14262",
    "title": "On derivations of evolving surface Navier-Stokes equations",
    "abstract": "In recent literature several derivations of incompressible Navier-Stokes type\nequations that model the dynamics of an evolving fluidic surface have been\npresented. These derivations differ in the physical principles used in the\nmodeling approach and in the coordinate systems in which the resulting\nequations are represented. This paper has overview character in the sense that\nwe put five different derivations of surface Navier-Stokes equations into one\nframework. This then allows a systematic comparison of the resulting surface\nNavier-Stokes equations and shows that some, but not all, of the resulting\nmodels are the same. Furthermore, based on a natural splitting approach in\ntangential and normal components of the velocity we show that all five\nderivations that we consider yield the same tangential surface Navier-Stokes\nequations.",
    "descriptor": "",
    "authors": [
      "Philip Brandner",
      "Arnold Reusken",
      "Paul Schwering"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Differential Geometry (math.DG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14262"
  },
  {
    "id": "arXiv:2110.14279",
    "title": "SiWa: See into Walls via Deep UWB Radar",
    "abstract": "Being able to see into walls is crucial for diagnostics of building health;\nit enables inspections of wall structure without undermining the structural\nintegrity. However, existing sensing devices do not seem to offer a full\ncapability in mapping the in-wall structure while identifying their status\n(e.g., seepage and corrosion). In this paper, we design and implement SiWa as a\nlow-cost and portable system for wall inspections. Built upon a customized\nIR-UWB radar, SiWa scans a wall as a user swipes its probe along the wall\nsurface; it then analyzes the reflected signals to synthesize an image and also\nto identify the material status. Although conventional schemes exist to handle\nthese problems individually, they require troublesome calibrations that largely\nprevent them from practical adoptions. To this end, we equip SiWa with a deep\nlearning pipeline to parse the rich sensory data. With an ingenious\nconstruction and innovative training, the deep learning modules perform\nstructural imaging and the subsequent analysis on material status, without the\nneed for parameter tuning and calibrations. We build SiWa as a prototype and\nevaluate its performance via extensive experiments and field studies; results\nconfirm that SiWa accurately maps in-wall structures, identifies their\nmaterials, and detects possible failures, suggesting a promising solution for\ndiagnosing building health with lower effort and cost.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Tianyue Zheng",
      "Zhe Chen",
      "Jun Luo",
      "Lin Ke",
      "Chaoyang Zhao",
      "Yaowen Yang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.14279"
  },
  {
    "id": "arXiv:2110.14317",
    "title": "Ask \"Who\", Not \"What\": Bitcoin Volatility Forecasting with Twitter Data",
    "abstract": "Understanding the variations in trading price (volatility), and its response\nto external information is a well-studied topic in finance. In this study, we\nfocus on volatility predictions for a relatively new asset class of\ncryptocurrencies (in particular, Bitcoin) using deep learning representations\nof public social media data from Twitter. For the field work, we extracted\nsemantic information and user interaction statistics from over 30 million\nBitcoin-related tweets, in conjunction with 15-minute intraday price data over\na 144-day horizon. Using this data, we built several deep learning\narchitectures that utilized a combination of the gathered information. For all\narchitectures, we conducted ablation studies to assess the influence of each\ncomponent and feature set in our model. We found statistical evidences for the\nhypotheses that: (i) temporal convolutional networks perform significantly\nbetter than both autoregressive and other deep learning-based models in the\nliterature, and (ii) the tweet author meta-information, even detached from the\ntweet itself, is a better predictor than the semantic content and tweet volume\nstatistics.",
    "descriptor": "\nComments: 11 pages, 11 figures, 6 tables\n",
    "authors": [
      "M. Eren Akbiyik",
      "Mert Erkul",
      "Killian Kaempf",
      "Vaiva Vasiliauskaite",
      "Nino Antulov-Fantulin"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.14317"
  },
  {
    "id": "arXiv:2110.14321",
    "title": "Almost periodic functions and an analytical method of solving the number  partitioning problem",
    "abstract": "In the present paper, we study the limit sets of the almost periodic\nfunctions $f(x)$. It is interesting that the values $r=\\inf|f(x)|$ and\n$R=\\sup|f(x)|$ may be expressed in the exact form. We show that the ring $r\\leq\n|z|\\leq R$ is the limit set of the almost periodic function $f(x)$ (under some\nnatural conditions on $f$). The exact expression for $r$ coincides with the\nwell known partition problem formula and gives a new analytical method of\nsolving the corresponding partition problem. Several interesting examples are\nconsidered. For instance, in the case of the five numbers, the well-known\nKarmarkar--Karp algorithm gives the value $m=2$ as the solution of the\npartition problem in our example, and our method gives the correct answer\n$m=0.$ The figures presented in Appendix illustrate our results.",
    "descriptor": "",
    "authors": [
      "Lev Sakhnovich"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Combinatorics (math.CO)",
      "Logic (math.LO)",
      "Numerical Analysis (math.NA)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2110.14321"
  },
  {
    "id": "arXiv:2110.14329",
    "title": "Feature selection revisited in the single-cell era",
    "abstract": "Feature selection techniques are essential for high-dimensional data\nanalysis. In the last two decades, their popularity has been fuelled by the\nincreasing availability of high-throughput biomolecular data where\nhigh-dimensionality is a common data property. Recent advances in\nbiotechnologies enable global profiling of various molecular and cellular\nfeatures at single-cell resolution, resulting in large-scale datasets with\nincreased complexity. These technological developments have led to a resurgence\nin feature selection research and application in the single-cell field. Here,\nwe revisit feature selection techniques and summarise recent developments. We\nreview their versatile application to a range of single-cell data types\nincluding those generated from traditional cytometry and imaging technologies\nand the latest array of single-cell omics technologies. We highlight some of\nthe challenges and future directions on which feature selection could have a\nsignificant impact. Finally, we consider the scalability and make general\nrecommendations on the utility of each type of feature selection method. We\nhope this review serves as a reference point to stimulate future research and\napplication of feature selection in the single-cell era.",
    "descriptor": "",
    "authors": [
      "Pengyi Yang",
      "Hao Huang",
      "Chunlei Liu"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14329"
  },
  {
    "id": "arXiv:2110.14332",
    "title": "Rainbow cycles for families of matchings",
    "abstract": "Given a graph G and a coloring of its edges, a subgraph of G is called\nrainbow if its edges have distinct colors. The rainbow girth of an edge\ncoloring of G is the minimum length of a rainbow cycle in G. A generalization\nof the famous Caccetta-Haggkvist conjecture (CHC), proposed by the first\nauthor, is that if G has n vertices, G is n-edge-colored and the size of every\ncolor class is k, then the rainbow girth is at most \\lceil \\frac{n}{k} \\rceil.\nIn the only known example showing sharpness of this conjecture, that stems from\nan example for the sharpness of CHC, the color classes are stars. This suggests\nthat in the antipodal case to stars, namely matchings, the result can be\nimproved. Indeed, we show that the rainbow girth of n matchings of size at\nleast 2 is O(\\log n), as compared with the general bound of \\lceil \\frac{n}{2}\n\\rceil.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Ron Aharoni",
      "He Guo"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2110.14332"
  },
  {
    "id": "arXiv:2110.14334",
    "title": "Spectral splitting method for nonlinear Schr\u00f6dinger equations with  quadratic potential",
    "abstract": "In this paper we propose a modified Lie-type spectral splitting approximation\nwhere the external potential is of quadratic type. It is proved that we can\napproximate the solution to a nonlinear Schroedinger equation by solving the\nlinear problem and treating the nonlinear term separately, with a rigorous\nestimate of the remainder term. Furthermore, we show by means of numerical\nexperiments that such a modified approximation is more efficient than the\nstandard one.",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Andrea Sacchetti"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.14334"
  },
  {
    "id": "arXiv:2110.14346",
    "title": "A Scalable Inference Method For Large Dynamic Economic Systems",
    "abstract": "The nature of available economic data has changed fundamentally in the last\ndecade due to the economy's digitisation. With the prevalence of often black\nbox data-driven machine learning methods, there is a necessity to develop\ninterpretable machine learning methods that can conduct econometric inference,\nhelping policymakers leverage the new nature of economic data. We therefore\npresent a novel Variational Bayesian Inference approach to incorporate a\ntime-varying parameter auto-regressive model which is scalable for big data.\nOur model is applied to a large blockchain dataset containing prices,\ntransactions of individual actors, analyzing transactional flows and price\nmovements on a very granular level. The model is extendable to any dataset\nwhich can be modelled as a dynamical system. We further improve the simple\nstate-space modelling by introducing non-linearities in the forward model with\nthe help of machine learning architectures.",
    "descriptor": "",
    "authors": [
      "Pratha Khandelwal",
      "Philip Nadler",
      "Rossella Arcucci",
      "William Knottenbelt",
      "Yi-Ke Guo"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.14346"
  },
  {
    "id": "arXiv:2110.14391",
    "title": "Distributed Principal Component Analysis with Limited Communication",
    "abstract": "We study efficient distributed algorithms for the fundamental problem of\nprincipal component analysis and leading eigenvector computation on the sphere,\nwhen the data are randomly distributed among a set of computational nodes. We\npropose a new quantized variant of Riemannian gradient descent to solve this\nproblem, and prove that the algorithm converges with high probability under a\nset of necessary spherical-convexity properties. We give bounds on the number\nof bits transmitted by the algorithm under common initialization schemes, and\ninvestigate the dependency on the problem dimension in each case.",
    "descriptor": "\nComments: Neurips 2021\n",
    "authors": [
      "Foivos Alimisis",
      "Peter Davies",
      "Bart Vandereycken",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.14391"
  },
  {
    "id": "arXiv:2110.14423",
    "title": "Vector-valued Gaussian Processes on Riemannian Manifolds via Gauge  Equivariant Projected Kernels",
    "abstract": "Gaussian processes are machine learning models capable of learning unknown\nfunctions in a way that represents uncertainty, thereby facilitating\nconstruction of optimal decision-making systems. Motivated by a desire to\ndeploy Gaussian processes in novel areas of science, a rapidly-growing line of\nresearch has focused on constructively extending these models to handle\nnon-Euclidean domains, including Riemannian manifolds, such as spheres and\ntori. We propose techniques that generalize this class to model vector fields\non Riemannian manifolds, which are important in a number of application areas\nin the physical sciences. To do so, we present a general recipe for\nconstructing gauge equivariant kernels, which induce Gaussian vector fields,\ni.e. vector-valued Gaussian processes coherent with geometry, from\nscalar-valued Riemannian kernels. We extend standard Gaussian process training\nmethods, such as variational inference, to this setting. This enables\nvector-valued Gaussian processes on Riemannian manifolds to be trained using\nstandard methods and makes them accessible to machine learning practitioners.",
    "descriptor": "",
    "authors": [
      "Michael Hutchinson",
      "Alexander Terenin",
      "Viacheslav Borovitskiy",
      "So Takao",
      "Yee Whye Teh",
      "Marc Peter Deisenroth"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14423"
  },
  {
    "id": "arXiv:2110.14426",
    "title": "Locally Differentially Private Bayesian Inference",
    "abstract": "In recent years, local differential privacy (LDP) has emerged as a technique\nof choice for privacy-preserving data collection in several scenarios when the\naggregator is not trustworthy. LDP provides client-side privacy by adding noise\nat the user's end. Thus, clients need not rely on the trustworthiness of the\naggregator.\nIn this work, we provide a noise-aware probabilistic modeling framework,\nwhich allows Bayesian inference to take into account the noise added for\nprivacy under LDP, conditioned on locally perturbed observations. Stronger\nprivacy protection (compared to the central model) provided by LDP protocols\ncomes at a much harsher privacy-utility trade-off. Our framework tackles\nseveral computational and statistical challenges posed by LDP for accurate\nuncertainty quantification under Bayesian settings. We demonstrate the efficacy\nof our framework in parameter estimation for univariate and multi-variate\ndistributions as well as logistic and linear regression.",
    "descriptor": "",
    "authors": [
      "Tejas Kulkarni",
      "Joonas J\u00e4lk\u00f6",
      "Samuel Kaski",
      "Antti Honkela"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14426"
  },
  {
    "id": "arXiv:2110.14427",
    "title": "The ODE Method for Asymptotic Statistics in Stochastic Approximation and  Reinforcement Learning",
    "abstract": "The paper concerns convergence and asymptotic statistics for stochastic\napproximation driven by Markovian noise: $$ \\theta_{n+1}= \\theta_n + \\alpha_{n\n+ 1} f(\\theta_n, \\Phi_{n+1}) \\,,\\quad n\\ge 0, $$ in which each\n$\\theta_n\\in\\Re^d$, $ \\{ \\Phi_n \\}$ is a Markov chain on a general state space\nX with stationary distribution $\\pi$, and $f:\\Re^d\\times \\text{X} \\to\\Re^d$. In\naddition to standard Lipschitz bounds on $f$, and conditions on the vanishing\nstep-size sequence $\\{\\alpha_n\\}$, it is assumed that the associated ODE is\nglobally asymptotically stable with stationary point denoted $\\theta^*$, where\n$\\bar f(\\theta)=E[f(\\theta,\\Phi)]$ with $\\Phi\\sim\\pi$. Moreover, the\nODE@$\\infty$ defined with respect to the vector field, $$ \\bar\nf_\\infty(\\theta):= \\lim_{r\\to\\infty} r^{-1} \\bar f(r\\theta) \\,,\\qquad\n\\theta\\in\\Re^d, $$ is asymptotically stable. The main contributions are\nsummarized as follows:\n(i) The sequence $\\theta$ is convergent if $\\Phi$ is geometrically ergodic,\nand subject to compatible bounds on $f$.\nThe remaining results are established under a stronger assumption on the\nMarkov chain: A slightly weaker version of the Donsker-Varadhan Lyapunov drift\ncondition known as (DV3).\n(ii) A Lyapunov function is constructed for the joint process\n$\\{\\theta_n,\\Phi_n\\}$ that implies convergence of $\\{ \\theta_n\\}$ in $L_4$.\n(iii) A functional CLT is established, as well as the usual one-dimensional\nCLT for the normalized error $z_n:= (\\theta_n-\\theta^*)/\\sqrt{\\alpha_n}$.\nMoment bounds combined with the CLT imply convergence of the normalized\ncovariance, $$ \\lim_{n \\to \\infty} E [ z_n z_n^T ] = \\Sigma_\\theta, $$ where\n$\\Sigma_\\theta$ is the asymptotic covariance appearing in the CLT.\n(iv) An example is provided where the Markov chain $\\Phi$ is geometrically\nergodic but it does not satisfy (DV3). While the algorithm is convergent, the\nsecond moment is unbounded.",
    "descriptor": "",
    "authors": [
      "Vivek Borkar",
      "Shuhang Chen",
      "Adithya Devraj",
      "Ioannis Kontoyiannis",
      "Sean Meyn"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14427"
  },
  {
    "id": "arXiv:2110.14431",
    "title": "Discrete Hamilton-Jacobi theory for systems with external forces",
    "abstract": "This paper is devoted to discrete mechanical systems subject to external\nforces. We introduce a discrete version of systems with Rayleigh-type forces,\nobtain the equations of motion and characterize the equivalence for these\nsystems. Additionally, we obtain a Noether's theorem and other theorem\ncharacterizing the Lie subalgebra of symmetries of a forced discrete Lagrangian\nsystem. Moreover, we develop a Hamilton-Jacobi theory for forced discrete\nHamiltonian systems. These results are useful for the construction of so-called\nvariational integrators, which, as we illustrate with some examples, are\nremarkably superior to the usual numerical integrators such as the Runge-Kutta\nmethod.",
    "descriptor": "\nComments: 30 pages, 3 figures\n",
    "authors": [
      "Manuel de Le\u00f3n",
      "Manuel Lainz",
      "Asier L\u00f3pez Gord\u00f3n"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Numerical Analysis (math.NA)",
      "Classical Physics (physics.class-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.14431"
  },
  {
    "id": "arXiv:2110.14440",
    "title": "Predictive Geological Mapping with Convolution Neural Network Using  Statistical Data Augmentation on a 3D Model",
    "abstract": "Airborne magnetic data are commonly used to produce preliminary geological\nmaps. Machine learning has the potential to partly fulfill this task rapidly\nand objectively, as geological mapping is comparable to a semantic segmentation\nproblem. Because this method requires a high-quality dataset, we developed a\ndata augmentation workflow that uses a 3D geological and magnetic\nsusceptibility model as input. The workflow uses soft-constrained Multi-Point\nStatistics, to create many synthetic 3D geological models, and Sequential\nGaussian Simulation algorithms, to populate the models with the appropriate\nmagnetic distribution. Then, forward modeling is used to compute the airborne\nmagnetic responses of the synthetic models, which are associated with their\ncounterpart surficial lithologies. A Gated Shape Convolutional Neural Network\nalgorithm was trained on a generated synthetic dataset to perform geological\nmapping of airborne magnetic data and detect lithological contacts. The\nalgorithm also provides attention maps highlighting the structures at different\nscales, and clustering was applied to its high-level features to do a\nsemi-supervised segmentation of the area. The validation conducted on a portion\nof the synthetic dataset and data from adjacent areas shows that the\nmethodology is suitable to segment the surficial geology using airborne\nmagnetic data. Especially, the clustering shows a good segmentation of the\nmagnetic anomalies into a pertinent geological map. Moreover, the first\nattention map isolates the structures at low scales and shows a pertinent\nrepresentation of the original data. Thus, our method can be used to produce\npreliminary geological maps of good quality and new representations of any area\nwhere a geological and petrophysical 3D model exists, or in areas sharing the\nsame geological context, using airborne magnetic data only.",
    "descriptor": "",
    "authors": [
      "Cedou Matthieu",
      "Gloaguen Erwan",
      "Blouin Martin",
      "Cat\u00e9 Antoine",
      "Paiement Jean-Philippe",
      "Tirdad Shiva"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.14440"
  },
  {
    "id": "arXiv:2110.14443",
    "title": "Failure-averse Active Learning for Physics-constrained Systems",
    "abstract": "Active learning is a subfield of machine learning that is devised for design\nand modeling of systems with highly expensive sampling costs. Industrial and\nengineering systems are generally subject to physics constraints that may\ninduce fatal failures when they are violated, while such constraints are\nfrequently underestimated in active learning. In this paper, we develop a novel\nactive learning method that avoids failures considering implicit physics\nconstraints that govern the system. The proposed approach is driven by two\ntasks: the safe variance reduction explores the safe region to reduce the\nvariance of the target model, and the safe region expansion aims to extend the\nexplorable region exploiting the probabilistic model of constraints. The global\nacquisition function is devised to judiciously optimize acquisition functions\nof two tasks, and its theoretical properties are provided. The proposed method\nis applied to the composite fuselage assembly process with consideration of\nmaterial failure using the Tsai-wu criterion, and it is able to achieve\nzero-failure without the knowledge of explicit failure regions.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Cheolhei Lee",
      "Xing Wang",
      "Jianguo Wu",
      "Xiaowei Yue"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14443"
  },
  {
    "id": "arXiv:2110.14453",
    "title": "Most direct product of graphs are Type 1",
    "abstract": "A \\textit{$k$-total coloring} of a graph $G$ is an assignment of $k$ colors\nto its elements (vertices and edges) so that adjacent or incident elements have\ndifferent colors. The total chromatic number is the smallest integer $k$ for\nwhich the graph $G$ has a $k$-total coloring. Clearly, this number is at least\n$\\Delta(G)+1$, where $\\Delta(G)$ is the maximum degree of $G$. When the lower\nbound is reached, the graph is said to be Type~1. The upper bound of\n$\\Delta(G)+2$ is a central problem that has been open for fifty years, is\nverified for graphs with maximum degree 4 but not for regular graphs.\nMost classified direct product of graphs are Type~1. The particular cases of\nthe direct product of cycle graphs $C_m \\times C_n$, for $m =3p, 5\\ell$ and\n$8\\ell$ with $p \\geq 2$ and $\\ell \\geq 1$, and arbitrary $n \\geq 3$, were\npreviously known to be Type 1 and motivated the conjecture that, except for\n$C_4 \\times C_4$, all direct product of cycle graphs $C_m \\times C_n$ with $m,n\n\\geq 3$ are Type 1.\nWe give a general pattern proving that all $C_m \\times C_n$ are Type 1,\nexcept for $C_4 \\times C_4$. dditionally, we investigate sufficient conditions\nto ensure that the direct product reaches the lower bound for the total\nchromatic number.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Diane Castonguay",
      "Celina M. H. de Figueiredo",
      "Luis Antonio Kowada",
      "Caroline Reis Patr\u00e3o",
      "Diana Sasaki"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.14453"
  },
  {
    "id": "arXiv:2110.14465",
    "title": "Unbiased Statistical Estimation and Valid Confidence Intervals Under  Differential Privacy",
    "abstract": "We present a method for producing unbiased parameter estimates and valid\nconfidence intervals under the constraints of differential privacy, a formal\nframework for limiting individual information leakage from sensitive data.\nPrior work in this area is limited in that it is tailored to calculating\nconfidence intervals for specific statistical procedures, such as mean\nestimation or simple linear regression. While other recent work can produce\nconfidence intervals for more general sets of procedures, they either yield\nonly approximately unbiased estimates, are designed for one-dimensional\noutputs, or assume significant user knowledge about the data-generating\ndistribution. Our method induces distributions of mean and covariance estimates\nvia the bag of little bootstraps (BLB) and uses them to privately estimate the\nparameters' sampling distribution via a generalized version of the CoinPress\nestimation algorithm. If the user can bound the parameters of the BLB-induced\nparameters and provide heavier-tailed families, the algorithm produces unbiased\nparameter estimates and valid confidence intervals which hold with arbitrarily\nhigh probability. These results hold in high dimensions and for any estimation\nprocedure which behaves nicely under the bootstrap.",
    "descriptor": "",
    "authors": [
      "Christian Covington",
      "Xi He",
      "James Honaker",
      "Gautam Kamath"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.14465"
  },
  {
    "id": "arXiv:2110.14476",
    "title": "An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic  Resonance Image using Implicit Neural Representation",
    "abstract": "High Resolution (HR) medical images provide rich anatomical structure details\nto facilitate early and accurate diagnosis. In MRI, restricted by hardware\ncapacity, scan time, and patient cooperation ability, isotropic 3D HR image\nacquisition typically requests long scan time and, results in small spatial\ncoverage and low SNR. Recent studies showed that, with deep convolutional\nneural networks, isotropic HR MR images could be recovered from low-resolution\n(LR) input via single image super-resolution (SISR) algorithms. However, most\nexisting SISR methods tend to approach a scale-specific projection between LR\nand HR images, thus these methods can only deal with a fixed up-sampling rate.\nFor achieving different up-sampling rates, multiple SR networks have to be\nbuilt up respectively, which is very time-consuming and resource-intensive. In\nthis paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for\nrecovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images\nwith different up-scaling rates is defined as learning a continuous implicit\nvoxel function from the observed LR images. Then the SR task is converted to\nrepresent the implicit voxel function via deep neural networks from a set of\npaired HR-LR training examples. The ArSSR model consists of an encoder network\nand a decoder network. Specifically, the convolutional encoder network is to\nextract feature maps from the LR input images and the fully-connected decoder\nnetwork is to approximate the implicit voxel function. Due to the continuity of\nthe learned function, a single ArSSR model can achieve arbitrary up-sampling\nrate reconstruction of HR images from any input LR image after training.\nExperimental results on three datasets show that the ArSSR model can achieve\nstate-of-the-art SR performance for 3D HR MR image reconstruction while using a\nsingle trained model to achieve arbitrary up-sampling scales.",
    "descriptor": "\nComments: 18 pages, 13 figures, 4 tables; submitted to Medical Image Analysis\n",
    "authors": [
      "Qing Wu",
      "Yuwei Li",
      "Yawen Sun",
      "Yan Zhou",
      "Hongjiang Wei",
      "Jingyi Yu",
      "Yuyao Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14476"
  },
  {
    "id": "arXiv:2110.14484",
    "title": "PL-Net: Progressive Learning Network for Medical Image Segmentation",
    "abstract": "In recent years, segmentation methods based on deep convolutional neural\nnetworks (CNNs) have made state-of-the-art achievements for many medical\nanalysis tasks. However, most of these approaches improve performance by\noptimizing the structure or adding new functional modules of the U-Net, which\nignoring the complementation and fusion of the coarse-grained and fine-grained\nsemantic information. To solve the above problems, we propose a medical image\nsegmentation framework called progressive learning network (PL-Net), which\nincludes internal progressive learning (IPL) and external progressive learning\n(EPL). PL-Net has the following advantages: (1) IPL divides feature extraction\ninto two \"steps\", which can mix different size receptive fields and capture\nsemantic information from coarse to fine granularity without introducing\nadditional parameters; (2) EPL divides the training process into two \"stages\"\nto optimize parameters, and realizes the fusion of coarse-grained information\nin the previous stage and fine-grained information in the latter stage. We\nevaluate our method in different medical image analysis tasks, and the results\nshow that the segmentation performance of PL-Net is better than the\nstate-of-the-art methods of U-Net and its variants.",
    "descriptor": "",
    "authors": [
      "Junlong Cheng",
      "Chengrui Gao",
      "Chaoqing Wang",
      "Zhangqiang Ming",
      "Yong Yang",
      "Min Zhu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14484"
  },
  {
    "id": "arXiv:2110.14502",
    "title": "Closing the \"Quantum Supremacy\" Gap: Achieving Real-Time Simulation of a  Random Quantum Circuit Using a New Sunway Supercomputer",
    "abstract": "We develop a high-performance tensor-based simulator for random quantum\ncircuits(RQCs) on the new Sunway supercomputer. Our major innovations include:\n(1) a near-optimal slicing scheme, and a path-optimization strategy that\nconsiders both complexity and compute density; (2) a three-level\nparallelization scheme that scales to about 42 million cores; (3) a fused\npermutation and multiplication design that improves the compute efficiency for\na wide range of tensor contraction scenarios; and (4) a mixed-precision scheme\nto further improve the performance. Our simulator effectively expands the scope\nof simulatable RQCs to include the 10*10(qubits)*(1+40+1)(depth) circuit, with\na sustained performance of 1.2 Eflops (single-precision), or 4.4 Eflops\n(mixed-precision)as a new milestone for classical simulation of quantum\ncircuits; and reduces the simulation sampling time of Google Sycamore to 304\nseconds, from the previously claimed 10,000 years.",
    "descriptor": "\nComments: 18 pages, 13 figures\n",
    "authors": [
      "Yong",
      "Fang",
      "Haohuan Fu",
      "Yuling Yang",
      "Jiawei Song",
      "Pengpeng Zhao",
      "Zhen Wang",
      "Dajia Peng",
      "Huarong Chen",
      "Chu Guo",
      "Heliang Huang",
      "Wenzhao Wu",
      "Dexun Chen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.14502"
  },
  {
    "id": "arXiv:2110.14507",
    "title": "Detecting the ultra low dimensionality of real networks",
    "abstract": "Reducing dimension redundancy to find simplifying patterns in\nhigh-dimensional datasets and complex networks has become a major endeavor in\nmany scientific fields. However, detecting the dimensionality of their latent\nspace is challenging but necessary to generate efficient embeddings to be used\nin a multitude of downstream tasks. Here, we propose a method to infer the\ndimensionality of networks without the need for any a priori spatial embedding.\nDue to the ability of hyperbolic geometry to capture the complex connectivity\nof real networks, we detect ultra low dimensionality far below values reported\nusing other approaches. We applied our method to real networks from different\ndomains and found unexpected regularities, including: tissue-specific\nbiomolecular networks being extremely low dimensional; brain connectomes being\nclose to the three dimensions of their anatomical embedding; and social\nnetworks and the Internet requiring slightly higher dimensionality. Beyond\npaving the way towards an ultra efficient dimensional reduction, our findings\nhelp address fundamental issues that hinge on dimensionality, such as\nuniversality in critical behavior.",
    "descriptor": "",
    "authors": [
      "Pedro Almagro",
      "Marian Boguna",
      "M. Angeles Serrano"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.14507"
  },
  {
    "id": "arXiv:2110.14549",
    "title": "Latent Equilibrium: A unified learning theory for arbitrarily fast  computation with arbitrarily slow neurons",
    "abstract": "The response time of physical computational elements is finite, and neurons\nare no exception. In hierarchical models of cortical networks each layer thus\nintroduces a response lag. This inherent property of physical dynamical systems\nresults in delayed processing of stimuli and causes a timing mismatch between\nnetwork output and instructive signals, thus afflicting not only inference, but\nalso learning. We introduce Latent Equilibrium, a new framework for inference\nand learning in networks of slow components which avoids these issues by\nharnessing the ability of biological neurons to phase-advance their output with\nrespect to their membrane potential. This principle enables quasi-instantaneous\ninference independent of network depth and avoids the need for phased\nplasticity or computationally expensive network relaxation phases. We jointly\nderive disentangled neuron and synapse dynamics from a prospective energy\nfunction that depends on a network's generalized position and momentum. The\nresulting model can be interpreted as a biologically plausible approximation of\nerror backpropagation in deep cortical networks with continuous-time, leaky\nneuronal dynamics and continuously active, local plasticity. We demonstrate\nsuccessful learning of standard benchmark datasets, achieving competitive\nperformance using both fully-connected and convolutional architectures, and\nshow how our principle can be applied to detailed models of cortical\nmicrocircuitry. Furthermore, we study the robustness of our model to\nspatio-temporal substrate imperfections to demonstrate its feasibility for\nphysical realization, be it in vivo or in silico.",
    "descriptor": "\nComments: Accepted for publication in Advances in Neural Information Processing Systems 34 (NeurIPS 2021); 13 pages, 4 figures; 10 pages of supplementary material, 1 supplementary figure\n",
    "authors": [
      "Paul Haider",
      "Benjamin Ellenberger",
      "Laura Kriener",
      "Jakob Jordan",
      "Walter Senn",
      "Mihai A. Petrovici"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.14549"
  },
  {
    "id": "arXiv:2110.14593",
    "title": "TA-Net: Topology-Aware Network for Gland Segmentation",
    "abstract": "Gland segmentation is a critical step to quantitatively assess the morphology\nof glands in histopathology image analysis. However, it is challenging to\nseparate densely clustered glands accurately. Existing deep learning-based\napproaches attempted to use contour-based techniques to alleviate this issue\nbut only achieved limited success. To address this challenge, we propose a\nnovel topology-aware network (TA-Net) to accurately separate densely clustered\nand severely deformed glands. The proposed TA-Net has a multitask learning\narchitecture and enhances the generalization of gland segmentation by learning\nshared representation from two tasks: instance segmentation and gland topology\nestimation. The proposed topology loss computes gland topology using gland\nskeletons and markers. It drives the network to generate segmentation results\nthat comply with the true gland topology. We validate the proposed approach on\nthe GlaS and CRAG datasets using three quantitative metrics, F1-score,\nobject-level Dice coefficient, and object-level Hausdorff distance. Extensive\nexperiments demonstrate that TA-Net achieves state-of-the-art performance on\nthe two datasets. TA-Net outperforms other approaches in the presence of\ndensely clustered glands.",
    "descriptor": "",
    "authors": [
      "Haotian Wang",
      "Min Xian",
      "Aleksandar Vakanski"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14593"
  },
  {
    "id": "arXiv:2110.14594",
    "title": "End-to-end LSTM based estimation of volcano event epicenter localization",
    "abstract": "In this paper, an end-to-end based LSTM scheme is proposed to address the\nproblem of volcano event localization without any a priori model relating phase\npicking with localization estimation. It is worth emphasizing that automatic\nphase picking in volcano signals is highly inaccurate because of the short\ndistances between the event epicenters and the seismograph stations. LSTM was\nchosen due to its capability to capture the dynamics of time varying signals,\nand to remove or add information within the memory cell state and model\nlong-term dependencies. A brief insight into LSTM is also discussed here. The\nresults presented in this paper show that the LSTM based architecture provided\na success rate, i.e., an error smaller than 1.0Km, equal to 48.5%, which in\nturn is dramatically superior to the one delivered by automatic phase picking.\nMoreover, the proposed end-to-end LSTM based method gave a success rate 18%\nhigher than CNN.",
    "descriptor": "\nComments: 16 pages, 7 figures\n",
    "authors": [
      "Nestor Becerra Yoma",
      "Jorge Wuth",
      "Andres Pinto",
      "Nicolas de Celis",
      "Jorge Celis",
      "Fernando Huenupan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.14594"
  },
  {
    "id": "arXiv:2110.14602",
    "title": "Towards a Theory of Evolution as Multilevel Learning",
    "abstract": "We apply the theory of learning to physically renormalizable systems in an\nattempt to develop a theory of biological evolution, including the origin of\nlife, as multilevel learning. We formulate seven fundamental principles of\nevolution that appear to be necessary and sufficient to render a universe\nobservable and show that they entail the major features of biological\nevolution, including replication and natural selection. These principles also\nfollow naturally from the theory of learning. We formulate the theory of\nevolution using the mathematical framework of neural networks, which provides\nfor detailed analysis of evolutionary phenomena. To demonstrate the potential\nof the proposed theoretical framework, we derive a generalized version of the\nCentral Dogma of molecular biology by analyzing the flow of information during\nlearning (back-propagation) and predicting (forward-propagation) the\nenvironment by evolving organisms. The more complex evolutionary phenomena,\nsuch as major transitions in evolution, in particular, the origin of life, have\nto be analyzed in the thermodynamic limit, which is described in detail in the\naccompanying paper.",
    "descriptor": "\nComments: 29 pages, 3 figures\n",
    "authors": [
      "Vitaly Vanchurin",
      "Yuri I. Wolf",
      "Mikhail I. Katsnelson",
      "Eugene V. Koonin"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14602"
  },
  {
    "id": "arXiv:2110.14609",
    "title": "Paving the Way for Consensus: Convergence of Block Gossip Algorithms",
    "abstract": "Gossip protocols are popular methods for average consensus problems in\ndistributed computing. We prove new convergence guarantees for a variety of\nsuch protocols, including path, clique, and synchronous pairwise gossip. These\narise by exploiting the connection between these protocols and the block\nrandomized Kaczmarz method for solving linear systems. Moreover, we extend\nexisting convergence results for block randomized Kaczmarz to allow for a more\ngeneral choice of blocks, rank-deficient systems, and provide a tighter\nconvergence rate guarantee. We furthermore apply this analysis to inconsistent\nconsensus models and obtain similar guarantees. An extensive empirical analysis\nof these methods is provided for a variety of synthetic networks.",
    "descriptor": "\nComments: 21 pages, 19 figures\n",
    "authors": [
      "Jamie Haddock",
      "Benjamin Jarman",
      "Chen Yap"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.14609"
  },
  {
    "id": "arXiv:2110.14622",
    "title": "Heterogeneous Multi-player Multi-armed Bandits: Closing the Gap and  Generalization",
    "abstract": "Despite the significant interests and many progresses in decentralized\nmulti-player multi-armed bandits (MP-MAB) problems in recent years, the regret\ngap to the natural centralized lower bound in the heterogeneous MP-MAB setting\nremains open. In this paper, we propose BEACON -- Batched Exploration with\nAdaptive COmmunicatioN -- that closes this gap. BEACON accomplishes this goal\nwith novel contributions in implicit communication and efficient exploration.\nFor the former, we propose a novel adaptive differential communication (ADC)\ndesign that significantly improves the implicit communication efficiency. For\nthe latter, a carefully crafted batched exploration scheme is developed to\nenable incorporation of the combinatorial upper confidence bound (CUCB)\nprinciple. We then generalize the existing linear-reward MP-MAB problems, where\nthe system reward is always the sum of individually collected rewards, to a new\nMP-MAB problem where the system reward is a general (nonlinear) function of\nindividual rewards. We extend BEACON to solve this problem and prove a\nlogarithmic regret. BEACON bridges the algorithm design and regret analysis of\ncombinatorial MAB (CMAB) and MP-MAB, two largely disjointed areas in MAB, and\nthe results in this paper suggest that this previously ignored connection is\nworth further investigation. Supplementary Material: pdf",
    "descriptor": "\nComments: Accepted to NeurIPS 2021, camera-ready version\n",
    "authors": [
      "Chengshuai Shi",
      "Wei Xiong",
      "Cong Shen",
      "Jing Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14622"
  },
  {
    "id": "arXiv:2110.14628",
    "title": "(Almost) Free Incentivized Exploration from Decentralized Learning  Agents",
    "abstract": "Incentivized exploration in multi-armed bandits (MAB) has witnessed\nincreasing interests and many progresses in recent years, where a principal\noffers bonuses to agents to do explorations on her behalf. However, almost all\nexisting studies are confined to temporary myopic agents. In this work, we\nbreak this barrier and study incentivized exploration with multiple and\nlong-term strategic agents, who have more complicated behaviors that often\nappear in real-world applications. An important observation of this work is\nthat strategic agents' intrinsic needs of learning benefit (instead of harming)\nthe principal's explorations by providing \"free pulls\". Moreover, it turns out\nthat increasing the population of agents significantly lowers the principal's\nburden of incentivizing. The key and somewhat surprising insight revealed from\nour results is that when there are sufficiently many learning agents involved,\nthe exploration process of the principal can be (almost) free. Our main results\nare built upon three novel components which may be of independent interest: (1)\na simple yet provably effective incentive-provision strategy; (2) a carefully\ncrafted best arm identification algorithm for rewards aggregated under unequal\nconfidences; (3) a high-probability finite-time lower bound of UCB algorithms.\nExperimental results are provided to complement the theoretical analysis.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021, camera-ready version\n",
    "authors": [
      "Chengshuai Shi",
      "Haifeng Xu",
      "Wei Xiong",
      "Cong Shen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.14628"
  },
  {
    "id": "arXiv:1705.09231",
    "title": "Neural Attribute Machines for Program Generation",
    "abstract": "Neural Attribute Machines for Program Generation",
    "descriptor": "",
    "authors": [
      "Matthew Amodio",
      "Swarat Chaudhuri",
      "Thomas W. Reps"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/1705.09231"
  },
  {
    "id": "arXiv:1711.01381",
    "title": "Finding branch-decompositions of matroids, hypergraphs, and more",
    "abstract": "Comments: 79 pages, 15 figures; Fix a few English issues. To appear in SIAM J. Discrete Math",
    "descriptor": "\nComments: 79 pages, 15 figures; Fix a few English issues. To appear in SIAM J. Discrete Math\n",
    "authors": [
      "Jisu Jeong",
      "Eun Jung Kim",
      "Sang-il Oum"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1711.01381"
  },
  {
    "id": "arXiv:1804.02864",
    "title": "Semantic Edge Detection with Diverse Deep Supervision",
    "abstract": "Comments: International Journal of Computer Vision",
    "descriptor": "\nComments: International Journal of Computer Vision\n",
    "authors": [
      "Yun Liu",
      "Ming-Ming Cheng",
      "Deng-Ping Fan",
      "Le Zhang",
      "JiaWang Bian",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1804.02864"
  },
  {
    "id": "arXiv:1804.03178",
    "title": "Power of Bonus in Pricing for Crowdsourcing",
    "abstract": "Power of Bonus in Pricing for Crowdsourcing",
    "descriptor": "",
    "authors": [
      "Suho Shin",
      "Hoyong Choi",
      "Yung Yi",
      "Jungseul Ok"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/1804.03178"
  },
  {
    "id": "arXiv:1901.02661",
    "title": "Artificial Intelligence for Dynamical Systems in Wireless  Communications: Modeling for the Future",
    "abstract": "Comments: Accepted author's version of the manuscript",
    "descriptor": "\nComments: Accepted author's version of the manuscript\n",
    "authors": [
      "Harun Siljak",
      "Irene Macaluso",
      "Nicola Marchetti"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Information Theory (cs.IT)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/1901.02661"
  },
  {
    "id": "arXiv:1901.05639",
    "title": "Machine learning with neural networks",
    "abstract": "Comments: Revised version, 241 pages",
    "descriptor": "\nComments: Revised version, 241 pages\n",
    "authors": [
      "B. Mehlig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1901.05639"
  },
  {
    "id": "arXiv:1904.12060",
    "title": "Every planar graph with $\u0394\\geqslant 8$ is totally  $(\u0394+2)$-choosable",
    "abstract": "Comments: 77 pages, 78 figures",
    "descriptor": "\nComments: 77 pages, 78 figures\n",
    "authors": [
      "Marthe Bonamy",
      "Th\u00e9o Pierron",
      "\u00c9ric Sopena"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/1904.12060"
  },
  {
    "id": "arXiv:1905.03527",
    "title": "Performance Analysis of Fog-Aided D2D Networks with Multicast-Based  Opportunistic Content Delivery",
    "abstract": "Comments: 30 pages, 17 figures, submitted to IEEE Transactions on Communications",
    "descriptor": "\nComments: 30 pages, 17 figures, submitted to IEEE Transactions on Communications\n",
    "authors": [
      "Xiaoshi Song",
      "Mengying Yuan",
      "Huan Zhou",
      "Haijun Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/1905.03527"
  },
  {
    "id": "arXiv:1906.08533",
    "title": "The spherical ensemble and quasi-Monte-Carlo designs",
    "abstract": "Comments: v1: 19 pages. v2: The explicit form of the main result (formula 1.8) has been improved, by improving Lemma 2.6. 19 pages (submitted version)",
    "descriptor": "\nComments: v1: 19 pages. v2: The explicit form of the main result (formula 1.8) has been improved, by improving Lemma 2.6. 19 pages (submitted version)\n",
    "authors": [
      "Robert J. Berman"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1906.08533"
  },
  {
    "id": "arXiv:1908.02962",
    "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and  Commonsense",
    "abstract": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and  Commonsense",
    "descriptor": "",
    "authors": [
      "Difei Gao",
      "Ruiping Wang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1908.02962"
  },
  {
    "id": "arXiv:1909.04497",
    "title": "Equity2Vec: End-to-end Deep Learning Framework for Cross-sectional Asset  Pricing",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Qiong Wu",
      "Christopher G. Brinton",
      "Zheng Zhang",
      "Andrea Pizzoferrato",
      "Zhenming Liu",
      "Mihai Cucuringu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Finance (q-fin.ST)"
    ],
    "url": "https://arxiv.org/abs/1909.04497"
  },
  {
    "id": "arXiv:1910.12026",
    "title": "On the Hardness of Energy Minimisation for Crystal Structure Prediction",
    "abstract": "Comments: Short version to be published in SOFSEM 2020",
    "descriptor": "\nComments: Short version to be published in SOFSEM 2020\n",
    "authors": [
      "Duncan Adamson",
      "Argyrios Deligkas",
      "Vladimir Gusev",
      "Igor Potapov"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/1910.12026"
  },
  {
    "id": "arXiv:1911.11815",
    "title": "Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",
    "abstract": "Comments: Appeared in Usenix Security Symposium 2020. Fixed an error in Theorem 1. For demo code, see this https URL . For slides, see this https URL . For the talk, see this https URL",
    "descriptor": "\nComments: Appeared in Usenix Security Symposium 2020. Fixed an error in Theorem 1. For demo code, see this https URL . For slides, see this https URL . For the talk, see this https URL\n",
    "authors": [
      "Minghong Fang",
      "Xiaoyu Cao",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1911.11815"
  },
  {
    "id": "arXiv:1912.02143",
    "title": "Landscape Complexity for the Empirical Risk of Generalized Linear Models",
    "abstract": "Comments: 18 pages and 18 pages appendix. Update to match the published version (v2). Corrections of remaining small typos (v3). Simplification of a technical argument in Appendix A (v4)",
    "descriptor": "\nComments: 18 pages and 18 pages appendix. Update to match the published version (v2). Corrections of remaining small typos (v3). Simplification of a technical argument in Appendix A (v4)\n",
    "authors": [
      "Antoine Maillard",
      "G\u00e9rard Ben Arous",
      "Giulio Biroli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/1912.02143"
  },
  {
    "id": "arXiv:1912.07696",
    "title": "PETSc TSAdjoint: a discrete adjoint ODE solver for first-order and  second-order sensitivity analysis",
    "abstract": "PETSc TSAdjoint: a discrete adjoint ODE solver for first-order and  second-order sensitivity analysis",
    "descriptor": "",
    "authors": [
      "Hong Zhang",
      "Emil M. Constantinescu",
      "Barry F. Smith"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/1912.07696"
  },
  {
    "id": "arXiv:1912.12740",
    "title": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,  and Systems",
    "abstract": "Practice of Streaming Processing of Dynamic Graphs: Concepts, Models,  and Systems",
    "descriptor": "",
    "authors": [
      "Maciej Besta",
      "Marc Fischer",
      "Vasiliki Kalavri",
      "Michael Kapralov",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Databases (cs.DB)",
      "Data Structures and Algorithms (cs.DS)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/1912.12740"
  },
  {
    "id": "arXiv:2001.09952",
    "title": "Compressed Sensing with 1D Total Variation: Breaking Sample Complexity  Barriers via Non-Uniform Recovery",
    "abstract": "Compressed Sensing with 1D Total Variation: Breaking Sample Complexity  Barriers via Non-Uniform Recovery",
    "descriptor": "",
    "authors": [
      "Martin Genzel",
      "Maximilian M\u00e4rz",
      "Robert Seidel"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2001.09952"
  },
  {
    "id": "arXiv:2001.11279",
    "title": "Goal-directed graph construction using reinforcement learning",
    "abstract": "Goal-directed graph construction using reinforcement learning",
    "descriptor": "",
    "authors": [
      "Victor-Alexandru Darvariu",
      "Stephen Hailes",
      "Mirco Musolesi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2001.11279"
  },
  {
    "id": "arXiv:2002.00080",
    "title": "Convergence rate analysis and improved iterations for numerical radius  computation",
    "abstract": "Comments: Revision #2",
    "descriptor": "\nComments: Revision #2\n",
    "authors": [
      "Tim Mitchell"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2002.00080"
  },
  {
    "id": "arXiv:2002.09784",
    "title": "Uniform Interpolants in EUF: Algorithms using DAG-representations",
    "abstract": "Uniform Interpolants in EUF: Algorithms using DAG-representations",
    "descriptor": "",
    "authors": [
      "Silvio Ghilardi",
      "Alessandro Gianola",
      "Deepak Kapur"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2002.09784"
  },
  {
    "id": "arXiv:2003.04298",
    "title": "On Compositions of Transformations in Contrastive Self-Supervised  Learning",
    "abstract": "Comments: Accepted to ICCV 2021. Code and pretrained models are available at this https URL",
    "descriptor": "\nComments: Accepted to ICCV 2021. Code and pretrained models are available at this https URL\n",
    "authors": [
      "Mandela Patrick",
      "Yuki M. Asano",
      "Polina Kuznetsova",
      "Ruth Fong",
      "Jo\u00e3o F. Henriques",
      "Geoffrey Zweig",
      "Andrea Vedaldi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2003.04298"
  },
  {
    "id": "arXiv:2006.00364",
    "title": "CLARINET: A RISC-V Based Framework for Posit Arithmetic Empiricism",
    "abstract": "CLARINET: A RISC-V Based Framework for Posit Arithmetic Empiricism",
    "descriptor": "",
    "authors": [
      "Niraj Sharma",
      "Riya Jain",
      "Madhumita Mohan",
      "Sachin Patkar",
      "Rainer Leupers",
      "Nikhil Rishiyur",
      "Farhad Merchant"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2006.00364"
  },
  {
    "id": "arXiv:2006.00386",
    "title": "Scheduling in the Random-Order Model",
    "abstract": "Scheduling in the Random-Order Model",
    "descriptor": "",
    "authors": [
      "Susanne Albers",
      "Maximilian Janke"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2006.00386"
  },
  {
    "id": "arXiv:2006.05057",
    "title": "Towards More Practical Adversarial Attacks on Graph Neural Networks",
    "abstract": "Comments: NeurIPS 2020, Code Link Update",
    "descriptor": "\nComments: NeurIPS 2020, Code Link Update\n",
    "authors": [
      "Jiaqi Ma",
      "Shuangrui Ding",
      "Qiaozhu Mei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.05057"
  },
  {
    "id": "arXiv:2006.06117",
    "title": "Geometric Solutions for General Actuator Routing on Inflated-Beam Soft  Growing Robots",
    "abstract": "Comments: 21 pages, 18 figures",
    "descriptor": "\nComments: 21 pages, 18 figures\n",
    "authors": [
      "Laura H Blumenschein",
      "Margaret Koehler",
      "Nathan S. Usevitch",
      "Elliot W. Hawkes",
      "D. Caleb Rucker",
      "Allison M. Okamura"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2006.06117"
  },
  {
    "id": "arXiv:2006.14804",
    "title": "Widening the Pipeline in Human-Guided Reinforcement Learning with  Explanation and Context-Aware Data Augmentation",
    "abstract": "Widening the Pipeline in Human-Guided Reinforcement Learning with  Explanation and Context-Aware Data Augmentation",
    "descriptor": "",
    "authors": [
      "Lin Guan",
      "Mudit Verma",
      "Sihang Guo",
      "Ruohan Zhang",
      "Subbarao Kambhampati"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2006.14804"
  },
  {
    "id": "arXiv:2006.14892",
    "title": "Well-posedness and numerical schemes for one-dimensional McKean-Vlasov  equations and interacting particle systems with discontinuous drift",
    "abstract": "Comments: 33 pages, 4 figures, revised introduction and Section 5",
    "descriptor": "\nComments: 33 pages, 4 figures, revised introduction and Section 5\n",
    "authors": [
      "Gunther Leobacher",
      "Christoph Reisinger",
      "Wolfgang Stockinger"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2006.14892"
  },
  {
    "id": "arXiv:2007.00514",
    "title": "Regularized Online Allocation Problems: Fairness and Beyond",
    "abstract": "Regularized Online Allocation Problems: Fairness and Beyond",
    "descriptor": "",
    "authors": [
      "Santiago Balseiro",
      "Haihao Lu",
      "Vahab Mirrokni"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.00514"
  },
  {
    "id": "arXiv:2007.01647",
    "title": "Learning intuitive physics and one-shot imitation using  state-action-prediction self-organizing maps",
    "abstract": "Comments: 27 pages, 5 figures",
    "descriptor": "\nComments: 27 pages, 5 figures\n",
    "authors": [
      "Martin Stetter",
      "Elmar W. Lang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2007.01647"
  },
  {
    "id": "arXiv:2007.07040",
    "title": "Hybrid divide-and-conquer approach for tree search algorithms",
    "abstract": "Comments: 48 pages, 13 figures",
    "descriptor": "\nComments: 48 pages, 13 figures\n",
    "authors": [
      "Mathys Rennela",
      "Sebastiaan Brand",
      "Alfons Laarman",
      "Vedran Dunjko"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2007.07040"
  },
  {
    "id": "arXiv:2008.01495",
    "title": "Generic identifiability of subnetworks in a linear dynamic network: the  full measurement case",
    "abstract": "Generic identifiability of subnetworks in a linear dynamic network: the  full measurement case",
    "descriptor": "",
    "authors": [
      "Shengling Shi",
      "Xiaodong Cheng",
      "Paul M. J. Van den Hof"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2008.01495"
  },
  {
    "id": "arXiv:2008.01645",
    "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data  with Dimensionality Reduction",
    "abstract": "Comments: This is the author's version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics. The final version of this record is available at: 10.1109/TVCG.2020.3028889",
    "descriptor": "\nComments: This is the author's version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics. The final version of this record is available at: 10.1109/TVCG.2020.3028889\n",
    "authors": [
      "Takanori Fujiwara",
      "Shilpika",
      "Naohisa Sakamoto",
      "Jorji Nonaka",
      "Keiji Yamamoto",
      "Kwan-Liu Ma"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.01645"
  },
  {
    "id": "arXiv:2009.03561",
    "title": "Local and Central Differential Privacy for Robustness and Privacy in  Federated Learning",
    "abstract": "Local and Central Differential Privacy for Robustness and Privacy in  Federated Learning",
    "descriptor": "",
    "authors": [
      "Mohammad Naseri",
      "Jamie Hayes",
      "Emiliano De Cristofaro"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2009.03561"
  },
  {
    "id": "arXiv:2009.13819",
    "title": "The Shapley Value of Inconsistency Measures for Functional Dependencies",
    "abstract": "The Shapley Value of Inconsistency Measures for Functional Dependencies",
    "descriptor": "",
    "authors": [
      "Ester Livshits",
      "Benny Kimelfeld"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2009.13819"
  },
  {
    "id": "arXiv:2010.01051",
    "title": "Neural Bootstrapper",
    "abstract": "Comments: 19 pages, 13 figures. Accepted for NeurIPS 2021. Corresponding Author: Sungbin Lim",
    "descriptor": "\nComments: 19 pages, 13 figures. Accepted for NeurIPS 2021. Corresponding Author: Sungbin Lim\n",
    "authors": [
      "Minsuk Shin",
      "Hyungjoo Cho",
      "Hyun-seok Min",
      "Sungbin Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.01051"
  },
  {
    "id": "arXiv:2010.04199",
    "title": "Multiscale Elliptic PDEs Upscaling and Function Approximation via  Subsampled Data",
    "abstract": "Multiscale Elliptic PDEs Upscaling and Function Approximation via  Subsampled Data",
    "descriptor": "",
    "authors": [
      "Yifan Chen",
      "Thomas Y. Hou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2010.04199"
  },
  {
    "id": "arXiv:2010.07708",
    "title": "A Reversible Dynamic Movement Primitive formulation",
    "abstract": "A Reversible Dynamic Movement Primitive formulation",
    "descriptor": "",
    "authors": [
      "Antonis Sidiropoulos",
      "Zoe Doulgeri"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2010.07708"
  },
  {
    "id": "arXiv:2010.07778",
    "title": "Local Differential Privacy for Regret Minimization in Reinforcement  Learning",
    "abstract": "Local Differential Privacy for Regret Minimization in Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Evrard Garcelon",
      "Vianney Perchet",
      "Ciara Pike-Burke",
      "Matteo Pirotta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.07778"
  },
  {
    "id": "arXiv:2010.09063",
    "title": "Enabling Fast Differentially Private SGD via Just-in-Time Compilation  and Vectorization",
    "abstract": "Comments: To appear in NeurIPS 2021",
    "descriptor": "\nComments: To appear in NeurIPS 2021\n",
    "authors": [
      "Pranav Subramani",
      "Nicholas Vadivelu",
      "Gautam Kamath"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2010.09063"
  },
  {
    "id": "arXiv:2010.11171",
    "title": "How Data Augmentation affects Optimization for Linear Regression",
    "abstract": "Comments: 31 pages, 3 figures, NeurIPS 2021",
    "descriptor": "\nComments: 31 pages, 3 figures, NeurIPS 2021\n",
    "authors": [
      "Boris Hanin",
      "Yi Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.11171"
  },
  {
    "id": "arXiv:2010.11295",
    "title": "Bidirectional Microrocker Bots Controlled via Neutral Position Offset",
    "abstract": "Comments: Manuscript has been changed significantly",
    "descriptor": "\nComments: Manuscript has been changed significantly\n",
    "authors": [
      "Tony Wang",
      "DeaGyu Kim",
      "Yifan Shi",
      "Zhijian Hao",
      "Azadeh Ansari"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.11295"
  },
  {
    "id": "arXiv:2010.12866",
    "title": "Optimal Algorithms for Stochastic Multi-Armed Bandits with Heavy Tailed  Rewards",
    "abstract": "Comments: 38 pages, 4 figures. Accepted for NeurIPS 2020",
    "descriptor": "\nComments: 38 pages, 4 figures. Accepted for NeurIPS 2020\n",
    "authors": [
      "Kyungjae Lee",
      "Hongjun Yang",
      "Sungbin Lim",
      "Songhwai Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.12866"
  },
  {
    "id": "arXiv:2010.13442",
    "title": "A Purely Regular Approach to Non-Regular Core Spanners",
    "abstract": "A Purely Regular Approach to Non-Regular Core Spanners",
    "descriptor": "",
    "authors": [
      "Markus L. Schmid",
      "Nicole Schweikardt"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2010.13442"
  },
  {
    "id": "arXiv:2010.15206",
    "title": "Rosella: A Self-Driving Distributed Scheduler for Heterogeneous Clusters",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Qiong Wu",
      "Zhenming Liu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.15206"
  },
  {
    "id": "arXiv:2010.16103",
    "title": "Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node  Representation Learning",
    "abstract": "Comments: Accepted to NeurIPS 2021",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Muhan Zhang",
      "Pan Li",
      "Yinglong Xia",
      "Kai Wang",
      "Long Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.16103"
  },
  {
    "id": "arXiv:2010.16188",
    "title": "Bridging Composite and Real: Towards End-to-end Deep Image Matting",
    "abstract": "Comments: Accepted by the International Journal of Computer Vision (IJCV). Both the datasets and source code are available at this https URL",
    "descriptor": "\nComments: Accepted by the International Journal of Computer Vision (IJCV). Both the datasets and source code are available at this https URL\n",
    "authors": [
      "Jizhizi Li",
      "Jing Zhang",
      "Stephen J. Maybank",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2010.16188"
  },
  {
    "id": "arXiv:2011.03173",
    "title": "Does enforcing fairness mitigate biases caused by subpopulation shift?",
    "abstract": "Does enforcing fairness mitigate biases caused by subpopulation shift?",
    "descriptor": "",
    "authors": [
      "Subha Maity",
      "Debarghya Mukherjee",
      "Mikhail Yurochkin",
      "Yuekai Sun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.03173"
  },
  {
    "id": "arXiv:2011.03902",
    "title": "Learning Neural Event Functions for Ordinary Differential Equations",
    "abstract": "Learning Neural Event Functions for Ordinary Differential Equations",
    "descriptor": "",
    "authors": [
      "Ricky T. Q. Chen",
      "Brandon Amos",
      "Maximilian Nickel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.03902"
  },
  {
    "id": "arXiv:2011.05586",
    "title": "Strict Enforcement of Conservation Laws and Invertibility in CNN-Based  Super Resolution for Scientific Datasets",
    "abstract": "Comments: 16 pages, 7 figures",
    "descriptor": "\nComments: 16 pages, 7 figures\n",
    "authors": [
      "Andrew Geiss",
      "Joseph C. Hardin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.05586"
  },
  {
    "id": "arXiv:2011.06391",
    "title": "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph  Neural Networks",
    "abstract": "Comments: 11 pages, published in IEEE IPDPS 2021",
    "descriptor": "\nComments: 11 pages, published in IEEE IPDPS 2021\n",
    "authors": [
      "Md. Khaledur Rahman",
      "Majedul Haque Sujon",
      "Ariful Azad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2011.06391"
  },
  {
    "id": "arXiv:2011.06741",
    "title": "Rebounding Bandits for Modeling Satiation Effects",
    "abstract": "Rebounding Bandits for Modeling Satiation Effects",
    "descriptor": "",
    "authors": [
      "Liu Leqi",
      "Fatma Kilinc-Karzan",
      "Zachary C. Lipton",
      "Alan L. Montgomery"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.06741"
  },
  {
    "id": "arXiv:2011.10510",
    "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach",
    "abstract": "Comments: 22 pages, 13 figures, 5 tables, and supplementary material included in the end of the paper",
    "descriptor": "\nComments: 22 pages, 13 figures, 5 tables, and supplementary material included in the end of the paper\n",
    "authors": [
      "M Quamer Nasim",
      "Tannistha Maiti",
      "Ayush Srivastava",
      "Tarry Singh",
      "Jie Mei"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2011.10510"
  },
  {
    "id": "arXiv:2012.06603",
    "title": "Non-asymptotic error estimates for the Laplace approximation in Bayesian  inverse problems",
    "abstract": "Non-asymptotic error estimates for the Laplace approximation in Bayesian  inverse problems",
    "descriptor": "",
    "authors": [
      "Tapio Helin",
      "Remo Kretschmann"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2012.06603"
  },
  {
    "id": "arXiv:2012.08101",
    "title": "Detecting and Adapting to Irregular Distribution Shifts in Bayesian  Online Learning",
    "abstract": "Comments: Published version, Neural Information Processing Systems 2021",
    "descriptor": "\nComments: Published version, Neural Information Processing Systems 2021\n",
    "authors": [
      "Aodong Li",
      "Alex Boyd",
      "Padhraic Smyth",
      "Stephan Mandt"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.08101"
  },
  {
    "id": "arXiv:2012.09814",
    "title": "Induced Disjoint Paths in AT-free Graphs",
    "abstract": "Comments: An extended abstract of this paper appeared in the proceedings of SWAT 2012",
    "descriptor": "\nComments: An extended abstract of this paper appeared in the proceedings of SWAT 2012\n",
    "authors": [
      "Petr A. Golovach",
      "Dani\u00ebl Paulusma",
      "Erik Jan van Leeuwen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2012.09814"
  },
  {
    "id": "arXiv:2012.11207",
    "title": "On Success and Simplicity: A Second Look at Transferable Targeted  Attacks",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Zhengyu Zhao",
      "Zhuoran Liu",
      "Martha Larson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.11207"
  },
  {
    "id": "arXiv:2012.13995",
    "title": "FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping",
    "abstract": "Comments: Appeared in NDSS 2021. For demo code, see this https URL . For slides, see this https URL . For the talk, see this https URL",
    "descriptor": "\nComments: Appeared in NDSS 2021. For demo code, see this https URL . For slides, see this https URL . For the talk, see this https URL\n",
    "authors": [
      "Xiaoyu Cao",
      "Minghong Fang",
      "Jia Liu",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2012.13995"
  },
  {
    "id": "arXiv:2101.00903",
    "title": "Data-driven estimation of the maximum sampling interval: analysis and  controller design for discrete-time systems",
    "abstract": "Comments: 16 pages, 4 figure, 1 table. Now contains 1) a disturbance description via multipliers, 2) extended proofs and 3) an extensive numerical case study, including a comparison of different data lengths, a discussion of complexity and a comparison with set membership estimation",
    "descriptor": "\nComments: 16 pages, 4 figure, 1 table. Now contains 1) a disturbance description via multipliers, 2) extended proofs and 3) an extensive numerical case study, including a comparison of different data lengths, a discussion of complexity and a comparison with set membership estimation\n",
    "authors": [
      "Stefan Wildhagen",
      "Julian Berberich",
      "Michael Hertneck",
      "Frank Allg\u00f6wer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2101.00903"
  },
  {
    "id": "arXiv:2101.04819",
    "title": "Parametricity for Nested Types and GADTs",
    "abstract": "Parametricity for Nested Types and GADTs",
    "descriptor": "",
    "authors": [
      "Patricia Johann",
      "Enrico Ghiorzi"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2101.04819"
  },
  {
    "id": "arXiv:2102.00218",
    "title": "Estimating the Unique Information of Continuous Variables",
    "abstract": "Estimating the Unique Information of Continuous Variables",
    "descriptor": "",
    "authors": [
      "Ari Pakman",
      "Amin Nejatbakhsh",
      "Dar Gilboa",
      "Abdullah Makkeh",
      "Luca Mazzucato",
      "Michael Wibral",
      "Elad Schneidman"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2102.00218"
  },
  {
    "id": "arXiv:2102.00865",
    "title": "Global types and event structure semantics for asynchronous multiparty  sessions",
    "abstract": "Global types and event structure semantics for asynchronous multiparty  sessions",
    "descriptor": "",
    "authors": [
      "Ilaria Castellani",
      "Mariangiola Dezani-Ciancaglini",
      "Paola Giannini"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2102.00865"
  },
  {
    "id": "arXiv:2102.01854",
    "title": "Provably Secure Federated Learning against Malicious Clients",
    "abstract": "Comments: Accepted by AAAI-21. For slides, see this https URL . For the talk, see this https URL",
    "descriptor": "\nComments: Accepted by AAAI-21. For slides, see this https URL . For the talk, see this https URL\n",
    "authors": [
      "Xiaoyu Cao",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.01854"
  },
  {
    "id": "arXiv:2102.02956",
    "title": "DetectorGuard: Provably Securing Object Detectors against Localized  Patch Hiding Attacks",
    "abstract": "DetectorGuard: Provably Securing Object Detectors against Localized  Patch Hiding Attacks",
    "descriptor": "",
    "authors": [
      "Chong Xiang",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.02956"
  },
  {
    "id": "arXiv:2102.04259",
    "title": "Concentration of Non-Isotropic Random Tensors with Applications to  Learning and Empirical Risk Minimization",
    "abstract": "Concentration of Non-Isotropic Random Tensors with Applications to  Learning and Empirical Risk Minimization",
    "descriptor": "",
    "authors": [
      "Mathieu Even",
      "Laurent Massouli\u00e9"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2102.04259"
  },
  {
    "id": "arXiv:2102.04426",
    "title": "Arbitrary Conditional Distributions with Energy",
    "abstract": "Comments: Accepted at NeurIPS 2021",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Ryan R. Strauss",
      "Junier B. Oliva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.04426"
  },
  {
    "id": "arXiv:2102.05591",
    "title": "On the Distribution of the Sum of Double-Nakagami-m Random Vectors and  Application in Randomly Reconfigurable Surfaces",
    "abstract": "Comments: 11 pages, 6 figures",
    "descriptor": "\nComments: 11 pages, 6 figures\n",
    "authors": [
      "Sotiris A. Tegos",
      "Dimitrios Tyrovolas",
      "Panagiotis D. Diamantoulakis",
      "Christos K. Liaskos",
      "George K. Karagiannidis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.05591"
  },
  {
    "id": "arXiv:2102.05855",
    "title": "Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient  Descent",
    "abstract": "Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient  Descent",
    "descriptor": "",
    "authors": [
      "Rishav Chourasia",
      "Jiayuan Ye",
      "Reza Shokri"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.05855"
  },
  {
    "id": "arXiv:2102.06062",
    "title": "Deep Learning with Label Differential Privacy",
    "abstract": "Comments: NeurIPS 2021; 29 pages, 6 figures",
    "descriptor": "\nComments: NeurIPS 2021; 29 pages, 6 figures\n",
    "authors": [
      "Badih Ghazi",
      "Noah Golowich",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Chiyuan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.06062"
  },
  {
    "id": "arXiv:2102.07804",
    "title": "Scaling Up Exact Neural Network Compression by ReLU Stability",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Thiago Serra",
      "Xin Yu",
      "Abhinav Kumar",
      "Srikumar Ramalingam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2102.07804"
  },
  {
    "id": "arXiv:2102.08098",
    "title": "GradInit: Learning to Initialize Neural Networks for Stable and  Efficient Training",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Chen Zhu",
      "Renkun Ni",
      "Zheng Xu",
      "Kezhi Kong",
      "W. Ronny Huang",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2102.08098"
  },
  {
    "id": "arXiv:2102.08473",
    "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model  Pretraining",
    "abstract": "Comments: NeurIPS 2021. (Code and Models: this https URL)",
    "descriptor": "\nComments: NeurIPS 2021. (Code and Models: this https URL)\n",
    "authors": [
      "Yu Meng",
      "Chenyan Xiong",
      "Payal Bajaj",
      "Saurabh Tiwary",
      "Paul Bennett",
      "Jiawei Han",
      "Xia Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.08473"
  },
  {
    "id": "arXiv:2102.11069",
    "title": "A PAC-Bayes Analysis of Adversarial Robustness",
    "abstract": "A PAC-Bayes Analysis of Adversarial Robustness",
    "descriptor": "",
    "authors": [
      "Paul Viallard",
      "Guillaume Vidot",
      "Amaury Habrard",
      "Emilie Morvant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11069"
  },
  {
    "id": "arXiv:2102.11628",
    "title": "FINE Samples for Learning with Noisy Labels",
    "abstract": "Comments: The Thirty-Fifth Annual Conference on Neural Information Processing Systems (NeurIPS 2021)",
    "descriptor": "\nComments: The Thirty-Fifth Annual Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Taehyeon Kim",
      "Jongwoo Ko",
      "Sangwook Cho",
      "Jinhwan Choi",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11628"
  },
  {
    "id": "arXiv:2102.11860",
    "title": "Automated Discovery of Adaptive Attacks on Adversarial Defenses",
    "abstract": "Comments: 21 pages, 3 figures, 10 tables. NeurIPS2021",
    "descriptor": "\nComments: 21 pages, 3 figures, 10 tables. NeurIPS2021\n",
    "authors": [
      "Chengyuan Yao",
      "Pavol Bielik",
      "Petar Tsankov",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11860"
  },
  {
    "id": "arXiv:2102.12033",
    "title": "Self-Diagnosing GAN: Diagnosing Underrepresented Samples in Generative  Adversarial Networks",
    "abstract": "Comments: Accepted to NeurIPS 2021",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Jinhee Lee",
      "Haeri Kim",
      "Youngkyu Hong",
      "Hye Won Chung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.12033"
  },
  {
    "id": "arXiv:2102.13156",
    "title": "Physics-Integrated Variational Autoencoders for Robust and Interpretable  Generative Modeling",
    "abstract": "Physics-Integrated Variational Autoencoders for Robust and Interpretable  Generative Modeling",
    "descriptor": "",
    "authors": [
      "Naoya Takeishi",
      "Alexandros Kalousis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13156"
  },
  {
    "id": "arXiv:2102.13380",
    "title": "A novel notion of barycenter for probability distributions based on  optimal weak mass transport",
    "abstract": "A novel notion of barycenter for probability distributions based on  optimal weak mass transport",
    "descriptor": "",
    "authors": [
      "Elsa Cazelles",
      "Felipe Tobar",
      "Joaqu\u00edn Fontbona"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.13380"
  },
  {
    "id": "arXiv:2102.13647",
    "title": "Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To  Game",
    "abstract": "Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy To  Game",
    "descriptor": "",
    "authors": [
      "Alexander G. Reisach",
      "Christof Seiler",
      "Sebastian Weichwald"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2102.13647"
  },
  {
    "id": "arXiv:2103.00928",
    "title": "A CPG-Based Agile and Versatile Locomotion Framework Using Proximal  Symmetry Loss",
    "abstract": "A CPG-Based Agile and Versatile Locomotion Framework Using Proximal  Symmetry Loss",
    "descriptor": "",
    "authors": [
      "Mohammadreza Kasaei",
      "Miguel Abreu",
      "Nuno Lau",
      "Artur Pereira",
      "Luis Paulo Reis"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.00928"
  },
  {
    "id": "arXiv:2103.08902",
    "title": "Differentiable Learning Under Triage",
    "abstract": "Differentiable Learning Under Triage",
    "descriptor": "",
    "authors": [
      "Nastaran Okati",
      "Abir De",
      "Manuel Gomez-Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.08902"
  },
  {
    "id": "arXiv:2103.10211",
    "title": "Space-Time Crop & Attend: Improving Cross-modal Video Representation  Learning",
    "abstract": "Comments: Accepted to ICCV 2021. Code at this https URL",
    "descriptor": "\nComments: Accepted to ICCV 2021. Code at this https URL\n",
    "authors": [
      "Mandela Patrick",
      "Yuki M. Asano",
      "Bernie Huang",
      "Ishan Misra",
      "Florian Metze",
      "Joao Henriques",
      "Andrea Vedaldi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.10211"
  },
  {
    "id": "arXiv:2103.11665",
    "title": "New constructions of MDS self-dual and self-orthogonal codes via GRS  codes",
    "abstract": "New constructions of MDS self-dual and self-orthogonal codes via GRS  codes",
    "descriptor": "",
    "authors": [
      "Ziteng Huang",
      "Weijun Fang",
      "Fang-Wei Fu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2103.11665"
  },
  {
    "id": "arXiv:2103.12513",
    "title": "On gray-box modeling for virtual flow metering",
    "abstract": "Comments: 37 pages, 28 figures",
    "descriptor": "\nComments: 37 pages, 28 figures\n",
    "authors": [
      "Mathilde Hotvedt",
      "Bjarne Grimstad",
      "Dag Ljungquist",
      "Lars Imsland"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.12513"
  },
  {
    "id": "arXiv:2103.14077",
    "title": "Nearly Horizon-Free Offline Reinforcement Learning",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Tongzheng Ren",
      "Jialian Li",
      "Bo Dai",
      "Simon S. Du",
      "Sujay Sanghavi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.14077"
  },
  {
    "id": "arXiv:2103.14158",
    "title": "InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform  Inversion",
    "abstract": "InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform  Inversion",
    "descriptor": "",
    "authors": [
      "Qili Zeng",
      "Shihang Feng",
      "Brendt Wohlberg",
      "Youzuo Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2103.14158"
  },
  {
    "id": "arXiv:2103.14353",
    "title": "Improved stability conditions for systems under aperiodic sampling:  model- and data-based analysis",
    "abstract": "Comments: 8 pages, 1 figure, 1 table. Final Version accepted for CDC",
    "descriptor": "\nComments: 8 pages, 1 figure, 1 table. Final Version accepted for CDC\n",
    "authors": [
      "Stefan Wildhagen",
      "Julian Berberich",
      "Matthias Hirche",
      "Frank Allg\u00f6wer"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.14353"
  },
  {
    "id": "arXiv:2103.16340",
    "title": "Scheduling in the Secretary Model",
    "abstract": "Scheduling in the Secretary Model",
    "descriptor": "",
    "authors": [
      "Susanne Albers",
      "Maximilian Janke"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2103.16340"
  },
  {
    "id": "arXiv:2104.00304",
    "title": "Log-domain decoding of quantum LDPC codes over binary finite fields",
    "abstract": "Comments: correct the definition of {\\cal S}^\\perp on page 4 and equation (17)",
    "descriptor": "\nComments: correct the definition of {\\cal S}^\\perp on page 4 and equation (17)\n",
    "authors": [
      "Ching-Yi Lai",
      "Kao-Yueh Kuo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2104.00304"
  },
  {
    "id": "arXiv:2104.02446",
    "title": "Upper paired domination versus upper domination",
    "abstract": "Upper paired domination versus upper domination",
    "descriptor": "",
    "authors": [
      "Hadi Alizadeh",
      "Didem G\u00f6z\u00fcpek"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2104.02446"
  },
  {
    "id": "arXiv:2104.03155",
    "title": "Human-robot collaborative object transfer using human motion prediction  based on Cartesian pose Dynamic Movement Primitives",
    "abstract": "Human-robot collaborative object transfer using human motion prediction  based on Cartesian pose Dynamic Movement Primitives",
    "descriptor": "",
    "authors": [
      "Antonis Sidiropoulos",
      "Yiannis Karayiannidis",
      "Zoe Doulgeri"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2104.03155"
  },
  {
    "id": "arXiv:2104.04646",
    "title": "DeepSITH: Efficient Learning via Decomposition of What and When Across  Time Scales",
    "abstract": "DeepSITH: Efficient Learning via Decomposition of What and When Across  Time Scales",
    "descriptor": "",
    "authors": [
      "Brandon Jacques",
      "Zoran Tiganj",
      "Marc W. Howard",
      "Per B. Sederberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.04646"
  },
  {
    "id": "arXiv:2104.05857",
    "title": "From partners to populations: A hierarchical Bayesian account of  coordination and convention",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Robert D. Hawkins",
      "Michael Franke",
      "Michael C. Frank",
      "Adele E. Goldberg",
      "Kenny Smith",
      "Thomas L. Griffiths",
      "Noah D. Goodman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.05857"
  },
  {
    "id": "arXiv:2104.12112",
    "title": "Improved Analysis and Rates for Variance Reduction under  Without-replacement Sampling Orders",
    "abstract": "Comments: Accepted by NeurIPS 2021",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Xinmeng Huang",
      "Kun Yuan",
      "Xianghui Mao",
      "Wotao Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2104.12112"
  },
  {
    "id": "arXiv:2104.14113",
    "title": "Regret Bounds for Gaussian-Process Optimization in Large Domains",
    "abstract": "Regret Bounds for Gaussian-Process Optimization in Large Domains",
    "descriptor": "",
    "authors": [
      "Manuel W\u00fcthrich",
      "Bernhard Sch\u00f6lkopf",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.14113"
  },
  {
    "id": "arXiv:2105.00642",
    "title": "One Model to Rule them All: Towards Zero-Shot Learning for Databases",
    "abstract": "One Model to Rule them All: Towards Zero-Shot Learning for Databases",
    "descriptor": "",
    "authors": [
      "Benjamin Hilprecht",
      "Carsten Binnig"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2105.00642"
  },
  {
    "id": "arXiv:2105.00853",
    "title": "An isogeometric boundary element method for three-dimensional  doubly-periodic layered structures in electromagnetics",
    "abstract": "Comments: First revision submitted to the journal `Engineering Analysis with Boundary Elements'",
    "descriptor": "\nComments: First revision submitted to the journal `Engineering Analysis with Boundary Elements'\n",
    "authors": [
      "Toru Takahashi",
      "Tetsuro Hirai",
      "Hiroshi Isakari",
      "Toshiro Matsumoto"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2105.00853"
  },
  {
    "id": "arXiv:2105.00949",
    "title": "CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient  Object Detection",
    "abstract": "Comments: 6 pages, 4 figures, 2 tables; Project Page: this https URL",
    "descriptor": "\nComments: 6 pages, 4 figures, 2 tables; Project Page: this https URL\n",
    "authors": [
      "Yi Zhang",
      "Lu Zhang",
      "Wassim Hamidouche",
      "Olivier Deforges"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.00949"
  },
  {
    "id": "arXiv:2105.01308",
    "title": "Defeating Super-Reactive Jammers With Deception Strategy: Modeling,  Signal Detection, and Performance Analysis",
    "abstract": "Comments: 30 pages",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Nguyen Van Huynh",
      "Diep N. Nguyen",
      "Dinh Thai Hoang",
      "Thang X. Vu",
      "Eryk Dutkiewicz",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.01308"
  },
  {
    "id": "arXiv:2105.05643",
    "title": "PoseContrast: Class-Agnostic Object Viewpoint Estimation in the Wild  with Pose-Aware Contrastive Learning",
    "abstract": "Comments: 3DV 2021 (oral). See project webpage this http URL",
    "descriptor": "\nComments: 3DV 2021 (oral). See project webpage this http URL\n",
    "authors": [
      "Yang Xiao",
      "Yuming Du",
      "Renaud Marlet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.05643"
  },
  {
    "id": "arXiv:2105.08195",
    "title": "Parallel Bayesian Optimization of Multiple Noisy Objectives with  Expected Hypervolume Improvement",
    "abstract": "Comments: To appear in Advances in Neural Information Processing Systems 34, 2021. 40 pages. Code is available at this https URL",
    "descriptor": "\nComments: To appear in Advances in Neural Information Processing Systems 34, 2021. 40 pages. Code is available at this https URL\n",
    "authors": [
      "Samuel Daulton",
      "Maximilian Balandat",
      "Eytan Bakshy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.08195"
  },
  {
    "id": "arXiv:2105.08473",
    "title": "An Internal Language for Categories Enriched over Generalised Metric  Spaces",
    "abstract": "Comments: Extended version of the CSL'22 paper \"An Internal Language for Categories Enriched over Generalised Metric Spaces\"",
    "descriptor": "\nComments: Extended version of the CSL'22 paper \"An Internal Language for Categories Enriched over Generalised Metric Spaces\"\n",
    "authors": [
      "Fredrik Dahlqvist",
      "Renato Neves"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2105.08473"
  },
  {
    "id": "arXiv:2105.11868",
    "title": "Detection and blind channel estimation for UAV-aided wireless sensor  networks in smart cities under mobile jamming attack",
    "abstract": "Comments: 19 pages, 14 figures, 6 tables, submitted to IEEE Internet of Things Journal",
    "descriptor": "\nComments: 19 pages, 14 figures, 6 tables, submitted to IEEE Internet of Things Journal\n",
    "authors": [
      "Donatella Darsena",
      "Giacinto Gelli",
      "Ivan Iudice",
      "Francesco Verde"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2105.11868"
  },
  {
    "id": "arXiv:2105.13504",
    "title": "Lattice partition recovery with dyadic CART",
    "abstract": "Lattice partition recovery with dyadic CART",
    "descriptor": "",
    "authors": [
      "Oscar Hernan Madrid Padilla",
      "Yi Yu",
      "Alessandro Rinaldo"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.13504"
  },
  {
    "id": "arXiv:2105.13655",
    "title": "Scheduling Jobs with Stochastic Holding Costs",
    "abstract": "Scheduling Jobs with Stochastic Holding Costs",
    "descriptor": "",
    "authors": [
      "Dabeen Lee",
      "Milan Vojnovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.13655"
  },
  {
    "id": "arXiv:2105.13831",
    "title": "Implicit Regularization in Matrix Sensing via Mirror Descent",
    "abstract": "Implicit Regularization in Matrix Sensing via Mirror Descent",
    "descriptor": "",
    "authors": [
      "Fan Wu",
      "Patrick Rebeschini"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.13831"
  },
  {
    "id": "arXiv:2105.14084",
    "title": "Support vector machines and linear regression coincide with very  high-dimensional features",
    "abstract": "Comments: 34 pages, 9 figures",
    "descriptor": "\nComments: 34 pages, 9 figures\n",
    "authors": [
      "Navid Ardeshir",
      "Clayton Sanford",
      "Daniel Hsu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.14084"
  },
  {
    "id": "arXiv:2105.14734",
    "title": "Dual-stream Network for Visual Recognition",
    "abstract": "Comments: Accepted by NeurIPS 2021",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Mingyuan Mao",
      "Renrui Zhang",
      "Honghui Zheng",
      "Peng Gao",
      "Teli Ma",
      "Yan Peng",
      "Errui Ding",
      "Baochang Zhang",
      "Shumin Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.14734"
  },
  {
    "id": "arXiv:2105.14887",
    "title": "Parameterised Complexity of Propositional Inclusion and Independence  Logic",
    "abstract": "Comments: A revised version",
    "descriptor": "\nComments: A revised version\n",
    "authors": [
      "Yasir Mahmood",
      "Jonni Virtema"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2105.14887"
  },
  {
    "id": "arXiv:2106.00225",
    "title": "Locally Valid and Discriminative Prediction Intervals for Deep Learning  Models",
    "abstract": "Comments: Advances in Neural Information Processing Systems 34 (NeurIPS 2021). Code is available at this https URL",
    "descriptor": "\nComments: Advances in Neural Information Processing Systems 34 (NeurIPS 2021). Code is available at this https URL\n",
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00225"
  },
  {
    "id": "arXiv:2106.00661",
    "title": "Reward is enough for convex MDPs",
    "abstract": "Reward is enough for convex MDPs",
    "descriptor": "",
    "authors": [
      "Tom Zahavy",
      "Brendan O'Donoghue",
      "Guillaume Desjardins",
      "Satinder Singh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.00661"
  },
  {
    "id": "arXiv:2106.00666",
    "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through  Object Detection",
    "abstract": "Comments: NeurIPS 2021 Camera Ready",
    "descriptor": "\nComments: NeurIPS 2021 Camera Ready\n",
    "authors": [
      "Yuxin Fang",
      "Bencheng Liao",
      "Xinggang Wang",
      "Jiemin Fang",
      "Jiyang Qi",
      "Rui Wu",
      "Jianwei Niu",
      "Wenyu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.00666"
  },
  {
    "id": "arXiv:2106.00762",
    "title": "A/B Testing for Recommender Systems in a Two-sided Marketplace",
    "abstract": "A/B Testing for Recommender Systems in a Two-sided Marketplace",
    "descriptor": "",
    "authors": [
      "Preetam Nandy",
      "Divya Venugopalan",
      "Chun Lo",
      "Shaunak Chatterjee"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.00762"
  },
  {
    "id": "arXiv:2106.01798",
    "title": "Implicit MLE: Backpropagating Through Discrete Exponential Family  Distributions",
    "abstract": "Comments: NeurIPS 2021 camera-ready; repo: this https URL",
    "descriptor": "\nComments: NeurIPS 2021 camera-ready; repo: this https URL\n",
    "authors": [
      "Mathias Niepert",
      "Pasquale Minervini",
      "Luca Franceschi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.01798"
  },
  {
    "id": "arXiv:2106.02383",
    "title": "PoDT: A Secure Multi-chains Consensus Scheme Against Diverse Miners  Behaviors Attacks in Blockchain Networks",
    "abstract": "Comments: 14 pages, 17 figures",
    "descriptor": "\nComments: 14 pages, 17 figures\n",
    "authors": [
      "Jingyu Feng",
      "Tao Wang",
      "Wenbo Zhang",
      "Teng Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.02383"
  },
  {
    "id": "arXiv:2106.02506",
    "title": "High Order Semi-implicit WENO Schemes for All Mach Full Euler System of  Gas Dynamics",
    "abstract": "High Order Semi-implicit WENO Schemes for All Mach Full Euler System of  Gas Dynamics",
    "descriptor": "",
    "authors": [
      "Sebastiano Boscarino",
      "Jing-Mei Qiu",
      "Giovanni Russo",
      "Tao Xiong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.02506"
  },
  {
    "id": "arXiv:2106.02997",
    "title": "Causal Abstractions of Neural Networks",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Atticus Geiger",
      "Hanson Lu",
      "Thomas Icard",
      "Christopher Potts"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02997"
  },
  {
    "id": "arXiv:2106.03257",
    "title": "Structured Reordering for Modeling Latent Alignments in Sequence  Transduction",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03257"
  },
  {
    "id": "arXiv:2106.03542",
    "title": "How Tight Can PAC-Bayes be in the Small Data Regime?",
    "abstract": "Comments: Accepted for publication at Neural Information Processing Systems 2021",
    "descriptor": "\nComments: Accepted for publication at Neural Information Processing Systems 2021\n",
    "authors": [
      "Andrew Y. K. Foong",
      "Wessel P. Bruinsma",
      "David R. Burt",
      "Richard E. Turner"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03542"
  },
  {
    "id": "arXiv:2106.03721",
    "title": "OoD-Bench: Benchmarking and Understanding Out-of-Distribution  Generalization Datasets and Algorithms",
    "abstract": "OoD-Bench: Benchmarking and Understanding Out-of-Distribution  Generalization Datasets and Algorithms",
    "descriptor": "",
    "authors": [
      "Nanyang Ye",
      "Kaican Li",
      "Lanqing Hong",
      "Haoyue Bai",
      "Yiting Chen",
      "Fengwei Zhou",
      "Zhenguo Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03721"
  },
  {
    "id": "arXiv:2106.03827",
    "title": "Stateful Strategic Regression",
    "abstract": "Comments: In the thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)",
    "descriptor": "\nComments: In the thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Keegan Harris",
      "Hoda Heidari",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2106.03827"
  },
  {
    "id": "arXiv:2106.03893",
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "abstract": "Comments: Accepted in Proceedings of NeurIPS 2021",
    "descriptor": "\nComments: Accepted in Proceedings of NeurIPS 2021\n",
    "authors": [
      "Devin Kreuzer",
      "Dominique Beaini",
      "William L. Hamilton",
      "Vincent L\u00e9tourneau",
      "Prudencio Tossou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03893"
  },
  {
    "id": "arXiv:2106.03894",
    "title": "Differentiable Quality Diversity",
    "abstract": "Comments: Accepted to NeurIPS 2021 (oral presentation)",
    "descriptor": "\nComments: Accepted to NeurIPS 2021 (oral presentation)\n",
    "authors": [
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.03894"
  },
  {
    "id": "arXiv:2106.04013",
    "title": "The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width  Limit at Initialization",
    "abstract": "The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width  Limit at Initialization",
    "descriptor": "",
    "authors": [
      "Mufan Bill Li",
      "Mihai Nica",
      "Daniel M. Roy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04013"
  },
  {
    "id": "arXiv:2106.04152",
    "title": "PlayVirtual: Augmenting Cycle-Consistent Virtual Trajectories for  Reinforcement Learning",
    "abstract": "Comments: Accepted to NeurIPS 2021",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Tao Yu",
      "Cuiling Lan",
      "Wenjun Zeng",
      "Mingxiao Feng",
      "Zhizheng Zhang",
      "Zhibo Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.04152"
  },
  {
    "id": "arXiv:2106.04243",
    "title": "Parameter Inference with Bifurcation Diagrams",
    "abstract": "Parameter Inference with Bifurcation Diagrams",
    "descriptor": "",
    "authors": [
      "Gregory Szep",
      "Neil Dalchau",
      "Attila Csikasz-Nagy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2106.04243"
  },
  {
    "id": "arXiv:2106.04379",
    "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Cameron Allen",
      "Neev Parikh",
      "Omer Gottesman",
      "George Konidaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.04379"
  },
  {
    "id": "arXiv:2106.04443",
    "title": "Robust Generalization despite Distribution Shift via Minimum  Discriminating Information",
    "abstract": "Comments: 23 pages, 4 figures",
    "descriptor": "\nComments: 23 pages, 4 figures\n",
    "authors": [
      "Tobias Sutter",
      "Andreas Krause",
      "Daniel Kuhn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04443"
  },
  {
    "id": "arXiv:2106.04759",
    "title": "Communication-efficient SGD: From Local SGD to One-Shot Averaging",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2006.02582",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2006.02582\n",
    "authors": [
      "Artin Spiridonoff",
      "Alex Olshevsky",
      "Ioannis Ch. Paschalidis"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.04759"
  },
  {
    "id": "arXiv:2106.04765",
    "title": "Predicting Deep Neural Network Generalization with Perturbation Response  Curves",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Yair Schiff",
      "Brian Quanz",
      "Payel Das",
      "Pin-Yu Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.04765"
  },
  {
    "id": "arXiv:2106.04772",
    "title": "HyCA: A Hybrid Computing Architecture for Fault Tolerant Deep Learning",
    "abstract": "HyCA: A Hybrid Computing Architecture for Fault Tolerant Deep Learning",
    "descriptor": "",
    "authors": [
      "Cheng Liu",
      "Cheng Chu",
      "Dawen Xu",
      "Ying Wang",
      "Qianlong Wang",
      "Huawei Li",
      "Xiaowei Li",
      "Kwang-Ting Cheng"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2106.04772"
  },
  {
    "id": "arXiv:2106.05445",
    "title": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive  Sample Size Approach",
    "abstract": "Exploiting Local Convergence of Quasi-Newton Methods Globally: Adaptive  Sample Size Approach",
    "descriptor": "",
    "authors": [
      "Qiujiang Jin",
      "Aryan Mokhtari"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05445"
  },
  {
    "id": "arXiv:2106.05480",
    "title": "Lower Bounds on Metropolized Sampling Methods for Well-Conditioned  Distributions",
    "abstract": "Comments: 46 pages, 1 figure. This version removes Gaussian upper bound claim",
    "descriptor": "\nComments: 46 pages, 1 figure. This version removes Gaussian upper bound claim\n",
    "authors": [
      "Yin Tat Lee",
      "Ruoqi Shen",
      "Kevin Tian"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05480"
  },
  {
    "id": "arXiv:2106.05931",
    "title": "Score-based Generative Modeling in Latent Space",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Arash Vahdat",
      "Karsten Kreis",
      "Jan Kautz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05931"
  },
  {
    "id": "arXiv:2106.06098",
    "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms",
    "abstract": "Comments: 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia",
    "descriptor": "\nComments: 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia\n",
    "authors": [
      "Guanya Shi",
      "Kamyar Azizzadenesheli",
      "Michael O'Connell",
      "Soon-Jo Chung",
      "Yisong Yue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.06098"
  },
  {
    "id": "arXiv:2106.06295",
    "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
    "abstract": "Comments: Accepted to NeurIPS 2021",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Kazuki Irie",
      "Imanol Schlag",
      "R\u00f3bert Csord\u00e1s",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06295"
  },
  {
    "id": "arXiv:2106.07009",
    "title": "Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising  without Clean Images",
    "abstract": "Comments: Camera ready version for NeurIPS 2021",
    "descriptor": "\nComments: Camera ready version for NeurIPS 2021\n",
    "authors": [
      "Kwanyoung Kim",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07009"
  },
  {
    "id": "arXiv:2106.07428",
    "title": "Audio Attacks and Defenses against AED Systems -- A Practical Study",
    "abstract": "Audio Attacks and Defenses against AED Systems -- A Practical Study",
    "descriptor": "",
    "authors": [
      "Rodrigo dos Santos",
      "Shirin Nilizadeh"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.07428"
  },
  {
    "id": "arXiv:2106.07644",
    "title": "A Continuized View on Nesterov Acceleration for Stochastic Gradient  Descent and Randomized Gossip",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2102.06035",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2102.06035\n",
    "authors": [
      "Mathieu Even",
      "Rapha\u00ebl Berthier",
      "Francis Bach",
      "Nicolas Flammarion",
      "Pierre Gaillard",
      "Hadrien Hendrikx",
      "Laurent Massouli\u00e9",
      "Adrien Taylor"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07644"
  },
  {
    "id": "arXiv:2106.07759",
    "title": "Kaizen: Continuously improving teacher using Exponential Moving Average  for semi-supervised speech recognition",
    "abstract": "Comments: Updated with camera ready version",
    "descriptor": "\nComments: Updated with camera ready version\n",
    "authors": [
      "Vimal Manohar",
      "Tatiana Likhomanenko",
      "Qiantong Xu",
      "Wei-Ning Hsu",
      "Ronan Collobert",
      "Yatharth Saraf",
      "Geoffrey Zweig",
      "Abdelrahman Mohamed"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07759"
  },
  {
    "id": "arXiv:2106.07914",
    "title": "Control Variates for Slate Off-Policy Evaluation",
    "abstract": "Control Variates for Slate Off-Policy Evaluation",
    "descriptor": "",
    "authors": [
      "Nikos Vlassis",
      "Ashok Chandrashekar",
      "Fernando Amat Gil",
      "Nathan Kallus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.07914"
  },
  {
    "id": "arXiv:2106.08601",
    "title": "Self-Supervised GANs with Label Augmentation",
    "abstract": "Comments: Accepted at NeurIPS 2021",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Liang Hou",
      "Huawei Shen",
      "Qi Cao",
      "Xueqi Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08601"
  },
  {
    "id": "arXiv:2106.08769",
    "title": "Knowledge-Adaptation Priors",
    "abstract": "Knowledge-Adaptation Priors",
    "descriptor": "",
    "authors": [
      "Mohammad Emtiyaz Khan",
      "Siddharth Swaroop"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08769"
  },
  {
    "id": "arXiv:2106.08847",
    "title": "NOMA Power Minimization of Downlink Spectrum Slicing for eMBB and URLLC  Users",
    "abstract": "Comments: This work has been submitted to the IEEE WCNC for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE WCNC for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Fabio Saggese",
      "Marco Moretti",
      "Petar Popovski"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2106.08847"
  },
  {
    "id": "arXiv:2106.09526",
    "title": "Exploring the Properties and Evolution of Neural Network Eigenspaces  during Training",
    "abstract": "Exploring the Properties and Evolution of Neural Network Eigenspaces  during Training",
    "descriptor": "",
    "authors": [
      "Mats L. Richter",
      "Leila Malihi",
      "Anne-Kathrin Patricia Windler",
      "Ulf Krumnack"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.09526"
  },
  {
    "id": "arXiv:2106.09620",
    "title": "Disentangling Identifiable Features from Noisy Data with Structured  Nonlinear ICA",
    "abstract": "Comments: Accepted for publication at NeurIPS 2021",
    "descriptor": "\nComments: Accepted for publication at NeurIPS 2021\n",
    "authors": [
      "Hermanni H\u00e4lv\u00e4",
      "Sylvain Le Corff",
      "Luc Leh\u00e9ricy",
      "Jonathan So",
      "Yongjie Zhu",
      "Elisabeth Gassiat",
      "Aapo Hyvarinen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.09620"
  },
  {
    "id": "arXiv:2106.10575",
    "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter  Optimization",
    "abstract": "Comments: Accepted at NeurIPS 2021",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Ondrej Bohdal",
      "Yongxin Yang",
      "Timothy Hospedales"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.10575"
  },
  {
    "id": "arXiv:2106.11520",
    "title": "BARTScore: Evaluating Generated Text as Text Generation",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Weizhe Yuan",
      "Graham Neubig",
      "Pengfei Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.11520"
  },
  {
    "id": "arXiv:2106.12620",
    "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision  Transformers",
    "abstract": "Comments: Accepted in NeurIPS 2021",
    "descriptor": "\nComments: Accepted in NeurIPS 2021\n",
    "authors": [
      "Bowen Pan",
      "Rameswar Panda",
      "Yifan Jiang",
      "Zhangyang Wang",
      "Rogerio Feris",
      "Aude Oliva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.12620"
  },
  {
    "id": "arXiv:2106.12674",
    "title": "Fairness via Representation Neutralization",
    "abstract": "Comments: Accepted by NeurIPS 2021",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Mengnan Du",
      "Subhabrata Mukherjee",
      "Guanchu Wang",
      "Ruixiang Tang",
      "Ahmed Hassan Awadallah",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.12674"
  },
  {
    "id": "arXiv:2106.13008",
    "title": "Autoformer: Decomposition Transformers with Auto-Correlation for  Long-Term Series Forecasting",
    "abstract": "Autoformer: Decomposition Transformers with Auto-Correlation for  Long-Term Series Forecasting",
    "descriptor": "",
    "authors": [
      "Haixu Wu",
      "Jiehui Xu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.13008"
  },
  {
    "id": "arXiv:2106.13430",
    "title": "Subgraph Federated Learning with Missing Neighbor Generation",
    "abstract": "Comments: Accepted to NeurIPS 2021 (spotlight presentation)",
    "descriptor": "\nComments: Accepted to NeurIPS 2021 (spotlight presentation)\n",
    "authors": [
      "Ke Zhang",
      "Carl Yang",
      "Xiaoxiao Li",
      "Lichao Sun",
      "Siu Ming Yiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2106.13430"
  },
  {
    "id": "arXiv:2106.14326",
    "title": "Last-iterate Convergence in Extensive-Form Games",
    "abstract": "Last-iterate Convergence in Extensive-Form Games",
    "descriptor": "",
    "authors": [
      "Chung-Wei Lee",
      "Christian Kroer",
      "Haipeng Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.14326"
  },
  {
    "id": "arXiv:2106.14942",
    "title": "Fast Training of Neural Lumigraph Representations using Meta Learning",
    "abstract": "Comments: Project website: this http URL",
    "descriptor": "\nComments: Project website: this http URL\n",
    "authors": [
      "Alexander W. Bergman",
      "Petr Kellnhofer",
      "Gordon Wetzstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.14942"
  },
  {
    "id": "arXiv:2106.15481",
    "title": "Interactive Dimensionality Reduction for Comparative Analysis",
    "abstract": "Comments: This is the author's version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics. The final version of this record is available at: 10.1109/TVCG.2021.3114807",
    "descriptor": "\nComments: This is the author's version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics. The final version of this record is available at: 10.1109/TVCG.2021.3114807\n",
    "authors": [
      "Takanori Fujiwara",
      "Xinhai Wei",
      "Jian Zhao",
      "Kwan-Liu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.15481"
  },
  {
    "id": "arXiv:2106.15577",
    "title": "As easy as APC: overcoming missing data and class imbalance in time  series with self-supervised learning",
    "abstract": "Comments: 15 pages",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Fiorella Wever",
      "T. Anderson Keller",
      "Victor Garcia",
      "Laura Symul"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.15577"
  },
  {
    "id": "arXiv:2106.15842",
    "title": "Dual Aspect Self-Attention based on Transformer for Remaining Useful  Life Prediction",
    "abstract": "Dual Aspect Self-Attention based on Transformer for Remaining Useful  Life Prediction",
    "descriptor": "",
    "authors": [
      "Zhizheng Zhang",
      "Wen Song",
      "Qiqiang Li"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.15842"
  },
  {
    "id": "arXiv:2107.00166",
    "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win  the Jackpot?",
    "abstract": "Comments: NeurIPS 2021 camera ready",
    "descriptor": "\nComments: NeurIPS 2021 camera ready\n",
    "authors": [
      "Xiaolong Ma",
      "Geng Yuan",
      "Xuan Shen",
      "Tianlong Chen",
      "Xuxi Chen",
      "Xiaohan Chen",
      "Ning Liu",
      "Minghai Qin",
      "Sijia Liu",
      "Zhangyang Wang",
      "Yanzhi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.00166"
  },
  {
    "id": "arXiv:2107.01003",
    "title": "PI$^2$ Parameters",
    "abstract": "PI$^2$ Parameters",
    "descriptor": "",
    "authors": [
      "Bob Briscoe"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2107.01003"
  },
  {
    "id": "arXiv:2107.01163",
    "title": "Unveiling the structure of wide flat minima in neural networks",
    "abstract": "Comments: 15 pages, 8 figures",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Carlo Baldassi",
      "Clarissa Lauditi",
      "Enrico M. Malatesta",
      "Gabriele Perugini",
      "Riccardo Zecchina"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2107.01163"
  },
  {
    "id": "arXiv:2107.02776",
    "title": "Counterfactual Explanations in Sequential Decision Making Under  Uncertainty",
    "abstract": "Comments: To appear at NeurIPS 2021",
    "descriptor": "\nComments: To appear at NeurIPS 2021\n",
    "authors": [
      "Stratis Tsirtsis",
      "Abir De",
      "Manuel Gomez-Rodriguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.02776"
  },
  {
    "id": "arXiv:2107.03263",
    "title": "Episodic Bandits with Stochastic Experts",
    "abstract": "Episodic Bandits with Stochastic Experts",
    "descriptor": "",
    "authors": [
      "Nihal Sharma",
      "Soumya Basu",
      "Karthikeyan Shanmugam",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.03263"
  },
  {
    "id": "arXiv:2107.03502",
    "title": "CSDI: Conditional Score-based Diffusion Models for Probabilistic Time  Series Imputation",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Yusuke Tashiro",
      "Jiaming Song",
      "Yang Song",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.03502"
  },
  {
    "id": "arXiv:2107.04165",
    "title": "Being Together in Place as a Catalyst for Scientific Advance",
    "abstract": "Being Together in Place as a Catalyst for Scientific Advance",
    "descriptor": "",
    "authors": [
      "Eamon Duede",
      "Misha Teplitskiy",
      "Karim Lakhani",
      "James Evans"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2107.04165"
  },
  {
    "id": "arXiv:2107.04770",
    "title": "Computer Vision-assisted Single-antenna and Single-anchor RSSI  Localization Harnessing Dynamic Blockage Events",
    "abstract": "Comments: Submitted to IEEE Sensors journal",
    "descriptor": "\nComments: Submitted to IEEE Sensors journal\n",
    "authors": [
      "Tomoya Sunami",
      "Sohei Itahara",
      "Yusuke Koda",
      "Takayuki Nishio",
      "Koji Yamamoto"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2107.04770"
  },
  {
    "id": "arXiv:2107.05201",
    "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors  to Improve Covariance Matrix Estimation",
    "abstract": "Comments: Published at ICAIF'21: ACM International Conference on AI in Finance",
    "descriptor": "\nComments: Published at ICAIF'21: ACM International Conference on AI in Finance\n",
    "authors": [
      "Hengxu Lin",
      "Dong Zhou",
      "Weiqing Liu",
      "Jiang Bian"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.05201"
  },
  {
    "id": "arXiv:2107.06106",
    "title": "Conservative Offline Distributional Reinforcement Learning",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Yecheng Jason Ma",
      "Dinesh Jayaraman",
      "Osbert Bastani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.06106"
  },
  {
    "id": "arXiv:2107.06754",
    "title": "Dynamic Power Systems Line Outage Detection Using Particle Filter and  Partially Observed States",
    "abstract": "Comments: Under review for IEEE Transactions on Power Systems; 9 pages, 7 figures",
    "descriptor": "\nComments: Under review for IEEE Transactions on Power Systems; 9 pages, 7 figures\n",
    "authors": [
      "Xiaozhou Yang",
      "Nan Chen",
      "Chao Zhai"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2107.06754"
  },
  {
    "id": "arXiv:2107.08698",
    "title": "Compact User-Specific Reconfigurable Intelligent Surfaces for Uplink  Transmission",
    "abstract": "Comments: 13 pages, 11 figures, accepted by IEEE Transactions on Communications",
    "descriptor": "\nComments: 13 pages, 11 figures, accepted by IEEE Transactions on Communications\n",
    "authors": [
      "Kunzan Liu",
      "Zijian Zhang",
      "Linglong Dai",
      "Lajos Hanzo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2107.08698"
  },
  {
    "id": "arXiv:2107.10211",
    "title": "Differentiable Annealed Importance Sampling and the Perils of Gradient  Noise",
    "abstract": "Comments: 23 pages, NeurIPS 2021",
    "descriptor": "\nComments: 23 pages, NeurIPS 2021\n",
    "authors": [
      "Guodong Zhang",
      "Kyle Hsu",
      "Jianing Li",
      "Chelsea Finn",
      "Roger Grosse"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.10211"
  },
  {
    "id": "arXiv:2107.13053",
    "title": "A Highly Linear and Flexible FPGA-Based Time-to-Digital Converter",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Yuanyuan Hua",
      "Danial Chitnis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2107.13053"
  },
  {
    "id": "arXiv:2107.13864",
    "title": "Continuation Newton methods with deflation techniques and quasi-genetic  evolution for global optimization problems",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2103.05829",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2103.05829\n",
    "authors": [
      "Xin-long Luo",
      "Hang Xiao"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2107.13864"
  },
  {
    "id": "arXiv:2107.14449",
    "title": "Synth-by-Reg (SbR): Contrastive learning for synthesis-based  registration of paired images",
    "abstract": "Comments: Best paper award in Simulation and Synthesis in Medical Imaging (SASHIMI) workshop",
    "descriptor": "\nComments: Best paper award in Simulation and Synthesis in Medical Imaging (SASHIMI) workshop\n",
    "authors": [
      "Adri\u00e0 Casamitjana",
      "Matteo Mancini",
      "Juan Eugenio Iglesias"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2107.14449"
  },
  {
    "id": "arXiv:2108.00049",
    "title": "Object-aware Contrastive Learning for Debiased Scene Representation",
    "abstract": "Comments: NeurIPS 2021. First two authors contributed equally",
    "descriptor": "\nComments: NeurIPS 2021. First two authors contributed equally\n",
    "authors": [
      "Sangwoo Mo",
      "Hyunwoo Kang",
      "Kihyuk Sohn",
      "Chun-Liang Li",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.00049"
  },
  {
    "id": "arXiv:2108.01099",
    "title": "Shift-Robust GNNs: Overcoming the Limitations of Localized Graph  Training Data",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Qi Zhu",
      "Natalia Ponomareva",
      "Jiawei Han",
      "Bryan Perozzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.01099"
  },
  {
    "id": "arXiv:2108.01938",
    "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by  Partial Differential Equations",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Moshe Eliasof",
      "Eldad Haber",
      "Eran Treister"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2108.01938"
  },
  {
    "id": "arXiv:2108.03021",
    "title": "Road Scenes Segmentation Across Different Domains by Disentangling  Latent Representations",
    "abstract": "Comments: 10 pages, 3 supplementary pages, 10 figures, 3 supplementary figures, 2 tables, 1 supplementary table",
    "descriptor": "\nComments: 10 pages, 3 supplementary pages, 10 figures, 3 supplementary figures, 2 tables, 1 supplementary table\n",
    "authors": [
      "Francesco Barbato",
      "Umberto Michieli",
      "Marco Toldo",
      "Pietro Zanuttigh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.03021"
  },
  {
    "id": "arXiv:2108.04055",
    "title": "The Role of Global Labels in Few-Shot Classification and How to Infer  Them",
    "abstract": "Comments: Conference on Neural Information Processing Systems 2021",
    "descriptor": "\nComments: Conference on Neural Information Processing Systems 2021\n",
    "authors": [
      "Ruohan Wang",
      "Massimiliano Pontil",
      "Carlo Ciliberto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.04055"
  },
  {
    "id": "arXiv:2108.04444",
    "title": "SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution  with Skip-Transformer",
    "abstract": "Comments: ICCV 2021 (Oral)",
    "descriptor": "\nComments: ICCV 2021 (Oral)\n",
    "authors": [
      "Peng Xiang",
      "Xin Wen",
      "Yu-Shen Liu",
      "Yan-Pei Cao",
      "Pengfei Wan",
      "Wen Zheng",
      "Zhizhong Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.04444"
  },
  {
    "id": "arXiv:2108.04454",
    "title": "CPNet: Cross-Parallel Network for Efficient Anomaly Detection",
    "abstract": "Comments: 8 pages, 4 figures, The 17th IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS 2021). (Accept)",
    "descriptor": "\nComments: 8 pages, 4 figures, The 17th IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS 2021). (Accept)\n",
    "authors": [
      "Youngsaeng Jin",
      "Jonghwan Hong",
      "David Han",
      "Hanseok Ko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.04454"
  },
  {
    "id": "arXiv:2108.04525",
    "title": "Hierarchical Structural Analysis Method for Complex Equation-oriented  Models",
    "abstract": "Comments: 23 pages, 10 figures",
    "descriptor": "\nComments: 23 pages, 10 figures\n",
    "authors": [
      "Chao Wang",
      "Li Wan",
      "Tifan Xiong",
      "Yuanlong Xie",
      "Shuting Wang",
      "Jianwan Ding",
      "Liping Chen"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2108.04525"
  },
  {
    "id": "arXiv:2108.05574",
    "title": "Implicit Sparse Regularization: The Impact of Depth and Early Stopping",
    "abstract": "Comments: 32 pages, accepted by NeurIPS 2021. arXiv admin note: text overlap with arXiv:1909.05122 by other authors",
    "descriptor": "\nComments: 32 pages, accepted by NeurIPS 2021. arXiv admin note: text overlap with arXiv:1909.05122 by other authors\n",
    "authors": [
      "Jiangyuan Li",
      "Thanh V. Nguyen",
      "Chinmay Hegde",
      "Raymond K. W. Wong"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.05574"
  },
  {
    "id": "arXiv:2108.07127",
    "title": "Active Learning for Massively Parallel Translation of Constrained Text  into Low Resource Languages",
    "abstract": "Active Learning for Massively Parallel Translation of Constrained Text  into Low Resource Languages",
    "descriptor": "",
    "authors": [
      "Zhong Zhou",
      "Alex Waibel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.07127"
  },
  {
    "id": "arXiv:2108.10591",
    "title": "Communication-hiding pipelined BiCGSafe methods for solving large linear  systems",
    "abstract": "Communication-hiding pipelined BiCGSafe methods for solving large linear  systems",
    "descriptor": "",
    "authors": [
      "Viet Q. H. Huynh",
      "Hiroshi Suito"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2108.10591"
  },
  {
    "id": "arXiv:2108.13747",
    "title": "In-body Bionanosensor Localization for Anomaly Detection via Inertial  Positioning and THz Backscattering Communication",
    "abstract": "Comments: 10 pages, 10 figures, submitted to IEEE Transactions on NanoBioscience",
    "descriptor": "\nComments: 10 pages, 10 figures, submitted to IEEE Transactions on NanoBioscience\n",
    "authors": [
      "Jennifer Simonjan",
      "Bige D. Unluturk",
      "Ian F. Akyildiz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2108.13747"
  },
  {
    "id": "arXiv:2109.00456",
    "title": "Weakly-Supervised Surface Crack Segmentation by Generating Pseudo-Labels  using Localization with a Classifier and Thresholding",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Jacob K\u00f6nig",
      "Mark Jenkins",
      "Mike Mannion",
      "Peter Barrie",
      "Gordon Morison"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.00456"
  },
  {
    "id": "arXiv:2109.02145",
    "title": "Temporal Shift Reinforcement Learning",
    "abstract": "Temporal Shift Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Deepak George Thomas",
      "Tichakorn Wongpiromsarn",
      "Ali Jannesari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.02145"
  },
  {
    "id": "arXiv:2109.02229",
    "title": "Efficient Combinatorial Optimization for Word-level Adversarial Textual  Attack",
    "abstract": "Efficient Combinatorial Optimization for Word-level Adversarial Textual  Attack",
    "descriptor": "",
    "authors": [
      "Shengcai Liu",
      "Ning Lu",
      "Cheng Chen",
      "Ke Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2109.02229"
  },
  {
    "id": "arXiv:2109.02515",
    "title": "Efficient diagonalization of symmetric matrices associated with graphs  of small treewidth",
    "abstract": "Efficient diagonalization of symmetric matrices associated with graphs  of small treewidth",
    "descriptor": "",
    "authors": [
      "Martin F\u00fcrer",
      "Carlos Hoppen",
      "Vilmar Trevisan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Symbolic Computation (cs.SC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2109.02515"
  },
  {
    "id": "arXiv:2109.07852",
    "title": "OpenFed: A Comprehensive and Versatile Open-Source Federated Learning  Framework",
    "abstract": "Comments: 18 pages, 3 figures, 1 table",
    "descriptor": "\nComments: 18 pages, 3 figures, 1 table\n",
    "authors": [
      "Dengsheng Chen",
      "Vince Tan",
      "Zhilin Lu",
      "Jie Hu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.07852"
  },
  {
    "id": "arXiv:2109.08075",
    "title": "Field Study in Deploying Restless Multi-Armed Bandits: Assisting  Non-Profits in Improving Maternal and Child Health",
    "abstract": "Field Study in Deploying Restless Multi-Armed Bandits: Assisting  Non-Profits in Improving Maternal and Child Health",
    "descriptor": "",
    "authors": [
      "Aditya Mate",
      "Lovish Madaan",
      "Aparna Taneja",
      "Neha Madhiwalla",
      "Shresth Verma",
      "Gargi Singh",
      "Aparna Hegde",
      "Pradeep Varakantham",
      "Milind Tambe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.08075"
  },
  {
    "id": "arXiv:2109.09314",
    "title": "Investigating the Relationship Between World Development Indicators and  the Occurrence of Disease Outbreaks in the 21st Century: A Case Study",
    "abstract": "Investigating the Relationship Between World Development Indicators and  the Occurrence of Disease Outbreaks in the 21st Century: A Case Study",
    "descriptor": "",
    "authors": [
      "Aboli Marathe",
      "Harsh Sakhrani",
      "Saloni Parekh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.09314"
  },
  {
    "id": "arXiv:2109.12067",
    "title": "Process tomography in general physical theories",
    "abstract": "Comments: 16 + 12 pages, new results added",
    "descriptor": "\nComments: 16 + 12 pages, new results added\n",
    "authors": [
      "Giulio Chiribella"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Logic in Computer Science (cs.LO)",
      "History and Philosophy of Physics (physics.hist-ph)"
    ],
    "url": "https://arxiv.org/abs/2109.12067"
  },
  {
    "id": "arXiv:2109.12380",
    "title": "A Compositional Feature Embedding and Similarity Metric for  Ultra-Fine-Grained Visual Categorization",
    "abstract": "Comments: Accepted by Digital Image Computing Techniques and Applications (DICTA) 2021",
    "descriptor": "\nComments: Accepted by Digital Image Computing Techniques and Applications (DICTA) 2021\n",
    "authors": [
      "Yajie Sun",
      "Miaohua Zhang",
      "Xiaohan Yu",
      "Yi Liao",
      "Yongsheng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.12380"
  },
  {
    "id": "arXiv:2109.12479",
    "title": "A new Lagrange multiplier approach for constructing structure-preserving  schemes, II. bound preserving",
    "abstract": "A new Lagrange multiplier approach for constructing structure-preserving  schemes, II. bound preserving",
    "descriptor": "",
    "authors": [
      "Qing Cheng",
      "Jie Shen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.12479"
  },
  {
    "id": "arXiv:2109.13103",
    "title": "Efficiently solving the thief orienteering problem with a max-min ant  colony optimization approach",
    "abstract": "Efficiently solving the thief orienteering problem with a max-min ant  colony optimization approach",
    "descriptor": "",
    "authors": [
      "Jonatas B. C. Chagas",
      "Markus Wagner"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.13103"
  },
  {
    "id": "arXiv:2109.13394",
    "title": "Compact Redistricting Plans Have Many Spanning Trees",
    "abstract": "Compact Redistricting Plans Have Many Spanning Trees",
    "descriptor": "",
    "authors": [
      "Ariel D. Procaccia",
      "Jamie Tucker-Foltz"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computers and Society (cs.CY)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2109.13394"
  },
  {
    "id": "arXiv:2109.14204",
    "title": "Resource sharing on endogenous networks",
    "abstract": "Resource sharing on endogenous networks",
    "descriptor": "",
    "authors": [
      "Philip Solimine",
      "Luke Boosey"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Social and Information Networks (cs.SI)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2109.14204"
  },
  {
    "id": "arXiv:2109.14567",
    "title": "Implicit Generative Copulas",
    "abstract": "Comments: Accepted at NeurIPS 2021",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Tim Janke",
      "Mohamed Ghanmi",
      "Florian Steinke"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2109.14567"
  },
  {
    "id": "arXiv:2109.15098",
    "title": "Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic  Camera Motion Extraction",
    "abstract": "Comments: Accepted for publication in 2021 AE-CAI Special Issue of the Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization",
    "descriptor": "\nComments: Accepted for publication in 2021 AE-CAI Special Issue of the Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization\n",
    "authors": [
      "Martin Huber",
      "S\u00e9bastien Ourselin",
      "Christos Bergeles",
      "Tom Vercauteren"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.15098"
  },
  {
    "id": "arXiv:2109.15142",
    "title": "Redesigning the Transformer Architecture with Insights from  Multi-particle Dynamical Systems",
    "abstract": "Comments: NeurIPS 2021 (spotlight)",
    "descriptor": "\nComments: NeurIPS 2021 (spotlight)\n",
    "authors": [
      "Subhabrata Dutta",
      "Tanya Gautam",
      "Soumen Chakrabarti",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.15142"
  },
  {
    "id": "arXiv:2110.00137",
    "title": "Iterative Teacher-Aware Learning",
    "abstract": "Iterative Teacher-Aware Learning",
    "descriptor": "",
    "authors": [
      "Luyao Yuan",
      "Dongruo Zhou",
      "Junhong Shen",
      "Jingdong Gao",
      "Jeffrey L. Chen",
      "Quanquan Gu",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.00137"
  },
  {
    "id": "arXiv:2110.01219",
    "title": "Hit and Lead Discovery with Explorative RL and Fragment-based Molecule  Generation",
    "abstract": "Comments: To be published in NeurIPS 2021",
    "descriptor": "\nComments: To be published in NeurIPS 2021\n",
    "authors": [
      "Soojung Yang",
      "Doyeong Hwang",
      "Seul Lee",
      "Seongok Ryu",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01219"
  },
  {
    "id": "arXiv:2110.01823",
    "title": "Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power  of Geometric Transformations",
    "abstract": "Comments: Accepted at NeurIPS 2021; First two authors contributed equally; Includes Supplementary Material",
    "descriptor": "\nComments: Accepted at NeurIPS 2021; First two authors contributed equally; Includes Supplementary Material\n",
    "authors": [
      "Shasha Li",
      "Abhishek Aich",
      "Shitong Zhu",
      "M. Salman Asif",
      "Chengyu Song",
      "Amit K. Roy-Chowdhury",
      "Srikanth V. Krishnamurthy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.01823"
  },
  {
    "id": "arXiv:2110.02424",
    "title": "Spectral Bias in Practice: The Role of Function Frequency in  Generalization",
    "abstract": "Spectral Bias in Practice: The Role of Function Frequency in  Generalization",
    "descriptor": "",
    "authors": [
      "Sara Fridovich-Keil",
      "Raphael Gontijo-Lopes",
      "Rebecca Roelofs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02424"
  },
  {
    "id": "arXiv:2110.02831",
    "title": "Lattice paths with a first return decomposition constrained by the  maximal height of a pattern",
    "abstract": "Comments: 6 pages, 4 tables",
    "descriptor": "\nComments: 6 pages, 4 tables\n",
    "authors": [
      "Jean-Luc Baril",
      "Sergey Kirgizov"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.02831"
  },
  {
    "id": "arXiv:2110.03237",
    "title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport",
    "abstract": "Comments: NeurIPS 2021 camera ready version of paper 4675",
    "descriptor": "\nComments: NeurIPS 2021 camera ready version of paper 4675\n",
    "authors": [
      "Max Daniels",
      "Tyler Maunu",
      "Paul Hand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03237"
  },
  {
    "id": "arXiv:2110.03786",
    "title": "Efficient large-scale image retrieval with deep feature orthogonality  and Hybrid-Swin-Transformers",
    "abstract": "Efficient large-scale image retrieval with deep feature orthogonality  and Hybrid-Swin-Transformers",
    "descriptor": "",
    "authors": [
      "Christof Henkel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03786"
  },
  {
    "id": "arXiv:2110.04361",
    "title": "SubTab: Subsetting Features of Tabular Data for Self-Supervised  Representation Learning",
    "abstract": "Comments: NeurIPS 2021. Code can be found at this https URL",
    "descriptor": "\nComments: NeurIPS 2021. Code can be found at this https URL\n",
    "authors": [
      "Talip Ucar",
      "Ehsan Hajiramezanali",
      "Lindsay Edwards"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04361"
  },
  {
    "id": "arXiv:2110.04745",
    "title": "Reinforcement Learning for Systematic FX Trading",
    "abstract": "Reinforcement Learning for Systematic FX Trading",
    "descriptor": "",
    "authors": [
      "Gabriel Borrageiro",
      "Nick Firoozye",
      "Paolo Barucca"
    ],
    "subjectives": [
      "Trading and Market Microstructure (q-fin.TR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04745"
  },
  {
    "id": "arXiv:2110.05529",
    "title": "HUNTER: AI based Holistic Resource Management for Sustainable Cloud  Computing",
    "abstract": "Comments: Accepted in Elsevier Journal of Systems and Software, 2021",
    "descriptor": "\nComments: Accepted in Elsevier Journal of Systems and Software, 2021\n",
    "authors": [
      "Shreshth Tuli",
      "Sukhpal Singh Gill",
      "Minxian Xu",
      "Peter Garraghan",
      "Rami Bahsoon",
      "Scharam Dustdar",
      "Rizos Sakellariou",
      "Omer Rana",
      "Rajkumar Buyya",
      "Giuliano Casale",
      "Nicholas R. Jennings"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.05529"
  },
  {
    "id": "arXiv:2110.05668",
    "title": "NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search",
    "abstract": "NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search",
    "descriptor": "",
    "authors": [
      "Renbo Tu",
      "Mikhail Khodak",
      "Nicholas Roberts",
      "Ameet Talwalkar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05668"
  },
  {
    "id": "arXiv:2110.05722",
    "title": "LightSeq2: Accelerated Training for Transformer-based Models on GPUs",
    "abstract": "Comments: 12 pages, 17 figures",
    "descriptor": "\nComments: 12 pages, 17 figures\n",
    "authors": [
      "Xiaohui Wang",
      "Ying Xiong",
      "Xian Qian",
      "Yang Wei",
      "Lei Li",
      "Mingxuan Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2110.05722"
  },
  {
    "id": "arXiv:2110.06211",
    "title": "Diagonalization $of$ Polynomial-Time Turing Machines Via  Nondeterministic Turing Machine",
    "abstract": "Comments: V $5%, addition of Section $5%. Feedbacks are welcome. It will be submitted to get feedbacks after careful considerations for months. arXiv admin note: text overlap with arXiv:2110.05942",
    "descriptor": "\nComments: V $5%, addition of Section $5%. Feedbacks are welcome. It will be submitted to get feedbacks after careful considerations for months. arXiv admin note: text overlap with arXiv:2110.05942\n",
    "authors": [
      "Tianrong Lin"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.06211"
  },
  {
    "id": "arXiv:2110.06917",
    "title": "Extracting Dynamical Models from Data",
    "abstract": "Comments: 35 pages, 20 figures",
    "descriptor": "\nComments: 35 pages, 20 figures\n",
    "authors": [
      "Michael F. Zimmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06917"
  },
  {
    "id": "arXiv:2110.07020",
    "title": "Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge",
    "abstract": "Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge",
    "descriptor": "",
    "authors": [
      "Junyi Huang",
      "Maxwell Benjamin Strome",
      "Ian Jenkins",
      "Parker Williams",
      "Bo Feng",
      "Yaning Wang",
      "Roman Wang",
      "Vaibhav Bagri",
      "Newman Cheng",
      "Iddo Drori"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07020"
  },
  {
    "id": "arXiv:2110.07179",
    "title": "Singular Zone in Quadrotor Yaw-Position Feedback Linearization",
    "abstract": "Comments: 17 pages, 11 figures",
    "descriptor": "\nComments: 17 pages, 11 figures\n",
    "authors": [
      "Zhe Shen",
      "Takeshi Tsuchiya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07179"
  },
  {
    "id": "arXiv:2110.07439",
    "title": "Inverse Problems Leveraging Pre-trained Contrastive Representations",
    "abstract": "Comments: Initial version. Final version to appear in Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)",
    "descriptor": "\nComments: Initial version. Final version to appear in Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Sriram Ravula",
      "Georgios Smyrnis",
      "Matt Jordan",
      "Alexandros G. Dimakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07439"
  },
  {
    "id": "arXiv:2110.08781",
    "title": "On Estimating the Probabilistic Region of Attraction for Partially  Unknown Nonlinear Systems: An Sum-of-Squares Approach",
    "abstract": "Comments: 8 pages, 7 figures, 1 algorithm, 1 table, 34 references. Submitted to the 34th Chinese Control and Decision Conference",
    "descriptor": "\nComments: 8 pages, 7 figures, 1 algorithm, 1 table, 34 references. Submitted to the 34th Chinese Control and Decision Conference\n",
    "authors": [
      "Hejun Huang",
      "Dongkun Han"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08781"
  },
  {
    "id": "arXiv:2110.08949",
    "title": "Real-time Mortality Prediction Using MIMIC-IV ICU Data Via Boosted  Nonparametric Hazards",
    "abstract": "Real-time Mortality Prediction Using MIMIC-IV ICU Data Via Boosted  Nonparametric Hazards",
    "descriptor": "",
    "authors": [
      "Zhale Nowroozilarki",
      "Arash Pakbin",
      "James Royalty",
      "Donald K.K. Lee",
      "Bobak J. Mortazavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08949"
  },
  {
    "id": "arXiv:2110.09161",
    "title": "Machine Covering in the Random-Order Model",
    "abstract": "Machine Covering in the Random-Order Model",
    "descriptor": "",
    "authors": [
      "Susanne Albers",
      "Waldo G\u00e1lvez",
      "Maximilian Janke"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.09161"
  },
  {
    "id": "arXiv:2110.09170",
    "title": "Continuation of Famous Art with AI: A Conditional Adversarial Network  Inpainting Approach",
    "abstract": "Continuation of Famous Art with AI: A Conditional Adversarial Network  Inpainting Approach",
    "descriptor": "",
    "authors": [
      "Jordan J. Bird"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.09170"
  },
  {
    "id": "arXiv:2110.09284",
    "title": "Carbon Neutrality in Data Center",
    "abstract": "Carbon Neutrality in Data Center",
    "descriptor": "",
    "authors": [
      "Zhiwei Cao",
      "Xin Zhou",
      "Han Hu",
      "Zhi Wang",
      "Yonggang Wen"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.09284"
  },
  {
    "id": "arXiv:2110.10540",
    "title": "On the Integration of Course of Action Playbooks into Shareable Cyber  Threat Intelligence",
    "abstract": "On the Integration of Course of Action Playbooks into Shareable Cyber  Threat Intelligence",
    "descriptor": "",
    "authors": [
      "Vasileios Mavroeidis",
      "Pavel Eis",
      "Martin Zadnik",
      "Marco Caseli",
      "Bret Jordan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.10540"
  },
  {
    "id": "arXiv:2110.10832",
    "title": "Ensemble of Averages: Improving Model Selection and Boosting Performance  in Domain Generalization",
    "abstract": "Ensemble of Averages: Improving Model Selection and Boosting Performance  in Domain Generalization",
    "descriptor": "",
    "authors": [
      "Devansh Arpit",
      "Huan Wang",
      "Yingbo Zhou",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.10832"
  },
  {
    "id": "arXiv:2110.10899",
    "title": "LARNet: Latent Action Representation for Human Action Synthesis",
    "abstract": "Comments: British Machine Vision Conference (BMVC) 2021",
    "descriptor": "\nComments: British Machine Vision Conference (BMVC) 2021\n",
    "authors": [
      "Naman Biyani",
      "Aayush J Rana",
      "Shruti Vyas",
      "Yogesh S Rawat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.10899"
  },
  {
    "id": "arXiv:2110.11054",
    "title": "A Geometric Approach for Computing the Kernel of a Polyhedron",
    "abstract": "A Geometric Approach for Computing the Kernel of a Polyhedron",
    "descriptor": "",
    "authors": [
      "Tommaso Sorgente",
      "Silvia Biasotti",
      "Michela Spagnuolo"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.11054"
  },
  {
    "id": "arXiv:2110.11216",
    "title": "User-friendly introduction to PAC-Bayes bounds",
    "abstract": "User-friendly introduction to PAC-Bayes bounds",
    "descriptor": "",
    "authors": [
      "Pierre Alquier"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.11216"
  },
  {
    "id": "arXiv:2110.11466",
    "title": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning  on HPC Systems",
    "abstract": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning  on HPC Systems",
    "descriptor": "",
    "authors": [
      "Steven Farrell",
      "Murali Emani",
      "Jacob Balma",
      "Lukas Drescher",
      "Aleksandr Drozd",
      "Andreas Fink",
      "Geoffrey Fox",
      "David Kanter",
      "Thorsten Kurth",
      "Peter Mattson",
      "Dawei Mu",
      "Amit Ruhela",
      "Kento Sato",
      "Koichi Shirahata",
      "Tsuguchika Tabaru",
      "Aristeidis Tsaris",
      "Jan Balewski",
      "Ben Cumming",
      "Takumi Danjo",
      "Jens Domke",
      "Takaaki Fukai",
      "Naoto Fukumoto",
      "Tatsuya Fukushi",
      "Balazs Gerofi",
      "Takumi Honda",
      "Toshiyuki Imamura",
      "Akihiko Kasagi",
      "Kentaro Kawakami",
      "Shuhei Kudo",
      "Akiyoshi Kuroda",
      "Maxime Martinasso",
      "Satoshi Matsuoka",
      "Henrique Mendon\u00e7a",
      "Kazuki Minami",
      "Prabhat Ram",
      "Takashi Sawada",
      "Mallikarjun Shankar",
      "Tom St. John",
      "Akihiro Tabuchi",
      "Venkatram Vishwanath",
      "Mohamed Wahib",
      "Masafumi Yamazaki",
      "Junqi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.11466"
  },
  {
    "id": "arXiv:2110.11923",
    "title": "Climbing the Diagonal Clifford Hierarchy",
    "abstract": "Comments: Jingzhen Hu and Qingzhong Liang contribute equally to this work. 15 pages, two columns, 8 figures, and 1 table. Comments welcome! This work is an application based on the mathematical framework introduced in arXiv:2109.13481",
    "descriptor": "\nComments: Jingzhen Hu and Qingzhong Liang contribute equally to this work. 15 pages, two columns, 8 figures, and 1 table. Comments welcome! This work is an application based on the mathematical framework introduced in arXiv:2109.13481\n",
    "authors": [
      "Jingzhen Hu",
      "Qingzhong Liang",
      "Robert Calderbank"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.11923"
  },
  {
    "id": "arXiv:2110.12000",
    "title": "Bank transactions embeddings help to uncover current macroeconomics",
    "abstract": "Bank transactions embeddings help to uncover current macroeconomics",
    "descriptor": "",
    "authors": [
      "Maria Begicheva",
      "Alexey Zaytsev"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.12000"
  },
  {
    "id": "arXiv:2110.12296",
    "title": "The Prevalence of Cybersecurity Misinformation on Social Media: Case  Studies on Phishing Reports and Zoom's Threats",
    "abstract": "The Prevalence of Cybersecurity Misinformation on Social Media: Case  Studies on Phishing Reports and Zoom's Threats",
    "descriptor": "",
    "authors": [
      "Mohit Singhal",
      "Nihal Kumarswamy",
      "Shreyasi Kinhekar",
      "Shirin Nilizadeh"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.12296"
  },
  {
    "id": "arXiv:2110.12310",
    "title": "Data-driven estimation of system norms via impulse response",
    "abstract": "Comments: 4 pages, 2 figures, journal",
    "descriptor": "\nComments: 4 pages, 2 figures, journal\n",
    "authors": [
      "L. V. Fiorio",
      "C. L. Remes",
      "L. Campestrini",
      "Y. R. de Novaes"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.12310"
  },
  {
    "id": "arXiv:2110.12490",
    "title": "Paperfetcher: A tool to automate handsearch for systematic reviews",
    "abstract": "Paperfetcher: A tool to automate handsearch for systematic reviews",
    "descriptor": "",
    "authors": [
      "Akash Pallath",
      "Qiyang Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.12490"
  },
  {
    "id": "arXiv:2110.12509",
    "title": "Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays  using Convolutional Neural Networks",
    "abstract": "Comments: v2: Corrected figure 3, corrected MAE formula, corrected Luna16 dataset reference",
    "descriptor": "\nComments: v2: Corrected figure 3, corrected MAE formula, corrected Luna16 dataset reference\n",
    "authors": [
      "Manuel Schultheiss",
      "Philipp Schmette",
      "Thorsten Sellerer",
      "Rafael Schick",
      "Kirsten Taphorn",
      "Korbinian Mechlem",
      "Lorenz Birnbacher",
      "Bernhard Renger",
      "Marcus R. Makowski",
      "Franz Pfeiffer",
      "Daniela Pfeiffer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.12509"
  },
  {
    "id": "arXiv:2110.13005",
    "title": "Myelin: An asynchronous, message-driven parallel framework for  extreme-scale deep learning",
    "abstract": "Myelin: An asynchronous, message-driven parallel framework for  extreme-scale deep learning",
    "descriptor": "",
    "authors": [
      "Siddharth Singh",
      "Abhinav Bhatele"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.13005"
  },
  {
    "id": "arXiv:2110.13211",
    "title": "Investigating the Perceived Precision and validity of a Field-Deployable  Machine Learning-based Tool to Detect Post-Traumatic Stress Disorder (PTSD)  Hyperarousal Events",
    "abstract": "Comments: 36 pages, 4 figures",
    "descriptor": "\nComments: 36 pages, 4 figures\n",
    "authors": [
      "Mahnoosh Sadeghi",
      "Farzan Sasangohar",
      "Anthony D McDonald"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.13211"
  },
  {
    "id": "arXiv:2110.13570",
    "title": "Learning Graph Representation of Person-specific Cognitive Processes  from Audio-visual Behaviours for Automatic Personality Recognition",
    "abstract": "Comments: Submitted to IJCV",
    "descriptor": "\nComments: Submitted to IJCV\n",
    "authors": [
      "Siyang Song",
      "Zilong Shao",
      "Shashank Jaiswal",
      "Linlin Shen",
      "Michel Valstar",
      "Hatice Gunes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.13570"
  },
  {
    "id": "arXiv:2110.13624",
    "title": "Technology Fitness Landscape and the Future of Innovation",
    "abstract": "Comments: 7 pages, 4 figures",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Shuo Jiang",
      "Jianxi Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13624"
  },
  {
    "id": "arXiv:2110.13625",
    "title": "Landmark-Guided Subgoal Generation in Hierarchical Reinforcement  Learning",
    "abstract": "Comments: Accepted to NeurIPS 2021",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Junsu Kim",
      "Younggyo Seo",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13625"
  }
]