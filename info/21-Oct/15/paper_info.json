[
  {
    "id": "arXiv:2110.06948",
    "title": "Challenges for Unsupervised Anomaly Detection in Particle Physics",
    "abstract": "Anomaly detection relies on designing a score to determine whether a\nparticular event is uncharacteristic of a given background distribution. One\nway to define a score is to use autoencoders, which rely on the ability to\nreconstruct certain types of data (background) but not others (signals). In\nthis paper, we study some challenges associated with variational autoencoders,\nsuch as the dependence on hyperparameters and the metric used, in the context\nof anomalous signal (top and $W$) jets in a QCD background. We find that the\nhyperparameter choices strongly affect the network performance and that the\noptimal parameters for one signal are non-optimal for another. In exploring the\nnetworks, we uncover a connection between the latent space of a variational\nautoencoder trained using mean-squared-error and the optimal transport\ndistances within the dataset. We then show that optimal transport distances to\nrepresentative events in the background dataset can be used directly for\nanomaly detection, with performance comparable to the autoencoders. Whether\nusing autoencoders or optimal transport distances for anomaly detection, we\nfind that the choices that best represent the background are not necessarily\nbest for signal identification. These challenges with unsupervised anomaly\ndetection bolster the case for additional exploration of semi-supervised or\nalternative approaches.",
    "descriptor": "\nComments: 22 + 2 pages, 8 figures, 2 tables\n",
    "authors": [
      "Katherine Fraser",
      "Samuel Homiller",
      "Rashmish K. Mishra",
      "Bryan Ostdiek",
      "Matthew D. Schwartz"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2110.06948"
  },
  {
    "id": "arXiv:2110.06956",
    "title": "Considering user agreement in learning to predict the aesthetic quality",
    "abstract": "How to robustly rank the aesthetic quality of given images has been a\nlong-standing ill-posed topic. Such challenge stems mainly from the diverse\nsubjective opinions of different observers about the varied types of content.\nThere is a growing interest in estimating the user agreement by considering the\nstandard deviation of the scores, instead of only predicting the mean aesthetic\nopinion score. Nevertheless, when comparing a pair of contents, few studies\nconsider how confident are we regarding the difference in the aesthetic scores.\nIn this paper, we thus propose (1) a re-adapted multi-task attention network to\npredict both the mean opinion score and the standard deviation in an end-to-end\nmanner; (2) a brand-new confidence interval ranking loss that encourages the\nmodel to focus on image-pairs that are less certain about the difference of\ntheir aesthetic scores. With such loss, the model is encouraged to learn the\nuncertainty of the content that is relevant to the diversity of observers'\nopinions, i.e., user disagreement. Extensive experiments have demonstrated that\nthe proposed multi-task aesthetic model achieves state-of-the-art performance\non two different types of aesthetic datasets, i.e., AVA and TMGA.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Suiyi Ling",
      "Andreas Pastor",
      "Junle Wang",
      "Patrick Le Callet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06956"
  },
  {
    "id": "arXiv:2110.06961",
    "title": "Language Modelling via Learning to Rank",
    "abstract": "We consider language modelling (LM) as a multi-label structured prediction\ntask by re-framing training from solely predicting a single ground-truth word\nto ranking a set of words which could continue a given context. To avoid\nannotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,\nand Born-Again models. This leads to a rank-based form of knowledge\ndistillation (KD). We also develop a method using $N$-grams to create a\nnon-probabilistic teacher which generates the ranks without the need of a\npre-trained LM.\nWe confirm the hypotheses that we can treat LMing as a ranking task and that\nwe can do so without the use of a pre-trained LM. We show that rank-based KD\ngenerally improves perplexity (PPL), often with statistical significance, when\ncompared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of\nthe method, $N$-grams act as competitive teachers and achieve similar\nperformance as using either BERT or a Born-Again model teachers. GPT-2 always\nacts as the best teacher, though, and using it and a Transformer-XL student on\nWiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and\nagainst a KL-based KD of 56.70.",
    "descriptor": "",
    "authors": [
      "Arvid Frydenlund",
      "Gagandeep Singh",
      "Frank Rudzicz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06961"
  },
  {
    "id": "arXiv:2110.06962",
    "title": "Open-Domain Question-Answering for COVID-19 and Other Emergent Domains",
    "abstract": "Since late 2019, COVID-19 has quickly emerged as the newest biomedical\ndomain, resulting in a surge of new information. As with other emergent\ndomains, the discussion surrounding the topic has been rapidly changing,\nleading to the spread of misinformation. This has created the need for a public\nspace for users to ask questions and receive credible, scientific answers. To\nfulfill this need, we turn to the task of open-domain question-answering, which\nwe can use to efficiently find answers to free-text questions from a large set\nof documents. In this work, we present such a system for the emergent domain of\nCOVID-19. Despite the small data size available, we are able to successfully\ntrain the system to retrieve answers from a large-scale corpus of published\nCOVID-19 scientific papers. Furthermore, we incorporate effective re-ranking\nand question-answering techniques, such as document diversity and multiple\nanswer spans. Our open-domain question-answering system can further act as a\nmodel for the quick development of similar systems that can be adapted and\nmodified for other developing emergent domains.",
    "descriptor": "\nComments: EMNLP 2021 Demo\n",
    "authors": [
      "Sharon Levy",
      "Kevin Mo",
      "Wenhan Xiong",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.06962"
  },
  {
    "id": "arXiv:2110.06972",
    "title": "Block Contextual MDPs for Continual Learning",
    "abstract": "In reinforcement learning (RL), when defining a Markov Decision Process\n(MDP), the environment dynamics is implicitly assumed to be stationary. This\nassumption of stationarity, while simplifying, can be unrealistic in many\nscenarios. In the continual reinforcement learning scenario, the sequence of\ntasks is another source of nonstationarity. In this work, we propose to examine\nthis continual reinforcement learning setting through the block contextual MDP\n(BC-MDP) framework, which enables us to relax the assumption of stationarity.\nThis framework challenges RL algorithms to handle both nonstationarity and rich\nobservation settings and, by additionally leveraging smoothness properties,\nenables us to study generalization bounds for this setting. Finally, we take\ninspiration from adaptive control to propose a novel algorithm that addresses\nthe challenges introduced by this more realistic BC-MDP setting, allows for\nzero-shot adaptation at evaluation time, and achieves strong performance on\nseveral nonstationary environments.",
    "descriptor": "\nComments: 26pages, Under Review\n",
    "authors": [
      "Shagun Sodhani",
      "Franziska Meier",
      "Joelle Pineau",
      "Amy Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06972"
  },
  {
    "id": "arXiv:2110.06976",
    "title": "Rethinking the Representational Continuity: Towards Unsupervised  Continual Learning",
    "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent advances in continual\nlearning are restricted to supervised continual learning (SCL) scenarios.\nConsequently, they are not scalable to real-world applications where the data\ndistribution is often biased and unannotated. In this work, we focus on\nunsupervised continual learning (UCL), where we learn the feature\nrepresentations on an unlabelled sequence of tasks and show that reliance on\nannotated data is not necessary for continual learning. We conduct a systematic\nstudy analyzing the learned feature representations and show that unsupervised\nvisual representations are surprisingly more robust to catastrophic forgetting,\nconsistently achieve better performance, and generalize better to\nout-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a\nsmoother loss landscape through qualitative analysis of the learned\nrepresentations and learns meaningful feature representations. Additionally, we\npropose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique\nthat leverages the interpolation between the current task and previous tasks'\ninstances to alleviate catastrophic forgetting for unsupervised\nrepresentations.",
    "descriptor": "",
    "authors": [
      "Divyam Madaan",
      "Jaehong Yoon",
      "Yuanchun Li",
      "Yunxin Liu",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06976"
  },
  {
    "id": "arXiv:2110.06977",
    "title": "Cloud-Assisted Collaborative Road Information Discovery with Gaussian  Process: Application to Road Profile Estimation",
    "abstract": "There is an increasing popularity in exploiting modern vehicles as mobile\nsensors to obtain important road information such as potholes, black ice and\nroad profile. Availability of such information has been identified as a key\nenabler for next-generation vehicles with enhanced safety, efficiency, and\ncomfort. However, existing road information discovery approaches have been\npredominately performed in a single-vehicle setting, which is inevitably\nsusceptible to vehicle model uncertainty and measurement errors. To overcome\nthese limitations, this paper presents a novel cloud-assisted collaborative\nestimation framework that can utilize multiple heterogeneous vehicles to\niteratively enhance estimation performance. Specifically, each vehicle combines\nits onboard measurements with a cloud-based Gaussian process (GP), crowdsourced\nfrom prior participating vehicles as \"pseudo-measurements\", into a local\nestimator to refine the estimation. The resultant local onboard estimation is\nthen sent back to the cloud to update the GP, where we utilize a noisy input GP\n(NIGP) method to explicitly handle uncertain GPS measurements. We employ the\nproposed framework to the application of collaborative road profile estimation.\nPromising results on extensive simulations and hardware-in-the-loop experiments\nshow that the proposed collaborative estimation can significantly enhance\nestimation and iteratively improve the performance from vehicle to vehicle,\ndespite vehicle heterogeneity, model uncertainty, and measurement noises.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Intelligent Transportation Systems\n",
    "authors": [
      "Mohammad R. Hajidavalloo",
      "Zhaojian Li",
      "Xin Xia",
      "Minghui Zheng",
      "Weichao Zhuang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.06977"
  },
  {
    "id": "arXiv:2110.06978",
    "title": "WAFFLE: Weighted Averaging for Personalized Federated Learning",
    "abstract": "In collaborative or federated learning, model personalization can be a very\neffective strategy to deal with heterogeneous training data across clients. We\nintroduce WAFFLE (Weighted Averaging For Federated LEarning), a personalized\ncollaborative machine learning algorithm based on SCAFFOLD. SCAFFOLD uses\nstochastic control variates to converge towards a model close to the globally\noptimal model even in tasks where the distribution of data and labels across\nclients is highly skewed. In contrast, WAFFLE uses the Euclidean distance\nbetween clients' updates to weigh their individual contributions and thus\nminimize the trained personalized model loss on the specific agent of interest.\nThrough a series of experiments, we compare our proposed new method to two\nrecent personalized federated learning methods, Weight Erosion and APFL, as\nwell as two global learning methods, federated averaging and SCAFFOLD. We\nevaluate our method using two categories of non-identical client data\ndistributions (concept shift and label skew) on two benchmark image data sets,\nMNIST and CIFAR10. Our experiments demonstrate the effectiveness of WAFFLE\ncompared with other methods, as it achieves or improves accuracy with faster\nconvergence.",
    "descriptor": "\nComments: Submitted to NeurIPS 2021 Workshop on New Frontiers in Federated Learning: Privacy, Fairness, Robustness, Personalization and Data Ownership\n",
    "authors": [
      "Martin Beaussart",
      "Felix Grimberg",
      "Mary-Anne Hartley",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06978"
  },
  {
    "id": "arXiv:2110.06980",
    "title": "Output Space Entropy Search Framework for Multi-Objective Bayesian  Optimization",
    "abstract": "We consider the problem of black-box multi-objective optimization (MOO) using\nexpensive function evaluations (also referred to as experiments), where the\ngoal is to approximate the true Pareto set of solutions by minimizing the total\nresource cost of experiments. For example, in hardware design optimization, we\nneed to find the designs that trade-off performance, energy, and area overhead\nusing expensive computational simulations. The key challenge is to select the\nsequence of experiments to uncover high-quality solutions using minimal\nresources. In this paper, we propose a general framework for solving MOO\nproblems based on the principle of output space entropy (OSE) search: select\nthe experiment that maximizes the information gained per unit resource cost\nabout the true Pareto front. We appropriately instantiate the principle of OSE\nsearch to derive efficient algorithms for the following four MOO problem\nsettings: 1) The most basic em single-fidelity setting, where experiments are\nexpensive and accurate; 2) Handling em black-box constraints} which cannot be\nevaluated without performing experiments; 3) The discrete multi-fidelity\nsetting, where experiments can vary in the amount of resources consumed and\ntheir evaluation accuracy; and 4) The em continuous-fidelity setting, where\ncontinuous function approximations result in a huge space of experiments.\nExperiments on diverse synthetic and real-world benchmarks show that our OSE\nsearch based algorithms improve over state-of-the-art methods in terms of both\ncomputational-efficiency and accuracy of MOO solutions.",
    "descriptor": "\nComments: Accepted to Journal of Artificial Intelligence Research. arXiv admin note: substantial text overlap with arXiv:2009.05700, arXiv:2009.01721, arXiv:2011.01542\n",
    "authors": [
      "Syrine Belakaria",
      "Aryan Deshwal",
      "Janardhan Rao Doppa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.06980"
  },
  {
    "id": "arXiv:2110.06981",
    "title": "FlexiTerm: A more efficient implementation of flexible multi-word term  recognition",
    "abstract": "Terms are linguistic signifiers of domain-specific concepts. Automated\nrecognition of multi-word terms (MWT) in free text is a sequence labelling\nproblem, which is commonly addressed using supervised machine learning methods.\nTheir need for manual annotation of training data makes it difficult to port\nsuch methods across domains. FlexiTerm, on the other hand, is a fully\nunsupervised method for MWT recognition from domain-specific corpora.\nOriginally implemented in Java as a proof of concept, it did not scale well,\nthus offering little practical value in the context of big data. In this paper,\nwe describe its re-implementation in Python and compare the performance of\nthese two implementations. The results demonstrated major improvements in terms\nof efficiency, which allow FlexiTerm to transition from the proof of concept to\nthe production-grade application.",
    "descriptor": "",
    "authors": [
      "Irena Spasic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.06981"
  },
  {
    "id": "arXiv:2110.06982",
    "title": "Masking Effects in Combined Hardness and Stiffness Rendering Using an  Encountered-Type Haptic Display",
    "abstract": "Rendering stable hard surfaces is an important problem in haptics for many\ntasks, including training simulators for orthopedic surgery or dentistry.\nCurrent impedance devices cannot provide enough force and stiffness to render a\nwall, and the high friction and inertia of admittance devices make it difficult\nto render free space. We propose to address these limitations by combining\nhaptic augmented reality, untethered haptic interaction, and an\nencountered-type haptic display. We attach a plate with the desired hardness on\nthe kinesthetic device's end-effector, which the user interacts with using an\nuntethered stylus. This method allows us to directly change the hardness of the\nend-effector based on the rendered object. In this paper, we evaluate how\nchanging the hardness of the end-effector can mask the device's stiffness and\naffect the user's perception. The results of our human subject experiment\nindicate that when the end-effector is made of a hard material, it is difficult\nfor users to perceive when the underlying stiffness being rendered by the\ndevice is changed, but this stiffness change is easy to distinguish while the\nend-effector is made of a soft material. These results show promise for our\napproach in avoiding the limitations of haptic devices when rendering hard\nsurfaces.",
    "descriptor": "",
    "authors": [
      "Naghmeh Zamani",
      "Heather Culbertson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.06982"
  },
  {
    "id": "arXiv:2110.06983",
    "title": "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative  Approach to Exploring Many-to-one Maps",
    "abstract": "Many-to-one maps are ubiquitous in machine learning, from the image\nrecognition model that assigns a multitude of distinct images to the concept of\n\"cat\" to the time series forecasting model which assigns a range of distinct\ntime-series to a single scalar regression value. While the primary use of such\nmodels is naturally to associate correct output to each input, in many problems\nit is also useful to be able to explore, understand, and sample from a model's\nfibers, which are the set of input values $x$ such that $f(x) = y$, for fixed\n$y$ in the output space. In this paper we show that popular generative\narchitectures are ill-suited to such tasks. Motivated by this we introduce a\nnovel generative architecture, a Bundle Network, based on the concept of a\nfiber bundle from (differential) topology. BundleNets exploit the idea of a\nlocal trivialization wherein a space can be locally decomposed into a product\nspace that cleanly encodes the many-to-one nature of the map. By enforcing this\ndecomposition in BundleNets and by utilizing state-of-the-art invertible\ncomponents, investigating a network's fibers becomes natural.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Nico Courts",
      "Henry Kvinge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.06983"
  },
  {
    "id": "arXiv:2110.06986",
    "title": "ADMM-DAD net: a deep unfolding network for analysis compressed sensing",
    "abstract": "In this paper, we propose a new deep unfolding neural network based on the\nADMM algorithm for analysis Compressed Sensing. The proposed network jointly\nlearns a redundant analysis operator for sparsification and reconstructs the\nsignal of interest. We compare our proposed network with a state-of-the-art\nunfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we\nconsider not only image, but also speech datasets as test examples.\nComputational experiments demonstrate that our proposed network outperforms the\nstate-of-the-art deep unfolding networks, consistently for both real-world\nimage and speech datasets.",
    "descriptor": "",
    "authors": [
      "Vasiliki Kouni",
      "Georgios Paraskevopoulos",
      "Holger Rauhut",
      "George C. Alexandropoulos"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06986"
  },
  {
    "id": "arXiv:2110.06988",
    "title": "Spectral Convergence of Symmetrized Graph Laplacian on manifolds with  boundary",
    "abstract": "We study the spectral convergence of a symmetrized Graph Laplacian matrix\ninduced by a Gaussian kernel evaluated on pairs of embedded data, sampled from\na manifold with boundary, a sub-manifold of $\\mathbb{R}^m$. Specifically, we\ndeduce the convergence rates for eigenpairs of the discrete Graph-Laplacian\nmatrix to the eigensolutions of the Laplace-Beltrami operator that are\nwell-defined on manifolds with boundary, including the homogeneous Neumann and\nDirichlet boundary conditions. For the Dirichlet problem, we deduce the\nconvergence of the \\emph{truncated Graph Laplacian}, which is recently\nnumerically observed in applications, and provide a detailed numerical\ninvestigation on simple manifolds. Our method of proof relies on the min-max\nargument over a compact and symmetric integral operator, leveraging the RKHS\ntheory for spectral convergence of integral operator and a recent pointwise\nasymptotic result of a Gaussian kernel integral operator on manifolds with\nboundary.",
    "descriptor": "\nComments: 6 figures\n",
    "authors": [
      "J. Wilson Peoples",
      "John Harlim"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.06988"
  },
  {
    "id": "arXiv:2110.06990",
    "title": "Scaling Laws for the Few-Shot Adaptation of Pre-trained Image  Classifiers",
    "abstract": "Empirical science of neural scaling laws is a rapidly growing area of\nsignificant importance to the future of machine learning, particularly in the\nlight of recent breakthroughs achieved by large-scale pre-trained models such\nas GPT-3, CLIP and DALL-e. Accurately predicting the neural network performance\nwith increasing resources such as data, compute and model size provides a more\ncomprehensive evaluation of different approaches across multiple scales, as\nopposed to traditional point-wise comparisons of fixed-size models on\nfixed-size benchmarks, and, most importantly, allows for focus on the\nbest-scaling, and thus most promising in the future, approaches. In this work,\nwe consider a challenging problem of few-shot learning in image classification,\nespecially when the target data distribution in the few-shot phase is different\nfrom the source, training, data distribution, in a sense that it includes new\nimage classes not encountered during training. Our current main goal is to\ninvestigate how the amount of pre-training data affects the few-shot\ngeneralization performance of standard image classifiers. Our key observations\nare that (1) such performance improvements are well-approximated by power laws\n(linear log-log plots) as the training set size increases, (2) this applies to\nboth cases of target data coming from either the same or from a different\ndomain (i.e., new classes) as the training data, and (3) few-shot performance\non new classes converges at a faster rate than the standard classification\nperformance on previously seen classes. Our findings shed new light on the\nrelationship between scale and generalization.",
    "descriptor": "",
    "authors": [
      "Gabriele Prato",
      "Simon Guiroy",
      "Ethan Caballero",
      "Irina Rish",
      "Sarath Chandar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06990"
  },
  {
    "id": "arXiv:2110.06991",
    "title": "Scalable Graph Embedding LearningOn A Single GPU",
    "abstract": "Graph embedding techniques have attracted growing interest since they convert\nthe graph data into continuous and low-dimensional space. Effective graph\nanalytic provides users a deeper understanding of what is behind the data and\nthus can benefit a variety of machine learning tasks. With the current scale of\nreal-world applications, most graph analytic methods suffer high computation\nand space costs. These methods and systems can process a network with thousands\nto a few million nodes. However, scaling to large-scale networks remains a\nchallenge. The complexity of training graph embedding system requires the use\nof existing accelerators such as GPU. In this paper, we introduce a hybrid\nCPU-GPU framework that addresses the challenges of learning embedding of\nlarge-scale graphs. The performance of our method is compared qualitatively and\nquantitatively with the existing embedding systems on common benchmarks. We\nalso show that our system can scale training to datasets with an order of\nmagnitude greater than a single machine's total memory capacity. The\neffectiveness of the learned embedding is evaluated within multiple downstream\napplications. The experimental results indicate the effectiveness of the\nlearned embedding in terms of performance and accuracy.",
    "descriptor": "",
    "authors": [
      "Azita Nouri",
      "Philip E. Davis",
      "Pradeep Subedi",
      "Manish Parashar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.06991"
  },
  {
    "id": "arXiv:2110.06997",
    "title": "Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation  with Multi-Armed Bandits",
    "abstract": "Training data for machine translation (MT) is often sourced from a multitude\nof large corpora that are multi-faceted in nature, e.g. containing contents\nfrom multiple domains or different levels of quality or complexity. Naturally,\nthese facets do not occur with equal frequency, nor are they equally important\nfor the test scenario at hand. In this work, we propose to optimize this\nbalance jointly with MT model parameters to relieve system developers from\nmanual schedule design. A multi-armed bandit is trained to dynamically choose\nbetween facets in a way that is most beneficial for the MT system. We evaluate\nit on three different multi-facet applications: balancing translationese and\nnatural training data, or data from multiple domains or multiple language\npairs. We find that bandit learning leads to competitive MT systems across\ntasks, and our analysis provides insights into its learned strategies and the\nunderlying data sets.",
    "descriptor": "\nComments: EMNLP Findings 2021\n",
    "authors": [
      "Julia Kreutzer",
      "David Vilar",
      "Artem Sokolov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06997"
  },
  {
    "id": "arXiv:2110.06999",
    "title": "Study of positional encoding approaches for Audio Spectrogram  Transformers",
    "abstract": "Transformers have revolutionized the world of deep learning, specially in the\nfield of natural language processing. Recently, the Audio Spectrogram\nTransformer (AST) was proposed for audio classification, leading to state of\nthe art results in several datasets. However, in order for ASTs to outperform\nCNNs, pretraining with ImageNet is needed. In this paper, we study one\ncomponent of the AST, the positional encoding, and propose several variants to\nimprove the performance of ASTs trained from scratch, without ImageNet\npretraining. Our best model, which incorporates conditional positional\nencodings, significantly improves performance on Audioset and ESC-50 compared\nto the original AST.",
    "descriptor": "\nComments: Submitted to ICASSP 2022. 5 pages, 3 figures\n",
    "authors": [
      "Leonardo Pepino",
      "Pablo Riera",
      "Luciana Ferrer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.06999"
  },
  {
    "id": "arXiv:2110.07002",
    "title": "Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation",
    "abstract": "Text autoencoders are often used for unsupervised conditional text generation\nby applying mappings in the latent space to change attributes to the desired\nvalues. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these\nmappings in the embedding space of an autoencoder. However, their method is\nrestricted to autoencoders with a single-vector embedding, which limits how\nmuch information can be retained. We address this issue by extending their\nmethod to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a\nvariable-size bag of vectors that grows with the size of the text, as in\nattention-based models. This allows to encode and reconstruct much longer texts\nthan standard autoencoders. Analogous to conventional autoencoders, we propose\nregularization techniques that facilitate learning meaningful operations in the\nlatent space. Finally, we adapt for a training scheme that learns to map an\ninput bag to an output bag, including a novel loss function and neural\narchitecture. Our experimental evaluations on unsupervised sentiment transfer\nand sentence summarization show that our method performs substantially better\nthan a standard autoencoder.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Florian Mai",
      "James Henderson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07002"
  },
  {
    "id": "arXiv:2110.07003",
    "title": "Sustainability Through Cognition Aware Safety Systems -- Next Level  Human-Machine-Interaction",
    "abstract": "Industrial Safety deals with the physical integrity of humans, machines and\nthe environment when they interact during production scenarios. Industrial\nSafety is subject to a rigorous certification process that leads to inflexible\nsettings, in which all changes are forbidden. With the progressing introduction\nof smart robotics and smart machinery to the factory floor, combined with an\nincreasing shortage of skilled workers, it becomes imperative that safety\nscenarios incorporate a flexible handling of the boundary between humans,\nmachines and the environment. In order to increase the well-being of workers,\nreduce accidents, and compensate for different skill sets, the configuration of\nmachines and the factory floor should be dynamically adapted, while still\nenforcing functional safety requirements. The contribution of this paper is as\nfollows: (1) We present a set of three scenarios, and discuss how industrial\nsafety mechanisms could be augmented through dynamic changes to the work\nenvironment in order to decrease potential accidents, and thus increase\nproductivity. (2) We introduce the concept of a Cognition Aware Safety System\n(CASS) and its architecture. The idea behind CASS is to integrate AI based\nreasoning about human load, stress, and attention with AI based selection of\nactions to avoid the triggering of safety stops. (3) And finally, we will\ndescribe the required performance measurement dimensions for a quantitative\nperformance measurement model to enable a comprehensive (triple bottom line)\nimpact assessment of CASS. Additionally we introduce a detailed guideline for\nexpert interviews to explore the feasibility of the approach for given\nscenarios.",
    "descriptor": "\nComments: 27 pages. 4 figures. 2 tables. pre-print journal paper. position paper\n",
    "authors": [
      "Juergen Mangler",
      "Konrad Diwol",
      "Dieter Etz",
      "Stefanie Rinderle-Ma",
      "Alois Ferscha",
      "Gerald Reiner",
      "Wolfgang Kastner",
      "Sebastien Bougain",
      "Christoph Pollak",
      "Michael Haslgr\u00fcbler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07003"
  },
  {
    "id": "arXiv:2110.07004",
    "title": "ES-Based Jacobian Enables Faster Bilevel Optimization",
    "abstract": "Bilevel optimization (BO) has arisen as a powerful tool for solving many\nmodern machine learning problems. However, due to the nested structure of BO,\nexisting gradient-based methods require second-order derivative approximations\nvia Jacobian- or/and Hessian-vector computations, which can be very costly in\npractice, especially with large neural network models. In this work, we propose\na novel BO algorithm, which adopts Evolution Strategies (ES) based method to\napproximate the response Jacobian matrix in the hypergradient of BO, and hence\nfully eliminates all second-order computations. We call our algorithm as ESJ\n(which stands for the ES-based Jacobian method) and further extend it to the\nstochastic setting as ESJ-S. Theoretically, we characterize the convergence\nguarantee and computational complexity for our algorithms. Experimentally, we\ndemonstrate the superiority of our proposed algorithms compared to the state of\nthe art methods on various bilevel problems. Particularly, in our experiment in\nthe few-shot meta-learning problem, we meta-learn the twelve millions\nparameters of a ResNet-12 network over the miniImageNet dataset, which\nevidently demonstrates the scalability of our ES-based bilevel approach and its\nfeasibility in the large-scale setting.",
    "descriptor": "\nComments: Submitted for publication\n",
    "authors": [
      "Daouda Sow",
      "Kaiyi Ji",
      "Yingbin Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07004"
  },
  {
    "id": "arXiv:2110.07007",
    "title": "Out-of-Distribution Robustness in Deep Learning Compression",
    "abstract": "In recent years, deep neural network (DNN) compression systems have proved to\nbe highly effective for designing source codes for many natural sources.\nHowever, like many other machine learning systems, these compressors suffer\nfrom vulnerabilities to distribution shifts as well as out-of-distribution\n(OOD) data, which reduces their real-world applications. In this paper, we\ninitiate the study of OOD robust compression. Considering robustness to two\ntypes of ambiguity sets (Wasserstein balls and group shifts), we propose\nalgorithmic and architectural frameworks built on two principled methods: one\nthat trains DNN compressors using distributionally-robust optimization (DRO),\nand the other which uses a structured latent code. Our results demonstrate that\nboth methods enforce robustness compared to a standard DNN compressor, and that\nusing a structured code can be superior to the DRO compressor. We observe\ntradeoffs between robustness and distortion and corroborate these findings\ntheoretically for a specific class of sources.",
    "descriptor": "\nComments: Initially published at ICML-2021 ITR3 Workshop\n",
    "authors": [
      "Eric Lei",
      "Hamed Hassani",
      "Shirin Saeedi Bidokhti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07007"
  },
  {
    "id": "arXiv:2110.07009",
    "title": "Covert Message Passing over Public Internet Platforms Using Model-Based  Format-Transforming Encryption",
    "abstract": "We introduce a new type of format-transforming encryption where the format of\nciphertexts is implicitly encoded within a machine-learned generative model.\nAround this primitive, we build a system for covert messaging over large,\npublic internet platforms (e.g., Twitter). Loosely, our system composes an\nauthenticated encryption scheme, with a method for encoding random ciphertext\nbits into samples from the generative model's family of seed-indexed\ntoken-distributions. By fixing a deployment scenario, we are forced to consider\nsystem-level and algorithmic solutions to real challenges -- such as\nreceiver-side parsing ambiguities, and the low information-carrying capacity of\nactual token-distributions -- that were elided in prior work. We use GPT-2 as\nour generative model so that our system cryptographically transforms plaintext\nbitstrings into natural-language covertexts suitable for posting to public\nplatforms. We consider adversaries with full view of the internet platform's\ncontent, whose goal is to surface posts that are using our system for covert\nmessaging. We carry out a suite of experiments to provide heuristic evidence of\nsecurity and to explore tradeoffs between operational efficiency and\ndetectability.",
    "descriptor": "",
    "authors": [
      "Luke A. Bauer",
      "James K. Howes IV",
      "Sam A. Markelon",
      "Vincent Bindschaedler",
      "Thomas Shrimpton"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07009"
  },
  {
    "id": "arXiv:2110.07011",
    "title": "Random Walks with Variable Restarts for Negative-Example-Informed Label  Propagation",
    "abstract": "Label propagation is frequently encountered in machine learning and data\nmining applications on graphs, either as a standalone problem or as part of\nnode classification. Many label propagation algorithms utilize random walks (or\nnetwork propagation), which provide limited ability to take into account\nnegatively-labeled nodes (i.e., nodes that are known to be not associated with\nthe label of interest). Specialized algorithms to incorporate negatively\nlabeled samples generally focus on learning or readjusting the edge weights to\ndrive walks away from negatively-labeled nodes and toward positively-labeled\nnodes. This approach has several disadvantages, as it increases the number of\nparameters to be learned, and does not necessarily drive the walk away from\nregions of the network that are rich in negatively-labeled nodes.\nWe reformulate random walk with restarts and network propagation to enable\n\"variable restarts\", that is the increased likelihood of restarting at a\npositively-labeled node when a negatively-labeled node is encountered. Based on\nthis reformulation, we develop CusTaRd, an algorithm that effectively combines\nvariable restart probabilities and edge re-weighting to avoid\nnegatively-labeled nodes. In addition to allowing variable restarts, CusTaRd\nsamples negatively-labeled nodes from neighbors of positively-labeled nodes to\nbetter characterize the difference between positively and negatively labeled\nnodes. To assess the performance of CusTaRd, we perform comprehensive\nexperiments on four network datasets commonly used in benchmarking label\npropagation and node classification algorithms. Our results show that CusTaRd\nconsistently outperforms competing algorithms that learn/readjust edge weights,\nand sampling of negatives from the close neighborhood of positives further\nimproves predictive accuracy.",
    "descriptor": "\nComments: 14 pages, 4 figures, 1 table. Submitted to SDM22\n",
    "authors": [
      "Sean Maxwell",
      "Mehmet Koyuturk"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07011"
  },
  {
    "id": "arXiv:2110.07018",
    "title": "Algebraic Reasoning of Quantum Programs via Non-Idempotent Kleene  Algebra",
    "abstract": "We investigate the algebraic reasoning of quantum programs inspired by the\nsuccess of classical program analysis based on Kleene algebra. One prominent\nexample of such is the famous Kleene Algebra with Tests (KAT), which has\nfurnished both theoretical insights and practical tools. The succinctness of\nalgebraic reasoning would be especially desirable for scalable analysis of\nquantum programs, given the involvement of exponential-size matrices in most of\nthe existing methods. A few key features of KAT including the idempotent law\nand the nice properties of classical tests, however, fail to hold in the\ncontext of quantum programs due to their unique quantum features, especially in\nbranching. We propose the Non-idempotent Kleena Algebra (NKA) as a natural\nalternative and identify complete and sound semantic models for NKA as well as\ntheir appropriate quantum interpretations. In light of applications of KAT, we\nare able to demonstrate algebraic proofs in NKA of quantum compiler\noptimization and the normal form of quantum while-programs. Moreover, we extend\nNKA with Tests (i.e., NKAT), where tests model quantum predicates following the\nrules of effect algebra, and illustrate how to encode propositional quantum\nHoare logic as NKAT theorems.",
    "descriptor": "",
    "authors": [
      "Yuxiang Peng",
      "Mingsheng Ying",
      "Xiaodi Wu"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07018"
  },
  {
    "id": "arXiv:2110.07019",
    "title": "The Design and Simulation of Biomimetic Fish Robot for Aquatic Creature  Study",
    "abstract": "In the application of underwater creature study, comparing with\npropeller-powered ROVs and servo motor actuated robotic fish, novel biomimetic\nfish robot designs with soft actuation structure could interact with aquatic\ncreatures closely and record authentic habitats and behaviours. This final\nproject report presents the detailed design process of a hydraulic soft\nactuator powered robotic fish for aquatic creature study capable of swimming\nalong the 3D trajectory. The robotic fish is designed based on the analysis of\nthe pro and cons of existing designs. Except for the mechanical and electronic\ndesigns and manufacturing method of crucial components, a simplified open-loop\ncontrol algorithm was designed to check the functionality of the application\nboard and microcontroller in the Proteus simulation environment. As the key\ncomponent of the robotic fish, Finite Element Method (FEM) simulations were\nconducted to visualise the soft actuator's deformation under different pressure\nto validate the design. Computational Fluid Dynamics (CFD) simulations were\nalso conducted to improve the hydrodynamic efficiency of the shape of robotic\nfish. Although physical manufacturing is impossible due to the pandemic, the\nsimulations show overall good performance in terms of control, actuation, and\nhydrodynamic efficiency.",
    "descriptor": "",
    "authors": [
      "Ningzhe Hou"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07019"
  },
  {
    "id": "arXiv:2110.07020",
    "title": "Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge",
    "abstract": "Kinship verification is the task of determining whether a parent-child,\nsibling, or grandparent-grandchild relationship exists between two people and\nis important in social media applications, forensic investigations, finding\nmissing children, and reuniting families. We demonstrate high quality kinship\nverification by participating in the FG 2021 Recognizing Families in the Wild\nchallenge which provides the largest publicly available dataset in the field.\nOur approach is among the top 3 winning entries in the competition. We ensemble\nmodels written by both human experts and OpenAI Codex. We make our models and\ncode publicly available.",
    "descriptor": "",
    "authors": [
      "Junyi Huang",
      "Maxwell Benjamin Strome",
      "Ian Jenkins",
      "Parker Williams",
      "Bo Feng",
      "Yaning Wang",
      "Roman Wang",
      "Vaibhav Bagri",
      "Newman Cheng",
      "Iddo Drori"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07020"
  },
  {
    "id": "arXiv:2110.07022",
    "title": "Lossy Compression with Universal Distortion",
    "abstract": "A novel variant of lossy coding is considered in which the distortion measure\nis revealed only to the encoder and only at run-time. Two forms of rate\nredundancy are used to analyze the performance, and achievability results of\nboth a pointwise and minimax nature are demonstrated. One proof uses\nappropriate quantization of the space of distortion measures while another uses\nideas from VC dimension and growth functions. Future research directions\npertaining to Rissanen's redundancy result are discussed.",
    "descriptor": "",
    "authors": [
      "Adeel Mahmood",
      "Aaron B. Wagner"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07022"
  },
  {
    "id": "arXiv:2110.07027",
    "title": "Comparison of SVD and factorized TDNN approaches for speech to text",
    "abstract": "This work concentrates on reducing the RTF and word error rate of a hybrid\nHMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We\nfind this architecture particularly useful for lightly reverberated\nenvironments. However, these models tend to demand more computation than is\ndesirable. In this work, we explore alternate architectures employing singular\nvalue decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as\nwell as to the affine transforms of every LSTM cell. We compare this approach\nwith specifying bottleneck layers similar to those introduced by SVD before\ntraining. Additionally, we reduced the search space of the decoding graph to\nmake it a better fit to operate in real-time applications. We report -61.57%\nrelative reduction in RTF and almost 1% relative decrease in WER for our\narchitecture trained on Fisher data along with reverberated versions of this\ndataset in order to match one of our target test distributions.",
    "descriptor": "\nComments: 4 pages, 1 figure, 3 tables\n",
    "authors": [
      "Jeffrey Josanne Michael",
      "Nagendra Kumar Goel",
      "Navneeth K",
      "Jonas Robertson",
      "Shravan Mishra"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07027"
  },
  {
    "id": "arXiv:2110.07028",
    "title": "AI Total: Analyzing Security ML Models with Imperfect Data in Production",
    "abstract": "Development of new machine learning models is typically done on manually\ncurated data sets, making them unsuitable for evaluating the models'\nperformance during operations, where the evaluation needs to be performed\nautomatically on incoming streams of new data. Unfortunately, pure reliance on\na fully automatic pipeline for monitoring model performance makes it difficult\nto understand if any observed performance issues are due to model performance,\npipeline issues, emerging data distribution biases, or some combination of the\nabove. With this in mind, we developed a web-based visualization system that\nallows the users to quickly gather headline performance numbers while\nmaintaining confidence that the underlying data pipeline is functioning\nproperly. It also enables the users to immediately observe the root cause of an\nissue when something goes wrong. We introduce a novel way to analyze\nperformance under data issues using a data coverage equalizer. We describe the\nvarious modifications and additional plots, filters, and drill-downs that we\nadded on top of the standard evaluation metrics typically tracked in machine\nlearning (ML) applications, and walk through some real world examples that\nproved valuable for introspecting our models.",
    "descriptor": "",
    "authors": [
      "Awalin Sopan",
      "Konstantin Berlin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07028"
  },
  {
    "id": "arXiv:2110.07029",
    "title": "Adaptive Elastic Training for Sparse Deep Learning on Heterogeneous  Multi-GPU Servers",
    "abstract": "Motivated by extreme multi-label classification applications, we consider\ntraining deep learning models over sparse data in multi-GPU servers. The\nvariance in the number of non-zero features across training batches and the\nintrinsic GPU heterogeneity combine to limit accuracy and increase the time to\nconvergence. We address these challenges with Adaptive SGD, an adaptive elastic\nmodel averaging stochastic gradient descent algorithm for heterogeneous\nmulti-GPUs that is characterized by dynamic scheduling, adaptive batch size\nscaling, and normalized model merging. Instead of statically partitioning\nbatches to GPUs, batches are routed based on the relative processing speed.\nBatch size scaling assigns larger batches to the faster GPUs and smaller\nbatches to the slower ones, with the goal to arrive at a steady state in which\nall the GPUs perform the same number of model updates. Normalized model merging\ncomputes optimal weights for every GPU based on the assigned batches such that\nthe combined model achieves better accuracy. We show experimentally that\nAdaptive SGD outperforms four state-of-the-art solutions in time-to-accuracy\nand is scalable with the number of GPUs.",
    "descriptor": "",
    "authors": [
      "Yujing Ma",
      "Florin Rusu",
      "Kesheng Wu",
      "Alexander Sim"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07029"
  },
  {
    "id": "arXiv:2110.07031",
    "title": "Improving the Robustness to Variations of Objects and Instructions with  a Neuro-Symbolic Approach for Interactive Instruction Following",
    "abstract": "An interactive instruction following task has been proposed as a benchmark\nfor learning to map natural language instructions and first-person vision into\nsequences of actions to interact with objects in a 3D simulated environment. We\nfind that an existing end-to-end neural model for this task is not robust to\nvariations of objects and language instructions. We assume that this problem is\ndue to the high sensitiveness of neural feature extraction to small changes in\nvision and language inputs. To mitigate this problem, we propose a\nneuro-symbolic approach that performs reasoning over high-level symbolic\nrepresentations that are robust to small changes in raw inputs. Our experiments\non the ALFRED dataset show that our approach significantly outperforms the\nexisting model by 18, 52, and 73 points in the success rate on the\nToggleObject, PickupObject, and SliceObject subtasks in unseen environments\nrespectively.",
    "descriptor": "\nComments: NILLI workshop at EMNLP 2021\n",
    "authors": [
      "Kazutoshi Shinoda",
      "Yuki Takezawa",
      "Masahiro Suzuki",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07031"
  },
  {
    "id": "arXiv:2110.07033",
    "title": "Compliance checking in reified IO logic via SHACL",
    "abstract": "Reified Input/Output (I/O) logic[21] has been recently proposed to model\nreal-world norms in terms of the logic in [11]. This is massively grounded on\nthe notion of reification, and it has specifically designed to model meaning of\nnatural language sentences, such as the ones occurring in existing legislation.\nThis paper presents a methodology to carry out compliance checking on reified\nI/O logic formulae. These are translated in SHACL (Shapes Constraint Language)\nshapes, a recent W3C recommendation to validate and reason with RDF\ntriplestores. Compliance checking is then enforced by validating RDF graphs\ndescribing states of affairs with respect to these SHACL shapes.",
    "descriptor": "",
    "authors": [
      "Livio Robaldo",
      "Kolawole J. Adebayo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07033"
  },
  {
    "id": "arXiv:2110.07034",
    "title": "How Does Momentum Benefit Deep Neural Networks Architecture Design? A  Few Case Studies",
    "abstract": "We present and review an algorithmic and theoretical framework for improving\nneural network architecture design via momentum. As case studies, we consider\nhow momentum can improve the architecture design for recurrent neural networks\n(RNNs), neural ordinary differential equations (ODEs), and transformers. We\nshow that integrating momentum into neural network architectures has several\nremarkable theoretical and empirical benefits, including 1) integrating\nmomentum into RNNs and neural ODEs can overcome the vanishing gradient issues\nin training RNNs and neural ODEs, resulting in effective learning long-term\ndependencies. 2) momentum in neural ODEs can reduce the stiffness of the ODE\ndynamics, which significantly enhances the computational efficiency in training\nand testing. 3) momentum can improve the efficiency and accuracy of\ntransformers.",
    "descriptor": "\nComments: 42 pages, 15 figures. arXiv admin note: text overlap with arXiv:2110.04840\n",
    "authors": [
      "Bao Wang",
      "Hedi Xia",
      "Tan Nguyen",
      "Stanley Osher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07034"
  },
  {
    "id": "arXiv:2110.07035",
    "title": "Bond Default Prediction with Text Embeddings, Undersampling and Deep  Learning",
    "abstract": "The special and important problems of default prediction for municipal bonds\nare addressed using a combination of text embeddings from a pre-trained\ntransformer network, a fully connected neural network, and synthetic\noversampling. The combination of these techniques provides significant\nimprovement in performance over human estimates, linear models, and boosted\nensemble models, on data with extreme imbalance. Less than 0.2% of municipal\nbonds default, but our technique predicts 9 out of 10 defaults at the time of\nissue, without using bond ratings, at a cost of false positives on less than\n0.1% non-defaulting bonds. The results hold the promise of reducing the cost of\ncapital for local public goods, which are vital for society, and bring\ntechniques previously used in personal credit and public equities (or national\nfixed income), as well as the current generation of embedding techniques, to\nsub-sovereign credit decisions.",
    "descriptor": "\nComments: 9 pages, 3 figures\n",
    "authors": [
      "Luke Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07035"
  },
  {
    "id": "arXiv:2110.07037",
    "title": "Solving multiscale steady radiative transfer equation using neural  networks with uniform stability",
    "abstract": "This paper concerns solving the steady radiative transfer equation with\ndiffusive scaling, using the physics informed neural networks (PINNs). The idea\nof PINNs is to minimize a least-square loss function, that consists of the\nresidual from the governing equation, the mismatch from the boundary\nconditions, and other physical constraints such as conservation. It is\nadvantageous of being flexible and easy to execute, and brings the potential\nfor high dimensional problems. Nevertheless, due the presence of small scales,\nthe vanilla PINNs can be extremely unstable for solving multiscale steady\ntransfer equations. In this paper, we propose a new formulation of the loss\nbased on the macro-micro decomposition. We prove that, the new loss function is\nuniformly stable with respect to the small Knudsen number in the sense that the\n$L^2$-error of the neural network solution is uniformly controlled by the loss.\nWhen the boundary condition is an-isotropic, a boundary layer emerges in the\ndiffusion limit and therefore brings an additional difficulty in training the\nneural network. To resolve this issue, we include a boundary layer corrector\nthat carries over the sharp transition part of the solution and leaves the rest\neasy to be approximated. The effectiveness of the new methodology is\ndemonstrated in extensive numerical examples.",
    "descriptor": "",
    "authors": [
      "Yulong Lu",
      "Li Wang",
      "Wuzhe Xu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07037"
  },
  {
    "id": "arXiv:2110.07038",
    "title": "Towards Efficient NLP: A Standard Evaluation and A Strong Baseline",
    "abstract": "Supersized pre-trained language models have pushed the accuracy of various\nNLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless\nSOTA accuracy, most works are pursuing improvement on other dimensions such as\nefficiency, leading to \"Pareto SOTA\". Different from accuracy, the metric for\nefficiency varies across different studies, making them hard to be fairly\ncompared. To that end, this work presents ELUE (Efficient Language\nUnderstanding Evaluation), a standard evaluation, and a public leaderboard for\nefficient NLP models. ELUE is dedicated to depicting the Pareto Front for\nvarious language understanding tasks, such that it can tell whether and how\nmuch a method achieves Pareto improvement. Along with the benchmark, we also\npre-train and release a strong baseline, ElasticBERT, whose elasticity is both\nstatic and dynamic. ElasticBERT is static in that it allows reducing model\nlayers on demand. ElasticBERT is dynamic in that it selectively executes parts\nof model layers conditioned on the input. We demonstrate the ElasticBERT,\ndespite its simplicity, outperforms or performs on par with SOTA compressed and\nearly exiting models. The ELUE benchmark is publicly available at\nthis http URL",
    "descriptor": "\nComments: Preprint. Work in progress. 10 pages\n",
    "authors": [
      "Xiangyang Liu",
      "Tianxiang Sun",
      "Junliang He",
      "Lingling Wu",
      "Xinyu Zhang",
      "Hao Jiang",
      "Zhao Cao",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07038"
  },
  {
    "id": "arXiv:2110.07039",
    "title": "Presenting a Larger Up-to-date Movie Dataset and Investigating the  Effects of Pre-released Attributes on Gross Revenue",
    "abstract": "Movie-making has become one of the most costly and risky endeavors in the\nentertainment industry. Continuous change in the preference of the audience\nmakes it harder to predict what kind of movie will be financially successful at\nthe box office. So, it is no wonder that cautious, intelligent stakeholders and\nlarge production houses will always want to know the probable revenue that will\nbe generated by a movie before making an investment. Researchers have been\nworking on finding an optimal strategy to help investors in making the right\ndecisions. But the lack of a large, up-to-date dataset makes their work harder.\nIn this work, we introduce an up-to-date, richer, and larger dataset that we\nhave prepared by scraping IMDb for researchers and data analysts to work with.\nThe compiled dataset contains the summery data of 7.5 million titles and detail\ninformation of more than 200K movies. Additionally, we perform different\nstatistical analysis approaches on our dataset to find out how a movie's\nrevenue is affected by different pre-released attributes such as budget,\nruntime, release month, content rating, genre etc. In our analysis, we have\nfound that having a star cast/director has a positive impact on generated\nrevenue. We introduce a novel approach for calculating the star power of a\nmovie. Based on our analysis we select a set of attributes as features and\ntrain different machine learning algorithms to predict a movie's expected\nrevenue. Based on generated revenue, we classified the movies in 10 categories\nand achieved a one-class-away accuracy rate of almost 60% (bingo accuracy of\n30%). All the generated datasets and analysis codes are available online. We\nalso made the source codes of our scraper bots public, so that researchers\ninterested in extending this work can easily modify these bots as they need and\nprepare their own up-to-date datasets.",
    "descriptor": "",
    "authors": [
      "Arnab Sen Sharma",
      "Tirtha Roy",
      "Sadique Ahmmod Rifat",
      "Maruf Ahmed Mridul"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.07039"
  },
  {
    "id": "arXiv:2110.07040",
    "title": "Data Incubation -- Synthesizing Missing Data for Handwriting Recognition",
    "abstract": "In this paper, we demonstrate how a generative model can be used to build a\nbetter recognizer through the control of content and style. We are building an\nonline handwriting recognizer from a modest amount of training samples. By\ntraining our controllable handwriting synthesizer on the same data, we can\nsynthesize handwriting with previously underrepresented content (e.g., URLs and\nemail addresses) and style (e.g., cursive and slanted). Moreover, we propose a\nframework to analyze a recognizer that is trained with a mixture of real and\nsynthetic training data. We use the framework to optimize data synthesis and\ndemonstrate significant improvement on handwriting recognition over a model\ntrained on real data only. Overall, we achieve a 66% reduction in Character\nError Rate.",
    "descriptor": "",
    "authors": [
      "Jen-Hao Rick Chang",
      "Martin Bresler",
      "Youssouf Chherawala",
      "Adrien Delaye",
      "Thomas Deselaers",
      "Ryan Dixon",
      "Oncel Tuzel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07040"
  },
  {
    "id": "arXiv:2110.07043",
    "title": "Why Out-of-distribution Detection in CNNs Does Not Like Mahalanobis --  and What to Use Instead",
    "abstract": "Convolutional neural networks applied for real-world classification tasks\nneed to recognize inputs that are far or out-of-distribution (OoD) with respect\nto the known or training data. To achieve this, many methods estimate\nclass-conditional posterior probabilities and use confidence scores obtained\nfrom the posterior distributions. Recent works propose to use multivariate\nGaussian distributions as models of posterior distributions at different layers\nof the CNN (i.e., for low- and upper-level features), which leads to the\nconfidence scores based on the Mahalanobis distance. However, this procedure\ninvolves estimating probability density in high dimensional data using the\ninsufficient number of observations (e.g. the dimensionality of features at the\nlast two layers in the ResNet-101 model are 2048 and 1024, with ca. 1000\nobservations per class used to estimate density). In this work, we want to\naddress this problem. We show that in many OoD studies in high-dimensional\ndata, LOF-based (Local Outlierness-Factor) methods outperform the parametric,\nMahalanobis distance-based methods. This motivates us to propose the\nnonparametric, LOF-based method of generating the confidence scores for CNNs.\nWe performed several feasibility studies involving ResNet-101 and\nEffcientNet-B3, based on CIFAR-10 and ImageNet (as known data), and CIFAR-100,\nSVHN, ImageNet2010, Places365, or ImageNet-O (as outliers). We demonstrated\nthat nonparametric LOF-based confidence estimation can improve current\nMahalanobis-based SOTA or obtain similar performance in a simpler way.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Kamil Szyc",
      "Tomasz Walkowiak",
      "Henryk Maciejewski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07043"
  },
  {
    "id": "arXiv:2110.07045",
    "title": "Investigating Health-Aware Smart-Nudging with Machine Learning to Help  People Pursue Healthier Eating-Habits",
    "abstract": "Food-choices and eating-habits directly contribute to our long-term health.\nThis makes the food recommender system a potential tool to address the global\ncrisis of obesity and malnutrition. Over the past decade,\nartificial-intelligence and medical researchers became more invested in\nresearching tools that can guide and help people make healthy and thoughtful\ndecisions around food and diet. In many typical (Recommender System) RS\ndomains, smart nudges have been proven effective in shaping users' consumption\npatterns. In recent years, knowledgeable nudging and incentifying choices\nstarted getting attention in the food domain as well. To develop smart nudging\nfor promoting healthier food choices, we combined Machine Learning and RS\ntechnology with food-healthiness guidelines from recognized health\norganizations, such as the World Health Organization, Food Standards Agency,\nand the National Health Service United Kingdom. In this paper, we discuss our\nresearch on, persuasive visualization for making users aware of the healthiness\nof the recommended recipes. Here, we propose three novel nudging technology,\nthe WHO-BubbleSlider, the FSA-ColorCoading, and the DRCI-MLCP, that encourage\nusers to choose healthier recipes. We also propose a Topic Modeling based\nportion-size recommendation algorithm. To evaluate our proposed smart-nudges,\nwe conducted an online user study with 96 participants and 92250 recipes.\nResults showed that, during the food decision-making process, appropriate\nhealthiness cues make users more likely to click, browse, and choose healthier\nrecipes over less healthy ones.",
    "descriptor": "",
    "authors": [
      "Mansura A Khan",
      "Khalil Muhammad",
      "Barry Smyth",
      "David Coyle"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07045"
  },
  {
    "id": "arXiv:2110.07046",
    "title": "Deep Metric Learning with Locality Sensitive Angular Loss for  Self-Correcting Source Separation of Neural Spiking Signals",
    "abstract": "Neurophysiological time series, such as electromyographic signal and\nintracortical recordings, are typically composed of many individual spiking\nsources, the recovery of which can give fundamental insights into the\nbiological system of interest or provide neural information for man-machine\ninterfaces. For this reason, source separation algorithms have become an\nincreasingly important tool in neuroscience and neuroengineering. However, in\nnoisy or highly multivariate recordings these decomposition techniques often\nmake a large number of errors, which degrades human-machine interfacing\napplications and often requires costly post-hoc manual cleaning of the output\nlabel set of spike timestamps. To address both the need for automated post-hoc\ncleaning and robust separation filters we propose a methodology based on deep\nmetric learning, using a novel loss function which maintains intra-class\nvariance, creating a rich embedding space suitable for both label cleaning and\nthe discovery of new activations. We then validate this method with an\nartificially corrupted label set based on source-separated high-density surface\nelectromyography recordings, recovering the original timestamps even in extreme\ndegrees of feature and class-dependent label noise. This approach enables a\nneural network to learn to accurately decode neurophysiological time series\nusing any imperfect method of labelling the signal.",
    "descriptor": "\nComments: 16 pages, 7 figures\n",
    "authors": [
      "Alexander Kenneth Clarke",
      "Dario Farina"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07046"
  },
  {
    "id": "arXiv:2110.07050",
    "title": "Competitive Multi-Agent Load Balancing with Adaptive Policies in  Wireless Networks",
    "abstract": "Using Machine Learning (ML) techniques for the next generation wireless\nnetworks have shown promising results in the recent years, due to high learning\nand adaptation capability of ML algorithms. More specifically, ML techniques\nhave been used for load balancing in Self-Organizing Networks (SON). In the\ncontext of load balancing and ML, several studies propose network management\nautomation (NMA) from the perspective of a single and centralized agent.\nHowever, a single agent domain does not consider the interaction among the\nagents. In this paper, we propose a more realistic load balancing approach\nusing novel Multi-Agent Deep Deterministic Policy Gradient with Adaptive\nPolicies (MADDPG-AP) scheme that considers throughput, resource block\nutilization and latency in the network. We compare our proposal with a\nsingle-agent RL algorithm named Clipped Double Q-Learning (CDQL) . Simulation\nresults reveal a significant improvement in latency, packet loss ratio and\nconvergence time",
    "descriptor": "",
    "authors": [
      "Pedro Enrique Iturria-Rivera",
      "Melike Erol-Kantarci"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07050"
  },
  {
    "id": "arXiv:2110.07052",
    "title": "6G: Connectivity in the Era of Distributed Intelligence",
    "abstract": "The confluence of 5G and AI is transforming wireless networks to deliver\ndiverse services at the Edge, driving towards a vision of pervasive distributed\nintelligence. Future 6G networks will need to deliver quality of experience\nthrough seamless integration of communication, computation and AI. Therefore,\nnetworks must become intelligent, distributed, scalable, and programmable\nplatforms across the continuum of data delivery to address the ever-increasing\nservice requirements and deployment complexity. We present novel results across\nthree research directions that are expected to be integral to 6G systems and\nalso discuss newer 6G metrics.",
    "descriptor": "",
    "authors": [
      "Shilpa Talwar",
      "Nageen Himayat",
      "Hosein Nikopour",
      "Feng Xue",
      "Geng Wu",
      "Vida Ilderem"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07052"
  },
  {
    "id": "arXiv:2110.07058",
    "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
    "abstract": "We introduce Ego4D, a massive-scale egocentric video dataset and benchmark\nsuite. It offers 3,025 hours of daily-life activity video spanning hundreds of\nscenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique\ncamera wearers from 74 worldwide locations and 9 different countries. The\napproach to collection is designed to uphold rigorous privacy and ethics\nstandards with consenting participants and robust de-identification procedures\nwhere relevant. Ego4D dramatically expands the volume of diverse egocentric\nvideo footage publicly available to the research community. Portions of the\nvideo are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,\nand/or synchronized videos from multiple egocentric cameras at the same event.\nFurthermore, we present a host of new benchmark challenges centered around\nunderstanding the first-person visual experience in the past (querying an\nepisodic memory), present (analyzing hand-object manipulation, audio-visual\nconversation, and social interactions), and future (forecasting activities). By\npublicly sharing this massive annotated dataset and benchmark suite, we aim to\npush the frontier of first-person perception. Project page:\nhttps://ego4d-data.org/",
    "descriptor": "",
    "authors": [
      "Kristen Grauman",
      "Andrew Westbury",
      "Eugene Byrne",
      "Zachary Chavis",
      "Antonino Furnari",
      "Rohit Girdhar",
      "Jackson Hamburger",
      "Hao Jiang",
      "Miao Liu",
      "Xingyu Liu",
      "Miguel Martin",
      "Tushar Nagarajan",
      "Ilija Radosavovic",
      "Santhosh Kumar Ramakrishnan",
      "Fiona Ryan",
      "Jayant Sharma",
      "Michael Wray",
      "Mengmeng Xu",
      "Eric Zhongcong Xu",
      "Chen Zhao",
      "Siddhant Bansal",
      "Dhruv Batra",
      "Vincent Cartillier",
      "Sean Crane",
      "Tien Do",
      "Morrie Doulaty",
      "Akshay Erapalli",
      "Christoph Feichtenhofer",
      "Adriano Fragomeni",
      "Qichen Fu",
      "Christian Fuegen",
      "Abrham Gebreselasie",
      "Cristina Gonzalez",
      "James Hillis",
      "Xuhua Huang",
      "Yifei Huang",
      "Wenqi Jia",
      "Weslie Khoo",
      "Jachym Kolar",
      "Satwik Kottur",
      "Anurag Kumar",
      "Federico Landini",
      "Chao Li",
      "Yanghao Li",
      "Zhenqiang Li",
      "Karttikeya Mangalam",
      "Raghava Modhugu",
      "Jonathan Munro",
      "Tullie Murrell",
      "Takumi Nishiyasu",
      "Will Price",
      "Paola Ruiz Puentes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07058"
  },
  {
    "id": "arXiv:2110.07059",
    "title": "Subspace Regularizers for Few-Shot Class Incremental Learning",
    "abstract": "Few-shot class incremental learning -- the problem of updating a trained\nclassifier to discriminate among an expanded set of classes with limited\nlabeled data -- is a key challenge for machine learning systems deployed in\nnon-stationary environments. Existing approaches to the problem rely on complex\nmodel architectures and training procedures that are difficult to tune and\nre-use. In this paper, we present an extremely simple approach that enables the\nuse of ordinary logistic regression classifiers for few-shot incremental\nlearning. The key to this approach is a new family of subspace regularization\nschemes that encourage weight vectors for new classes to lie close to the\nsubspace spanned by the weights of existing classes. When combined with\npretrained convolutional feature extractors, logistic regression models trained\nwith subspace regularization outperform specialized, state-of-the-art\napproaches to few-shot incremental image classification by up to 22% on the\nminiImageNet dataset. Because of its simplicity, subspace regularization can be\nstraightforwardly extended to incorporate additional background information\nabout the new classes (including class names and descriptions specified in\nnatural language); these further improve accuracy by up to 2%. Our results show\nthat simple geometric regularization of class representations offers an\neffective tool for continual learning.",
    "descriptor": "",
    "authors": [
      "Afra Feyza Aky\u00fcrek",
      "Ekin Aky\u00fcrek",
      "Derry Wijaya",
      "Jacob Andreas"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07059"
  },
  {
    "id": "arXiv:2110.07064",
    "title": "Variance Minimization in the Wasserstein Space for Invariant Causal  Prediction",
    "abstract": "Selecting powerful predictors for an outcome is a cornerstone task for\nmachine learning. However, some types of questions can only be answered by\nidentifying the predictors that causally affect the outcome. A recent approach\nto this causal inference problem leverages the invariance property of a causal\nmechanism across differing experimental environments (Peters et al., 2016;\nHeinze-Deml et al., 2018). This method, invariant causal prediction (ICP), has\na substantial computational defect -- the runtime scales exponentially with the\nnumber of possible causal variables. In this work, we show that the approach\ntaken in ICP may be reformulated as a series of nonparametric tests that scales\nlinearly in the number of predictors. Each of these tests relies on the\nminimization of a novel loss function -- the Wasserstein variance -- that is\nderived from tools in optimal transport theory and is used to quantify\ndistributional variability across environments. We prove under mild assumptions\nthat our method is able to recover the set of identifiable direct causes, and\nwe demonstrate in our experiments that it is competitive with other benchmark\ncausal discovery algorithms.",
    "descriptor": "",
    "authors": [
      "Guillaume Martinet",
      "Alexander Strzalkowski",
      "Barbara E. Engelhardt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2110.07064"
  },
  {
    "id": "arXiv:2110.07066",
    "title": "An algorithm for a fairer and better voting system",
    "abstract": "The major finding, of this article, is an ensemble method, but more exactly,\na novel, better ranked voting system (and other variations of it), that aims to\nsolve the problem of finding the best candidate to represent the voters. We\nhave the source code on GitHub, for making realistic simulations of elections,\nbased on artificial intelligence for comparing different variations of the\nalgorithm, and other already known algorithms.\nWe have convincing evidence that our algorithm is better than Instant-Runoff\nVoting, Preferential Block Voting, Single Transferable Vote, and First Past The\nPost (if certain, natural conditions are met, to support the wisdom of the\ncrowds). By also comparing with the best voter, we demonstrated the wisdom of\nthe crowds, suggesting that democracy (distributed system) is a better option\nthan dictatorship (centralized system), if those certain, natural conditions\nare met.\nVoting systems are not restricted to politics, they are ensemble methods for\nartificial intelligence, but the context of this article is natural\nintelligence. It is important to find a system that is fair (e.g. freedom of\nexpression on the ballot exists), especially when the outcome of the voting\nsystem has social impact: some voting systems have the unfair inevitability to\ntrend (over time) towards the same two major candidates (Duverger's law).",
    "descriptor": "\nComments: 26 pages (4 essential pages to read, 17 pages of simulation results, 4 pages of appendix, 1 page of table of contents and references)\n",
    "authors": [
      "Gabriel-Claudiu Grama"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07066"
  },
  {
    "id": "arXiv:2110.07067",
    "title": "Offline Reinforcement Learning for Autonomous Driving with Safety and  Exploration Enhancement",
    "abstract": "Reinforcement learning (RL) is a powerful data-driven control method that has\nbeen largely explored in autonomous driving tasks. However, conventional RL\napproaches learn control policies through trial-and-error interactions with the\nenvironment and therefore may cause disastrous consequences such as collisions\nwhen testing in real traffic. Offline RL has recently emerged as a promising\nframework to learn effective policies from previously-collected, static\ndatasets without the requirement of active interactions, making it especially\nappealing for autonomous driving applications. Despite promising, existing\noffline RL algorithms such as Batch-Constrained deep Q-learning (BCQ) generally\nlead to rather conservative policies with limited exploration efficiency. To\naddress such issues, this paper presents an enhanced BCQ algorithm by employing\na learnable parameter noise scheme in the perturbation model to increase the\ndiversity of observed actions. In addition, a Lyapunov-based safety enhancement\nstrategy is incorporated to constrain the explorable state space within a safe\nregion. Experimental results in highway and parking traffic scenarios show that\nour approach outperforms the conventional RL method, as well as the\nstate-of-the-art offline RL algorithms.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Tianyu Shi",
      "Dong Chen",
      "Kaian Chen",
      "Zhaojian Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07067"
  },
  {
    "id": "arXiv:2110.07070",
    "title": "Fast Hand Detection in Collaborative Learning Environments",
    "abstract": "Long-term object detection requires the integration of frame-based results\nover several seconds. For non-deformable objects, long-term detection is often\naddressed using object detection followed by video tracking. Unfortunately,\ntracking is inapplicable to objects that undergo dramatic changes in appearance\nfrom frame to frame. As a related example, we study hand detection over long\nvideo recordings in collaborative learning environments. More specifically, we\ndevelop long-term hand detection methods that can deal with partial occlusions\nand dramatic changes in appearance.\nOur approach integrates object-detection, followed by time projections,\nclustering, and small region removal to provide effective hand detection over\nlong videos. The hand detector achieved average precision (AP) of 72% at 0.5\nintersection over union (IoU). The detection results were improved to 81% by\nusing our optimized approach for data augmentation. The method runs at 4.7x the\nreal-time with AP of 81% at 0.5 intersection over the union. Our method reduced\nthe number of false-positive hand detections by 80% by improving IoU ratios\nfrom 0.2 to 0.5. The overall hand detection system runs at 4x real-time.",
    "descriptor": "",
    "authors": [
      "Sravani Teeparthi",
      "Venkatesh Jatla",
      "Marios S. Pattichis",
      "Sylvia Celedon Pattichis",
      "Carlos LopezLeiva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07070"
  },
  {
    "id": "arXiv:2110.07077",
    "title": "Federated Learning Over Cellular-Connected UAV Networks with Non-IID  Datasets",
    "abstract": "Federated learning (FL) is a promising distributed learning technique\nparticularly suitable for wireless learning scenarios since it can accomplish a\nlearning task without raw data transportation so as to preserve data privacy\nand lower network resource consumption. However, current works on FL over\nwireless communication do not profoundly study the fundamental performance of\nFL that suffers from data delivery outage due to network interference and data\nheterogeneity among mobile clients. To accurately exploit the performance of FL\nover wireless communication, this paper proposes a new FL model over a\ncellular-connected unmanned aerial vehicle (UAV) network, which characterizes\ndata delivery outage from UAV clients to their server and data heterogeneity\namong the datasets of UAV clients. We devise a simulation-based approach to\nevaluating the convergence performance of the proposed FL model. We then\npropose a tractable analytical framework of the uplink outage probability in\nthe cellular-connected UAV network and derive a neat expression of the uplink\noutage probability, which reveals how the proposed FL model is impacted by data\ndelivery outage and UAV deployment. Extensive numerical simulations are\nconducted to show the consistency between the estimated and simulated\nperformances.",
    "descriptor": "\nComments: 6 pages, 3 figures, conference\n",
    "authors": [
      "Di-Chun Liang",
      "Chun-Hung Liu",
      "Rung-Hung Gau",
      "Lu Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07077"
  },
  {
    "id": "arXiv:2110.07079",
    "title": "Modelling wave propagation in elastic solids via high-order accurate  implicit-mesh discontinuous Galerkin methods",
    "abstract": "A high-order accurate implicit-mesh discontinuous Galerkin framework for wave\npropagation in single-phase and bi-phase solids is presented. The framework\nbelongs to the embedded-boundary techniques and its novelty regards the spatial\ndiscretization, which enables boundary and interface conditions to be enforced\nwith high-order accuracy on curved embedded geometries. High-order accuracy is\nachieved via high-order quadrature rules for implicitly-defined domains and\nboundaries, whilst a cell-merging strategy addresses the presence of small cut\ncells. The framework is used to discretize the governing equations of\nelastodynamics, written using a first-order hyperbolic momentum-strain\nformulation, and an exact Riemann solver is employed to compute the numerical\nflux at the interface between dissimilar materials with general anisotropic\nproperties. The space-discretized equations are then advanced in time using\nexplicit high-order Runge-Kutta algorithms. Several two- and three-dimensional\nnumerical tests including dynamic adaptive mesh refinement are presented to\ndemonstrate the high-order accuracy and the capability of the method in the\nelastodynamic analysis of single- and bi-phases solids containing complex\ngeometries.",
    "descriptor": "",
    "authors": [
      "Vincenzo Gulizzi",
      "Robert Saye"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07079"
  },
  {
    "id": "arXiv:2110.07082",
    "title": "The Impact of Spatiotemporal Augmentations on Self-Supervised  Audiovisual Representation Learning",
    "abstract": "Contrastive learning of auditory and visual perception has been extremely\nsuccessful when investigated individually. However, there are still major\nquestions on how we could integrate principles learned from both domains to\nattain effective audiovisual representations. In this paper, we present a\ncontrastive framework to learn audiovisual representations from unlabeled\nvideos. The type and strength of augmentations utilized during self-supervised\npre-training play a crucial role for contrastive frameworks to work\nsufficiently. Hence, we extensively investigate composition of temporal\naugmentations suitable for learning audiovisual representations; we find lossy\nspatio-temporal transformations that do not corrupt the temporal coherency of\nvideos are the most effective. Furthermore, we show that the effectiveness of\nthese transformations scales with higher temporal resolution and stronger\ntransformation intensity. Compared to self-supervised models pre-trained on\nonly sampling-based temporal augmentation, self-supervised models pre-trained\nwith our temporal augmentations lead to approximately 6.5% gain on linear\nclassifier performance on AVE dataset. Lastly, we show that despite their\nsimplicity, our proposed transformations work well across self-supervised\nlearning frameworks (SimSiam, MoCoV3, etc), and benchmark audiovisual dataset\n(AVE).",
    "descriptor": "",
    "authors": [
      "Haider Al-Tahan",
      "Yalda Mohsenzadeh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07082"
  },
  {
    "id": "arXiv:2110.07083",
    "title": "Dynamic Conflict Resolution of IoT Services in Smart Homes",
    "abstract": "We propose a novel conflict resolution framework for IoT services in\nmulti-resident smart homes. The proposed framework employs a preference\nextraction model based on a temporal proximity strategy. We design a preference\naggregation model using a matrix factorization-based approach (i.e., singular\nvalue decomposition). The concepts of current resident item matrix and ideal\nresident item matrix are introduced as key criteria to cater to the conflict\nresolution framework. Finally, a set of experiments on real-world datasets are\nconducted to show the effectiveness of the proposed approach.",
    "descriptor": "\nComments: 15 pages, 5 figures, accepted and to be published in the proceedings of 19th International Conference on Service Oriented Computing (ICSOC 2021)\n",
    "authors": [
      "Dipankar Chaki",
      "Athman Bouguettaya"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.07083"
  },
  {
    "id": "arXiv:2110.07084",
    "title": "Online Bipartite Matching with Reusable Resources",
    "abstract": "We study the classic online bipartite matching problem with a twist: offline\nnodes are reusable any number of times. Every offline node $i$ becomes\navailable $d$ steps after it was assigned to. Nothing better than a\n$0.5$-approximation, obtained by the trivial deterministic greedy algorithm,\nwas known for this problem. We give the first approximation factor beating\n$0.5$, namely a $0.505$ approximation, by suitably adapting and interpreting\nthe powerful technique of Online Correlated Selection.",
    "descriptor": "",
    "authors": [
      "Steven Delong",
      "Alireza Farhadi",
      "Rad Niazadeh",
      "Balasubramanian Sivan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07084"
  },
  {
    "id": "arXiv:2110.07087",
    "title": "Readability and Understandability of Snippets Recommended by  General-purpose Web Search Engines: a Comparative Study",
    "abstract": "Developers often search for reusable code snippets on general-purpose web\nsearch engines like Google, Yahoo! or Microsoft Bing. But some of these code\nsnippets may have poor quality in terms of readability or understandability. In\nthis paper, we propose an empirical analysis to analyze the readability and\nunderstandability score from snippets extracted from the web using three\nindependent variables: ranking, general-purpose web search engine, and\nrecommended site. We collected the top-5 recommended sites and their respective\ncode snippet recommendations using Google, Yahoo!, and Bing for 9,480 queries,\nand evaluate their readability and understandability scores. We found that some\nrecommended sites have significantly better readability and understandability\nscores than others. The better-ranked code snippet is not necessarily more\nreadable or understandable than a lower-ranked code snippet for all\ngeneral-purpose web search engines. Moreover, considering the readability\nscore, Google has better-ranked code snippets compared to Yahoo! or Microsoft\nBing",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Carlos Eduardo C. Dantas",
      "Marcelo A. Maia"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.07087"
  },
  {
    "id": "arXiv:2110.07090",
    "title": "MIMICause : Defining, identifying and predicting types of causal  relationships between biomedical concepts from clinical notes",
    "abstract": "Understanding of causal narratives communicated in clinical notes can help\nmake strides towards personalized healthcare. In this work, MIMICause, we\npropose annotation guidelines, develop an annotated corpus and provide baseline\nscores to identify types and direction of causal relations between a pair of\nbiomedical concepts in clinical notes; communicated implicitly or explicitly,\nidentified either in a single sentence or across multiple sentences.\nWe annotate a total of 2714 de-identified examples sampled from the 2018 n2c2\nshared task dataset and train four different language model based\narchitectures. Annotation based on our guidelines achieved a high\ninter-annotator agreement i.e. Fleiss' kappa score of 0.72 and our model for\nidentification of causal relation achieved a macro F1 score of 0.56 on test\ndata. The high inter-annotator agreement for clinical text shows the quality of\nour annotation guidelines while the provided baseline F1 score sets the\ndirection for future research towards understanding narratives in clinical\ntexts.",
    "descriptor": "",
    "authors": [
      "Vivek Khetan",
      "Md Imbesat Hassan Rizvi",
      "Jessica Huber",
      "Paige Bartusiak",
      "Bogdan Sacaleanu",
      "Andrew Fano"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07090"
  },
  {
    "id": "arXiv:2110.07096",
    "title": "Identifying Introductions in Podcast Episodes from Automatically  Generated Transcripts",
    "abstract": "As the volume of long-form spoken-word content such as podcasts explodes,\nmany platforms desire to present short, meaningful, and logically coherent\nsegments extracted from the full content. Such segments can be consumed by\nusers to sample content before diving in, as well as used by the platform to\npromote and recommend content. However, little published work is focused on the\nsegmentation of spoken-word content, where the errors (noise) in transcripts\ngenerated by automatic speech recognition (ASR) services poses many challenges.\nHere we build a novel dataset of complete transcriptions of over 400 podcast\nepisodes, in which we label the position of introductions in each episode.\nThese introductions contain information about the episodes' topics, hosts, and\nguests, providing a valuable summary of the episode content, as it is created\nby the authors. We further augment our dataset with word substitutions to\nincrease the amount of available training data. We train three Transformer\nmodels based on the pre-trained BERT and different augmentation strategies,\nwhich achieve significantly better performance compared with a static embedding\nmodel, showing that it is possible to capture generalized, larger-scale\nstructural information from noisy, loosely-organized speech data. This is\nfurther demonstrated through an analysis of the models' inner architecture. Our\nmethods and dataset can be used to facilitate future work on the\nstructure-based segmentation of spoken-word content.",
    "descriptor": "\nComments: Accepted in PodRecs 2021, a RecSys workshop\n",
    "authors": [
      "Elise Jing",
      "Kristiana Schneck",
      "Dennis Egan",
      "Scott A. Waterman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07096"
  },
  {
    "id": "arXiv:2110.07097",
    "title": "A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained  Inter-species Classification",
    "abstract": "This study aims to explore different pre-trained models offered in the\nTorchvision package which is available in the PyTorch library. And investigate\ntheir effectiveness on fine-grained images classification. Transfer Learning is\nan effective method of achieving extremely good performance with insufficient\ntraining data. In many real-world situations, people cannot collect sufficient\ndata required to train a deep neural network model efficiently. Transfer\nLearning models are pre-trained on a large data set, and can bring a good\nperformance on smaller datasets with significantly lower training time.\nTorchvision package offers us many models to apply the Transfer Learning on\nsmaller datasets. Therefore, researchers may need a guideline for the selection\nof a good model. We investigate Torchvision pre-trained models on four\ndifferent data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, and\nOxford 102 Flowers. These data sets have images of different resolutions, class\nnumbers, and different achievable accuracies. We also apply their usual\nfully-connected layer and the Spinal fully-connected layer to investigate the\neffectiveness of SpinalNet. The Spinal fully-connected layer brings better\nperformance in most situations. We apply the same augmentation for different\nmodels for the same data set for a fair comparison. This paper may help future\nComputer Vision researchers in choosing a proper Transfer Learning model.",
    "descriptor": "\nComments: Accepted\n",
    "authors": [
      "Feras Albardi",
      "H M Dipu Kabir",
      "Md Mahbub Islam Bhuiyan",
      "Parham M. Kebria",
      "Abbas Khosravi",
      "Saeid Nahavandi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07097"
  },
  {
    "id": "arXiv:2110.07099",
    "title": "An Energy-Based Discontinuous Galerkin Method with Tame CFL Numbers for  the Wave Equation",
    "abstract": "We extend and analyze the energy-based discontinuous Galerkin method for\nsecond order wave equations on staggered and structured meshes. By combining\nspatial staggering with local time-stepping near boundaries, the method\novercomes the typical numerical stiffness associated with high order piecewise\npolynomial approximations. In one space dimension with periodic boundary\nconditions and suitably chosen numerical fluxes, we prove bounds on the spatial\noperators that establish stability for CFL numbers $c \\frac {\\Delta t}{h} < C$\nindependent of order when stability-enhanced explicit time-stepping schemes of\nmatching order are used. For problems on bounded domains and in higher\ndimensions we demonstrate numerically that one can march explicitly with large\ntime steps at high order temporal and spatial accuracy.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Daniel Appel\u00f6",
      "Lu Zhang",
      "Thomas Hagstrom",
      "Fengyan Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07099"
  },
  {
    "id": "arXiv:2110.07103",
    "title": "Video-based cattle identification and action recognition",
    "abstract": "We demonstrate a working prototype for the monitoring of cow welfare by\nautomatically analysing the animal behaviours. Deep learning models have been\ndeveloped and tested with videos acquired in a farm, and a precision of 81.2\\%\nhas been achieved for cow identification. An accuracy of 84.4\\% has been\nachieved for the detection of drinking events, and 94.4\\% for the detection of\ngrazing events. Experimental results show that the proposed deep learning\nmethod can be used to identify the behaviours of individual animals to enable\nautomated farm provenance. Our raw and ground-truth dataset will be released as\nthe first public video dataset for cow identification and action recognition.\nRecommendations for further development are also provided.",
    "descriptor": "\nComments: 5 pages, 7 figures, DICTA2021\n",
    "authors": [
      "Chuong Nguyen",
      "Dadong Wang",
      "Karl Von Richter",
      "Philip Valencia",
      "Flavio A. P. Alvarenga",
      "Gregory Bishop-Hurley"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07103"
  },
  {
    "id": "arXiv:2110.07105",
    "title": "Integrated Path Planning and Tracking Control of Marine Current Turbine  in Uncertain Ocean Environments",
    "abstract": "This paper presents an integrated path planning and tracking control of\nmarine hydrokinetic energy harvesting devices. To address the highly nonlinear\nand uncertain oceanic environment, the path planner is designed based on a\nreinforcement learning (RL) approach by fully exploring the historical ocean\ncurrent profiles. The planner will search for a path to optimize a chosen cost\ncriterion, such as maximizing the total harvested energy for a given time.\nModel predictive control (MPC) is then utilized to design the tracking control\nfor the optimal path command from the planner subject to problem constraints.\nThe planner and the tracking control are accommodated in an integrated\nframework to optimize these two parts in a real-time manner. The proposed\napproach is validated on a marine current turbine (MCT) that executes vertical\nwaypoint path searching to maximize the net power due to spatiotemporal\nuncertainties in the ocean environment, as well as the path following via an\nMPC tracking controller to navigate the MCT to the optimal path. Results\ndemonstrate that the path planning increases harnessed power compared to the\nbaseline (i.e., maintaining MCT at an equilibrium depth), and the tracking\ncontroller can successfully follow the reference path under different shear\nprofiles.",
    "descriptor": "",
    "authors": [
      "Arezoo Hasankhani",
      "Ertugrul Baris Ondes",
      "Yufei Tang",
      "Cornel Sultan",
      "James VanZwieten"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07105"
  },
  {
    "id": "arXiv:2110.07107",
    "title": "On Downlink Interference Decoding In Multi-Cell Massive MIMO Systems",
    "abstract": "In this paper, the downlink of a multi-cell massive MIMO system is considered\nwhere the channel state information (CSI) is estimated via pilot symbols that\nare orthogonal in a cell but re-used in other cells. Re-using the pilots,\nhowever, contaminates the CSI estimate at each base station (BS) by the channel\nof the users sharing the same pilot in other cells. The resulting inter-cell\ninterference does not vanish even when the number of BS antennas $M$ is large,\ni.e., $M\\rightarrow\\infty$, and thus the rates achieved by treating\ninterference as noise (TIN) saturate even if $M\\rightarrow\\infty$. In this\npaper, interference aware decoding schemes based on simultaneous unique\ndecoding (SD) and simultaneous non-unique decoding (SND) of the full\ninterference or a part of the interference (PD) are studied with two different\nlinear precoding techniques: maximum ratio transmission (MRT) and zero forcing\n(ZF). The resulting rates are shown to grow unbounded as $M\\rightarrow\\infty$.\nIn addition, the rates achievable via SD/SND/PD for finite $M$ are derived\nusing a worst-case uncorrelated noise technique, which are shown to scale as\n$\\mathcal{O}(\\log M)$. To compare the performance of different schemes, the\nmaximum symmetric rate problem is studied, where it is confirmed that with\nlarge, yet practical, values of $M$, SND strictly outperforms TIN, and also\nthat PD strictly outperforms SND.",
    "descriptor": "\nComments: 6 pages, 4 figures, presented at Biennial Symposium on Communications (BSC 2021)\n",
    "authors": [
      "Meysam Shahrbaf Motlagh",
      "Subhajit Majhi",
      "Patrick Mitran",
      "Hideki Ochiai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07107"
  },
  {
    "id": "arXiv:2110.07110",
    "title": "Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast",
    "abstract": "Though image-level weakly supervised semantic segmentation (WSSS) has\nachieved great progress with Class Activation Map (CAM) as the cornerstone, the\nlarge supervision gap between classification and segmentation still hampers the\nmodel to generate more complete and precise pseudo masks for segmentation.\nIn this study, we explore two implicit but intuitive constraints, i.e.,\ncross-view feature semantic consistency and intra(inter)-class\ncompactness(dispersion), to narrow the supervision gap.\nTo this end, we propose two novel pixel-to-prototype contrast regularization\nterms that are conducted cross different views and within per single view of an\nimage, respectively. Besides, we adopt two sample mining strategies, named\nsemi-hard prototype mining and hard pixel sampling, to better leverage hard\nexamples while reducing incorrect contrasts caused due to the absence of\nprecise pixel-wise labels.\nOur method can be seamlessly incorporated into existing WSSS models without\nany changes to the base network and does not incur any extra inference burden.\nExperiments on standard benchmark show that our method consistently improves\ntwo strong baselines by large margins, demonstrating the effectiveness of our\nmethod. Specifically, built on top of SEAM, we improve the initial seed mIoU on\nPASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed with our method, we\nincrease the segmentation mIoU of EPS from 70.8% to 73.6%, achieving new\nstate-of-the-art.",
    "descriptor": "",
    "authors": [
      "Ye Du",
      "Zehua Fu",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07110"
  },
  {
    "id": "arXiv:2110.07111",
    "title": "A Novel Traffic Simulation Framework for Testing Autonomous Vehicles  Using SUMO and CARLA",
    "abstract": "Traffic simulation is an efficient and cost-effective way to test Autonomous\nVehicles (AVs) in a complex and dynamic environment. Numerous studies have been\nconducted for AV evaluation using traffic simulation over the past decades.\nHowever, the current simulation environments fall behind on two fronts -- the\nbackground vehicles (BVs) fail to simulate naturalistic driving behavior and\nthe existing environments do not test the entire pipeline in a modular fashion.\nThis study aims to propose a simulation framework that creates a complex and\nnaturalistic traffic environment. Specifically, we combine a modified version\nof the Simulation of Urban MObility (SUMO) simulator with the Cars Learning to\nAct (CARLA) simulator to generate a simulation environment that could emulate\nthe complexities of the external environment while providing realistic sensor\noutputs to the AV pipeline. In a past research work, we created an open-source\nPython package called SUMO-Gym which generates a realistic road network and\nnaturalistic traffic through SUMO and combines that with OpenAI Gym to provide\nease of use for the end user. We propose to extend our developed software by\nadding CARLA, which in turn will enrich the perception of the ego vehicle by\nproviding realistic sensors outputs of the AVs surrounding environment. Using\nthe proposed framework, AVs perception, planning, and control could be tested\nin a complex and realistic driving environment. The performance of the proposed\nframework in constructing output generation and AV evaluations are demonstrated\nusing several case studies.",
    "descriptor": "",
    "authors": [
      "Pei Li",
      "Arpan Kusari",
      "David J. LeBlanc"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07111"
  },
  {
    "id": "arXiv:2110.07118",
    "title": "Nuisance-Label Supervision: Robustness Improvement by Free Labels",
    "abstract": "In this paper, we present a Nuisance-label Supervision (NLS) module, which\ncan make models more robust to nuisance factor variations. Nuisance factors are\nthose irrelevant to a task, and an ideal model should be invariant to them. For\nexample, an activity recognition model should perform consistently regardless\nof the change of clothes and background. But our experiments show existing\nmodels are far from this capability. So we explicitly supervise a model with\nnuisance labels to make extracted features less dependent on nuisance factors.\nAlthough the values of nuisance factors are rarely annotated, we demonstrate\nthat besides existing annotations, nuisance labels can be acquired freely from\ndata augmentation and synthetic data. Experiments show consistent improvement\nin robustness towards image corruption and appearance change in action\nrecognition.",
    "descriptor": "\nComments: ICCV 2021 Workshop\n",
    "authors": [
      "Xinyue Wei",
      "Weichao Qiu",
      "Yi Zhang",
      "Zihao Xiao",
      "Alan Yuille"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07118"
  },
  {
    "id": "arXiv:2110.07120",
    "title": "Brittle interpretations: The Vulnerability of TCAV and Other  Concept-based Explainability Tools to Adversarial Attack",
    "abstract": "Methods for model explainability have become increasingly critical for\ntesting the fairness and soundness of deep learning. A number of explainability\ntechniques have been developed which use a set of examples to represent a\nhuman-interpretable concept in a model's activations. In this work we show that\nthese explainability methods can suffer the same vulnerability to adversarial\nattacks as the models they are meant to analyze. We demonstrate this phenomenon\non two well-known concept-based approaches to the explainability of deep\nlearning models: TCAV and faceted feature visualization. We show that by\ncarefully perturbing the examples of the concept that is being investigated, we\ncan radically change the output of the interpretability method, e.g. showing\nthat stripes are not an important factor in identifying images of a zebra. Our\nwork highlights the fact that in safety-critical applications, there is need\nfor security around not only the machine learning pipeline but also the model\ninterpretation process.",
    "descriptor": "",
    "authors": [
      "Davis Brown",
      "Henry Kvinge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07120"
  },
  {
    "id": "arXiv:2110.07121",
    "title": "Secure Precoding in MIMO-NOMA: A Deep Learning Approach",
    "abstract": "A novel signaling design for secure transmission over two-user multiple-input\nmultiple-output non-orthogonal multiple access channel using deep neural\nnetworks (DNNs) is proposed. The goal of the DNN is to form the covariance\nmatrix of users' signals such that the message of each user is transmitted\nreliably while being confidential from its counterpart. The proposed DNN\nlinearly precodes each user's signal before superimposing them and achieves\nnear-optimal performance with significantly lower run time. Simulation results\nshow that the proposed models reach about 98% of the secrecy capacity rates.\nThe spectral efficiency of the DNN precoder is much higher than that of\nexisting analytical linear precoders--e.g., generalized singular value\ndecomposition--and its on-the-fly complexity is several times less than the\nexisting iterative methods.",
    "descriptor": "\nComments: Accepted for publication in the IEEE Wireless Communications Letters\n",
    "authors": [
      "Jordan Pauls",
      "Mojtaba Vaezi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07121"
  },
  {
    "id": "arXiv:2110.07122",
    "title": "Deconfounded Causal Collaborative Filtering",
    "abstract": "Recommender systems may be confounded by various types of confounding factors\n(also called confounders) that may lead to inaccurate recommendations and\nsacrificed recommendation performance. Current approaches to solving the\nproblem usually design each specific model for each specific confounder.\nHowever, real-world systems may include a huge number of confounders and thus\ndesigning each specific model for each specific confounder is unrealistic. More\nimportantly, except for those \"explicit confounders\" that researchers can\nmanually identify and process such as item's position in the ranking list,\nthere are also many \"latent confounders\" that are beyond the imagination of\nresearchers. For example, users' rating on a song may depend on their current\nmood or the current weather, and users' preference on ice creams may depend on\nthe air temperature. Such latent confounders may be unobservable in the\nrecorded training data. To solve the problem, we propose a deconfounded causal\ncollaborative filtering model. We first frame user behaviors with unobserved\nconfounders into a causal graph, and then we design a front-door adjustment\nmodel carefully fused with machine learning to deconfound the influence of\nunobserved confounders. The proposed model is able to handle both global\nconfounders and personalized confounders. Experiments on real-world e-commerce\ndatasets show that our method is able to deconfound unobserved confounders to\nachieve better recommendation performance.",
    "descriptor": "\nComments: 9 pages, 5 figures; comments and suggestions are highly appreciated\n",
    "authors": [
      "Shuyuan Xu",
      "Juntao Tan",
      "Shelby Heinecke",
      "Jia Li",
      "Yongfeng Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07122"
  },
  {
    "id": "arXiv:2110.07126",
    "title": "Root Finding With Interval Arithmetic",
    "abstract": "We consider the solution of nonlinear equations in one real variable, the\nproblem usually called by root finding. Although this is an old problem, we\nbelieve that some aspects of its solution using interval arithmetic are not\nwell understood, and we present our views on this subject. We argue that\nproblems with just one variable are much simpler than problems with more\nvariables, and we should use specific methods for them. We provide an\nimplementation of our ideas in C++, and make this code available under the\nMozilla Public License 2.0.",
    "descriptor": "",
    "authors": [
      "Walter F. Mascarenhas"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07126"
  },
  {
    "id": "arXiv:2110.07127",
    "title": "Monitoring the Mental State of Cooperativeness for Guiding an Elderly  Person in Sit-to-Stand Assistance",
    "abstract": "In providing physical assistance to elderly people, ensuring cooperative\nbehavior from the elderly persons is a critical requirement. In sit-to-stand\nassistance, for example, an older adult must lean forward, so that the body\nmass can shift towards the feet before a caregiver starts lifting the body. An\nexperienced caregiver guides the older adult through verbal communications and\nphysical interactions, so that the older adult may be cooperative throughout\nthe process. This guidance is of paramount importance and is a major challenge\nin introducing a robotic aid to the eldercare environment. The wide-scope goal\nof the current work is to develop an intelligent eldercare robot that can a)\nmonitor the mental state of an older adult, and b) guide the older adult\nthrough an assisting procedure so that he/she can be cooperative in being\nassisted. The current work presents a basic modeling framework for describing a\nhuman's physical behaviors reflecting an internal mental state, and an\nalgorithm for estimating the mental state through interactive observations. The\nsit-to-stand assistance problem is considered for the initial study. A simple\nKalman Filter is constructed for estimating the level of cooperativeness in\nresponse to applied cues, with a thresholding scheme being used to make\njudgments on the cooperativeness state.",
    "descriptor": "\nComments: submitted to IEEE-RAS International Conference on Robotics and Automation (ICRA) 2022\n",
    "authors": [
      "John Bell",
      "H. Harry Asada"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07127"
  },
  {
    "id": "arXiv:2110.07128",
    "title": "WebAssembly enables low latency interoperable augmented and virtual  reality software",
    "abstract": "There is a clear difference in runtime performance between native\napplications that use augmented/virtual reality (AR/VR) device-specific\nhardware and comparable web-based implementations. Here we show that\nWebAssembly (Wasm) offers a promising developer solution that can bring\nnear-native low latency performance to web-based applications, enabling\nhardware-agnostic interoperability at scale through portable bytecode that runs\non any WiFi or cellular data network-enabled AR/VR device. Many software\napplication areas have begun to realize Wasm's potential as a key enabling\ntechnology, but it has yet to establish a robust presence in the AR/VR domain.\nWhen considering the limitations of current web-based AR/VR development\ntechnologies such as WebXR, which provides an existing application programming\ninterface (API) that enables AR/VR capabilities for web-based programs, Wasm\ncan resolve critical issues faced with just-in-time (JIT) compilation, slow\nrun-times, large file sizes and big data, among other challenges. Existing\napplications using Wasm-based WebXR are sparse but growing, and the potential\nfor porting native applications to use this emerging framework will benefit the\nweb-based AR/VR application space and bring it closer to its native\ncounterparts in terms of performance. Taken together, this kind of standardized\n\"write-once-deploy-everywhere\" software framework for AR/VR applications has\nthe potential to consolidate user experiences across different head-mounted\ndisplays and other compatible hardware devices to ultimately create an\ninteroperable AR/VR ecosystem.",
    "descriptor": "\nComments: 9 pages, 2 figures\n",
    "authors": [
      "Bohdan B. Khomtchouk"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07128"
  },
  {
    "id": "arXiv:2110.07130",
    "title": "Region Semantically Aligned Network for Zero-Shot Learning",
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes based on the\nknowledge of seen classes. Previous methods focused on learning direct\nembeddings from global features to the semantic space in hope of knowledge\ntransfer from seen classes to unseen classes. However, an unseen class shares\nlocal visual features with a set of seen classes and leveraging global visual\nfeatures makes the knowledge transfer ineffective. To tackle this problem, we\npropose a Region Semantically Aligned Network (RSAN), which maps local features\nof unseen classes to their semantic attributes. Instead of using global\nfeatures which are obtained by an average pooling layer after an image encoder,\nwe directly utilize the output of the image encoder which maintains local\ninformation of the image. Concretely, we obtain each attribute from a specific\nregion of the output and exploit these attributes for recognition. As a result,\nthe knowledge of seen classes can be successfully transferred to unseen classes\nin a region-bases manner. In addition, we regularize the image encoder through\nattribute regression with a semantic knowledge to extract robust and\nattribute-related visual features. Experiments on several standard ZSL datasets\nreveal the benefit of the proposed RSAN method, outperforming state-of-the-art\nmethods.",
    "descriptor": "\nComments: Accepted to CIKM 2021\n",
    "authors": [
      "Ziyang Wang",
      "Yunhao Gou",
      "Jingjing Li",
      "Yu Zhang",
      "Yang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07130"
  },
  {
    "id": "arXiv:2110.07131",
    "title": "Reverse Maximum Inner Product Search: How to efficiently find users who  would like to buy my item?",
    "abstract": "The MIPS (maximum inner product search), which finds the item with the\nhighest inner product with a given query user, is an essential problem in the\nrecommendation field. It is usual that e-commerce companies face situations\nwhere they want to promote and sell new or discounted items. In these\nsituations, we have to consider a question: who are interested in the items and\nhow to find them? This paper answers this question by addressing a new problem\ncalled reverse maximum inner product search (reverse MIPS). Given a query\nvector and two sets of vectors (user vectors and item vectors), the problem of\nreverse MIPS finds a set of user vectors whose inner product with the query\nvector is the maximum among the query and item vectors. Although the importance\nof this problem is clear, its straightforward implementation incurs a\ncomputationally expensive cost. We therefore propose Simpfer, a simple, fast,\nand exact algorithm for reverse MIPS. In an offline phase, Simpfer builds a\nsimple index that maintains a lower-bound of the maximum inner product. By\nexploiting this index, Simpfer judges whether the query vector can have the\nmaximum inner product or not, for a given user vector, in a constant time.\nBesides, our index enables filtering user vectors, which cannot have the\nmaximum inner product with the query vector, in a batch. We theoretically\ndemonstrate that Simpfer outperforms baselines employing state-of-the-art MIPS\ntechniques. Furthermore, our extensive experiments on real datasets show that\nSimpfer is at least two orders magnitude faster than the baselines.",
    "descriptor": "\nComments: Accepted to RecSys2021\n",
    "authors": [
      "Daichi Amagata",
      "Takahiro Hara"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.07131"
  },
  {
    "id": "arXiv:2110.07137",
    "title": "A CLIP-Enhanced Method for Video-Language Understanding",
    "abstract": "This technical report summarizes our method for the Video-And-Language\nUnderstanding Evaluation (VALUE) challenge\n(https://value-benchmark.github.io/challenge\\_2021.html). We propose a\nCLIP-Enhanced method to incorporate the image-text pretrained knowledge into\ndownstream video-text tasks. Combined with several other improved designs, our\nmethod outperforms the state-of-the-art by $2.4\\%$ ($57.58$ to $60.00$)\nMeta-Ave score on VALUE benchmark.",
    "descriptor": "\nComments: 3 pages, 1 figure, 2 tables. Technical report\n",
    "authors": [
      "Guohao Li",
      "Feng He",
      "Zhifan Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07137"
  },
  {
    "id": "arXiv:2110.07139",
    "title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text  Style Transfer",
    "abstract": "Adversarial attacks and backdoor attacks are two common security threats that\nhang over deep learning. Both of them harness task-irrelevant features of data\nin their implementation. Text style is a feature that is naturally irrelevant\nto most NLP tasks, and thus suitable for adversarial and backdoor attacks. In\nthis paper, we make the first attempt to conduct adversarial and backdoor\nattacks based on text style transfer, which is aimed at altering the style of a\nsentence while preserving its meaning. We design an adversarial attack method\nand a backdoor attack method, and conduct extensive experiments to evaluate\nthem. Experimental results show that popular NLP models are vulnerable to both\nadversarial and backdoor attacks based on text style transfer -- the attack\nsuccess rates can exceed 90% without much effort. It reflects the limited\nability of NLP models to handle the feature of text style that has not been\nwidely realized. In addition, the style transfer-based adversarial and backdoor\nattack methods show superiority to baselines in many aspects. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/StyleAttack.",
    "descriptor": "\nComments: Accepted by the main conference of EMNLP 2021 as a long paper. The camera-ready version\n",
    "authors": [
      "Fanchao Qi",
      "Yangyi Chen",
      "Xurui Zhang",
      "Mukai Li",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07139"
  },
  {
    "id": "arXiv:2110.07141",
    "title": "SoGCN: Second-Order Graph Convolutional Networks",
    "abstract": "Graph Convolutional Networks (GCN) with multi-hop aggregation is more\nexpressive than one-hop GCN but suffers from higher model complexity. Finding\nthe shortest aggregation range that achieves comparable expressiveness and\nminimizes this side effect remains an open question. We answer this question by\nshowing that multi-layer second-order graph convolution (SoGC) is sufficient to\nattain the ability of expressing polynomial spectral filters with arbitrary\ncoefficients. Compared to models with one-hop aggregation, multi-hop\npropagation, and jump connections, SoGC possesses filter representational\ncompleteness while being lightweight, efficient, and easy to implement.\nThereby, we suggest that SoGC is a simple design capable of forming the basic\nbuilding block of GCNs, playing the same role as $3 \\times 3$ kernels in CNNs.\nWe build our Second-Order Graph Convolutional Networks (SoGCN) with SoGC and\ndesign a synthetic dataset to verify its filter fitting capability to validate\nthese points. For real-world tasks, we present the state-of-the-art performance\nof SoGCN on the benchmark of node classification, graph classification, and\ngraph regression datasets.",
    "descriptor": "\nComments: 15 pages, 7 figures\n",
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Hua Lin",
      "Jianbo Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07141"
  },
  {
    "id": "arXiv:2110.07143",
    "title": "bert2BERT: Towards Reusable Pretrained Language Models",
    "abstract": "In recent years, researchers tend to pre-train ever-larger language models to\nexplore the upper limit of deep models. However, large language model\npre-training costs intensive computational resources and most of the models are\ntrained from scratch without reusing the existing pre-trained models, which is\nwasteful. In this paper, we propose bert2BERT, which can effectively transfer\nthe knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a\nlarge model (e.g., BERT_LARGE) through parameter initialization and\nsignificantly improve the pre-training efficiency of the large model.\nSpecifically, we extend the previous function-preserving on Transformer-based\nlanguage model, and further improve it by proposing advanced knowledge for\nlarge model's initialization. In addition, a two-stage pre-training method is\nproposed to further accelerate the training process. We did extensive\nexperiments on representative PLMs (e.g., BERT and GPT) and demonstrate that\n(1) our method can save a significant amount of training cost compared with\nbaselines including learning from scratch, StackBERT and MSLT; (2) our method\nis generic and applicable to different types of pre-trained models. In\nparticular, bert2BERT saves about 45% and 47% computational cost of\npre-training BERT_BASE and GPT_BASE by reusing the models of almost their half\nsizes. The source code will be publicly available upon publication.",
    "descriptor": "",
    "authors": [
      "Cheng Chen",
      "Yichun Yin",
      "Lifeng Shang",
      "Xin Jiang",
      "Yujia Qin",
      "Fengyu Wang",
      "Zhi Wang",
      "Xiao Chen",
      "Zhiyuan Liu",
      "Qun Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07143"
  },
  {
    "id": "arXiv:2110.07145",
    "title": "SpongeCake: A Layered Microflake Surface Appearance Model",
    "abstract": "In this paper, we propose SpongeCake: a layered BSDF model where each layer\nis a volumetric scattering medium, defined using microflake or other phase\nfunctions. We omit any reflecting and refracting interfaces between the layers.\nThe first advantage of this formulation is that an exact and analytic solution\nfor single scattering, regardless of the number of volumetric layers, can be\nderived. We propose to approximate multiple scattering by an additional\nsingle-scattering lobe with modified parameters and a Lambertian lobe. We use a\nparameter mapping neural network to find the parameters of the newly added\nlobes to closely approximate the multiple scattering effect. Despite the\nabsence of layer interfaces, we demonstrate that many common material effects\ncan be achieved with layers of SGGX microflake and other volumes with\nappropriate parameters. A normal mapping effect can also be achieved through\nmapping of microflake orientations, which avoids artifacts common in standard\nnormal maps. Thanks to the analytical formulation, our model is very fast to\nevaluate and sample. Through various parameter settings, our model is able to\nhandle many types of materials, like plastics, wood, cloth, etc., opening a\nnumber of practical applications.",
    "descriptor": "",
    "authors": [
      "Beibei Wang",
      "Wenhua Jin",
      "Milo\u0161 Ha\u0161an",
      "Ling-Qi Yan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.07145"
  },
  {
    "id": "arXiv:2110.07150",
    "title": "Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering  Approach for Open-Domain Question Answering",
    "abstract": "Open-Retrieval Generative Question Answering (GenQA) is proven to deliver\nhigh-quality, natural-sounding answers in English. In this paper, we present\nthe first generalization of the GenQA approach for the multilingual\nenvironment. To this end, we present the GenTyDiQA dataset, which extends the\nTyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed\nanswers in Arabic, Bengali, English, Japanese, and Russian. For all these\nlanguages, we show that a GenQA sequence-to-sequence-based model outperforms a\nstate-of-the-art Answer Sentence Selection model. We also show that a\nmultilingually-trained model competes with, and in some cases outperforms, its\nmonolingual counterparts. Finally, we show that our system can even compete\nwith strong baselines, even when fed with information from a variety of\nlanguages. Essentially, our system is able to answer a question in any language\nof our language set using information from many languages, making it the first\nLanguage-Agnostic GenQA system.",
    "descriptor": "",
    "authors": [
      "Benjamin Muller",
      "Luca Soldaini",
      "Rik Koncel-Kedziorski",
      "Eric Lind",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07150"
  },
  {
    "id": "arXiv:2110.07152",
    "title": "DeepSSM: A Blueprint for Image-to-Shape Deep Learning Models",
    "abstract": "Statistical shape modeling (SSM) characterizes anatomical variations in a\npopulation of shapes generated from medical images. SSM requires consistent\nshape representation across samples in shape cohort. Establishing this\nrepresentation entails a processing pipeline that includes anatomy\nsegmentation, re-sampling, registration, and non-linear optimization. These\nshape representations are then used to extract low-dimensional shape\ndescriptors that facilitate subsequent analyses in different applications.\nHowever, the current process of obtaining these shape descriptors from imaging\ndata relies on human and computational resources, requiring domain expertise\nfor segmenting anatomies of interest. Moreover, this same taxing pipeline needs\nto be repeated to infer shape descriptors for new image data using a\npre-trained/existing shape model. Here, we propose DeepSSM, a deep\nlearning-based framework for learning the functional mapping from images to\nlow-dimensional shape descriptors and their associated shape representations,\nthereby inferring statistical representation of anatomy directly from 3D\nimages. Once trained using an existing shape model, DeepSSM circumvents the\nheavy and manual pre-processing and segmentation and significantly improves the\ncomputational time, making it a viable solution for fully end-to-end SSM\napplications. In addition, we introduce a model-based data-augmentation\nstrategy to address data scarcity. Finally, this paper presents and analyzes\ntwo different architectural variants of DeepSSM with different loss functions\nusing three medical datasets and their downstream clinical application.\nExperiments showcase that DeepSSM performs comparably or better to the\nstate-of-the-art SSM both quantitatively and on application-driven downstream\ntasks. Therefore, DeepSSM aims to provide a comprehensive blueprint for deep\nlearning-based image-to-shape models.",
    "descriptor": "\nComments: pre-print\n",
    "authors": [
      "Riddhish Bhalodia",
      "Shireen Elhabian",
      "Jadie Adams",
      "Wenzheng Tao",
      "Ladislav Kavan",
      "Ross Whitaker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07152"
  },
  {
    "id": "arXiv:2110.07157",
    "title": "Bandwidth Utilization Side-Channel on ML Inference Accelerators",
    "abstract": "Accelerators used for machine learning (ML) inference provide great\nperformance benefits over CPUs. Securing confidential model in inference\nagainst off-chip side-channel attacks is critical in harnessing the performance\nadvantage in practice. Data and memory address encryption has been recently\nproposed to defend against off-chip attacks. In this paper, we demonstrate that\nbandwidth utilization on the interface between accelerators and the weight\nstorage can serve a side-channel for leaking confidential ML model\narchitecture. This side channel is independent of the type of interface, leaks\neven in the presence of data and memory address encryption and can be monitored\nthrough performance counters or through bus contention from an on-chip\nunprivileged process.",
    "descriptor": "",
    "authors": [
      "Sarbartha Banerjee",
      "Shijia Wei",
      "Prakash Ramrakhyani",
      "Mohit Tiwari"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07157"
  },
  {
    "id": "arXiv:2110.07159",
    "title": "Causally Estimating the Sensitivity of Neural NLP Models to Spurious  Features",
    "abstract": "Recent work finds modern natural language processing (NLP) models relying on\nspurious features for prediction. Mitigating such effects is thus important.\nDespite this need, there is no quantitative measure to evaluate or compare the\neffects of different forms of spurious features in NLP. We address this gap in\nthe literature by quantifying model sensitivity to spurious features with a\ncausal estimand, dubbed CENT, which draws on the concept of average treatment\neffect from the causality literature. By conducting simulations with four\nprominent NLP models -- TextRNN, BERT, RoBERTa and XLNet -- we rank the models\nagainst their sensitivity to artificial injections of eight spurious features.\nWe further hypothesize and validate that models that are more sensitive to a\nspurious feature will be less robust against perturbations with this feature\nduring inference. Conversely, data augmentation with this feature improves\nrobustness to similar perturbations. We find statistically significant inverse\ncorrelations between sensitivity and robustness, providing empirical support\nfor our hypothesis.",
    "descriptor": "",
    "authors": [
      "Yunxiang Zhang",
      "Liangming Pan",
      "Samson Tan",
      "Min-Yen Kan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07159"
  },
  {
    "id": "arXiv:2110.07160",
    "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation  with Enhanced Topic Coherence",
    "abstract": "This paper proposes a transformer over transformer framework, called\nTransformer$^2$, to perform neural text segmentation. It consists of two\ncomponents: bottom-level sentence encoders using pre-trained transformers, and\nan upper-level transformer-based segmentation model based on the sentence\nembeddings. The bottom-level component transfers the pre-trained knowledge\nlearned from large external corpora under both single and pair-wise supervised\nNLP tasks to model the sentence embeddings for the documents. Given the\nsentence embeddings, the upper-level transformer is trained to recover the\nsegmentation boundaries as well as the topic labels of each sentence. Equipped\nwith a multi-task loss and the pre-trained knowledge, Transformer$^2$ can\nbetter capture the semantic coherence within the same segments. Our experiments\nshow that (1) Transformer$^2$ manages to surpass state-of-the-art text\nsegmentation models in terms of a commonly-used semantic coherence measure; (2)\nin most cases, both single and pair-wise pre-trained knowledge contribute to\nthe model performance; (3) bottom-level sentence encoders pre-trained on\nspecific languages yield better performance than those pre-trained on specific\ndomains.",
    "descriptor": "",
    "authors": [
      "Kelvin Lo",
      "Yuan Jin",
      "Weicong Tan",
      "Ming Liu",
      "Lan Du",
      "Wray Buntine"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07160"
  },
  {
    "id": "arXiv:2110.07161",
    "title": "Neural Attention-Aware Hierarchical Topic Model",
    "abstract": "Neural topic models (NTMs) apply deep neural networks to topic modelling.\nDespite their success, NTMs generally ignore two important aspects: (1) only\ndocument-level word count information is utilized for the training, while more\nfine-grained sentence-level information is ignored, and (2) external semantic\nknowledge regarding documents, sentences and words are not exploited for the\ntraining. To address these issues, we propose a variational autoencoder (VAE)\nNTM model that jointly reconstructs the sentence and document word counts using\ncombinations of bag-of-words (BoW) topical embeddings and pre-trained semantic\nembeddings. The pre-trained embeddings are first transformed into a common\nlatent topical space to align their semantics with the BoW embeddings. Our\nmodel also features hierarchical KL divergence to leverage embeddings of each\ndocument to regularize those of their sentences, thereby paying more attention\nto semantically relevant sentences. Both quantitative and qualitative\nexperiments have shown the efficacy of our model in 1) lowering the\nreconstruction errors at both the sentence and document levels, and 2)\ndiscovering more coherent topics from real-world datasets.",
    "descriptor": "",
    "authors": [
      "Yuan Jin",
      "He Zhao",
      "Ming Liu",
      "Lan Du",
      "Wray Buntine"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07161"
  },
  {
    "id": "arXiv:2110.07163",
    "title": "A Data Analysis Study on Human Liver Blood Circulation",
    "abstract": "The liver has a unique blood supply system and plays an important role in the\nhuman blood circulatory system. Thus, hemodynamic problems related to the liver\nserve as an important part in clinical diagnosis and treatment. Although\nestimating parameters in these hemodynamic models is essential to the study of\nliver models, due to the limitations of medical measurement methods and\nconstraints of ethics on clinical studies, it is impossible to directly measure\nthe parameters of blood vessels in livers. Furthermore, as an important part of\nthe systemic blood circulation, livers' studies are supposed to be in\nconjunction with other blood vessels. In this article, we present an innovative\nmethod to fix parameters of an individual liver in a human blood circulation\nusing non-invasive clinical measurements. The method consists of a 1-D blood\nflow model of human arteries and veins, a 0-D model reflecting the peripheral\nresistance of capillaries and a lumped parameter circuit model for human\nlivers. We apply the finite element method in fluid mechanics of these models\nto a numerical study, based on non-invasive blood related measures of 33\nindividuals. The estimated results of human blood vessel characteristic and\nliver model parameters are verified from the perspective of Stroke Value\nVariation, which shows the effectiveness of our estimation method.",
    "descriptor": "",
    "authors": [
      "Ting Wu",
      "Keqin Liu",
      "Emily Fainman",
      "Ying Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07163"
  },
  {
    "id": "arXiv:2110.07165",
    "title": "Semantically Distributed Robust Optimization for Vision-and-Language  Inference",
    "abstract": "Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&L tasks.",
    "descriptor": "\nComments: preprint; code available at this https URL\n",
    "authors": [
      "Tejas Gokhale",
      "Abhishek Chaudhary",
      "Pratyay Banerjee",
      "Chitta Baral",
      "Yezhou Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07165"
  },
  {
    "id": "arXiv:2110.07166",
    "title": "MoFE: Mixture of Factual Experts for Controlling Hallucinations in  Abstractive Summarization",
    "abstract": "Neural abstractive summarization models are susceptible to generating\nfactually inconsistent content, a phenomenon known as hallucination. This\nlimits the usability and adoption of these systems in real-world applications.\nTo reduce the presence of hallucination, we propose the Mixture of Factual\nExperts (MoFE) model, which combines multiple summarization experts that each\ntarget a specific type of error. We train our experts using reinforcement\nlearning (RL) to minimize the error defined by two factual consistency metrics:\nentity overlap and dependency arc entailment. We construct MoFE by combining\nthe experts using two ensembling strategies (weights and logits) and evaluate\nthem on two summarization datasets (XSUM and CNN/DM). Our experiments on BART\nmodels show that the MoFE improves performance according to both entity overlap\nand dependency arc entailment, without a significant performance drop on\nstandard ROUGE metrics. The performance improvement also transfers to unseen\nfactual consistency metrics, such as question answer-based factuality\nevaluation metric and BERTScore precision with respect to the source document.",
    "descriptor": "",
    "authors": [
      "Prafulla Kumar Choubey",
      "Jesse Vig",
      "Wenhao Liu",
      "Nazneen Fatema Rajani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07166"
  },
  {
    "id": "arXiv:2110.07171",
    "title": "SGoLAM: Simultaneous Goal Localization and Mapping for Multi-Object Goal  Navigation",
    "abstract": "We present SGoLAM, short for simultaneous goal localization and mapping,\nwhich is a simple and efficient algorithm for Multi-Object Goal navigation.\nGiven an agent equipped with an RGB-D camera and a GPS/Compass sensor, our\nobjective is to have the agent navigate to a sequence of target objects in\nrealistic 3D environments. Our pipeline fully leverages the strength of\nclassical approaches for visual navigation, by decomposing the problem into two\nkey components: mapping and goal localization. The mapping module converts the\ndepth observations into an occupancy map, and the goal localization module\nmarks the locations of goal objects. The agent's policy is determined using the\ninformation provided by the two modules: if a current goal is found, plan\ntowards the goal and otherwise, perform exploration. As our approach does not\nrequire any training of neural networks, it could be used in an off-the-shelf\nmanner, and amenable for fast generalization in new, unseen environments.\nNonetheless, our approach performs on par with the state-of-the-art\nlearning-based approaches. SGoLAM is ranked 2nd in the CVPR 2021 MultiON\n(Multi-Object Goal Navigation) challenge. We have made our code publicly\navailable at \\emph{https://github.com/eunsunlee/SGoLAM}.",
    "descriptor": "",
    "authors": [
      "Junho Kim",
      "Eun Sun Lee",
      "Mingi Lee",
      "Donsu Zhang",
      "Young Min Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07171"
  },
  {
    "id": "arXiv:2110.07172",
    "title": "Additive Schwarz Methods for Convex Optimization with Backtracking",
    "abstract": "This paper presents a novel backtracking strategy for additive Schwarz\nmethods for general convex optimization problems as an acceleration scheme. The\nproposed backtracking strategy is independent of local solvers, so that it can\nbe applied to any algorithms that can be represented in an abstract framework\nof additive Schwarz methods. Allowing for adaptive increasing and decreasing of\nthe step size along the iterations, the convergence rate of an algorithm is\ngreatly improved. Improved convergence rate of the algorithm is proven\nrigorously. In addition, combining the proposed backtracking strategy with a\nmomentum acceleration technique, we propose a further accelerated additive\nSchwarz method. Numerical results for various convex optimization problems that\nsupport our theory are presented.",
    "descriptor": "\nComments: 20 pages, 3 figures\n",
    "authors": [
      "Jongho Park"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07172"
  },
  {
    "id": "arXiv:2110.07173",
    "title": "Adaptive Pad\u00e9-Chebyshev Type Approximation of Piecewise Smooth  Functions",
    "abstract": "A piecewise Pad\\'e-Chebyshev type (PiPCT) approximation method is proposed to\nminimize the Gibbs phenomenon in approximating piecewise smooth functions. A\ntheorem on $L^1$-error estimate is proved for sufficiently smooth functions\nusing a decay property of the Chebyshev coefficients. Numerical experiments are\nperformed to show that the PiPCT method accurately captures isolated\nsingularities of a function without using the positions and the types of\nsingularities. Further, an adaptive partition approach to the PiPCT method is\ndeveloped (referred to as the APiPCT method) to achieve the required accuracy\nwith a lesser computational cost. Numerical experiments are performed to show\nsome advantages of using the PiPCT and APiPCT methods compared to some\nwell-known methods in the literature.",
    "descriptor": "\nComments: This is the second version of our previously submitted article on the same title. arXiv admin note: substantial text overlap with arXiv:1910.10385\n",
    "authors": [
      "S. Akansha",
      "S. Baskar"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07173"
  },
  {
    "id": "arXiv:2110.07174",
    "title": "Context-gloss Augmentation for Improving Word Sense Disambiguation",
    "abstract": "The goal of Word Sense Disambiguation (WSD) is to identify the sense of a\npolysemous word in a specific context. Deep-learning techniques using BERT have\nachieved very promising results in the field and different methods have been\nproposed to integrate structured knowledge to enhance performance. At the same\ntime, an increasing number of data augmentation techniques have been proven to\nbe useful for NLP tasks. Building upon previous works leveraging BERT and\nWordNet knowledge, we explore different data augmentation techniques on\ncontext-gloss pairs to improve the performance of WSD. In our experiment, we\nshow that both sentence-level and word-level augmentation methods are effective\nstrategies for WSD. Also, we find out that performance can be improved by\nadding hypernyms' glosses obtained from a lexical knowledge base. We compare\nand analyze different context-gloss augmentation techniques, and the results\nshow that applying back translation on gloss performs the best.",
    "descriptor": "\nComments: 4 pages, 1 figure\n",
    "authors": [
      "Guan-Ting Lin",
      "Manuel Giambi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07174"
  },
  {
    "id": "arXiv:2110.07178",
    "title": "Symbolic Knowledge Distillation: from General Language Models to  Commonsense Models",
    "abstract": "The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.",
    "descriptor": "",
    "authors": [
      "Peter West",
      "Chandra Bhagavatula",
      "Jack Hessel",
      "Jena D. Hwang",
      "Liwei Jiang",
      "Ronan Le Bras",
      "Ximing Lu",
      "Sean Welleck",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07178"
  },
  {
    "id": "arXiv:2110.07179",
    "title": "Singular Zone in Quadrotor Yaw-Position Feedback Linearization",
    "abstract": "It is well known that the conventional quadrotor is an under-actuated MIMO\nsystem. The number of the inputs is less than the degree of freedom. One\napproach in controlling this non-holonomic system is feedback linearization. In\nthe frequently cited (305 times) application, the yaw and the position are\nselected as the controlled variables. It is reported that no singularity is\nfound in their delta matrix, making it possible to apply the inversion within a\nwide range. However, we find the ignored singular zone within the range of\ninterest. The unreported singular area can cause the failure in the controller\ndesign. This paper visualizes this uninvertible area and details the deduction\nprocess.",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Zhe Shen",
      "Takeshi Tsuchiya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07179"
  },
  {
    "id": "arXiv:2110.07181",
    "title": "Relation-aware Heterogeneous Graph for User Profiling",
    "abstract": "User profiling has long been an important problem that investigates user\ninterests in many real applications. Some recent works regard users and their\ninteracted objects as entities of a graph and turn the problem into a node\nclassification task. However, they neglect the difference of distinct\ninteraction types, e.g. user clicks an item v.s.user purchases an item, and\nthus cannot incorporate such information well. To solve these issues, we\npropose to leverage the relation-aware heterogeneous graph method for user\nprofiling, which also allows capturing significant meta relations. We adopt the\nquery, key, and value mechanism in a transformer fashion for heterogeneous\nmessage passing so that entities can effectively interact with each other. Via\nsuch interactions on different relation types, our model can generate\nrepresentations with rich information for the user profile prediction. We\nconduct experiments on two real-world e-commerce datasets and observe a\nsignificant performance boost of our approach.",
    "descriptor": "\nComments: CIKM2021 Accepted\n",
    "authors": [
      "Qilong Yan",
      "Yufeng Zhang",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07181"
  },
  {
    "id": "arXiv:2110.07182",
    "title": "Adversarial examples by perturbing high-level features in intermediate  decoder layers",
    "abstract": "We propose a novel method for creating adversarial examples. Instead of\nperturbing pixels, we use an encoder-decoder representation of the input image\nand perturb intermediate layers in the decoder. This changes the high-level\nfeatures provided by the generative model. Therefore, our perturbation\npossesses semantic meaning, such as a longer beak or green tints. We formulate\nthis task as an optimization problem by minimizing the Wasserstein distance\nbetween the adversarial and initial images under a misclassification\nconstraint. We employ the projected gradient method with a simple inexact\nprojection. Due to the projection, all iterations are feasible, and our method\nalways generates adversarial images. We perform numerical experiments on the\nMNIST and ImageNet datasets in both targeted and untargeted settings. We\ndemonstrate that our adversarial images are much less vulnerable to\nsteganographic defence techniques than pixel-based attacks. Moreover, we show\nthat our method modifies key features such as edges and that defence techniques\nbased on adversarial training are vulnerable to our attacks.",
    "descriptor": "",
    "authors": [
      "Vojt\u011bch \u010cerm\u00e1k",
      "Luk\u00e1\u0161 Adam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07182"
  },
  {
    "id": "arXiv:2110.07184",
    "title": "Self-Supervised Domain Adaptation for Visual Navigation with Global Map  Consistency",
    "abstract": "We propose a light-weight, self-supervised adaptation for a visual navigation\nagent to generalize to unseen environment. Given an embodied agent trained in a\nnoiseless environment, our objective is to transfer the agent to a noisy\nenvironment where actuation and odometry sensor noise is present. Our method\nencourages the agent to maximize the consistency between the global maps\ngenerated at different time steps in a round-trip trajectory. The proposed task\nis completely self-supervised, not requiring any supervision from ground-truth\npose data or explicit noise model. In addition, optimization of the task\nobjective is extremely light-weight, as training terminates within a few\nminutes on a commodity GPU. Our experiments show that the proposed task helps\nthe agent to successfully transfer to new, noisy environments. The transferred\nagent exhibits improved localization and mapping accuracy, further leading to\nenhanced performance in downstream visual navigation tasks. Moreover, we\ndemonstrate test-time adaptation with our self-supervised task to show its\npotential applicability in real-world deployment.",
    "descriptor": "\nComments: Accepted to WACV 2022\n",
    "authors": [
      "Eun Sun Lee",
      "Junho Kim",
      "Young Min Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07184"
  },
  {
    "id": "arXiv:2110.07186",
    "title": "An FPGA-Based Fully Pipelined Bilateral Grid for Real-Time Image  Denoising",
    "abstract": "The bilateral filter (BF) is widely used in image processing because it can\nperform denoising while preserving edges. It has disadvantages in that it is\nnonlinear, and its computational complexity and hardware resources are directly\nproportional to its window size. Thus far, several approximation methods and\nhardware implementations have been proposed to solve these problems. However,\nprocessing large-scale and high-resolution images in real time under severe\nhardware resource constraints remains a challenge. This paper proposes a\nreal-time image denoising system that uses an FPGA based on the bilateral grid\n(BG). In the BG, a 2D image consisting of x- and y-axes is projected onto a 3D\nspace called a \"grid,\" which consists of axes that correlate to the\nx-component, y-component, and intensity value of the input image. This grid is\nthen blurred using the Gaussian filter, and the output image is generated by\ninterpolating the grid. Although it is possible to change the window size in\nthe BF, it is impossible to change it on the input image in the BG. This makes\nit difficult to associate the BG with the BF and to obtain the property of\nsuppressing the increase in hardware resources when the window radius is\nenlarged. This study demonstrates that a BG with a variable-sized window can be\nrealized by introducing the window radius parameter wherein the window radius\non the grid is always 1. We then implement this BG on an FPGA in a fully\npipelined manner. Further, we verify that our design suppresses the increase in\nhardware resources even when the window size is enlarged and outperforms the\nexisting designs in terms of computation speed and hardware resources.",
    "descriptor": "\nComments: 7 pages, 12 figures, 2 tables, FPL 2021 (Full paper)\n",
    "authors": [
      "Nobuho Hashimoto",
      "Shinya Takamaeda-Yamazaki"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2110.07186"
  },
  {
    "id": "arXiv:2110.07187",
    "title": "Revisiting IPA-based Cross-lingual Text-to-speech",
    "abstract": "International Phonetic Alphabet (IPA) has been widely used in cross-lingual\ntext-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\nIPA itself has been understudied in cross-lingual TTS. In this paper, we report\nsome empirical findings of building a cross-lingual TTS model using IPA as\ninputs. Experiments show that the way to process the IPA and suprasegmental\nsequence has a negligible impact on the CL VC performance. Furthermore, we find\nthat using a dataset including one speaker per language to build an IPA-based\nTTS system would fail CL VC since the language-unique IPA and tone/stress\nsymbols could leak the speaker information. In addition, we experiment with\ndifferent combinations of speakers in the training dataset to further\ninvestigate the effect of the number of speakers on the CL VC performance.",
    "descriptor": "\nComments: Submitting to ICASSP2022\n",
    "authors": [
      "Haitong Zhang",
      "Yue Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07187"
  },
  {
    "id": "arXiv:2110.07188",
    "title": "Does the Layout Really Matter? A Study on Visual Model Accuracy  Estimation",
    "abstract": "In visual interactive labeling, users iteratively assign labels to data items\nuntil the machine model reaches an acceptable accuracy. A crucial step of this\nprocess is to inspect the model's accuracy and decide whether it is necessary\nto label additional elements. In scenarios with no or very little labeled data,\nvisual inspection of the predictions is required. Similarity-preserving\nscatterplots created through a dimensionality reduction algorithm are a common\nvisualization that is used in these cases. Previous studies investigated the\neffects of layout and image complexity on tasks like labeling. However, model\nevaluation has not been studied systematically. We present the results of an\nexperiment studying the influence of image complexity and visual grouping of\nimages on model accuracy estimation. We found that users outperform traditional\nautomated approaches when estimating a model's accuracy. Furthermore, while the\ncomplexity of images impacts the overall performance, the layout of the items\nin the plot has little to no effect on estimations.",
    "descriptor": "",
    "authors": [
      "Nicolas Grossmann",
      "J\u00fcrgen Bernard",
      "Michael Sedlmair",
      "Manuela Waldner"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07188"
  },
  {
    "id": "arXiv:2110.07190",
    "title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs",
    "abstract": "Graph neural networks (GNNs) and label propagation represent two interrelated\nmodeling strategies designed to exploit graph structure in tasks such as node\nproperty prediction. The former is typically based on stacked message-passing\nlayers that share neighborhood information to transform node features into\npredictive embeddings. In contrast, the latter involves spreading label\ninformation to unlabeled nodes via a parameter-free diffusion process, but\noperates independently of the node features. Given then that the material\ndifference is merely whether features or labels are smoothed across the graph,\nit is natural to consider combinations of the two for improving performance. In\nthis regard, it has recently been proposed to use a randomly-selected portion\nof the training labels as GNN inputs, concatenated with the original node\nfeatures for making predictions on the remaining labels. This so-called label\ntrick accommodates the parallel use of features and labels, and is foundational\nto many of the top-ranking submissions on the Open Graph Benchmark (OGB)\nleaderboard. And yet despite its wide-spread adoption, thus far there has been\nlittle attempt to carefully unpack exactly what statistical properties the\nlabel trick introduces into the training pipeline, intended or otherwise. To\nthis end, we prove that under certain simplifying assumptions, the stochastic\nlabel trick can be reduced to an interpretable, deterministic training\nobjective composed of two factors. The first is a data-fitting term that\nnaturally resolves potential label leakage issues, while the second serves as a\nregularization factor conditioned on graph structure that adapts to graph size\nand connectivity. Later, we leverage this perspective to motivate a broader\nrange of label trick use cases, and provide experiments to verify the efficacy\nof these extensions.",
    "descriptor": "",
    "authors": [
      "Yangkun Wang",
      "Jiarui Jin",
      "Weinan Zhang",
      "Yongyi Yang",
      "Jiuhai Chen",
      "Quan Gan",
      "Yong Yu",
      "Zheng Zhang",
      "Zengfeng Huang",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07190"
  },
  {
    "id": "arXiv:2110.07197",
    "title": "Semi-supervised Multi-task Learning for Semantics and Depth",
    "abstract": "Multi-Task Learning (MTL) aims to enhance the model generalization by sharing\nrepresentations between related tasks for better performance. Typical MTL\nmethods are jointly trained with the complete multitude of ground-truths for\nall tasks simultaneously. However, one single dataset may not contain the\nannotations for each task of interest. To address this issue, we propose the\nSemi-supervised Multi-Task Learning (SemiMTL) method to leverage the available\nsupervisory signals from different datasets, particularly for semantic\nsegmentation and depth estimation tasks. To this end, we design an adversarial\nlearning scheme in our semi-supervised training by leveraging unlabeled data to\noptimize all the task branches simultaneously and accomplish all tasks across\ndatasets with partial annotations. We further present a domain-aware\ndiscriminator structure with various alignment formulations to mitigate the\ndomain discrepancy issue among datasets. Finally, we demonstrate the\neffectiveness of the proposed method to learn across different datasets on\nchallenging street view and remote sensing benchmarks.",
    "descriptor": "\nComments: Accepted at WACV 2022\n",
    "authors": [
      "Yufeng Wang",
      "Yi-Hsuan Tsai",
      "Wei-Chih Hung",
      "Wenrui Ding",
      "Shuo Liu",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07197"
  },
  {
    "id": "arXiv:2110.07198",
    "title": "Rethinking Self-Supervision Objectives for Generalizable Coherence  Modeling",
    "abstract": "Although large-scale pre-trained neural models have shown impressive\nperformances in a variety of tasks, their ability to generate coherent text\nthat appropriately models discourse phenomena is harder to evaluate and less\nunderstood. Given the claims of improved text generation quality across various\nsystems, we consider the coherence evaluation of machine generated text to be\none of the principal applications of coherence models that needs to be\ninvestigated. We explore training data and self-supervision objectives that\nresult in a model that generalizes well across tasks and can be used\noff-the-shelf to perform such evaluations. Prior work in neural coherence\nmodeling has primarily focused on devising new architectures, and trained the\nmodel to distinguish coherent and incoherent text through pairwise\nself-supervision on the permuted documents task. We instead use a basic model\narchitecture and show significant improvements over state of the art within the\nsame training regime. We then design a harder self-supervision objective by\nincreasing the ratio of negative samples within a contrastive learning setup,\nand enhance the model further through automatic hard negative mining coupled\nwith a large global negative queue encoded by a momentum encoder. We show\nempirically that increasing the density of negative samples improves the basic\nmodel, and using a global negative queue further improves and stabilizes the\nmodel while training with hard negative samples. We evaluate the coherence\nmodel on task-independent test sets that resemble real-world use cases and show\nsignificant improvements in coherence evaluations of downstream applications.",
    "descriptor": "",
    "authors": [
      "Prathyusha Jwalapuram",
      "Shafiq Joty",
      "Xiang Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07198"
  },
  {
    "id": "arXiv:2110.07200",
    "title": "Inverse analysis of material parameters in coupled multi-physics biofilm  models",
    "abstract": "In this article we propose an inverse analysis algorithm to find the best fit\nof multiple material parameters in different coupled multi-physics biofilm\nmodels. We use a nonlinear continuum mechanical approach to model biofilm\ndeformation that occurs in flow cell experiments. The objective function is\nbased on a simple geometrical measurement of the distance of the fluid biofilm\ninterface between model and experiments. A Levenberg-Marquardt algorithm based\non finite difference approximation is used as an optimizer. The proposed method\nuses a moderate to low amount of model evaluations. For a first presentation\nand evaluation the algorithm is applied and tested on different numerical\nexamples based on generated numerical results and the addition of Gaussian\nnoise. Achieved numerical results show that the proposed method serves well for\ndifferent physical effects investigated and numerical approaches chosen for the\nmodel. Presented examples show the inverse analysis for multiple parameters in\nbiofilm models including fluid-solid interaction effects, poroelasticity,\nheterogeneous material properties and growth.",
    "descriptor": "",
    "authors": [
      "Harald Willmann",
      "Wolfgang A. Wall"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2110.07200"
  },
  {
    "id": "arXiv:2110.07201",
    "title": "Coarse to Fine: Video Retrieval before Moment Localization",
    "abstract": "The current state-of-the-art methods for video corpus moment retrieval (VCMR)\noften use similarity-based feature alignment approach for the sake of\nconvenience and speed. However, late fusion methods like cosine similarity\nalignment are unable to make full use of the information from both query texts\nand videos. In this paper, we combine feature alignment with feature fusion to\npromote the performance on VCMR.",
    "descriptor": "",
    "authors": [
      "Zijian Gao",
      "Huanyu Liu",
      "Jingyu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07201"
  },
  {
    "id": "arXiv:2110.07202",
    "title": "Unrolled Variational Bayesian Algorithm for Image Blind Deconvolution",
    "abstract": "In this paper, we introduce a variational Bayesian algorithm (VBA) for image\nblind deconvolution. Our generic framework incorporates smoothness priors on\nthe unknown blur/image and possible affine constraints (e.g., sum to one) on\nthe blur kernel. One of our main contributions is the integration of VBA within\na neural network paradigm, following an unrolling methodology. The proposed\narchitecture is trained in a supervised fashion, which allows us to optimally\nset two key hyperparameters of the VBA model and lead to further improvements\nin terms of resulting visual quality. Various experiments involving\ngrayscale/color images and diverse kernel shapes, are performed. The numerical\nexamples illustrate the high performance of our approach when compared to\nstate-of-the-art techniques based on optimization, Bayesian estimation, or deep\nlearning.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Yunshi Huang",
      "Emilie Chouzenoux",
      "Jean-Christophe Pesquet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07202"
  },
  {
    "id": "arXiv:2110.07206",
    "title": "Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad  Weather",
    "abstract": "Visual perception in autonomous driving is a crucial part of a vehicle to\nnavigate safely and sustainably in different traffic conditions. However, in\nbad weather such as heavy rain and haze, the performance of visual perception\nis greatly affected by several degrading effects. Recently, deep learning-based\nperception methods have addressed multiple degrading effects to reflect\nreal-world bad weather cases but have shown limited success due to 1) high\ncomputational costs for deployment on mobile devices and 2) poor relevance\nbetween image enhancement and visual perception in terms of the model ability.\nTo solve these issues, we propose a task-driven image enhancement network\nconnected to the high-level vision task, which takes in an image corrupted by\nbad weather as input. Specifically, we introduce a novel low memory network to\nreduce most of the layer connections of dense blocks for less memory and\ncomputational cost while maintaining high performance. We also introduce a new\ntask-driven training strategy to robustly guide the high-level task model\nsuitable for both high-quality restoration of images and highly accurate\nperception. Experiment results demonstrate that the proposed method improves\nthe performance among lane and 2D object detection, and depth estimation\nlargely under adverse weather in terms of both low memory and accuracy.",
    "descriptor": "\nComments: Accepted in ICRA21\n",
    "authors": [
      "Younkwan Lee",
      "Jihyo Jeon",
      "Yeongmin Ko",
      "Byunggwan Jeon",
      "Moongu Jeon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07206"
  },
  {
    "id": "arXiv:2110.07209",
    "title": "A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss  Pairs for Interpretation",
    "abstract": "Pun location is to identify the punning word (usually a word or a phrase that\nmakes the text ambiguous) in a given short text, and pun interpretation is to\nfind out two different meanings of the punning word. Most previous studies\nadopt limited word senses obtained by WSD(Word Sense Disambiguation) technique\nor pronunciation information in isolation to address pun location. For the task\nof pun interpretation, related work pays attention to various WSD algorithms.\nIn this paper, a model called DANN (Dual-Attentive Neural Network) is proposed\nfor pun location, effectively integrates word senses and pronunciation with\ncontext information to address two kinds of pun at the same time. Furthermore,\nwe treat pun interpretation as a classification task and construct pungloss\npairs as processing data to solve this task. Experiments on the two benchmark\ndatasets show that our proposed methods achieve new state-of-the-art results.\nOur source code is available in the public code repository.",
    "descriptor": "",
    "authors": [
      "Shen Liu",
      "Meirong Ma",
      "Hao Yuan",
      "Jianchao Zhu",
      "Yuanbin Wu",
      "Man Lan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07209"
  },
  {
    "id": "arXiv:2110.07210",
    "title": "Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data",
    "abstract": "Recently, sequence-to-sequence (seq-to-seq) models have been successfully\napplied in text-to-speech (TTS) to synthesize speech for single-language text.\nTo synthesize speech for multiple languages usually requires multi-lingual\nspeech from the target speaker. However, it is both laborious and expensive to\ncollect high-quality multi-lingual TTS data for the target speakers. In this\npaper, we proposed to use low-quality code-switched found data from the\nnon-target speakers to achieve cross-lingual voice cloning for the target\nspeakers. Experiments show that our proposed method can generate high-quality\ncode-switched speech in the target voices in terms of both naturalness and\nspeaker consistency. More importantly, we find that our method can achieve a\ncomparable result to the state-of-the-art (SOTA) performance in cross-lingual\nvoice cloning.",
    "descriptor": "",
    "authors": [
      "Haitong Zhang",
      "Yue Lin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07210"
  },
  {
    "id": "arXiv:2110.07225",
    "title": "Web Search via an Efficient and Effective Brain-Machine Interface",
    "abstract": "While search technologies have evolved to be robust and ubiquitous, the\nfundamental interaction paradigm has remained relatively stable for decades.\nWith the maturity of the Brain-Machine Interface, we build an efficient and\neffective communication system between human beings and search engines based on\nelectroencephalogram~(EEG) signals, called Brain-Machine Search Interface(BMSI)\nsystem. The BMSI system provides functions including query reformulation and\nsearch result interaction. In our system, users can perform search tasks\nwithout having to use the mouse and keyboard. Therefore, it is useful for\napplication scenarios in which hand-based interactions are infeasible, e.g, for\nusers with severe neuromuscular disorders. Besides, based on brain signals\ndecoding, our system can provide abundant and valuable user-side context\ninformation(e.g., real-time satisfaction feedback, extensive context\ninformation, and a clearer description of information needs) to the search\nengine, which is hard to capture in the previous paradigm. In our\nimplementation, the system can decode user satisfaction from brain signals in\nreal-time during the interaction process and re-rank the search results list\nbased on user satisfaction feedback. The demo video is available at\nthis http URL",
    "descriptor": "",
    "authors": [
      "Xuesong Chen",
      "Ziyi Ye",
      "Xiaohui Xie",
      "Yiqun Liu",
      "Weihang Su",
      "Shuqi Zhu",
      "Min Zhang",
      "Shaoping Ma"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.07225"
  },
  {
    "id": "arXiv:2110.07232",
    "title": "Procrastinated Tree Search: Black-box Optimization with Delayed, Noisy,  and Multi-fidelity Feedback",
    "abstract": "In black-box optimization problems, we aim to maximize an unknown objective\nfunction, where the function is only accessible through feedbacks of an\nevaluation or simulation oracle. In real-life, the feedbacks of such oracles\nare often noisy and available after some unknown delay that may depend on the\ncomputation time of the oracle. Additionally, if the exact evaluations are\nexpensive but coarse approximations are available at a lower cost, the\nfeedbacks can have multi-fidelity. In order to address this problem, we propose\na generic extension of hierarchical optimistic tree search (HOO), called\nProCrastinated Tree Search (PCTS), that flexibly accommodates a delay and\nnoise-tolerant bandit algorithm. We provide a generic proof technique to\nquantify regret of PCTS under delayed, noisy, and multi-fidelity feedbacks.\nSpecifically, we derive regret bounds of PCTS enabled with delayed-UCB1 (DUCB1)\nand delayed-UCB-V (DUCBV) algorithms. Given a horizon $T$, PCTS retains the\nregret bound of non-delayed HOO for expected delay of $O(\\log T)$ and worsens\nby $O(T^{\\frac{1-\\alpha}{d+2}})$ for expected delays of $O(T^{1-\\alpha})$ for\n$\\alpha \\in (0,1]$. We experimentally validate on multiple synthetic functions\nand hyperparameter tuning problems that PCTS outperforms the state-of-the-art\nblack-box optimization methods for feedbacks with different noise levels,\ndelays, and fidelity.",
    "descriptor": "",
    "authors": [
      "Junxiong Wang",
      "Debabrota Basu",
      "Immanuel Trummer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07232"
  },
  {
    "id": "arXiv:2110.07233",
    "title": "Optimal Update in Energy Harvesting Aided Terahertz Communications with  Random Blocking",
    "abstract": "In this paper, we consider an information update system where wireless sensor\nsends timely updates to the destination over a random blocking terahertz\nchannel with the supply of harvested energy and reliable energy backup. The\npaper aims to find the optimal information updating policy that minimize the\ntime-average weighted sum of the Age of information(AoI) and the reliable\nenergy costs by formulating an infinite state Markov decision process(MDP).\nWith the derivation of the monotonicity of value function on each component,\nthe optimal information updating policy is proved to have a threshold\nstructure. Based on this special structure, an algorithm for efficiently\ncomputing the optimal policy is proposed. Numerical results show that the\noptimal updating policy proposed outperforms baseline policies.",
    "descriptor": "\nComments: 9 pages, 4 Postscript figures\n",
    "authors": [
      "Lixin Wang",
      "Fuzhou Peng",
      "Xiang Chen",
      "Shidong Zhou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07233"
  },
  {
    "id": "arXiv:2110.07235",
    "title": "HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive  Media",
    "abstract": "We introduce HUMAN4D, a large and multimodal 4D dataset that contains a\nvariety of human activities simultaneously captured by a professional\nmarker-based MoCap, a volumetric capture and an audio recording system. By\ncapturing 2 female and $2$ male professional actors performing various\nfull-body movements and expressions, HUMAN4D provides a diverse set of motions\nand poses encountered as part of single- and multi-person daily, physical and\nsocial activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),\nvolumetric and audio data. Despite the existence of multi-view color datasets\ncaptured with the use of hardware (HW) synchronization, to the best of our\nknowledge, HUMAN4D is the first and only public resource that provides\nvolumetric depth maps with high synchronization precision due to the use of\nintra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned\nand rigged 3D character complements HUMAN4D to enable joint research on\ntime-varying and high-quality dynamic meshes. We provide evaluation baselines\nby benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D\ncompression methods. For the former, we apply 2D and 3D pose estimation\nalgorithms both on single- and multi-view data cues. For the latter, we\nbenchmark open-source 3D codecs on volumetric data respecting online volumetric\nvideo encoding and steady bit-rates. Furthermore, qualitative and quantitative\nvisual comparison between mesh-based volumetric data reconstructed in different\nqualities showcases the available options with respect to 4D representations.\nHUMAN4D is introduced to the computer vision and graphics research communities\nto enable joint research on spatio-temporally aligned pose, volumetric, mRGBD\nand audio data cues. The dataset and its code are available\nhttps://tofis.github.io/myurls/human4d.",
    "descriptor": "",
    "authors": [
      "nargyros Chatzitofis",
      "Leonidas Saroglou",
      "Prodromos Boutis",
      "Petros Drakoulis",
      "Nikolaos Zioulis",
      "Shishir Subramanyam",
      "Bart Kevelham",
      "Caecilia Charbonnier",
      "Pablo Cesar",
      "Dimitrios Zarpalas",
      "Stefanos Kollias",
      "Petros Daras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07235"
  },
  {
    "id": "arXiv:2110.07238",
    "title": "How to train RNNs on chaotic data?",
    "abstract": "Recurrent neural networks (RNNs) are wide-spread machine learning tools for\nmodeling sequential and time series data. They are notoriously hard to train\nbecause their loss gradients backpropagated in time tend to saturate or diverge\nduring training. This is known as the exploding and vanishing gradient problem.\nPrevious solutions to this issue either built on rather complicated,\npurpose-engineered architectures with gated memory buffers, or - more recently\n- imposed constraints that ensure convergence to a fixed point or restrict (the\neigenspectrum of) the recurrence matrix. Such constraints, however, convey\nsevere limitations on the expressivity of the RNN. Essential intrinsic dynamics\nsuch as multistability or chaos are disabled. This is inherently at disaccord\nwith the chaotic nature of many, if not most, time series encountered in nature\nand society. Here we offer a comprehensive theoretical treatment of this\nproblem by relating the loss gradients during RNN training to the Lyapunov\nspectrum of RNN-generated orbits. We mathematically prove that RNNs producing\nstable equilibrium or cyclic behavior have bounded gradients, whereas the\ngradients of RNNs with chaotic dynamics always diverge. Based on these analyses\nand insights, we offer an effective yet simple training technique for chaotic\ndata and guidance on how to choose relevant hyperparameters according to the\nLyapunov spectrum.",
    "descriptor": "",
    "authors": [
      "Zahra Monfared",
      "Jonas M. Mikhaeil",
      "Daniel Durstewitz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07238"
  },
  {
    "id": "arXiv:2110.07240",
    "title": "Causal Transformers Perform Below Chance on Recursive Nested  Constructions, Unlike Humans",
    "abstract": "Recursive processing is considered a hallmark of human linguistic abilities.\nA recent study evaluated recursive processing in recurrent neural language\nmodels (RNN-LMs) and showed that such models perform below chance level on\nembedded dependencies within nested constructions -- a prototypical example of\nrecursion in natural language. Here, we study if state-of-the-art Transformer\nLMs do any better. We test four different Transformer LMs on two different\ntypes of nested constructions, which differ in whether the embedded (inner)\ndependency is short or long range. We find that Transformers achieve\nnear-perfect performance on short-range embedded dependencies, significantly\nbetter than previous results reported for RNN-LMs and humans. However, on\nlong-range embedded dependencies, Transformers' performance sharply drops below\nchance level. Remarkably, the addition of only three words to the embedded\ndependency caused Transformers to fall from near-perfect to below-chance\nperformance. Taken together, our results reveal Transformers' shortcoming when\nit comes to recursive, structure-based, processing.",
    "descriptor": "\nComments: None\n",
    "authors": [
      "Yair Lakretz",
      "Th\u00e9o Desbordes",
      "Dieuwke Hupkes",
      "Stanislas Dehaene"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07240"
  },
  {
    "id": "arXiv:2110.07244",
    "title": "Building Chinese Biomedical Language Models via Multi-Level Text  Discrimination",
    "abstract": "Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a biomedical PLM in Chinese built with a new pre-training framework.\nThis new framework trains eHealth as a discriminator through both token-level\nand sequence-level discrimination. The former is to detect input tokens\ncorrupted by a generator and select their original signals from plausible\ncandidates, while the latter is to further distinguish corruptions of a same\noriginal sequence from those of the others. As such, eHealth can learn language\nsemantics at both the token and sequence levels. Extensive experiments on 11\nChinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. The pre-trained model is\navailable to the public at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and the\ncode will also be released later.",
    "descriptor": "",
    "authors": [
      "Quan Wang",
      "Songtai Dai",
      "Benfeng Xu",
      "Yajuan Lyu",
      "Yong Zhu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07244"
  },
  {
    "id": "arXiv:2110.07246",
    "title": "HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with  Dual Coordination Mechanism",
    "abstract": "Multi-agent reinforcement learning often suffers from the exponentially\nlarger action space caused by a large number of agents. In this paper, we\npropose a novel value decomposition framework HAVEN based on hierarchical\nreinforcement learning for the fully cooperative multi-agent problems. In order\nto address instabilities that arise from the concurrent optimization of\nhigh-level and low-level policies and another concurrent optimization of\nagents, we introduce the dual coordination mechanism of inter-layer strategies\nand inter-agent strategies. HAVEN does not require domain knowledge and\npretraining at all, and can be applied to any value decomposition variants. Our\nmethod is demonstrated to achieve superior results to many baselines on\nStarCraft II micromanagement tasks and offers an efficient solution to\nmulti-agent hierarchical reinforcement learning in fully cooperative scenarios.",
    "descriptor": "\nComments: 11 pages, 5 figures, 2 tables\n",
    "authors": [
      "Zhiwei Xu",
      "Yunpeng Bai",
      "Bin Zhang",
      "Dapeng Li",
      "Guoliang Fan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07246"
  },
  {
    "id": "arXiv:2110.07248",
    "title": "Synchronization and Balancing around Simple Closed Polar Curves with  Bounded Trajectories and Control Saturation",
    "abstract": "The problem of synchronization and balancing around simple closed polar\ncurves is addressed for unicycle-type multi-agent systems. Leveraging the\nconcept of barrier Lyapunov function in conjunction with bounded Lyapunov-like\ncurve-phase potential functions, we propose distributed feedback control laws\nand show that the agents asymptotically stabilize to the desired closed curve,\ntheir trajectories remain bounded within a compact set, and their turn-rates\nadhere to the saturation limits. We also characterize the explicit nature of\nthe boundary of this trajectory-constraining set based on the magnitude of the\nsafe distance of the exterior boundary from the desired curve. We further\nestablish a connection between the perimeters and areas of the\ntrajectory-constraining set with that of the desired curve. We obtain bounds on\ndifferent quantities of interest in the post-design analysis and provide\nsimulation results to illustrate the theoretical findings.",
    "descriptor": "",
    "authors": [
      "Aditya Hegde",
      "Anoop Jain"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07248"
  },
  {
    "id": "arXiv:2110.07253",
    "title": "Rethinking Point Cloud Filtering: A Non-Local Position Based Approach",
    "abstract": "Existing position based point cloud filtering methods can hardly preserve\nsharp geometric features. In this paper, we rethink point cloud filtering from\na non-learning non-local non-normal perspective, and propose a novel position\nbased approach for feature-preserving point cloud filtering. Unlike normal\nbased techniques, our method does not require the normal information. The core\nidea is to first design a similarity metric to search the non-local similar\npatches of a queried local patch. We then map the non-local similar patches\ninto a canonical space and aggregate the non-local information. The aggregated\noutcome (i.e. coordinate) will be inversely mapped into the original space. Our\nmethod is simple yet effective. Extensive experiments validate our method, and\nshow that it generally outperforms position based methods (deep learning and\nnon-learning), and generates better or comparable outcomes to normal based\ntechniques (deep learning and non-learning).",
    "descriptor": "",
    "authors": [
      "Jinxi Wang",
      "Jincen Jiang",
      "Xuequan Lu",
      "Meili Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.07253"
  },
  {
    "id": "arXiv:2110.07262",
    "title": "Proactive Mobility Management of UEs using Sequence-to-Sequence Modeling",
    "abstract": "Beyond 5G networks will operate at high frequencies with wide bandwidths.\nThis brings both opportunities and challenges. Opportunities include high\nthroughput connectivity with low latency. However, one of the main challenges\nin these networks is due to the high path loss at operating frequencies, which\nrequires network to be deployed densely to provide coverage. Since these cells\nhave small inter-site-distance (ISD), the dwell-time of the UEs in these cells\nare small, thus supporting mobility in these types of dense networks is a\nchallenge and require frequent beam or cell reassignments. A pro-active\nmobility management scheme which exploits the trajectory can provide better\nprediction of cells and beams as UEs move in the coverage area. We propose an\nAI based method using sequence-to-sequence modeling for the estimation of\nhandover cells/beams along with dwell-time using the trajectory information of\nthe UE. Results indicate that for a dense deployment an accuracy of more than\n90 percent can be achieved for handover cell estimation with very low mean\nabsolute error (MAE) for dwell-time.",
    "descriptor": "\nComments: 5 Pages, 11 Figures, Submitted to ICC 2022\n",
    "authors": [
      "Vijaya Yajnanarayana"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07262"
  },
  {
    "id": "arXiv:2110.07270",
    "title": "Spherical polar coordinate transformation for integration of singular  functions on tetrahedra",
    "abstract": "A method is presented for the evaluation of integrals on tetrahedra where the\nintegrand has a singularity at one vertex. The approach uses a transformation\nto spherical polar coordinates which explicitly eliminates the singularity and\nfacilitates the evaluation of integration limits. The method can also be\nimplemented in an adaptive form which gives convergence to a required\ntolerance. Results from the method are compared to the output from an exact\nanalytical method and show high accuracy. In particular, when the adaptive\nalgorithm is used, highly accurate results are found for poorly conditioned\ntetrahedra which normally present difficulties for numerical quadrature\ntechniques.",
    "descriptor": "",
    "authors": [
      "Michael J. Carley"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07270"
  },
  {
    "id": "arXiv:2110.07272",
    "title": "Relighting Humans in the Wild: Monocular Full-Body Human Relighting with  Domain Adaptation",
    "abstract": "The modern supervised approaches for human image relighting rely on training\ndata generated from 3D human models. However, such datasets are often small\n(e.g., Light Stage data with a small number of individuals) or limited to\ndiffuse materials (e.g., commercial 3D scanned human models). Thus, the human\nrelighting techniques suffer from the poor generalization capability and\nsynthetic-to-real domain gap. In this paper, we propose a two-stage method for\nsingle-image human relighting with domain adaptation. In the first stage, we\ntrain a neural network for diffuse-only relighting. In the second stage, we\ntrain another network for enhancing non-diffuse reflection by learning\nresiduals between real photos and images reconstructed by the diffuse-only\nnetwork. Thanks to the second stage, we can achieve higher generalization\ncapability against various cloth textures, while reducing the domain gap.\nFurthermore, to handle input videos, we integrate illumination-aware deep video\nprior to greatly reduce flickering artifacts even with challenging settings\nunder dynamic illuminations.",
    "descriptor": "",
    "authors": [
      "Daichi Tajima",
      "Yoshihiro Kanamori",
      "Yuki Endo"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.07272"
  },
  {
    "id": "arXiv:2110.07274",
    "title": "An Approach to Mispronunciation Detection and Diagnosis with Acoustic,  Phonetic and Linguistic (APL) Embeddings",
    "abstract": "Many mispronunciation detection and diagnosis (MD&D) research approaches try\nto exploit both the acoustic and linguistic features as input. Yet the\nimprovement of the performance is limited, partially due to the shortage of\nlarge amount annotated training data at the phoneme level. Phonetic embeddings,\nextracted from ASR models trained with huge amount of word level annotations,\ncan serve as a good representation of the content of input speech, in a\nnoise-robust and speaker-independent manner. These embeddings, when used as\nimplicit phonetic supplementary information, can alleviate the data shortage of\nexplicit phoneme annotations. We propose to utilize Acoustic, Phonetic and\nLinguistic (APL) embedding features jointly for building a more powerful MD\\&D\nsystem. Experimental results obtained on the L2-ARCTIC database show the\nproposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the\ndetection accuracy, diagnosis error rate and the F-measure, respectively.",
    "descriptor": "",
    "authors": [
      "Wenxuan Ye",
      "Shaoguang Mao",
      "Frank Soong",
      "Wenshan Wu",
      "Yan Xia",
      "Jonathan Tien",
      "Zhiyong Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07274"
  },
  {
    "id": "arXiv:2110.07275",
    "title": "Order Constraints in Optimal Transport",
    "abstract": "Optimal transport is a framework for comparing measures whereby a cost is\nincurred for transporting one measure to another. Recent works have aimed to\nimprove optimal transport plans through the introduction of various forms of\nstructure. We introduce novel order constraints into the optimal transport\nformulation to allow for the incorporation of structure. While there will are\nnow quadratically many constraints as before, we prove a $\\delta-$approximate\nsolution to the order-constrained optimal transport problem can be obtained in\n$\\mathcal{O}(L^2\\delta^{-2} \\kappa(\\delta(2cL_\\infty (1+(mn)^{1/2}))^{-1})\n\\cdot mn\\log mn)$ time. We derive computationally efficient lower bounds that\nallow for an explainable approach to adding structure to the optimal transport\nplan through order constraints. We demonstrate experimentally that order\nconstraints improve explainability using the e-SNLI (Stanford Natural Language\nInference) dataset that includes human-annotated rationales for each\nassignment.",
    "descriptor": "\nComments: Preprint. 8 pages of main + 2 pages references, and 10 pages supplementary\n",
    "authors": [
      "Fabian Lim",
      "Laura Wynter",
      "Shiau Hong Lim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07275"
  },
  {
    "id": "arXiv:2110.07276",
    "title": "Carousel Memory: Rethinking the Design of Episodic Memory for Continual  Learning",
    "abstract": "Continual Learning (CL) is an emerging machine learning paradigm that aims to\nlearn from a continuous stream of tasks without forgetting knowledge learned\nfrom the previous tasks. To avoid performance decrease caused by forgetting,\nprior studies exploit episodic memory (EM), which stores a subset of the past\nobserved samples while learning from new non-i.i.d. data. Despite the promising\nresults, since CL is often assumed to execute on mobile or IoT devices, the EM\nsize is bounded by the small hardware memory capacity and makes it infeasible\nto meet the accuracy requirements for real-world applications. Specifically,\nall prior CL methods discard samples overflowed from the EM and can never\nretrieve them back for subsequent training steps, incurring loss of information\nthat would exacerbate catastrophic forgetting. We explore a novel hierarchical\nEM management strategy to address the forgetting issue. In particular, in\nmobile and IoT devices, real-time data can be stored not just in high-speed\nRAMs but in internal storage devices as well, which offer significantly larger\ncapacity than the RAMs. Based on this insight, we propose to exploit the\nabundant storage to preserve past experiences and alleviate the forgetting by\nallowing CL to efficiently migrate samples between memory and storage without\nbeing interfered by the slow access speed of the storage. We call it Carousel\nMemory (CarM). As CarM is complementary to existing CL methods, we conduct\nextensive evaluations of our method with seven popular CL methods and show that\nCarM significantly improves the accuracy of the methods across different\nsettings by large margins in final average accuracy (up to 28.4%) while\nretaining the same training efficiency.",
    "descriptor": "",
    "authors": [
      "Soobee Lee",
      "Minindu Weerakoon",
      "Jonghyun Choi",
      "Minjia Zhang",
      "Di Wang",
      "Myeongjae Jeon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07276"
  },
  {
    "id": "arXiv:2110.07280",
    "title": "P-Adapters: Robustly Extracting Factual Information from Language Models  with Diverse Prompts",
    "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of\nthe factual information extracted from Large Language Models (LLMs) depends on\nthe prompts used to query them. This inconsistency is problematic because\ndifferent users will query LLMs for the same information using different\nwording, but should receive the same, accurate responses regardless. In this\nwork we aim to address this shortcoming by introducing P-Adapters: lightweight\nmodels that sit between the embedding layer and first attention layer of LLMs.\nThey take LLM embeddings as input and output continuous prompts that are used\nto query the LLM. Additionally, we investigate Mixture of Experts (MoE) models\nthat learn a set of continuous prompts (\"experts\") and select one to query the\nLLM. They require a separate classifier trained on human-annotated data to map\nnatural language prompts to the continuous ones. P-Adapters perform comparably\nto the more complex MoE models in extracting factual information from BERT and\nRoBERTa while eliminating the need for additional annotations. P-Adapters show\nbetween 12-26% absolute improvement in precision and 36-50% absolute\nimprovement in consistency over a baseline of only using natural language\nqueries. Finally, we investigate what makes a P-adapter successful and conclude\nthat access to the LLM's embeddings of the original natural language prompt,\nparticularly the subject of the entity pair being asked about, is a significant\nfactor.",
    "descriptor": "\nComments: 15 pages, 6 figures, 4 tables\n",
    "authors": [
      "Benjamin Newman",
      "Prafulla Kumar Choubey",
      "Nazneen Rajani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07280"
  },
  {
    "id": "arXiv:2110.07283",
    "title": "DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and  Retro-Reflectors",
    "abstract": "In this paper, a marker-based, single-person optical motion capture method\n(DeepMoCap) is proposed using multiple spatio-temporally aligned infrared-depth\nsensors and retro-reflective straps and patches (reflectors). DeepMoCap\nexplores motion capture by automatically localizing and labeling reflectors on\ndepth images and, subsequently, on 3D space. Introducing a non-parametric\nrepresentation to encode the temporal correlation among pairs of colorized\ndepthmaps and 3D optical flow frames, a multi-stage Fully Convolutional Network\n(FCN) architecture is proposed to jointly learn reflector locations and their\ntemporal dependency among sequential frames. The extracted reflector 2D\nlocations are spatially mapped in 3D space, resulting in robust 3D optical data\nextraction. The subject's motion is efficiently captured by applying a\ntemplate-based fitting technique on the extracted optical data. Two datasets\nhave been created and made publicly available for evaluation purposes; one\ncomprising multi-view depth and 3D optical flow annotated images (DMC2.5D), and\na second, consisting of spatio-temporally aligned multi-view depth images along\nwith skeleton, inertial and ground truth MoCap data (DMC3D). The FCN model\noutperforms its competitors on the DMC2.5D dataset using 2D Percentage of\nCorrect Keypoints (PCK) metric, while the motion capture outcome is evaluated\nagainst RGB-D and inertial data fusion approaches on DMC3D, outperforming the\nnext best method by 4.5% in total 3D PCK accuracy.",
    "descriptor": "",
    "authors": [
      "Anargyros Chatzitofis",
      "Dimitrios Zarpalas",
      "Stefanos Kollias",
      "Petros Daras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07283"
  },
  {
    "id": "arXiv:2110.07285",
    "title": "Optimization-based modelling and game-theoretic framework for  techno-economic analysis of demand-side flexibility: a real case study",
    "abstract": "This paper proposes a two-step framework for techno-economic analysis of a\ndemand-side flexibility service in distribution networks. Step one applies\noptimization-based modelling to propose a generic problem formulation which\ndetermines the offer curve, in terms of available flexible capacity and its\nmarginal cost, for flexible distribution-connected assets. These offer curves\nform an input to the second step, which uses a multi-agent iterative game\nframework to determine the benefits of demand-side flexibility for the\nDistribution System Operator (DSO) and the service providers. The combined\ntwo-step framework simultaneously accounts for the objectives of each\nflexibility provider, technical constraints of flexible assets, customer\npreferences, market clearing mechanisms, and strategic bidding by service\nproviders, omission of any of which can lead to erroneous results. The proposed\ntwo-step framework has been applied to a real case study in the North East of\nEngland to examine four market mechanisms and three bidding strategies. The\nresults showed that among all considered market mechanisms, flexibility markets\nthat operate under discriminatory pricing, such as pay-as-bid and Dutch reverse\nauctions, are prone to manipulations, especially in the lack of competition. In\ncontrast, uniform pricing pay-as-cleared auction provides limited opportunities\nfor manipulation even when competition is low.",
    "descriptor": "\nComments: 33 pages, 5 figures, submitted to Applied Energy\n",
    "authors": [
      "Timur Sayfutdinov",
      "Charalampos Patsios",
      "David Greenwood",
      "Meltem Peker",
      "Ilias Sarantakos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07285"
  },
  {
    "id": "arXiv:2110.07288",
    "title": "View Vertically: A Hierarchical Network for Trajectory Prediction via  Fourier Spectrums",
    "abstract": "Learning to understand and predict future motions or behaviors for agents\nlike humans and robots are critical to various autonomous platforms, such as\nbehavior analysis, robot navigation, and self-driving cars. Intrinsic factors\nsuch as agents' diversified personalities and decision-making styles bring rich\nand diverse changes and multi-modal characteristics to their future plannings.\nBesides, the extrinsic interactive factors have also brought rich and varied\nchanges to their trajectories. Previous methods mostly treat trajectories as\ntime sequences, and reach great prediction performance. In this work, we try to\nfocus on agents' trajectories in another view, i.e., the Fourier spectrums, to\nexplore their future behavior rules in a novel hierarchical way. We propose the\nTransformer-based V model, which concatenates two continuous keypoints\nestimation and spectrum interpolation sub-networks, to model and predict\nagents' trajectories with spectrums in the keypoints and interactions levels\nrespectively. Experimental results show that V outperforms most of current\nstate-of-the-art methods on ETH-UCY and SDD trajectories dataset for about 15\\%\nquantitative improvements, and performs better qualitative results.",
    "descriptor": "",
    "authors": [
      "Conghao Wong",
      "Beihao Xia",
      "Ziming Hong",
      "Qinmu Peng",
      "Xinge You"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07288"
  },
  {
    "id": "arXiv:2110.07291",
    "title": "Only Time Will Tell: Modelling Communication for Information Diffusion  in Software Engineering",
    "abstract": "In this manuscript, we present a novel model based on time-varying\nhypergraphs for rendering communication networks in software engineering as\ndistributed and self-organizing information repositories, capable of encoding\nand decoding information over time. Our model overcomes the inherent\nlimitations of traditional, static graph-based models and enables research on\nthe time-dependent spread of information, the so-called information diffusion,\nwithin those networks. We perform a discrete-event computer simulation\nrendering an arbitrary information diffusion in a communication network typical\nfor software engineering, namely code review, in order to (1) present an\nempirical showcase of our model rendering the code review at Microsoft, (2)\nvalidate our model in comparison with an equivalent, but time-aggregated model,\nand, thereby, (3) demonstrate the outstanding importance of time for research\non communication networks. Lastly, we discuss the implications, the practical\napplications, and an outlook on future research on communication networks as\ndistributed and decentralized information repositories -- unlocked by our novel\nmodel.",
    "descriptor": "",
    "authors": [
      "Michael Dorner",
      "Daniel Mendez",
      "Krzysztof Wnuk",
      "Jacek Czerwonka"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.07291"
  },
  {
    "id": "arXiv:2110.07292",
    "title": "Sign and Relevance learning",
    "abstract": "Standard models of biologically realistic, or inspired, reinforcement\nlearning employ a global error signal which implies shallow networks. However,\ndeep networks could offer a drastically superior performance by feeding the\nerror signal backwards through such a network which in turn is not biologically\nrealistic as it requires symmetric weights between top-down and bottom-up\npathways. Instead, we present a network combining local learning with global\nmodulation where neuromodulation controls the amount of plasticity change in\nthe whole network, while only the sign of the error is backpropagated through\nthe network. The neuromodulation can be understood as a rectified error, or\nrelevance, signal while the bottom-up sign of the error signal decides between\nlong-term potentiation and long-term depression. We demonstrate the performance\nof this paradigm with a real robotic task.",
    "descriptor": "\nComments: 26 pages, 12 figures\n",
    "authors": [
      "Sama Daryanavard",
      "Bernd Porr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.07292"
  },
  {
    "id": "arXiv:2110.07296",
    "title": "Resource Allocation for Simultaneous Wireless Information and Power  Transfer Systems: A Tutorial Overview",
    "abstract": "Over the last decade, simultaneous wireless information and power transfer\n(SWIPT) has become a practical and promising solution for connecting and\nrecharging battery-limited devices, thanks to significant advances in low-power\nelectronics technology and wireless communications techniques. To realize the\npromised potentials, advanced resource allocation design plays a decisive role\nin revealing, understanding, and exploiting the intrinsic rate-energy tradeoff\ncapitalizing on the dual use of radio frequency (RF) signals for wireless\ncharging and communication. In this paper, we provide a comprehensive tutorial\noverview of SWIPT from the perspective of resource allocation design. The\nfundamental concepts, system architectures, and RF energy harvesting (EH)\nmodels are introduced. In particular, three commonly adopted EH models, namely\nthe linear EH model, the nonlinear saturation EH model, and the nonlinear\ncircuit-based EH model are characterized and discussed. Then, for a typical\nwireless system setup, we establish a generalized resource allocation design\nframework which subsumes conventional resource allocation design problems as\nspecial cases. Subsequently, we elaborate on relevant tools from optimization\ntheory and exploit them for solving representative resource allocation design\nproblems for SWIPT systems with and without perfect channel state information\n(CSI) available at the transmitter, respectively. The associated technical\nchallenges and insights are also highlighted. Furthermore, we discuss several\npromising and exciting future research directions for resource allocation\ndesign for SWIPT systems intertwined with cutting-edge communication\ntechnologies, such as intelligent reflecting surfaces, unmanned aerial\nvehicles, mobile edge computing, federated learning, and machine learning.",
    "descriptor": "\nComments: 21 pages, 5 figures, To appear in Proceedings of the IEEE\n",
    "authors": [
      "Zhiqiang Wei",
      "Xianghao Yu",
      "Derrick Wing Kwan Ng",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.07296"
  },
  {
    "id": "arXiv:2110.07298",
    "title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based  on Prompt Tuning of T5",
    "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.",
    "descriptor": "",
    "authors": [
      "Chengwei Qin",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07298"
  },
  {
    "id": "arXiv:2110.07301",
    "title": "Multi-task problems are not multi-objective",
    "abstract": "Multi-objective optimization (MOO) aims at finding a set of optimal\nconfigurations for a given set of objectives. A recent line of work applies MOO\nmethods to the typical Machine Learning (ML) setting, which becomes\nmulti-objective if a model should optimize more than one objective, for\ninstance in fair machine learning. These works also use Multi-Task Learning\n(MTL) problems to benchmark MOO algorithms treating each task as independent\nobjective.\nIn this work we show that MTL problems do not resemble the characteristics of\nMOO problems. In particular, MTL losses are not competing in case of a\nsufficiently expressive single model. As a consequence, a single model can\nperform just as well as optimizing all objectives with independent models,\nrendering MOO inapplicable. We provide evidence with extensive experiments on\nthe widely used Multi-Fashion-MNIST datasets. Our results call for new\nbenchmarks to evaluate MOO algorithms for ML. Our code is available at:\nhttps://github.com/ruchtem/moo-mtl.",
    "descriptor": "",
    "authors": [
      "Michael Ruchte",
      "Josif Grabocka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07301"
  },
  {
    "id": "arXiv:2110.07303",
    "title": "Aspect-Sentiment-Multiple-Opinion Triplet Extraction",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term\n(aspect), sentiment and opinion term (opinion) triplets from sentences and can\ntell a complete story, i.e., the discussed aspect, the sentiment toward the\naspect, and the cause of the sentiment. ASTE is a charming task, however, one\ntriplet extracted by ASTE only includes one opinion of the aspect, but an\naspect in a sentence may have multiple corresponding opinions and one opinion\nonly provides part of the reason why the aspect has this sentiment, as a\nconsequence, some triplets extracted by ASTE are hard to understand, and\nprovide erroneous information for downstream tasks. In this paper, we introduce\na new task, named Aspect Sentiment Multiple Opinions Triplet Extraction\n(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions\ntriplets. Specifically, one triplet extracted by ASMOTE contains all opinions\nabout the aspect and can tell the exact reason that the aspect has the\nsentiment. We propose an Aspect-Guided Framework (AGF) to address this task.\nAGF first extracts aspects, then predicts their opinions and sentiments.\nMoreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF\nimproves the performance of the sentiment classification using the extracted\nopinions. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach.",
    "descriptor": "\nComments: NLPCC 2021\n",
    "authors": [
      "Fang Wang",
      "Yuncong Li",
      "Sheng-hua Zhong",
      "Cunxiang Yin",
      "Yancheng He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07303"
  },
  {
    "id": "arXiv:2110.07304",
    "title": "An Empirical Investigation of Multi-bridge Multilingual NMT models",
    "abstract": "In this paper, we present an extensive investigation of multi-bridge,\nmany-to-many multilingual NMT models (MB-M2M) ie., models trained on\nnon-English language pairs in addition to English-centric language pairs. In\naddition to validating previous work which shows that MB-M2M models can\novercome zeroshot translation problems, our analysis reveals the following\nresults about multibridge models: (1) it is possible to extract a reasonable\namount of parallel corpora between non-English languages for low-resource\nlanguages (2) with limited non-English centric data, MB-M2M models are\ncompetitive with or outperform pivot models, (3) MB-M2M models can outperform\nEnglish-Any models and perform at par with Any-English models, so a single\nmultilingual NMT system can serve all translation directions.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Anoop Kunchukuttan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07304"
  },
  {
    "id": "arXiv:2110.07305",
    "title": "DI-AA: An Interpretable White-box Attack for Fooling Deep Neural  Networks",
    "abstract": "White-box Adversarial Example (AE) attacks towards Deep Neural Networks\n(DNNs) have a more powerful destructive capacity than black-box AE attacks in\nthe fields of AE strategies. However, almost all the white-box approaches lack\ninterpretation from the point of view of DNNs. That is, adversaries did not\ninvestigate the attacks from the perspective of interpretable features, and few\nof these approaches considered what features the DNN actually learns. In this\npaper, we propose an interpretable white-box AE attack approach, DI-AA, which\nexplores the application of the interpretable approach of the deep Taylor\ndecomposition in the selection of the most contributing features and adopts the\nLagrangian relaxation optimization of the logit output and L_p norm to further\ndecrease the perturbation. We compare DI-AA with six baseline attacks\n(including the state-of-the-art attack AutoAttack) on three datasets.\nExperimental results reveal that our proposed approach can 1) attack non-robust\nmodels with comparatively low perturbation, where the perturbation is closer to\nor lower than the AutoAttack approach; 2) break the TRADES adversarial training\nmodels with the highest success rate; 3) the generated AE can reduce the robust\naccuracy of the robust black-box models by 16% to 31% in the black-box transfer\nattack.",
    "descriptor": "\nComments: 9 pages, 5 figures, 7 tables\n",
    "authors": [
      "Yixiang Wang",
      "Jiqiang Liu",
      "Xiaolin Chang",
      "Jianhua Wang",
      "Ricardo J. Rodr\u00edguez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07305"
  },
  {
    "id": "arXiv:2110.07307",
    "title": "ClonalNet: Classifying Better by Focusing on Confusing Categories",
    "abstract": "Existing neural classification networks predominately adopt one-hot encoding\ndue to its simplicity in representing categorical data. However, the one-hot\nrepresentation neglects inter-category correlations, which may result in poor\ngeneralization. Herein, we observe that a pre-trained baseline network has paid\nattention to the target image region even though it incorrectly predicts the\nimage, revealing which categories confuse the baseline. This observation\nmotivates us to consider inter-category correlations. Therefore, we propose a\nclonal network, named ClonalNet, which learns to discriminate between confusing\ncategories derived from the pre-trained baseline. The ClonalNet architecture\ncan be identical or smaller than the baseline architecture. When identical,\nClonalNet is a clonal version of the baseline but does not share weights. When\nsmaller, the training process of ClonalNet resembles that of the standard\nknowledge distillation. The difference from knowledge distillation is that we\ndesign a focusing-picking loss to optimize ClonalNet. This novel loss enforces\nClonalNet to concentrate on confusing categories and make more confident\npredictions on ground-truth labels with the baseline reference. Experiments\nshow that ClonalNet significantly outperforms baseline networks and knowledge\ndistillation.",
    "descriptor": "",
    "authors": [
      "Xue Zhang",
      "Hui-Liang Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07307"
  },
  {
    "id": "arXiv:2110.07309",
    "title": "Cell-Free Massive MIMO for 6G Wireless Communication Networks",
    "abstract": "The recently commercialized fifth-generation (5G) wireless communication\nnetworks achieved many improvements, including air interface enhancement,\nspectrum expansion, and network intensification by several key technologies,\nsuch as massive multiple-input multiple-output (MIMO), millimeter-wave\ncommunications, and ultra-dense networking. Despite the deployment of 5G\ncommercial systems, wireless communications is still facing many challenges to\nenable connected intelligence and a myriad of applications such as industrial\nInternet-of-things, autonomous systems, brain-computer interfaces, digital\ntwin, tactile Internet, etc. Therefore, it is urgent to start research on the\nsixth-generation (6G) wireless communication systems. Among the candidate\ntechnologies for such systems, cell-free massive MIMO which combines the\nadvantages of distributed systems and massive MIMO is considered as a key\nsolution to enhance the wireless transmission efficiency and becomes the\ninternational frontier. In this paper, we present a comprehensive study on\ncell-free massive MIMO for 6G wireless communication networks, especially from\nthe signal processing perspective. We focus on enabling physical layer\ntechnologies for cell-free massive MIMO, such as user association, pilot\nassignment, transmitter and receiver design, as well as power control and\nallocation. Furthermore, some current and future research problems are\nhighlighted.",
    "descriptor": "\nComments: 28 pages, 4 figures, 4 tables, Accepted by Journal of Communications and Information Networks\n",
    "authors": [
      "Hengtao He",
      "Xianghao Yu",
      "Jun Zhang",
      "S.H. Song",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.07309"
  },
  {
    "id": "arXiv:2110.07310",
    "title": "Solving Aspect Category Sentiment Analysis as a Text Generation Task",
    "abstract": "Aspect category sentiment analysis has attracted increasing research\nattention. The dominant methods make use of pre-trained language models by\nlearning effective aspect category-specific representations, and adding\nspecific output layers to its pre-trained representation. We consider a more\ndirect way of making use of pre-trained language models, by casting the ACSA\ntasks into natural language generation tasks, using natural language sentences\nto represent the output. Our method allows more direct use of pre-trained\nknowledge in seq2seq language models by directly following the task setting\nduring pre-training. Experiments on several benchmarks show that our method\ngives the best reported results, having large advantages in few-shot and\nzero-shot settings.",
    "descriptor": "\nComments: EMNLP 2021 main conference\n",
    "authors": [
      "Jian Liu",
      "Zhiyang Teng",
      "Leyang Cui",
      "Hanmeng Liu",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07310"
  },
  {
    "id": "arXiv:2110.07311",
    "title": "SpecSinGAN: Sound Effect Variation Synthesis Using Single-Image GANs",
    "abstract": "Single-image generative adversarial networks learn from the internal\ndistribution of a single training example to generate variations of it,\nremoving the need of a large dataset. In this paper we introduce SpecSinGAN, an\nunconditional generative architecture that takes a single one-shot sound effect\n(e.g., a footstep; a character jump) and produces novel variations of it, as if\nthey were different takes from the same recording session. We explore the use\nof multi-channel spectrograms to train the model on the various layers that\ncomprise a single sound effect. A listening study comparing our model to real\nrecordings and to digital signal processing procedural audio models in terms of\nsound plausibility and variation revealed that SpecSinGAN is more plausible and\nvaried than the procedural audio models considered, when using multi-channel\nspectrograms. Sound examples can be found at the project website:\nhttps://www.adrianbarahonarios.com/specsingan/",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Adri\u00e1n Barahona-R\u00edos",
      "Tom Collins"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07311"
  },
  {
    "id": "arXiv:2110.07313",
    "title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks",
    "abstract": "Representation learning from unlabeled data has been of major interest in\nartificial intelligence research. While self-supervised speech representation\nlearning has been popular in the speech research community, very few works have\ncomprehensively analyzed audio representation learning for non-speech audio\ntasks. In this paper, we propose a self-supervised audio representation\nlearning method and apply it to a variety of downstream non-speech audio tasks.\nWe combine the well-known wav2vec 2.0 framework, which has shown success in\nself-supervised learning for speech tasks, with parameter-efficient conformer\narchitectures. On the AudioSet benchmark, we achieve a mean average precision\n(mAP) score of 0.415, which is a new state-of-the-art on this dataset through\naudio-only self-supervised learning. Our fine-tuned conformers also surpass or\nmatch the performance of previous systems pre-trained in a supervised way on\nseveral downstream tasks. We further discuss the important design\nconsiderations for both pre-training and fine-tuning.",
    "descriptor": "\nComments: 4 pages. Submitted to ICASSP in Oct 2021\n",
    "authors": [
      "Sangeeta Srivastava",
      "Yun Wang",
      "Andros Tjandra",
      "Anurag Kumar",
      "Chunxi Liu",
      "Kritika Singh",
      "Yatharth Saraf"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07313"
  },
  {
    "id": "arXiv:2110.07317",
    "title": "ReGVD: Revisiting Graph Neural Networks for Vulnerability Detection",
    "abstract": "Identifying vulnerabilities in the source code is essential to protect the\nsoftware systems from cyber security attacks. It, however, is also a\nchallenging step that requires specialized expertise in security and code\nrepresentation. Inspired by the successful applications of pre-trained\nprogramming language (PL) models such as CodeBERT and graph neural networks\n(GNNs), we propose ReGVD, a general and novel graph neural network-based model\nfor vulnerability detection. In particular, ReGVD views a given source code as\na flat sequence of tokens and then examines two effective methods of utilizing\nunique tokens and indexes respectively to construct a single graph as an input,\nwherein node features are initialized only by the embedding layer of a\npre-trained PL model. Next, ReGVD leverages a practical advantage of residual\nconnection among GNN layers and explores a beneficial mixture of graph-level\nsum and max poolings to return a graph embedding for the given source code.\nExperimental results demonstrate that ReGVD outperforms the existing\nstate-of-the-art models and obtain the highest accuracy on the real-world\nbenchmark dataset from CodeXGLUE for vulnerability detection.",
    "descriptor": "\nComments: The first two authors contributed equally. The code will be available soon at: this https URL\n",
    "authors": [
      "Van-Anh Nguyen",
      "Dai Quoc Nguyen",
      "Van Nguyen",
      "Trung Le",
      "Quan Hung Tran",
      "Dinh Phung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07317"
  },
  {
    "id": "arXiv:2110.07322",
    "title": "Modeling dynamic target deformation in camera calibration",
    "abstract": "Most approaches to camera calibration rely on calibration targets of\nwell-known geometry. During data acquisition, calibration target and camera\nsystem are typically moved w.r.t. each other, to allow image coverage and\nperspective versatility. We show that moving the target can lead to small\ntemporary deformations of the target, which can introduce significant errors\ninto the calibration result. While static inaccuracies of calibration targets\nhave been addressed in previous works, to our knowledge, none of the existing\napproaches can capture time-varying, dynamic deformations. To achieve\nhigh-accuracy calibrations despite moving the target, we propose a way to\nexplicitly model dynamic target deformations in camera calibration. This is\nachieved by using a low-dimensional deformation model with only few parameters\nper image, which can be optimized jointly with target poses and intrinsics. We\ndemonstrate the effectiveness of modeling dynamic deformations using different\ncalibration targets and show its significance in a structure-from-motion\napplication.",
    "descriptor": "\nComments: Accepted for publication at IEEE/CVF, WACV 2022\n",
    "authors": [
      "Annika Hagemann",
      "Moritz Knorr",
      "Christoph Stiller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07322"
  },
  {
    "id": "arXiv:2110.07330",
    "title": "WMDecompose: A Framework for Leveraging the Interpretable Properties of  Word Mover's Distance in Sociocultural Analysis",
    "abstract": "Despite the increasing popularity of NLP in the humanities and social\nsciences, advances in model performance and complexity have been accompanied by\nconcerns about interpretability and explanatory power for sociocultural\nanalysis. One popular model that balances complexity and legibility is Word\nMover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has\nnonetheless been used and further developed in ways which frequently discard\nits most interpretable aspect: namely, the word-level distances required for\ntranslating a set of words into another set of words. To address this apparent\ngap, we introduce WMDecompose: a model and Python library that 1) decomposes\ndocument-level distances into their constituent word-level distances, and 2)\nsubsequently clusters words to induce thematic elements, such that useful\nlexical information is retained and summarized for analysis. To illustrate its\npotential in a social scientific context, we apply it to a longitudinal social\nmedia corpus to explore the interrelationship between conspiracy theories and\nconservative American discourses. Finally, because of the full WMD model's high\ntime-complexity, we additionally suggest a method of sampling document pairs\nfrom large datasets in a reproducible way, with tight bounds that prevent\nextrapolation of unreliable results due to poor sampling practices.",
    "descriptor": "\nComments: SIGHUM @EMNLP2021\n",
    "authors": [
      "Mikael Brunila",
      "Jack LaViolette"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07330"
  },
  {
    "id": "arXiv:2110.07331",
    "title": "Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language  Models",
    "abstract": "Plug-and-play functionality allows deep learning models to adapt well to\ndifferent tasks without requiring any parameters modified. Recently,\nprefix-tuning was shown to be a plug-and-play method on various text generation\ntasks by simply inserting corresponding continuous vectors into the inputs.\nHowever, sequence labeling tasks invalidate existing plug-and-play methods\nsince different label sets demand changes to the architecture of the model\nclassifier. In this work, we propose the use of label word prediction instead\nof classification to totally reuse the architecture of pre-trained models for\nsequence labeling tasks. Specifically, for each task, a label word set is first\nconstructed by selecting a high-frequency word for each class respectively, and\nthen, task-specific vectors are inserted into the inputs and optimized to\nmanipulate the model predictions towards the corresponding label words. As a\nresult, by simply switching the plugin vectors on the input, a frozen\npre-trained language model is allowed to perform different tasks. Experimental\nresults on three sequence labeling tasks show that the performance of the\nproposed method can achieve comparable performance with standard fine-tuning\nwith only 0.1\\% task-specific parameters. In addition, our method is up to 70\ntimes faster than non-plug-and-play methods while switching different tasks\nunder the resource-constrained scenario.",
    "descriptor": "\nComments: Work in Progress\n",
    "authors": [
      "Xin Zhou",
      "Ruotian Ma",
      "Tao Gui",
      "Yiding Tan",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07331"
  },
  {
    "id": "arXiv:2110.07333",
    "title": "A Survey on Legal Question Answering Systems",
    "abstract": "Many legal professionals think that the explosion of information about local,\nregional, national, and international legislation makes their practice more\ncostly, time-consuming, and even error-prone. The two main reasons for this are\nthat most legislation is usually unstructured, and the tremendous amount and\npace with which laws are released causes information overload in their daily\ntasks. In the case of the legal domain, the research community agrees that a\nsystem allowing to generate automatic responses to legal questions could\nsubstantially impact many practical implications in daily activities. The\ndegree of usefulness is such that even a semi-automatic solution could\nsignificantly help to reduce the workload to be faced. This is mainly because a\nQuestion Answering system could be able to automatically process a massive\namount of legal resources to answer a question or doubt in seconds, which means\nthat it could save resources in the form of effort, money, and time to many\nprofessionals in the legal sector. In this work, we quantitatively and\nqualitatively survey the solutions that currently exist to meet this challenge.",
    "descriptor": "\nComments: 57 pages, 1 figure, 10 tables\n",
    "authors": [
      "Jorge Martinez-Gil"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07333"
  },
  {
    "id": "arXiv:2110.07336",
    "title": "RPT: Toward Transferable Model on Heterogeneous Researcher Data via  Pre-Training",
    "abstract": "With the growth of the academic engines, the mining and analysis acquisition\nof massive researcher data, such as collaborator recommendation and researcher\nretrieval, has become indispensable. It can improve the quality of services and\nintelligence of academic engines. Most of the existing studies for researcher\ndata mining focus on a single task for a particular application scenario and\nlearning a task-specific model, which is usually unable to transfer to\nout-of-scope tasks. The pre-training technology provides a generalized and\nsharing model to capture valuable information from enormous unlabeled data. The\nmodel can accomplish multiple downstream tasks via a few fine-tuning steps. In\nthis paper, we propose a multi-task self-supervised learning-based researcher\ndata pre-training model named RPT. Specifically, we divide the researchers'\ndata into semantic document sets and community graph. We design the\nhierarchical Transformer and the local community encoder to capture information\nfrom the two categories of data, respectively. Then, we propose three\nself-supervised learning objectives to train the whole model. Finally, we also\npropose two transfer modes of RPT for fine-tuning in different scenarios. We\nconduct extensive experiments to evaluate RPT, results on three downstream\ntasks verify the effectiveness of pre-training for researcher data mining.",
    "descriptor": "",
    "authors": [
      "Ziyue Qiao",
      "Yanjie Fu",
      "Pengyang Wang",
      "Meng Xiao",
      "Zhiyuan Ning",
      "Yi Du",
      "Yuanchun Zhou"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Digital Libraries (cs.DL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07336"
  },
  {
    "id": "arXiv:2110.07337",
    "title": "Topic-time Heatmaps for Human-in-the-loop Topic Detection and Tracking",
    "abstract": "The essential task of Topic Detection and Tracking (TDT) is to organize a\ncollection of news media into clusters of stories that pertain to the same\nreal-world event. To apply TDT models to practical applications such as search\nengines and discovery tools, human guidance is needed to pin down the scope of\nan \"event\" for the corpus of interest. In this work in progress, we explore a\nhuman-in-the-loop method that helps users iteratively fine-tune TDT algorithms\nso that both the algorithms and the users themselves better understand the\nnature of the events. We generate a visual overview of the entire corpus,\nallowing the user to select regions of interest from the overview, and then ask\na series of questions to affirm (or reject) that the selected documents belong\nto the same event. The answers to these questions supplement the training data\nfor the event similarity model that underlies the system.",
    "descriptor": "\nComments: Accepted to DaSH Workshop, KDD 2021\n",
    "authors": [
      "Doug Beeferman",
      "Hang Jiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07337"
  },
  {
    "id": "arXiv:2110.07342",
    "title": "FILM: Following Instructions in Language with Modular Methods",
    "abstract": "Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume\nlearned hidden states will simultaneously integrate semantics from the language\nand vision to perform state tracking, spatial memory, exploration, and\nlong-term planning. In contrast, we propose a modular method with structured\nrepresentations that (1) builds a semantic map of the scene, and (2) performs\nexploration with a semantic search policy, to achieve the natural language\ngoal. Our modular method achieves SOTA performance (24.46%) with a substantial\n(8.17 % absolute) gap from previous work while using less data by eschewing\nboth expert trajectories and low-level instructions. Leveraging low-level\nlanguage, however, can further increase our performance (26.49%). Our findings\nsuggest that an explicit spatial memory and a semantic search policy can\nprovide a stronger and more general representation for state-tracking and\nguidance, even in the absence of expert trajectories or low-level instructions.",
    "descriptor": "",
    "authors": [
      "So Yeon Min",
      "Devendra Singh Chaplot",
      "Pradeep Ravikumar",
      "Yonatan Bisk",
      "Ruslan Salakhutdinov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07342"
  },
  {
    "id": "arXiv:2110.07346",
    "title": "On the ESL algorithm for solving energy games",
    "abstract": "We propose a variant of an algorithm introduced by Schewe and also studied by\nLuttenberger for solving parity or mean-payoff games. We present it over energy\ngames and call our variant the ESL algorithm (for Energy-Schewe-Luttenberger).\nWe find that using potential reductions as introduced by Gurvich, Karzanov and\nKhachiyan allows for a simple and elegant presentation of the algorithm, which\nrepeatedly applies a natural generalisation of Dijkstra's algorithm to the\ntwo-player setting due to Khachiyan, Gurvich and Zhao.",
    "descriptor": "\nComments: 14 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2110.04533\n",
    "authors": [
      "Antonio Casares",
      "Pierre Ohlmann"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.07346"
  },
  {
    "id": "arXiv:2110.07347",
    "title": "Improved Drug-target Interaction Prediction with Intermolecular Graph  Transformer",
    "abstract": "The identification of active binding drugs for target proteins (termed as\ndrug-target interaction prediction) is the key challenge in virtual screening,\nwhich plays an essential role in drug discovery. Although recent deep\nlearning-based approaches achieved better performance than molecular docking,\nexisting models often neglect certain aspects of the intermolecular\ninformation, hindering the performance of prediction. We recognize this problem\nand propose a novel approach named Intermolecular Graph Transformer (IGT) that\nemploys a dedicated attention mechanism to model intermolecular information\nwith a three-way Transformer-based architecture. IGT outperforms\nstate-of-the-art approaches by 9.1% and 20.5% over the second best for binding\nactivity and binding pose prediction respectively, and shows superior\ngeneralization ability to unseen receptor proteins. Furthermore, IGT exhibits\npromising drug screening ability against SARS-CoV-2 by identifying 83.1% active\ndrugs that have been validated by wet-lab experiments with near-native\npredicted binding poses.",
    "descriptor": "",
    "authors": [
      "Siyuan Liu",
      "Yusong Wang",
      "Tong Wang",
      "Yifan Deng",
      "Liang He",
      "Bin Shao",
      "Jian Yin",
      "Nanning Zheng",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.07347"
  },
  {
    "id": "arXiv:2110.07348",
    "title": "A Framework for Risk Assessment and Optimal Line Upgrade Selection to  Mitigate Wildfire Risk",
    "abstract": "As wildfires in the United States are becoming more frequent and severe,\nmitigating wildfire ignition risk from power line faults is an increasingly\ncrucial effort. Long-term ignition prevention strategies, especially converting\noverhead lines to underground cables, are expensive. Thus, it is important to\nprioritize upgrades on lines that will reduce wildfire ignition risk the most.\nHowever, since so many factors contribute to ignition risk, it is difficult to\nquantify the wildfire risk associated with power lines. This paper examines how\nvarious risk definitions based on historical wildfire risk maps can be used to\ninform transmission upgrade planning. These risk metrics are evaluated using an\noptimization model that determines which overhead lines should be undergrounded\nsuch that the total wildfire risk in the network is minimized. The risk\nassignment and upgrade selection are tested on both a synthetic network and the\nactual transmission lines in California.",
    "descriptor": "\nComments: 7 pages, 10 figures\n",
    "authors": [
      "Sofia Taylor",
      "Line A. Roald"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07348"
  },
  {
    "id": "arXiv:2110.07349",
    "title": "A Functional Abstraction of Typed Invocation Contexts",
    "abstract": "In their paper \"A Functional Abstraction of Typed Contexts\", Danvy and\nFilinski show how to derive a type system of the shift and reset operators from\na CPS semantics. In this paper, we show how this method scales to Felleisen's\ncontrol and prompt operators. Compared to shift and reset, control and prompt\nexhibit a more dynamic behavior, in that they can manipulate a trail of\ncontexts surrounding the invocation of previously captured continuations. Our\nkey observation is that, by adopting a functional representation of trails in\nthe CPS semantics, we can derive a type system that encodes all and only\nconstraints imposed by the CPS semantics.",
    "descriptor": "\nComments: 29 pages, 9 figures, Submitted to LMCS special issue on FSCD 2021\n",
    "authors": [
      "Youyou Cong",
      "Chiaki Ishio",
      "Kaho Honda",
      "Kenichi Asai"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.07349"
  },
  {
    "id": "arXiv:2110.07351",
    "title": "Shortened Polarization Kernels",
    "abstract": "A shortening method for large polarization kernels is presented, which\nresults in shortened kernels with the highest error exponent if applied to\nkernels of size up to 32. It uses lower and upper bounds on partial distances\nfor quick elimination of unsuitable shortening patterns.\nThe proposed algorithm is applied to some kernels of sizes 16 and 32 to\nobtain shortened kernels of sizes from 9 to 31. These kernels are used in\nmixed-kernel polar codes of various lengths. Numerical results demonstrate the\nadvantage of polar codes with shortened large kernels compared with shortened\nand punctured Arikan polar codes, and polar codes with small kernels.",
    "descriptor": "\nComments: Accepted to IEEE GLOBECOM 2021 Workshop on Channel Coding beyond 5G. arXiv admin note: text overlap with arXiv:2101.10269\n",
    "authors": [
      "Grigorii Trofimiuk"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07351"
  },
  {
    "id": "arXiv:2110.07354",
    "title": "Music Playlist Title Generation: A Machine-Translation Approach",
    "abstract": "We propose a machine-translation approach to automatically generate a\nplaylist title from a set of music tracks. We take a sequence of track IDs as\ninput and a sequence of words in a playlist title as output, adapting the\nsequence-to-sequence framework based on Recurrent Neural Network (RNN) and\nTransformer to the music data. Considering the orderless nature of music tracks\nin a playlist, we propose two techniques that remove the order of the input\nsequence. One is data augmentation by shuffling and the other is deleting the\npositional encoding. We also reorganize the existing music playlist datasets to\ngenerate phrase-level playlist titles. The result shows that the Transformer\nmodels generally outperform the RNN model. Also, removing the order of input\nsequence improves the performance further.",
    "descriptor": "\nComments: Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio, 22th International Society for Music Information Retrieval Conference (ISMIR)\n",
    "authors": [
      "SeungHeon Doh",
      "Junwon Lee",
      "Juhan Nam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07354"
  },
  {
    "id": "arXiv:2110.07355",
    "title": "Floating Isogeometric Analysis",
    "abstract": "We propose Floating Isogeometric Analysis (FLIGA), which extends the concepts\nof IGA to Lagrangian extreme deformation analysis. The method is based on a\nnovel tensor-product construction of B-Splines for the update of the basis\nfunctions in one direction of the parametric space. With basis functions\n'floating' deformation-dependently in this direction, mesh distortion is\novercome for problems in which extreme deformations occur predominantly along\nthe associated (possibly curved) physical axis. In doing so, we preserve the\nnumerical advantages of splines over many meshless basis functions, while\navoiding remeshing. We employ material point integration for numerical\nquadrature attributing a Lagrangian character to our technique. The paper\nintroduces the method and reviews the fundamental properties of the FLIGA basis\nfunctions, including a numerical patch test. The performance of FLIGA is then\nnumerically investigated on the benchmark of Newtonian and viscoelastic\nTaylor-Couette flow. Finally, we simulate a viscoelastic extrusion-based\nadditive manufacturing process, which served as the original motivation for the\nnew approach.",
    "descriptor": "",
    "authors": [
      "Helge C. Hille",
      "Siddhant Kumar",
      "Laura De Lorenzis"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2110.07355"
  },
  {
    "id": "arXiv:2110.07356",
    "title": "Medically Aware GPT-3 as a Data Generator for Medical Dialogue  Summarization",
    "abstract": "In medical dialogue summarization, summaries must be coherent and must\ncapture all the medically relevant information in the dialogue. However,\nlearning effective models for summarization require large amounts of labeled\ndata which is especially hard to obtain. We present an algorithm to create\nsynthetic training data with an explicit focus on capturing medically relevant\ninformation. We utilize GPT-3 as the backbone of our algorithm and scale 210\nhuman labeled examples to yield results comparable to using 6400 human labeled\nexamples (~30x) leveraging low-shot learning and an ensemble method. In\ndetailed experiments, we show that this approach produces high quality training\ndata that can further be combined with human labeled data to get summaries that\nare strongly preferable to those produced by models trained on human data alone\nboth in terms of medical accuracy and coherency.",
    "descriptor": "\nComments: Accepted to Machine learning for healthcare 2021\n",
    "authors": [
      "Bharath Chintagunta",
      "Namit Katariya",
      "Xavier Amatriain",
      "Anitha Kannan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07356"
  },
  {
    "id": "arXiv:2110.07357",
    "title": "Unsupervised Text Mining of COVID-19 Records",
    "abstract": "Since the beginning of coronavirus, the disease has spread worldwide and\ndrastically changed many aspects of the human's lifestyle. Twitter as a\npowerful tool can help researchers measure public health in response to\nCOVID-19. According to the high volume of data production on social networks,\nautomated text mining approaches can help search, read and summarize helpful\ninformation. This paper preprocessed the existing medical dataset regarding\nCOVID-19 named CORD-19 and annotated the dataset for supervised classification\ntasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset\nfor the research community. This may contribute towards finding new solutions\nfor some social interventions that COVID-19 has made. The preprocessed version\nof the mentioned dataset is publicly available through Github.",
    "descriptor": "",
    "authors": [
      "Mohamad Zamini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07357"
  },
  {
    "id": "arXiv:2110.07358",
    "title": "Memory-Based Semantic Parsing",
    "abstract": "We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.",
    "descriptor": "",
    "authors": [
      "Parag Jain",
      "Mirella Lapata"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07358"
  },
  {
    "id": "arXiv:2110.07362",
    "title": "Preconditioners for robust optimal control problems under uncertainty",
    "abstract": "The discretization of robust quadratic optimal control problems under\nuncertainty using the finite element method and the stochastic collocation\nmethod leads to large saddle-point systems, which are fully coupled across the\nrandom realizations. Despite its relevance for numerous engineering problems,\nthe solution of such systems is notoriusly challenging. In this manuscript, we\nstudy efficient preconditioners for all-at-once approaches using both an\nalgebraic and an operator preconditioning framework. We show in particular that\nfor values of the regularization parameter not too small, the saddle-point\nsystem can be efficiently solved by preconditioning in parallel all the state\nand adjoint equations. For small values of the regularization parameter,\nrobustness can be recovered by the additional solution of a small linear\nsystem, which however couples all realizations. A mean approximation and a\nChebyshev semi-iterative method are investigated to solve this reduced system.\nOur analysis considers a random elliptic partial differential equation whose\ndiffusion coefficient $\\kappa(x,\\omega)$ is modeled as an almost surely\ncontinuous and positive random field, though not necessarily uniformly bounded\nand coercive. We further provide estimates on the dependence of the\npreconditioned system on the variance of the random field. Such estimates\ninvolve either the first or second moment of the random variables $1/\\min_{x\\in\n\\overline{D}} \\kappa(x,\\omega)$ and $\\max_{x\\in \\overline{D}}\\kappa(x,\\omega)$,\nwhere $D$ is the spatial domain. The theoretical results are confirmed by\nnumerical experiments, and implementation details are further addressed.",
    "descriptor": "\nComments: 30 pages, 4 figures\n",
    "authors": [
      "Fabio Nobile",
      "Tommaso Vanzan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07362"
  },
  {
    "id": "arXiv:2110.07365",
    "title": "DynoLoc: Infrastructure-free RF Tracking in Dynamic Indoor Environments",
    "abstract": "Promising solutions exist today that can accurately track mobile entities\nindoor using visual inertial odometry in favorable visual conditions, or by\nleveraging fine-grained ranging (RF, ultrasonic, IR, etc.) to reference\nanchors. However, they are unable to directly cater to \"dynamic\" indoor\nenvironments (e.g. first responder scenarios, multi-player AR/VR gaming in\neveryday spaces, etc.) that are devoid of such favorable conditions. Indeed, we\nshow that the need for \"infrastructure-free\", and robustness to \"node mobility\"\nand \"visual conditions\" in such environments, motivates a robust RF-based\napproach along with the need to address a novel and challenging variant of its\ninfrastructure-free (i.e. peer-to-peer) localization problem that is\nlatency-bounded - accurate tracking of mobile entities imposes a latency budget\nthat not only affects the solution computation but also the collection of\npeer-to-peer ranges themselves.\nIn this work, we present the design and deployment of DynoLoc that addresses\nthis latency-bounded infrastructure-free RF localization problem. To this end,\nDynoLoc unravels the fundamental tradeoff between latency and localization\naccuracy and incorporates design elements that judiciously leverage the\navailable ranging resources to adaptively estimate the joint topology of nodes,\ncoupled with robust algorithm that maximizes the localization accuracy even in\nthe face of practical environmental artifacts (wireless connectivity and\nmultipath, node mobility, etc.). This allows DynoLoc to track (every second) a\nnetwork of few tens of mobile entities even at speeds of 1-2 m/s with median\naccuracies under 1-2 m (compared to 5m+ with baselines), without infrastructure\nsupport. We demonstrate DynoLoc's potential in a real-world firefighters'\ndrill, as well as two other use cases of (i) multi-player AR/VR gaming, and\n(ii) active shooter tracking by first responders.",
    "descriptor": "",
    "authors": [
      "Md. Shaifur Rahman",
      "Ayon Chakraborty",
      "Karthikeyan Sunderasan",
      "Sampath Rangarajan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07365"
  },
  {
    "id": "arXiv:2110.07366",
    "title": "Privacy Impact Assessment: Comparing methodologies with a focus on  practicality",
    "abstract": "Privacy and data protection have become more and more important in recent\nyears since an increasing number of enterprises and startups are harvesting\npersonal data as a part of their business model. One central requirement of the\nGDPR is the implementation of a data protection impact assessment for privacy\ncritical systems. However, the law does not dictate or recommend the use of any\nparticular framework. In this paper we compare different data protection impact\nassessment frameworks. We have developed a comparison and evaluation\nmethodology and applied this to three popular impact assessment frameworks. The\nresult of this comparison shows the weaknesses and strengths, but also clearly\nindicates that none of the tested frameworks fulfill all desired properties.\nThus, the development of a new or improved data protection impact assessment\nframework is an important open issue for future work, especially for sector\nspecific applications.",
    "descriptor": "",
    "authors": [
      "Tamas Bisztray",
      "Nils Gruschka"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07366"
  },
  {
    "id": "arXiv:2110.07367",
    "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and  Passage Re-ranking",
    "abstract": "In various natural language processing tasks, passage retrieval and passage\nre-ranking are two key procedures in finding and ranking relevant information.\nSince both the two procedures contribute to the final performance, it is\nimportant to jointly optimize them in order to achieve mutual improvement. In\nthis paper, we propose a novel joint training approach for dense passage\nretrieval and passage re-ranking. A major contribution is that we introduce the\ndynamic listwise distillation, where we design a unified listwise training\napproach for both the retriever and the re-ranker. During the dynamic\ndistillation, the retriever and the re-ranker can be adaptively improved\naccording to each other's relevance information. We also propose a hybrid data\naugmentation strategy to construct diverse training instances for listwise\ntraining approach. Extensive experiments show the effectiveness of our approach\non both MSMARCO and Natural Questions datasets. Our code is available at\nhttps://github.com/PaddlePaddle/RocketQA.",
    "descriptor": "\nComments: EMNLP 2021\n",
    "authors": [
      "Ruiyang Ren",
      "Yingqi Qu",
      "Jing Liu",
      "Wayne Xin Zhao",
      "Qiaoqiao She",
      "Hua Wu",
      "Haifeng Wang",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07367"
  },
  {
    "id": "arXiv:2110.07370",
    "title": "Ethics lines and Machine learning: a design and simulation of an  Association Rules Algorithm for exploiting the data",
    "abstract": "Data mining techniques offer great opportunities for developing ethics lines,\ntools for communication, participation and innovation whose main aim is to\nensure improvements and compliance with the values, conduct and commitments\nmaking up the code of ethics. The aim of this study is to suggest a process for\nexploiting the data generated by the data generated and collected from an\nethics line by extracting rules of association and applying the Apriori\nalgorithm. This makes it possible to identify anomalies and behaviour patterns\nrequiring action to review, correct, promote or expand them, as appropriate.\nFinally, I offer a simulated application of the Apriori algorithm, supplying it\nwith synthetic data to find out its potential, strengths and limitations.",
    "descriptor": "",
    "authors": [
      "Patrici Calvo",
      "Rebeca Egea-Moreno"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07370"
  },
  {
    "id": "arXiv:2110.07374",
    "title": "Physics informed neural networks for continuum micromechanics",
    "abstract": "Recently, physics informed neural networks have successfully been applied to\na broad variety of problems in applied mathematics and engineering. The\nprinciple idea is to use a neural network as a global ansatz function to\npartial differential equations. Due to the global approximation, physics\ninformed neural networks have difficulties in displaying localized effects and\nstrong non-linear solutions by optimization. In this work we consider material\nnon-linearities invoked by material inhomogeneities with sharp phase\ninterfaces. This constitutes a challenging problem for a method relying on a\nglobal ansatz. To overcome convergence issues, adaptive training strategies and\ndomain decomposition are studied. It is shown, that the domain decomposition\napproach is able to accurately resolve nonlinear stress, displacement and\nenergy fields in heterogeneous microstructures obtained from real-world\n$\\mu$CT-scans.",
    "descriptor": "",
    "authors": [
      "Alexander Henkes",
      "Henning Wessels",
      "Rolf Mahnken"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07374"
  },
  {
    "id": "arXiv:2110.07375",
    "title": "Multiple Style Transfer via Variational AutoEncoder",
    "abstract": "Modern works on style transfer focus on transferring style from a single\nimage. Recently, some approaches study multiple style transfer; these, however,\nare either too slow or fail to mix multiple styles. We propose ST-VAE, a\nVariational AutoEncoder for latent space-based style transfer. It performs\nmultiple style transfer by projecting nonlinear styles to a linear latent\nspace, enabling to merge styles via linear interpolation before transferring\nthe new style to the content image. To evaluate ST-VAE, we experiment on COCO\nfor single and multiple style transfer. We also present a case study revealing\nthat ST-VAE outperforms other methods while being faster, flexible, and setting\na new path for multiple style transfer.",
    "descriptor": "\nComments: 5 papges, 4 figures\n",
    "authors": [
      "Zhi-Song Liu",
      "Vicky Kalogeiton",
      "Marie-Paule Cani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07375"
  },
  {
    "id": "arXiv:2110.07376",
    "title": "Domain Adaptation on Semantic Segmentation with Separate Affine  Transformation in Batch Normalization",
    "abstract": "In recent years, unsupervised domain adaptation (UDA) for semantic\nsegmentation has brought many researchers'attention. Many of them take an\napproach to design a complex system so as to better align the gap between\nsource and target domain. Instead, we focus on the very basic structure of the\ndeep neural network, Batch Normalization, and propose to replace the Sharing\nAffine Transformation with our proposed Separate Affine Transformation (SEAT).\nThe proposed SEAT is simple, easily implemented and easy to integrate into\nexisting adversarial learning based UDA methods. Also, to further improve the\nadaptation quality, we introduce multi level adaptation by adding the\nlower-level features to the higher-level ones before feeding them to the\ndiscriminator, without adding extra discriminator like others. Experiments show\nthat the proposed methods is less complex without losing performance accuracy\nwhen compared with other UDA methods.",
    "descriptor": "",
    "authors": [
      "Junhao Yan",
      "Woonsok Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07376"
  },
  {
    "id": "arXiv:2110.07378",
    "title": "Scheduler-Pointed False Data Injection Attack for Event-Based Remote  State Estimation",
    "abstract": "In this paper, an attack problem is investigated for event-based remote state\nestimation in cyber-physical systems. Our objective is to degrade the effect of\nthe event-based scheduler while bypassing a $\\chi^2$ false data detector. A\ntwo-channel scheduler-pointed false data injection attack strategy is proposed\nby modifying the numerical characteristics of innovation signals. The attack\nstrategy is proved to be always existent, and an algorithm is provided to find\nit. Under the proposed attack strategy, the scheduler becomes almost invalid\nand the performance of the remote estimator is degraded. Numerical simulations\nare used to illustrate our theoretical results.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Qiulin Xu",
      "Junlin Xiong"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07378"
  },
  {
    "id": "arXiv:2110.07379",
    "title": "Towards Safer Transportation: a self-supervised learning approach for  traffic video deraining",
    "abstract": "Video monitoring of traffic is useful for traffic management and control,\ntraffic counting, and traffic law enforcement. However, traffic monitoring\nduring inclement weather such as rain is a challenging task because video\nquality is corrupted by streaks of falling rain on the video image, and this\nhinders reliable characterization not only of the road environment but also of\nroad-user behavior during such adverse weather events. This study proposes a\ntwo-stage self-supervised learning method to remove rain streaks in traffic\nvideos. The first and second stages address intra- and inter-frame noise,\nrespectively. The results indicated that the model exhibits satisfactory\nperformance in terms of the image visual quality and the Peak Signal-Noise\nRatio value.",
    "descriptor": "\nComments: Under review for presentation at TRB 2022 Annual Meeting\n",
    "authors": [
      "Shuya Zong",
      "Sikai Chen",
      "Samuel Labi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07379"
  },
  {
    "id": "arXiv:2110.07380",
    "title": "Reason induced visual attention for explainable autonomous driving",
    "abstract": "Deep learning (DL) based computer vision (CV) models are generally considered\nas black boxes due to poor interpretability. This limitation impedes efficient\ndiagnoses or predictions of system failure, thereby precluding the widespread\ndeployment of DLCV models in safety-critical tasks such as autonomous driving.\nThis study is motivated by the need to enhance the interpretability of DL model\nin autonomous driving and therefore proposes an explainable DL-based framework\nthat generates textual descriptions of the driving environment and makes\nappropriate decisions based on the generated descriptions. The proposed\nframework imitates the learning process of human drivers by jointly modeling\nthe visual input (images) and natural language, while using the language to\ninduce the visual attention in the image. The results indicate strong\nexplainability of autonomous driving decisions obtained by focusing on relevant\nfeatures from visual inputs. Furthermore, the output attention maps enhance the\ninterpretability of the model not only by providing meaningful explanation to\nthe model behavior but also by identifying the weakness of and potential\nimprovement directions for the model.",
    "descriptor": "\nComments: Under review for presentation at TRB 2022 Annual Meeting\n",
    "authors": [
      "Sikai Chen",
      "Jiqian Dong",
      "Runjia Du",
      "Yujie Li",
      "Samuel Labi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07380"
  },
  {
    "id": "arXiv:2110.07382",
    "title": "Transferring Semantic Knowledge Into Language Encoders",
    "abstract": "We introduce semantic form mid-tuning, an approach for transferring semantic\nknowledge from semantic meaning representations into transformer-based language\nencoders. In mid-tuning, we learn to align the text of general sentences -- not\ntied to any particular inference task -- and structured semantic\nrepresentations of those sentences. Our approach does not require gold\nannotated semantic representations. Instead, it makes use of automatically\ngenerated semantic representations, such as from off-the-shelf PropBank and\nFrameNet semantic parsers. We show that this alignment can be learned\nimplicitly via classification or directly via triplet loss. Our method yields\nlanguage encoders that demonstrate improved predictive performance across\ninference, reading comprehension, textual similarity, and other semantic tasks\ndrawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our\napproach on three popular baseline models, where our experimental results and\nanalysis concludes that current pre-trained language models can further benefit\nfrom structured semantic frames with the proposed mid-tuning method, as they\ninject additional task-agnostic knowledge to the encoder, improving the\ngenerated embeddings as well as the linguistic properties of the given model,\nas evident from improvements on a popular sentence embedding toolkit and a\nvariety of probing tasks.",
    "descriptor": "",
    "authors": [
      "Mohammad Umair",
      "Francis Ferraro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07382"
  },
  {
    "id": "arXiv:2110.07383",
    "title": "The Neglected Sibling: Isotropic Gaussian Posterior for VAE",
    "abstract": "Deep generative models have been widely used in several areas of NLP, and\nvarious techniques have been proposed to augment them or address their training\nchallenges. In this paper, we propose a simple modification to Variational\nAutoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows\nfor better utilisation of their latent representation space. This model avoids\nthe sub-optimal behavior of VAEs related to inactive dimensions in the\nrepresentation space. We provide both theoretical analysis, and empirical\nevidence on various datasets and tasks that show IGP leads to consistent\nimprovement on several quantitative and qualitative grounds, from downstream\ntask performance and sample efficiency to robustness. Additionally, we give\ninsights about the representational properties encouraged by IGP and also show\nthat its gain generalises to image domain as well.",
    "descriptor": "",
    "authors": [
      "Lan Zhang",
      "Wray Buntine",
      "Ehsan Shareghi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07383"
  },
  {
    "id": "arXiv:2110.07385",
    "title": "Few-shot Controllable Style Transfer for Low-Resource Settings: A Study  in Indian Languages",
    "abstract": "Style transfer is the task of rewriting an input sentence into a target style\nwhile approximately preserving its content. While most prior literature assumes\naccess to large style-labelled corpora, recent work (Riley et al. 2021) has\nattempted \"few-shot\" style transfer using only 3-10 sentences at inference for\nextracting the target style. In this work we consider one such low resource\nsetting where no datasets are available: style transfer for Indian languages.\nWe find that existing few-shot methods perform this task poorly, with a strong\ntendency to copy inputs verbatim. We push the state-of-the-art for few-shot\nstyle transfer with a new method modeling the stylistic difference between\nparaphrases. When compared to prior work using automatic and human evaluations,\nour model achieves 2-3x better performance and output diversity in formality\ntransfer and code-mixing addition across five Indian languages. Moreover, our\nmethod is better able to control the amount of style transfer using an input\nscalar knob. We report promising qualitative results for several attribute\ntransfer directions, including sentiment transfer, text simplification, gender\nneutralization and text anonymization, all without retraining the model.\nFinally we found model evaluation to be difficult due to the lack of evaluation\ndatasets and metrics for Indian languages. To facilitate further research in\nformality transfer for Indic languages, we crowdsource annotations for 4000\nsentence pairs in four languages, and use this dataset to design our automatic\nevaluation suite.",
    "descriptor": "\nComments: preprint, 30 pages\n",
    "authors": [
      "Kalpesh Krishna",
      "Deepak Nathani",
      "Xavier Garcia",
      "Bidisha Samanta",
      "Partha Talukdar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07385"
  },
  {
    "id": "arXiv:2110.07391",
    "title": "Distribution Locational Marginal Pricing Under Uncertainty Considering  Coordination of Distribution and Wholesale Markets",
    "abstract": "An effective distribution electricity market (DEM) is required to manage the\nrapidly growing small-scale distributed energy resources (DERs) in distribution\nsystems (DSs). This paper proposes a day-ahead DEM clearing and pricing\nmechanism to account for the uncertainty of DERs and the coordination with the\nwholesale electricity market (WEM) through a bi-level model. The upper-level\nmodel clears the WEM in the transmission system (TS) and forms the locational\nmarginal price (LMP) and uncertainty LMP (ULMP) for energy and\nuncertainty/reserve, respectively. In the lower level, a robust scheduling\nmodel considering WEM-DEM coordination and uncertainties is proposed to clear\nthe DEM. Accordingly, the distribution LMPs (DLMPs) for active power, reactive\npower and uncertainty/reserve are derived to reward the energy/reserve\nprovision and charge uncertain resources in the DEM, which provide effective\nprice signals for managing not only the voltage and congestion, but also the\nuncertainty in DSs. A heterogeneous decomposition (HGD) algorithm is utilized\nto solve the bi-level model in a decentralized manner with limited information\ninteraction between TS and DSs, which guarantees the solution efficiency and\ninformation privacy. The effectiveness of the proposed method is verified via\nnumerous case studies.",
    "descriptor": "",
    "authors": [
      "Zongzheng Zhao",
      "Yixin Liu",
      "Li Guo",
      "Linquan Bai",
      "Chengshan Wang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07391"
  },
  {
    "id": "arXiv:2110.07392",
    "title": "Provably Efficient Multi-Agent Reinforcement Learning with Fully  Decentralized Communication",
    "abstract": "A challenge in reinforcement learning (RL) is minimizing the cost of sampling\nassociated with exploration. Distributed exploration reduces sampling\ncomplexity in multi-agent RL (MARL). We investigate the benefits to performance\nin MARL when exploration is fully decentralized. Specifically, we consider a\nclass of online, episodic, tabular $Q$-learning problems under time-varying\nreward and transition dynamics, in which agents can communicate in a\ndecentralized manner.We show that group performance, as measured by the bound\non regret, can be significantly improved through communication when each agent\nuses a decentralized message-passing protocol, even when limited to sending\ninformation up to its $\\gamma$-hop neighbors. We prove regret and sample\ncomplexity bounds that depend on the number of agents, communication network\nstructure and $\\gamma.$ We show that incorporating more agents and more\ninformation sharing into the group learning scheme speeds up convergence to the\noptimal policy. Numerical simulations illustrate our results and validate our\ntheoretical claims.",
    "descriptor": "",
    "authors": [
      "Justin Lidard",
      "Udari Madhushani",
      "Naomi Ehrich Leonard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07392"
  },
  {
    "id": "arXiv:2110.07393",
    "title": "M2MeT: The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription  Challenge",
    "abstract": "Recent development of speech signal processing, such as speech recognition,\nspeaker diarization, etc., has inspired numerous applications of speech\ntechnologies. The meeting scenario is one of the most valuable and, at the same\ntime, most challenging scenarios for speech technologies. Speaker diarization\nand multi-speaker automatic speech recognition in meeting scenarios have\nattracted increasing attention. However, the lack of large public real meeting\ndata has been a major obstacle for advancement of the field. Therefore, we\nrelease the \\emph{AliMeeting} corpus, which consists of 120 hours of real\nrecorded Mandarin meeting data, including far-field data collected by 8-channel\nmicrophone array as well as near-field data collected by each participants'\nheadset microphone. Moreover, we will launch the Multi-channel Multi-party\nMeeting Transcription Challenge (M2MeT), as an ICASSP2022 Signal Processing\nGrand Challenge. The challenge consists of two tracks, namely speaker\ndiarization and multi-speaker ASR. In this paper we provide a detailed\nintroduction of the dateset, rules, evaluation methods and baseline systems,\naiming to further promote reproducible research in this field.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Fan Yu",
      "Shiliang Zhang",
      "Yihui Fu",
      "Lei Xie",
      "Siqi Zheng",
      "Zhihao Du",
      "Weilong Huang",
      "Pengcheng Guo",
      "Zhijie Yan",
      "Bin Ma",
      "Xin Xu",
      "Hui Bu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07393"
  },
  {
    "id": "arXiv:2110.07395",
    "title": "Offline Reinforcement Learning with Soft Behavior Regularization",
    "abstract": "Most prior approaches to offline reinforcement learning (RL) utilize\n\\textit{behavior regularization}, typically augmenting existing off-policy\nactor critic algorithms with a penalty measuring divergence between the policy\nand the offline data. However, these approaches lack guaranteed performance\nimprovement over the behavior policy. In this work, we start from the\nperformance difference between the learned policy and the behavior policy, we\nderive a new policy learning objective that can be used in the offline setting,\nwhich corresponds to the advantage function value of the behavior policy,\nmultiplying by a state-marginal density ratio. We propose a practical way to\ncompute the density ratio and demonstrate its equivalence to a state-dependent\nbehavior regularization. Unlike state-independent regularization used in prior\napproaches, this \\textit{soft} regularization allows more freedom of policy\ndeviation at high confidence states, leading to better performance and\nstability. We thus term our resulting algorithm Soft Behavior-regularized Actor\nCritic (SBAC). Our experimental results show that SBAC matches or outperforms\nthe state-of-the-art on a set of continuous control locomotion and manipulation\ntasks.",
    "descriptor": "",
    "authors": [
      "Haoran Xu",
      "Xianyuan Zhan",
      "Jianxiong Li",
      "Honglei Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07395"
  },
  {
    "id": "arXiv:2110.07398",
    "title": "The Time Domain Linear Sampling Method for Determining the Shape of a  Scatterer using Electromagnetic Waves",
    "abstract": "The time domain linear sampling method (TD-LSM) solves inverse scattering\nproblems using time domain data by creating an indicator function for the\nsupport of the unknown scatterer. It involves only solving a linear integral\nequation called the near-field equation using different data from sampling\npoints that probe the domain where the scatterer is located. To date, the\nmethod has been used for the acoustic wave equation and has been tested for\nseveral different types of scatterers, i.e. sound hard, impedance, and\npenetrable, and for wave-guides. In this paper, we extend the TD-LSM to the\ntime dependent Maxwell's system with impedance boundary conditions - a similar\nanalysis handles the case of a perfectly electrically conducting (PEC) body. We\nprovide an analysis that supports the use of the TD-LSM for this problem, and\npreliminary numerical tests of the algorithm. Our analysis relies on the\nLaplace transform approach previously used for the acoustic wave equation. This\nis the first application of the TD-LSM in electromagnetism.",
    "descriptor": "",
    "authors": [
      "Timo L\u00e4hivaara",
      "Peter Monk",
      "Virginia Selgas"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07398"
  },
  {
    "id": "arXiv:2110.07399",
    "title": "Soft robotic shell with active thermal display",
    "abstract": "Almost all robotic systems in use have hard shells, which is limiting in many\nways their full potential of physical interaction with humans or their\nsurrounding environment. Robots with soft-shell covers offer an alternative\nmorphology which is more pleasant in both appearance and for haptic human\ninteraction. A persisting challenge in such soft-shell robotic covers is the\nsimultaneous realization of softness and heat-conducting properties. Such\nheat-conducting properties are important for enabling temperature-control of\nrobotic covers in the range that is comfortable for human touch. The presented\nsoft-shell robotic cover is composed of a linked two-layer structure: (i) the\ninner layer, with built-in pipes for water circulation, is soft and acts as a\nthermal-isolation layer between the cover and the robot structure, whereas (ii)\nthe outer layer, which can be patterned with a given desired texture and color,\nallows heat transfer from the circulating water of the inner part to the\nsurface. Moreover, we demonstrate the ability to integrate our prototype cover\nwith a humanoid robot equipped with capacitance sensors. This fabrication\ntechnique enables robotic cover possibilities, including tunable color, surface\ntexture, and size, that are likely to have applications in a variety of robotic\nsystems.",
    "descriptor": "",
    "authors": [
      "Yukiko Osawa",
      "Yuho Kinbara",
      "Masakazu Kageoka",
      "Kenji Iida",
      "Abderrahmane Kheddar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07399"
  },
  {
    "id": "arXiv:2110.07402",
    "title": "Self-Supervised Learning by Estimating Twin Class Distributions",
    "abstract": "We present TWIST, a novel self-supervised representation learning method by\nclassifying large-scale unlabeled datasets in an end-to-end way. We employ a\nsiamese network terminated by a softmax operation to produce twin class\ndistributions of two augmented images. Without supervision, we enforce the\nclass distributions of different augmentations to be consistent. In the\nmeantime, we regularize the class distributions to make them sharp and diverse.\nSpecifically, we minimize the entropy of the distribution for each sample to\nmake the class prediction for each sample assertive and maximize the entropy of\nthe mean distribution to make the predictions of different samples diverse. In\nthis way, TWIST can naturally avoid the trivial solutions without specific\ndesigns such as asymmetric network, stop-gradient operation, or momentum\nencoder. Different from the clustering-based methods which alternate between\nclustering and learning, our method is a single learning process guided by a\nunified loss function. As a result, TWIST outperforms state-of-the-art methods\non a wide range of tasks, including unsupervised classification, linear\nclassification, semi-supervised learning, transfer learning, and some dense\nprediction tasks such as detection and segmentation.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Feng Wang",
      "Tao Kong",
      "Rufeng Zhang",
      "Huaping Liu",
      "Hang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07402"
  },
  {
    "id": "arXiv:2110.07404",
    "title": "Socially assistive robots' deployment in healthcare settings: a global  perspective",
    "abstract": "One of the major areas where social robots are finding their place in society\nis for healthcare-related applications. Yet, very little research has mapped\nthe deployment of socially assistive robots (SARs) in real settings. Using a\ndocumentary research method, we were able to trace back 279 experiences of SARs\ndeployments in hospitals, elderly care centers, occupational health centers,\nprivate homes, and educational institutions worldwide from 33 different\ncountries, and involving 52 different robot models. We retrieved, analyzed, and\nclassified the functions that SARs develop in these experiences, the areas in\nwhich they are deployed, the principal manufacturers, and the robot models that\nare being adopted. The functions we identified for SARs are entertainment,\ncompanionship, telepresence, edutainment, providing general and personalized\ninformation or advice, monitoring, promotion of physical exercise and\nrehabilitation, testing and pre-diagnosis, delivering supplies, patient\nregistration, giving location indications, patient simulator, protective\nmeasure enforcement, medication and well-being adherence, translating and\nhaving conversations in multiple languages, psychological therapy, patrolling,\ninteracting with digital devices, and disinfection. Our work provides an\nin-depth picture of the current state of the art of SARs' deployment in real\nscenarios for healthcare-related applications and contributes to understanding\nbetter the role of these machines in the healthcare sector.",
    "descriptor": "\nComments: 28 pages, 10 figures\n",
    "authors": [
      "Laura Aymerich-Franch",
      "Iliana Ferrer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07404"
  },
  {
    "id": "arXiv:2110.07406",
    "title": "A Data-driven Probabilistic-based Flexibility Region Estimation Method  for Aggregated Distributed Energy Resources",
    "abstract": "This paper presents a data-driven, distributionally robust chance-constrained\noptimization method for estimating the real and reactive power controllability\nof aggregated distributed energy resources (DER). At the DER-level, a\ntwo-dimensional flexibility region can be formed based on the real and reactive\npower regulating limits of each DER considering forecast uncertainty. At the\nfeeder-level, an aggregated flexibility region is computed via a\nmulti-directional search method. In each search direction, extend the real and\nreactive power of each controllable DER towards its operational limits until:\ni) all DERs' maximum operational limits are reached or, ii) one or more of the\ndistribution network operational limits are violated. The method enables three\nkey features for operating aggregated DER resources: controllability\nestimation, visualization, and risk quantification. Simulation results\ndemonstrated the effectiveness of the algorithm and quantified the impact of\ndifferent parameter settings on the flexibility region boundary changes. The\nproposed algorithm is robust and computationally efficient and can meet\nreal-time computing needs.",
    "descriptor": "",
    "authors": [
      "Mingzhi Zhang",
      "Xiangqi Zhu",
      "Ning Lu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07406"
  },
  {
    "id": "arXiv:2110.07410",
    "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models  for Automated Audio Captioning",
    "abstract": "Automated audio captioning (AAC) is the task of automatically generating\ntextual descriptions for general audio signals. A captioning system has to\nidentify various information from the input signal and express it with natural\nlanguage. Existing works mainly focus on investigating new methods and try to\nimprove their performance measured on existing datasets. Having attracted\nattention only recently, very few works on AAC study the performance of\nexisting pre-trained audio and natural language processing resources. In this\npaper, we evaluate the performance of off-the-shelf models with a\nTransformer-based captioning approach. We utilize the freely available Clotho\ndataset to compare four different pre-trained machine listening models, four\nword embedding models, and their combinations in many different settings. Our\nevaluation suggests that YAMNet combined with BERT embeddings produces the best\ncaptions. Moreover, in general, fine-tuning pre-trained word embeddings can\nlead to better performance. Finally, we show that sequences of audio embeddings\ncan be processed using a Transformer encoder to produce higher-quality\ncaptions.",
    "descriptor": "\nComments: 5 pages, 4 figures. Accepted at Detection and Classification of Acoustic Scenes and Events 2021 (DCASE2021)\n",
    "authors": [
      "Benno Weck",
      "Xavier Favory",
      "Konstantinos Drossos",
      "Xavier Serra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07410"
  },
  {
    "id": "arXiv:2110.07413",
    "title": "RGB-D Image Inpainting Using Generative Adversarial Network with a Late  Fusion Approach",
    "abstract": "Diminished reality is a technology that aims to remove objects from video\nimages and fills in the missing region with plausible pixels. Most conventional\nmethods utilize the different cameras that capture the same scene from\ndifferent viewpoints to allow regions to be removed and restored. In this\npaper, we propose an RGB-D image inpainting method using generative adversarial\nnetwork, which does not require multiple cameras. Recently, an RGB image\ninpainting method has achieved outstanding results by employing a generative\nadversarial network. However, RGB inpainting methods aim to restore only the\ntexture of the missing region and, therefore, does not recover geometric\ninformation (i.e, 3D structure of the scene). We expand conventional image\ninpainting method to RGB-D image inpainting to jointly restore the texture and\ngeometry of missing regions from a pair of RGB and depth images. Inspired by\nother tasks that use RGB and depth images (e.g., semantic segmentation and\nobject detection), we propose late fusion approach that exploits the advantage\nof RGB and depth information each other. The experimental results verify the\neffectiveness of our proposed method.",
    "descriptor": "\nComments: Accepted at AVR 2020\n",
    "authors": [
      "Ryo Fujii",
      "Ryo Hachiuma",
      "Hideo Saito"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07413"
  },
  {
    "id": "arXiv:2110.07415",
    "title": "A Simple, Strong and Robust Baseline for Distantly Supervised Relation  Extraction",
    "abstract": "Distantly supervised relation extraction (DS-RE) is generally framed as a\nmulti-instance multi-label (MI-ML) task, where the optimal aggregation of\ninformation from multiple instances is of key importance. Intra-bag attention\n(Lin et al., 2016) is an example of a popularly used aggregation scheme for\nthis framework. Apart from this scheme, however, there is not much to choose\nfrom in the DS-RE literature as most of the advances in this field are focused\non improving the instance-encoding step rather than the instance-aggregation\nstep. With recent works leveraging large pre-trained language models as\nencoders, the increased capacity of models might allow for more flexibility in\nthe instance-aggregation step. In this work, we explore this hypothesis and\ncome up with a novel aggregation scheme which we call Passage-Att. Under this\naggregation scheme, we combine all instances mentioning an entity pair into a\n\"passage of instances\", which is summarized independently for each relation\nclass. These summaries are used to predict the validity of a potential triple.\nWe show that our Passage-Att with BERT as passage encoder achieves\nstate-of-the-art performance in three different settings (monolingual DS,\nmonolingual DS with manually-annotated test set, multilingual DS).",
    "descriptor": "",
    "authors": [
      "Vipul Rathore",
      "Kartikeya Badola",
      "Mausam",
      "Parag Singla"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07415"
  },
  {
    "id": "arXiv:2110.07420",
    "title": "Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal  Frames",
    "abstract": "Social concepts referring to non-physical objects--such as revolution,\nviolence, or friendship--are powerful tools to describe, index, and query the\ncontent of visual data, including ever-growing collections of art images from\nthe Cultural Heritage (CH) field. While much progress has been made towards\ncomplete image understanding in computer vision, automatic detection of social\nconcepts evoked by images is still a challenge. This is partly due to the\nwell-known semantic gap problem, worsened for social concepts given their lack\nof unique physical features, and reliance on more unspecific features than\nconcrete concepts. In this paper, we propose the translation of recent\ncognitive theories about social concept representation into a software approach\nto represent them as multimodal frames, by integrating multisensory data. Our\nmethod focuses on the extraction, analysis, and integration of multimodal\nfeatures from visual art material tagged with the concepts of interest. We\ndefine a conceptual model and present a novel ontology for formally\nrepresenting social concepts as multimodal frames. Taking the Tate Gallery's\ncollection as an empirical basis, we experiment our method on a corpus of art\nimages to provide a proof of concept of its potential. We discuss further\ndirections of research, and provide all software, data sources, and results.",
    "descriptor": "\nComments: First International Workshop on Multisensory Data and Knowledge at the 3rd Conference on Language, Data and Knowledge (2021)\n",
    "authors": [
      "Delfina Sol Martinez Pandiani",
      "Valentina Presutti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07420"
  },
  {
    "id": "arXiv:2110.07421",
    "title": "On some batch code properties of the simplex code",
    "abstract": "The binary $k$-dimensional simplex code is known to be a $2^{k-1}$-batch code\nand is conjectured to be a $2^{k-1}$-functional batch code. Here, we offer a\nsimple, constructive proof of a result that is \"in between\" these two\nproperties. Our approach is to relate these properties to certain (old and new)\nadditive problems in finite abelian groups. We also formulate a conjecture for\nfinite abelian groups that generalizes the above-mentioned conjecture.",
    "descriptor": "",
    "authors": [
      "Henk D.L. Hollmann",
      "Karan Khathuria",
      "Ago-Erik Riet",
      "Vitaly Skachek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.07421"
  },
  {
    "id": "arXiv:2110.07424",
    "title": "Towards a modern CMake workflow",
    "abstract": "Modern CMake offers the features to manage versatile and complex projects\nwith ease. With respect to OMNeT++ projects, a workflow relying on CMake\nenables projects to combine discrete event simulation and production code in a\ncommon development environment. Such a combination means less maintenance\neffort and thus potentially more sustainable and long-living software. This\npaper highlights the significant improvements since the first attempt of using\nCMake in OMNeT++ projects. In particular, a state-of-the-art integration of\nOMNeT++ in Visual Studio Code including support for debugging and\nmulti-platform compilation is presented. Last but not least, an exemplary use\ncase demonstrates the powerful mix of production and simulation code in a\ncommon software architecture supported by the OMNeT++ CMake package.",
    "descriptor": "\nComments: Published in: M. Marek, G. Nardini, V. Vesely (Eds.), Proceedings of the 8th OMNeT++ Community Summit, Virtual Summit, September 8-10, 2021\n",
    "authors": [
      "Heinz-Peter Liechtenecker",
      "Raphael Riebl"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.07424"
  },
  {
    "id": "arXiv:2110.07426",
    "title": "Fundamental Limits of Combinatorial Multi-Access Caching",
    "abstract": "This work identifies the fundamental limits of Multi-Access Coded Caching\n(MACC) where each user is connected to multiple caches in a manner that follows\na generalized combinatorial topology. This topology stands out as it allows for\nunprecedented coding gains, even with very modest cache resources. First, we\nextend the setting and the scheme presented by Muralidhar et al. to a much more\ngeneral topology that supports both a much denser range of users and the\ncoexistence of users connected to different numbers of caches, all while\nmaintaining the astounding coding gains, here proven to be exactly optimal,\nassociated with the combinatorial topology. This is achieved, for this\ngeneralized topology, with a novel information-theoretic converse that we\npresent here, which establishes, together with the scheme, the exact optimal\nperformance under the assumption of uncoded placement. We subsequently consider\ndifferent connectivity ensembles, including the very general scenario of the\nentire ensemble of all possible network connectivities/topologies, where any\nuser can be connected to any subset of caches arbitrarily. For these settings,\nwe develop novel converse bounds on the optimal performance averaged over the\nensemble's different connectivities, considering the additional realistic\nscenario that the connectivity at delivery time is entirely unknown during\ncache placement. This novel analysis of topological ensembles leaves open the\npossibility that currently-unknown topologies may yield even higher gains, a\nhypothesis that is part of the bigger question of which network topology yields\nthe most caching gains.",
    "descriptor": "\nComments: 48 pages, 2 figures. Submitted to IEEE Transactions on Information Theory\n",
    "authors": [
      "Federico Brunero",
      "Petros Elia"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07426"
  },
  {
    "id": "arXiv:2110.07428",
    "title": "Understanding Model Robustness to User-generated Noisy Texts",
    "abstract": "Sensitivity of deep-neural models to input noise is known to be a challenging\nproblem. In NLP, model performance often deteriorates with naturally occurring\nnoise, such as spelling errors. To mitigate this issue, models may leverage\nartificially noised data. However, the amount and type of generated noise has\nso far been determined arbitrarily. We therefore propose to model the errors\nstatistically from grammatical-error-correction corpora. We present a thorough\nevaluation of several state-of-the-art NLP systems' robustness in multiple\nlanguages, with tasks including morpho-syntactic analysis, named entity\nrecognition, neural machine translation, a subset of the GLUE benchmark and\nreading comprehension. We also compare two approaches to address the\nperformance drop: a) training the NLP models with noised data generated by our\nframework; and b) reducing the input noise with external system for natural\nlanguage correction. The code is released at https://github.com/ufal/kazitext.",
    "descriptor": "\nComments: Accepted to W-NUT 2021\n",
    "authors": [
      "Jakub N\u00e1plava",
      "Martin Popel",
      "Milan Straka",
      "Jana Strakov\u00e1"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07428"
  },
  {
    "id": "arXiv:2110.07430",
    "title": "Detecting Renewal States in Chains of Variable Length via Intrinsic  Bayes Factors",
    "abstract": "Markov chains with variable length are useful parsimonious stochastic models\nable to generate most stationary sequence of discrete symbols. The idea is to\nidentify the suffixes of the past, called contexts, that are relevant to\npredict the future symbol. Sometimes a single state is a context, and looking\nat the past and finding this specific state makes the further past irrelevant.\nThese states are called renewal states and they split the chain into\nindependent blocks. In order to identify renewal states for chains with\nvariable length, we propose the use of Intrinsic Bayes Factor to evaluate the\nplausibility of each set of renewal states. In this case, the difficulty lies\nin finding the marginal posterior distribution for the random context trees for\ngeneral prior distribution on the space of context trees and Dirichlet prior\nfor the transition probabilities. To show the strength of our method, we\nanalyzed artificial datasets generated from two binary models models and one\nexample coming from the field of Linguistics.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Victor Freguglia",
      "Nancy Garcia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2110.07430"
  },
  {
    "id": "arXiv:2110.07431",
    "title": "Towards More Effective and Economic Sparsely-Activated Model",
    "abstract": "The sparsely-activated models have achieved great success in natural language\nprocessing through large-scale parameters and relatively low computational\ncost, and gradually become a feasible technique for training and implementing\nextremely large models. Due to the limit of communication cost, activating\nmultiple experts is hardly affordable during training and inference. Therefore,\nprevious work usually activate just one expert at a time to alleviate\nadditional communication cost. Such routing mechanism limits the upper bound of\nmodel performance. In this paper, we first investigate a phenomenon that\nincreasing the number of activated experts can boost the model performance with\nhigher sparse ratio. To increase the number of activated experts without an\nincrease in computational cost, we propose SAM (Switch and Mixture) routing, an\nefficient hierarchical routing mechanism that activates multiple experts in a\nsame device (GPU). Our methods shed light on the training of extremely large\nsparse models and experiments prove that our models can achieve significant\nperformance gain with great efficiency improvement.",
    "descriptor": "",
    "authors": [
      "Hao Jiang",
      "Ke Zhan",
      "Jianwei Qu",
      "Yongkang Wu",
      "Zhaoye Fei",
      "Xinyu Zhang",
      "Lei Chen",
      "Zhicheng Dou",
      "Xipeng Qiu",
      "Zikai Guo",
      "Ruofei Lai",
      "Jiawen Wu",
      "Enrui Hu",
      "Yinxia Zhang",
      "Yantao Jia",
      "Fan Yu",
      "Zhao Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07431"
  },
  {
    "id": "arXiv:2110.07433",
    "title": "Possibilistic Fuzzy Local Information C-Means with Automated Feature  Selection for Seafloor Segmentation",
    "abstract": "The Possibilistic Fuzzy Local Information C-Means (PFLICM) method is\npresented as a technique to segment side-look synthetic aperture sonar (SAS)\nimagery into distinct regions of the sea-floor. In this work, we investigate\nand present the results of an automated feature selection approach for SAS\nimage segmentation. The chosen features and resulting segmentation from the\nimage will be assessed based on a select quantitative clustering validity\ncriterion and the subset of the features that reach a desired threshold will be\nused for the segmentation process.",
    "descriptor": "\nComments: Proc. SPIE 10628, Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XXIII (30 April 2018), 14 pages, 7 figures, 5 tables\n",
    "authors": [
      "Joshua Peeples",
      "Daniel Suen",
      "Alina Zare",
      "James Keller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07433"
  },
  {
    "id": "arXiv:2110.07435",
    "title": "Adaptive Differentially Private Empirical Risk Minimization",
    "abstract": "We propose an adaptive (stochastic) gradient perturbation method for\ndifferentially private empirical risk minimization. At each iteration, the\nrandom noise added to the gradient is optimally adapted to the stepsize; we\nname this process adaptive differentially private (ADP) learning. Given the\nsame privacy budget, we prove that the ADP method considerably improves the\nutility guarantee compared to the standard differentially private method in\nwhich vanilla random noise is added. Our method is particularly useful for\ngradient-based algorithms with time-varying learning rates, including variants\nof AdaGrad (Duchi et al., 2011). We provide extensive numerical experiments to\ndemonstrate the effectiveness of the proposed adaptive differentially private\nalgorithm.",
    "descriptor": "",
    "authors": [
      "Xiaoxia Wu",
      "Lingxiao Wang",
      "Irina Cristali",
      "Quanquan Gu",
      "Rebecca Willett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07435"
  },
  {
    "id": "arXiv:2110.07436",
    "title": "Asymmetric Graph Representation Learning",
    "abstract": "Despite the enormous success of graph neural networks (GNNs), most existing\nGNNs can only be applicable to undirected graphs where relationships among\nconnected nodes are two-way symmetric (i.e., information can be passed back and\nforth). However, there is a vast amount of applications where the information\nflow is asymmetric, leading to directed graphs where information can only be\npassed in one direction. For example, a directed edge indicates that the\ninformation can only be conveyed forwardly from the start node to the end node,\nbut not backwardly. To accommodate such an asymmetric structure of directed\ngraphs within the framework of GNNs, we propose a simple yet remarkably\neffective framework for directed graph analysis to incorporate such one-way\ninformation passing. We define an incoming embedding and an outgoing embedding\nfor each node to model its sending and receiving features respectively. We\nfurther develop two steps in our directed GNN model with the first one to\naggregate/update the incoming features of nodes and the second one to\naggregate/update the outgoing features. By imposing the two roles for each\nnode, the likelihood of a directed edge can be calculated based on the outgoing\nembedding of the start node and the incoming embedding of the end node. The\nlog-likelihood of all edges plays a natural role of regularization for the\nproposed model, which can alleviate the over-smoothing problem of the deep\nGNNs. Extensive experiments on multiple real-world directed graphs demonstrate\noutstanding performances of the proposed model in both node-level and\ngraph-level tasks.",
    "descriptor": "",
    "authors": [
      "Zhuo Tan",
      "Bin Liu",
      "Guosheng Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07436"
  },
  {
    "id": "arXiv:2110.07437",
    "title": "Non-Invasive Fault Detection of Stator Windings of Induction Motors",
    "abstract": "Condition monitoring of induction motor has been widely researched over\nrecent years due to its ability to monitor operating characteristics and the\nhealth status of induction motor. Various methods have been used to monitor\ninduction motors such as thermal monitoring and vibration analysis. This paper\nintroduces an alternative method which is to use an inductive coupling method\nto extract in-circuit impedance of induction motor. This method allows for an\nonline measurement of the system under test (SUT) preventing any unnecessary\nshutdowns. These unnecessary shutdowns may incur a loss of revenue for relevant\nindustries that depends heavily on induction motor operations. Two sets of\nexperiments were conducted in this paper, the first experiment was to examine\nthe accuracy of the proposed method by simulating various SUT using simulated\nresistor, inductor, and capacitor (RLC) network. The next experiment was to\ndetect incipient stator faults such as turn to turn faults in an induction\nmotor. This proposed method measures the impedance of the stator winding of an\ninduction motor to identify abnormalities. In addition, this method allows for\nan accurate in-circuit impedance measurement without the influence of motor\nload and frequency changes as well as faults such as bearing and rotor faults.",
    "descriptor": "\nComments: This paper has been submitted to an international conference\n",
    "authors": [
      "Rayyan Bin Fairuz"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07437"
  },
  {
    "id": "arXiv:2110.07439",
    "title": "Inverse Problems Leveraging Pre-trained Contrastive Representations",
    "abstract": "We study a new family of inverse problems for recovering representations of\ncorrupted data. We assume access to a pre-trained representation learning\nnetwork R(x) that operates on clean images, like CLIP. The problem is to\nrecover the representation of an image R(x), if we are only given a corrupted\nversion A(x), for some known forward operator A. We propose a supervised\ninversion method that uses a contrastive objective to obtain excellent\nrepresentations for highly corrupted images. Using a linear probe on our robust\nrepresentations, we achieve a higher accuracy than end-to-end supervised\nbaselines when classifying images with various types of distortions, including\nblurring, additive noise, and random pixel masking. We evaluate on a subset of\nImageNet and observe that our method is robust to varying levels of distortion.\nOur method outperforms end-to-end baselines even with a fraction of the labeled\ndata in a wide range of forward operators.",
    "descriptor": "\nComments: Initial version. Final version to appear in Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021)\n",
    "authors": [
      "Sriram Ravula",
      "Georgios Smyrnis",
      "Matt Jordan",
      "Alexandros G. Dimakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07439"
  },
  {
    "id": "arXiv:2110.07443",
    "title": "DeepOrder: Deep Learning for Test Case Prioritization in Continuous  Integration Testing",
    "abstract": "Continuous integration testing is an important step in the modern software\nengineering life cycle. Test prioritization is a method that can improve the\nefficiency of continuous integration testing by selecting test cases that can\ndetect faults in the early stage of each cycle. As continuous integration\ntesting produces voluminous test execution data, test history is a commonly\nused artifact in test prioritization. However, existing test prioritization\ntechniques for continuous integration either cannot handle large test history\nor are optimized for using a limited number of historical test cycles. We show\nthat such a limitation can decrease fault detection effectiveness of\nprioritized test suites.\nThis work introduces DeepOrder, a deep learning-based model that works on the\nbasis of regression machine learning. DeepOrder ranks test cases based on the\nhistorical record of test executions from any number of previous test cycles.\nDeepOrder learns failed test cases based on multiple factors including the\nduration and execution status of test cases. We experimentally show that deep\nneural networks, as a simple regression model, can be efficiently used for test\ncase prioritization in continuous integration testing. DeepOrder is evaluated\nwith respect to time-effectiveness and fault detection effectiveness in\ncomparison with an industry practice and the state of the art approaches. The\nresults show that DeepOrder outperforms the industry practice and\nstate-of-the-art test prioritization approaches in terms of these two metrics.",
    "descriptor": "\nComments: 10 pages, 9 figures, conference paper\n",
    "authors": [
      "Aizaz Sharif",
      "Dusica Marijan",
      "Marius Liaaen"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07443"
  },
  {
    "id": "arXiv:2110.07444",
    "title": "Designing Language Technologies for Social Good: The Road not Taken",
    "abstract": "Development of speech and language technology for social good (LT4SG),\nespecially those targeted at the welfare of marginalized communities and\nspeakers of low-resource and under-served languages, has been a prominent theme\nof research within NLP, Speech, and the AI communities. Researchers have mostly\nrelied on their individual expertise, experiences or ad hoc surveys for\nprioritization of language technologies that provide social good to the\nend-users. This has been criticized by several scholars who argue that work on\nLT4SG must include the target linguistic communities during the design and\ndevelopment process. However, none of the LT4SG work and their critiques\nsuggest principled techniques for prioritization of the technologies and\nmethods for inclusion of the end-user during the development cycle. Drawing\ninspiration from the fields of Economics, Ethics, Psychology, and Participatory\nDesign, here we chart out a set of methodologies for prioritizing LT4SG that\nare aligned with the end-user preferences. We then analyze several LT4SG\nefforts in light of the proposed methodologies and bring out their hidden\nassumptions and potential pitfalls. While the current study is limited to\nlanguage technologies, we believe that the principles and prioritization\ntechniques highlighted here are applicable more broadly to AI for Social Good.",
    "descriptor": "",
    "authors": [
      "Namrata Mukhija",
      "Monojit Choudhury",
      "Kalika Bali"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07444"
  },
  {
    "id": "arXiv:2110.07448",
    "title": "Human-Robot Collaboration and Machine Learning: A Systematic Review of  Recent Research",
    "abstract": "Technological progress increasingly envisions the use of robots interacting\nwith people in everyday life. Human-robot collaboration (HRC) is the approach\nthat explores the interaction between a human and a robot, during the\ncompletion of an actual physical task. Such interplay is explored both at the\ncognitive and physical level, by respectively analysing the mutual exchange of\ninformation and mechanical power. In HRC works, a cognitive model is typically\nbuilt, which collects inputs from the environment and from the user, elaborates\nand translates these into information that can be used by the robot itself. HRC\nstudies progressively employ machine learning algorithms to build the cognitive\nmodels and behavioural block that elaborates the acquired external inputs. This\nis a promising approach still in its early stages and with the potential of\nsignificant benefit from the growing field of machine learning. Consequently,\nthis paper proposes a thorough literature review of the use of machine learning\ntechniques in the context of human-robot collaboration. The\ncollection,selection and analysis of the set of 45 key papers, selected from\nthe wide review of the literature on robotics and machine learning, allowed the\nidentification of the current trends in HRC. In particular, a clustering of\nworks based on the type of collaborative tasks, evaluation metrics and\ncognitive variables modelled is proposed. With these premises, a deep analysis\non different families of machine learning algorithms and their properties,\nalong with the sensing modalities used, was carried out. The salient aspects of\nthe analysis are discussed to show trends and suggest possible challenges to\ntackle in the future research.",
    "descriptor": "",
    "authors": [
      "Francesco Semeraro",
      "Alexander Griffiths",
      "Angelo Cangelosi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07448"
  },
  {
    "id": "arXiv:2110.07449",
    "title": "zk-Fabric, a Polylithic Syntax Zero Knowledge Joint Proof System",
    "abstract": "In this paper, we create a single-use and full syntax zero-knowledge proof\nsystem, a.k.a zk-Fabric. Comparing with zk-SNARKS and another variant\nzero-knowledge proofing system, zkBOO and it's variant zkBOO++. We present\nmultiple new approaches on how to use partitioned garbled circuits to achieve a\njoint zero-knowledge proof system, with the benefits of less overhead and full\nsyntax verification. zk-Fabric based on partitioned garbled circuits has the\nadvantage of being versatile and single-use, meaning it can be applied to\narbitrary circuits with more comprehensive statements, and it can achieve the\nnon-interactivity among all participants. One of the protocols proposed within\nis used for creating a new kind of partitioned garbled circuits to match the\ncomprehensive Boolean logical expression with multiple variables, we use the\nterm \"polythitic syntax\" to refer to the context-based multiple variables in a\ncomprehensive statement. We also designed a joint zero knowledge proof protocol\nthat uses partitioned garbled circuits",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Sheng Sun",
      "Dr. Tong Wen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.07449"
  },
  {
    "id": "arXiv:2110.07450",
    "title": "Bugs in our Pockets: The Risks of Client-Side Scanning",
    "abstract": "Our increasing reliance on digital technology for personal, economic, and\ngovernment affairs has made it essential to secure the communications and\ndevices of private citizens, businesses, and governments. This has led to\npervasive use of cryptography across society. Despite its evident advantages,\nlaw enforcement and national security agencies have argued that the spread of\ncryptography has hindered access to evidence and intelligence. Some in industry\nand government now advocate a new technology to access targeted data:\nclient-side scanning (CSS). Instead of weakening encryption or providing law\nenforcement with backdoor keys to decrypt communications, CSS would enable\non-device analysis of data in the clear. If targeted information were detected,\nits existence and, potentially, its source, would be revealed to the agencies;\notherwise, little or no information would leave the client device. Its\nproponents claim that CSS is a solution to the encryption versus public safety\ndebate: it offers privacy -- in the sense of unimpeded end-to-end encryption --\nand the ability to successfully investigate serious crime. In this report, we\nargue that CSS neither guarantees efficacious crime prevention nor prevents\nsurveillance. Indeed, the effect is the opposite. CSS by its nature creates\nserious security and privacy risks for all society while the assistance it can\nprovide for law enforcement is at best problematic. There are multiple ways in\nwhich client-side scanning can fail, can be evaded, and can be abused.",
    "descriptor": "\nComments: 46 pages, 3 figures\n",
    "authors": [
      "Hal Abelson",
      "Ross Anderson",
      "Steven M. Bellovin",
      "Josh Benaloh",
      "Matt Blaze",
      "Jon Callas",
      "Whitfield Diffie",
      "Susan Landau",
      "Peter G. Neumann",
      "Ronald L. Rivest",
      "Jeffrey I. Schiller",
      "Bruce Schneier",
      "Vanessa Teague",
      "Carmela Troncoso"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07450"
  },
  {
    "id": "arXiv:2110.07462",
    "title": "On Adversarial Vulnerability of PHM algorithms: An Initial Study",
    "abstract": "With proliferation of deep learning (DL) applications in diverse domains,\nvulnerability of DL models to adversarial attacks has become an increasingly\ninteresting research topic in the domains of Computer Vision (CV) and Natural\nLanguage Processing (NLP). DL has also been widely adopted to diverse PHM\napplications, where data are primarily time-series sensor measurements. While\nthose advanced DL algorithms/models have resulted in an improved PHM\nalgorithms' performance, the vulnerability of those PHM algorithms to\nadversarial attacks has not drawn much attention in the PHM community. In this\npaper we attempt to explore the vulnerability of PHM algorithms. More\nspecifically, we investigate the strategies of attacking PHM algorithms by\nconsidering several unique characteristics associated with time-series sensor\nmeasurements data. We use two real-world PHM applications as examples to\nvalidate our attack strategies and to demonstrate that PHM algorithms indeed\nare vulnerable to adversarial attacks.",
    "descriptor": "",
    "authors": [
      "Weizhong Yan",
      "Zhaoyuan Yang",
      "Jianwei Qiu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07462"
  },
  {
    "id": "arXiv:2110.07467",
    "title": "Hybrid Quantum-Classical Neural Network for Cloud-supported In-Vehicle  Cyberattack Detection",
    "abstract": "A classical computer works with ones and zeros, whereas a quantum computer\nuses ones, zeros, and superpositions of ones and zeros, which enables quantum\ncomputers to perform a vast number of calculations simultaneously compared to\nclassical computers. In a cloud-supported cyber-physical system environment,\nrunning a machine learning application in quantum computers is often difficult,\ndue to the existing limitations of the current quantum devices. However, with\nthe combination of quantum-classical neural networks (NN), complex and\nhigh-dimensional features can be extracted by the classical NN to a reduced but\nmore informative feature space to be processed by the existing quantum\ncomputers. In this study, we develop a hybrid quantum-classical NN to detect an\namplitude shift cyber-attack on an in-vehicle control area network (CAN)\ndataset. We show that using the hybrid quantum classical NN, it is possible to\nachieve an attack detection accuracy of 94%, which is higher than a Long\nshort-term memory (LSTM) NN (87%) or quantum NN alone (62%)",
    "descriptor": "\nComments: 4 pages, 3 figures\n",
    "authors": [
      "Mhafuzul Islam",
      "Mashrur Chowdhury",
      "Zadid Khan",
      "Sakib Mahmud Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2110.07467"
  },
  {
    "id": "arXiv:2110.07469",
    "title": "Shaping Large Population Agent Behaviors Through Entropy-Regularized  Mean-Field Games",
    "abstract": "Mean-field games (MFG) were introduced to efficiently analyze approximate\nNash equilibria in large population settings. In this work, we consider\nentropy-regularized mean-field games with a finite state-action space in a\ndiscrete time setting. We show that entropy regularization provides the\nnecessary regularity conditions, that are lacking in the standard finite mean\nfield games. Such regularity conditions enable us to design fixed-point\niteration algorithms to find the unique mean-field equilibrium (MFE).\nFurthermore, the reference policy used in the regularization provides an extra\nmeans, through which one can control the behavior of the population. We first\nformulate the problem as a stochastic game with a large population of $N$\nhomogeneous agents. We establish conditions for the existence of a Nash\nequilibrium in the limiting case as $N$ tends to infinity, and we demonstrate\nthat the Nash equilibrium for the infinite population case is also an\n$\\epsilon$-Nash equilibrium for the $N$-agent regularized game, where the\nsub-optimality $\\epsilon$ is of order $\\mathcal{O}\\big(1/\\sqrt{N}\\big)$.\nFinally, we verify the theoretical guarantees through a resource allocation\nexample and demonstrate the efficacy of using a reference policy to control the\nbehavior of a large population of agents.",
    "descriptor": "",
    "authors": [
      "Yue Guan",
      "Mi Zhou",
      "Ali Pakniyat",
      "Panagiotis Tsiotras"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07469"
  },
  {
    "id": "arXiv:2110.07470",
    "title": "Universally Rank Consistent Ordinal Regression in Neural Networks",
    "abstract": "Despite the pervasiveness of ordinal labels in supervised learning, it\nremains common practice in deep learning to treat such problems as categorical\nclassification using the categorical cross entropy loss. Recent methods\nattempting to address this issue while respecting the ordinal structure of the\nlabels have resorted to converting ordinal regression into a series of extended\nbinary classification subtasks. However, the adoption of such methods remains\ninconsistent due to theoretical and practical limitations. Here we address\nthese limitations by demonstrating that the subtask probabilities form a Markov\nchain. We show how to straightforwardly modify neural network architectures to\nexploit this fact and thereby constrain predictions to be universally rank\nconsistent. We furthermore prove that all rank consistent solutions can be\nrepresented within this formulation. Using diverse benchmarks and the\nreal-world application of a specialized recurrent neural network for COVID-19\nprognosis, we demonstrate the practical superiority of this method versus the\ncurrent state-of-the-art. The method is open sourced as user-friendly PyTorch\nand TensorFlow packages.",
    "descriptor": "",
    "authors": [
      "Garrett Jenkinson",
      "Kia Khezeli",
      "Gavin R. Oliver",
      "John Kalantari",
      "Eric W. Klee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07470"
  },
  {
    "id": "arXiv:2110.07472",
    "title": "Capacity of Group-invariant Linear Readouts from Equivariant  Representations: How Many Objects can be Linearly Classified Under All  Possible Views?",
    "abstract": "Equivariance has emerged as a desirable property of representations of\nobjects subject to identity-preserving transformations that constitute a group,\nsuch as translations and rotations. However, the expressivity of a\nrepresentation constrained by group equivariance is still not fully understood.\nWe address this gap by providing a generalization of Cover's Function Counting\nTheorem that quantifies the number of linearly separable and group-invariant\nbinary dichotomies that can be assigned to equivariant representations of\nobjects. We find that the fraction of separable dichotomies is determined by\nthe dimension of the space that is fixed by the group action. We show how this\nrelation extends to operations such as convolutions, element-wise\nnonlinearities, and global and local pooling. While other operations do not\nchange the fraction of separable dichotomies, local pooling decreases the\nfraction, despite being a highly nonlinear operation. Finally, we test our\ntheory on intermediate representations of randomly initialized and fully\ntrained convolutional neural networks and find perfect agreement.",
    "descriptor": "",
    "authors": [
      "Matthew Farrell",
      "Blake Bordelon",
      "Shubhendu Trivedi",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07472"
  },
  {
    "id": "arXiv:2110.07474",
    "title": "MReD: A Meta-Review Dataset for Controllable Text Generation",
    "abstract": "When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited.A typical example is when\nusing CNN/Daily Mail dataset for controllable text summarization, there is no\nguided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and control variables to guide\nthe generation, which can only be built with deep understanding of the domain\nknowledge. Motivated by this vi-sion, our paper introduces a new text\ngeneration dataset, named MReD. Our new dataset consists of 7,089 meta-reviews\nand all its 45k meta-review sentences are manually annotated as one of the\ncarefully defined 9 categories, including abstract, strength, decision, etc. We\npresent experimental results on start-of-the-art summarization models, and\npropose methods for controlled generation on both extractive and abstractive\nmodels using our annotated data. By exploring various settings and analaysing\nthe model behavior with respect to the control inputs, we demonstrate the\nchallenges and values of our dataset. MReD allows us to have a better\nunderstanding of the meta-review corpora and enlarge the research room for\ncontrollable text generation.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Chenhui Shen",
      "Liying Cheng",
      "Ran Zhou",
      "Lidong Bing",
      "Yang You",
      "Luo Si"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07474"
  },
  {
    "id": "arXiv:2110.07476",
    "title": "Query and Extract: Refining Event Extraction as Type-oriented Binary  Decoding",
    "abstract": "Event extraction is typically modeled as a multi-class classification problem\nwhere both event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that takes event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\ntwo public benchmarks, ACE and ERE, demonstrate that our approach achieves\nstate-of-the-art performance on each dataset and significantly outperforms\nexisting methods on zero-shot event extraction. We will make all the programs\npublicly available once the paper is accepted.",
    "descriptor": "",
    "authors": [
      "Sijia Wang",
      "Mo Yu",
      "Shiyu Chang",
      "Lichao Sun",
      "Lifu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07476"
  },
  {
    "id": "arXiv:2110.07477",
    "title": "Finetuning Large-Scale Pre-trained Language Models for Conversational  Recommendation with Knowledge Graph",
    "abstract": "In this paper, we present a pre-trained language model (PLM) based framework\ncalled RID for conversational recommender system (CRS). RID finetunes the\nlarge-scale PLMs such as DialoGPT, together with a pre-trained Relational Graph\nConvolutional Network (RGCN) to encode the node representations of an\nitem-oriented knowledge graph. The former aims to generate fluent and diverse\ndialogue responses based on the strong language generation ability of PLMs,\nwhile the latter is to facilitate the item recommendation by learning better\nnode embeddings on the structural knowledge base. To unify two modules of\ndialogue generation and item recommendation into a PLMs-based framework, we\nexpand the generation vocabulary of PLMs to include an extra item vocabulary,\nand introduces a vocabulary pointer to control when to recommend target items\nin the generation process. Extensive experiments on the benchmark dataset\nReDial show RID significantly outperforms the state-of-the-art methods on both\nevaluations of dialogue and recommendation.",
    "descriptor": "",
    "authors": [
      "Lingzhi Wang",
      "Huang Hu",
      "Lei Sha",
      "Can Xu",
      "Kam-Fai Wong",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07477"
  },
  {
    "id": "arXiv:2110.07479",
    "title": "VABO: Violation-Aware Bayesian Optimization for Closed-Loop Control  Performance Optimization with Unmodeled Constraints",
    "abstract": "We study the problem of performance optimization of closed-loop control\nsystems with unmodeled dynamics. Bayesian optimization (BO) has been\ndemonstrated effective for improving closed-loop performance by automatically\ntuning controller gains or reference setpoints in a model-free manner. However,\nBO methods have rarely been tested on dynamical systems with unmodeled\nconstraints. In this paper, we propose a violation-aware BO algorithm (VABO)\nthat optimizes closed-loop performance while simultaneously learning\nconstraint-feasible solutions. Unlike classical constrained BO methods which\nallow an unlimited constraint violations, or safe BO algorithms that are\nconservative and try to operate with near-zero violations, we allow budgeted\nconstraint violations to improve constraint learning and accelerate\noptimization. We demonstrate the effectiveness of our proposed VABO method for\nenergy minimization of industrial vapor compression systems.",
    "descriptor": "",
    "authors": [
      "Wenjie Xu",
      "Colin N Jones",
      "Bratislav Svetozarevic",
      "Christopher R. Laughman",
      "Ankush Chakrabarty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07479"
  },
  {
    "id": "arXiv:2110.07480",
    "title": "Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named  Entity Recognition",
    "abstract": "Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To increase performance on span representation and classification, it\nis crucial to effectively integrate all useful information of different\nformats, which we refer to heterogeneous factors including tokens, labels,\nboundaries, and related spans. To fuse these heterogeneous factors, we propose\na novel triaffine mechanism including triaffine attention and scoring, which\ninteracts with multiple factors in both the stages of representation and\nclassification. Experiments results show that our proposed method achieves the\nstate-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005,\nGENIA, and KBP2017.",
    "descriptor": "",
    "authors": [
      "Zheng Yuan",
      "Chuanqi Tan",
      "Songfang Huang",
      "Fei Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07480"
  },
  {
    "id": "arXiv:2110.07483",
    "title": "On the Pitfalls of Analyzing Individual Neurons in Language Models",
    "abstract": "While many studies have shown that linguistic information is encoded in\nhidden word representations, few have studied individual neurons, to show how\nand in which neurons it is encoded. Among these, the common approach is to use\nan external probe to rank neurons according to their relevance to some\nlinguistic attribute, and to evaluate the obtained ranking using the same probe\nthat produced it. We show two pitfalls in this methodology: 1. It confounds\ndistinct factors: probe quality and ranking quality. We separate them and draw\nconclusions on each. 2. It focuses on encoded information, rather than\ninformation that is used by the model. We show that these are not the same. We\ncompare two recent ranking methods and a simple one we introduce, and evaluate\nthem with regard to both of these aspects.",
    "descriptor": "",
    "authors": [
      "Omer Antverg",
      "Yonatan Belinkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07483"
  },
  {
    "id": "arXiv:2110.07486",
    "title": "Robust monolithic solvers for the Stokes-Darcy problem with the Darcy  equation in primal form",
    "abstract": "We construct mesh-independent and parameter-robust monolithic solvers for the\ncoupled primal Stokes-Darcy problem. Three different formulations and their\ndiscretizations in terms of conforming and non-conforming finite element\nmethods and finite volume methods are considered. In each case, robust\npreconditioners are derived using a unified theoretical framework. In\nparticular, the suggested preconditioners utilize operators in fractional\nSobolev spaces. Numerical experiments demonstrate the parameter-robustness of\nthe proposed solvers.",
    "descriptor": "",
    "authors": [
      "Wietse M. Boon",
      "Timo Koch",
      "Miroslav Kuchta",
      "Kent-Andre Mardal"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07486"
  },
  {
    "id": "arXiv:2110.07493",
    "title": "Parallel Algebraic Effect Handlers",
    "abstract": "Algebraic effects and handlers support composable and structured control-flow\nabstraction. However, existing designs of algebraic effects often require\neffects to be executed sequentially. This paper studies parallel algebraic\neffect handlers. In particular, we formalize {\\lambda}p, an untyped lambda\ncalculus which models two key features, effect handlers and parallelizable\ncomputations, the latter of which takes the form of a for expression as\ninspired by the Dex programming language. We present various interesting\nexamples expressible in our calculus, and provide a Haskell implementation. We\nhope this paper provides a basis for future designs and implementations of\nparallel algebraic effect handlers.",
    "descriptor": "\nComments: Short paper submitted to the ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM) 2022\n",
    "authors": [
      "Ningning Xie",
      "Daniel D. Johnson",
      "Dougal Maclaurin",
      "Adam Paszke"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.07493"
  },
  {
    "id": "arXiv:2110.07495",
    "title": "Simple Baseline for Single Human Motion Forecasting",
    "abstract": "Global human motion forecasting is important in many fields, which is the\ncombination of global human trajectory prediction and local human pose\nprediction. Visual and social information are often used to boost model\nperformance, however, they may consume too much computational resource. In this\npaper, we establish a simple but effective baseline for single human motion\nforecasting without visual and social information, equipped with useful\ntraining tricks. Our method \"futuremotion_ICCV21\" outperforms existing methods\nby a large margin on SoMoF benchmark. We hope our work provide new ideas for\nfuture research.",
    "descriptor": "\nComments: ICCV SoMoF Workshop, 2021\n",
    "authors": [
      "Chenxi Wang",
      "Yunfeng Wang",
      "Zixuan Huang",
      "Zhiwen Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07495"
  },
  {
    "id": "arXiv:2110.07498",
    "title": "End-to-end Keyword Spotting using Xception-1d",
    "abstract": "The field of conversational agents is growing fast and there is an increasing\nneed for algorithms that enhance natural interaction. In this work we show how\nwe achieved state of the art results in the Keyword Spotting field by adapting\nand tweaking the Xception algorithm, which achieved outstanding results in\nseveral computer vision tasks. We obtained about 96\\% accuracy when classifying\naudio clips belonging to 35 different categories, beating human annotation at\nthe most complex tasks proposed.",
    "descriptor": "\nComments: In proceedings of ESANN 2021 conference. 5 pages + references\n",
    "authors": [
      "Iv\u00e1n Vall\u00e9s-P\u00e9rez",
      "Juan G\u00f3mez-Sanchis",
      "Marcelino Mart\u00ednez-Sober",
      "Joan Vila-Franc\u00e9s",
      "Antonio J. Serrano-L\u00f3pez",
      "Emilio Soria-Olivas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07498"
  },
  {
    "id": "arXiv:2110.07505",
    "title": "AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local  Differential Privacy",
    "abstract": "For protecting users' private data, local differential privacy (LDP) has been\nleveraged to provide the privacy-preserving range query, thus supporting\nfurther statistical analysis. However, existing LDP-based range query\napproaches are limited by their properties, i.e., collecting user data\naccording to a pre-defined structure. These static frameworks would incur\nexcessive noise added to the aggregated data especially in the low privacy\nbudget setting. In this work, we propose an Adaptive Hierarchical Decomposition\n(AHEAD) protocol, which adaptively and dynamically controls the built tree\nstructure, so that the injected noise is well controlled for maintaining high\nutility. Furthermore, we derive a guideline for properly choosing parameters\nfor AHEAD so that the overall utility can be consistently competitive while\nrigorously satisfying LDP. Leveraging multiple real and synthetic datasets, we\nextensively show the effectiveness of AHEAD in both low and high dimensional\nrange query scenarios, as well as its advantages over the state-of-the-art\nmethods. In addition, we provide a series of useful observations for deploying\nAHEAD in practice.",
    "descriptor": "\nComments: To Appear in the ACM Conference on Computer and Communications Security (CCS) 2021\n",
    "authors": [
      "Linkang Du",
      "Zhikun Zhang",
      "Shaojie Bai",
      "Changchang Liu",
      "Shouling Ji",
      "Peng Cheng",
      "Jiming Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07505"
  },
  {
    "id": "arXiv:2110.07510",
    "title": "Omni-Training for Data-Efficient Deep Learning",
    "abstract": "Learning a generalizable deep model from a few examples in a short time\nremains a major challenge of machine learning, which has impeded its wide\ndeployment to many scenarios. Recent advances reveal that a properly\npre-trained model endows an important property: transferability. A higher\ntransferability of the learned representations indicates a better\ngeneralizability across domains of different distributions (domain\ntransferability), or across tasks of different semantics (task\ntransferability). Transferability has become the key to enable data-efficient\ndeep learning, however, existing pre-training methods focus only on the domain\ntransferability while meta-training methods only on the task transferability.\nThis restricts their data-efficiency in downstream scenarios of diverging\ndomains and tasks. A finding of this paper is that even a tight combination of\npre-training and meta-training cannot achieve both kinds of transferability.\nThis motivates the proposed Omni-Training framework towards data-efficient deep\nlearning. Our first contribution is Omni-Net, a tri-flow architecture. Besides\nthe joint representation flow, Omni-Net introduces two new parallel flows for\npre-training and meta-training, respectively responsible for learning\nrepresentations of domain transferability and task transferability. Omni-Net\ncoordinates the parallel flows by routing them via the joint-flow, making each\ngain the other kind of transferability. Our second contribution is Omni-Loss,\nin which a mean-teacher regularization is imposed to learn generalizable and\nstabilized representations. Omni-Training is a general framework that\naccommodates many existing pre-training and meta-training algorithms. A\nthorough evaluation on cross-task and cross-domain datasets in classification,\nregression and reinforcement learning problems shows that Omni-Training\nconsistently outperforms the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Yang Shu",
      "Zhangjie Cao",
      "Jinghan Gao",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07510"
  },
  {
    "id": "arXiv:2110.07511",
    "title": "Contrastive Proposal Extension with Sequential Network for Weakly  Supervised Object Detection",
    "abstract": "Weakly supervised object detection (WSOD) has attracted more and more\nattention since it only uses image-level labels and can save huge annotation\ncosts. Most of the WSOD methods use Multiple Instance Learning (MIL) as their\nbasic framework, which regard it as an instance classification problem.\nHowever, these methods based on MIL tends to converge only on the most\ndiscriminate regions of different instances, rather than their corresponding\ncomplete regions, that is, insufficient integrity. Inspired by the habit of\nobserving things by the human, we propose a new method by comparing the initial\nproposals and the extension ones to optimize those initial proposals.\nSpecifically, we propose one new strategy for WSOD by involving contrastive\nproposal extension (CPE), which consists of multiple directional contrastive\nproposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM\nnetwork and corresponding decoders. %\\textcolor{red}{with temporal network}.\nFirstly, the boundary of initial proposals in MIL is extended to different\npositions according to well-designed sequential order. Then, CPE compares the\nextended proposal and the initial proposal by extracting the feature semantics\nof them using the encoders, and calculates the integrity of the initial\nproposal to optimize the score of the initial proposal.",
    "descriptor": "",
    "authors": [
      "Pei Lv",
      "Suqi Hu",
      "Tianran Hao",
      "Haohan Ji",
      "Lisha Cui",
      "Haoyi Fan",
      "Mingliang Xu",
      "Changsheng Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07511"
  },
  {
    "id": "arXiv:2110.07514",
    "title": "Advances in Scaling Community Discovery Methods for Large Signed Graph  Networks",
    "abstract": "Community detection is a common task in social network analysis (SNA) with\napplications in a variety of fields including medicine, criminology, and\nbusiness. Despite the popularity of community detection, there is no clear\nconsensus on the most effective methodology for signed networks. In this paper,\nwe summarize the development of community detection in signed networks and\nevaluate current state-of-the-art techniques on several real-world data sets.\nFirst, we give a comprehensive background of community detection in signed\ngraphs. Next, we compare various adaptations of the Laplacian matrix in\nrecovering ground-truth community labels via spectral clustering in small\nsigned graph data sets. Then, we evaluate the scalability of leading algorithms\non small, large, dense, and sparse real-world signed graph networks. We\nconclude with a discussion of our novel findings and recommendations for\nextensions and improvements in state-of-the-art techniques for signed graph\ncommunity discovery in large, sparse, real-world signed graphs.",
    "descriptor": "\nComments: 35 pages, 18 figures\n",
    "authors": [
      "Maria Tomasso",
      "Lucas Rusnak",
      "Jelena Te\u0161i\u0107"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07514"
  },
  {
    "id": "arXiv:2110.07515",
    "title": "Non-Autoregressive Translation with Layer-Wise Prediction and Deep  Supervision",
    "abstract": "How do we perform efficient inference while retaining high translation\nquality? Existing neural machine translation models, such as Transformer,\nachieve high performance, but they decode words one by one, which is\ninefficient. Recent non-autoregressive translation models speed up the\ninference, but their quality is still inferior. In this work, we propose DSLP,\na highly efficient and high-performance model for machine translation. The key\ninsight is to train a non-autoregressive Transformer with Deep Supervision and\nfeed additional Layer-wise Predictions. We conducted extensive experiments on\nfour translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).\nResults show that our approach consistently improves the BLEU scores compared\nwith respective base models. Specifically, our best variant outperforms the\nautoregressive model on three translation tasks, while being 14.8 times more\nefficient in inference.",
    "descriptor": "",
    "authors": [
      "Chenyang Huang",
      "Hao Zhou",
      "Osmar R. Za\u00efane",
      "Lili Mou",
      "Lei Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07515"
  },
  {
    "id": "arXiv:2110.07518",
    "title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety  Failures",
    "abstract": "Current open-domain conversational models can easily be made to talk in\ninadequate ways. Online learning from conversational feedback given by the\nconversation partner is a promising avenue for a model to improve and adapt, so\nas to generate fewer of these safety failures. However, current\nstate-of-the-art models tend to react to feedback with defensive or oblivious\nresponses. This makes for an unpleasant experience and may discourage\nconversation partners from giving feedback in the future. This work proposes\nSaFeRDialogues, a task and dataset of graceful responses to conversational\nfeedback about safety failures. We collect a dataset of 10k dialogues\ndemonstrating safety failures, feedback signaling them, and a response\nacknowledging the feedback. We show how fine-tuning on this dataset results in\nconversations that human raters deem considerably more likely to lead to a\ncivil conversation, without sacrificing engagingness or general conversational\nability.",
    "descriptor": "",
    "authors": [
      "Megan Ung",
      "Jing Xu",
      "Y-Lan Boureau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07518"
  },
  {
    "id": "arXiv:2110.07519",
    "title": "Fast Data Series Indexing for In-Memory Data",
    "abstract": "Data series similarity search is a core operation for several data series\nanalysis applications across many different domains. However, the\nstate-of-the-art techniques fail to deliver the time performance required for\ninteractive exploration, or analysis of large data series collections. In this\nwork, we propose MESSI, the first data series index designed for in-memory\noperation on modern hardware. Our index takes advantage of the modern hardware\nparallelization opportunities (i.e., SIMD instructions, multi-socket and\nmulti-core architectures), in order to accelerate both index construction and\nsimilarity search processing times. Moreover, it benefits from a careful design\nin the setup and coordination of the parallel workers and data structures, so\nthat it maximizes its performance for in-memory operations. MESSI supports\nsimilarity search using both the Euclidean and Dynamic Time Warping (DTW)\ndistances. Our experiments with synthetic and real datasets demonstrate that\noverall MESSI is up to 4x faster at index construction, and up to 11x faster at\nquery answering than the state-of-the-art parallel approach. MESSI is the first\nto answer exact similarity search queries on 100GB datasets in ~50msec\n(30-75msec across diverse datasets), which enables real-time, interactive data\nexploration on very large data series collections.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2009.00786\n",
    "authors": [
      "Botao Peng",
      "Panagiota Fatourou",
      "Themis Palpanas"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.07519"
  },
  {
    "id": "arXiv:2110.07520",
    "title": "Comparative Opinion Summarization via Collaborative Decoding",
    "abstract": "Opinion summarization focuses on generating summaries that reflect popular\nopinions of multiple reviews for a single entity (e.g., a hotel or a product.)\nWhile generated summaries offer general and concise information about a\nparticular entity, the information may be insufficient to help the user compare\nmultiple entities. Thus, the user may still struggle with the question \"Which\none should I pick?\" In this paper, we propose a {\\em comparative opinion\nsummarization} task, which is to generate two contrastive summaries and one\ncommon summary from two given sets of reviews from different entities. We\ndevelop a comparative summarization framework CoCoSum, which consists of two\nfew-shot summarization models that are jointly used to generate contrastive and\ncommon summaries. Experimental results on a newly created benchmark CoCoTrip\nshow that CoCoSum can produce high-quality contrastive and common summaries\nthan state-of-the-art opinion summarization models.",
    "descriptor": "",
    "authors": [
      "Hayate Iso",
      "Xiaolan Wang",
      "Yoshihiko Suhara"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07520"
  },
  {
    "id": "arXiv:2110.07521",
    "title": "Multi-objective Clustering: A Data-driven Analysis of MOCLE, MOCK and  $\u0394$-MOCK",
    "abstract": "We present a data-driven analysis of MOCK, $\\Delta$-MOCK, and MOCLE. These\nare three closely related approaches that use multi-objective optimization for\ncrisp clustering. More specifically, based on a collection of 12 datasets\npresenting different proprieties, we investigate the performance of MOCLE and\nMOCK compared to the recently proposed $\\Delta$-MOCK. Besides performing a\nquantitative analysis identifying which method presents a good/poor performance\nwith respect to another, we also conduct a more detailed analysis on why such a\nbehavior happened. Indeed, the results of our analysis provide useful insights\ninto the strengths and weaknesses of the methods investigated.",
    "descriptor": "\nComments: Submitted to ICONIP 2021\n",
    "authors": [
      "Adriano Kultzak",
      "Cristina Y. Morimoto",
      "Aurora Pozo",
      "Marc\u00edlio C. P. de Souto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07521"
  },
  {
    "id": "arXiv:2110.07524",
    "title": "Representation Decoupling for Open-Domain Passage Retrieval",
    "abstract": "Training dense passage representations via contrastive learning (CL) has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Recent studies mainly\nfocus on optimizing this CL framework by improving the sampling strategy or\nextra pretraining. Different from previous studies, this work devotes itself to\ninvestigating the influence of conflicts in the widely used CL strategy in\nODPR, motivated by our observation that a passage can be organized by multiple\nsemantically different sentences, thus modeling such a passage as a unified\ndense vector is not optimal. We call such conflicts Contrastive Conflicts. In\nthis work, we propose to solve it with a representation decoupling method, by\ndecoupling the passage representations into contextual sentence-level ones, and\ndesign specific CL strategies to mediate these conflicts. Experiments on widely\nused datasets including Natural Questions, Trivia QA, and SQuAD verify the\neffectiveness of our method, especially on the dataset where the conflicting\nproblem is severe. Our method also presents good transferability across the\ndatasets, which further supports our idea of mediating Contrastive Conflicts.",
    "descriptor": "",
    "authors": [
      "Bohong Wu",
      "Zhuosheng Zhang",
      "Jinyuan Wang",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07524"
  },
  {
    "id": "arXiv:2110.07525",
    "title": "Connection Management xAPP for O-RAN RIC: A Graph Neural Network and  Reinforcement Learning Approach",
    "abstract": "Connection management is an important problem for any wireless network to\nensure smooth and well-balanced operation throughout. Traditional methods for\nconnection management (specifically user-cell association) consider sub-optimal\nand greedy solutions such as connection of each user to a cell with maximum\nreceive power. However, network performance can be improved by leveraging\nmachine learning (ML) and artificial intelligence (AI) based solutions. The\nnext generation software defined 5G networks defined by the Open Radio Access\nNetwork (O-RAN) alliance facilitates the inclusion of ML/AI based solutions for\nvarious network problems. In this paper, we consider intelligent connection\nmanagement based on the O-RAN network architecture to optimize user association\nand load balancing in the network. We formulate connection management as a\ncombinatorial graph optimization problem. We propose a deep reinforcement\nlearning (DRL) solution that uses the underlying graph to learn the weights of\nthe graph neural networks (GNN) for optimal user-cell association. We consider\nthree candidate objective functions: sum user throughput, cell coverage, and\nload balancing. Our results show up to 10% gain in throughput, 45-140% gain\ncell coverage, 20-45% gain in load balancing depending on network deployment\nconfigurations compared to baseline greedy techniques.",
    "descriptor": "\nComments: paper accepted to the IEEE International Conference on Machine Learning and Applications (ICMLA 2021)\n",
    "authors": [
      "Oner Orhan",
      "Vasuki Narasimha Swamy",
      "Thomas Tetzlaff",
      "Marcel Nassar",
      "Hosein Nikopour",
      "Shilpa Talwar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07525"
  },
  {
    "id": "arXiv:2110.07532",
    "title": "Drone technology: interdisciplinary systematic assessment of knowledge  gaps and potential solutions",
    "abstract": "Despite being a hot research topic for a decade, drones are still not part of\nour everyday life. In this article, we analyze the reasons for this state of\naffairs and look for ways of improving the situation. We rely on the\nachievements of the so-called Technology Assessment (TA), an interdisciplinary\nresearch field aiming at providing knowledge for better-informed and\nwell-reflected decisions concerning new technologies. We demonstrate that the\nmost critical area requiring further development is safety. Since Unmanned\nAerial System Traffic Management (UTM) systems promise to address this problem\nin a systematic manner, we also indicate relevant solutions for UTM that have\nto be designed by wireless experts. Moreover, we suggest project implementation\nguidelines for several drone applications. The guidelines take into account the\npublic acceptance levels estimated in state of the art literature of the\ncorrespondent field.",
    "descriptor": "",
    "authors": [
      "Evgenii Vinogradov",
      "Sofie Pollin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computers and Society (cs.CY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07532"
  },
  {
    "id": "arXiv:2110.07534",
    "title": "Understanding the Evolution of Blockchain Ecosystems: A Longitudinal  Measurement Study of Bitcoin, Ethereum, and EOSIO",
    "abstract": "The continuing expansion of the blockchain ecosystems has attracted much\nattention from the research community. However, although a large number of\nresearch studies have been proposed to understand the diverse characteristics\nof individual blockchain systems (e.g., Bitcoin or Ethereum), little is known\nat a comprehensive level on the evolution of blockchain ecosystems at scale,\nlongitudinally, and across multiple blockchains. We argue that understanding\nthe dynamics of blockchain ecosystems could provide unique insights that cannot\nbe achieved through studying a single static snapshot or a single blockchain\nnetwork alone. Based on billions of transaction records collected from three\nrepresentative and popular blockchain systems (Bitcoin, Ethereum and EOSIO)\nover 10 years, we conduct the first study on the evolution of multiple\nblockchain ecosystems from different perspectives. Our exploration suggests\nthat, although the overall blockchain ecosystem shows promising growth over the\nlast decade, a number of worrying outliers exist that have disrupted its\nevolution.",
    "descriptor": "",
    "authors": [
      "Ningyu He",
      "Weihang Su",
      "Zhou Yu",
      "Xinyu Liu",
      "Fengyi Zhao",
      "Haoyu Wang",
      "Xiapu Luo",
      "Gareth Tyson",
      "Lei Wu",
      "Yao Guo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07534"
  },
  {
    "id": "arXiv:2110.07542",
    "title": "ALFRED: Virtual Memory for Intermittent Computing",
    "abstract": "We present ALFRED: a virtual memory abstraction that resolves the dichotomy\nbetween volatile and non-volatile memory in intermittent computing.\nMixed-volatile microcontrollers allow programmers to allocate part of the\napplication state onto non-volatile main memory. Programmers are therefore to\nexplore manually the trade-off between simpler management of persistent state\nagainst the energy overhead for non-volatile memory operations and\nintermittence anomalies due to re-execution of non-idempotent code. This\napproach is laborious and yields sub-optimal performance. We take a different\nstand with ALFRED: we provide programmers with a virtual memory abstraction\ndetached from the specific volatile nature of memory and automatically\ndetermine an efficient mapping from virtual to volatile or non-volatile memory.\nUnlike existing works, ALFRED does not require programmers to learn a new\nprogramming model or language syntax, while the mapping is entirely resolved at\ncompile-time, reducing the run-time energy overhead. We implement ALFRED\nthrough a series of program machine-level code transformations. Compared to\nexisting systems, we demonstrate that ALFRED reduces energy consumption by up\nto two orders of magnitude given a fixed workload. This enables the workloads\nto finish sooner, as the use of available energy shifts from ensuring forward\nprogress to useful application processing.",
    "descriptor": "",
    "authors": [
      "Andrea Maioli",
      "Luca Mottola"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.07542"
  },
  {
    "id": "arXiv:2110.07546",
    "title": "Active SLAM over Continuous Trajectory and Control: A  Covariance-Feedback Approach",
    "abstract": "This paper proposes a novel active Simultaneous Localization and Mapping\n(SLAM) method with continuous trajectory optimization over a stochastic robot\ndynamics model. The problem is formalized as a stochastic optimal control over\nthe continuous robot kinematic model to minimize a cost function that involves\nthe covariance matrix of the landmark states. We tackle the problem by\nseparately obtaining an open-loop control sequence subject to deterministic\ndynamics by iterative Covariance Regulation (iCR) and a closed-loop feedback\ncontrol under stochastic robot and covariance dynamics by Linear Quadratic\nRegulator (LQR). The proposed optimization method captures the coupling between\nlocalization and mapping in predicting uncertainty evolution and synthesizes\nhighly informative sensing trajectories. We demonstrate its performance in\nactive landmark-based SLAM using relative-position measurements with a limited\nfield of view.",
    "descriptor": "\nComments: 8 pages, 4 figures, submitted to American Control Conference 2022\n",
    "authors": [
      "Shumon Koga",
      "Arash Asgharivaskasi",
      "Nikolay Atanasov"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07546"
  },
  {
    "id": "arXiv:2110.07547",
    "title": "Reconfigurable, Intelligent, and SustainableWireless Environments for 6G  Smart Connectivity",
    "abstract": "Various visions on the forthcoming sixth Generation (6G) networks point\ntowards flexible connect-and-compute technologies to support future innovative\nservices and the corresponding use cases. 6G should be capable to accommodate\never-evolving and heterogeneous applications, future regulations, and diverse\nuser-, service-, and location-based requirements. A key element towards\nbuilding smart and energy sustainable wireless systems beyond 5G is the\nReconfigurable Intelligent Surface (RIS), which offers programmable control and\nshaping of the wireless propagation environment.\nCapitalizing on this technology potential, in this article we introduce two\nnew concepts: i) wireless environment as a service, which leverages a novel\nRIS-empowered networking paradigm to trade off diverse, and usually\nconflicting, connectivity objectives; and ii) performance-boosted areas enabled\nby RIS-based connectivity, representing competing service provisioning areas\nthat are highly spatially and temporally focused. We discuss the key\ntechnological enablers and research challenges with the proposed networking\nparadigm, and highlight the potential profound role of RISs in the recent Open\nRadio Access Network (O-RAN) architecture.",
    "descriptor": "\nComments: accepted to be published on IEEE Communications Magazine\n",
    "authors": [
      "Emilio Calvanese Strinati",
      "George C. Alexandropoulos",
      "Henk Wymeersch",
      "Benoit Denis",
      "Vincenzo Sciancalepore",
      "Raffaele D'Errico",
      "Antonio Clemente",
      "Dinh-Thuy Phan-Huy",
      "Elisabeth De Carvalho",
      "Petar Popovski"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07547"
  },
  {
    "id": "arXiv:2110.07549",
    "title": "Time Series Clustering for Human Behavior Pattern Mining",
    "abstract": "Human behavior modeling deals with learning and understanding of behavior\npatterns inherent in humans' daily routines. Existing pattern mining techniques\neither assume human dynamics is strictly periodic, or require the number of\nmodes as input, or do not consider uncertainty in the sensor data. To handle\nthese issues, in this paper, we propose a novel clustering approach for\nmodeling human behavior (named, MTpattern) from time-series data. For mining\nfrequent human behavior patterns effectively, we utilize a three-stage\npipeline: (1) represent time series data into sequence of regularly sampled\nequal-sized unit time intervals for better analysis, (2) a new distance measure\nscheme is proposed to cluster similar sequences which can handle temporal\nvariation and uncertainty in the data, and (3) exploit an exemplar-based\nclustering mechanism and fine-tune its parameters to output minimum number of\nclusters with given permissible distance constraints and without knowing the\nnumber of modes present in the data. Then, the average of all sequences in a\ncluster is considered as a human behavior pattern. Empirical studies on two\nreal-world datasets and a simulated dataset demonstrate the effectiveness of\nMTpattern w.r.to internal and external measures of clustering.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Rohan Kabra",
      "Divya Saxena",
      "Dhaval Patel",
      "Jiannong Cao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07549"
  },
  {
    "id": "arXiv:2110.07550",
    "title": "The Irrationality of Neural Rationale Models",
    "abstract": "Neural rationale models are popular for interpretable predictions of NLP\ntasks. In these, a selector extracts segments of the input text, called\nrationales, and passes these segments to a classifier for prediction. Since the\nrationale is the only information accessible to the classifier, it is plausibly\ndefined as the explanation. Is such a characterization unconditionally correct?\nIn this paper, we argue to the contrary, with both philosophical perspectives\nand empirical evidence suggesting that rationale models are, perhaps, less\nrational and interpretable than expected. We call for more rigorous and\ncomprehensive evaluations of these models to ensure desired properties of\ninterpretability are indeed achieved. The code can be found at\nhttps://github.com/yimingz89/Neural-Rationale-Analysis.",
    "descriptor": "\nComments: 10 pages (7 pages of main text), 2 figures\n",
    "authors": [
      "Yiming Zheng",
      "Serena Booth",
      "Julie Shah",
      "Yilun Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07550"
  },
  {
    "id": "arXiv:2110.07552",
    "title": "BI-RADS BERT & Using Section Tokenization to Understand Radiology  Reports",
    "abstract": "Radiology reports are the main form of communication between radiologists and\nother clinicians, and contain important information for patient care. However\nin order to use this information for research it is necessary to convert the\nraw text into structured data suitable for analysis. Domain specific contextual\nword embeddings have been shown to achieve impressive accuracy at such natural\nlanguage processing tasks in medicine. In this work we pre-trained a contextual\nembedding BERT model using breast radiology reports and developed a classifier\nthat incorporated the embedding with auxiliary global textual features in order\nto perform a section tokenization task. This model achieved a 98% accuracy at\nsegregating free text reports into sections of information outlined in the\nBreast Imaging Reporting and Data System (BI-RADS) lexicon, a significant\nimprovement over the Classic BERT model without auxiliary information. We then\nevaluated whether using section tokenization improved the downstream extraction\nof the following fields: modality/procedure, previous cancer, menopausal\nstatus, purpose of exam, breast density and background parenchymal enhancement.\nUsing the BERT model pre-trained on breast radiology reports combined with\nsection tokenization resulted in an overall accuracy of 95.9% in field\nextraction. This is a 17% improvement compared to an overall accuracy of 78.9%\nfor field extraction for models without section tokenization and with Classic\nBERT embeddings. Our work shows the strength of using BERT in radiology report\nanalysis and the advantages of section tokenization in identifying key features\nof patient factors recorded in breast radiology reports.",
    "descriptor": "",
    "authors": [
      "Grey Kuling",
      "Dr. Belinda Curpen",
      "Anne L. Martel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07552"
  },
  {
    "id": "arXiv:2110.07554",
    "title": "Looper: An end-to-end ML platform for product decisions",
    "abstract": "Modern software systems and products increasingly rely on machine learning\nmodels to make data-driven decisions based on interactions with users and\nsystems, e.g., compute infrastructure. For broader adoption, this practice must\n(i) accommodate software engineers without ML backgrounds, and (ii) provide\nmechanisms to optimize for product goals. In this work, we describe general\nprinciples and a specific end-to-end ML platform, Looper, which offers\neasy-to-use APIs for decision-making and feedback collection. Looper supports\nthe full end-to-end ML lifecycle from online data collection to model training,\ndeployment, inference, and extends support to evaluation and tuning against\nproduct goals. We outline the platform architecture and overall impact of\nproduction deployment. We also describe the learning curve and summarize\nexperiences from platform adopters.",
    "descriptor": "\nComments: 10 pages + references, 5 figures\n",
    "authors": [
      "Igor L. Markov",
      "Hanson Wang",
      "Nitya Kasturi",
      "Shaun Singh",
      "Sze Wai Yuen",
      "Mia Garrard",
      "Sarah Tran",
      "Yin Huang",
      "Zehui Wang",
      "Igor Glotov",
      "Tanvi Gupta",
      "Boshuang Huang",
      "Peng Chen",
      "Xiaowen Xie",
      "Michael Belkin",
      "Sal Uryasev",
      "Sam Howie",
      "Eytan Bakshy",
      "Norm Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.07554"
  },
  {
    "id": "arXiv:2110.07557",
    "title": "AIR-Net: Adaptive and Implicit Regularization Neural Network for Matrix  Completion",
    "abstract": "Conventionally, the matrix completion (MC) model aims to recover a matrix\nfrom partially observed elements. Accurate recovery necessarily requires a\nregularization encoding priors of the unknown matrix/signal properly. However,\nencoding the priors accurately for the complex natural signal is difficult, and\neven then, the model might not generalize well outside the particular matrix\ntype. This work combines adaptive and implicit low-rank regularization that\ncaptures the prior dynamically according to the current recovered matrix.\nFurthermore, we aim to answer the question: how does adaptive regularization\naffect implicit regularization? We utilize neural networks to represent\nAdaptive and Implicit Regularization and named the proposed model\n\\textit{AIR-Net}. Theoretical analyses show that the adaptive part of the\nAIR-Net enhances implicit regularization. In addition, the adaptive regularizer\nvanishes at the end, thus can avoid saturation issues. Numerical experiments\nfor various data demonstrate the effectiveness of AIR-Net, especially when the\nlocations of missing elements are not randomly chosen. With complete\nflexibility to select neural networks for matrix representation, AIR-Net can be\nextended to solve more general inverse problems.",
    "descriptor": "",
    "authors": [
      "Zhemin Li",
      "Hongxia Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07557"
  },
  {
    "id": "arXiv:2110.07560",
    "title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer",
    "abstract": "Fine-tuning all parameters of a pre-trained model has become the mainstream\napproach for transfer learning. To increase its efficiency and prevent\ncatastrophic forgetting and interference, techniques like adapters and sparse\nfine-tuning have been developed. Adapters are modular, as they can be combined\nto adapt a model towards different facets of knowledge (e.g., dedicated\nlanguage and/or task adapters). Sparse fine-tuning is expressive, as it\ncontrols the behavior of all model components. In this work, we introduce a new\nfine-tuning method with both these desirable properties. In particular, we\nlearn sparse, real-valued masks based on a simple variant of the Lottery Ticket\nHypothesis. Task-specific masks are obtained from annotated data in a source\nlanguage, and language-specific masks from masked language modeling in a target\nlanguage. Both these masks can then be composed with the pre-trained model.\nUnlike adapter-based fine-tuning, this method neither increases the number of\nparameters at inference time nor alters the original model architecture. Most\nimportantly, it outperforms adapters in zero-shot cross-lingual transfer by a\nlarge margin in a series of multilingual benchmarks, including Universal\nDependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we\nadditionally find that sparsity is crucial to prevent both 1) interference\nbetween the fine-tunings to be composed and 2) overfitting. We release the code\nand models at https://github.com/cambridgeltl/composable-sft.",
    "descriptor": "",
    "authors": [
      "Alan Ansell",
      "Edoardo Maria Ponti",
      "Anna Korhonen",
      "Ivan Vuli\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07560"
  },
  {
    "id": "arXiv:2110.07566",
    "title": "Practical Benefits of Feature Feedback Under Distribution Shift",
    "abstract": "In attempts to develop sample-efficient algorithms, researcher have explored\nmyriad mechanisms for collecting and exploiting feature feedback, auxiliary\nannotations provided for training (but not test) instances that highlight\nsalient evidence. Examples include bounding boxes around objects and salient\nspans in text. Despite its intuitive appeal, feature feedback has not delivered\nsignificant gains in practical problems as assessed on iid holdout sets.\nHowever, recent works on counterfactually augmented data suggest an alternative\nbenefit of supplemental annotations: lessening sensitivity to spurious patterns\nand consequently delivering gains in out-of-domain evaluations. Inspired by\nthese findings, we hypothesize that while the numerous existing methods for\nincorporating feature feedback have delivered negligible in-sample gains, they\nmay nevertheless generalize better out-of-domain. In experiments addressing\nsentiment analysis, we show that feature feedback methods perform significantly\nbetter on various natural out-of-domain datasets even absent differences on\nin-domain evaluation. By contrast, on natural language inference tasks,\nperformance remains comparable. Finally, we compare those tasks where feature\nfeedback does (and does not) help.",
    "descriptor": "",
    "authors": [
      "Anurag Katakkar",
      "Weiqin Wang",
      "Clay H. Yoo",
      "Zachary C. Lipton",
      "Divyansh Kaushik"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07566"
  },
  {
    "id": "arXiv:2110.07567",
    "title": "Resource-constrained Federated Edge Learning with Heterogeneous Data:  Formulation and Analysis",
    "abstract": "Efficient collaboration between collaborative machine learning and wireless\ncommunication technology, forming a Federated Edge Learning (FEEL), has spawned\na series of next-generation intelligent applications. However, due to the\nopenness of network connections, the FEEL framework generally involves hundreds\nof remote devices (or clients), resulting in expensive communication costs,\nwhich is not friendly to resource-constrained FEEL. To address this issue, we\npropose a distributed approximate Newton-type algorithm with fast convergence\nspeed to alleviate the problem of FEEL resource (in terms of communication\nresources) constraints. Specifically, the proposed algorithm is improved based\non distributed L-BFGS algorithm and allows each client to approximate the\nhigh-cost Hessian matrix by computing the low-cost Fisher matrix in a\ndistributed manner to find a \"better\" descent direction, thereby speeding up\nconvergence. Second, we prove that the proposed algorithm has linear\nconvergence in strongly convex and non-convex cases and analyze its\ncomputational and communication complexity. Similarly, due to the heterogeneity\nof the connected remote devices, FEEL faces the challenge of heterogeneous data\nand non-IID (Independent and Identically Distributed) data. To this end, we\ndesign a simple but elegant training scheme, namely FedOVA, to solve the\nheterogeneous statistical challenge brought by heterogeneous data. In this way,\nFedOVA first decomposes a multi-class classification problem into more\nstraightforward binary classification problems and then combines their\nrespective outputs using ensemble learning. In particular, the scheme can be\nwell integrated with our communication efficient algorithm to serve FEEL.\nNumerical results verify the effectiveness and superiority of the proposed\nalgorithm.",
    "descriptor": "\nComments: Under View\n",
    "authors": [
      "Yi Liu",
      "Yuanshao Zhu",
      "James J.Q. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07567"
  },
  {
    "id": "arXiv:2110.07570",
    "title": "sMGC: A Complex-Valued Graph Convolutional Network via Magnetic  Laplacian for Directed Graphs",
    "abstract": "Recent advancements in Graph Neural Networks have led to state-of-the-art\nperformance on representation learning of graphs for node classification.\nHowever, the majority of existing works process directed graphs by\nsymmetrization, which may cause loss of directional information. In this paper,\nwe propose the magnetic Laplacian that preserves edge directionality by\nencoding it into complex phase as a deformation of the combinatorial Laplacian.\nIn addition, we design an Auto-Regressive Moving-Average (ARMA) filter that is\ncapable of learning global features from graphs. To reduce time complexity,\nTaylor expansion is applied to approximate the filter. We derive complex-valued\noperations in graph neural network and devise a simplified Magnetic Graph\nConvolution network, namely sMGC. Our experiment results demonstrate that sMGC\nis a fast, powerful, and widely applicable GNN.",
    "descriptor": "\nComments: 9 pages, 7 figures, 5 tables\n",
    "authors": [
      "Jie Zhang",
      "Bo Hui",
      "Po-Wei Harn",
      "Min-Te Sun",
      "Wei-Shinn Ku"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.07570"
  },
  {
    "id": "arXiv:2110.07572",
    "title": "LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in  Semantic Parsing",
    "abstract": "Semantic parsing is the task of producing a structured meaning representation\nfor natural language utterances or questions. Recent research has pointed out\nthat the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle\nto generalize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation (MR)\ndirectly as a graph and not as a sequence. To this end we propose LAGr, the\nLabeling Aligned Graphs algorithm that produces semantic parses by predicting\nnode and edge labels for a complete multi-layer input-aligned graph. The\nstrongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas\nweakly-supervised LAGr infers alignments for originally unaligned target graphs\nusing an approximate MAP inference procedure. On the COGS and CFQ compositional\ngeneralization benchmarks the strongly- and weakly- supervised LAGr algorithms\nachieve significant improvements upon the baseline seq2seq parsers.",
    "descriptor": "",
    "authors": [
      "Dora Jambor",
      "Dzmitry Bahdanau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07572"
  },
  {
    "id": "arXiv:2110.07574",
    "title": "Delphi: Towards Machine Ethics and Norms",
    "abstract": "What would it take to teach a machine to behave ethically? While broad\nethical rules may seem straightforward to state (\"thou shalt not kill\"),\napplying such rules to real-world situations is far more complex. For example,\nwhile \"helping a friend\" is generally a good thing to do, \"helping a friend\nspread fake news\" is not. We identify four underlying challenges towards\nmachine ethics and norms: (1) an understanding of moral precepts and social\nnorms; (2) the ability to perceive real-world situations visually or by reading\nnatural language descriptions; (3) commonsense reasoning to anticipate the\noutcome of alternative actions in different contexts; (4) most importantly, the\nability to make ethical judgments given the interplay between competing values\nand their grounding in different contexts (e.g., the right to freedom of\nexpression vs. preventing the spread of fake news).\nOur paper begins to address these questions within the deep learning\nparadigm. Our prototype model, Delphi, demonstrates strong promise of\nlanguage-based commonsense moral reasoning, with up to 92.1% accuracy vetted by\nhumans. This is in stark contrast to the zero-shot performance of GPT-3 of\n52.3%, which suggests that massive scale alone does not endow pre-trained\nneural language models with human values. Thus, we present Commonsense Norm\nBank, a moral textbook customized for machines, which compiles 1.7M examples of\npeople's ethical judgments on a broad spectrum of everyday situations. In\naddition to the new resources and baseline performances for future research,\nour study provides new insights that lead to several important open research\nquestions: differentiating between universal human values and personal values,\nmodeling different moral frameworks, and explainable, consistent approaches to\nmachine ethics.",
    "descriptor": "",
    "authors": [
      "Liwei Jiang",
      "Jena D. Hwang",
      "Chandra Bhagavatula",
      "Ronan Le Bras",
      "Maxwell Forbes",
      "Jon Borchardt",
      "Jenny Liang",
      "Oren Etzioni",
      "Maarten Sap",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07574"
  },
  {
    "id": "arXiv:2110.07575",
    "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
    "abstract": "Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.",
    "descriptor": "\nComments: Presented at Interspeech 2021. This version contains additional experiments on the Spoken ObjectNet test set\n",
    "authors": [
      "Ian Palmer",
      "Andrew Rouditchenko",
      "Andrei Barbu",
      "Boris Katz",
      "James Glass"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07575"
  },
  {
    "id": "arXiv:2110.07577",
    "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model  Tuning",
    "abstract": "Conventional fine-tuning of pre-trained language models tunes all model\nparameters and stores a full model copy for each downstream task, which has\nbecome increasingly infeasible as the model size grows larger. Recent\nparameter-efficient language model tuning (PELT) methods manage to match the\nperformance of fine-tuning with much fewer trainable parameters and perform\nespecially well when the training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and downstream tasks.\nIn light of model diversity and the difficulty of model selection, we propose a\nunified framework, UniPELT, which incorporates different PELT methods as\nsubmodules and learns to activate the ones that best suit the current data or\ntask setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves\n1~3pt gains compared to the best individual PELT method that it incorporates\nand even outperforms fine-tuning under different setups. Moreover, UniPELT\noften surpasses the upper bound when taking the best performance of all its\nsubmodules used individually on each task, indicating that a mixture of\nmultiple PELT methods may be inherently more effective than single methods.",
    "descriptor": "",
    "authors": [
      "Yuning Mao",
      "Lambert Mathias",
      "Rui Hou",
      "Amjad Almahairi",
      "Hao Ma",
      "Jiawei Han",
      "Wen-tau Yih",
      "Madian Khabsa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07577"
  },
  {
    "id": "arXiv:2110.07578",
    "title": "Learning Temporal 3D Human Pose Estimation with Pseudo-Labels",
    "abstract": "We present a simple, yet effective, approach for self-supervised 3D human\npose estimation. Unlike the prior work, we explore the temporal information\nnext to the multi-view self-supervision. During training, we rely on\ntriangulating 2D body pose estimates of a multiple-view camera system. A\ntemporal convolutional neural network is trained with the generated 3D\nground-truth and the geometric multi-view consistency loss, imposing\ngeometrical constraints on the predicted 3D body skeleton. During inference,\nour model receives a sequence of 2D body pose estimates from a single-view to\npredict the 3D body pose for each of them. An extensive evaluation shows that\nour method achieves state-of-the-art performance in the Human3.6M and\nMPI-INF-3DHP benchmarks. Our code and models are publicly available at\n\\url{https://github.com/vru2020/TM_HPE/}.",
    "descriptor": "\nComments: Accepted for publication at AVSS 2021. Project page:this https URL\n",
    "authors": [
      "Arij Bouazizi",
      "Ulrich Kressel",
      "Vasileios Belagiannis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07578"
  },
  {
    "id": "arXiv:2110.07579",
    "title": "Diffusion Normalizing Flow",
    "abstract": "We present a novel generative modeling method called diffusion normalizing\nflow based on stochastic differential equations (SDEs). The algorithm consists\nof two neural SDEs: a forward SDE that gradually adds noise to the data to\ntransform the data into Gaussian random noise, and a backward SDE that\ngradually removes the noise to sample from the data distribution. By jointly\ntraining the two neural SDEs to minimize a common cost function that quantifies\nthe difference between the two, the backward SDE converges to a diffusion\nprocess the starts with a Gaussian distribution and ends with the desired data\ndistribution. Our method is closely related to normalizing flow and diffusion\nprobabilistic models and can be viewed as a combination of the two. Compared\nwith normalizing flow, diffusion normalizing flow is able to learn\ndistributions with sharp boundaries. Compared with diffusion probabilistic\nmodels, diffusion normalizing flow requires fewer discretization steps and thus\nhas better sampling efficiency. Our algorithm demonstrates competitive\nperformance in both high-dimension data density estimation and image generation\ntasks.",
    "descriptor": "\nComments: Neurips 2021\n",
    "authors": [
      "Qinsheng Zhang",
      "Yongxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07579"
  },
  {
    "id": "arXiv:2110.07580",
    "title": "Graph Condensation for Graph Neural Networks",
    "abstract": "Given the prevalence of large-scale graphs in real-world applications, the\nstorage and time for training neural models have raised increasing concerns. To\nalleviate the concerns, we propose and study the problem of graph condensation\nfor graph neural networks (GNNs). Specifically, we aim to condense the large,\noriginal graph into a small, synthetic and highly-informative graph, such that\nGNNs trained on the small graph and large graph have comparable performance. We\napproach the condensation problem by imitating the GNN training trajectory on\nthe original graph through the optimization of a gradient matching loss and\ndesign a strategy to condense node futures and structural information\nsimultaneously. Extensive experiments have demonstrated the effectiveness of\nthe proposed framework in condensing different graph datasets into informative\nsmaller graphs. In particular, we are able to approximate the original test\naccuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while\nreducing their graph size by more than 99.9%, and the condensed graphs can be\nused to train various GNN architectures.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Wei Jin",
      "Lingxiao Zhao",
      "Shichang Zhang",
      "Yozen Liu",
      "Jiliang Tang",
      "Neil Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07580"
  },
  {
    "id": "arXiv:2110.07581",
    "title": "Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant  Representations",
    "abstract": "Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models' evaluation. Source code of this paper will be released.",
    "descriptor": "",
    "authors": [
      "Ji Xin",
      "Chenyan Xiong",
      "Ashwin Srinivasan",
      "Ankita Sharma",
      "Damien Jose",
      "Paul N. Bennett"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07581"
  },
  {
    "id": "arXiv:2110.07582",
    "title": "Network Representation Learning: From Preprocessing, Feature Extraction  to Node Embedding",
    "abstract": "Network representation learning (NRL) advances the conventional graph mining\nof social networks, knowledge graphs, and complex biomedical and physics\ninformation networks. Over dozens of network representation learning algorithms\nhave been reported in the literature. Most of them focus on learning node\nembeddings for homogeneous networks, but they differ in the specific encoding\nschemes and specific types of node semantics captured and used for learning\nnode embedding. This survey paper reviews the design principles and the\ndifferent node embedding techniques for network representation learning over\nhomogeneous networks. To facilitate the comparison of different node embedding\nalgorithms, we introduce a unified reference framework to divide and generalize\nthe node embedding learning process on a given network into preprocessing\nsteps, node feature extraction steps and node embedding model training for a\nNRL task such as link prediction and node clustering. With this unifying\nreference framework, we highlight the representative methods, models, and\ntechniques used at different stages of the node embedding model learning\nprocess. This survey not only helps researchers and practitioners to gain an\nin-depth understanding of different network representation learning techniques\nbut also provides practical guidelines for designing and developing the next\ngeneration of network representation learning algorithms and systems.",
    "descriptor": "",
    "authors": [
      "Jingya Zhou",
      "Ling Liu",
      "Wenqi Wei",
      "Jianxi Fan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07582"
  },
  {
    "id": "arXiv:2110.07584",
    "title": "Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and  Partial Differential Equation in a Loop",
    "abstract": "This paper investigates unsupervised learning of Full-Waveform Inversion\n(FWI), which has been widely used in geophysics to estimate subsurface velocity\nmaps from seismic data. This problem is mathematically formulated by a second\norder partial differential equation (PDE), but is hard to solve. Moreover,\nacquiring velocity map is extremely expensive, making it impractical to scale\nup a supervised approach to train the mapping from seismic data to velocity\nmaps with convolutional neural networks (CNN). We address these difficulties by\nintegrating PDE and CNN in a loop, thus shifting the paradigm to unsupervised\nlearning that only requires seismic data. In particular, we use finite\ndifference to approximate the forward modeling of PDE as a differentiable\noperator (from velocity map to seismic data) and model its inversion by CNN\n(from seismic data to velocity map). Hence, we transform the supervised\ninversion task into an unsupervised seismic data reconstruction task. We also\nintroduce a new large-scale dataset OpenFWI, to establish a more challenging\nbenchmark for the community. Experiment results show that our model (using\nseismic data alone) yields comparable accuracy to the supervised counterpart\n(using both seismic data and velocity map). Furthermore, it outperforms the\nsupervised model when involving more seismic data.",
    "descriptor": "",
    "authors": [
      "Peng Jin",
      "Xitong Zhang",
      "Yinpeng Chen",
      "Sharon Xiaolei Huang",
      "Zicheng Liu",
      "Youzuo Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07584"
  },
  {
    "id": "arXiv:2110.07586",
    "title": "Can Explanations Be Useful for Calibrating Black Box Models?",
    "abstract": "One often wants to take an existing, trained NLP model and use it on data\nfrom a new domain. While fine-tuning or few-shot learning can be used to adapt\nthe base model, there is no one simple recipe to getting these working;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. To this end, we study how to improve a black box\nmodel's performance on a new domain given examples from the new domain by\nleveraging explanations of the model's behavior. Our approach first extracts a\nset of features combining human intuition about the task with model\nattributions generated by black box interpretation techniques, and then uses a\nsimple model to calibrate or rerank the model's predictions based on the\nfeatures. We experiment with our method on two tasks, extractive question\nanswering and natural language inference, covering adaptation from several\npairs of domains. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models. We show that the\ncalibration features transfer to some extent between tasks and shed light on\nhow to effectively use them.",
    "descriptor": "",
    "authors": [
      "Xi Ye",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07586"
  },
  {
    "id": "arXiv:2110.07588",
    "title": "Playing for 3D Human Recovery",
    "abstract": "Image- and video-based 3D human recovery (i.e. pose and shape estimation)\nhave achieved substantial progress. However, due to the prohibitive cost of\nmotion capture, existing datasets are often limited in scale and diversity,\nwhich hinders the further development of more powerful models. In this work, we\nobtain massive human sequences as well as their 3D ground truths by playing\nvideo games. Specifically, we contribute, GTA-Human, a mega-scale and\nhighly-diverse 3D human dataset generated with the GTA-V game engine. With a\nrich set of subjects, actions, and scenarios, GTA-Human serves as both an\neffective training source. Notably, the \"unreasonable effectiveness of data\"\nphenomenon is validated in 3D human recovery using our game-playing data. A\nsimple frame-based baseline trained on GTA-Human already outperforms more\nsophisticated methods by a large margin; for video-based methods, GTA-Human\ndemonstrates superiority over even the in-domain training set. We extend our\nstudy to larger models to observe the same consistent improvements, and the\nstudy on supervision signals suggests the rich collection of SMPL annotations\nis key. Furthermore, equipped with the diverse annotations in GTA-Human, we\nsystematically investigate the performance of various methods under a wide\nspectrum of real-world variations, e.g. camera angles, poses, and occlusions.\nWe hope our work could pave way for scaling up 3D human recovery to the real\nworld.",
    "descriptor": "",
    "authors": [
      "Zhongang Cai",
      "Mingyuan Zhang",
      "Jiawei Ren",
      "Chen Wei",
      "Daxuan Ren",
      "Jiatong Li",
      "Zhengyu Lin",
      "Haiyu Zhao",
      "Shuai Yi",
      "Lei Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07588"
  },
  {
    "id": "arXiv:2110.07592",
    "title": "Speech Toxicity Analysis: A New Spoken Language Processing Task",
    "abstract": "Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text with no existing work on\ntoxicity detection from spoken utterances. In this paper, we propose a new\nSpoken Language Processing task of detecting toxicity from spoken speech. We\nintroduce DeToxy, the first publicly available toxicity annotated dataset for\nEnglish speech, sourced from various openly available speech databases,\nconsisting of over 2 million utterances. Finally, we also provide analysis on\nhow a spoken speech corpus annotated for toxicity can help facilitate the\ndevelopment of E2E models which better capture various prosodic cues in speech,\nthereby boosting toxicity classification on spoken utterances.",
    "descriptor": "\nComments: 5 pages, submitted to ICASSP 2022\n",
    "authors": [
      "Sreyan Ghosh",
      "Samden Lepcha",
      "S Sakshi",
      "Rajiv Ratn Shah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07592"
  },
  {
    "id": "arXiv:2110.07594",
    "title": "The Neural MMO Platform for Massively Multiagent Research",
    "abstract": "Neural MMO is a computationally accessible research platform that combines\nlarge agent populations, long time horizons, open-ended tasks, and modular game\nsystems. Existing environments feature subsets of these properties, but Neural\nMMO is the first to combine them all. We present Neural MMO as free and open\nsource software with active support, ongoing development, documentation, and\nadditional training, logging, and visualization tools to help users adapt to\nthis new setting. Initial baselines on the platform demonstrate that agents\ntrained in large populations explore more and learn a progression of skills. We\nraise other more difficult problems such as many-team cooperation as open\nresearch questions which Neural MMO is well-suited to answer. Finally, we\ndiscuss current limitations of the platform, potential mitigations, and plans\nfor continued development.",
    "descriptor": "",
    "authors": [
      "Joseph Suarez",
      "Yilun Du",
      "Clare Zhu",
      "Igor Mordatch",
      "Phillip Isola"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.07594"
  },
  {
    "id": "arXiv:2110.07595",
    "title": "Compressibility of Distributed Document Representations",
    "abstract": "Contemporary natural language processing (NLP) revolves around learning from\nlatent document representations, generated either implicitly by neural language\nmodels or explicitly by methods such as doc2vec or similar. One of the key\nproperties of the obtained representations is their dimension. Whilst the\ncommonly adopted dimensions of 256 and 768 offer sufficient performance on many\ntasks, it is many times unclear whether the default dimension is the most\nsuitable choice for the subsequent downstream learning tasks. Furthermore,\nrepresentation dimensions are seldom subject to hyperparameter tuning due to\ncomputational constraints. The purpose of this paper is to demonstrate that a\nsurprisingly simple and efficient recursive compression procedure can be\nsufficient to both significantly compress the initial representation, but also\npotentially improve its performance when considering the task of text\nclassification. Having smaller and less noisy representations is the desired\nproperty during deployment, as orders of magnitude smaller models can\nsignificantly reduce the computational overload and with it the deployment\ncosts. We propose CoRe, a straightforward, representation learner-agnostic\nframework suitable for representation compression. The CoRe's performance is\nshowcased and studied on a collection of 17 real-life corpora from biomedical,\nnews, social media, and literary domains. We explored CoRe's behavior when\nconsidering contextual and non-contextual document representations, different\ncompression levels, and 9 different compression algorithms. Current results\nbased on more than 100,000 compression experiments indicate that recursive\nSingular Value Decomposition offers a very good trade-off between the\ncompression efficiency and performance, making CoRe useful in many existing,\nrepresentation-dependent NLP pipelines.",
    "descriptor": "\nComments: Accepted to ICDM2021\n",
    "authors": [
      "Bla\u017e \u0160krlj",
      "Matej Petkovi\u010d"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07595"
  },
  {
    "id": "arXiv:2110.07596",
    "title": "Retrieval-guided Counterfactual Generation for QA",
    "abstract": "Deep NLP models have been shown to learn spurious correlations, leaving them\nbrittle to input perturbations. Recent work has shown that counterfactual or\ncontrastive data -- i.e. minimally perturbed inputs -- can reveal these\nweaknesses, and that data augmentation using counterfactuals can help\nameliorate them. Proposed techniques for generating counterfactuals rely on\nhuman annotations, perturbations based on simple heuristics, and meaning\nrepresentation frameworks. We focus on the task of creating counterfactuals for\nquestion answering, which presents unique challenges related to world\nknowledge, semantic diversity, and answerability. To address these challenges,\nwe develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual\nevaluation and training data with minimal human supervision. Using an\nopen-domain QA framework and question generation model trained on original task\ndata, we create counterfactuals that are fluent, semantically diverse, and\nautomatically labeled. Data augmentation with RGF counterfactuals improves\nperformance on out-of-domain and challenging evaluation sets over and above\nexisting methods, in both the reading comprehension and open-domain QA\nsettings. Moreover, we find that RGF data leads to significant improvements in\na model's robustness to local perturbations.",
    "descriptor": "",
    "authors": [
      "Bhargavi Paranjape",
      "Matthew Lamm",
      "Ian Tenney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07596"
  },
  {
    "id": "arXiv:2110.07600",
    "title": "PointAcc: Efficient Point Cloud Accelerator",
    "abstract": "Deep learning on point clouds plays a vital role in a wide range of\napplications such as autonomous driving and AR/VR. These applications interact\nwith people in real-time on edge devices and thus require low latency and low\nenergy. Compared to projecting the point cloud to 2D space, directly processing\nthe 3D point cloud yields higher accuracy and lower #MACs. However, the\nextremely sparse nature of point cloud poses challenges to hardware\nacceleration. For example, we need to explicitly determine the nonzero outputs\nand search for the nonzero neighbors (mapping operation), which is unsupported\nin existing accelerators. Furthermore, explicit gather and scatter of sparse\nfeatures are required, resulting in large data movement overhead. In this\npaper, we comprehensively analyze the performance bottleneck of modern point\ncloud networks on CPU/GPU/TPU. To address the challenges, we then present\nPointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse\nmapping operations onto one versatile ranking-based kernel, streams the sparse\ncomputation with configurable caching, and temporally fuses consecutive dense\nlayers to reduce the memory footprint. Evaluated on 8 point cloud models across\n4 applications, PointAcc achieves 3.7X speedup and 22X energy savings over RTX\n2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the\nprior accelerator Mesorasi by 100X speedup with 9.1% higher accuracy running\nsegmentation on the S3DIS dataset. PointAcc paves the way for efficient point\ncloud recognition.",
    "descriptor": "\nComments: Accepted by Mircro 2021\n",
    "authors": [
      "Yujun Lin",
      "Zhekai Zhang",
      "Haotian Tang",
      "Hanrui Wang",
      "Song Han"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2110.07600"
  },
  {
    "id": "arXiv:2110.07602",
    "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally  Across Scales and Tasks",
    "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work and our results reveal that existing\nmethods of prompt tuning do not perform well for normal-sized pre-trained\nmodels and for hard sequence tasks, indicating lack of universality. We present\na novel empirical finding that properly-optimized prompt tuning can be\nuniversally effective across a wide range of model scales and NLU tasks, where\nit matches the performance of fine-tuning while having only 0.1\\%-3\\% tuned\nparameters. Our method P-Tuning v2 is not a new method but a version of\nprefix-tuning \\cite{li2021prefix} optimized and adapted for NLU. Given the\nuniversality and simplicity of P-Tuning v2, we believe it can serve as an\nalternative for fine-tuning and a strong baseline for future research.",
    "descriptor": "",
    "authors": [
      "Xiao Liu",
      "Kaixuan Ji",
      "Yicheng Fu",
      "Zhengxiao Du",
      "Zhilin Yang",
      "Jie Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07602"
  },
  {
    "id": "arXiv:2110.07603",
    "title": "Sub-word Level Lip Reading With Visual Attention",
    "abstract": "The goal of this paper is to learn strong lip reading models that can\nrecognise speech in silent videos. Most prior works deal with the open-set\nvisual speech recognition problem by adapting existing automatic speech\nrecognition techniques on top of trivially pooled visual features. Instead, in\nthis paper we focus on the unique challenges encountered in lip reading and\npropose tailored solutions. To that end we make the following contributions:\n(1) we propose an attention-based pooling mechanism to aggregate visual speech\nrepresentations; (2) we use sub-word units for lip reading for the first time\nand show that this allows us to better model the ambiguities of the task; (3)\nwe propose a training pipeline that balances the lip reading performance with\nother key factors such as data and compute efficiency. Following the above, we\nobtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks\nwhen training on public datasets, and even surpass models trained on\nlarge-scale industrial datasets by using an order of magnitude less data. Our\nbest model achieves 22.6% word error rate on the LRS2 dataset, a performance\nunprecedented for lip reading models, significantly reducing the performance\ngap between lip reading and automatic speech recognition.",
    "descriptor": "",
    "authors": [
      "Prajwal K R",
      "Triantafyllos Afouras",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07603"
  },
  {
    "id": "arXiv:2110.07604",
    "title": "NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in  the Wild",
    "abstract": "Recent history has seen a tremendous growth of work exploring implicit\nrepresentations of geometry and radiance, popularized through Neural Radiance\nFields (NeRF). Such works are fundamentally based on a (implicit) {\\em\nvolumetric} representation of occupancy, allowing them to model diverse scene\nstructure including translucent objects and atmospheric obscurants. But because\nthe vast majority of real-world scenes are composed of well-defined surfaces,\nwe introduce a {\\em surface} analog of such implicit models called Neural\nReflectance Surfaces (NeRS). NeRS learns a neural shape representation of a\nclosed surface that is diffeomorphic to a sphere, guaranteeing water-tight\nreconstructions. Even more importantly, surface parameterizations allow NeRS to\nlearn (neural) bidirectional surface reflectance functions (BRDFs) that\nfactorize view-dependent appearance into environmental illumination, diffuse\ncolor (albedo), and specular \"shininess.\" Finally, rather than illustrating our\nresults on synthetic scenes or controlled in-the-lab capture, we assemble a\nnovel dataset of multi-view images from online marketplaces for selling goods.\nSuch \"in-the-wild\" multi-view image sets pose a number of challenges, including\na small number of views with unknown/rough camera estimates. We demonstrate\nthat surface-based neural reconstructions enable learning from such data,\noutperforming volumetric neural rendering-based reconstructions. We hope that\nNeRS serves as a first step toward building scalable, high-quality libraries of\nreal-world shape, materials, and illumination. The project page with code and\nvideo visualizations can be found at\nhttps://jasonyzhang.com/ners}{jasonyzhang.com/ners.",
    "descriptor": "\nComments: In NeurIPS 2021\n",
    "authors": [
      "Jason Y. Zhang",
      "Gengshan Yang",
      "Shubham Tulsiani",
      "Deva Ramanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07604"
  },
  {
    "id": "arXiv:2110.06931",
    "title": "A neural simulation-based inference approach for characterizing the  Galactic Center $\u03b3$-ray excess",
    "abstract": "The nature of the Fermi gamma-ray Galactic Center Excess (GCE) has remained a\npersistent mystery for over a decade. Although the excess is broadly compatible\nwith emission expected due to dark matter annihilation, an explanation in terms\nof a population of unresolved astrophysical point sources e.g., millisecond\npulsars, remains viable. The effort to uncover the origin of the GCE is\nhampered in particular by an incomplete understanding of diffuse emission of\nGalactic origin. This can lead to spurious features that make it difficult to\nrobustly differentiate smooth emission, as expected for a dark matter origin,\nfrom more \"clumpy\" emission expected for a population of relatively bright,\nunresolved point sources. We use recent advancements in the field of\nsimulation-based inference, in particular density estimation techniques using\nnormalizing flows, in order to characterize the contribution of modeled\ncomponents, including unresolved point source populations, to the GCE. Compared\nto traditional techniques based on the statistical distribution of photon\ncounts, our machine learning-based method is able to utilize more of the\ninformation contained in a given model of the Galactic Center emission, and in\nparticular can perform posterior parameter estimation while accounting for\npixel-to-pixel spatial correlations in the gamma-ray map. This makes the method\ndemonstrably more resilient to certain forms of model misspecification. On\napplication to Fermi data, the method generically attributes a smaller fraction\nof the GCE flux to unresolved point sources when compared to traditional\napproaches. We nevertheless infer such a contribution to make up a\nnon-negligible fraction of the GCE across all analysis variations considered,\nwith at least $38^{+9}_{-19}\\%$ of the excess attributed to unresolved points\nsources in our baseline analysis.",
    "descriptor": "\nComments: 20+3 pages, 10+4 figures\n",
    "authors": [
      "Siddharth Mishra-Sharma",
      "Kyle Cranmer"
    ],
    "subjectives": [
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.06931"
  },
  {
    "id": "arXiv:2110.06933",
    "title": "Style-based quantum generative adversarial networks for Monte Carlo  events",
    "abstract": "We propose and assess an alternative quantum generator architecture in the\ncontext of generative adversarial learning for Monte Carlo event generation,\nused to simulate particle physics processes at the Large Hadron Collider (LHC).\nWe validate this methodology by implementing the quantum network on artificial\ndata generated from known underlying distributions. The network is then applied\nto Monte Carlo-generated datasets of specific LHC scattering processes. The new\nquantum generator architecture leads to an improvement in state-of-the-art\nimplementations while maintaining shallow-depth networks. Moreover, the quantum\ngenerator successfully learns the underlying distribution functions even if\ntrained with small training sample sets; this is particularly interesting for\ndata augmentation applications. We deploy this novel methodology on two\ndifferent quantum hardware architectures, trapped-ion and superconducting\ntechnologies, to test its hardware-independent viability.",
    "descriptor": "\nComments: 14 pages, 10 figures, code available in this https URL\n",
    "authors": [
      "Carlos Bravo-Prieto",
      "Julien Baglio",
      "Marco C\u00e8",
      "Anthony Francis",
      "Dorota M. Grabowska",
      "Stefano Carrazza"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.06933"
  },
  {
    "id": "arXiv:2110.06942",
    "title": "Provably accurate simulation of gauge theories and bosonic systems",
    "abstract": "Quantum many-body systems involving bosonic modes or gauge fields have\ninfinite-dimensional local Hilbert spaces which must be truncated to perform\nsimulations of real-time dynamics on classical or quantum computers. To analyze\nthe truncation error, we develop methods for bounding the rate of growth of\nlocal quantum numbers such as the occupation number of a mode at a lattice\nsite, or the electric field at a lattice link. Our approach applies to various\nmodels of bosons interacting with spins or fermions, and also to both abelian\nand non-abelian gauge theories. We show that if states in these models are\ntruncated by imposing an upper limit $\\Lambda$ on each local quantum number,\nand if the initial state has low local quantum numbers, then an error at most\n$\\epsilon$ can be achieved by choosing $\\Lambda$ to scale polylogarithmically\nwith $\\epsilon^{-1}$, an exponential improvement over previous bounds based on\nenergy conservation. For the Hubbard-Holstein model, we numerically compute a\nbound on $\\Lambda$ that achieves accuracy $\\epsilon$, obtaining significantly\nimproved estimates in various parameter regimes. We also establish a criterion\nfor truncating the Hamiltonian with a provable guarantee on the accuracy of\ntime evolution. Building on that result, we formulate quantum algorithms for\ndynamical simulation of lattice gauge theories and of models with bosonic\nmodes; the gate complexity depends almost linearly on spacetime volume in the\nformer case, and almost quadratically on time in the latter case. We establish\na lower bound showing that there are systems involving bosons for which this\nquadratic scaling with time cannot be improved. By applying our result on the\ntruncation error in time evolution, we also prove that spectrally isolated\nenergy eigenstates can be approximated with accuracy $\\epsilon$ by truncating\nlocal quantum numbers at $\\Lambda=\\textrm{polylog}(\\epsilon^{-1})$.",
    "descriptor": "\nComments: 42 pages, 3 figures\n",
    "authors": [
      "Yu Tong",
      "Victor V. Albert",
      "Jarrod R. McClean",
      "John Preskill",
      "Yuan Su"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2110.06942"
  },
  {
    "id": "arXiv:2110.06964",
    "title": "The Complexity of Bipartite Gaussian Boson Sampling",
    "abstract": "Gaussian boson sampling is a model of photonic quantum computing that has\nattracted attention as a platform for building quantum devices capable of\nperforming tasks that are out of reach for classical devices. There is\ntherefore significant interest, from the perspective of computational\ncomplexity theory, in solidifying the mathematical foundation for the hardness\nof simulating these devices. We show that, under the standard\nAnti-Concentration and Permanent-of-Gaussians conjectures, there is no\nefficient classical algorithm to sample from ideal Gaussian boson sampling\ndistributions (even approximately) unless the polynomial hierarchy collapses.\nThe hardness proof holds in the regime where the number of modes scales\nquadratically with the number of photons, a setting in which hardness was\nwidely believed to hold but that nevertheless had no definitive proof.\nCrucial to the proof is a new method for programming a Gaussian boson\nsampling device so that the output probabilities are proportional to the\npermanents of submatrices of an arbitrary matrix. This technique is a\ngeneralization of Scattershot BosonSampling that we call BipartiteGBS. We also\nmake progress towards the goal of proving hardness in the regime where there\nare fewer than quadratically more modes than photons (i.e., the high-collision\nregime) by showing that the ability to approximate permanents of matrices with\nrepeated rows/columns confers the ability to approximate permanents of matrices\nwith no repetitions. The reduction suffices to prove that GBS is hard in the\nconstant-collision regime.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Daniel Grier",
      "Daniel J. Brod",
      "Juan Miguel Arrazola",
      "Marcos Benicio de Andrade Alonso",
      "Nicol\u00e1s Quesada"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2110.06964"
  },
  {
    "id": "arXiv:2110.06968",
    "title": "Interpretable AI forecasting for numerical relativity waveforms of  quasi-circular, spinning, non-precessing binary black hole mergers",
    "abstract": "We present a deep-learning artificial intelligence model that is capable of\nlearning and forecasting the late-inspiral, merger and ringdown of numerical\nrelativity waveforms that describe quasi-circular, spinning, non-precessing\nbinary black hole mergers. We used the NRHybSur3dq8 surrogate model to produce\ntrain, validation and test sets of $\\ell=|m|=2$ waveforms that cover the\nparameter space of binary black hole mergers with mass-ratios $q\\leq8$ and\nindividual spins $|s^z_{\\{1,2\\}}| \\leq 0.8$. These waveforms cover the time\nrange $t\\in[-5000\\textrm{M}, 130\\textrm{M}]$, where $t=0M$ marks the merger\nevent, defined as the maximum value of the waveform amplitude. We harnessed the\nThetaGPU supercomputer at the Argonne Leadership Computing Facility to train\nour AI model using a training set of 1.5 million waveforms. We used 16 NVIDIA\nDGX A100 nodes, each consisting of 8 NVIDIA A100 Tensor Core GPUs and 2 AMD\nRome CPUs, to fully train our model within 3.5 hours. Our findings show that\nartificial intelligence can accurately forecast the dynamical evolution of\nnumerical relativity waveforms in the time range $t\\in[-100\\textrm{M},\n130\\textrm{M}]$. Sampling a test set of 190,000 waveforms, we find that the\naverage overlap between target and predicted waveforms is $\\gtrsim99\\%$ over\nthe entire parameter space under consideration. We also combined scientific\nvisualization and accelerated computing to identify what components of our\nmodel take in knowledge from the early and late-time waveform evolution to\naccurately forecast the latter part of numerical relativity waveforms. This\nwork aims to accelerate the creation of scalable, computationally efficient and\ninterpretable artificial intelligence models for gravitational wave\nastrophysics.",
    "descriptor": "\nComments: 17 pages, 7 figures, 1 appendix\n",
    "authors": [
      "Asad Khan",
      "E. A. Huerta",
      "Huihuo Zheng"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06968"
  },
  {
    "id": "arXiv:2110.06975",
    "title": "Guided Policy Search using Sequential Convex Programming for  Initialization of Trajectory Optimization Algorithms",
    "abstract": "Nonlinear trajectory optimization algorithms have been developed to handle\noptimal control problems with nonlinear dynamics and nonconvex constraints in\ntrajectory planning. The performance and computational efficiency of many\ntrajectory optimization methods are sensitive to the initial guess, i.e., the\ntrajectory guess needed by the recursive trajectory optimization algorithm.\nMotivated by this observation, we tackle the initialization problem for\ntrajectory optimization via policy optimization. To optimize a policy, we\npropose a guided policy search method that has two key components: i)\nTrajectory update; ii) Policy update. The trajectory update involves offline\nsolutions of a large number of trajectory optimization problems from different\ninitial states via Sequential Convex Programming (SCP). Here we take a single\nSCP step to generate the trajectory iterate for each problem. In conjunction\nwith these iterates, we also generate additional trajectories around each\niterate via a feedback control law. Then all these trajectories are used by a\nstochastic gradient descent algorithm to update the neural network policy,\ni.e., the policy update step. As a result, the trained policy makes it possible\nto generate trajectory candidates that are close to the optimality and\nfeasibility and that provide excellent initial guesses for the trajectory\noptimization methods. We validate the proposed method via a real-world\n6-degree-of-freedom powered descent guidance problem for a reusable rocket.",
    "descriptor": "\nComments: Initial submission to American Control Conference (ACC) 2022\n",
    "authors": [
      "Taewan Kim",
      "Purnanand Elango",
      "Danylo Malyuta",
      "Behcet Acikmese"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.06975"
  },
  {
    "id": "arXiv:2110.06996",
    "title": "A Novel Clustering-Based Algorithm for Continuous and Non-invasive  Cuff-Less Blood Pressure Estimation",
    "abstract": "Continuous blood pressure (BP) measurements can reflect a bodys response to\ndiseases and serve as a predictor of cardiovascular and other health\nconditions. While current cuff-based BP measurement methods are incapable of\nproviding continuous BP readings, invasive BP monitoring methods also tend to\ncause patient dissatisfaction and can potentially cause infection. In this\nresearch, we developed a method for estimating blood pressure based on the\nfeatures extracted from Electrocardiogram (ECG) and Photoplethysmogram (PPG)\nsignals and the Arterial Blood Pressure (ABP) data. The vector of features\nextracted from the preprocessed ECG and PPG signals is used in this approach,\nwhich include Pulse Transit Time (PTT), PPG Intensity Ratio (PIR), and Heart\nRate (HR), as the input of a clustering algorithm and then developing separate\nregression models like Random Forest Regression, Gradient Boosting Regression,\nand Multilayer Perceptron Regression algorithms for each resulting cluster. We\nevaluated and compared the findings to create the model with the highest\naccuracy by applying the clustering approach and identifying the optimal number\nof clusters, and eventually the acceptable prediction model. The paper compares\nthe results obtained with and without this clustering. The results show that\nthe proposed clustering approach helps obtain more accurate estimates of\nSystolic Blood Pressure (SBP) and Diastolic Blood Pressure (DBP). Given the\ninconsistency, high dispersion, and multitude of trends in the datasets for\ndifferent features, using the clustering approach improved the estimation\naccuracy by 50-60%.",
    "descriptor": "",
    "authors": [
      "Ali Farki",
      "Reza Baradaran Kazemzadeh",
      "Elham Akhondzadeh Noughabi"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.06996"
  },
  {
    "id": "arXiv:2110.06998",
    "title": "Refining bridge-block decompositions through two-stage and recursive  tree partitioning",
    "abstract": "In transmission networks power flows and network topology are deeply\nintertwined due to power flow physics. Recent literature shows that specific\nnetwork substructures named bridge-blocks prevent line failures from\npropagating globally. A two-stage and recursive tree partitioning approach have\nbeen proposed to create more bridge-blocks in transmission networks, improving\ntheir robustness against cascading line failures. In this paper we consider the\nproblem of refining the bridge-block decomposition of a given power network\nwith minimal impact on the maximum congestion. We propose two new solution\nmethods, depending on the preferred power flow model. More specifically, (i) we\nintroduce a novel MILP-based approach that uses the DC approximation to solve\nmore efficiently the second-stage optimization problem of the two-stage\napproach and (ii) we show how the existing recursive approach can be extended\nto work with AC power flows, drastically improving the running times when\ncompared to the pre-existing AC-based two-stage method.",
    "descriptor": "\nComments: Submitted to the 22nd Power Systems Computation Conference (PSCC 2022)\n",
    "authors": [
      "Leon Lan",
      "Alessandro Zocca"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.06998"
  },
  {
    "id": "arXiv:2110.07000",
    "title": "An MILP-based approach to tree partitioning with minimal power flow  disruption and generator coherency constraints",
    "abstract": "Tree partitioning has recently been proposed in the power systems literature\nas a less severe alternative to controlled islanding. In this paper, we\nformulate an optimization problem to tree partition a network with minimal\npower flow disruption and generator coherency constraints. We propose a\nsingle-stage MILP formulation to compute optimal solutions. Numerical\nexperiments show that our MILP-based approach drastically decreases the power\nflow disruption when compared to an earlier proposed two-stage approach based\non spectral clustering. Moreover, using a search space reduction procedure\nbased on the Steiner Tree problem, the MILP-based approach computes\nnear-optimal tree partitions in sub-second time for instances up to 500 buses.",
    "descriptor": "\nComments: Submitted to the 22nd Power Systems Computation Conference (PSCC 2022)\n",
    "authors": [
      "Leon Lan",
      "Alessandro Zocca"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07000"
  },
  {
    "id": "arXiv:2110.07024",
    "title": "Stability and Efficiency of Random Serial Dictatorship",
    "abstract": "This paper establishes non-asymptotic convergence of the cutoffs in Random\nserial dictatorship in an environment with many students, many schools, and\narbitrary student preferences. Convergence is shown to hold when the number of\nschools, $m$, and the number of students, $n$, satisfy the relation $m \\ln m\n\\ll n$, and we provide an example showing that this result is sharp.\nWe differ significantly from prior work in the mechanism design literature in\nour use of analytic tools from randomized algorithms and discrete probability,\nwhich allow us to show concentration of the RSD lottery probabilities and\ncutoffs even against adversarial student preferences.",
    "descriptor": "",
    "authors": [
      "Suhas Vijaykumar"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.07024"
  },
  {
    "id": "arXiv:2110.07053",
    "title": "Robust MIMO Detection using Hypernetworks with Learned Regularizers",
    "abstract": "Optimal symbol detection in multiple-input multiple-output (MIMO) systems is\nknown to be an NP-hard problem. Recently, there has been a growing interest to\nget reasonably close to the optimal solution using neural networks while\nkeeping the computational complexity in check. However, existing work based on\ndeep learning shows that it is difficult to design a generic network that works\nwell for a variety of channels. In this work, we propose a method that tries to\nstrike a balance between symbol error rate (SER) performance and generality of\nchannels. Our method is based on hypernetworks that generate the parameters of\na neural network-based detector that works well on a specific channel. We\npropose a general framework by regularizing the training of the hypernetwork\nwith some pre-trained instances of the channel-specific method. Through\nnumerical experiments, we show that our proposed method yields high performance\nfor a set of prespecified channel realizations while generalizing well to all\nchannels drawn from a specific distribution.",
    "descriptor": "",
    "authors": [
      "Nicolas Zilberstein",
      "Chris Dick",
      "Rahman Doost-Mohammady",
      "Ashutosh Sabharwal",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07053"
  },
  {
    "id": "arXiv:2110.07055",
    "title": "Continual learning using lattice-free MMI for speech recognition",
    "abstract": "Continual learning (CL), or domain expansion, recently became a popular topic\nfor automatic speech recognition (ASR) acoustic modeling because practical\nsystems have to be updated frequently in order to work robustly on types of\nspeech not observed during initial training. While sequential adaptation allows\ntuning a system to a new domain, it may result in performance degradation on\nthe old domains due to catastrophic forgetting. In this work we explore\nregularization-based CL for neural network acoustic models trained with the\nlattice-free maximum mutual information (LF-MMI) criterion. We simulate domain\nexpansion by incrementally adapting the acoustic model on different public\ndatasets that include several accents and speaking styles. We investigate two\nwell-known CL techniques, elastic weight consolidation (EWC) and learning\nwithout forgetting (LWF), which aim to reduce forgetting by preserving model\nweights or network outputs. We additionally introduce a sequence-level LWF\nregularization, which exploits posteriors from the denominator graph of LF-MMI\nto further reduce forgetting. Empirical results show that the proposed\nsequence-level LWF can improve the best average word error rate across all\ndomains by up to 9.4% relative compared with using regular LWF.",
    "descriptor": "\nComments: Submitted to ICASSP 2022 - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Copyright 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses\n",
    "authors": [
      "Hossein Hadian",
      "Arseniy Gorin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07055"
  },
  {
    "id": "arXiv:2110.07057",
    "title": "High-throughput Phenotyping of Nematode Cysts",
    "abstract": "The beet cyst nematode (BCN) Heterodera schachtii is a plant pest responsible\nfor crop loss on a global scale. Here, we introduce a high-throughput system\nbased on computer vision that allows quantifying BCN infestation and\ncharacterizing nematode cysts through phenotyping. After recording microscopic\nimages of soil extracts in a standardized setting, an instance segmentation\nalgorithm serves to detect nematode cysts in these samples. Going beyond fast\nand precise cyst counting, the image-based approach enables quantification of\ncyst density and phenotyping of morphological features of cysts under different\nconditions, providing the basis for high-throughput applications in agriculture\nand plant breeding research.",
    "descriptor": "",
    "authors": [
      "Long Chen",
      "Matthias Daub",
      "Hans-Georg Luigs",
      "Marcus Jansen",
      "Martin Strauch",
      "Dorit Merhof"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07057"
  },
  {
    "id": "arXiv:2110.07069",
    "title": "CloudPred: Predicting Patient Phenotypes From Single-cell RNA-seq",
    "abstract": "Single-cell RNA sequencing (scRNA-seq) has the potential to provide powerful,\nhigh-resolution signatures to inform disease prognosis and precision medicine.\nThis paper takes an important first step towards this goal by developing an\ninterpretable machine learning algorithm, CloudPred, to predict individuals'\ndisease phenotypes from their scRNA-seq data. Predicting phenotype from\nscRNA-seq is challenging for standard machine learning methods -- the number of\ncells measured can vary by orders of magnitude across individuals and the cell\npopulations are also highly heterogeneous. Typical analysis creates pseudo-bulk\nsamples which are biased toward prior annotations and also lose the single cell\nresolution. CloudPred addresses these challenges via a novel end-to-end\ndifferentiable learning algorithm which is coupled with a biologically informed\nmixture of cell types model. CloudPred automatically infers the cell\nsubpopulation that are salient for the phenotype without prior annotations. We\ndeveloped a systematic simulation platform to evaluate the performance of\nCloudPred and several alternative methods we propose, and find that CloudPred\noutperforms the alternative methods across several settings. We further\nvalidated CloudPred on a real scRNA-seq dataset of 142 lupus patients and\ncontrols. CloudPred achieves AUROC of 0.98 while identifying a specific\nsubpopulation of CD4 T cells whose presence is highly indicative of lupus.\nCloudPred is a powerful new framework to predict clinical phenotypes from\nscRNA-seq data and to identify relevant cells.",
    "descriptor": "\nComments: Preprint of an article published in Pacific Symposium on Biocomputing \\copyright\\ 2021 World Scientific Publishing Co., Singapore, this http URL\n",
    "authors": [
      "Bryan He",
      "Matthew Thomson",
      "Meena Subramaniam",
      "Richard Perez",
      "Chun Jimmie Ye",
      "James Zou"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07069"
  },
  {
    "id": "arXiv:2110.07098",
    "title": "Escaping Saddle Points in Nonconvex Minimax Optimization via  Cubic-Regularized Gradient Descent-Ascent",
    "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve\nnonconvex minimax optimization problems. However, the existing GDA-type\nalgorithms can only find first-order stationary points of the envelope function\nof nonconvex minimax optimization problems, which does not rule out the\npossibility to get stuck at suboptimal saddle points. In this paper, we develop\nCubic-GDA -- the first GDA-type algorithm for escaping strict saddle points in\nnonconvex-strongly-concave minimax optimization. Specifically, the algorithm\nuses gradient ascent to estimate the second-order information of the minimax\nobjective function, and it leverages the cubic regularization technique to\nefficiently escape the strict saddle points. Under standard smoothness\nassumptions on the objective function, we show that Cubic-GDA admits an\nintrinsic potential function whose value monotonically decreases in the minimax\noptimization process. Such a property leads to a desired global convergence of\nCubic-GDA to a second-order stationary point at a sublinear rate. Moreover, we\nanalyze the convergence rate of Cubic-GDA in the full spectrum of a gradient\ndominant-type nonconvex geometry. Our result shows that Cubic-GDA achieves an\norderwise faster convergence rate than the standard GDA for a wide spectrum of\ngradient dominant geometry. Our study bridges minimax optimization with\nsecond-order optimization and may inspire new developments along this\ndirection.",
    "descriptor": "\nComments: 23 pages, no figures. arXiv admin note: text overlap with arXiv:2102.04653\n",
    "authors": [
      "Ziyi Chen",
      "Yi Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07098"
  },
  {
    "id": "arXiv:2110.07106",
    "title": "A Robotic Antenna Alignment and Tracking System for Millimeter Wave  Propagation Modeling",
    "abstract": "In this paper, we discuss the design of a sliding-correlator channel sounder\nfor 28 GHz propagation modeling on the NSF POWDER testbed in Salt Lake City,\nUT. Beam-alignment is mechanically achieved via a fully autonomous robotic\nantenna tracking platform, designed using commercial off-the-shelf components.\nEquipped with an Apache Zookeeper/Kafka managed fault-tolerant\npublish-subscribe framework, we demonstrate tracking response times of 27.8 ms,\nin addition to superior scalability over state-of-the-art mechanical\nbeam-steering systems. Enhanced with real-time kinematic correction streams,\nour geo-positioning subsystem achieves a 3D accuracy of 17 cm, while our\nprincipal axes positioning subsystem achieves an average accuracy of 1.1\ndegrees across yaw and pitch movements. Finally, by facilitating remote\norchestration (via managed containers), uninhibited rotation (via\nencapsulation), and real-time positioning visualization (via Dash/MapBox), we\nexhibit a proven prototype well-suited for V2X measurements.",
    "descriptor": "\nComments: Submitted to -- and yet to be presented (and archived) -- in the proceedings of the 2022 USNC-URSI National Radio Science Meeting (NRSM)\n",
    "authors": [
      "Bharath Keshavamurthy",
      "Yaguang Zhang",
      "Christopher R. Anderson",
      "Nicolo Michelusi",
      "James V. Krogmeier",
      "David J. Love"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07106"
  },
  {
    "id": "arXiv:2110.07112",
    "title": "On the Sample Complexity of Decentralized Linear Quadratic Regulator  with Partially Nested Information Structure",
    "abstract": "We study the problem of control policy design for decentralized\nstate-feedback linear quadratic control with a partially nested information\nstructure, when the system model is unknown. We propose a model-based learning\nsolution, which consists of two steps. First, we estimate the unknown system\nmodel from a single system trajectory of finite length, using least squares\nestimation. Next, based on the estimated system model, we design a control\npolicy that satisfies the desired information structure. We show that the\nsuboptimality gap between our control policy and the optimal decentralized\ncontrol policy (designed using accurate knowledge of the system model) scales\nlinearly with the estimation error of the system model. Using this result, we\nprovide an end-to-end sample complexity result for learning decentralized\ncontrollers for a linear quadratic control problem with a partially nested\ninformation structure.",
    "descriptor": "",
    "authors": [
      "Lintao Ye",
      "Hao Zhu",
      "Vijay Gupta"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07112"
  },
  {
    "id": "arXiv:2110.07116",
    "title": "Auxiliary Loss of Transformer with Residual Connection for End-to-End  Speaker Diarization",
    "abstract": "End-to-end neural diarization (EEND) with self-attention directly predicts\nspeaker labels from inputs and enables the handling of overlapped speech.\nAlthough the EEND outperforms clustering-based speaker diarization (SD), it\ncannot be further improved by simply increasing the number of encoder blocks\nbecause the last encoder block is dominantly supervised compared with lower\nblocks. This paper proposes a new residual auxiliary EEND (RX-EEND) learning\narchitecture for transformers to enforce the lower encoder blocks to learn more\naccurately. The auxiliary loss is applied to the output of each encoder block,\nincluding the last encoder block. The effect of auxiliary loss on the learning\nof the encoder blocks can be further increased by adding a residual connection\nbetween the encoder blocks of the EEND. Performance evaluation and ablation\nstudy reveal that the auxiliary loss in the proposed RX-EEND provides relative\nreductions in the diarization error rate (DER) by 50.3% and 21.0% on the\nsimulated and CALLHOME (CH) datasets, respectively, compared with\nself-attentive EEND (SA-EEND). Furthermore, the residual connection used in\nRX-EEND further relatively reduces the DER by 8.1% for CH dataset.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Yechan Yu",
      "Dongkeon Park",
      "Hong Kook Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07116"
  },
  {
    "id": "arXiv:2110.07117",
    "title": "FAIR Data Pipeline: provenance-driven data management for traceable  scientific workflows",
    "abstract": "Modern epidemiological analyses to understand and combat the spread of\ndisease depend critically on access to, and use of, data. Rapidly evolving\ndata, such as data streams changing during a disease outbreak, are particularly\nchallenging. Data management is further complicated by data being imprecisely\nidentified when used. Public trust in policy decisions resulting from such\nanalyses is easily damaged and is often low, with cynicism arising where claims\nof \"following the science\" are made without accompanying evidence. Tracing the\nprovenance of such decisions back through open software to primary data would\nclarify this evidence, enhancing the transparency of the decision-making\nprocess. Here, we demonstrate a Findable, Accessible, Interoperable and\nReusable (FAIR) data pipeline developed during the COVID-19 pandemic that\nallows easy annotation of data as they are consumed by analyses, while tracing\nthe provenance of scientific outputs back through the analytical source code to\ndata sources. Such a tool provides a mechanism for the public, and fellow\nscientists, to better assess the trust that should be placed in scientific\nevidence, while allowing scientists to support policy-makers in openly\njustifying their decisions. We believe that tools such as this should be\npromoted for use across all areas of policy-facing research.",
    "descriptor": "",
    "authors": [
      "Sonia Natalie Mitchell",
      "Andrew Lahiff",
      "Nathan Cummings",
      "Jonathan Hollocombe",
      "Bram Boskamp",
      "Dennis Reddyhoff",
      "Ryan Field",
      "Kristian Zarebski",
      "Antony Wilson",
      "Martin Burke",
      "Blair Archibald",
      "Paul Bessell",
      "Richard Blackwell",
      "Lisa A Boden",
      "Alys Brett",
      "Sam Brett",
      "Ruth Dundas",
      "Jessica Enright",
      "Alejandra N. Gonzalez-Beltran",
      "Claire Harris",
      "Ian Hinder",
      "Christopher David Hughes",
      "Martin Knight",
      "Vino Mano",
      "Ciaran McMonagle",
      "Dominic Mellor",
      "Sibylle Mohr",
      "Glenn Marion",
      "Louise Matthews",
      "Iain J. McKendrick",
      "Christopher Mark Pooley",
      "Thibaud Porphyre",
      "Aaron Reeves",
      "Edward Townsend",
      "Robert Turner",
      "Jeremy Walton",
      "Richard Reeve"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2110.07117"
  },
  {
    "id": "arXiv:2110.07124",
    "title": "Multi-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same  Class with Auxiliary Duplicating Permutation Invariant Training",
    "abstract": "Sound event localization and detection (SELD) involves identifying the\ndirection-of-arrival (DOA) and the event class. The SELD methods with a\nclass-wise output format make the model predict activities of all sound event\nclasses and corresponding locations. The class-wise methods can output\nactivity-coupled Cartesian DOA (ACCDOA) vectors, which enable us to solve a\nSELD task with a single target using a single network. However, there is still\na challenge in detecting the same event class from multiple locations. To\novercome this problem while maintaining the advantages of the class-wise\nformat, we extended ACCDOA to a multi one and proposed auxiliary duplicating\npermutation invariant training (ADPIT). The multi- ACCDOA format (a class- and\ntrack-wise output format) enables the model to solve the cases with overlaps\nfrom the same class. The class-wise ADPIT scheme enables each track of the\nmulti-ACCDOA format to learn with the same target as the single-ACCDOA format.\nIn evaluations with the DCASE 2021 Task 3 dataset, the model trained with the\nmulti-ACCDOA format and with the class-wise ADPIT detects overlapping events\nfrom the same class while maintaining its performance in the other cases. Also,\nthe proposed method performed comparably to state-of-the-art SELD methods with\nfewer parameters.",
    "descriptor": "\nComments: 5 pages, 3 figures, submitted to IEEE ICASSP 2022\n",
    "authors": [
      "Kazuki Shimada",
      "Yuichiro Koyama",
      "Shusuke Takahashi",
      "Naoya Takahashi",
      "Emiru Tsunoo",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07124"
  },
  {
    "id": "arXiv:2110.07136",
    "title": "Federated Learning for COVID-19 Detection with Generative Adversarial  Networks in Edge Cloud Computing",
    "abstract": "COVID-19 has spread rapidly across the globe and become a deadly pandemic.\nRecently, many artificial intelligence-based approaches have been used for\nCOVID-19 detection, but they often require public data sharing with cloud\ndatacentres and thus remain privacy concerns. This paper proposes a new\nfederated learning scheme, called FedGAN, to generate realistic COVID-19 images\nfor facilitating privacy-enhanced COVID-19 detection with generative\nadversarial networks (GANs) in edge cloud computing. Particularly, we first\npropose a GAN where a discriminator and a generator based on convolutional\nneural networks (CNNs) at each edge-based medical institution alternatively are\ntrained to mimic the real COVID-19 data distribution. Then, we propose a new\nfederated learning solution which allows local GANs to collaborate and exchange\nlearned parameters with a cloud server, aiming to enrich the global GAN model\nfor generating realistic COVID-19 images without the need for sharing actual\ndata. To enhance the privacy in federated COVID-19 data analytics, we integrate\na differential privacy solution at each hospital institution. Moreover, we\npropose a new blockchain-based FedGAN framework for secure COVID-19 data\nanalytics, by decentralizing the FL process with a new mining solution for low\nrunning latency. Simulations results demonstrate the superiority of our\napproach for COVID-19 detection over the state-of-the-art schemes.",
    "descriptor": "\nComments: Accepted at IEEE Internet of Things Journal, 14 pages\n",
    "authors": [
      "Dinh C. Nguyen",
      "Ming Ding",
      "Pubudu N. Pathirana",
      "Aruna Seneviratne",
      "Albert Y. Zomaya"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07136"
  },
  {
    "id": "arXiv:2110.07147",
    "title": "Unsupervised Data-Driven Nuclei Segmentation For Histology Images",
    "abstract": "An unsupervised data-driven nuclei segmentation method for histology images,\ncalled CBM, is proposed in this work. CBM consists of three modules applied in\na block-wise manner: 1) data-driven color transform for energy compaction and\ndimension reduction, 2) data-driven binarization, and 3) incorporation of\ngeometric priors with morphological processing. CBM comes from the first letter\nof the three modules - \"Color transform\", \"Binarization\" and \"Morphological\nprocessing\". Experiments on the MoNuSeg dataset validate the effectiveness of\nthe proposed CBM method. CBM outperforms all other unsupervised methods and\noffers a competitive standing among supervised models based on the Aggregated\nJaccard Index (AJI) metric.",
    "descriptor": "\nComments: 5 pages, 4 figures, 3 tables\n",
    "authors": [
      "Vasileios Magoulianitis",
      "Peida Han",
      "Yijing Yang",
      "C.-C. Jay Kuo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.07147"
  },
  {
    "id": "arXiv:2110.07185",
    "title": "VLBInet: Radio Interferometry Data Classification for EHT with Neural  Networks",
    "abstract": "The Event Horizon Telescope (EHT) recently released the first horizon-scale\nimages of the black hole in M87. Combined with other astronomical data, these\nimages constrain the mass and spin of the hole as well as the accretion rate\nand magnetic flux trapped on the hole. An important question for the EHT is how\nwell key parameters, such as trapped magnetic flux and the associated disk\nmodels, can be extracted from present and future EHT VLBI data products. The\nprocess of modeling visibilities and analyzing them is complicated by the fact\nthat the data are sparsely sampled in the Fourier domain while most of the\ntheory/simulation is constructed in the image domain. Here we propose a\ndata-driven approach to analyze complex visibilities and closure quantities for\nradio interferometric data with neural networks. Using mock interferometric\ndata, we show that our neural networks are able to infer the accretion state as\neither high magnetic flux (MAD) or low magnetic flux (SANE), suggesting that it\nis possible to perform parameter extraction directly in the visibility domain\nwithout image reconstruction. We have applied VLBInet to real M87 EHT data\ntaken on four different days in 2017 (April 5, 6, 10, 11), and our neural\nnetworks give a score prediction 0.52, 0.4, 0.43, 0.76 for each day, with an\naverage score 0.53, which shows no significant indication for the data to lean\ntoward either the MAD or SANE state.",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Joshua Yao-Yu Lin",
      "Dominic W. Pesce",
      "George N. Wong",
      "Ajay Uppili Arasanipalai",
      "Ben S. Prather",
      "Charles F. Gammie"
    ],
    "subjectives": [
      "High Energy Astrophysical Phenomena (astro-ph.HE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07185"
  },
  {
    "id": "arXiv:2110.07191",
    "title": "CNN-DST: ensemble deep learning based on Dempster-Shafer theory for  vibration-based fault recognition",
    "abstract": "Nowadays, using vibration data in conjunction with pattern recognition\nmethods is one of the most common fault detection strategies for structures.\nHowever, their performances depend on the features extracted from vibration\ndata, the features selected to train the classifier, and the classifier used\nfor pattern recognition. Deep learning facilitates the fault detection\nprocedure by automating the feature extraction and selection, and\nclassification procedure. Though, deep learning approaches have challenges in\ndesigning its structure and tuning its hyperparameters, which may result in a\nlow generalization capability. Therefore, this study proposes an ensemble deep\nlearning framework based on a convolutional neural network (CNN) and\nDempster-Shafer theory (DST), called CNN-DST. In this framework, several CNNs\nwith the proposed structure are first trained, and then, the outputs of the\nCNNs selected by the proposed technique are combined by using an improved\nDST-based method. To validate the proposed CNN-DST framework, it is applied to\nan experimental dataset created by the broadband vibrational responses of\npolycrystalline Nickel alloy first-stage turbine blades with different types\nand severities of damage. Through statistical analysis, it is shown that the\nproposed CNN-DST framework classifies the turbine blades with an average\nprediction accuracy of 97.19%. The proposed CNN-DST framework is benchmarked\nwith other state-of-the-art classification methods, demonstrating its high\nperformance. The robustness of the proposed CNN-DST framework with respect to\nmeasurement noise is investigated, showing its high noise-resistance. Further,\nbandwidth analysis reveals that most of the required information for detecting\nfaulty samples is available in a small frequency range.",
    "descriptor": "",
    "authors": [
      "Vahid Yaghoubi",
      "Liangliang Cheng",
      "Wim Van Paepegem",
      "Mathias Kersemans"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07191"
  },
  {
    "id": "arXiv:2110.07192",
    "title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual  Text-to-Speech",
    "abstract": "In this paper, we present a FastPitch-based non-autoregressive cross-lingual\nText-to-Speech (TTS) model built with language independent input representation\nand monolingual force aligners. We propose a phoneme length regulator that\nsolves the length mismatch problem between language-independent phonemes and\nmonolingual alignment results. Our experiments show that (1) an increasing\nnumber of training speakers encourages non-autoregressive cross-lingual TTS\nmodel to disentangle speaker and language representations, and (2) variance\nadaptors of FastPitch model can help disentangle speaker identity from learned\nrepresentations in cross-lingual TTS. The subjective evaluation shows that our\nproposed model is able to achieve decent speaker consistency and similarity. We\nfurther improve the naturalness of Mandarin-dominated mixed-lingual utterances\nby utilizing the controllability of our proposed model.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Haoyue Zhan",
      "Xinyuan Yu",
      "Haitong Zhang",
      "Yang Zhang",
      "Yue Lin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07192"
  },
  {
    "id": "arXiv:2110.07205",
    "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-training for Spoken Language  Processing",
    "abstract": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-training natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the speech/text input through\nthe pre-nets, the shared encoder-decoder network models the sequence to\nsequence transformation, and then the post-nets generate the output in the\nspeech/text modality based on the decoder output. Particularly, SpeechT5 can\npre-train on a large scale of unlabeled speech and text data to improve the\ncapability of the speech and textual modeling. To align the textual and speech\ninformation into a unified semantic space, we propose a cross-modal vector\nquantization method with random mixing-up to bridge speech and text. Extensive\nevaluations on a wide variety of spoken language processing tasks, including\nvoice conversion, automatic speech recognition, text to speech, and speaker\nidentification, show the superiority of the proposed SpeechT5 framework.",
    "descriptor": "\nComments: work in process\n",
    "authors": [
      "Junyi Ao",
      "Rui Wang",
      "Long Zhou",
      "Shujie Liu",
      "Shuo Ren",
      "Yu Wu",
      "Tom Ko",
      "Qing Li",
      "Yu Zhang",
      "Zhihua Wei",
      "Yao Qian",
      "Jinyu Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07205"
  },
  {
    "id": "arXiv:2110.07211",
    "title": "Control-oriented Modeling of Bend Propagation in an Octopus Arm",
    "abstract": "Bend propagation in an octopus arm refers to a stereotypical maneuver whereby\nan octopus pushes a bend (localized region of large curvature) from the base to\nthe tip of the arm. Bend propagation arises from the complex interplay between\nmechanics of the flexible arm, forces generated by internal muscles, and\nenvironmental effects (buoyancy and drag) from of the surrounding fluid. In\npart due to this complexity, much of prior modeling and analysis work has\nrelied on the use of high dimensional computational models. The contribution of\nthis paper is to present a control-oriented reduced order model based upon a\nnovel parametrization of the curvature of the octopus arm. The parametrization\nis motivated by the experimental results. The reduced order model is related to\nand derived from a computational model which is also presented. The results\nfrom the two sets of models are compared using numerical simulations which is\nshown to lead to useful qualitative insights into bend propagation. A\ncomparison between the reduced order model and experimental data is also\nreported.",
    "descriptor": "",
    "authors": [
      "Tixian Wang",
      "Udit Halder",
      "Ekaterina Gribkova",
      "Mattia Gazzola",
      "Prashant G. Mehta"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07211"
  },
  {
    "id": "arXiv:2110.07216",
    "title": "FedSpeech: Federated Text-to-Speech with Continual Learning",
    "abstract": "Federated learning enables collaborative training of machine learning models\nunder strict privacy restrictions and federated text-to-speech aims to\nsynthesize natural speech of multiple users with a few audio training samples\nstored in their devices locally. However, federated text-to-speech faces\nseveral challenges: very few training samples from each speaker are available,\ntraining samples are all stored in local device of each user, and global model\nis vulnerable to various attacks. In this paper, we propose a novel federated\nlearning architecture based on continual learning approaches to overcome the\ndifficulties above. Specifically, 1) we use gradual pruning masks to isolate\nparameters for preserving speakers' tones; 2) we apply selective masks for\neffectively reusing knowledge from tasks; 3) a private speaker embedding is\nintroduced to keep users' privacy. Experiments on a reduced VCTK dataset\ndemonstrate the effectiveness of FedSpeech: it nearly matches multi-task\ntraining in terms of multi-speaker speech quality; moreover, it sufficiently\nretains the speakers' tones and even outperforms the multi-task training in the\nspeaker similarity experiment.",
    "descriptor": "\nComments: Accepted by IJCAI 2021\n",
    "authors": [
      "Ziyue Jiang",
      "Yi Ren",
      "Ming Lei",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07216"
  },
  {
    "id": "arXiv:2110.07217",
    "title": "Alpine Permafrost Modeling: On the influence of topography driven  lateral fluxes",
    "abstract": "Alpine permafrost environments are highly vulnerable and sensitive to changes\nin regional and global climate trends. Thawing and degradation of permafrost\nhas numerous adverse environmental, economic, and societal impacts.\nMathematical modeling and numerical simulations provide powerful tools for\npredicting the degree of degradation and evolution of subsurface permafrost as\na result of global warming. A particularly significant characteristic of alpine\nenvironments is the high variability in their topography and geomorphology\nwhich drives large lateral thermal and fluid fluxes. Additionally, harsh winds,\nextreme weather conditions, and various degrees of saturation have to be\nconsidered. The combination of large lateral fluxes and unsaturated ground\nmakes alpine systems markedly different from Arctic permafrost environments and\ngeneral geotechnical ground freezing applications, and therefore, alpine\npermafrost demands its own specialized modeling approaches. In this research\nwork, we present a multi-physics permafrost model tailored to alpine regions.\nIn particular, we resolve the ice-water phase transitions, unsaturated\nconditions, and capillary actions, and account for the impact of the evolving\npore volume on fluid-matrix interactions. Moreover, the approach is\nmulti-dimensional, and therefore, inherently resolves fluxes along topographic\ngradients. Through numerical cases studies based on the elevation profiles of\nthe two prominent peaks of the Zugspitze (DE) and the Matterhorn (CH), we show\nthe strong influence of topography driven thermal and fluid fluxes on active\nlayer dynamics and the distribution of permafrost.",
    "descriptor": "\nComments: 20 pages (without appendix), 9 figures\n",
    "authors": [
      "Jonas Beddrich",
      "Shubhangi Gupta",
      "Barbara Wohlmuth",
      "Gabriele Chiogna"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07217"
  },
  {
    "id": "arXiv:2110.07221",
    "title": "Learning a Compressive Sensing Matrix with Structural Constraints via  Maximum Mean Discrepancy Optimization",
    "abstract": "We introduce a learning-based algorithm to obtain a measurement matrix for\ncompressive sensing related recovery problems. The focus lies on matrices with\na constant modulus constraint which typically represent a network of analog\nphase shifters in hybrid precoding/combining architectures. We interpret a\nmatrix with restricted isometry property as a mapping of points from a high- to\na low-dimensional hypersphere. We argue that points on the low-dimensional\nhypersphere, namely, in the range of the matrix, should be uniformly\ndistributed to increase robustness against measurement noise. This notion is\nformalized in an optimization problem which uses one of the maximum mean\ndiscrepancy metrics in the objective function. Recent success of such metrics\nin neural network related topics motivate a solution of the problem based on\nmachine learning. Numerical experiments show better performance than random\nmeasurement matrices that are generally employed in compressive sensing\ncontexts. Further, we adapt a method from the literature to the constant\nmodulus constraint. This method can also compete with random matrices and it is\nshown to harmonize well with the proposed learning-based approach if it is used\nas an initialization. Lastly, we describe how other structural matrix\nconstraints, e.g., a Toeplitz constraint, can be taken into account, too.",
    "descriptor": "",
    "authors": [
      "Michael Koller",
      "Wolfgang Utschick"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07221"
  },
  {
    "id": "arXiv:2110.07234",
    "title": "On the Stability of Low Pass Graph Filter With a Large Number of Edge  Rewires",
    "abstract": "Recently, the stability of graph filters has been studied as one of the key\ntheoretical properties driving the highly successful graph convolutional neural\nnetworks (GCNs). The stability of a graph filter characterizes the effect of\ntopology perturbation on the output of a graph filter, a fundamental building\nblock for GCNs. Many existing results have focused on the regime of small\nperturbation with a small number of edge rewires. However, the number of edge\nrewires can be large in many applications. To study the latter case, this work\ndeparts from the previous analysis and proves a bound on the stability of graph\nfilter relying on the filter's frequency response. Assuming the graph filter is\nlow pass, we show that the stability of the filter depends on perturbation to\nthe community structure. As an application, we show that for stochastic block\nmodel graphs, the graph filter distance converges to zero when the number of\nnodes approaches infinity. Numerical simulations validate our findings.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Hoang-Son Nguyen",
      "Yiran He",
      "Hoi-To Wai"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07234"
  },
  {
    "id": "arXiv:2110.07239",
    "title": "Solving Large Break Minimization Problems in a Mirrored Double  Round-robin Tournament Using Quantum Annealing",
    "abstract": "Quantum annealing (QA) has gained considerable attention because it can be\napplied to combinatorial optimization problems, which have numerous\napplications in logistics, scheduling, and finance. In recent years, research\non solving practical combinatorial optimization problems using them has\naccelerated. However, researchers struggle to find practical combinatorial\noptimization problems, for which quantum annealers outperform other\nmathematical optimization solvers. Moreover, there are only a few studies that\ncompare the performance of quantum annealers with one of the most sophisticated\nmathematical optimization solvers, such as Gurobi and CPLEX. In our study, we\ndetermine that QA demonstrates better performance than the solvers in the break\nminimization problem in a mirrored double round-robin tournament (MDRRT). We\nalso explain the desirable performance of QA for the sparse interaction between\nvariables and a problem without constraints. In this process, we demonstrate\nthat the break minimization problem in an MDRRT can be expressed as a 4-regular\ngraph. Through computational experiments, we solve this problem using our QA\napproach and two-integer programming approaches, which were performed using the\nlatest quantum annealer D-Wave Advantage, and the sophisticated mathematical\noptimization solver, Gurobi, respectively. Further, we compare the quality of\nthe solutions and the computational time. QA was able to determine the exact\nsolution in 0.05 seconds for problems with 20 teams, which is a practical size.\nIn the case of 36 teams, it took 84.8 s for the integer programming method to\nreach the objective function value, which was obtained by the quantum annealer\nin 0.05 s. These results not only present the break minimization problem in an\nMDRRT as an example of applying QA to practical optimization problems, but also\ncontribute to find problems that can be effectively solved by QA.",
    "descriptor": "\nComments: 12pages, 2 figures\n",
    "authors": [
      "Michiya Kuramata",
      "Ryota Katsuki",
      "Kazuhide Nakata"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.07239"
  },
  {
    "id": "arXiv:2110.07320",
    "title": "Quantum R\u00e9nyi divergences and the strong converse exponent of state  discrimination in operator algebras",
    "abstract": "The sandwiched R\\'enyi $\\alpha$-divergences of two finite-dimensional quantum\nstates play a distinguished role among the many quantum versions of R\\'enyi\ndivergences as the tight quantifiers of the trade-off between the two error\nprobabilities in the strong converse domain of state discrimination. In this\npaper we show the same for the sandwiched R\\'enyi divergences of two normal\nstates on an injective von Neumann algebra, thereby establishing the\noperational significance of these quantities. Moreover, we show that in this\nsetting, again similarly to the finite-dimensional case, the sandwiched R\\'enyi\ndivergences coincide with the regularized measured R\\'enyi divergences, another\ndistinctive feature of the former quantities. Our main tool is an approximation\ntheorem (martingale convergence) for the sandwiched R\\'enyi divergences, which\nmay be used for the extension of various further results from the\nfinite-dimensional to the von Neumann algebra setting.\nWe also initiate the study of the sandwiched R\\'enyi divergences of pairs of\nstates on a $C^*$-algebra, and show that the above operational interpretation,\nas well as the equality to the regularized measured R\\'enyi divergence, holds\nmore generally for pairs of states on a nuclear $C^*$-algebra.",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Fumio Hiai",
      "Mil\u00e1n Mosonyi"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Mathematical Physics (math-ph)",
      "Operator Algebras (math.OA)"
    ],
    "url": "https://arxiv.org/abs/2110.07320"
  },
  {
    "id": "arXiv:2110.07323",
    "title": "Multidisciplinary Design Optimization Approach to Integrated Space  Mission Planning and Spacecraft Design",
    "abstract": "Space mission planning and spacecraft design are tightly coupled and need to\nbe considered together for optimal performance; however, this integrated\noptimization problem results in a large-scale Mixed-Integer Nonlinear\nProgramming (MINLP) problem, which is challenging to solve. In response to this\nchallenge, this paper proposes a new solution approach to this MINLP problem by\niterative solving a set of coupled subproblems via the augmented Lagrangian\ncoordination approach following the philosophy of Multi-disciplinary Design\nOptimization (MDO). The proposed approach leverages the unique structure of the\nproblem that enables its decomposition into a set of coupled subproblems of\ndifferent types: a Mixed-Integer Quadratic Programming (MIQP) subproblem for\nmission planning and one or more Nonlinear Programming (NLP) subproblem(s) for\nspacecraft design. Since specialized MIQP or NLP solvers can be applied to each\nsubproblem, the proposed approach can efficiently solve the otherwise\nintractable integrated MINLP problem. An automatic and effective method to find\nan initial solution for this iterative approach is also proposed so that the\noptimization can be performed without the need for a user-defined initial\nguess. In the demonstration case study, a human lunar exploration mission\nsequence is optimized with a subsystem-level parametric spacecraft design\nmodel. Compared to the state-of-the-art method, the proposed formulation can\nobtain a better solution with a shorter computational time even without\nparallelization. For larger problems, the proposed solution approach can also\nbe easily parallelizable and thus is expected to be further advantageous and\nscalable.",
    "descriptor": "\nComments: 22 pages, 4 figures, to be presented at the AIAA ASCEND conference 2021, to be submitted to the Journal of Spacecraft and Rockets\n",
    "authors": [
      "Masafumi Isaji",
      "Yuji Takubo",
      "Koki Ho"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07323"
  },
  {
    "id": "arXiv:2110.07353",
    "title": "Interpretable transformed ANOVA approximation on the example of the  prevention of forest fires",
    "abstract": "The distribution of data points is a key component in machine learning. In\nmost cases, one uses min-max normalization to obtain nodes in $[0,1]$ or\nZ-score normalization for standard normal distributed data. In this paper, we\napply transformation ideas in order to design a complete orthonormal system in\nthe $\\mathrm{L}_2$ space of functions with the standard normal distribution as\nintegration weight. Subsequently, we are able to apply the explainable ANOVA\napproximation for this basis and use Z-score transformed data in the method. We\ndemonstrate the applicability of this procedure on the well-known forest fires\ndata set from the UCI machine learning repository. The attribute ranking\nobtained from the ANOVA approximation provides us with crucial information\nabout which variables in the data set are the most important for the detection\nof fires.",
    "descriptor": "",
    "authors": [
      "Daniel Potts",
      "Michael Schmischke"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07353"
  },
  {
    "id": "arXiv:2110.07359",
    "title": "Energy Cooperative Transmission Policy for Energy Harvesting Tags",
    "abstract": "To sustain communication reliability and use the harvested energy\neffectively, it is crucial to consider energy sharing between energy harvesting\ntags (EHT) in a multiple access network, which are basic building blocks for\nthe internet of things (IoT) applications. This technique also achieves higher\nthroughput compared with the non-cooperative strategies despite energy losses\noccurred during energy transfer. We propose an energy cooperative communication\nstrategy for a multiple access network of tags that depends on the harvested\nbattery energy. We develop an optimal transmission policy for EHTs that\nmaximizes the long-term joint average throughput using a Markov decision\nprocess (MDP) model. Simulation results show that the proposed energy\ncooperative policy produces improved performance than traditional policies.",
    "descriptor": "\nComments: 6 pages, 5 figures, 2 tables, conference\n",
    "authors": [
      "Mengistu Abera Mulatu",
      "Thembelihle Dlamini",
      "Thokozani Shongwe",
      "Mzabalazo Lupupa"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07359"
  },
  {
    "id": "arXiv:2110.07360",
    "title": "Multi-center, multi-vendor automated segmentation of left ventricular  anatomy in contrast-enhanced MRI",
    "abstract": "Accurate delineation of the left ventricular boundaries in late\ngadolinium-enhanced magnetic resonance imaging (LGE-MRI) is an essential step\nfor scar tissue quantification and patient-specific assessment of myocardial\ninfarction. Many deep-learning techniques have been proposed to perform\nautomatic segmentations of the left ventricle (LV) in LGE-MRI showing\nsegmentations as accurate as those obtained by expert cardiologists. Thus far,\nthe existing models have been overwhelmingly developed and evaluated with\nLGE-MRI datasets from single clinical centers. However, in practice, LGE-MRI\nimages vary significantly between clinical centers within and across countries,\nin particular due to differences in the MRI scanners, imaging conditions,\ncontrast injection protocols and local clinical practise. This work\ninvestigates for the first time multi-center and multi-vendor LV segmentation\nin LGE-MRI, by proposing, implementing and evaluating in detail several\nstrategies to enhance model generalizability across clinical cites. These\ninclude data augmentation to artificially augment the image variability in the\ntraining sample, image harmonization to align the distributions of LGE-MRI\nimages across centers, and transfer learning to adjust existing single-center\nmodels to unseen images from new clinical sites. The results obtained based on\na new multi-center LGE-MRI dataset acquired in four clinical centers in Spain,\nFrance and China, show that the combination of data augmentation and transfer\nlearning can lead to single-center models that generalize well to new clinical\ncenters not included in the original training. The proposed framework shows the\npotential for developing clinical tools for automated LV segmentation in\nLGE-MRI that can be deployed in multiple clinical centers across distinct\ngeographical locations.",
    "descriptor": "",
    "authors": [
      "Carla Sendra-Balcells",
      "V\u00edctor M. Campello",
      "Carlos Mart\u00edn-Isla",
      "David Vilades Medel",
      "Mart\u00edn Lu\u00eds Descalzo",
      "Andrea Guala",
      "Jos\u00e9 F. Rodr\u00edguez Palomares",
      "Karim Lekadir"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07360"
  },
  {
    "id": "arXiv:2110.07403",
    "title": "A more direct and better variant of New Q-Newton's method Backtracking  for m equations in m variables",
    "abstract": "In this paper we propose a variant of New Q-Newton's method Backtracking\n(which is relevant to Levenberg-Marquardt algorithm, but with the argumentation\nchosen by New Q-Newton's method idea for good theoretical guarantee, and with\nBacktracking line search added) for to use specifically with systems of m\nequations in m variables.\nWe fix $\\delta _0=0 $ and $\\delta _1=2$, and a number $0<\\tau <1$. If $A$ is\na square matrix, we denote by $minsp(A)=\\min \\{|\\lambda |: $ $\\lambda$ is an\neigenvalue of $A\\}$. Also, we denote by $A^{\\intercal}$ the transpose of $A$.\nGiven $F:\\mathbb{R}^m\\rightarrow \\mathbb{R}^m$ a $C^1$ function, we denote by\n$H(x)=JF(x)$ the Jacobian of $F$, and $f(x)=||F(x)||^2$. If $x\\in \\mathbb{R}^m$\nis such that $F(x)\\not= 0$, we define various :\n$\\delta (x)$ be the first element $\\delta _j$ in $\\{\\delta _0,\\delta _1\\}$ so\nthat $minsp(H(x)+\\delta _j||F(x)||^{\\tau } )\\geq \\min \\{ ||F(x)||^{\\tau},1\\}$;\n$A(x)=H(x)^{\\intercal}H(x)+\\delta (x)||F(x)||^{\\tau }Id$;\n$w(x)=A(x)^{-1}H(x)^{\\intercal}F(x)$; (if $x$ is close to a non-degenerate\nzero of $F$, then $w(x)=H(x)^{-1}F(x)$ the usual Newton's direction) Note: we\ncan normalise $w(x)/\\max \\{||w(x)|| ,1\\}$ if needed\n$\\gamma (x)$ is chosen from Armijo's Backtracking line search: it is the\nlargest number $\\gamma$ among $\\{1,1/2,(1/2)^2,\\ldots \\}$ so that: $f(x-\\gamma\nw(x))-f(x)\\leq -\\gamma <w(x),H(x)^{\\intercal}F(x)>$;\nThe update rule of our method is $x\\mapsto x-\\gamma (x)w(x)$.\nGood theoretical guarantees are proven, in particular for systems of\npolynomial equations. In \"generic situations\", we will also discuss a way to\navoid that the limit of the constructed sequence is a solution of\n$H(x)^{\\intercal}F(x)=0$ but not of $F(x)=0$.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Tuyen Trung Truong"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Complex Variables (math.CV)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07403"
  },
  {
    "id": "arXiv:2110.07409",
    "title": "The Geometry of Memoryless Stochastic Policy Optimization in  Infinite-Horizon POMDPs",
    "abstract": "We consider the problem of finding the best memoryless stochastic policy for\nan infinite-horizon partially observable Markov decision process (POMDP) with\nfinite state and action spaces with respect to either the discounted or mean\nreward criterion. We show that the (discounted) state-action frequencies and\nthe expected cumulative reward are rational functions of the policy, whereby\nthe degree is determined by the degree of partial observability. We then\ndescribe the optimization problem as a linear optimization problem in the space\nof feasible state-action frequencies subject to polynomial constraints that we\ncharacterize explicitly. This allows us to address the combinatorial and\ngeometric complexity of the optimization problem using recent tools from\npolynomial optimization. In particular, we demonstrate how the partial\nobservability constraints can lead to multiple smooth and non-smooth local\noptimizers and we estimate the number of critical points.",
    "descriptor": "\nComments: Preprint, 37 pages, 5 figures\n",
    "authors": [
      "Guido Mont\u00fafar",
      "Johannes M\u00fcller"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2110.07409"
  },
  {
    "id": "arXiv:2110.07419",
    "title": "Student-t Networks for Melody Estimation",
    "abstract": "Melody estimation or melody extraction refers to the extraction of the\nprimary or fundamental dominant frequency in a melody. This sequence of\nfrequencies obtained represents the pitch of the dominant melodic line from\nrecorded music audio signals. The music signal may be monophonic or polyphonic.\nThe melody extraction problem from audio signals gets complicated when we start\ndealing with polyphonic audio data. This is because in generalized audio\nsignals,the sounds are highly correlated over both frequency and time domains.\nThis complex overlap of many sounds, makes identification of predominant\nfrequency challenging.",
    "descriptor": "",
    "authors": [
      "Udhav Gupta",
      "Bhavesh Jain"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07419"
  },
  {
    "id": "arXiv:2110.07432",
    "title": "Trading Data for Wind Power Forecasting: A Regression Market with Lasso  Regularization",
    "abstract": "This paper proposes a regression market for wind agents to monetize data\ntraded among themselves for wind power forecasting. Existing literature on data\nmarkets often treats data disclosure as a binary choice or modulates the data\nquality based on the mismatch between the offer and bid prices. As a result,\nthe market disadvantages either the data sellers due to the overestimation of\ntheir willingness to disclose data, or the data buyers due to the lack of\nuseful data being provided. Our proposed regression market determines the data\npayment based on the least absolute shrinkage and selection operator (lasso),\nwhich not only provides the data buyer with a means for selecting useful\nfeatures, but also enables each data seller to individualize the threshold for\ndata payment. Using both synthetic data and real-world wind data, the case\nstudies demonstrate a reduction in the overall losses for wind agents who buy\ndata, as well as additional financial benefits to those who sell data.",
    "descriptor": "",
    "authors": [
      "Liyang Han",
      "Pierre Pinson",
      "Jalal Kazempour"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07432"
  },
  {
    "id": "arXiv:2110.07460",
    "title": "IB-GAN: A Unified Approach for Multivariate Time Series Classification  under Class Imbalance",
    "abstract": "Classification of large multivariate time series with strong class imbalance\nis an important task in real-world applications. Standard methods of class\nweights, oversampling, or parametric data augmentation do not always yield\nsignificant improvements for predicting minority classes of interest.\nNon-parametric data augmentation with Generative Adversarial Networks (GANs)\noffers a promising solution. We propose Imputation Balanced GAN (IB-GAN), a\nnovel method that joins data augmentation and classification in a one-step\nprocess via an imputation-balancing approach. IB-GAN uses imputation and\nresampling techniques to generate higher quality samples from randomly masked\nvectors than from white noise, and augments classification through a\nclass-balanced set of real and synthetic samples. Imputation hyperparameter\n$p_{miss}$ allows for regularization of classifier variability by tuning\ninnovations introduced via generator imputation. IB-GAN is simple to train and\nmodel-agnostic, pairing any deep learning classifier with a\ngenerator-discriminator duo and resulting in higher accuracy for under-observed\nclasses. Empirical experiments on open-source UCR data and proprietary 90K\nproduct dataset show significant performance gains against state-of-the-art\nparametric and GAN baselines.",
    "descriptor": "",
    "authors": [
      "Grace Deng",
      "Cuize Han",
      "Tommaso Dreossi",
      "Clarence Lee",
      "David S. Matteson"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07460"
  },
  {
    "id": "arXiv:2110.07468",
    "title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice  Generation",
    "abstract": "High-fidelity singing voice synthesis is challenging for neural vocoders due\nto extremely long continuous pronunciation, high sampling rate and strong\nexpressiveness. Existing neural vocoders designed for text-to-speech cannot\ndirectly be applied to singing voice synthesis because they result in glitches\nin the generated spectrogram and poor high-frequency reconstruction. To tackle\nthe difficulty of singing modeling, in this paper, we propose SingGAN, a\nsinging voice vocoder with generative adversarial network. Specifically, 1)\nSingGAN uses source excitation to alleviate the glitch problem in the\nspectrogram; and 2) SingGAN adopts multi-band discriminators and introduces\nfrequency-domain loss and sub-band feature matching loss to supervise\nhigh-frequency reconstruction. To our knowledge, SingGAN is the first vocoder\ndesigned towards high-fidelity multi-speaker singing voice synthesis.\nExperimental results show that SingGAN synthesizes singing voices with much\nhigher quality (0.41 MOS gains) over the previous method. Further experiments\nshow that combined with FastSpeech~2 as an acoustic model, SingGAN achieves\nhigh robustness in the singing voice synthesis pipeline and also performs well\nin speech synthesis.",
    "descriptor": "\nComments: vocoder, generative adversarial network, singing voice synthesis\n",
    "authors": [
      "Feiyang Chen",
      "Rongjie Huang",
      "Chenye Cui",
      "Yi Ren",
      "Jinglin Liu",
      "Zhou Zhao",
      "Nicholas Yuan",
      "Baoxing Huai"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07468"
  },
  {
    "id": "arXiv:2110.07471",
    "title": "Stability Analysis of Unfolded WMMSE for Power Allocation",
    "abstract": "Power allocation is one of the fundamental problems in wireless networks and\na wide variety of algorithms address this problem from different perspectives.\nA common element among these algorithms is that they rely on an estimation of\nthe channel state, which may be inaccurate on account of hardware defects,\nnoisy feedback systems, and environmental and adversarial disturbances.\nTherefore, it is essential that the output power allocation of these algorithms\nis stable with respect to input perturbations, to the extent that the\nvariations in the output are bounded for bounded variations in the input. In\nthis paper, we focus on UWMMSE -- a modern algorithm leveraging graph neural\nnetworks --, and illustrate its stability to additive input perturbations of\nbounded energy through both theoretical analysis and empirical validation.",
    "descriptor": "\nComments: Under review at IEEE ICASSP 2022\n",
    "authors": [
      "Arindam Chowdhury",
      "Fernando Gama",
      "Santiago Segarra"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07471"
  },
  {
    "id": "arXiv:2110.07478",
    "title": "Inferring Manifolds From Noisy Data Using Gaussian Processes",
    "abstract": "In analyzing complex datasets, it is often of interest to infer lower\ndimensional structure underlying the higher dimensional observations. As a\nflexible class of nonlinear structures, it is common to focus on Riemannian\nmanifolds. Most existing manifold learning algorithms replace the original data\nwith lower dimensional coordinates without providing an estimate of the\nmanifold in the observation space or using the manifold to denoise the original\ndata. This article proposes a new methodology for addressing these problems,\nallowing interpolation of the estimated manifold between fitted data points.\nThe proposed approach is motivated by novel theoretical properties of local\ncovariance matrices constructed from noisy samples on a manifold. Our results\nenable us to turn a global manifold reconstruction problem into a local\nregression problem, allowing application of Gaussian processes for\nprobabilistic manifold reconstruction. In addition to theory justifying the\nalgorithm, we provide simulated and real data examples to illustrate the\nperformance.",
    "descriptor": "\nComments: 42 pages, 14 figures\n",
    "authors": [
      "David B Dunson",
      "Nan Wu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07478"
  },
  {
    "id": "arXiv:2110.07492",
    "title": "A Theory of Quantum Subspace Diagonalization",
    "abstract": "Quantum subspace diagonalization methods are an exciting new class of\nalgorithms for solving large scale eigenvalue problems using quantum computers.\nUnfortunately, these methods require the solution of an ill-conditioned\ngeneralized eigenvalue problem, with a matrix pencil corrupted by a\nnon-negligible amount of noise that is far above the machine precision. Despite\npessimistic predictions from classical perturbation theories, these methods can\nperform reliably well if the generalized eigenvalue problem is solved using a\nstandard truncation strategy. We provide a theoretical analysis of this\nsurprising phenomenon, proving that under certain natural conditions, a quantum\nsubspace diagonalization algorithm can accurately compute the smallest\neigenvalue of a large Hermitian matrix. We give numerical experiments\ndemonstrating the effectiveness of the theory and providing practical guidance\nfor the choice of truncation level.",
    "descriptor": "\nComments: 42 pages, 13 figures\n",
    "authors": [
      "Ethan N. Epperly",
      "Lin Lin",
      "Yuji Nakatsukasa"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07492"
  },
  {
    "id": "arXiv:2110.07509",
    "title": "TDACNN: Target-domain-free Domain Adaptation Convolutional Neural  Network for Drift Compensation in Gas Sensors",
    "abstract": "Sensor drift is a long-existing unpredictable problem that deteriorates the\nperformance of gaseous substance recognition, calling for an antidrift domain\nadaptation algorithm. However, the prerequisite for traditional methods to\nachieve fine results is to have data from both nondrift distributions (source\ndomain) and drift distributions (target domain) for domain alignment, which is\nusually unrealistic and unachievable in real-life scenarios. To compensate for\nthis, in this paper, deep learning based on a target-domain-free domain\nadaptation convolutional neural network (TDACNN) is proposed. The main concept\nis that CNNs extract not only the domain-specific features of samples but also\nthe domain-invariant features underlying both the source and target domains.\nMaking full use of these various levels of embedding features can lead to\ncomprehensive utilization of different levels of characteristics, thus\nachieving drift compensation by the extracted intermediate features between two\ndomains. In the TDACNN, a flexible multibranch backbone with a multiclassifier\nstructure is proposed under the guidance of bionics, which utilizes multiple\nembedding features comprehensively without involving target domain data during\ntraining. A classifier ensemble method based on maximum mean discrepancy (MMD)\nis proposed to evaluate all the classifiers jointly based on the credibility of\nthe pseudolabel. To optimize network training, an additive angular margin\nsoftmax loss with parameter dynamic adjustment is utilized. Experiments on two\ndrift datasets under different settings demonstrate the superiority of TDACNN\ncompared with several state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Yuelin Zhang",
      "Jia Yan",
      "Zehuan Wanga",
      "Xiaoyan Peng",
      "Yutong Tian",
      "Shukai Duan"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07509"
  },
  {
    "id": "arXiv:2110.07522",
    "title": "Two-Stage Homotopy Method to Incorporate Discrete Control Variables into  AC-OPF",
    "abstract": "Alternating-Current Optimal Power Flow (AC-OPF) is an optimization problem\ncritical for planning and operating the power grid. The problem is\ntraditionally formulated using only continuous variables. Typically, control\ndevices with discrete-valued settings, which provide valuable flexibility to\nthe network and improve resilience, are omitted from AC-OPF formulations due to\nthe difficulty of integrality constraints. We propose a two-stage homotopy\nalgorithm to solve the AC-OPF problem with discrete-valued control settings.\nThis method does not rely on prior knowledge of control settings or other\ninitial conditions. The first stage relaxes the discrete settings to continuous\nvariables and solves the optimization using a robust homotopy technique. Once\nthe solution has been obtained using relaxed models, second homotopy problem\ngradually transforms the relaxed settings to their nearest feasible discrete\nvalues. We test the proposed algorithm on several large networks with switched\nshunts and adjustable transformers and show it can outperform a similar\nstate-of-the-art solver.",
    "descriptor": "\nComments: Under review: submitted for consideration for 22nd Power Systems Computation Conference\n",
    "authors": [
      "Timothy McNamara",
      "Amritanshu Pandey",
      "Aayushya Agarwal",
      "Larry Pileggi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07522"
  },
  {
    "id": "arXiv:2110.07531",
    "title": "Predictive models of RNA degradation through dual crowdsourcing",
    "abstract": "Messenger RNA-based medicines hold immense potential, as evidenced by their\nrapid deployment as COVID-19 vaccines. However, worldwide distribution of mRNA\nmolecules has been limited by their thermostability, which is fundamentally\nlimited by the intrinsic instability of RNA molecules to a chemical degradation\nreaction called in-line hydrolysis. Predicting the degradation of an RNA\nmolecule is a key task in designing more stable RNA-based therapeutics. Here,\nwe describe a crowdsourced machine learning competition (\"Stanford\nOpenVaccine\") on Kaggle, involving single-nucleotide resolution measurements on\n6043 102-130-nucleotide diverse RNA constructs that were themselves solicited\nthrough crowdsourcing on the RNA design platform Eterna. The entire experiment\nwas completed in less than 6 months. Winning models demonstrated test set\nerrors that were better by 50% than the previous state-of-the-art DegScore\nmodel. Furthermore, these models generalized to blindly predicting orthogonal\ndegradation data on much longer mRNA molecules (504-1588 nucleotides) with\nimproved accuracy over DegScore and other models. Top teams integrated natural\nlanguage processing architectures and data augmentation techniques with\npredictions from previous dynamic programming models for RNA secondary\nstructure. These results indicate that such models are capable of representing\nin-line hydrolysis with excellent accuracy, supporting their use for designing\nstabilized messenger RNAs. The integration of two crowdsourcing platforms, one\nfor data set creation and another for machine learning, may be fruitful for\nother urgent problems that demand scientific discovery on rapid timescales.",
    "descriptor": "",
    "authors": [
      "Hannah K. Wayment-Steele",
      "Wipapat Kladwang",
      "Andrew M. Watkins",
      "Do Soon Kim",
      "Bojan Tunguz",
      "Walter Reade",
      "Maggie Demkin",
      "Jonathan Romano",
      "Roger Wellington-Oguri",
      "John J. Nicol",
      "Jiayang Gao",
      "Kazuki Onodera",
      "Kazuki Fujikawa",
      "Hanfei Mao",
      "Gilles Vandewiele",
      "Michele Tinti",
      "Bram Steenwinckel",
      "Takuya Ito",
      "Taiga Noumi",
      "Shujun He",
      "Keiichiro Ishi",
      "Youhan Lee",
      "Fatih \u00d6zt\u00fcrk",
      "Anthony Chiu",
      "Emin \u00d6zt\u00fcrk",
      "Karim Amer",
      "Mohamed Fares",
      "Eterna Participants",
      "Rhiju Das"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2110.07531"
  },
  {
    "id": "arXiv:2110.07537",
    "title": "Toward Degradation-Robust Voice Conversion",
    "abstract": "Any-to-any voice conversion technologies convert the vocal timbre of an\nutterance to any speaker even unseen during training. Although there have been\nseveral state-of-the-art any-to-any voice conversion models, they were all\nbased on clean utterances to convert successfully. However, in real-world\nscenarios, it is difficult to collect clean utterances of a speaker, and they\nare usually degraded by noises or reverberations. It thus becomes highly\ndesired to understand how these degradations affect voice conversion and build\na degradation-robust model. We report in this paper the first comprehensive\nstudy on the degradation robustness of any-to-any voice conversion. We show\nthat the performance of state-of-the-art models nowadays was severely hampered\ngiven degraded utterances. To this end, we then propose speech enhancement\nconcatenation and denoising training to improve the robustness. In addition to\ncommon degradations, we also consider adversarial noises, which alter the model\noutput significantly yet are human-imperceptible. It was shown that both\nconcatenations with off-the-shelf speech enhancement models and denoising\ntraining on voice conversion models could improve the robustness, while each of\nthem had pros and cons.",
    "descriptor": "\nComments: Submitted to ICASSP 2022, equal contribution from first two authors\n",
    "authors": [
      "Chien-yu Huang",
      "Kai-Wei Chang",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07537"
  },
  {
    "id": "arXiv:2110.07545",
    "title": "Automatic Generation of Grover Quantum Oracles for Arbitrary Data  Structures",
    "abstract": "The steadily growing research interest in quantum computing - together with\nthe accompanying technological advances in the realization of quantum hardware\n- fuels the development of meaningful real-world applications, as well as\nimplementations for well-known quantum algorithms. One of the most prominent\nexamples till today is Grover's algorithm, which can be used for efficient\nsearch in unstructured databases. Quantum oracles that are frequently masked as\nblack boxes play an important role in Grover's algorithm. Hence, the automatic\ngeneration of oracles is of paramount importance. Moreover, the automatic\ngeneration of the corresponding circuits for a Grover quantum oracle is deeply\nlinked to the synthesis of reversible quantum logic, which - despite numerous\nadvances in the field - still remains a challenge till today in terms of\nsynthesizing efficient and scalable circuits for complex boolean functions.\nIn this paper, we present a flexible method for automatically encoding\nunstructured databases into oracles, which can then be efficiently searched\nwith Grover's algorithm. Furthermore, we develop a tailor-made method for\nquantum logic synthesis, which vastly improves circuit complexity over other\ncurrent approaches. Finally, we present another logic synthesis method that\nconsiders the requirements of scaling onto real world backends. We compare our\nmethod with other approaches through evaluating the oracle generation for\nrandom databases and analyzing the resulting circuit complexities using various\nmetrics.",
    "descriptor": "",
    "authors": [
      "Raphael Seidel",
      "Colin Kai-Uwe Becker",
      "Sebastian Bock",
      "Nikolay Tcholtchev",
      "Ilie-Daniel Gheorge-Pop",
      "Manfred Hauswirth"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2110.07545"
  },
  {
    "id": "arXiv:2110.07553",
    "title": "A Flat Wall Theorem for Matching Minors in Bipartite Graphs",
    "abstract": "A major step in the graph minors theory of Robertson and Seymour is the\ntransition from the Grid Theorem which, in some sense uniquely, describes areas\nof large treewidth within a graph, to a notion of local flatness of these areas\nin form of the existence of a large flat wall within any huge grid of an\nH-minor free graph. In this paper, we prove a matching theoretic analogue of\nthe Flat Wall Theorem for bipartite graphs excluding a fixed matching minor.\nOur result builds on a a tight relationship between structural digraph theory\nand matching theory and allows us to deduce a Flat Wall Theorem for digraphs\nwhich substantially differs from a previously established directed variant of\nthis theorem.",
    "descriptor": "",
    "authors": [
      "Archontia C. Giannopoulou",
      "Sebastian Wiederrecht"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.07553"
  },
  {
    "id": "arXiv:2110.07583",
    "title": "Near optimal sample complexity for matrix and tensor normal models via  geodesic convexity",
    "abstract": "The matrix normal model, the family of Gaussian matrix-variate distributions\nwhose covariance matrix is the Kronecker product of two lower dimensional\nfactors, is frequently used to model matrix-variate data. The tensor normal\nmodel generalizes this family to Kronecker products of three or more factors.\nWe study the estimation of the Kronecker factors of the covariance matrix in\nthe matrix and tensor models. We show nonasymptotic bounds for the error\nachieved by the maximum likelihood estimator (MLE) in several natural metrics.\nIn contrast to existing bounds, our results do not rely on the factors being\nwell-conditioned or sparse. For the matrix normal model, all our bounds are\nminimax optimal up to logarithmic factors, and for the tensor normal model our\nbound for the largest factor and overall covariance matrix are minimax optimal\nup to constant factors provided there are enough samples for any estimator to\nobtain constant Frobenius error. In the same regimes as our sample complexity\nbounds, we show that an iterative procedure to compute the MLE known as the\nflip-flop algorithm converges linearly with high probability. Our main tool is\ngeodesic strong convexity in the geometry on positive-definite matrices induced\nby the Fisher information metric. This strong convexity is determined by the\nexpansion of certain random quantum channels. We also provide numerical\nevidence that combining the flip-flop algorithm with a simple shrinkage\nestimator can improve performance in the undersampled regime.",
    "descriptor": "",
    "authors": [
      "Cole Franks",
      "Rafael Oliveira",
      "Akshay Ramachandran",
      "Michael Walter"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07583"
  },
  {
    "id": "arXiv:1702.05079",
    "title": "Information Systems with Witnesses: The Function Space Construction",
    "abstract": "Comments: 40 pages. arXiv admin note: text overlap with arXiv:1610.02260",
    "descriptor": "\nComments: 40 pages. arXiv admin note: text overlap with arXiv:1610.02260\n",
    "authors": [
      "Dieter Spreen"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1702.05079"
  },
  {
    "id": "arXiv:1801.08640",
    "title": "Considerations When Learning Additive Explanations for Black-Box Models",
    "abstract": "Comments: Previously titled \"Learning Global Additive Explanations for Neural Nets Using Model Distillation\". A short version was presented at NeurIPS 2018 Machine Learning for Health Workshop",
    "descriptor": "\nComments: Previously titled \"Learning Global Additive Explanations for Neural Nets Using Model Distillation\". A short version was presented at NeurIPS 2018 Machine Learning for Health Workshop\n",
    "authors": [
      "Sarah Tan",
      "Giles Hooker",
      "Paul Koch",
      "Albert Gordo",
      "Rich Caruana"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1801.08640"
  },
  {
    "id": "arXiv:1811.12471",
    "title": "Unlabeled Compression Schemes Exceeding the VC-dimension",
    "abstract": "Unlabeled Compression Schemes Exceeding the VC-dimension",
    "descriptor": "",
    "authors": [
      "D\u00f6m\u00f6t\u00f6r P\u00e1lv\u00f6lgyi",
      "G\u00e1bor Tardos"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1811.12471"
  },
  {
    "id": "arXiv:1812.00301",
    "title": "Plan-Recognition-Driven Attention Modeling for Visual Recognition",
    "abstract": "Plan-Recognition-Driven Attention Modeling for Visual Recognition",
    "descriptor": "",
    "authors": [
      "Yantian Zha",
      "Yikang Li",
      "Tianshu Yu",
      "Subbarao Kambhampati",
      "Baoxin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1812.00301"
  },
  {
    "id": "arXiv:1902.03667",
    "title": "Differential Similarity in Higher Dimensional Spaces: Theory and  Applications",
    "abstract": "Comments: 76 pages, 29 figures. Revised and expanded to include Section 7 on the CIFAR-10 dataset and Section 8 on Future Work",
    "descriptor": "\nComments: 76 pages, 29 figures. Revised and expanded to include Section 7 on the CIFAR-10 dataset and Section 8 on Future Work\n",
    "authors": [
      "L. Thorne McCarty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1902.03667"
  },
  {
    "id": "arXiv:1904.06217",
    "title": "Political Text Scaling Meets Computational Semantics",
    "abstract": "Comments: Updated version - accepted for Transactions on Data Science (TDS)",
    "descriptor": "\nComments: Updated version - accepted for Transactions on Data Science (TDS)\n",
    "authors": [
      "Federico Nanni",
      "Goran Glavas",
      "Ines Rehbein",
      "Simone Paolo Ponzetto",
      "Heiner Stuckenschmidt"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/1904.06217"
  },
  {
    "id": "arXiv:1909.09902",
    "title": "Deep Reinforcement Learning with Modulated Hebbian plus Q Network  Architecture",
    "abstract": "Deep Reinforcement Learning with Modulated Hebbian plus Q Network  Architecture",
    "descriptor": "",
    "authors": [
      "Pawel Ladosz",
      "Eseoghene Ben-Iwhiwhu",
      "Jeffery Dick",
      "Yang Hu",
      "Nicholas Ketz",
      "Soheil Kolouri",
      "Jeffrey L. Krichmar",
      "Praveen Pilly",
      "Andrea Soltoggio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1909.09902"
  },
  {
    "id": "arXiv:1910.11379",
    "title": "Information Based Data-Driven Characterization of Stability and  Influence in Power Systems",
    "abstract": "Information Based Data-Driven Characterization of Stability and  Influence in Power Systems",
    "descriptor": "",
    "authors": [
      "Subhrajit Sinha",
      "Pranav Sharma",
      "Venkataramana Ajjarapu",
      "Umesh Vaidya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/1910.11379"
  },
  {
    "id": "arXiv:1911.01479",
    "title": "CC-circuits and the expressive power of nilpotent algebras",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Michael Kompatscher"
    ],
    "subjectives": [
      "Rings and Algebras (math.RA)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/1911.01479"
  },
  {
    "id": "arXiv:1911.01528",
    "title": "BAS: An Answer Selection Method Using BERT Language Model",
    "abstract": "Comments: 28 pages, 19 figures, 8 tables, Journal of Computing and Security",
    "descriptor": "\nComments: 28 pages, 19 figures, 8 tables, Journal of Computing and Security\n",
    "authors": [
      "Jamshid Mozafari",
      "Afsaneh Fatemi",
      "Mohammad Ali Nematbakhsh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/1911.01528"
  },
  {
    "id": "arXiv:1911.03129",
    "title": "A Novel Sybil Attack Detection Scheme Based on Edge Computing for Mobile  IoT Environment",
    "abstract": "Comments: 14 pages, 9 figures",
    "descriptor": "\nComments: 14 pages, 9 figures\n",
    "authors": [
      "Manli Yuan",
      "Liwei Lin",
      "Zhengyu Wu",
      "Xiucai Ye"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/1911.03129"
  },
  {
    "id": "arXiv:1911.04404",
    "title": "Learning Weighted Automata over Principal Ideal Domains",
    "abstract": "Learning Weighted Automata over Principal Ideal Domains",
    "descriptor": "",
    "authors": [
      "Gerco van Heerdt",
      "Clemens Kupke",
      "Jurriaan Rot",
      "Alexandra Silva"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/1911.04404"
  },
  {
    "id": "arXiv:2002.03016",
    "title": "Safe Wasserstein Constrained Deep Q-Learning",
    "abstract": "Safe Wasserstein Constrained Deep Q-Learning",
    "descriptor": "",
    "authors": [
      "Aaron Kandel",
      "Scott J. Moura"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.03016"
  },
  {
    "id": "arXiv:2002.08724",
    "title": "High-resolution signal recovery via generalized sampling and functional  principal component analysis",
    "abstract": "Comments: Accepted for publication in Springer's Advances in Computational Mathematics",
    "descriptor": "\nComments: Accepted for publication in Springer's Advances in Computational Mathematics\n",
    "authors": [
      "Milana Gataric"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.08724"
  },
  {
    "id": "arXiv:2003.02437",
    "title": "Drone-based RGB-Infrared Cross-Modality Vehicle Detection via  Uncertainty-Aware Learning",
    "abstract": "Drone-based RGB-Infrared Cross-Modality Vehicle Detection via  Uncertainty-Aware Learning",
    "descriptor": "",
    "authors": [
      "Yiming Sun",
      "Bing Cao",
      "Pengfei Zhu",
      "Qinghua Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.02437"
  },
  {
    "id": "arXiv:2003.06107",
    "title": "A Spatial-Temporal Attentive Network with Spatial Continuity for  Trajectory Prediction",
    "abstract": "Comments: bad model settings and unclear results",
    "descriptor": "\nComments: bad model settings and unclear results\n",
    "authors": [
      "Beihao Xia",
      "Conghao Wang",
      "Qinmu Peng",
      "Xinge You",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2003.06107"
  },
  {
    "id": "arXiv:2005.02979",
    "title": "A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical  Systems",
    "abstract": "A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical  Systems",
    "descriptor": "",
    "authors": [
      "Anthony Corso",
      "Robert J. Moss",
      "Mark Koren",
      "Ritchie Lee",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.02979"
  },
  {
    "id": "arXiv:2005.08054",
    "title": "Classification vs regression in overparameterized regimes: Does the loss  function matter?",
    "abstract": "Classification vs regression in overparameterized regimes: Does the loss  function matter?",
    "descriptor": "",
    "authors": [
      "Vidya Muthukumar",
      "Adhyyan Narang",
      "Vignesh Subramanian",
      "Mikhail Belkin",
      "Daniel Hsu",
      "Anant Sahai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.08054"
  },
  {
    "id": "arXiv:2005.12161",
    "title": "Triangularized Orthogonalization-free Method for Solving Extreme  Eigenvalue Problems",
    "abstract": "Triangularized Orthogonalization-free Method for Solving Extreme  Eigenvalue Problems",
    "descriptor": "",
    "authors": [
      "Weiguo Gao",
      "Yingzhou Li",
      "Bichen Lu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2005.12161"
  },
  {
    "id": "arXiv:2006.07942",
    "title": "Duplicity Games for Deception Design with an Application to Insider  Threat Mitigation",
    "abstract": "Duplicity Games for Deception Design with an Application to Insider  Threat Mitigation",
    "descriptor": "",
    "authors": [
      "Linan Huang",
      "Quanyan Zhu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2006.07942"
  },
  {
    "id": "arXiv:2006.09365",
    "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",
    "abstract": "Comments: v4 is a major overhaul of the paper and has significantly stronger theory and experiments",
    "descriptor": "\nComments: v4 is a major overhaul of the paper and has significantly stronger theory and experiments\n",
    "authors": [
      "Sai Praneeth Karimireddy",
      "Lie He",
      "Martin Jaggi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.09365"
  },
  {
    "id": "arXiv:2006.13192",
    "title": "Adversarial Robustness of Deep Sensor Fusion Models",
    "abstract": "Adversarial Robustness of Deep Sensor Fusion Models",
    "descriptor": "",
    "authors": [
      "Shaojie Wang",
      "Tong Wu",
      "Ayan Chakrabarti",
      "Yevgeniy Vorobeychik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2006.13192"
  },
  {
    "id": "arXiv:2007.02252",
    "title": "Spatial-Angular Attention Network for Light Field Reconstruction",
    "abstract": "Comments: 15 pages, 13 figures and 5 tables, Accepted by IEEE Transactions on Image Processing (IEEE TIP)",
    "descriptor": "\nComments: 15 pages, 13 figures and 5 tables, Accepted by IEEE Transactions on Image Processing (IEEE TIP)\n",
    "authors": [
      "Gaochang Wu",
      "Yingqian Wang",
      "Yebin Liu",
      "Lu Fang",
      "Tianyou Chai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.02252"
  },
  {
    "id": "arXiv:2007.08073",
    "title": "Unseen Object Instance Segmentation for Robotic Environments",
    "abstract": "Comments: Journal version of arXiv:1907.13236",
    "descriptor": "\nComments: Journal version of arXiv:1907.13236\n",
    "authors": [
      "Christopher Xie",
      "Yu Xiang",
      "Arsalan Mousavian",
      "Dieter Fox"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2007.08073"
  },
  {
    "id": "arXiv:2008.01169",
    "title": "Deep Knowledge Tracing with Learning Curves",
    "abstract": "Comments: 10 pages",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Shanghui Yang",
      "Mengxia Zhu",
      "Xuesong Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2008.01169"
  },
  {
    "id": "arXiv:2008.03053",
    "title": "Efficient and Optimal Algorithms for Tree Summarization with Weighted  Terminologies",
    "abstract": "Efficient and Optimal Algorithms for Tree Summarization with Weighted  Terminologies",
    "descriptor": "",
    "authors": [
      "Xuliang Zhu",
      "Xin Huang",
      "Byron Choi",
      "Jianliang Xu",
      "William K. Cheung",
      "Yanchun Zhang",
      "Jiming Liu"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2008.03053"
  },
  {
    "id": "arXiv:2008.09872",
    "title": "LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video  Recommendation System",
    "abstract": "Comments: 6 pages,4 figures",
    "descriptor": "\nComments: 6 pages,4 figures\n",
    "authors": [
      "Xuanji Xiao",
      "Huabin Chen",
      "Yuzhen Liu",
      "Xing Yao",
      "Pei Liu",
      "Chaosheng Fan",
      "Nian Ji",
      "Xirong Jiang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2008.09872"
  },
  {
    "id": "arXiv:2009.01802",
    "title": "Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs",
    "abstract": "Bipartite Matching in Nearly-linear Time on Moderately Dense Graphs",
    "descriptor": "",
    "authors": [
      "Jan van den Brand",
      "Yin-Tat Lee",
      "Danupon Nanongkai",
      "Richard Peng",
      "Thatchaphol Saranurak",
      "Aaron Sidford",
      "Zhao Song",
      "Di Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2009.01802"
  },
  {
    "id": "arXiv:2009.07204",
    "title": "New Instances of Quadratic APN Functions",
    "abstract": "Comments: 18 pages. This is the version accepted to IEEE Transactions on Information Theory",
    "descriptor": "\nComments: 18 pages. This is the version accepted to IEEE Transactions on Information Theory\n",
    "authors": [
      "Christof Beierle",
      "Gregor Leander"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2009.07204"
  },
  {
    "id": "arXiv:2009.07719",
    "title": "Domain-invariant Similarity Activation Map Contrastive Learning for  Retrieval-based Long-term Visual Localization",
    "abstract": "Comments: Published in IEEE/CAA Journal of Automatica Sinica",
    "descriptor": "\nComments: Published in IEEE/CAA Journal of Automatica Sinica\n",
    "authors": [
      "Hanjiang Hu",
      "Hesheng Wang",
      "Zhe Liu",
      "Weidong Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.07719"
  },
  {
    "id": "arXiv:2009.12619",
    "title": "Enabling and Emerging Sensing Technologies for Crowd Avoidance in Public  Transportation: A Review",
    "abstract": "Comments: 11 pages, 2 figures, 2 tables, submitted to IEEE Sensors Journal",
    "descriptor": "\nComments: 11 pages, 2 figures, 2 tables, submitted to IEEE Sensors Journal\n",
    "authors": [
      "Donatella Darsena",
      "Giacinto Gelli",
      "Ivan Iudice",
      "Francesco Verde"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2009.12619"
  },
  {
    "id": "arXiv:2010.01089",
    "title": "Unsupervised Point Cloud Pre-Training via Occlusion Completion",
    "abstract": "Comments: sync with ICCV camera ready",
    "descriptor": "\nComments: sync with ICCV camera ready\n",
    "authors": [
      "Hanchen Wang",
      "Qi Liu",
      "Xiangyu Yue",
      "Joan Lasenby",
      "Matthew J. Kusner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.01089"
  },
  {
    "id": "arXiv:2010.01496",
    "title": "Explaining Deep Neural Networks",
    "abstract": "Comments: PhD Thesis, University of Oxford",
    "descriptor": "\nComments: PhD Thesis, University of Oxford\n",
    "authors": [
      "Oana-Maria Camburu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.01496"
  },
  {
    "id": "arXiv:2010.07027",
    "title": "A Light Heterogeneous Graph Collaborative Filtering Model using Textual  Information",
    "abstract": "Comments: Accepted by Knowledge-Based Systems",
    "descriptor": "\nComments: Accepted by Knowledge-Based Systems\n",
    "authors": [
      "Chaoyang Wang",
      "Zhiqiang Guo",
      "Guohui Li",
      "Jianjun Li",
      "Peng Pan",
      "Ke Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.07027"
  },
  {
    "id": "arXiv:2010.09809",
    "title": "Parametrized topological complexity of collision-free motion planning in  the plane",
    "abstract": "Comments: revision includes an appendix on fibrations of certain mapping spaces",
    "descriptor": "\nComments: revision includes an appendix on fibrations of certain mapping spaces\n",
    "authors": [
      "Daniel C. Cohen",
      "Michael Farber",
      "Shmuel Weinberger"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Robotics (cs.RO)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2010.09809"
  },
  {
    "id": "arXiv:2010.12643",
    "title": "Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question  Answering",
    "abstract": "Comments: 7 pages",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Arij Riabi",
      "Thomas Scialom",
      "Rachel Keraron",
      "Beno\u00eet Sagot",
      "Djam\u00e9 Seddah",
      "Jacopo Staiano"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2010.12643"
  },
  {
    "id": "arXiv:2010.14995",
    "title": "Accelerated Probabilistic Power Flow in Electrical Distribution Networks  via Model Order Reduction and Neumann Series Expansion",
    "abstract": "Accelerated Probabilistic Power Flow in Electrical Distribution Networks  via Model Order Reduction and Neumann Series Expansion",
    "descriptor": "",
    "authors": [
      "Samuel Chevalier",
      "Luca Schenato",
      "Luca Daniel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.14995"
  },
  {
    "id": "arXiv:2010.15690",
    "title": "Analyzing the tree-layer structure of Deep Forests",
    "abstract": "Analyzing the tree-layer structure of Deep Forests",
    "descriptor": "",
    "authors": [
      "Ludovic Arnould",
      "Claire Boyer",
      "Erwan Scornet",
      "Sorbonne Lpsm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2010.15690"
  },
  {
    "id": "arXiv:2011.00931",
    "title": "Point Transformer",
    "abstract": "Point Transformer",
    "descriptor": "",
    "authors": [
      "Nico Engel",
      "Vasileios Belagiannis",
      "Klaus Dietmayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.00931"
  },
  {
    "id": "arXiv:2011.05234",
    "title": "Testability of relations between permutations",
    "abstract": "Comments: 42 pages; this version was accepted to FOCS 2021",
    "descriptor": "\nComments: 42 pages; this version was accepted to FOCS 2021\n",
    "authors": [
      "Oren Becker",
      "Alexander Lubotzky",
      "Jonathan Mosheiff"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2011.05234"
  },
  {
    "id": "arXiv:2011.12866",
    "title": "Deep Physics-aware Inference of Cloth Deformation for Monocular Human  Performance Capture",
    "abstract": "Deep Physics-aware Inference of Cloth Deformation for Monocular Human  Performance Capture",
    "descriptor": "",
    "authors": [
      "Yue Li",
      "Marc Habermann",
      "Bernhard Thomaszewski",
      "Stelian Coros",
      "Thabo Beeler",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2011.12866"
  },
  {
    "id": "arXiv:2011.13606",
    "title": "A Construction of Maximally Recoverable Codes with Order-Optimal Field  Size",
    "abstract": "A Construction of Maximally Recoverable Codes with Order-Optimal Field  Size",
    "descriptor": "",
    "authors": [
      "Han Cai",
      "Ying Miao",
      "Moshe Schwartz",
      "Xiaohu Tang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2011.13606"
  },
  {
    "id": "arXiv:2011.14730",
    "title": "Isomorphism Testing for Graphs Excluding Small Topological Subgraphs",
    "abstract": "Comments: 45 pages, 2 figures. The second version improves on the presentation of the results and corrects minor inaccuracies. An extended abstract is to be published in the proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA 2022). arXiv admin note: text overlap with arXiv:2004.07671",
    "descriptor": "\nComments: 45 pages, 2 figures. The second version improves on the presentation of the results and corrects minor inaccuracies. An extended abstract is to be published in the proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA 2022). arXiv admin note: text overlap with arXiv:2004.07671\n",
    "authors": [
      "Daniel Neuen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2011.14730"
  },
  {
    "id": "arXiv:2012.01950",
    "title": "Co-mining: Self-Supervised Learning for Sparsely Annotated Object  Detection",
    "abstract": "Comments: Accepted to AAAI 2021. Code is available at this https URL",
    "descriptor": "\nComments: Accepted to AAAI 2021. Code is available at this https URL\n",
    "authors": [
      "Tiancai Wang",
      "Tong Yang",
      "Jiale Cao",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.01950"
  },
  {
    "id": "arXiv:2012.04524",
    "title": "Construction of optimal spectral methods in phase retrieval",
    "abstract": "Comments: 14 pages + references and appendix. v2: Version updated to match the one accepted at MSML 2021. v3: Adding a reference to a previous work mentioning marginal stability and its connection to Bayes-optimality",
    "descriptor": "\nComments: 14 pages + references and appendix. v2: Version updated to match the one accepted at MSML 2021. v3: Adding a reference to a previous work mentioning marginal stability and its connection to Bayes-optimality\n",
    "authors": [
      "Antoine Maillard",
      "Florent Krzakala",
      "Yue M. Lu",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)"
    ],
    "url": "https://arxiv.org/abs/2012.04524"
  },
  {
    "id": "arXiv:2101.00371",
    "title": "On-the-Fly Attention Modulation for Neural Generation",
    "abstract": "Comments: 10 pages, 3 figures",
    "descriptor": "\nComments: 10 pages, 3 figures\n",
    "authors": [
      "Yue Dong",
      "Chandra Bhagavatula",
      "Ximing Lu",
      "Jena D. Hwang",
      "Antoine Bosselut",
      "Jackie Chi Kit Cheung",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.00371"
  },
  {
    "id": "arXiv:2101.03591",
    "title": "Tietze Equivalences as Weak Equivalences",
    "abstract": "Tietze Equivalences as Weak Equivalences",
    "descriptor": "",
    "authors": [
      "Simon Henry",
      "Samuel Mimram"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2101.03591"
  },
  {
    "id": "arXiv:2101.11055",
    "title": "LDLE: Low Distortion Local Eigenmaps",
    "abstract": "Comments: 66 pages, 32 figures, preprint. Rearranged figures",
    "descriptor": "\nComments: 66 pages, 32 figures, preprint. Rearranged figures\n",
    "authors": [
      "Dhruv Kohli",
      "Alexander Cloninger",
      "Gal Mishne"
    ],
    "subjectives": [
      "Spectral Theory (math.SP)",
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.11055"
  },
  {
    "id": "arXiv:2102.04159",
    "title": "Deep Residual Learning in Spiking Neural Networks",
    "abstract": "Comments: Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021",
    "descriptor": "\nComments: Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021\n",
    "authors": [
      "Wei Fang",
      "Zhaofei Yu",
      "Yanqi Chen",
      "Tiejun Huang",
      "Timoth\u00e9e Masquelier",
      "Yonghong Tian"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2102.04159"
  },
  {
    "id": "arXiv:2102.06605",
    "title": "Unleashing the Power of Contrastive Self-Supervised Visual Models via  Contrast-Regularized Fine-Tuning",
    "abstract": "Comments: NeurIPS 2021. Source code: this https URL",
    "descriptor": "\nComments: NeurIPS 2021. Source code: this https URL\n",
    "authors": [
      "Yifan Zhang",
      "Bryan Hooi",
      "Dapeng Hu",
      "Jian Liang",
      "Jiashi Feng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06605"
  },
  {
    "id": "arXiv:2102.07833",
    "title": "Quasi-Monte Carlo Software",
    "abstract": "Comments: 25 pages, 7 figures, to be published in the MCQMC2020 Proceedings",
    "descriptor": "\nComments: 25 pages, 7 figures, to be published in the MCQMC2020 Proceedings\n",
    "authors": [
      "Sou-Cheng T. Choi",
      "Fred J. Hickernell",
      "R. Jagadeeswaran",
      "Michael J. McCourt",
      "Aleksei G. Sorokin"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2102.07833"
  },
  {
    "id": "arXiv:2102.10256",
    "title": "Generalized Group Testing",
    "abstract": "Generalized Group Testing",
    "descriptor": "",
    "authors": [
      "Xiwei Cheng",
      "Sidharth Jaggi",
      "Qiaoqiao Zhou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.10256"
  },
  {
    "id": "arXiv:2102.12778",
    "title": "Lie Group integrators for mechanical systems",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Elena Celledoni",
      "Ergys \u00c7okaj",
      "Andrea Leone",
      "Davide Murari",
      "Brynjulf Owren"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.12778"
  },
  {
    "id": "arXiv:2103.03413",
    "title": "Routing algorithms as tools for integrating social distancing with  emergency evacuation",
    "abstract": "Routing algorithms as tools for integrating social distancing with  emergency evacuation",
    "descriptor": "",
    "authors": [
      "Yi-Lin Tsai",
      "Chetanya Rastogi",
      "Peter K. Kitanidis",
      "Christopher B. Field"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.03413"
  },
  {
    "id": "arXiv:2103.06960",
    "title": "Characterizing Partisan Political Narrative Frameworks about COVID-19 on  Twitter",
    "abstract": "Comments: 16 pages, 5 figures. To be published in EPJ Data Science",
    "descriptor": "\nComments: 16 pages, 5 figures. To be published in EPJ Data Science\n",
    "authors": [
      "Elise Jing",
      "Yong-Yeol Ahn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2103.06960"
  },
  {
    "id": "arXiv:2103.08255",
    "title": "Sample-efficient Reinforcement Learning Representation Learning with  Curiosity Contrastive Forward Dynamics Model",
    "abstract": "Sample-efficient Reinforcement Learning Representation Learning with  Curiosity Contrastive Forward Dynamics Model",
    "descriptor": "",
    "authors": [
      "Thanh Nguyen",
      "Tung M. Luu",
      "Thang Vu",
      "Chang D. Yoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.08255"
  },
  {
    "id": "arXiv:2103.08753",
    "title": "Adaptive Gradient Online Control",
    "abstract": "Adaptive Gradient Online Control",
    "descriptor": "",
    "authors": [
      "Deepan Muthirayan",
      "Jianjun Yuan",
      "Pramod P. Khargonekar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.08753"
  },
  {
    "id": "arXiv:2103.10179",
    "title": "Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light  Fields via Multi-Task Deep Learning",
    "abstract": "Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light  Fields via Multi-Task Deep Learning",
    "descriptor": "",
    "authors": [
      "Maximilian Schambach",
      "Jiayang Shi",
      "Michael Heizmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.10179"
  },
  {
    "id": "arXiv:2103.11233",
    "title": "Spark Deficient Gabor Frame Provides a Novel Analysis Operator for  Compressed Sensing",
    "abstract": "Spark Deficient Gabor Frame Provides a Novel Analysis Operator for  Compressed Sensing",
    "descriptor": "",
    "authors": [
      "Vasiliki Kouni",
      "Holger Rauhut"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2103.11233"
  },
  {
    "id": "arXiv:2103.12591",
    "title": "BoXHED 2.0: Scalable boosting of dynamic survival analysis",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Arash Pakbin",
      "Xiaochen Wang",
      "Bobak J. Mortazavi",
      "Donald K.K. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.12591"
  },
  {
    "id": "arXiv:2103.12954",
    "title": "Convergence Analysis of Nonconvex Distributed Stochastic Zeroth-order  Coordinate Method",
    "abstract": "Convergence Analysis of Nonconvex Distributed Stochastic Zeroth-order  Coordinate Method",
    "descriptor": "",
    "authors": [
      "Shengjun Zhang",
      "Yunlong Dong",
      "Dong Xie",
      "Lisha Yao",
      "Colleen P. Bailey",
      "Shengli Fu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.12954"
  },
  {
    "id": "arXiv:2103.13392",
    "title": "Scalable Pareto Front Approximation for Deep Multi-Objective Learning",
    "abstract": "Comments: Accepted at ICDM 2021 as short paper. Adapt title to match published version",
    "descriptor": "\nComments: Accepted at ICDM 2021 as short paper. Adapt title to match published version\n",
    "authors": [
      "Michael Ruchte",
      "Josif Grabocka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.13392"
  },
  {
    "id": "arXiv:2103.14533",
    "title": "3D Point Cloud Registration with Multi-Scale Architecture and  Unsupervised Transfer Learning",
    "abstract": "Comments: Accepted to 3DV 2021",
    "descriptor": "\nComments: Accepted to 3DV 2021\n",
    "authors": [
      "Sofiane Horache",
      "Jean-Emmanuel Deschaud",
      "Fran\u00e7ois Goulette"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.14533"
  },
  {
    "id": "arXiv:2103.17263",
    "title": "Rethinking Self-supervised Correspondence Learning: A Video Frame-level  Similarity Perspective",
    "abstract": "Comments: ICCV 2021 (oral). Project page and code: this https URL",
    "descriptor": "\nComments: ICCV 2021 (oral). Project page and code: this https URL\n",
    "authors": [
      "Jiarui Xu",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.17263"
  },
  {
    "id": "arXiv:2104.04073",
    "title": "Direct-PoseNet: Absolute Pose Regression with Photometric Consistency",
    "abstract": "Direct-PoseNet: Absolute Pose Regression with Photometric Consistency",
    "descriptor": "",
    "authors": [
      "Shuai Chen",
      "Zirui Wang",
      "Victor Prisacariu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.04073"
  },
  {
    "id": "arXiv:2104.07961",
    "title": "Advanced Deep Networks for 3D Mitochondria Instance Segmentation",
    "abstract": "Comments: 4 pages",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Mingxing Li",
      "Chang Chen",
      "Xiaoyu Liu",
      "Wei Huang",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.07961"
  },
  {
    "id": "arXiv:2104.08261",
    "title": "Adaptive Robust Model Predictive Control with Matched and Unmatched  Uncertainty",
    "abstract": "Comments: Major revision",
    "descriptor": "\nComments: Major revision\n",
    "authors": [
      "Rohan Sinha",
      "James Harrison",
      "Spencer M. Richards",
      "Marco Pavone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2104.08261"
  },
  {
    "id": "arXiv:2104.08790",
    "title": "Misinfo Reaction Frames: Reasoning about Readers' Reactions to News  Headlines",
    "abstract": "Misinfo Reaction Frames: Reasoning about Readers' Reactions to News  Headlines",
    "descriptor": "",
    "authors": [
      "Saadia Gabriel",
      "Skyler Hallinan",
      "Maarten Sap",
      "Pemi Nguyen",
      "Franziska Roesner",
      "Eunsol Choi",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08790"
  },
  {
    "id": "arXiv:2104.10236",
    "title": "A Game Theoretic Approach to a Problem in Polymatroid Maximization",
    "abstract": "A Game Theoretic Approach to a Problem in Polymatroid Maximization",
    "descriptor": "",
    "authors": [
      "Lisa Hellerstein",
      "Thomas Lidbetter"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2104.10236"
  },
  {
    "id": "arXiv:2104.10291",
    "title": "Soft Expectation and Deep Maximization for Image Feature Detection",
    "abstract": "Comments: 9 pages, 3 figures, 2 tables",
    "descriptor": "\nComments: 9 pages, 3 figures, 2 tables\n",
    "authors": [
      "Alexander Mai",
      "Allen Yang",
      "Dominique E. Meyer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.10291"
  },
  {
    "id": "arXiv:2104.11120",
    "title": "PyArmadillo: a streamlined linear algebra library for Python",
    "abstract": "PyArmadillo: a streamlined linear algebra library for Python",
    "descriptor": "",
    "authors": [
      "Jason Rumengan",
      "Terry Yue Zhuo",
      "Conrad Sanderson"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2104.11120"
  },
  {
    "id": "arXiv:2104.11710",
    "title": "Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct  Speech Translation",
    "abstract": "Comments: Accepted to ICNLSP 2021",
    "descriptor": "\nComments: Accepted to ICNLSP 2021\n",
    "authors": [
      "Marco Gaido",
      "Matteo Negri",
      "Mauro Cettolo",
      "Marco Turchi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2104.11710"
  },
  {
    "id": "arXiv:2104.12046",
    "title": "Quantization of Deep Neural Networks for Accurate Edge Computing",
    "abstract": "Comments: 11 pages, 3 figures, 10 tables, accepted by the ACM Journal on Emerging Technologies in Computing Systems (JETC)",
    "descriptor": "\nComments: 11 pages, 3 figures, 10 tables, accepted by the ACM Journal on Emerging Technologies in Computing Systems (JETC)\n",
    "authors": [
      "Wentao Chen",
      "Hailong Qiu",
      "Jian Zhuang",
      "Chutong Zhang",
      "Yu Hu",
      "Qing Lu",
      "Tianchen Wang",
      "Yiyu Shi",
      "Meiping Huang",
      "Xiaowe Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.12046"
  },
  {
    "id": "arXiv:2104.12101",
    "title": "Efficient Binary Decision Diagram Manipulation in External Memory",
    "abstract": "Comments: 36 pages, 12 figures and 7 tables",
    "descriptor": "\nComments: 36 pages, 12 figures and 7 tables\n",
    "authors": [
      "Steffan Christ S\u00f8lvsten",
      "Jaco van de Pol",
      "Anna Blume Jakobsen",
      "Mathias Weller Berg Thomasen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2104.12101"
  },
  {
    "id": "arXiv:2105.00666",
    "title": "Unsupervised Document Expansion for Information Retrieval with  Stochastic Text Generation",
    "abstract": "Comments: SDP@NAACL2021",
    "descriptor": "\nComments: SDP@NAACL2021\n",
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "ChaeHun Park",
      "Jong C. Park"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2105.00666"
  },
  {
    "id": "arXiv:2105.01632",
    "title": "Solo: A Lightweight Static Analysis for Differential Privacy",
    "abstract": "Solo: A Lightweight Static Analysis for Differential Privacy",
    "descriptor": "",
    "authors": [
      "Chike Abuah",
      "David Darais",
      "Joseph P. Near"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.01632"
  },
  {
    "id": "arXiv:2105.08832",
    "title": "A Contraction Theory Approach to Optimization Algorithms from  Acceleration Flows",
    "abstract": "A Contraction Theory Approach to Optimization Algorithms from  Acceleration Flows",
    "descriptor": "",
    "authors": [
      "Pedro Cisneros-Velarde",
      "Francesco Bullo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2105.08832"
  },
  {
    "id": "arXiv:2105.09226",
    "title": "Detection of Emotions in Hindi-English Code Mixed Text Data",
    "abstract": "Detection of Emotions in Hindi-English Code Mixed Text Data",
    "descriptor": "",
    "authors": [
      "Divyansh Singh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.09226"
  },
  {
    "id": "arXiv:2105.11314",
    "title": "RobeCzech: Czech RoBERTa, a monolingual contextualized language  representation model",
    "abstract": "Comments: Published in TSD 2021",
    "descriptor": "\nComments: Published in TSD 2021\n",
    "authors": [
      "Milan Straka",
      "Jakub N\u00e1plava",
      "Jana Strakov\u00e1",
      "David Samuel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.11314"
  },
  {
    "id": "arXiv:2105.13203",
    "title": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point  Solving",
    "abstract": "Conic Blackwell Algorithm: Parameter-Free Convex-Concave Saddle-Point  Solving",
    "descriptor": "",
    "authors": [
      "Julien Grand-Cl\u00e9ment",
      "Christian Kroer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2105.13203"
  },
  {
    "id": "arXiv:2105.13677",
    "title": "ResT: An Efficient Transformer for Visual Recognition",
    "abstract": "Comments: ResT is an efficient multi-scale vision Transformer that can tackle input images with arbitrary size. arXiv admin note: text overlap with arXiv:2103.14030 by other authors",
    "descriptor": "\nComments: ResT is an efficient multi-scale vision Transformer that can tackle input images with arbitrary size. arXiv admin note: text overlap with arXiv:2103.14030 by other authors\n",
    "authors": [
      "Qinglong Zhang",
      "Yubin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.13677"
  },
  {
    "id": "arXiv:2106.00899",
    "title": "Feedback Interconnected Density Estimation and Control",
    "abstract": "Feedback Interconnected Density Estimation and Control",
    "descriptor": "",
    "authors": [
      "Tongjia Zheng",
      "Qing Han",
      "Hai Lin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.00899"
  },
  {
    "id": "arXiv:2106.02078",
    "title": "Improving Neural Network Robustness via Persistency of Excitation",
    "abstract": "Improving Neural Network Robustness via Persistency of Excitation",
    "descriptor": "",
    "authors": [
      "Kaustubh Sridhar",
      "Oleg Sokolsky",
      "Insup Lee",
      "James Weimer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02078"
  },
  {
    "id": "arXiv:2106.02356",
    "title": "PCA Initialization for Approximate Message Passing in Rotationally  Invariant Models",
    "abstract": "Comments: 72 pages, 2 figures, appeared in Neural Information Processing Systems (NeurIPS), 2021",
    "descriptor": "\nComments: 72 pages, 2 figures, appeared in Neural Information Processing Systems (NeurIPS), 2021\n",
    "authors": [
      "Marco Mondelli",
      "Ramji Venkataramanan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.02356"
  },
  {
    "id": "arXiv:2106.02615",
    "title": "Consensus Multiplicative Weights Update: Learning to Learn using  Projector-based Game Signatures",
    "abstract": "Consensus Multiplicative Weights Update: Learning to Learn using  Projector-based Game Signatures",
    "descriptor": "",
    "authors": [
      "Nelson Vadori",
      "Rahul Savani",
      "Thomas Spooner",
      "Sumitra Ganesh"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02615"
  },
  {
    "id": "arXiv:2106.03504",
    "title": "Boosting 5G mm-Wave IAB Reliability with Reconfigurable Intelligent  Surfaces",
    "abstract": "Boosting 5G mm-Wave IAB Reliability with Reconfigurable Intelligent  Surfaces",
    "descriptor": "",
    "authors": [
      "Paolo Fiore",
      "Eugenio Moro",
      "Ilario Filippini",
      "Antonio Capone",
      "Danilo De Donno"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.03504"
  },
  {
    "id": "arXiv:2106.05306",
    "title": "DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact",
    "abstract": "DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact",
    "descriptor": "",
    "authors": [
      "Yifei Li",
      "Tao Du",
      "Kui Wu",
      "Jie Xu",
      "Wojciech Matusik"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.05306"
  },
  {
    "id": "arXiv:2106.06333",
    "title": "Invariant Information Bottleneck for Domain Generalization",
    "abstract": "Comments: The work is in progress",
    "descriptor": "\nComments: The work is in progress\n",
    "authors": [
      "Bo Li",
      "Yifei Shen",
      "Yezhen Wang",
      "Wenzhen Zhu",
      "Colorado J. Reed",
      "Jun Zhang",
      "Dongsheng Li",
      "Kurt Keutzer",
      "Han Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.06333"
  },
  {
    "id": "arXiv:2106.07847",
    "title": "Learning Stable Classifiers by Transferring Unstable Features",
    "abstract": "Learning Stable Classifiers by Transferring Unstable Features",
    "descriptor": "",
    "authors": [
      "Yujia Bao",
      "Shiyu Chang",
      "Regina Barzilay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.07847"
  },
  {
    "id": "arXiv:2106.09669",
    "title": "Improving On-Screen Sound Separation for Open-Domain Videos with  Audio-Visual Self-Attention",
    "abstract": "Improving On-Screen Sound Separation for Open-Domain Videos with  Audio-Visual Self-Attention",
    "descriptor": "",
    "authors": [
      "Efthymios Tzinis",
      "Scott Wisdom",
      "Tal Remez",
      "John R. Hershey"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.09669"
  },
  {
    "id": "arXiv:2106.10989",
    "title": "Pre-training also Transfers Non-Robustness",
    "abstract": "Pre-training also Transfers Non-Robustness",
    "descriptor": "",
    "authors": [
      "Jiaming Zhang",
      "Jitao Sang",
      "Qi Yi",
      "Yunfan Yang",
      "Huiwen Dong",
      "Jian Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.10989"
  },
  {
    "id": "arXiv:2106.13950",
    "title": "Self-Evolving Integrated Vertical Heterogeneous Networks",
    "abstract": "Comments: 25 pages, 5 figures, 2 tables",
    "descriptor": "\nComments: 25 pages, 5 figures, 2 tables\n",
    "authors": [
      "Amin Farajzadeh",
      "Mohammad G. Khoshkholgh",
      "Halim Yanikomeroglu",
      "Ozgur Ercetin"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2106.13950"
  },
  {
    "id": "arXiv:2106.14568",
    "title": "Deep Ensembling with No Overhead for either Training or Testing: The  All-Round Blessings of Dynamic Sparsity",
    "abstract": "Comments: preprint version",
    "descriptor": "\nComments: preprint version\n",
    "authors": [
      "Shiwei Liu",
      "Tianlong Chen",
      "Zahra Atashgahi",
      "Xiaohan Chen",
      "Ghada Sokar",
      "Elena Mocanu",
      "Mykola Pechenizkiy",
      "Zhangyang Wang",
      "Decebal Constantin Mocanu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.14568"
  },
  {
    "id": "arXiv:2106.16064",
    "title": "Efficient Sparse Matrix Kernels based on Adaptive Workload-Balancing and  Parallel-Reduction",
    "abstract": "Efficient Sparse Matrix Kernels based on Adaptive Workload-Balancing and  Parallel-Reduction",
    "descriptor": "",
    "authors": [
      "Guyue Huang",
      "Guohao Dai",
      "Yu Wang",
      "Yufei Ding",
      "Yuan Xie"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.16064"
  },
  {
    "id": "arXiv:2107.02783",
    "title": "SAGE: Intrusion Alert-driven Attack Graph Extractor",
    "abstract": "Comments: Appeared at VizSec '21 (proceedings) and KDD AI4Cyber '21 (without proceedings)",
    "descriptor": "\nComments: Appeared at VizSec '21 (proceedings) and KDD AI4Cyber '21 (without proceedings)\n",
    "authors": [
      "Azqa Nadeem",
      "Sicco Verwer",
      "Shanchieh Jay Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.02783"
  },
  {
    "id": "arXiv:2107.03645",
    "title": "A hybrid virtual sensing approach for approximating non-linear dynamic  system behavior using LSTM networks",
    "abstract": "Comments: 18 pages, 10 figures",
    "descriptor": "\nComments: 18 pages, 10 figures\n",
    "authors": [
      "Leonhard Heindel",
      "Peter Hantschke",
      "Markus K\u00e4stner"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.03645"
  },
  {
    "id": "arXiv:2107.08360",
    "title": "A Duality-based Approach for Real-time Obstacle Avoidance between  Polytopes with Control Barrier Functions",
    "abstract": "Comments: submitted to 2022 American Control Conference (ACC) with full version of proofs in the appendix",
    "descriptor": "\nComments: submitted to 2022 American Control Conference (ACC) with full version of proofs in the appendix\n",
    "authors": [
      "Akshay Thirugnanam",
      "Jun Zeng",
      "Koushil Sreenath"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2107.08360"
  },
  {
    "id": "arXiv:2107.09003",
    "title": "Constraints Penalized Q-Learning for Safe Offline Reinforcement Learning",
    "abstract": "Comments: Spotlight @ RL4RealLife workshop in ICML 2021",
    "descriptor": "\nComments: Spotlight @ RL4RealLife workshop in ICML 2021\n",
    "authors": [
      "Haoran Xu",
      "Xianyuan Zhan",
      "Xiangyu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.09003"
  },
  {
    "id": "arXiv:2107.12527",
    "title": "Physics-Enforced Modeling for Insertion Loss of Transmission Lines by  Deep Neural Networks",
    "abstract": "Physics-Enforced Modeling for Insertion Loss of Transmission Lines by  Deep Neural Networks",
    "descriptor": "",
    "authors": [
      "Liang Chen",
      "Lesley Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.12527"
  },
  {
    "id": "arXiv:2108.00295",
    "title": "Fair Representation Learning using Interpolation Enabled Disentanglement",
    "abstract": "Fair Representation Learning using Interpolation Enabled Disentanglement",
    "descriptor": "",
    "authors": [
      "Akshita Jha",
      "Bhanukiran Vinzamuri",
      "Chandan K. Reddy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.00295"
  },
  {
    "id": "arXiv:2108.00620",
    "title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection",
    "abstract": "Comments: International Conference on 3D Vision (3DV 2021)",
    "descriptor": "\nComments: International Conference on 3D Vision (3DV 2021)\n",
    "authors": [
      "Shi Qiu",
      "Yunfan Wu",
      "Saeed Anwar",
      "Chongyi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.00620"
  },
  {
    "id": "arXiv:2108.00719",
    "title": "ConveRT for FAQ Answering",
    "abstract": "Comments: Accepted at bnaicbenelearn2021",
    "descriptor": "\nComments: Accepted at bnaicbenelearn2021\n",
    "authors": [
      "Maxime De Bruyn",
      "Ehsan Lotfi",
      "Jeska Buhmann",
      "Walter Daelemans"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.00719"
  },
  {
    "id": "arXiv:2108.01490",
    "title": "Extending dynamic mode decomposition to data from multiple outputs",
    "abstract": "Comments: Submitted to the American Control Conference 2022",
    "descriptor": "\nComments: Submitted to the American Control Conference 2022\n",
    "authors": [
      "Nibodh Boddupalli"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.01490"
  },
  {
    "id": "arXiv:2108.08187",
    "title": "ME-PCN: Point Completion Conditioned on Mask Emptiness",
    "abstract": "Comments: Accepted to ICCV 2021; typos corrected",
    "descriptor": "\nComments: Accepted to ICCV 2021; typos corrected\n",
    "authors": [
      "Bingchen Gong",
      "Yinyu Nie",
      "Yiqun Lin",
      "Xiaoguang Han",
      "Yizhou Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.08187"
  },
  {
    "id": "arXiv:2108.08263",
    "title": "Selectively-Amortized Resource Bounding (Extended Version)",
    "abstract": "Comments: This is an extended version of SAS'21 paper (with appendices)",
    "descriptor": "\nComments: This is an extended version of SAS'21 paper (with appendices)\n",
    "authors": [
      "Tianhan Lu",
      "Bor-Yuh Evan Chang",
      "Ashutosh Trivedi"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2108.08263"
  },
  {
    "id": "arXiv:2108.08339",
    "title": "Real-time Bangla License Plate Recognition System for Low Resource  Video-based Applications",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Alif Ashrafee",
      "Akib Mohammed Khan",
      "Mohammad Sabik Irbaz",
      "MD Abdullah Al Nasim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.08339"
  },
  {
    "id": "arXiv:2108.11206",
    "title": "Towards Dynamic Threat Modelling in 5G Core Networks Based on MITRE  ATT&CK",
    "abstract": "Towards Dynamic Threat Modelling in 5G Core Networks Based on MITRE  ATT&CK",
    "descriptor": "",
    "authors": [
      "Robert Pell",
      "Sotiris Moschoyiannis",
      "Emmanouil Panaousis",
      "Ryan Heartfield"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2108.11206"
  },
  {
    "id": "arXiv:2108.11877",
    "title": "A Deep Learning Loss Function based on Auditory Power Compression for  Speech Enhancement",
    "abstract": "Comments: 7 pages, 4 figures",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Tianrui Wang",
      "Weibin Zhu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2108.11877"
  },
  {
    "id": "arXiv:2108.11892",
    "title": "Dynamics of Wealth Inequality in Simple Artificial Societies",
    "abstract": "Comments: 12 pages, 2 tables, 5 figures. Presented at the Social Simulation Conference 2021. arXiv admin note: substantial text overlap with arXiv:2101.09817",
    "descriptor": "\nComments: 12 pages, 2 tables, 5 figures. Presented at the Social Simulation Conference 2021. arXiv admin note: substantial text overlap with arXiv:2101.09817\n",
    "authors": [
      "John C. Stevenson"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Multiagent Systems (cs.MA)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2108.11892"
  },
  {
    "id": "arXiv:2108.13741",
    "title": "Monolingual versus Multilingual BERTology for Vietnamese Extractive  Multi-Document Summarization",
    "abstract": "Monolingual versus Multilingual BERTology for Vietnamese Extractive  Multi-Document Summarization",
    "descriptor": "",
    "authors": [
      "Huy Quoc To",
      "Kiet Van Nguyen",
      "Ngan Luu-Thuy Nguyen",
      "Anh Gia-Tuan Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.13741"
  },
  {
    "id": "arXiv:2109.00648",
    "title": "The VoicePrivacy 2020 Challenge: Results and findings",
    "abstract": "Comments: Submitted to the Special Issue on Voice Privacy (Computer Speech and Language Journal - Elsevier); under review",
    "descriptor": "\nComments: Submitted to the Special Issue on Voice Privacy (Computer Speech and Language Journal - Elsevier); under review\n",
    "authors": [
      "Natalia Tomashenko",
      "Xin Wang",
      "Emmanuel Vincent",
      "Jose Patino",
      "Brij Mohan Lal Srivastava",
      "Paul-Gauthier No\u00e9",
      "Andreas Nautsch",
      "Nicholas Evans",
      "Junichi Yamagishi",
      "Benjamin O'Brien",
      "Ana\u00efs Chanclu",
      "Jean-Fran\u00e7ois Bonastre",
      "Massimiliano Todisco",
      "Mohamed Maouche"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2109.00648"
  },
  {
    "id": "arXiv:2109.01925",
    "title": "Ordinal Maximin Share Approximation for Goods",
    "abstract": "Comments: More accurate approximation ratio; better text flow and figure titles",
    "descriptor": "\nComments: More accurate approximation ratio; better text flow and figure titles\n",
    "authors": [
      "Hadi Hosseini",
      "Andrew Searns",
      "Erel Segal-Halevi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2109.01925"
  },
  {
    "id": "arXiv:2109.06304",
    "title": "Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to  Corpus Exploration",
    "abstract": "Comments: EMNLP 2021 Conference Camera Ready",
    "descriptor": "\nComments: EMNLP 2021 Conference Camera Ready\n",
    "authors": [
      "Shufan Wang",
      "Laure Thompson",
      "Mohit Iyyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.06304"
  },
  {
    "id": "arXiv:2109.10187",
    "title": "Oriented Object Detection in Aerial Images Based on Area Ratio of  Parallelogram",
    "abstract": "Oriented Object Detection in Aerial Images Based on Area Ratio of  Parallelogram",
    "descriptor": "",
    "authors": [
      "Xinyi Yu",
      "Mi Lin",
      "Jiangping Lu",
      "Linlin Ou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.10187"
  },
  {
    "id": "arXiv:2109.10203",
    "title": "Generalized minimum 0-extension problem and discrete convexity",
    "abstract": "Comments: replaced \"zigzag SDA\" with \"diamond SDA\" with a better complexity bound",
    "descriptor": "\nComments: replaced \"zigzag SDA\" with \"diamond SDA\" with a better complexity bound\n",
    "authors": [
      "Martin Dvorak",
      "Vladimir Kolmogorov"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2109.10203"
  },
  {
    "id": "arXiv:2109.10604",
    "title": "NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering  Dataset",
    "abstract": "Comments: Findings of EMNLP 2021. Code will be released at: this https URL",
    "descriptor": "\nComments: Findings of EMNLP 2021. Code will be released at: this https URL\n",
    "authors": [
      "Qiyuan Zhang",
      "Lei Wang",
      "Sicheng Yu",
      "Shuohang Wang",
      "Yang Wang",
      "Jing Jiang",
      "Ee-Peng Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.10604"
  },
  {
    "id": "arXiv:2109.11308",
    "title": "Breaking BERT: Understanding its Vulnerabilities for Biomedical Named  Entity Recognition through Adversarial Attack",
    "abstract": "Breaking BERT: Understanding its Vulnerabilities for Biomedical Named  Entity Recognition through Adversarial Attack",
    "descriptor": "",
    "authors": [
      "Anne Dirkson",
      "Suzan Verberne",
      "Wessel Kraaij"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2109.11308"
  },
  {
    "id": "arXiv:2109.11436",
    "title": "Piecewise Pad\u00e9-Chebyshev Reconstruction of Bivariate Piecewise Smooth  Functions",
    "abstract": "Comments: 26 pages, 15 figures",
    "descriptor": "\nComments: 26 pages, 15 figures\n",
    "authors": [
      "Akansha Singh"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.11436"
  },
  {
    "id": "arXiv:2109.11728",
    "title": "AES Systems Are Both Overstable And Oversensitive: Explaining Why And  Proposing Defenses",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2012.13872",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2012.13872\n",
    "authors": [
      "Yaman Kumar Singla",
      "Swapnil Parekh",
      "Somesh Singh",
      "Junyi Jessy Li",
      "Rajiv Ratn Shah",
      "Changyou Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2109.11728"
  },
  {
    "id": "arXiv:2109.14333",
    "title": "Distribution Knowledge Embedding for Graph Pooling",
    "abstract": "Comments: 8 pages, 4 figures, 4 tables",
    "descriptor": "\nComments: 8 pages, 4 figures, 4 tables\n",
    "authors": [
      "Kaixuan Chen",
      "Jie Song",
      "Shunyu Liu",
      "Na Yu",
      "Zunlei Feng",
      "Gengshi Han",
      "Mingli Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.14333"
  },
  {
    "id": "arXiv:2109.15089",
    "title": "Biologically Plausible Training Mechanisms for Self-Supervised Learning  in Deep Networks",
    "abstract": "Biologically Plausible Training Mechanisms for Self-Supervised Learning  in Deep Networks",
    "descriptor": "",
    "authors": [
      "Mufeng Tang",
      "Yibo Yang",
      "Yali Amit"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2109.15089"
  },
  {
    "id": "arXiv:2109.15196",
    "title": "Multilingual AMR Parsing with Noisy Knowledge Distillation",
    "abstract": "Comments: EMNLP21 (findings)",
    "descriptor": "\nComments: EMNLP21 (findings)\n",
    "authors": [
      "Deng Cai",
      "Xin Li",
      "Jackie Chun-Sing Ho",
      "Lidong Bing",
      "Wai Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.15196"
  },
  {
    "id": "arXiv:2109.15215",
    "title": "A first moment proof of the Johansson-Molloy theorem",
    "abstract": "A first moment proof of the Johansson-Molloy theorem",
    "descriptor": "",
    "authors": [
      "Fran\u00e7ois Pirot",
      "Eoin Hurley"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2109.15215"
  },
  {
    "id": "arXiv:2110.00128",
    "title": "Probabilistic Object Maps for Long-Term Robot Localization",
    "abstract": "Comments: 7 pages, 5 figures; corrected typos in equations 11, 12 and 13",
    "descriptor": "\nComments: 7 pages, 5 figures; corrected typos in equations 11, 12 and 13\n",
    "authors": [
      "Amanda Adkins",
      "Joydeep Biswas"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.00128"
  },
  {
    "id": "arXiv:2110.00165",
    "title": "Large-scale ASR Domain Adaptation using Self- and Semi-supervised  Learning",
    "abstract": "Comments: ICASSP 2022 submitted, 5 pages, 2 figures, 5 tables",
    "descriptor": "\nComments: ICASSP 2022 submitted, 5 pages, 2 figures, 5 tables\n",
    "authors": [
      "Dongseong Hwang",
      "Ananya Misra",
      "Zhouyuan Huo",
      "Nikhil Siddhartha",
      "Shefali Garg",
      "David Qiu",
      "Khe Chai Sim",
      "Trevor Strohman",
      "Fran\u00e7oise Beaufays",
      "Yanzhang He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.00165"
  },
  {
    "id": "arXiv:2110.00678",
    "title": "Speech Technology for Everyone: Automatic Speech Recognition for  Non-Native English with Transfer Learning",
    "abstract": "Comments: All authors contributed equally. Paper accepted to International Conference on Natural Language and Speech Processing 2021 (ICNLSP 2021)",
    "descriptor": "\nComments: All authors contributed equally. Paper accepted to International Conference on Natural Language and Speech Processing 2021 (ICNLSP 2021)\n",
    "authors": [
      "Toshiko Shibano",
      "Xinyi Zhang",
      "Mia Taige Li",
      "Haejin Cho",
      "Peter Sullivan",
      "Muhammad Abdul-Mageed"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.00678"
  },
  {
    "id": "arXiv:2110.01052",
    "title": "Learn then Test: Calibrating Predictive Algorithms to Achieve Risk  Control",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Anastasios N. Angelopoulos",
      "Stephen Bates",
      "Emmanuel J. Cand\u00e8s",
      "Michael I. Jordan",
      "Lihua Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.01052"
  },
  {
    "id": "arXiv:2110.01601",
    "title": "NeuFENet: Neural Finite Element Solutions with Theoretical Bounds for  Parametric PDEs",
    "abstract": "NeuFENet: Neural Finite Element Solutions with Theoretical Bounds for  Parametric PDEs",
    "descriptor": "",
    "authors": [
      "Biswajit Khara",
      "Aditya Balu",
      "Ameya Joshi",
      "Soumik Sarkar",
      "Chinmay Hegde",
      "Adarsh Krishnamurthy",
      "Baskar Ganapathysubramanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.01601"
  },
  {
    "id": "arXiv:2110.02491",
    "title": "Data-Centric AI Requires Rethinking Data Notion",
    "abstract": "Data-Centric AI Requires Rethinking Data Notion",
    "descriptor": "",
    "authors": [
      "Mustafa Hajij",
      "Ghada Zamzmi",
      "Karthikeyan Natesan Ramamurthy",
      "Aldo Guzman Saenz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Category Theory (math.CT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.02491"
  },
  {
    "id": "arXiv:2110.02627",
    "title": "MovingFashion: a Benchmark for the Video-to-Shop Challenge",
    "abstract": "Comments: Accepted at WACV 2022",
    "descriptor": "\nComments: Accepted at WACV 2022\n",
    "authors": [
      "Marco Godi",
      "Christian Joppi",
      "Geri Skenderi",
      "Marco Cristani"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02627"
  },
  {
    "id": "arXiv:2110.02661",
    "title": "PlumeCityNet: Multi-Resolution Air Quality Forecasting",
    "abstract": "Comments: 9 pages, 6 figures. arXiv admin note: substantial text overlap with arXiv:2006.09204",
    "descriptor": "\nComments: 9 pages, 6 figures. arXiv admin note: substantial text overlap with arXiv:2006.09204\n",
    "authors": [
      "Thibaut Cassard",
      "Gr\u00e9goire Jauvion",
      "Antoine All\u00e9on",
      "Boris Quennehen",
      "David Lissmyr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.02661"
  },
  {
    "id": "arXiv:2110.02665",
    "title": "A structure preserving shift-invert infinite Arnoldi algorithm for a  class of delay eigenvalue problems with Hamiltonian symmetry",
    "abstract": "A structure preserving shift-invert infinite Arnoldi algorithm for a  class of delay eigenvalue problems with Hamiltonian symmetry",
    "descriptor": "",
    "authors": [
      "Pieter Appeltans",
      "Wim Michiels"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.02665"
  },
  {
    "id": "arXiv:2110.03037",
    "title": "Reactive Locomotion Decision-Making and Robust Motion Planning for  Real-Time Perturbation Recovery",
    "abstract": "Reactive Locomotion Decision-Making and Robust Motion Planning for  Real-Time Perturbation Recovery",
    "descriptor": "",
    "authors": [
      "Zhaoyuan Gu",
      "Nathan Boyd",
      "Ye Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03037"
  },
  {
    "id": "arXiv:2110.03104",
    "title": "Hybrid Pointer Networks for Traveling Salesman Problems Optimization",
    "abstract": "Hybrid Pointer Networks for Traveling Salesman Problems Optimization",
    "descriptor": "",
    "authors": [
      "Ahmed Stohy",
      "Heba-Tullah Abdelhakam",
      "Sayed Ali",
      "Mohammed Elhenawy",
      "Abdallah A Hassan",
      "Mahmoud Masoud",
      "Sebastien Glaser",
      "Andry Rakotonirainy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03104"
  },
  {
    "id": "arXiv:2110.03735",
    "title": "Adversarial Unlearning of Backdoors via Implicit Hypergradient",
    "abstract": "Comments: 9 pages main text, 3 pages references, 12 pages appendix, 5 figures",
    "descriptor": "\nComments: 9 pages main text, 3 pages references, 12 pages appendix, 5 figures\n",
    "authors": [
      "Yi Zeng",
      "Si Chen",
      "Won Park",
      "Z. Morley Mao",
      "Ming Jin",
      "Ruoxi Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03735"
  },
  {
    "id": "arXiv:2110.03821",
    "title": "Uniform Guarded Fragments",
    "abstract": "Comments: Fixed a bug in the proof of lemma 8, simplified the proof of lemma 9 and minor changes to presentation",
    "descriptor": "\nComments: Fixed a bug in the proof of lemma 8, simplified the proof of lemma 9 and minor changes to presentation\n",
    "authors": [
      "Reijo Jaakkola"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.03821"
  },
  {
    "id": "arXiv:2110.03825",
    "title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural  Networks",
    "abstract": "Comments: NeurIPS 2021",
    "descriptor": "\nComments: NeurIPS 2021\n",
    "authors": [
      "Hanxun Huang",
      "Yisen Wang",
      "Sarah Monazam Erfani",
      "Quanquan Gu",
      "James Bailey",
      "Xingjun Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03825"
  },
  {
    "id": "arXiv:2110.03922",
    "title": "Neural Tangent Kernel Eigenvalues Accurately Predict Generalization",
    "abstract": "Comments: 10 pages (main text), 24 pages (total), 10 figures",
    "descriptor": "\nComments: 10 pages (main text), 24 pages (total), 10 figures\n",
    "authors": [
      "James B. Simon",
      "Madeline Dickens",
      "Michael R. DeWeese"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03922"
  },
  {
    "id": "arXiv:2110.04079",
    "title": "A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection",
    "abstract": "Comments: 29 pages, 7 figures, under review by CACIE",
    "descriptor": "\nComments: 29 pages, 7 figures, under review by CACIE\n",
    "authors": [
      "Yongqi Dong",
      "Sandeep Patil",
      "Bart van Arem",
      "Haneen Farah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.04079"
  },
  {
    "id": "arXiv:2110.05076",
    "title": "A Closer Look at Prototype Classifier for Few-shot Image Classification",
    "abstract": "Comments: 10 pages with 6 appendix section",
    "descriptor": "\nComments: 10 pages with 6 appendix section\n",
    "authors": [
      "Mingcheng Hou",
      "Issei Sato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05076"
  },
  {
    "id": "arXiv:2110.05096",
    "title": "Density-Based Clustering with Kernel Diffusion",
    "abstract": "Density-Based Clustering with Kernel Diffusion",
    "descriptor": "",
    "authors": [
      "Chao Zheng",
      "Yingjie Chen",
      "Chong Chen",
      "Jianqiang Huang",
      "Xian-Sheng Hua"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05096"
  },
  {
    "id": "arXiv:2110.05204",
    "title": "CLIP4Caption ++: Multi-CLIP for Video Caption",
    "abstract": "Comments: 4 pages, VALUE Challenge 2021 captioning task chamionship solution",
    "descriptor": "\nComments: 4 pages, VALUE Challenge 2021 captioning task chamionship solution\n",
    "authors": [
      "Mingkang Tang",
      "Zhanyu Wang",
      "Zhaoyang Zeng",
      "Fengyun Rao",
      "Dian Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05204"
  },
  {
    "id": "arXiv:2110.05263",
    "title": "2021 Drexel Society of Artificial Intelligence Research Conference",
    "abstract": "Comments: Six papers",
    "descriptor": "\nComments: Six papers\n",
    "authors": [
      "Ethan Jacob Moyer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05263"
  },
  {
    "id": "arXiv:2110.05286",
    "title": "Learning from Ambiguous Demonstrations with Self-Explanation Guided  Reinforcement Learning",
    "abstract": "Learning from Ambiguous Demonstrations with Self-Explanation Guided  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Yantian Zha",
      "Lin Guan",
      "Subbarao Kambhampati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05286"
  },
  {
    "id": "arXiv:2110.05481",
    "title": "Which Samples Should be Learned First: Easy or Hard?",
    "abstract": "Comments: 32 pages,21 figures",
    "descriptor": "\nComments: 32 pages,21 figures\n",
    "authors": [
      "Xiaoling Zhou",
      "Ou Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.05481"
  },
  {
    "id": "arXiv:2110.05601",
    "title": "A Time-Optimized Content Creation Workflow for Remote Teaching",
    "abstract": "Comments: Accepted at SIGCSE-TS 2022",
    "descriptor": "\nComments: Accepted at SIGCSE-TS 2022\n",
    "authors": [
      "Sebastian Hofst\u00e4tter",
      "Sophia Althammer",
      "Mete Sertkan",
      "Allan Hanbury"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.05601"
  },
  {
    "id": "arXiv:2110.05896",
    "title": "LaoPLM: Pre-trained Language Models for Lao",
    "abstract": "LaoPLM: Pre-trained Language Models for Lao",
    "descriptor": "",
    "authors": [
      "Nankai Lin",
      "Yingwen Fu",
      "Chuwei Chen",
      "Ziyu Yang",
      "Shengyi Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.05896"
  },
  {
    "id": "arXiv:2110.06211",
    "title": "Diagonalizing Against Polynomial-Time Bounded Turing Machines Via  Nondeterministic Turing Machine",
    "abstract": "Comments: The completely extended version of the preliminary report. Comments are welcome",
    "descriptor": "\nComments: The completely extended version of the preliminary report. Comments are welcome\n",
    "authors": [
      "Tianrong Lin"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.06211"
  },
  {
    "id": "arXiv:2110.06305",
    "title": "An enumeration of 1-perfect ternary codes",
    "abstract": "An enumeration of 1-perfect ternary codes",
    "descriptor": "",
    "authors": [
      "Minjia Shi",
      "Denis S. Krotov"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2110.06305"
  },
  {
    "id": "arXiv:2110.06356",
    "title": "Poncelet Parabola Pirouettes",
    "abstract": "Comments: 18 pages, 19 figures",
    "descriptor": "\nComments: 18 pages, 19 figures\n",
    "authors": [
      "Dan Reznik"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.06356"
  },
  {
    "id": "arXiv:2110.06366",
    "title": "Time Masking for Temporal Language Models",
    "abstract": "Comments: 9 pages, accepted to WSDM 2022",
    "descriptor": "\nComments: 9 pages, accepted to WSDM 2022\n",
    "authors": [
      "Guy D. Rosin",
      "Ido Guy",
      "Kira Radinsky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.06366"
  },
  {
    "id": "arXiv:2110.06523",
    "title": "Recommending POIs for Tourists by User Behavior Modeling and  Pseudo-Rating",
    "abstract": "Comments: 16 pages, 10 figures",
    "descriptor": "\nComments: 16 pages, 10 figures\n",
    "authors": [
      "Kun Yi",
      "Ryu Yamagishi",
      "Taishan Li",
      "Zhengyang Bai",
      "Qiang Ma"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.06523"
  },
  {
    "id": "arXiv:2110.06526",
    "title": "Practice Problems for Hardware Engineers",
    "abstract": "Comments: 166 pages, Kindle Direct Publishing, Amazon 1st Edition",
    "descriptor": "\nComments: 166 pages, Kindle Direct Publishing, Amazon 1st Edition\n",
    "authors": [
      "Shahin Nazarian"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2110.06526"
  },
  {
    "id": "arXiv:2110.06565",
    "title": "Duality Temporal-channel-frequency Attention Enhanced Speaker  Representation Learning",
    "abstract": "Duality Temporal-channel-frequency Attention Enhanced Speaker  Representation Learning",
    "descriptor": "",
    "authors": [
      "Li Zhang",
      "Qing Wang",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.06565"
  },
  {
    "id": "arXiv:2110.06595",
    "title": "Refcat: The Internet Archive Scholar Citation Graph",
    "abstract": "Refcat: The Internet Archive Scholar Citation Graph",
    "descriptor": "",
    "authors": [
      "Martin Czygan",
      "Helge Holzmann",
      "Bryan Newbold"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2110.06595"
  },
  {
    "id": "arXiv:2110.06696",
    "title": "Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese",
    "abstract": "Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese",
    "descriptor": "",
    "authors": [
      "Zhuosheng Zhang",
      "Hanqing Zhang",
      "Keming Chen",
      "Yuhang Guo",
      "Jingyun Hua",
      "Yulong Wang",
      "Ming Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06696"
  },
  {
    "id": "arXiv:2110.06763",
    "title": "Efficient Estimation in NPIV Models: A Comparison of Various Neural  Networks-Based Estimators",
    "abstract": "Efficient Estimation in NPIV Models: A Comparison of Various Neural  Networks-Based Estimators",
    "descriptor": "",
    "authors": [
      "Jiafeng Chen",
      "Xiaohong Chen",
      "Elie Tamer"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.06763"
  },
  {
    "id": "arXiv:2110.06864",
    "title": "ByteTrack: Multi-Object Tracking by Associating Every Detection Box",
    "abstract": "ByteTrack: Multi-Object Tracking by Associating Every Detection Box",
    "descriptor": "",
    "authors": [
      "Yifu Zhang",
      "Peize Sun",
      "Yi Jiang",
      "Dongdong Yu",
      "Zehuan Yuan",
      "Ping Luo",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06864"
  }
]