[
  {
    "id": "arXiv:2110.07607",
    "title": "HumBugDB: A Large-scale Acoustic Mosquito Dataset",
    "abstract": "This paper presents the first large-scale multi-species dataset of acoustic\nrecordings of mosquitoes tracked continuously in free flight. We present 20\nhours of audio recordings that we have expertly labelled and tagged precisely\nin time. Significantly, 18 hours of recordings contain annotations from 36\ndifferent species. Mosquitoes are well-known carriers of diseases such as\nmalaria, dengue and yellow fever. Collecting this dataset is motivated by the\nneed to assist applications which utilise mosquito acoustics to conduct surveys\nto help predict outbreaks and inform intervention policy. The task of detecting\nmosquitoes from the sound of their wingbeats is challenging due to the\ndifficulty in collecting recordings from realistic scenarios. To address this,\nas part of the HumBug project, we conducted global experiments to record\nmosquitoes ranging from those bred in culture cages to mosquitoes captured in\nthe wild. Consequently, the audio recordings vary in signal-to-noise ratio and\ncontain a broad range of indoor and outdoor background environments from\nTanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in\ndetail how we collected, labelled and curated the data. The data is provided\nfrom a PostgreSQL database, which contains important metadata such as the\ncapture method, age, feeding status and gender of the mosquitoes. Additionally,\nwe provide code to extract features and train Bayesian convolutional neural\nnetworks for two key tasks: the identification of mosquitoes from their\ncorresponding background environments, and the classification of detected\nmosquitoes into species. Our extensive dataset is both challenging to machine\nlearning researchers focusing on acoustic identification, and critical to\nentomologists, geo-spatial modellers and other domain experts to understand\nmosquito behaviour, model their distribution, and manage the threat they pose\nto humans.",
    "descriptor": "\nComments: Accepted at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. 10 pages main, 39 pages including appendix. This paper accompanies the dataset found at this https URL with corresponding code at this https URL\n",
    "authors": [
      "Ivan Kiskin",
      "Marianne Sinka",
      "Adam D. Cobb",
      "Waqas Rafique",
      "Lawrence Wang",
      "Davide Zilli",
      "Benjamin Gutteridge",
      "Rinita Dam",
      "Theodoros Marinos",
      "Yunpeng Li",
      "Dickson Msaky",
      "Emmanuel Kaindoa",
      "Gerard Killeen",
      "Eva Herreros-Moya",
      "Kathy J. Willis",
      "Stephen J. Roberts"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07607"
  },
  {
    "id": "arXiv:2110.07631",
    "title": "More Efficient Sampling for Tensor Decomposition",
    "abstract": "Recent papers have developed alternating least squares (ALS) methods for CP\nand tensor ring decomposition with a per-iteration cost which is sublinear in\nthe number of input tensor entries for low-rank decomposition. However, the\nper-iteration cost of these methods still has an exponential dependence on the\nnumber of tensor modes. In this paper, we propose sampling-based ALS methods\nfor the CP and tensor ring decompositions whose cost does not have this\nexponential dependence, thereby significantly improving on the previous\nstate-of-the-art. We provide a detailed theoretical analysis and also apply the\nmethods in a feature extraction experiment.",
    "descriptor": "\nComments: 32 pages, 4 figures\n",
    "authors": [
      "Osman Asif Malik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07631"
  },
  {
    "id": "arXiv:2110.07636",
    "title": "A Survey of Machine Learning Algorithms for Detecting Ransomware  Encryption Activity",
    "abstract": "A survey of machine learning techniques trained to detect ransomware is\npresented. This work builds upon the efforts of Taylor et al. in using\nsensor-based methods that utilize data collected from built-in instruments like\nCPU power and temperature monitors to identify encryption activity. Exploratory\ndata analysis (EDA) shows the features most useful from this simulated data are\nclock speed, temperature, and CPU load. These features are used in training\nmultiple algorithms to determine an optimal detection approach. Performance is\nevaluated with accuracy, F1 score, and false-negative rate metrics. The\nMultilayer Perceptron with three hidden layers achieves scores of 97% in\naccuracy and F1 and robust data preparation. A random forest model produces\nscores of 93% accuracy and 92% F1, showing that sensor-based detection is\ncurrently a viable option to detect even zero-day ransomware attacks before the\ncode fully executes.",
    "descriptor": "\nComments: 9 pages, 8 figures, 3 tables\n",
    "authors": [
      "Erik Larsen",
      "David Noever",
      "Korey MacVittie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07636"
  },
  {
    "id": "arXiv:2110.07640",
    "title": "Sparks: Inspiration for Science Writing using Language Models",
    "abstract": "Large-scale language models are rapidly improving, performing well on a wide\nvariety of tasks with little to no customization. In this work we investigate\nhow language models can support science writing, a challenging writing task\nthat is both open-ended and highly constrained. We present a system for\ngenerating \"sparks\", sentences related to a scientific concept intended to\ninspire writers. We find that our sparks are more coherent and diverse than a\ncompetitive language model baseline, and approach a human-created gold\nstandard. In a study with 13 PhD students writing on topics of their own\nselection, we find three main use cases of sparks: aiding with crafting\ndetailed sentences, providing interesting angles to engage readers, and\ndemonstrating common reader perspectives. We also report on the various reasons\nsparks were considered unhelpful, and discuss how we might improve language\nmodels as writing support tools.",
    "descriptor": "",
    "authors": [
      "Katy Ilonka Gero",
      "Vivian Liu",
      "Lydia B. Chilton"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07640"
  },
  {
    "id": "arXiv:2110.07641",
    "title": "Non-deep Networks",
    "abstract": "Depth is the hallmark of deep neural networks. But more depth means more\nsequential computation and higher latency. This begs the question -- is it\npossible to build high-performing \"non-deep\" neural networks? We show that it\nis. To do so, we use parallel subnetworks instead of stacking one layer after\nanother. This helps effectively reduce depth while maintaining high\nperformance. By utilizing parallel substructures, we show, for the first time,\nthat a network with a depth of just 12 can achieve top-1 accuracy over 80% on\nImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with\na low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the\nscaling rules for our design and show how to increase performance without\nchanging the network's depth. Finally, we provide a proof of concept for how\nnon-deep networks could be used to build low-latency recognition systems. Code\nis available at https://github.com/imankgoyal/NonDeepNetworks.",
    "descriptor": "",
    "authors": [
      "Ankit Goyal",
      "Alexey Bochkovskiy",
      "Jia Deng",
      "Vladlen Koltun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07641"
  },
  {
    "id": "arXiv:2110.07646",
    "title": "Talking Detection In Collaborative Learning Environments",
    "abstract": "We study the problem of detecting talking activities in collaborative\nlearning videos. Our approach uses head detection and projections of the\nlog-magnitude of optical flow vectors to reduce the problem to a simple\nclassification of small projection images without the need for training\ncomplex, 3-D activity classification systems. The small projection images are\nthen easily classified using a simple majority vote of standard classifiers.\nFor talking detection, our proposed approach is shown to significantly\noutperform single activity systems. We have an overall accuracy of 59% compared\nto 42% for Temporal Segment Network (TSN) and 45% for Convolutional 3D (C3D).\nIn addition, our method is able to detect multiple talking instances from\nmultiple speakers, while also detecting the speakers themselves.",
    "descriptor": "",
    "authors": [
      "Wenjing Shi",
      "Marios S. Pattichis",
      "Sylvia Celed\u00f3n-Pattichis",
      "Carlos L\u00f3pezLeiva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07646"
  },
  {
    "id": "arXiv:2110.07647",
    "title": "Towards Understanding the Data Dependency of Mixup-style Training",
    "abstract": "In the Mixup training paradigm, a model is trained using convex combinations\nof data points and their associated labels. Despite seeing very few true data\npoints during training, models trained using Mixup seem to still minimize the\noriginal empirical risk and exhibit better generalization and robustness on\nvarious tasks when compared to standard training. In this paper, we investigate\nhow these benefits of Mixup training rely on properties of the data in the\ncontext of classification. For minimizing the original empirical risk, we\ncompute a closed form for the Mixup-optimal classification, which allows us to\nconstruct a simple dataset on which minimizing the Mixup loss can provably lead\nto learning a classifier that does not minimize the empirical loss on the data.\nOn the other hand, we also give sufficient conditions for Mixup training to\nalso minimize the original empirical risk. For generalization, we characterize\nthe margin of a Mixup classifier, and use this to understand why the decision\nboundary of a Mixup classifier can adapt better to the full structure of the\ntraining data when compared to standard training. In contrast, we also show\nthat, for a large class of linear models and linearly separable datasets, Mixup\ntraining leads to learning the same classifier as standard training.",
    "descriptor": "\nComments: 25 pages, 13 figures\n",
    "authors": [
      "Muthu Chidambaram",
      "Xiang Wang",
      "Yuzheng Hu",
      "Chenwei Wu",
      "Rong Ge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07647"
  },
  {
    "id": "arXiv:2110.07653",
    "title": "Non-intrusive reduced-order models for parametric partial differential  equations via data-driven operator inference",
    "abstract": "This work formulates a new approach to reduced modeling of parameterized,\ntime-dependent partial differential equations (PDEs). The method employs\nOperator Inference, a scientific machine learning framework combining\ndata-driven learning and physics-based modeling. The parametric structure of\nthe governing equations is embedded directly into the reduced-order model, and\nparameterized reduced-order operators are learned via a data-driven linear\nregression problem. The result is a reduced-order model that can be solved\nrapidly to map parameter values to approximate PDE solutions. Such\nparameterized reduced-order models may be used as physics-based surrogates for\nuncertainty quantification and inverse problems that require many forward\nsolves of parametric PDEs. Numerical issues such as well-posedness and the need\nfor appropriate regularization in the learning problem are considered, and an\nalgorithm for hyperparameter selection is presented. The method is illustrated\nfor a parametric heat equation and demonstrated for the FitzHugh-Nagumo neuron\nmodel.",
    "descriptor": "",
    "authors": [
      "Shane A McQuarrie",
      "Parisa Khodabakhshi",
      "Karen E Willcox"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07653"
  },
  {
    "id": "arXiv:2110.07654",
    "title": "Residual2Vec: Debiasing graph embedding with random graphs",
    "abstract": "Graph embedding maps a graph into a convenient vector-space representation\nfor graph analysis and machine learning applications. Many graph embedding\nmethods hinge on a sampling of context nodes based on random walks. However,\nrandom walks can be a biased sampler due to the structural properties of\ngraphs. Most notably, random walks are biased by the degree of each node, where\na node is sampled proportionally to its degree. The implication of such biases\nhas not been clear, particularly in the context of graph representation\nlearning. Here, we investigate the impact of the random walks' bias on graph\nembedding and propose residual2vec, a general graph embedding method that can\ndebias various structural biases in graphs by using random graphs. We\ndemonstrate that this debiasing not only improves link prediction and\nclustering performance but also allows us to explicitly model salient\nstructural properties in graph embedding.",
    "descriptor": "\nComments: 28 pages, 8 figures, 3 tables\n",
    "authors": [
      "Sadamori Kojaku",
      "Jisung Yoon",
      "Isabel Constantino",
      "Yong-Yeol Ahn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07654"
  },
  {
    "id": "arXiv:2110.07658",
    "title": "Predicting Solar Flares with Remote Sensing and Machine Learning",
    "abstract": "High energy solar flares and coronal mass ejections have the potential to\ndestroy Earth's ground and satellite infrastructures, causing trillions of\ndollars in damage and mass human suffering. Destruction of these critical\nsystems would disable power grids and satellites, crippling communications and\ntransportation. This would lead to food shortages and an inability to respond\nto emergencies. A solution to this impending problem is proposed herein using\nsatellites in solar orbit that continuously monitor the Sun, use artificial\nintelligence and machine learning to calculate the probability of massive solar\nexplosions from this sensed data, and then signal defense mechanisms that will\nmitigate the threat. With modern technology there may be only safeguards that\ncan be implemented with enough warning, which is why the best algorithm must be\nidentified and continuously trained with existing and new data to maximize true\npositive rates while minimizing false negatives. This paper conducts a survey\nof current machine learning models using open source solar flare prediction\ndata. The rise of edge computing allows machine learning hardware to be placed\non the same satellites as the sensor arrays, saving critical time by not having\nto transmit remote sensing data across the vast distances of space. A system of\nsystems approach will allow enough warning for safety measures to be put into\nplace mitigating the risk of disaster.",
    "descriptor": "\nComments: 16 pages, 10 figures, 3 tables\n",
    "authors": [
      "Erik Larsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Solar and Stellar Astrophysics (astro-ph.SR)"
    ],
    "url": "https://arxiv.org/abs/2110.07658"
  },
  {
    "id": "arXiv:2110.07660",
    "title": "A Semi-Supervised Approach for Abnormal Event Prediction on Large  Operational Network Time-Series Data",
    "abstract": "Large network logs, recording multivariate time series generated from\nheterogeneous devices and sensors in a network, can often reveal important\ninformation about abnormal activities, such as network intrusions and device\nmalfunctions. Existing machine learning methods for anomaly detection on\nmultivariate time series typically assume that 1) normal sequences would have\nconsistent behavior for training unsupervised models, or 2) require a large set\nof labeled normal and abnormal sequences for supervised models. However, in\npractice, normal network activities can demonstrate significantly varying\nsequence patterns (e.g., before and after rerouting partial network traffic).\nAlso, the recorded abnormal events can be sparse. This paper presents a novel\nsemi-supervised method that efficiently captures dependencies between network\ntime series and across time points to generate meaningful representations of\nnetwork activities for predicting abnormal events. The method can use the\nlimited labeled data to explicitly learn separable embedding space for normal\nand abnormal samples and effectively leverage unlabeled data to handle training\ndata scarcity. The experiments demonstrate that our approach significantly\noutperformed state-of-the-art approaches for event detection on a large\nreal-world network log.",
    "descriptor": "\nComments: 9 pages + 3 supplementary pages; submitted to SDM2022\n",
    "authors": [
      "Yijun Lin",
      "Yao-Yi Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07660"
  },
  {
    "id": "arXiv:2110.07661",
    "title": "Distribution-Free Federated Learning with Conformal Predictions",
    "abstract": "Federated learning has attracted considerable interest for collaborative\nmachine learning in healthcare to leverage separate institutional datasets\nwhile maintaining patient privacy.\nHowever, additional challenges such as poor calibration and lack of\ninterpretability may also hamper widespread deployment of federated models into\nclinical practice and lead to user distrust or misuse of ML tools in\nhigh-stakes clinical decision-making.\nIn this paper, we propose to address these challenges by incorporating an\nadaptive conformal framework into federated learning to ensure\ndistribution-free prediction sets that provide coverage guarantees and\nuncertainty estimates without requiring any additional modifications to the\nmodel or assumptions.\nEmpirical results on the MedMNIST medical imaging benchmark demonstrate our\nfederated method provide tighter coverage in lower average cardinality over\nlocal conformal predictions on 6 different medical imaging benchmark datasets\nin 2D and 3D multi-class classification tasks.\nFurther, we correlate class entropy and prediction set size to assess task\nuncertainty with conformal methods.",
    "descriptor": "",
    "authors": [
      "Charles Lu",
      "Jayasheree Kalpathy-Cramer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07661"
  },
  {
    "id": "arXiv:2110.07663",
    "title": "Auto-Tuned Preconditioners for the Spectral Element Method on GPUs",
    "abstract": "The Poisson pressure solve resulting from the spectral element discretization\nof the incompressible Navier-Stokes equation requires fast, robust, and\nscalable preconditioning. In the current work, a parallel scaling study of\nChebyshev-accelerated Schwarz and Jacobi preconditioning schemes is presented,\nwith special focus on GPU architectures, such as OLCF's Summit. Convergence\nproperties of the Chebyshev-accelerated schemes are compared with alternative\nmethods, such as low-order preconditioners combined with algebraic multigrid.\nPerformance and scalability results are presented for a variety of\npreconditioner and solver settings. The authors demonstrate that\nChebyshev-accelerated-Schwarz methods provide a robust and effective smoothing\nstrategy when using $p$-multigrid as a preconditioner in a Krylov-subspace\nprojector. At the same time, optimal preconditioning parameters can vary for\ndifferent geometries, problem sizes, and processor counts. This variance\nmotivates the development of an autotuner to optimize solver parameters\non-line, during the course of production simulations.",
    "descriptor": "\nComments: 12 pages, 6 figures, submitted to SIAM Conference on Parallel Processing for Scientific Computing (PP22) proceedings\n",
    "authors": [
      "Malachi Phillips",
      "Stefan Kerkemeier",
      "Paul Fischer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.07663"
  },
  {
    "id": "arXiv:2110.07667",
    "title": "Interactive Analysis of CNN Robustness",
    "abstract": "While convolutional neural networks (CNNs) have found wide adoption as\nstate-of-the-art models for image-related tasks, their predictions are often\nhighly sensitive to small input perturbations, which the human vision is robust\nagainst. This paper presents Perturber, a web-based application that allows\nusers to instantaneously explore how CNN activations and predictions evolve\nwhen a 3D input scene is interactively perturbed. Perturber offers a large\nvariety of scene modifications, such as camera controls, lighting and shading\neffects, background modifications, object morphing, as well as adversarial\nattacks, to facilitate the discovery of potential vulnerabilities. Fine-tuned\nmodel versions can be directly compared for qualitative evaluation of their\nrobustness. Case studies with machine learning experts have shown that\nPerturber helps users to quickly generate hypotheses about model\nvulnerabilities and to qualitatively compare model behavior. Using quantitative\nanalyses, we could replicate users' insights with other CNN architectures and\ninput images, yielding new insights about the vulnerability of adversarially\ntrained models.",
    "descriptor": "\nComments: Accepted at Pacific Graphics 2021\n",
    "authors": [
      "Stefan Sietzen",
      "Mathias Lechner",
      "Judy Borowski",
      "Ramin Hasani",
      "Manuela Waldner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07667"
  },
  {
    "id": "arXiv:2110.07668",
    "title": "Augmenting Imitation Experience via Equivariant Representations",
    "abstract": "The robustness of visual navigation policies trained through imitation often\nhinges on the augmentation of the training image-action pairs. Traditionally,\nthis has been done by collecting data from multiple cameras, by using standard\ndata augmentations from computer vision, such as adding random noise to each\nimage, or by synthesizing training images. In this paper we show that there is\nanother practical alternative for data augmentation for visual navigation based\non extrapolating viewpoint embeddings and actions nearby the ones observed in\nthe training data. Our method makes use of the geometry of the visual\nnavigation problem in 2D and 3D and relies on policies that are functions of\nequivariant embeddings, as opposed to images. Given an image-action pair from a\ntraining navigation dataset, our neural network model predicts the latent\nrepresentations of images at nearby viewpoints, using the equivariance\nproperty, and augments the dataset. We then train a policy on the augmented\ndataset. Our simulation results indicate that policies trained in this way\nexhibit reduced cross-track error, and require fewer interventions compared to\npolicies trained using standard augmentation methods. We also show similar\nresults in autonomous visual navigation by a real ground robot along a path of\nover 500m.",
    "descriptor": "\nComments: 7 pages (including references), 15 figures\n",
    "authors": [
      "Dhruv Sharma",
      "Alihusein Kuwajerwala",
      "Florian Shkurti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07668"
  },
  {
    "id": "arXiv:2110.07674",
    "title": "Appearance Editing with Free-viewpoint Neural Rendering",
    "abstract": "We present a neural rendering framework for simultaneous view synthesis and\nappearance editing of a scene from multi-view images captured under known\nenvironment illumination. Existing approaches either achieve view synthesis\nalone or view synthesis along with relighting, without direct control over the\nscene's appearance. Our approach explicitly disentangles the appearance and\nlearns a lighting representation that is independent of it. Specifically, we\nindependently estimate the BRDF and use it to learn a lighting-only\nrepresentation of the scene. Such disentanglement allows our approach to\ngeneralize to arbitrary changes in appearance while performing view synthesis.\nWe show results of editing the appearance of a real scene, demonstrating that\nour approach produces plausible appearance editing. The performance of our view\nsynthesis approach is demonstrated to be at par with state-of-the-art\napproaches on both real and synthetic data.",
    "descriptor": "",
    "authors": [
      "Pulkit Gera",
      "Aakash KT",
      "Dhawal Sirikonda",
      "Parikshit Sakurikar",
      "P.J. Narayanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07674"
  },
  {
    "id": "arXiv:2110.07676",
    "title": "A data-driven model reduction method for parabolic inverse source  problems and its convergence analysis",
    "abstract": "In this paper, we propose a data-driven model reduction method to solve\nparabolic inverse source problems efficiently. Our method consists of offline\nand online stages. In the off-line stage, we explore the low-dimensional\nstructures in the solution space of the parabolic partial differential\nequations (PDEs) in the forward problem with a given class of source functions\nand construct a small number of proper orthogonal decomposition (POD) basis\nfunctions to achieve significant dimension reduction. Equipped with the POD\nbasis functions, we can solve the forward problem extremely fast in the online\nstage. Thus, we develop a fast algorithm to solve the optimization problem in\nthe parabolic inverse source problems, which is referred to as the POD\nalgorithm in this paper. Under a weak regularity assumption on the solution of\nthe parabolic PDEs, we prove the convergence of the POD algorithm in solving\nthe forward parabolic PDEs. In addition, we obtain the error estimate of the\nPOD algorithm for parabolic inverse source problems. Finally, we present\nnumerical examples to demonstrate the accuracy and efficiency of the proposed\nmethod. Our numerical results show that the POD algorithm provides considerable\ncomputational savings over the finite element method.",
    "descriptor": "",
    "authors": [
      "Zhongjian Wang",
      "Wenlong Zhang",
      "Zhiwen Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07676"
  },
  {
    "id": "arXiv:2110.07678",
    "title": "Performance Analysis of a MIMO System with Bursty Traffic in the  presence of Energy Harvesting Jammer",
    "abstract": "This paper explores the role of multiple antennas in mitigating jamming\nattacks for the Rayleigh fading environment with exogenous random traffic\narrival. The jammer is assumed to have energy harvesting ability where energy\narrives according to Bernoulli process. The outage probabilities are derived\nwith different assumptions on the number of antennas at the transmitter and\nreceiver. The outage probability for the Alamouti space-time code is also\nderived. The work characterizes the average service rate for different antenna\nconfigurations taking into account of random arrival of data and energy at the\ntransmitter and jammer, respectively. In many practical applications, latency\nand timely updates are of importance, thus, delay and Average Age of\nInformation (AAoI) are the meaningful metrics to be considered. The work\ncharacterizes these metrics under jamming attack. The impact of finite and\ninfinite energy battery size at the jammer on various performance metrics is\nalso explored. Two optimization problems are considered to explore the\ninterplay between AAoI and delay under jamming attack. Furthermore, our results\nshow that Alamouti code can significantly improve the performance of the system\neven under jamming attack, with less power budget. The paper also demonstrates\nhow the developed results can be useful for multiuser scenarios.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Green Communications and Networking\n",
    "authors": [
      "Sujatha Allipuram",
      "Parthajit Mohapatra",
      "Nikolaos Pappas",
      "Shabnam Parmar",
      "Saswat Chakrabarti"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07678"
  },
  {
    "id": "arXiv:2110.07679",
    "title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented  Dialogue Systems",
    "abstract": "Much recent progress in task-oriented dialogue (ToD) systems has been driven\nby available annotation data across multiple domains for training. Over the\nlast few years, there has been a move towards data curation for multilingual\nToD systems that are applicable to serve people speaking different languages.\nHowever, existing multilingual ToD datasets either have a limited coverage of\nlanguages due to the high cost of data curation, or ignore the fact that\ndialogue entities barely exist in countries speaking these languages. To tackle\nthese limitations, we introduce a novel data curation method that generates\nGlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English\nToD dataset for three unexplored use cases. Our method is based on translating\ndialogue templates and filling them with local entities in the target-language\ncountries. We release our dataset as well as a set of strong baselines to\nencourage research on learning multilingual ToD systems for real use cases.",
    "descriptor": "",
    "authors": [
      "Bosheng Ding",
      "Junjie Hu",
      "Lidong Bing",
      "Sharifah Mahani Aljunied",
      "Shafiq Joty",
      "Luo Si",
      "Chunyan Miao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07679"
  },
  {
    "id": "arXiv:2110.07681",
    "title": "Large Scale Substitution-based Word Sense Induction",
    "abstract": "We present a word-sense induction method based on pre-trained masked language\nmodels (MLMs), which can cheaply scale to large vocabularies and large corpora.\nThe result is a corpus which is sense-tagged according to a corpus-derived\nsense inventory and where each sense is associated with indicative words.\nEvaluation on English Wikipedia that was sense-tagged using our method shows\nthat both the induced senses, and the per-instance sense assignment, are of\nhigh quality even compared to WSD methods, such as Babelfy. Furthermore, by\ntraining a static word embeddings algorithm on the sense-tagged corpus, we\nobtain high-quality static senseful embeddings. These outperform existing\nsenseful embeddings techniques on the WiC dataset and on a new outlier\ndetection dataset we developed. The data driven nature of the algorithm allows\nto induce corpora-specific senses, which may not appear in standard sense\ninventories, as we demonstrate using a case study on the scientific domain.",
    "descriptor": "",
    "authors": [
      "Matan Eyal",
      "Shoval Sadde",
      "Hillel Taub-Tabib",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07681"
  },
  {
    "id": "arXiv:2110.07682",
    "title": "Sound and Complete Neural Network Repair with Minimality and Locality  Guarantees",
    "abstract": "We present a novel methodology for repairing neural networks that use ReLU\nactivation functions. Unlike existing methods that rely on modifying the\nweights of a neural network which can induce a global change in the function\nspace, our approach applies only a localized change in the function space while\nstill guaranteeing the removal of the buggy behavior. By leveraging the\npiecewise linear nature of ReLU networks, our approach can efficiently\nconstruct a patch network tailored to the linear region where the buggy input\nresides, which when combined with the original network, provably corrects the\nbehavior on the buggy input. Our method is both sound and complete -- the\nrepaired network is guaranteed to fix the buggy input, and a patch is\nguaranteed to be found for any buggy input. Moreover, our approach preserves\nthe continuous piecewise linear nature of ReLU networks, automatically\ngeneralizes the repair to all the points including other undetected buggy\ninputs inside the repair region, is minimal in terms of changes in the function\nspace, and guarantees that outputs on inputs away from the repair region are\nunaltered. On several benchmarks, we show that our approach significantly\noutperforms existing methods in terms of locality and limiting negative side\neffects.",
    "descriptor": "\nComments: 16 pages, 3 figures\n",
    "authors": [
      "Feisi Fu",
      "Wenchao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07682"
  },
  {
    "id": "arXiv:2110.07683",
    "title": "An Optimization Perspective on Realizing Backdoor Injection Attacks on  Deep Neural Networks in Hardware",
    "abstract": "State-of-the-art deep neural networks (DNNs) have been proven to be\nvulnerable to adversarial manipulation and backdoor attacks. Backdoored models\ndeviate from expected behavior on inputs with predefined triggers while\nretaining performance on clean data. Recent works focus on software simulation\nof backdoor injection during the inference phase by modifying network weights,\nwhich we find often unrealistic in practice due to the hardware restriction\nsuch as bit allocation in memory. In contrast, in this work, we investigate the\nviability of backdoor injection attacks in real-life deployments of DNNs on\nhardware and address such practical issues in hardware implementation from a\nnovel optimization perspective. We are motivated by the fact that the\nvulnerable memory locations are very rare, device-specific, and sparsely\ndistributed. Consequently, we propose a novel network training algorithm based\non constrained optimization for realistic backdoor injection attack in\nhardware. By modifying parameters uniformly across the convolutional and\nfully-connected layers as well as optimizing the trigger pattern together, we\nachieve the state-of-the-art attack performance with fewer bit flips. For\ninstance, our method on a hardware-deployed ResNet-20 model trained on CIFAR-10\ncan achieve over 91% test accuracy and 94% attack success rate by flipping only\n10 bits out of 2.2 million bits.",
    "descriptor": "",
    "authors": [
      "M. Caner Tol",
      "Saad Islam",
      "Berk Sunar",
      "Ziming Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.07683"
  },
  {
    "id": "arXiv:2110.07686",
    "title": "Making Document-Level Information Extraction Right for the Right Reasons",
    "abstract": "Document-level information extraction is a flexible framework compatible with\napplications where information is not necessarily localized in a single\nsentence. For example, key features of a diagnosis in radiology a report may\nnot be explicitly stated, but nevertheless can be inferred from the report's\ntext. However, document-level neural models can easily learn spurious\ncorrelations from irrelevant information. This work studies how to ensure that\nthese models make correct inferences from complex text and make those\ninferences in an auditable way: beyond just being right, are these models\n\"right for the right reasons?\" We experiment with post-hoc evidence extraction\nin a predict-select-verify framework using feature attribution techniques.\nWhile this basic approach can extract reasonable evidence, it can be\nregularized with small amounts of evidence supervision during training, which\nsubstantially improves the quality of extracted evidence. We evaluate on two\ndomains: a small-scale labeled dataset of brain MRI reports and a large-scale\nmodified version of DocRED (Yao et al., 2019) and show that models'\nplausibility can be improved with no loss in accuracy.",
    "descriptor": "\nComments: 9 pages (14 with references and appendix), 3 figures\n",
    "authors": [
      "Liyan Tang",
      "Dhruv Rajan",
      "Suyash Mohan",
      "Abhijeet Pradhan",
      "R. Nick Bryan",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07686"
  },
  {
    "id": "arXiv:2110.07689",
    "title": "First-Order Modal $\u03be$-Calculus",
    "abstract": "This paper proposes first-order modal $\\xi$-calculus as well as genealogical\nKripke models. Inspired by modal $\\mu$-calculus, first-order modal\n$\\xi$-calculus takes a quite similar form and extends its inductive\nexpressivity onto a different dimension. We elaborate on several vivid examples\nthat demonstrate this logic's profound utility, especially for depicting\ngenealogy of concurrent computer processes. Bisimulation notion for the logic\nhas also been thoroughly examined.",
    "descriptor": "",
    "authors": [
      "Xinyu Wang"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.07689"
  },
  {
    "id": "arXiv:2110.07692",
    "title": "Shaping embodied agent behavior with activity-context priors from  egocentric video",
    "abstract": "Complex physical tasks entail a sequence of object interactions, each with\nits own preconditions -- which can be difficult for robotic agents to learn\nefficiently solely through their own experience. We introduce an approach to\ndiscover activity-context priors from in-the-wild egocentric video captured\nwith human worn cameras. For a given object, an activity-context prior\nrepresents the set of other compatible objects that are required for activities\nto succeed (e.g., a knife and cutting board brought together with a tomato are\nconducive to cutting). We encode our video-based prior as an auxiliary reward\nfunction that encourages an agent to bring compatible objects together before\nattempting an interaction. In this way, our model translates everyday human\nexperience into embodied agent skills. We demonstrate our idea using egocentric\nEPIC-Kitchens video of people performing unscripted kitchen activities to\nbenefit virtual household robotic agents performing various complex tasks in\nAI2-iTHOR, significantly accelerating agent learning. Project page:\nthis http URL",
    "descriptor": "",
    "authors": [
      "Tushar Nagarajan",
      "Kristen Grauman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07692"
  },
  {
    "id": "arXiv:2110.07693",
    "title": "Is Stance Detection Topic-Independent and Cross-topic Generalizable? --  A Reproduction Study",
    "abstract": "Cross-topic stance detection is the task to automatically detect stances\n(pro, against, or neutral) on unseen topics. We successfully reproduce\nstate-of-the-art cross-topic stance detection work (Reimers et. al., 2019), and\nsystematically analyze its reproducibility. Our attention then turns to the\ncross-topic aspect of this work, and the specificity of topics in terms of\nvocabulary and socio-cultural context. We ask: To what extent is stance\ndetection topic-independent and generalizable across topics? We compare the\nmodel's performance on various unseen topics, and find topic (e.g. abortion,\ncloning), class (e.g. pro, con), and their interaction affecting the model's\nperformance. We conclude that investigating performance on different topics,\nand addressing topic-specific vocabulary and context, is a future avenue for\ncross-topic stance detection.",
    "descriptor": "\nComments: Accepted at the 8th Workshop on Argument Mining, 2021 co-located with EMNLP 2021. Cite the published version\n",
    "authors": [
      "Myrthe Reuver",
      "Suzan Verberne",
      "Roser Morante",
      "Antske Fokkens"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07693"
  },
  {
    "id": "arXiv:2110.07699",
    "title": "Safety-aware Policy Optimisation for Autonomous Racing",
    "abstract": "To be viable for safety-critical applications, such as autonomous driving and\nassistive robotics, autonomous agents should adhere to safety constraints\nthroughout the interactions with their environments. Instead of learning about\nsafety by collecting samples, including unsafe ones, methods such as\nHamilton-Jacobi (HJ) reachability compute safe sets with theoretical guarantees\nusing models of the system dynamics. However, HJ reachability is not scalable\nto high-dimensional systems, and the guarantees hinge on the quality of the\nmodel. In this work, we inject HJ reachability theory into the constrained\nMarkov decision process (CMDP) framework, as a control-theoretical approach for\nsafety analysis via model-free updates on state-action pairs. Furthermore, we\ndemonstrate that the HJ safety value can be learned directly on vision context,\nthe highest-dimensional problem studied via the method to-date. We evaluate our\nmethod on several benchmark tasks, including Safety Gym and Learn-to-Race\n(L2R), a recently-released high-fidelity autonomous racing environment. Our\napproach has significantly fewer constraint violations in comparison to other\nconstrained RL baselines, and achieve the new state-of-the-art results on the\nL2R benchmark task.",
    "descriptor": "\nComments: 22 pages, 14 figures, 3 tables\n",
    "authors": [
      "Bingqing Chen",
      "Jonathan Francis",
      "James Herman",
      "Jean Oh",
      "Eric Nyberg",
      "Sylvia L. Herbert"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07699"
  },
  {
    "id": "arXiv:2110.07700",
    "title": "Hindsight Network Credit Assignment: Efficient Credit Assignment in  Networks of Discrete Stochastic Units",
    "abstract": "Training neural networks with discrete stochastic variables presents a unique\nchallenge. Backpropagation is not directly applicable, nor are the\nreparameterization tricks used in networks with continuous stochastic\nvariables. To address this challenge, we present Hindsight Network Credit\nAssignment (HNCA), a novel learning algorithm for networks of discrete\nstochastic units. HNCA works by assigning credit to each unit based on the\ndegree to which its output influences its immediate children in the network. We\nprove that HNCA produces unbiased gradient estimates with reduced variance\ncompared to the REINFORCE estimator, while the computational cost is similar to\nthat of backpropagation. We first apply HNCA in a contextual bandit setting to\noptimize a reward function that is unknown to the agent. In this setting, we\nempirically demonstrate that HNCA significantly outperforms REINFORCE,\nindicating that the variance reduction implied by our theoretical analysis is\nsignificant and impactful. We then show how HNCA can be extended to optimize a\nmore general function of the outputs of a network of stochastic units, where\nthe function is known to the agent. We apply this extended version of HNCA to\ntrain a discrete variational auto-encoder and empirically show it compares\nfavourably to other strong methods. We believe that the ideas underlying HNCA\ncan help stimulate new ways of thinking about efficient credit assignment in\nstochastic compute graphs.",
    "descriptor": "",
    "authors": [
      "Kenny Young"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07700"
  },
  {
    "id": "arXiv:2110.07701",
    "title": "Exposing Query Identification for Search Transparency",
    "abstract": "Search systems control the exposure of ranked content to searchers. In many\ncases, creators value not only the exposure of their content but, moreover, an\nunderstanding of the specific searches where the content is surfaced. The\nproblem of identifying which queries expose a given piece of content in the\nranking results is an important and relatively under-explored search\ntransparency challenge. Exposing queries are useful for quantifying various\nissues of search bias, privacy, data protection, security, and search engine\noptimization.\nExact identification of exposing queries in a given system is computationally\nexpensive, especially in dynamic contexts such as web search. In quest of a\nmore lightweight solution, we explore the feasibility of approximate exposing\nquery identification (EQI) as a retrieval task by reversing the role of queries\nand documents in two classes of search systems: dense dual-encoder models and\ntraditional BM25 models. We then propose how this approach can be improved\nthrough metric learning over the retrieval embedding space. We further derive\nan evaluation metric to measure the quality of a ranking of exposing queries,\nas well as conducting an empirical analysis focusing on various practical\naspects of approximate EQI.",
    "descriptor": "",
    "authors": [
      "Ruohan Li",
      "Jianxiang Li",
      "Bhaskar Mitra",
      "Fernando Diaz",
      "Asia J. Biega"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07701"
  },
  {
    "id": "arXiv:2110.07703",
    "title": "ASK: Adaptively Selecting Key Local Features for RGB-D Scene Recognition",
    "abstract": "Indoor scene images usually contain scattered objects and various scene\nlayouts, which make RGB-D scene classification a challenging task. Existing\nmethods still have limitations for classifying scene images with great spatial\nvariability. Thus, how to extract local patch-level features effectively using\nonly image labels is still an open problem for RGB-D scene recognition. In this\npaper, we propose an efficient framework for RGB-D scene recognition, which\nadaptively selects important local features to capture the great spatial\nvariability of scene images. Specifically, we design a differentiable local\nfeature selection (DLFS) module, which can extract the appropriate number of\nkey local scenerelated features. Discriminative local theme-level and\nobject-level representations can be selected with the DLFS module from the\nspatially-correlated multi-modal RGB-D features. We take advantage of the\ncorrelation between RGB and depth modalities to provide more cues for selecting\nlocal features. To ensure that discriminative local features are selected, the\nvariational mutual information maximization loss is proposed. Additionally, the\nDLFS module can be easily extended to select local features of different\nscales. By concatenating the local-orderless and global structured multi-modal\nfeatures, the proposed framework can achieve state-of-the-art performance on\npublic RGB-D scene recognition datasets.",
    "descriptor": "",
    "authors": [
      "Zhitong Xiong",
      "Yuan Yuan",
      "Qi Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07703"
  },
  {
    "id": "arXiv:2110.07704",
    "title": "Uplink Power Control in Integrated Access and Backhaul Networks",
    "abstract": "Integrated access and backhaul (IAB) network is a novel radio access network\n(RAN) solution, enabling network densification for 5G and beyond. In this\npaper, we use power control combined with resource allocation algorithms to\ndevelop efficient IAB networks with high service coverage. Particularly, we\ndevelop a genetic algorithm-based solution for the power control of both user\nequipments and IAB nodes such that the network uplink service coverage\nprobability is maximized. Finally, considering millimeter wave channel models,\nwe study the effect of different parameters including minimum data rate\nrequirement, coverage distance and transmit power on the network performance.\nAs we show, a power allocation schemes with well-tuned parameters can improve\nthe uplink performance of IAB networks considerably. Moreover, with millimeter\nwave communications and a proper network deployment, the effect of interference\non the service coverage probability is negligible.",
    "descriptor": "\nComments: 6 pages, 6 figures\n",
    "authors": [
      "Olalekan Peter Adare",
      "Haitham Babbili",
      "Charitha Madapatha",
      "Behrooz Makki",
      "Tommy Svensson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07704"
  },
  {
    "id": "arXiv:2110.07706",
    "title": "On the proper interval completion problem within some chordal subclasses",
    "abstract": "Given a property (graph class) $\\Pi$, a graph $G$, and an integer $k$, the\n\\emph{$\\Pi$-completion} problem consists in deciding whether we can turn $G$\ninto a graph with the property $\\Pi$ by adding at most $k$ edges to $G$. The\n$\\Pi$-completion problem is known to be NP-hard for general graphs when $\\Pi$\nis the property of being a proper interval graph (PIG). In this work, we study\nthe PIG-completion problem %when $\\Pi$ is the class of proper interval graphs\n(PIG) within different subclasses of chordal graphs. We show that the problem\nremains NP-complete even when restricted to split graphs. We then turn our\nattention to positive results and present polynomial time algorithms to solve\nthe PIG-completion problem when the input is restricted to caterpillar and\nthreshold graphs. We also present an efficient algorithm for the minimum\nco-bipartite-completion for quasi-threshold graphs, which provides a lower\nbound for the PIG-completion problem within this graph class.",
    "descriptor": "\nComments: 13 pages, 2 figures\n",
    "authors": [
      "Fran\u00e7ois Dross",
      "Claire Hilaire",
      "Ivo Koch",
      "Valeria Leoni",
      "Nina Pardal",
      "Mar\u00eda In\u00e9s Lopez Pujato",
      "Vinicius Fernandes dos Santos"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2110.07706"
  },
  {
    "id": "arXiv:2110.07710",
    "title": "Semi-automated checking for regulatory compliance in e-Health",
    "abstract": "One of the main issues of every business process is to be compliant with\nlegal rules. This work presents a methodology to check in a semi-automated way\nthe regulatory compliance of a business process. We analyse an e-Health\nhospital service in particular: the Hospital at Home (HaH) service. The paper\nshows, at first, the analysis of the hospital business using the Business\nProcess Management and Notation (BPMN) standard language, then, the\nformalization in Defeasible Deontic Logic (DDL) of some rules of the European\nGeneral Data Protection Regulation (GDPR). The aim is to show how to combine a\nset of tasks of a business with a set of rules to be compliant with, using a\ntool.",
    "descriptor": "",
    "authors": [
      "Ilaria Angela Amantea",
      "Livio Robaldo",
      "Emilio Sulis",
      "Guido Boella",
      "Guido Governatori"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07710"
  },
  {
    "id": "arXiv:2110.07716",
    "title": "Adversarial Scene Reconstruction and Object Detection System for  Assisting Autonomous Vehicle",
    "abstract": "In the current computer vision era classifying scenes through video\nsurveillance systems is a crucial task. Artificial Intelligence (AI) Video\nSurveillance technologies have been advanced remarkably while artificial\nintelligence and deep learning ascended into the system. Adopting the superior\ncompounds of deep learning visual classification methods achieved enormous\naccuracy in classifying visual scenes. However, the visual classifiers face\ndifficulties examining the scenes in dark visible areas, especially during the\nnighttime. Also, the classifiers face difficulties in identifying the contexts\nof the scenes. This paper proposed a deep learning model that reconstructs dark\nvisual scenes to clear scenes like daylight, and the method recognizes visual\nactions for the autonomous vehicle. The proposed model achieved 87.3 percent\naccuracy for scene reconstruction and 89.2 percent in scene understanding and\ndetection tasks.",
    "descriptor": "",
    "authors": [
      "Md Foysal Haque",
      "Hay-Youn Lim",
      "Dae-Seong Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07716"
  },
  {
    "id": "arXiv:2110.07717",
    "title": "Deep Human-guided Conditional Variational Generative Modeling for  Automated Urban Planning",
    "abstract": "Urban planning designs land-use configurations and can benefit building\nlivable, sustainable, safe communities. Inspired by image generation, deep\nurban planning aims to leverage deep learning to generate land-use\nconfigurations. However, urban planning is a complex process. Existing studies\nusually ignore the need of personalized human guidance in planning, and spatial\nhierarchical structure in planning generation. Moreover, the lack of\nlarge-scale land-use configuration samples poses a data sparsity challenge.\nThis paper studies a novel deep human guided urban planning method to jointly\nsolve the above challenges. Specifically, we formulate the problem into a deep\nconditional variational autoencoder based framework. In this framework, we\nexploit the deep encoder-decoder design to generate land-use configurations. To\ncapture the spatial hierarchy structure of land uses, we enforce the decoder to\ngenerate both the coarse-grained layer of functional zones, and the\nfine-grained layer of POI distributions. To integrate human guidance, we allow\nhumans to describe what they need as texts and use these texts as a model\ncondition input. To mitigate training data sparsity and improve model\nrobustness, we introduce a variational Gaussian embedding mechanism. It not\njust allows us to better approximate the embedding space distribution of\ntraining data and sample a larger population to overcome sparsity, but also\nadds more probabilistic randomness into the urban planning generation to\nimprove embedding diversity so as to improve robustness. Finally, we present\nextensive experiments to validate the enhanced performances of our method.",
    "descriptor": "\nComments: ICDM2021\n",
    "authors": [
      "Dongjie Wang",
      "Kunpeng Liu",
      "Pauline Johnson",
      "Leilei Sun",
      "Bowen Du",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07717"
  },
  {
    "id": "arXiv:2110.07718",
    "title": "Adversarial Attack across Datasets",
    "abstract": "It has been observed that Deep Neural Networks (DNNs) are vulnerable to\ntransfer attacks in the query-free black-box setting. However, all the previous\nstudies on transfer attack assume that the white-box surrogate models possessed\nby the attacker and the black-box victim models are trained on the same\ndataset, which means the attacker implicitly knows the label set and the input\nsize of the victim model. However, this assumption is usually unrealistic as\nthe attacker may not know the dataset used by the victim model, and further,\nthe attacker needs to attack any randomly encountered images that may not come\nfrom the same dataset. Therefore, in this paper we define a new Generalized\nTransferable Attack (GTA) problem where we assume the attacker has a set of\nsurrogate models trained on different datasets (with different label sets and\nimage sizes), and none of them is equal to the dataset used by the victim\nmodel. We then propose a novel method called Image Classification Eraser (ICE)\nto erase classification information for any encountered images from arbitrary\ndataset. Extensive experiments on Cifar-10, Cifar-100, and TieredImageNet\ndemonstrate the effectiveness of the proposed ICE on the GTA problem.\nFurthermore, we show that existing transfer attack methods can be modified to\ntackle the GTA problem, but with significantly worse performance compared with\nICE.",
    "descriptor": "",
    "authors": [
      "Yunxiao Qin",
      "Yuanhao Xiong",
      "Jinfeng Yi",
      "Cho-Jui Hsieh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07718"
  },
  {
    "id": "arXiv:2110.07719",
    "title": "Certified Patch Robustness via Smoothed Vision Transformers",
    "abstract": "Certified patch defenses can guarantee robustness of an image classifier to\narbitrary changes within a bounded contiguous region. But, currently, this\nrobustness comes at a cost of degraded standard accuracies and slower inference\ntimes. We demonstrate how using vision transformers enables significantly\nbetter certified patch robustness that is also more computationally efficient\nand does not incur a substantial drop in standard accuracy. These improvements\nstem from the inherent ability of the vision transformer to gracefully handle\nlargely masked images. Our code is available at\nhttps://github.com/MadryLab/smoothed-vit.",
    "descriptor": "",
    "authors": [
      "Hadi Salman",
      "Saachi Jain",
      "Eric Wong",
      "Aleksander M\u0105dry"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07719"
  },
  {
    "id": "arXiv:2110.07720",
    "title": "Decomposing Convolutional Neural Networks into Reusable and Replaceable  Modules",
    "abstract": "Training from scratch is the most common way to build a Convolutional Neural\nNetwork (CNN) based model. What if we can build new CNN models by reusing parts\nfrom previously build CNN models? What if we can improve a CNN model by\nreplacing (possibly faulty) parts with other parts? In both cases, instead of\ntraining, can we identify the part responsible for each output class (module)\nin the model(s) and reuse or replace only the desired output classes to build a\nmodel? Prior work has proposed decomposing dense-based networks into modules\n(one for each output class) to enable reusability and replaceability in various\nscenarios. However, this work is limited to the dense layers and based on the\none-to-one relationship between the nodes in consecutive layers. Due to the\nshared architecture in the CNN model, prior work cannot be adapted directly. In\nthis paper, we propose to decompose a CNN model used for image classification\nproblems into modules for each output class. These modules can further be\nreused or replaced to build a new model. We have evaluated our approach with\nCIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet\nmodels and found that enabling decomposition comes with a small cost (2.38% and\n0.81% for top-1 and top-5 accuracy, respectively). Also, building a model by\nreusing or replacing modules can be done with a 2.3% and 0.5% average loss of\naccuracy. Furthermore, reusing and replacing these modules reduces CO2e\nemission by ~37 times compared to training the model from scratch.",
    "descriptor": "",
    "authors": [
      "Rangeet Pan",
      "Hridesh Rajan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07720"
  },
  {
    "id": "arXiv:2110.07722",
    "title": "The Sigma-Max System Induced from Randomness and Fuzziness",
    "abstract": "This paper managed to induce probability theory (sigma system) and\npossibility theory (max system) respectively from randomness and fuzziness,\nthrough which the premature theory of possibility is expected to be well\nfounded. Such an objective is achieved by addressing three open key issues: a)\nthe lack of clear mathematical definitions of randomness and fuzziness; b) the\nlack of intuitive mathematical definition of possibility; c) the lack of\nabstraction procedure of the axiomatic definitions of probability/possibility\nfrom their intuitive definitions. Especially, the last issue involves the\nquestion why the key axiom of \"maxitivity\" is adopted for possibility measure.\nBy taking advantage of properties of the well-defined randomness and fuzziness,\nwe derived the important conclusion that \"max\" is the only but un-strict\ndisjunctive operator that is applicable across the fuzzy event space, and is an\nexact operator for fuzzy feature extraction that assures the max inference is\nan exact mechanism. It is fair to claim that the long-standing problem of lack\nof consensus to the foundation of possibility theory is well resolved, which\nwould facilitate wider adoption of possibility theory in practice and promote\ncross prosperity of the two uncertainty theories of probability and\npossibility.",
    "descriptor": "",
    "authors": [
      "Wei Mei",
      "Ming Li",
      "Yuanzeng Cheng",
      "Limin Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.07722"
  },
  {
    "id": "arXiv:2110.07723",
    "title": "EMDS-7: Environmental Microorganism Image Dataset Seventh Version for  Multiple Object Detection Evaluation",
    "abstract": "The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a\nmicroscopic image data set, including the original Environmental Microorganism\nimages (EMs) and the corresponding object labeling files in \".XML\" format file.\nThe EMDS-7 data set consists of 41 types of EMs, which has a total of 2365\nimages and 13216 labeled objects. The EMDS-7 database mainly focuses on the\nobject detection. In order to prove the effectiveness of EMDS-7, we select the\nmost commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and\nRetinaNet) and evaluation indices for testing and evaluation.",
    "descriptor": "",
    "authors": [
      "Hechen Yang",
      "Chen Li",
      "Xin Zhao",
      "Jiawei Zhang",
      "Pingli Ma",
      "Peng Zhao",
      "Ao Chen",
      "Tao Jiang",
      "Hongzan Sunand Marcin Grzegorzek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07723"
  },
  {
    "id": "arXiv:2110.07726",
    "title": "Multifocal Stereoscopic Projection Mapping",
    "abstract": "Stereoscopic projection mapping (PM) allows a user to see a three-dimensional\n(3D) computer-generated (CG) object floating over physical surfaces of\narbitrary shapes around us using projected imagery. However, the current\nstereoscopic PM technology only satisfies binocular cues and is not capable of\nproviding correct focus cues, which causes a vergence--accommodation conflict\n(VAC). Therefore, we propose a multifocal approach to mitigate VAC in\nstereoscopic PM. Our primary technical contribution is to attach electrically\nfocus-tunable lenses (ETLs) to active shutter glasses to control both vergence\nand accommodation. Specifically, we apply fast and periodical focal sweeps to\nthe ETLs, which causes the \"virtual image'\" (as an optical term) of a scene\nobserved through the ETLs to move back and forth during each sweep period. A 3D\nCG object is projected from a synchronized high-speed projector only when the\nvirtual image of the projected imagery is located at a desired distance. This\nprovides an observer with the correct focus cues required. In this study, we\nsolve three technical issues that are unique to stereoscopic PM: (1) The 3D CG\nobject is displayed on non-planar and even moving surfaces; (2) the physical\nsurfaces need to be shown without the focus modulation; (3) the shutter glasses\nadditionally need to be synchronized with the ETLs and the projector. We also\ndevelop a novel compensation technique to deal with the \"lens breathing\"\nartifact that varies the retinal size of the virtual image through focal length\nmodulation. Further, using a proof-of-concept prototype, we demonstrate that\nour technique can present the virtual image of a target 3D CG object at the\ncorrect depth. Finally, we validate the advantage provided by our technique by\ncomparing it with conventional stereoscopic PM using a user study on a\ndepth-matching task.",
    "descriptor": "",
    "authors": [
      "Sorashi Kimura",
      "Daisuke Iwai",
      "Parinya Punpongsanon",
      "Kosuke Sato"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07726"
  },
  {
    "id": "arXiv:2110.07727",
    "title": "Active Learning of Neural Collision Handler for Complex 3D Mesh  Deformations",
    "abstract": "We present a robust learning algorithm to detect and handle collisions in 3D\ndeforming meshes. Our collision detector is represented as a bilevel deep\nautoencoder with an attention mechanism that identifies colliding mesh\nsub-parts. We use a numerical optimization algorithm to resolve penetrations\nguided by the network. Our learned collision handler can resolve collisions for\nunseen, high-dimensional meshes with thousands of vertices. To obtain stable\nnetwork performance in such large and unseen spaces, we progressively insert\nnew collision data based on the errors in network inferences. We automatically\nlabel these data using an analytical collision detector and progressively\nfine-tune our detection networks. We evaluate our method for collision handling\nof complex, 3D meshes coming from several datasets with different shapes and\ntopologies, including datasets corresponding to dressed and undressed human\nposes, cloth simulations, and human hand poses acquired using multiview capture\nsystems. Our approach outperforms supervised learning methods and achieves\n$93.8-98.1\\%$ accuracy compared to the groundtruth by analytic methods.\nCompared to prior learning methods, our approach results in a $5.16\\%-25.50\\%$\nlower false negative rate in terms of collision checking and a $9.65\\%-58.91\\%$\nhigher success rate in collision handling.",
    "descriptor": "",
    "authors": [
      "Qingyang Tan",
      "Zherong Pan",
      "Breannan Smith",
      "Takaaki Shiratori",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.07727"
  },
  {
    "id": "arXiv:2110.07729",
    "title": "An Independent Study of Reinforcement Learning and Autonomous Driving",
    "abstract": "Reinforcement learning has become one of the most trending subjects in the\nrecent decade. It has seen applications in various fields such as robot\nmanipulations, autonomous driving, path planning, computer gaming, etc. We\naccomplished three tasks during the course of this project. Firstly, we studied\nthe Q-learning algorithm for tabular environments and applied it successfully\nto an OpenAi Gym environment, Taxi. Secondly, we gained an understanding of and\nimplemented the deep Q-network algorithm for Cart-Pole environment. Thirdly, we\nalso studied the application of reinforcement learning in autonomous driving\nand its combination with safety check constraints (safety controllers). We\ntrained a rough autonomous driving agent using highway-gym environment and\nexplored the effects of various environment configurations like reward\nfunctions on the agent training performance.",
    "descriptor": "\nComments: 32 pages in total, 7 figures, 3 appendices, 5 tables\n",
    "authors": [
      "Hanzhi Yang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07729"
  },
  {
    "id": "arXiv:2110.07731",
    "title": "CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training",
    "abstract": "With the rise of large-scale pre-trained language models, open-domain\nquestion-answering (ODQA) has become an important research topic in NLP. Based\non the popular pre-training fine-tuning approach, we posit that an additional\nin-domain pre-training stage using a large-scale, natural, and diverse\nquestion-answering (QA) dataset can be beneficial for ODQA. Consequently, we\npropose a novel QA dataset based on the Common Crawl project in this paper.\nUsing the readily available schema.org annotation, we extract around 130\nmillion multilingual question-answer pairs, including about 60 million English\ndata-points. With this previously unseen number of natural QA pairs, we\npre-train popular language models to show the potential of large-scale\nin-domain pre-training for the task of question-answering. In our experiments,\nwe find that pre-training question-answering models on our Common Crawl\nQuestion Answering dataset (CCQA) achieves promising results in zero-shot, low\nresource and fine-tuned settings across multiple tasks, models and benchmarks.",
    "descriptor": "",
    "authors": [
      "Patrick Huber",
      "Armen Aghajanyan",
      "Barlas O\u011fuz",
      "Dmytro Okhonko",
      "Wen-tau Yih",
      "Sonal Gupta",
      "Xilun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07731"
  },
  {
    "id": "arXiv:2110.07732",
    "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves  Systematic Generalization",
    "abstract": "Despite successes across a broad range of applications, Transformers have\nlimited success in systematic generalization. The situation is especially\nfrustrating in the case of algorithmic tasks, where they often fail to find\nintuitive solutions that route relevant information to the right node/operation\nat the right time in the grid represented by Transformer columns. To facilitate\nthe learning of useful control flow, we propose two modifications to the\nTransformer architecture, copy gate and geometric attention. Our novel Neural\nData Router (NDR) achieves 100% length generalization accuracy on the classic\ncompositional table lookup task, as well as near-perfect accuracy on the simple\narithmetic task and a new variant of ListOps testing for generalization across\ncomputational depth. NDR's attention and gating patterns tend to be\ninterpretable as an intuitive form of neural routing. Our code is public.",
    "descriptor": "",
    "authors": [
      "R\u00f3bert Csord\u00e1s",
      "Kazuki Irie",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.07732"
  },
  {
    "id": "arXiv:2110.07733",
    "title": "Identifying Similar Test Cases That Are Specified in Natural Language",
    "abstract": "Software testing is still a manual process in many industries, despite the\nrecent improvements in automated testing techniques. As a result, test cases\nare often specified in natural language by different employees and many\nredundant test cases might exist in the test suite. This increases the (already\nhigh) cost of test execution. Manually identifying similar test cases is a\ntime-consuming and error-prone task. Therefore, in this paper, we propose an\nunsupervised approach to identify similar test cases. Our approach uses a\ncombination of text embedding, text similarity and clustering techniques to\nidentify similar test cases. We evaluate five different text embedding\ntechniques, two text similarity metrics, and two clustering techniques to\ncluster similar test steps and four techniques to identify similar test cases\nfrom the test step clusters. Through an evaluation in an industrial setting, we\nshowed that our approach achieves a high performance to cluster test steps (an\nF-score of 87.39%) and identify similar test cases (an F-score of 83.47%).\nFurthermore, a validation with developers indicates several different practical\nusages of our approach (such as identifying redundant and legacy test cases),\nwhich help to reduce the testing manual effort and time.",
    "descriptor": "",
    "authors": [
      "Markos Viggiato",
      "Dale Paas",
      "Chris Buzon",
      "Cor-Paul Bezemer"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.07733"
  },
  {
    "id": "arXiv:2110.07734",
    "title": "Meta-Reinforcement Learning Based Resource Allocation for Dynamic V2X  Communications",
    "abstract": "This paper studies the allocation of shared resources between\nvehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) links in\nvehicle-to-everything (V2X) communications. In existing algorithms, dynamic\nvehicular environments and quantization of continuous power become the\nbottlenecks for providing an effective and timely resource allocation policy.\nIn this paper, we develop two algorithms to deal with these difficulties.\nFirst, we propose a deep reinforcement learning (DRL)-based resource allocation\nalgorithm to improve the performance of both V2I and V2V links. Specifically,\nthe algorithm uses deep Q-network (DQN) to solve the sub-band assignment and\ndeep deterministic policy-gradient (DDPG) to solve the continuous power\nallocation problem. Second, we propose a meta-based DRL algorithm to enhance\nthe fast adaptability of the resource allocation policy in the dynamic\nenvironment. Numerical results demonstrate that the proposed DRL-based\nalgorithm can significantly improve the performance compared to the DQN-based\nalgorithm that quantizes continuous power. In addition, the proposed meta-based\nDRL algorithm can achieve the required fast adaptation in the new environment\nwith limited experiences.",
    "descriptor": "\nComments: Published in IEEE Transactions on Vehicular Technology, Sept. 2021\n",
    "authors": [
      "Yi Yuan",
      "Gan Zheng",
      "Kai-Kit Wong",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07734"
  },
  {
    "id": "arXiv:2110.07735",
    "title": "Continual Learning on Noisy Data Streams via Self-Purified Replay",
    "abstract": "Continually learning in the real world must overcome many challenges, among\nwhich noisy labels are a common and inevitable issue. In this work, we present\na repla-ybased continual learning framework that simultaneously addresses both\ncatastrophic forgetting and noisy labels for the first time. Our solution is\nbased on two observations; (i) forgetting can be mitigated even with noisy\nlabels via self-supervised learning, and (ii) the purity of the replay buffer\nis crucial. Building on this regard, we propose two key components of our\nmethod: (i) a self-supervised replay technique named Self-Replay which can\ncircumvent erroneous training signals arising from noisy labeled data, and (ii)\nthe Self-Centered filter that maintains a purified replay buffer via\ncentrality-based stochastic graph ensembles. The empirical results on MNIST,\nCIFAR-10, CIFAR-100, and WebVision with real-world noise demonstrate that our\nframework can maintain a highly pure replay buffer amidst noisy streamed data\nwhile greatly outperforming the combinations of the state-of-the-art continual\nlearning and noisy label learning methods. The source code is available at\nthis http URL",
    "descriptor": "\nComments: Published at ICCV 2021 main conference\n",
    "authors": [
      "Chris Dongjoo Kim",
      "Jinseo Jeong",
      "Sangwoo Moon",
      "Gunhee Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07735"
  },
  {
    "id": "arXiv:2110.07736",
    "title": "Identifying and Mitigating Spurious Correlations for Improving  Robustness in NLP Models",
    "abstract": "Recently, NLP models have achieved remarkable progress across a variety of\ntasks; however, they have also been criticized for being not robust. Many\nrobustness problems can be attributed to models exploiting spurious\ncorrelations, or shortcuts between the training data and the task labels.\nModels may fail to generalize to out-of-distribution data or be vulnerable to\nadversarial attacks if spurious correlations are exploited through the training\nprocess. In this paper, we aim to automatically identify such spurious\ncorrelations in NLP models at scale. We first leverage existing\ninterpretability methods to extract tokens that significantly affect model's\ndecision process from the input text. We then distinguish \"genuine\" tokens and\n\"spurious\" tokens by analyzing model predictions across multiple corpora and\nfurther verify them through knowledge-aware perturbations. We show that our\nproposed method can effectively and efficiently identify a scalable set of\n\"shortcuts\", and mitigating these leads to more robust models in multiple\napplications.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Tianlu Wang",
      "Diyi Yang",
      "Xuezhi Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07736"
  },
  {
    "id": "arXiv:2110.07742",
    "title": "Beyond Classification: Directly Training Spiking Neural Networks for  Semantic Segmentation",
    "abstract": "Spiking Neural Networks (SNNs) have recently emerged as the low-power\nalternative to Artificial Neural Networks (ANNs) because of their sparse,\nasynchronous, and binary event-driven processing. Due to their energy\nefficiency, SNNs have a high possibility of being deployed for real-world,\nresource-constrained systems such as autonomous vehicles and drones. However,\nowing to their non-differentiable and complex neuronal dynamics, most previous\nSNN optimization methods have been limited to image recognition. In this paper,\nwe explore the SNN applications beyond classification and present semantic\nsegmentation networks configured with spiking neurons. Specifically, we first\ninvestigate two representative SNN optimization techniques for recognition\ntasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic\nsegmentation datasets. We observe that, when converted from ANNs, SNNs suffer\nfrom high latency and low performance due to the spatial variance of features.\nTherefore, we directly train networks with surrogate gradient learning,\nresulting in lower latency and higher performance than ANN-SNN conversion.\nMoreover, we redesign two fundamental ANN segmentation architectures (i.e.,\nFully Convolutional Networks and DeepLab) for the SNN domain. We conduct\nexperiments on two public semantic segmentation benchmarks including the PASCAL\nVOC2012 dataset and the DDD17 event-based dataset. In addition to showing the\nfeasibility of SNNs for semantic segmentation, we show that SNNs can be more\nrobust and energy-efficient compared to their ANN counterparts in this domain.",
    "descriptor": "",
    "authors": [
      "Youngeun Kim",
      "Joshua Chough",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07742"
  },
  {
    "id": "arXiv:2110.07749",
    "title": "Attention-Free Keyword Spotting",
    "abstract": "Till now, attention-based models have been used with great success in the\nkeyword spotting problem domain. However, in light of recent advances in deep\nlearning, the question arises whether self-attention is truly irreplaceable for\nrecognizing speech keywords. We thus explore the usage of gated MLPs --\npreviously shown to be alternatives to transformers in vision tasks -- for the\nkeyword spotting task. We verify our approach on the Google Speech Commands\nV2-35 dataset and show that it is possible to obtain performance comparable to\nthe state of the art without any apparent usage of self-attention.",
    "descriptor": "\nComments: Submitted to ICASSP-2022 (5 pages)\n",
    "authors": [
      "Mashrur M. Morshed",
      "Ahmad Omar Ahsan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07749"
  },
  {
    "id": "arXiv:2110.07750",
    "title": "Is that a Duiker or Dik Dik Next to the Giraffe? Impacts of Uncertainty  on Classification Efficiency in Citizen Science",
    "abstract": "Quality control is an ongoing concern in citizen science that is often\nmanaged by replication to consensus in online tasks such as image\nclassification. Numerous factors can lead to disagreement, including image\nquality problems, interface specifics, and the complexity of the content\nitself. We conducted trace ethnography with statistical and qualitative\nanalyses of six Snapshot Safari projects to understand the content\ncharacteristics that can lead to uncertainty and low consensus. This study\ncontributes content categorization based on aggregate classifications to\ncharacterize image complexity, with analysis that confirms that the categories\nimpact classification efficiency, and an inductively generated set of\nadditional image quality issues that also impact volunteers' ability to\nconfidently classify content. The results suggest that different\nconceptualizations and measures of consensus may be needed for different types\nof content, and aggregate responses offer a way to identify content that needs\ndifferent handling when complexity cannot be determined $a$ $priori$.",
    "descriptor": "",
    "authors": [
      "Vinod Kumar Ahuja",
      "Holly K. Rosser",
      "Andrea Grover"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07750"
  },
  {
    "id": "arXiv:2110.07751",
    "title": "Leveraging Spatial and Temporal Correlations in Sparsified Mean  Estimation",
    "abstract": "We study the problem of estimating at a central server the mean of a set of\nvectors distributed across several nodes (one vector per node). When the\nvectors are high-dimensional, the communication cost of sending entire vectors\nmay be prohibitive, and it may be imperative for them to use sparsification\ntechniques. While most existing work on sparsified mean estimation is agnostic\nto the characteristics of the data vectors, in many practical applications such\nas federated learning, there may be spatial correlations (similarities in the\nvectors sent by different nodes) or temporal correlations (similarities in the\ndata sent by a single node over different iterations of the algorithm) in the\ndata vectors. We leverage these correlations by simply modifying the decoding\nmethod used by the server to estimate the mean. We provide an analysis of the\nresulting estimation error as well as experiments for PCA, K-Means and Logistic\nRegression, which show that our estimators consistently outperform more\nsophisticated and expensive sparsification methods.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021\n",
    "authors": [
      "Divyansh Jhunjhunwala",
      "Ankur Mallick",
      "Advait Gadhikar",
      "Swanand Kadhe",
      "Gauri Joshi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07751"
  },
  {
    "id": "arXiv:2110.07752",
    "title": "Hindsight: Posterior-guided training of retrievers for improved  open-ended generation",
    "abstract": "Many text generation systems benefit from using a retriever to retrieve\npassages from a textual knowledge corpus (e.g., Wikipedia) which are then\nprovided as additional context to the generator. For open-ended generation\ntasks (like generating informative utterances in conversations) many varied\npassages may be equally relevant and we find that existing methods that jointly\ntrain the retriever and generator underperform: the retriever may not find\nrelevant passages even amongst the top-10 and hence the generator may not learn\na preference to ground its generated output in them. We propose using an\nadditional guide retriever that is allowed to use the target output and \"in\nhindsight\" retrieve relevant passages during training. We model the guide\nretriever after the posterior distribution Q of passages given the input and\nthe target output and train it jointly with the standard retriever and the\ngenerator by maximizing the evidence lower bound (ELBo) in expectation over Q.\nFor informative conversations from the Wizard of Wikipedia dataset, with\nposterior-guided training, the retriever finds passages with higher relevance\nin the top-10 (23% relative improvement), the generator's responses are more\ngrounded in the retrieved passage (19% relative improvement) and the end-to-end\nsystem produces better overall output (6.4% relative improvement).",
    "descriptor": "",
    "authors": [
      "Ashwin Paranjape",
      "Omar Khattab",
      "Christopher Potts",
      "Matei Zaharia",
      "Christopher D. Manning"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.07752"
  },
  {
    "id": "arXiv:2110.07753",
    "title": "On Efficient Range-Summability of IID Random Variables in Two or Higher  Dimensions",
    "abstract": "$d$-dimensional efficient range-summability ($d$D-ERS) of a long list of\nrandom variables (RVs) is a fundamental algorithmic problem that has\napplications to two important families of database problems, namely, fast\napproximate wavelet tracking (FAWT) on data streams and approximately answering\nrange-sum queries over a data cube. In this work, we propose a novel solution\nframework to $d$D-ERS for $d>1$ on RVs that have Gaussian or Poisson\ndistribution. Our solutions are the first ones that compute any rectangular\nrange-sum of the RVs in polylogarithmic time. Furthermore, we develop a novel\n$k$-wise independence theory that allows our $d$D-ERS solutions to have both\nhigh computational efficiencies and strong provable independence guarantees.\nFinally, we generalize existing DST-based solutions for 1D-ERS to 2D, and\ncharacterize a sufficient and likely necessary condition on the target\ndistribution for this generalization to be feasible.",
    "descriptor": "",
    "authors": [
      "Jingfan Meng",
      "Huayi Wang",
      "Jun Xu",
      "Mitsunori Ogihara"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07753"
  },
  {
    "id": "arXiv:2110.07755",
    "title": "Multi-Stage Sparse Resource Allocation for Control of Spreading  Processes over Networks",
    "abstract": "In this paper we propose a method for sparse dynamic allocation of resources\nto bound the risk of spreading processes, such as epidemics and wildfires,\nusing convex optimization and dynamic programming techniques. Here, risk is\ndefined as the risk of an outbreak, i.e. the product of the probability of an\noutbreak occurring over a time interval and the future impact of that outbreak,\nand we can allocate budgeted resources each time step to bound or minimize the\nrisk. Our method in particular provides sparsity of resources, which is\nimportant due to the large network structures involved with spreading processes\nand has advantages when resources can not be distributed widely.",
    "descriptor": "\nComments: Conference submission. arXiv admin note: text overlap with arXiv:2107.05878, arXiv:2003.07555\n",
    "authors": [
      "Vera L. J. Somers",
      "Ian R. Manchester"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07755"
  },
  {
    "id": "arXiv:2110.07758",
    "title": "\"Knights\": First Place Submission for VIPriors21 Action Recognition  Challenge at ICCV 2021",
    "abstract": "This technical report presents our approach \"Knights\" to solve the action\nrecognition task on a small subset of Kinetics-400 i.e. Kinetics400ViPriors\nwithout using any extra-data. Our approach has 3 main components:\nstate-of-the-art Temporal Contrastive self-supervised pretraining, video\ntransformer models, and optical flow modality. Along with the use of standard\ntest-time augmentation, our proposed solution achieves 73% on\nKinetics400ViPriors test set, which is the best among all of the other entries\nVisual Inductive Priors for Data-Efficient Computer Vision's Action Recognition\nChallenge, ICCV 2021.",
    "descriptor": "\nComments: Challenge results are available at this https URL\n",
    "authors": [
      "Ishan Dave",
      "Naman Biyani",
      "Brandon Clark",
      "Rohit Gupta",
      "Yogesh Rawat",
      "Mubarak Shah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07758"
  },
  {
    "id": "arXiv:2110.07766",
    "title": "3D Reconstruction of Curvilinear Structures with Stereo Matching  DeepConvolutional Neural Networks",
    "abstract": "Curvilinear structures frequently appear in microscopy imaging as the object\nof interest. Crystallographic defects, i.edislocations, are one of the\ncurvilinear structures that have been repeatedly investigated under\ntransmission electronmicroscopy (TEM) and their 3D structural information is of\ngreat importance for understanding the properties ofmaterials. 3D information\nof dislocations is often obtained by tomography which is a cumbersome process\nsince itis required to acquire many images with different tilt angles and\nsimilar imaging conditions. Although, alternativestereoscopy methods lower the\nnumber of required images to two, they still require human intervention and\nshape priorsfor accurate 3D estimation. We propose a fully automated pipeline\nfor both detection and matching of curvilinearstructures in stereo pairs by\nutilizing deep convolutional neural networks (CNNs) without making any prior\nassumptionon 3D shapes. In this work, we mainly focus on 3D reconstruction of\ndislocations from stereo pairs of TEM images.",
    "descriptor": "",
    "authors": [
      "Okan Alting\u00f6vde",
      "Anastasiia Mishchuk",
      "Gulnaz Ganeeva",
      "Emad Oveisi",
      "Cecile Hebert",
      "Pascal Fua"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07766"
  },
  {
    "id": "arXiv:2110.07769",
    "title": "Using the Semantic Information G Measure to Explain and Extend  Rate-Distortion Functions and Maximum Entropy Distributions",
    "abstract": "In the rate-distortion function and the Maximum Entropy (ME) method, Minimum\nMutual In-formation (MMI) distributions and ME distributions are expressed by\nBayes-like formulas, in-cluding Negative Exponential Functions (NEFs) and\npartition functions. Why do these non-probability functions exist in Bayes-like\nformulas? On the other hand, the rate-distortion function has three\ndisadvantages: (1) the distortion function is subjectively defined; (2) the\ndefi-nition of the distortion function between instances and labels is often\ndifficult; (3) it cannot be used for data compression according to the labels'\nsemantic meanings. The author has proposed using the semantic information G\nmeasure with both statistical probability and logical probability before. We\ncan now explain NEFs as truth functions, partition functions as logical\nprobabilities, Bayes-like formulas as semantic Bayes' formulas, MMI as Semantic\nMutual Information (SMI), and ME as extreme ME minus SMI. In overcoming the\nabove disadvantages, this paper sets up the relationship between truth\nfunctions and distortion functions, obtains truth functions from samples by\nmachine learning, and constructs constraint conditions with truth functions to\nextend rate-distortion functions. Two examples are used to help readers\nunderstand the MMI iteration and to support the theoretical results. Using\ntruth functions and the semantic information G measure, we can combine machine\nlearning and data compression, including semantic com-pression. We need further\nstudies to explore general data compression and recovery, according to the\nsemantic meaning.",
    "descriptor": "\nComments: 22 pages, 5 figures\n",
    "authors": [
      "Chenguang Lu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07769"
  },
  {
    "id": "arXiv:2110.07770",
    "title": "Human factors engineering research on single pilot operations for large  commercial aircraft: Status and prospect",
    "abstract": "The civil aviation community is actively exploring and developing the\nsolutions of single pilot operations SPO for large commercial aircraft. Human\nfactors engineering research for SPO has been launched, and the research mainly\nfocuses on three research solutions: flight deck airborne equipment upgrade,\nflight support from ground stations, and the combined SPO solution of \"flight\ndeck airborne equipment upgrade, flight support from ground stations\". This\npaper reviews and analyzez the progress of human factors engineering research\non SPO. The preliminary research outcome tends to support the combined SPO\nsolution. However, the current human factors engineering research is not\ncomprehensive and cannot provide a complete human factors engineering solution\nfor SPO. For future human factors engineering research, this paper analyzes the\nkey human factors issues on SPO and points out the gaps in the current research\nand the areas for future work. Finally, this paper puts forward an overall\nstrategy and recommendations for future human factors engineering research on\nSPO.",
    "descriptor": "\nComments: in Chinese language\n",
    "authors": [
      "Wei Xu",
      "Yong Chen",
      "Wenjun Dong",
      "Dayong Dong",
      "Liezhong Ge"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07770"
  },
  {
    "id": "arXiv:2110.07771",
    "title": "Assessing Risks and Modeling Threats in the Internet of Things",
    "abstract": "Threat modeling and risk assessments are common ways to identify, estimate,\nand prioritize risk to national, organizational, and individual operations and\nassets. Several threat modeling and risk assessment approaches have been\nproposed prior to the advent of the Internet of Things (IoT) that focus on\nthreats and risks in information technology (IT). Due to shortcomings in these\napproaches and the fact that there are significant differences between the IoT\nand IT, we synthesize and adapt these approaches to provide a threat modeling\nframework that focuses on threats and risks in the IoT. In doing so, we develop\nan IoT attack taxonomy that describes the adversarial assets, adversarial\nactions, exploitable vulnerabilities, and compromised properties that are\ncomponents of any IoT attack. We use this IoT attack taxonomy as the foundation\nfor designing a joint risk assessment and maturity assessment framework that is\nimplemented as an interactive online tool. The assessment framework this tool\nencodes provides organizations with specific recommendations about where\nresources should be devoted to mitigate risk. The usefulness of this IoT\nframework is highlighted by case study implementations in the context of\nmultiple industrial manufacturing companies, and the interactive implementation\nof this framework is available at this http URL",
    "descriptor": "",
    "authors": [
      "Paul Griffioen",
      "Bruno Sinopoli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07771"
  },
  {
    "id": "arXiv:2110.07774",
    "title": "4D flight trajectory prediction using a hybrid Deep Learning prediction  method based on ADS-B technology: a case study of Hartsfield-Jackson Atlanta  International Airport(ATL)",
    "abstract": "The core of any flight schedule is the trajectories. In particular, 4D\ntrajectories are the most crucial component for flight attribute prediction. In\nparticular, 4D trajectories are the most crucial component for flight attribute\nprediction. Each trajectory contains spatial and temporal features that are\nassociated with uncertainties that make the prediction process complex. Today\nbecause of the increasing demand for air transportation, it is compulsory for\nairports and airlines to have an optimized schedule to use all of the airport's\ninfrastructure potential. This is possible using advanced trajectory prediction\nmethods. This paper proposes a novel hybrid deep learning model to extract the\nspatial and temporal features considering the uncertainty of the prediction\nmodel for Hartsfield-Jackson Atlanta International Airport(ATL). Automatic\nDependent Surveillance-Broadcast (ADS-B) data are used as input to the models.\nThis research is conducted in three steps: (a) data preprocessing; (b)\nprediction by a hybrid Convolutional Neural Network and Gated Recurrent Unit\n(CNN-GRU) along with a 3D-CNN model; (c) The third and last step is the\ncomparison of the model's performance with the proposed model by comparing the\nexperimental results. The deep model uncertainty is considered using the\nMont-Carlo dropout (MC-Dropout). Mont-Carlo dropouts are added to the network\nlayers to enhance the model's prediction performance by a robust approach of\nswitching off between different neurons. The results show that the proposed\nmodel has low error measurements compared to the other models (i.e., 3D CNN,\nCNN-GRU). The model with MC-dropout reduces the error further by an average of\n21 %.",
    "descriptor": "\nComments: 17 pages, 10 figures\n",
    "authors": [
      "Hesam Sahfienya",
      "Amelia C. Regan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07774"
  },
  {
    "id": "arXiv:2110.07775",
    "title": "Creating User Interface Mock-ups from High-Level Text Descriptions with  Deep-Learning Models",
    "abstract": "The design process of user interfaces (UIs) often begins with articulating\nhigh-level design goals. Translating these high-level design goals into\nconcrete design mock-ups, however, requires extensive effort and UI design\nexpertise. To facilitate this process for app designers and developers, we\nintroduce three deep-learning techniques to create low-fidelity UI mock-ups\nfrom a natural language phrase that describes the high-level design goal (e.g.\n\"pop up displaying an image and other options\"). In particular, we contribute\ntwo retrieval-based methods and one generative method, as well as\npre-processing and post-processing techniques to ensure the quality of the\ncreated UI mock-ups. We quantitatively and qualitatively compare and contrast\neach method's ability in suggesting coherent, diverse and relevant UI design\nmock-ups. We further evaluate these methods with 15 professional UI designers\nand practitioners to understand each method's advantages and disadvantages. The\ndesigners responded positively to the potential of these methods for assisting\nthe design process.",
    "descriptor": "",
    "authors": [
      "Forrest Huang",
      "Gang Li",
      "Xin Zhou",
      "John F. Canny",
      "Yang Li"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07775"
  },
  {
    "id": "arXiv:2110.07778",
    "title": "NeuroView: Explainable Deep Network Decision Making",
    "abstract": "Deep neural networks (DNs) provide superhuman performance in numerous\ncomputer vision tasks, yet it remains unclear exactly which of a DN's units\ncontribute to a particular decision. NeuroView is a new family of DN\narchitectures that are interpretable/explainable by design. Each member of the\nfamily is derived from a standard DN architecture by vector quantizing the unit\noutput values and feeding them into a global linear classifier. The resulting\narchitecture establishes a direct, causal link between the state of each unit\nand the classification decision. We validate NeuroView on standard datasets and\nclassification tasks to show that how its unit/class mapping aids in\nunderstanding the decision-making process.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "CJ Barberan",
      "Randall Balestriero",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07778"
  },
  {
    "id": "arXiv:2110.07780",
    "title": "An Artificial Bee Colony Based Algorithm for Continuous Distributed  Constraint Optimization Problems",
    "abstract": "Distributed Constraint Optimization Problems (DCOPs) are a frequently used\nframework in which a set of independent agents choose values from their\nrespective discrete domains to maximize their utility. Although this\nformulation is typically appropriate, there are a number of real-world\napplications in which the decision variables are continuous-valued and the\nconstraints are represented in functional form. To address this, Continuous\nDistributed Constraint Optimization Problems (C-DCOPs), an extension of the\nDCOPs paradigm, have recently grown the interest of the multi-agent systems\nfield. To date, among different approaches, population-based algorithms are\nshown to be most effective for solving C-DCOPs. Considering the potential of\npopulation-based approaches, we propose a new C-DCOPs solver inspired by a\nwell-known population-based algorithm Artificial Bee Colony (ABC).\nAdditionally, we provide a new exploration method that aids in the further\nimprovement of the algorithm's solution quality. Finally, We theoretically\nprove that our approach is an anytime algorithm and empirically show it\nproduces significantly better results than the state-of-the-art C-DCOPs\nalgorithms.",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "K. M. Merajul Arefin",
      "Mashrur Rashik",
      "Saaduddin Mahmud",
      "Md. Mosaddek Khan"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.07780"
  },
  {
    "id": "arXiv:2110.07782",
    "title": "Active Learning for Improved Semi-Supervised Semantic Segmentation in  Satellite Images",
    "abstract": "Remote sensing data is crucial for applications ranging from monitoring\nforest fires and deforestation to tracking urbanization. Most of these tasks\nrequire dense pixel-level annotations for the model to parse visual information\nfrom limited labeled data available for these satellite images. Due to the\ndearth of high-quality labeled training data in this domain, there is a need to\nfocus on semi-supervised techniques. These techniques generate pseudo-labels\nfrom a small set of labeled examples which are used to augment the labeled\ntraining set. This makes it necessary to have a highly representative and\ndiverse labeled training set. Therefore, we propose to use an active\nlearning-based sampling strategy to select a highly representative set of\nlabeled training data. We demonstrate our proposed method's effectiveness on\ntwo existing semantic segmentation datasets containing satellite images: UC\nMerced Land Use Classification Dataset and DeepGlobe Land Cover Classification\nDataset. We report a 27% improvement in mIoU with as little as 2% labeled data\nusing active learning sampling strategies over randomly sampling the small set\nof labeled training data.",
    "descriptor": "\nComments: Accepted to Winter Conference on Applications of Computer Vision 2022 (WACV 2022)\n",
    "authors": [
      "Shasvat Desai",
      "Debasmita Ghose"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07782"
  },
  {
    "id": "arXiv:2110.07785",
    "title": "Scalable Causal Structure Learning: New Opportunities in Biomedicine",
    "abstract": "This paper gives a practical tutorial on popular causal structure learning\nmodels with examples of real-world data to help healthcare audiences understand\nand apply them. We review prominent traditional, score-based and\nmachine-learning based schemes for causal structure discovery, study some of\ntheir performance over some benchmark datasets, and discuss some of the\napplications to biomedicine. In the case of sufficient data, machine\nlearning-based approaches can be scalable, can include a greater number of\nvariables than traditional approaches, and can potentially be applied in many\nbiomedical applications.",
    "descriptor": "",
    "authors": [
      "Pulakesh Upadhyaya",
      "Kai Zhang",
      "Can Li",
      "Xiaoqian Jiang",
      "Yejin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.07785"
  },
  {
    "id": "arXiv:2110.07786",
    "title": "Learning the Koopman Eigendecomposition: A Diffeomorphic Approach",
    "abstract": "We present a novel data-driven approach for learning linear representations\nof a class of stable nonlinear systems using Koopman eigenfunctions. By\nlearning the conjugacy map between a nonlinear system and its Jacobian\nlinearization through a Normalizing Flow one can guarantee the learned function\nis a diffeomorphism. Using this diffeomorphism, we construct eigenfunctions of\nthe nonlinear system via the spectral equivalence of conjugate systems -\nallowing the construction of linear predictors for nonlinear systems. The\nuniversality of the diffeomorphism learner leads to the universal approximation\nof the nonlinear system's Koopman eigenfunctions. The developed method is also\nsafe as it guarantees the model is asymptotically stable regardless of the\nrepresentation accuracy. To our best knowledge, this is the first work to close\nthe gap between the operator, system and learning theories. The efficacy of our\napproach is shown through simulation examples.",
    "descriptor": "\nComments: Submitted to the 2022 American Control Conference\n",
    "authors": [
      "Petar Bevanda",
      "Johannes Kirmayr",
      "Stefan Sosnowski",
      "Sandra Hirche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07786"
  },
  {
    "id": "arXiv:2110.07789",
    "title": "Toward Learning Context-Dependent Tasks from Demonstration for  Tendon-Driven Surgical Robots",
    "abstract": "Tendon-driven robots, a type of continuum robot, have the potential to reduce\nthe invasiveness of surgery by enabling access to difficult-to-reach anatomical\ntargets. In the future, the automation of surgical tasks for these robots may\nhelp reduce surgeon strain in the face of a rapidly growing population.\nHowever, directly encoding surgical tasks and their associated context for\nthese robots is infeasible. In this work we take steps toward a system that is\nable to learn to successfully perform context-dependent surgical tasks by\nlearning directly from a set of expert demonstrations. We present three models\ntrained on the demonstrations conditioned on a vector encoding the context of\nthe demonstration. We then use these models to plan and execute motions for the\ntendon-driven robot similar to the demonstrations for novel context not seen in\nthe training set. We demonstrate the efficacy of our method on three\nsurgery-inspired tasks.",
    "descriptor": "\nComments: 7 pages, 6 figures, to be published in the proceedings of the 2021 International Symposium on Medical Robotics (ISMR)\n",
    "authors": [
      "Yixuan Huang",
      "Michael Bentley",
      "Tucker Hermans",
      "Alan Kuntz"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07789"
  },
  {
    "id": "arXiv:2110.07790",
    "title": "DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving Perception",
    "abstract": "Multi-object tracking and segmentation (MOTS) is a critical task for\nautonomous driving applications. The existing MOTS studies face two critical\nchallenges: 1) the published datasets inadequately capture the real-world\ncomplexity for network training to address various driving settings; 2) the\nworking pipeline annotation tool is under-studied in the literature to improve\nthe quality of MOTS learning examples. In this work, we introduce the\nDG-Labeler and DGL-MOTS dataset to facilitate the training data annotation for\nthe MOTS task and accordingly improve network training accuracy and efficiency.\nDG-Labeler uses the novel Depth-Granularity Module to depict the instance\nspatial relations and produce fine-grained instance masks. Annotated by\nDG-Labeler, our DGL-MOTS dataset exceeds the prior effort (i.e., KITTI MOTS and\nBDD100K) in data diversity, annotation quality, and temporal representations.\nResults on extensive cross-dataset evaluations indicate significant performance\nimprovements for several state-of-the-art methods trained on our DGL-MOTS\ndataset. We believe our DGL-MOTS Dataset and DG-Labeler hold the valuable\npotential to boost the visual perception of future transportation.",
    "descriptor": "",
    "authors": [
      "Yiming Cui",
      "Zhiwen Cao",
      "Yixin Xie",
      "Xingyu Jiang",
      "Feng Tao",
      "Yingjie Chen",
      "Lin Li",
      "Dongfang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07790"
  },
  {
    "id": "arXiv:2110.07792",
    "title": "A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text  Classification",
    "abstract": "We present a multilingual bag-of-entities model that effectively boosts the\nperformance of zero-shot cross-lingual text classification by extending a\nmultilingual pre-trained language model (e.g., M-BERT). It leverages the\nmultilingual nature of Wikidata: entities in multiple languages representing\nthe same concept are defined with a unique identifier. This enables entities\ndescribed in multiple languages to be represented using shared embeddings. A\nmodel trained on entity features in a resource-rich language can thus be\ndirectly applied to other languages. Our experimental results on cross-lingual\ntopic classification (using the MLDoc and TED-CLDC datasets) and entity typing\n(using the SHINRA2020-ML dataset) show that the proposed model consistently\noutperforms state-of-the-art models.",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Sosuke Nishikawa",
      "Ikuya Yamada",
      "Yoshimasa Tsuruoka",
      "Isao Echizen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07792"
  },
  {
    "id": "arXiv:2110.07795",
    "title": "Sharp $L^\\infty$ estimates of HDG methods for Poisson equation II: 3D",
    "abstract": "In [SIAM J. Numer. Anal., 59 (2), 720-745], we proved quasi-optimal\n$L^\\infty$ estimates (up to logarithmic factors) for the solution of Poisson's\nequation by a hybridizable discontinuous Galerkin (HDG) method. However, the\nestimates only work in 2D. In this paper, we obtain sharp (without logarithmic\nfactors) $L^\\infty$ estimates for the HDG method in both 2D and 3D. Numerical\nexperiments are presented to confirm our theoretical result.",
    "descriptor": "",
    "authors": [
      "Gang Chen",
      "Peter Monk",
      "Yangwen Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07795"
  },
  {
    "id": "arXiv:2110.07796",
    "title": "Occupancy Estimation from Thermal Images",
    "abstract": "We propose a non-intrusive, and privacy-preserving occupancy estimation\nsystem for smart environments. The proposed scheme uses thermal images to\ndetect the number of people in a given area. The occupancy estimation model is\ndesigned using the concepts of intensity-based and motion-based human\nsegmentation. The notion of difference catcher, connected component labeling,\nnoise filter, and memory propagation are utilized to estimate the occupancy\nnumber. We use a real dataset to demonstrate the effectiveness of the proposed\nsystem.",
    "descriptor": "\nComments: 4 pages, 2 figures. This is an accepted demo paper and to be published in the proceedings of 19th International Conference on Service Oriented Computing (ICSOC 2021)\n",
    "authors": [
      "Zishan Qin",
      "Dipankar Chaki",
      "Abdallah Lakhdari",
      "Amani Abusafia",
      "Athman Bouguettaya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07796"
  },
  {
    "id": "arXiv:2110.07797",
    "title": "EFENet: Reference-based Video Super-Resolution with Enhanced Flow  Estimation",
    "abstract": "In this paper, we consider the problem of reference-based video\nsuper-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference\nframe to super-resolve a low-resolution (LR) video sequence. The existing\napproaches to RefVSR essentially attempt to align the reference and the input\nsequence, in the presence of resolution gap and long temporal range. However,\nthey either ignore temporal structure within the input sequence, or suffer\naccumulative alignment errors. To address these issues, we propose EFENet to\nexploit simultaneously the visual cues contained in the HR reference and the\ntemporal information contained in the LR sequence. EFENet first globally\nestimates cross-scale flow between the reference and each LR frame. Then our\nnovel flow refinement module of EFENet refines the flow regarding the furthest\nframe using all the estimated flows, which leverages the global temporal\ninformation within the sequence and therefore effectively reduces the alignment\nerrors. We provide comprehensive evaluations to validate the strengths of our\napproach, and to demonstrate that the proposed framework outperforms the\nstate-of-the-art methods. Code is available at\nhttps://github.com/IndigoPurple/EFENet.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Yaping Zhao",
      "Mengqi Ji",
      "Ruqi Huang",
      "Bin Wang",
      "Shengjin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07797"
  },
  {
    "id": "arXiv:2110.07798",
    "title": "Dynamics of Cross-Platform Attention to Retracted Papers: Pervasiveness,  Audience Skepticism, and Timing of Retractions",
    "abstract": "Retracted papers often circulate widely on social media, online news outlets\nand other websites before their official retraction. The spread of potentially\ninaccurate or misleading results from retracted papers can harm the scientific\ncommunity and the public. Here we quantify the amount and type of attention\n3,985 retracted papers received over time in different online platforms,\nranging from social media to knowledge repositories. Comparing to a set of\nnon-retracted control papers, we show that retracted papers receive more\nattention after publication. This tendency seems to be more pronounced on news\noutlets and knowledge repositories. This finding indicates that untrustworthy\nresearch penetrates even curated platforms and is often shared uncritically,\namplifying the negative impact on the public. At the same time, we find that\nposts on Twitter tend to express more uncertainty about retracted than about\ncontrol papers, suggesting that these posts could help identify potentially\nflawed scientific findings. We also find that, around the time they are\nretracted, papers generate discussions that are mostly about the retraction\nincident rather than about the results of the paper, showing that by this point\npapers have exhausted attention to their findings and highlighting the limited\neffect of retractions in reducing uncritical conversations. Our findings reveal\nthe extent to which retracted papers are discussed on different online\nplatforms and identify at scale audience skepticism towards them. They also\nshow that retractions come too late, which has implications for efforts to\nbetter time retraction notices.",
    "descriptor": "",
    "authors": [
      "Hao Peng",
      "Daniel M. Romero",
      "Em\u0151ke-\u00c1gnes Horv\u00e1t"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Digital Libraries (cs.DL)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.07798"
  },
  {
    "id": "arXiv:2110.07801",
    "title": "Adversarial Purification through Representation Disentanglement",
    "abstract": "Deep learning models are vulnerable to adversarial examples and make\nincomprehensible mistakes, which puts a threat on their real-world deployment.\nCombined with the idea of adversarial training, preprocessing-based defenses\nare popular and convenient to use because of their task independence and good\ngeneralizability. Current defense methods, especially purification, tend to\nremove ``noise\" by learning and recovering the natural images. However,\ndifferent from random noise, the adversarial patterns are much easier to be\noverfitted during model training due to their strong correlation to the images.\nIn this work, we propose a novel adversarial purification scheme by presenting\ndisentanglement of natural images and adversarial perturbations as a\npreprocessing defense. With extensive experiments, our defense is shown to be\ngeneralizable and make significant protection against unseen strong adversarial\nattacks. It reduces the success rates of state-of-the-art \\textbf{ensemble}\nattacks from \\textbf{61.7\\%} to \\textbf{14.9\\%} on average, superior to a\nnumber of existing methods. Notably, our defense restores the perturbed images\nperfectly and does not hurt the clean accuracy of backbone models, which is\nhighly desirable in practice.",
    "descriptor": "",
    "authors": [
      "Tao Bai",
      "Jun Zhao",
      "Lanqing Guo",
      "Bihan Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07801"
  },
  {
    "id": "arXiv:2110.07803",
    "title": "ContraQA: Question Answering under Contradicting Contexts",
    "abstract": "With a rise in false, inaccurate, and misleading information in propaganda,\nnews, and social media, real-world Question Answering (QA) systems face the\nchallenges of synthesizing and reasoning over contradicting information to\nderive correct answers. This urgency gives rise to the need to make QA systems\nrobust to misinformation, a topic previously unexplored. We study the risk of\nmisinformation to QA models by investigating the behavior of the QA model under\ncontradicting contexts that are mixed with both real and fake information. We\ncreate the first large-scale dataset for this problem, namely Contra-QA, which\ncontains over 10K human-written and model-generated contradicting pairs of\ncontexts. Experiments show that QA models are vulnerable under contradicting\ncontexts brought by misinformation. To defend against such a threat, we build a\nmisinformation-aware QA system as a counter-measure that integrates question\nanswering and misinformation detection in a joint fashion.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Liangming Pan",
      "Wenhu Chen",
      "Min-Yen Kan",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07803"
  },
  {
    "id": "arXiv:2110.07804",
    "title": "Alternative Input Signals Ease Transfer in Multilingual Machine  Translation",
    "abstract": "Recent work in multilingual machine translation (MMT) has focused on the\npotential of positive transfer between languages, particularly cases where\nhigher-resourced languages can benefit lower-resourced ones. While training an\nMMT model, the supervision signals learned from one language pair can be\ntransferred to the other via the tokens shared by multiple source languages.\nHowever, the transfer is inhibited when the token overlap among source\nlanguages is small, which manifests naturally when languages use different\nwriting systems. In this paper, we tackle inhibited transfer by augmenting the\ntraining data with alternative signals that unify different writing systems,\nsuch as phonetic, romanized, and transliterated input. We test these signals on\nIndic and Turkic languages, two language families where the writing systems\ndiffer but languages still share common features. Our results indicate that a\nstraightforward multi-source self-ensemble -- training a model on a mixture of\nvarious signals and ensembling the outputs of the same model fed with different\nsignals during inference, outperforms strong ensemble baselines by 1.3 BLEU\npoints on both language families. Further, we find that incorporating\nalternative inputs via self-ensemble can be particularly effective when\ntraining set is small, leading to +5 BLEU when only 5% of the total training\ndata is accessible. Finally, our analysis demonstrates that including\nalternative signals yields more consistency and translates named entities more\naccurately, which is crucial for increased factuality of automated systems.",
    "descriptor": "",
    "authors": [
      "Simeng Sun",
      "Angela Fan",
      "James Cross",
      "Vishrav Chaudhary",
      "Chau Tran",
      "Philipp Koehn",
      "Francisco Guzman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07804"
  },
  {
    "id": "arXiv:2110.07806",
    "title": "Communicating Patient Health Data: A Wicked Problem",
    "abstract": "Designing patient-collected health data visualizations to support discussing\npatient data during clinical visits is a challenging problem due to the\nheterogeneity of the parties involved: patients, healthcare providers, and\nhealthcare systems. Designers must ensure that all parties' needs are met. This\ncomplexity makes it challenging to find a definitive solution that can work for\nevery individual. We have approached this research problem -- communicating\npatient data during clinical visits -- as a wicked problem. In this article, we\noutline how wicked problem characteristics apply to our research problem. We\nthen describe the research methodologies we employed to explore the design\nspace of individualized patient data visualization solutions. Last, we reflect\non the insights and experiences we gained through this exploratory design\nprocess. We conclude with a call to action for researchers and visualization\ndesigners to consider patients' and healthcare providers' individualities when\ndesigning patient data visualizations.",
    "descriptor": "\nComments: 8 pages, 4 figures, Journal of IEEE Computer Graphics and Applications - Department: People in Practice\n",
    "authors": [
      "Fateme Rajabiyazdi",
      "Charles Perin",
      "Lora Oehlberg",
      "Sheelagh Carpendale"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.07806"
  },
  {
    "id": "arXiv:2110.07807",
    "title": "Provable Regret Bounds for Deep Online Learning and Control",
    "abstract": "The use of deep neural networks has been highly successful in reinforcement\nlearning and control, although few theoretical guarantees for deep learning\nexist for these problems. There are two main challenges for deriving\nperformance guarantees: a) control has state information and thus is inherently\nonline and b) deep networks are non-convex predictors for which online learning\ncannot provide provable guarantees in general.\nBuilding on the linearization technique for overparameterized neural\nnetworks, we derive provable regret bounds for efficient online learning with\ndeep neural networks. Specifically, we show that over any sequence of convex\nloss functions, any low-regret algorithm can be adapted to optimize the\nparameters of a neural network such that it competes with the best net in\nhindsight. As an application of these results in the online setting, we obtain\nprovable bounds for online episodic control with deep neural network\ncontrollers.",
    "descriptor": "",
    "authors": [
      "Xinyi Chen",
      "Edgar Minasyan",
      "Jason D. Lee",
      "Elad Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07807"
  },
  {
    "id": "arXiv:2110.07808",
    "title": "Mobility Aware Edge Computing Segmentation Towards Localized  Orchestration",
    "abstract": "The current trend in end-user devices' advancements in computing and\ncommunication capabilities makes edge computing an attractive solution to pave\nthe way for the coveted ultra-low latency services. The success of the edge\ncomputing networking paradigm depends on the proper orchestration of the edge\nservers. Several Edge applications and services are intolerant to latency,\nespecially in 5G and beyond networks, such as intelligent video surveillance,\nE-health, Internet of Vehicles, and augmented reality applications. The edge\ndevices underwent rapid growth in both capabilities and size to cope with the\nservice demands. Orchestrating it on the cloud was a prominent trend during the\npast decade. However, the increasing number of edge devices poses a significant\nburden on the orchestration delay. In addition to the growth in edge devices,\nthe high mobility of users renders traditional orchestration schemes\nimpractical for contemporary edge networks. Proper segmentation of the edge\nspace becomes necessary to adapt these schemes to address these challenges. In\nthis paper, we introduce a segmentation technique employing lax clustering and\nsegregated mobility-based clustering. We then apply latency mapping to these\nclusters. The proposed scheme's main objective is to create subspaces\n(segments) that enable light and efficient edge orchestration by reducing the\nprocessing time and the core cloud communication overhead. A bench-marking\nsimulation is conducted with the results showing decreased mobility-related\nfailures and reduced orchestration delay.",
    "descriptor": "\nComments: accepted at ISNCC 2021\n",
    "authors": [
      "Sam Aleyadeh",
      "Abdallah Moubayed",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07808"
  },
  {
    "id": "arXiv:2110.07809",
    "title": "PTQ-SL: Exploring the Sub-layerwise Post-training Quantization",
    "abstract": "Network quantization is a powerful technique to compress convolutional neural\nnetworks. The quantization granularity determines how to share the scaling\nfactors in weights, which affects the performance of network quantization. Most\nexisting approaches share the scaling factors layerwisely or channelwisely for\nquantization of convolutional layers. Channelwise quantization and layerwise\nquantization have been widely used in various applications. However, other\nquantization granularities are rarely explored. In this paper, we will explore\nthe sub-layerwise granularity that shares the scaling factor across multiple\ninput and output channels. We propose an efficient post-training quantization\nmethod in sub-layerwise granularity (PTQ-SL). Then we systematically experiment\non various granularities and observe that the prediction accuracy of the\nquantized neural network has a strong correlation with the granularity.\nMoreover, we find that adjusting the position of the channels can improve the\nperformance of sub-layerwise quantization. Therefore, we propose a method to\nreorder the channels for sub-layerwise quantization. The experiments\ndemonstrate that the sub-layerwise quantization with appropriate channel\nreordering can outperform the channelwise quantization.",
    "descriptor": "",
    "authors": [
      "Zhihang Yuan",
      "Yiqi Chen",
      "Chenhao Xue",
      "Chenguang Zhang",
      "Qiankun Wang",
      "Qiankun Wang",
      "Guangyu Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07809"
  },
  {
    "id": "arXiv:2110.07810",
    "title": "Towards Statistical and Computational Complexities of Polyak Step Size  Gradient Descent",
    "abstract": "We study the statistical and computational complexities of the Polyak step\nsize gradient descent algorithm under generalized smoothness and Lojasiewicz\nconditions of the population loss function, namely, the limit of the empirical\nloss function when the sample size goes to infinity, and the stability between\nthe gradients of the empirical and population loss functions, namely, the\npolynomial growth on the concentration bound between the gradients of sample\nand population loss functions. We demonstrate that the Polyak step size\ngradient descent iterates reach a final statistical radius of convergence\naround the true parameter after logarithmic number of iterations in terms of\nthe sample size. It is computationally cheaper than the polynomial number of\niterations on the sample size of the fixed-step size gradient descent algorithm\nto reach the same final statistical radius when the population loss function is\nnot locally strongly convex. Finally, we illustrate our general theory under\nthree statistical examples: generalized linear model, mixture model, and mixed\nlinear regression model.",
    "descriptor": "\nComments: First three authors contributed equally. 40 pages, 4 figures\n",
    "authors": [
      "Tongzheng Ren",
      "Fuheng Cui",
      "Alexia Atsidakou",
      "Sujay Sanghavi",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07810"
  },
  {
    "id": "arXiv:2110.07811",
    "title": "Cascaded Fast and Slow Models for Efficient Semantic Code Search",
    "abstract": "The goal of natural language semantic code search is to retrieve a\nsemantically relevant code snippet from a fixed set of candidates using a\nnatural language query. Existing approaches are neither effective nor efficient\nenough towards a practical semantic code search system. In this paper, we\npropose an efficient and accurate semantic code search framework with cascaded\nfast and slow models, in which a fast transformer encoder model is learned to\noptimize a scalable index for fast retrieval followed by learning a slow\nclassification-based re-ranking model to improve the performance of the top K\nresults from the fast retrieval. To further reduce the high memory cost of\ndeploying two separate models in practice, we propose to jointly train the fast\nand slow model based on a single transformer encoder with shared parameters.\nThe proposed cascaded approach is not only efficient and scalable, but also\nachieves state-of-the-art results with an average mean reciprocal ranking (MRR)\nscore of 0.7795 (across 6 programming languages) as opposed to the previous\nstate-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Akhilesh Deepak Gotmare",
      "Junnan Li",
      "Shafiq Joty",
      "Steven C.H. Hoi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.07811"
  },
  {
    "id": "arXiv:2110.07812",
    "title": "Towards fast weak adversarial training to solve high dimensional  parabolic partial differential equations using XNODE-WAN",
    "abstract": "Due to the curse of dimensionality, solving high dimensional parabolic\npartial differential equations (PDEs) has been a challenging problem for\ndecades. Recently, a weak adversarial network (WAN) proposed in (Y.Zang et al.,\n2020) offered a flexible and computationally efficient approach to tackle this\nproblem defined on arbitrary domains by leveraging the weak solution. WAN\nreformulates the PDE problem as a generative adversarial network, where the\nweak solution (primal network) and the test function (adversarial network) are\nparameterized by the multi-layer deep neural networks (DNNs). However, it is\nnot yet clear whether DNNs are the most effective model for the parabolic PDE\nsolutions as they do not take into account the fundamentally different roles\nplayed by time and spatial variables in the solution. To reinforce the\ndifference, we design a novel so-called XNODE model for the primal network,\nwhich is built on the neural ODE (NODE) model with additional spatial\ndependency to incorporate the a priori information of the PDEs and serve as a\nuniversal and effective approximation to the solution. The proposed hybrid\nmethod (XNODE-WAN), by integrating the XNODE model within the WAN framework,\nleads to significant improvement in the performance and efficiency of training.\nNumerical results show that our method can reduce the training time to a\nfraction of that of the WAN model.",
    "descriptor": "\nComments: 35 pages, 7 figures\n",
    "authors": [
      "Paul Valsecchi Oliva",
      "Yue Wu",
      "Cuiyu He",
      "Hao Ni"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07812"
  },
  {
    "id": "arXiv:2110.07814",
    "title": "Meta-learning via Language Model In-context Tuning",
    "abstract": "The goal of meta-learning is to learn to adapt to a new task with only a few\nlabeled examples. To tackle this problem in NLP, we propose $\\textit{in-context\ntuning}$, which recasts adaptation and prediction as a simple sequence\nprediction problem: to form the input sequence, we concatenate the task\ninstruction, the labeled examples, and the target input to predict; to\nmeta-train the model to learn from in-context examples, we fine-tune a\npre-trained language model (LM) to predict the target label from the input\nsequences on a collection of tasks.\nWe benchmark our method on two collections of text classification tasks: LAMA\nand BinaryClfs. Compared to first-order MAML which adapts the model with\ngradient descent, our method better leverages the inductive bias of LMs to\nperform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROC\nscore on BinaryClfs, with increasing advantage w.r.t. model size. Compared to\nnon-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning\ndirectly learns to learn from in-context examples. On BinaryClfs, in-context\ntuning improves the average AUC-ROC score by an absolute $10\\%$, and reduces\nthe variance with respect to example ordering by 6x and example choices by 2x.",
    "descriptor": "",
    "authors": [
      "Yanda Chen",
      "Ruiqi Zhong",
      "Sheng Zha",
      "George Karypis",
      "He He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07814"
  },
  {
    "id": "arXiv:2110.07816",
    "title": "Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?",
    "abstract": "Multilingual Neural Machine Translation (MNMT) trains a single NMT model that\nsupports translation between multiple languages, rather than training separate\nmodels for different languages. Learning a single model can enhance the\nlow-resource translation by leveraging data from multiple languages. However,\nthe performance of an MNMT model is highly dependent on the type of languages\nused in training, as transferring knowledge from a diverse set of languages\ndegrades the translation performance due to negative transfer. In this paper,\nwe propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which\ncapitalises on language groups generated according to typological features and\nphylogeny of languages to overcome the issue of negative transfer. HKD\ngenerates a set of multilingual teacher-assistant models via a selective\nknowledge distillation mechanism based on the language groups, and then distils\nthe ultimate multilingual model from those assistants in an adaptive way.\nExperimental results derived from the TED dataset with 53 languages demonstrate\nthe effectiveness of our approach in avoiding the negative transfer effect in\nMNMT, leading to an improved translation performance (about 1 BLEU score on\naverage) compared to strong baselines.",
    "descriptor": "",
    "authors": [
      "Fahimeh Saleh",
      "Wray Buntine",
      "Gholamreza Haffari",
      "Lan Du"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07816"
  },
  {
    "id": "arXiv:2110.07821",
    "title": "Gait-based Frailty Assessment using Image Representation of IMU Signals  and Deep CNN",
    "abstract": "Frailty is a common and critical condition in elderly adults, which may lead\nto further deterioration of health. However, difficulties and complexities\nexist in traditional frailty assessments based on activity-related\nquestionnaires. These can be overcome by monitoring the effects of frailty on\nthe gait. In this paper, it is shown that by encoding gait signals as images,\ndeep learning-based models can be utilized for the classification of gait type.\nTwo deep learning models (a) SS-CNN, based on single stride input images, and\n(b) MS-CNN, based on 3 consecutive strides were proposed. It was shown that\nMS-CNN performs best with an accuracy of 85.1\\%, while SS-CNN achieved an\naccuracy of 77.3\\%. This is because MS-CNN can observe more features\ncorresponding to stride-to-stride variations which is one of the key symptoms\nof frailty. Gait signals were encoded as images using STFT, CWT, and GAF. While\nthe MS-CNN model using GAF images achieved the best overall accuracy and\nprecision, CWT has a slightly better recall. This study demonstrates how image\nencoded gait data can be used to exploit the full potential of deep learning\nCNN models for the assessment of frailty.",
    "descriptor": "\nComments: Accepted in 43rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2021)\n",
    "authors": [
      "Muhammad Zeeshan Arshad",
      "Dawoon Jung",
      "Mina Park",
      "Hyungeun Shin",
      "Jinwook Kim",
      "Kyung-Ryoul Mun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.07821"
  },
  {
    "id": "arXiv:2110.07822",
    "title": "On Extending Amdahl's law to Learn Computer Performance",
    "abstract": "The problem of learning parallel computer performance is investigated in the\ncontext of multicore processors. Given a fixed workload, the effect of varying\nsystem configuration on performance is sought. Conventionally, the performance\nspeedup due to a single resource enhancement is formulated using Amdahl's law.\nHowever, in case of multiple configurable resources the conventional\nformulation results in several disconnected speedup equations that cannot be\ncombined together to determine the overall speedup. To solve this problem, we\npropose to (1) extend Amdahl's law to accommodate multiple configurable\nresources into the overall speedup equation, and (2) transform the speedup\nequation into a multivariable regression problem suitable for machine learning.\nUsing experimental data from two benchmarks (SPECCPU 2017 and PCMark 10) and\nfour hardware platforms (Intel Xeon 8180M, AMD EPYC 7702P, Intel CoffeeLake\n8700K, and AMD Ryzen 3900X), analytical models are developed and\ncross-validated. Findings indicate that in most cases, the models result in an\naverage cross-validated accuracy higher than 95%, thereby validating the\nproposed extension of Amdahl's law. The proposed methodology enables rapid\ngeneration of intelligent analytical models to support future industrial\ndevelopment, optimization, and simulation needs.",
    "descriptor": "\nComments: 20 pages, 5 figures\n",
    "authors": [
      "Chaitanya Poolla",
      "Rahul Saxena"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.07822"
  },
  {
    "id": "arXiv:2110.07826",
    "title": "Machine Learning Algorithms In User Authentication Schemes",
    "abstract": "In the past two decades, the number of mobile products being created by\ncompanies has grown exponentially. However, although these devices are\nconstantly being upgraded with the newest features, the security measures used\nto protect these devices has stayed relatively the same over the past two\ndecades. The vast difference in growth patterns between devices and their\nsecurity is opening up the risk for more and more devices to easily become\ninfiltrated by nefarious users. Working off of previous work in the field, this\nstudy looks at the different Machine Learning algorithms used in user\nauthentication schemes involving touch dynamics and device movement. This study\naims to give a comprehensive overview of the current uses of different machine\nlearning algorithms that are frequently used in user authentication schemas\ninvolving touch dynamics and device movement. The benefits, limitations, and\nsuggestions for future work will be thoroughly discussed throughout this paper.",
    "descriptor": "",
    "authors": [
      "Laura Pryor",
      "Dr. Rushit Dave",
      "Dr. Naeem Seliya",
      "Dr. Evelyn R Sowells Boone"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07826"
  },
  {
    "id": "arXiv:2110.07827",
    "title": "DirectQuote: A Dataset for Direct Quotation Extraction and Attribution  in News Articles",
    "abstract": "Quotation extraction and attribution are challenging tasks, aiming at\ndetermining the spans containing quotations and attributing each quotation to\nthe original speaker. Applying this task to news data is highly related to\nfact-checking, media monitoring and news tracking. Direct quotations are more\ntraceable and informative, and therefore of great significance among different\ntypes of quotations. Therefore, this paper introduces DirectQuote, a corpus\ncontaining 19,760 paragraphs and 10,279 direct quotations manually annotated\nfrom online news media. To the best of our knowledge, this is the largest and\nmost complete corpus that focuses on direct quotations in news texts. We ensure\nthat each speaker in the annotation can be linked to a specific named entity on\nWikidata, benefiting various downstream tasks. In addition, for the first time,\nwe propose several sequence labeling models as baseline methods to extract and\nattribute quotations simultaneously in an end-to-end manner.",
    "descriptor": "",
    "authors": [
      "Yuanchi Zhang",
      "Yang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07827"
  },
  {
    "id": "arXiv:2110.07829",
    "title": "FedSEAL: Semi-Supervised Federated Learning with Self-Ensemble Learning  and Negative Learning",
    "abstract": "Federated learning (FL), a popular decentralized and privacy-preserving\nmachine learning (FL) framework, has received extensive research attention in\nrecent years. The majority of existing works focus on supervised learning (SL)\nproblems where it is assumed that clients carry labeled datasets while the\nserver has no data. However, in realistic scenarios, clients are often unable\nto label their data due to the lack of expertise and motivation while the\nserver may host a small amount of labeled data. How to reasonably utilize the\nserver labeled data and the clients' unlabeled data is thus of paramount\npractical importance. In this paper, we propose a new FL algorithm, called\nFedSEAL, to solve this Semi-Supervised Federated Learning (SSFL) problem. Our\nalgorithm utilizes self-ensemble learning and complementary negative learning\nto enhance both the accuracy and the efficiency of clients' unsupervised\nlearning on unlabeled data, and orchestrates the model training on both the\nserver side and the clients' side. Our experimental results on Fashion-MNIST\nand CIFAR10 datasets in the SSFL setting validate the effectiveness of our\nmethod, which outperforms the state-of-the-art SSFL methods by a large margin.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Jieming Bian",
      "Zhu Fu",
      "Jie Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07829"
  },
  {
    "id": "arXiv:2110.07831",
    "title": "RAP: Robustness-Aware Perturbations for Defending against Backdoor  Attacks on NLP Models",
    "abstract": "Backdoor attacks, which maliciously control a well-trained model's outputs of\nthe instances with specific triggers, are recently shown to be serious threats\nto the safety of reusing deep neural networks (DNNs). In this work, we propose\nan efficient online defense mechanism based on robustness-aware perturbations.\nSpecifically, by analyzing the backdoor training process, we point out that\nthere exists a big gap of robustness between poisoned and clean samples.\nMotivated by this observation, we construct a word-based robustness-aware\nperturbation to distinguish poisoned samples from clean samples to defend\nagainst the backdoor attacks on natural language processing (NLP) models.\nMoreover, we give a theoretical analysis about the feasibility of our\nrobustness-aware perturbation-based defense method. Experimental results on\nsentiment analysis and toxic detection tasks show that our method achieves\nbetter defending performance and much lower computational costs than existing\nonline defense methods. Our code is available at\nhttps://github.com/lancopku/RAP.",
    "descriptor": "\nComments: EMNLP 2021 (main conference), long paper, camera-ready version\n",
    "authors": [
      "Wenkai Yang",
      "Yankai Lin",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07831"
  },
  {
    "id": "arXiv:2110.07832",
    "title": "A Modern Analysis of Aging Machine Learning Based IoT Cybersecurity  Methods",
    "abstract": "Modern scientific advancements often contribute to the introduction and\nrefinement of never-before-seen technologies. This can be quite the task for\nhumans to maintain and monitor and as a result, our society has become reliant\non machine learning to assist in this task. With new technology comes new\nmethods and thus new ways to circumvent existing cyber security measures. This\nstudy examines the effectiveness of three distinct Internet of Things cyber\nsecurity algorithms currently used in industry today for malware and intrusion\ndetection: Random Forest (RF), Support-Vector Machine (SVM), and K-Nearest\nNeighbor (KNN). Each algorithm was trained and tested on the Aposemat IoT-23\ndataset which was published in January 2020 with the earliest of captures from\n2018 and latest from 2019. The RF, SVM, and KNN reached peak accuracies of\n92.96%, 86.23%, and 91.48%, respectively, in intrusion detection and 92.27%,\n83.52%, and 89.80% in malware detection. It was found all three algorithms are\ncapable of being effectively utilized for the current landscape of IoT cyber\nsecurity in 2021.",
    "descriptor": "",
    "authors": [
      "Sam Strecker",
      "Rushit Dave",
      "Nyle Siddiqui",
      "Naeem Seliya"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07832"
  },
  {
    "id": "arXiv:2110.07833",
    "title": "Span Detection for Aspect-Based Sentiment Analysis in Vietnamese",
    "abstract": "Aspect-based sentiment analysis plays an essential role in natural language\nprocessing and artificial intelligence. Recently, researchers only focused on\naspect detection and sentiment classification but ignoring the sub-task of\ndetecting user opinion span, which has enormous potential in practical\napplications. In this paper, we present a new Vietnamese dataset (UIT-ViSD4SA)\nconsisting of 35,396 human-annotated spans on 11,122 feedback comments for\nevaluating the span detection in aspect-based sentiment analysis. Besides, we\nalso propose a novel system using Bidirectional Long Short-Term Memory (BiLSTM)\nwith a Conditional Random Field (CRF) layer (BiLSTM-CRF) for the span detection\ntask in Vietnamese aspect-based sentiment analysis. The best result is a 62.76%\nF1 score (macro) for span detection using BiLSTM-CRF with embedding fusion of\nsyllable embedding, character embedding, and contextual embedding from\nXLM-RoBERTa. In future work, span detection will be extended in many NLP tasks\nsuch as constructive detection, emotion recognition, complaint analysis, and\nopinion mining. Our dataset is freely available at\nhttps://github.com/kimkim00/UIT-ViSD4SA for research purposes.",
    "descriptor": "",
    "authors": [
      "Kim Thi-Thanh Nguyen",
      "Sieu Khai Huynh",
      "Luong Luc Phan",
      "Phuc Huynh Pham",
      "Duc-Vu Nguyen",
      "Kiet Van Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07833"
  },
  {
    "id": "arXiv:2110.07837",
    "title": "Cross-Lingual Fine-Grained Entity Typing",
    "abstract": "The growth of cross-lingual pre-trained models has enabled NLP tools to\nrapidly generalize to new languages. While these models have been applied to\ntasks involving entities, their ability to explicitly predict typological\nfeatures of these entities across languages has not been established. In this\npaper, we present a unified cross-lingual fine-grained entity typing model\ncapable of handling over 100 languages and analyze this model's ability to\ngeneralize to languages and entities unseen during training. We train this\nmodel on cross-lingual training data collected from Wikipedia hyperlinks in\nmultiple languages (training languages). During inference, our model takes an\nentity mention and context in a particular language (test language, possibly\nnot in the training languages) and predicts fine-grained types for that entity.\nGeneralizing to new languages and unseen entities are the fundamental\nchallenges of this entity typing setup, so we focus our evaluation on these\nsettings and compare against simple yet powerful string match baselines.\nExperimental results show that our approach outperforms the baselines on unseen\nlanguages such as Japanese, Tamil, Arabic, Serbian, and Persian. In addition,\nour approach substantially improves performance on unseen entities (even in\nunseen languages) over the baselines, and human evaluation shows a strong\nability to predict relevant types in these settings.",
    "descriptor": "",
    "authors": [
      "Nila Selvaraj",
      "Yasumasa Onoe",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07837"
  },
  {
    "id": "arXiv:2110.07840",
    "title": "ESPnet2-TTS: Extending the Edge of TTS Research",
    "abstract": "This paper describes ESPnet2-TTS, an end-to-end text-to-speech (E2E-TTS)\ntoolkit. ESPnet2-TTS extends our earlier version, ESPnet-TTS, by adding many\nnew features, including: on-the-fly flexible pre-processing, joint training\nwith neural vocoders, and state-of-the-art TTS models with extensions like\nfull-band E2E text-to-waveform modeling, which simplify the training pipeline\nand further enhance TTS performance. The unified design of our recipes enables\nusers to quickly reproduce state-of-the-art E2E-TTS results. We also provide\nmany pre-trained models in a unified Python interface for inference, offering a\nquick means for users to generate baseline samples and build demos.\nExperimental evaluations with English and Japanese corpora demonstrate that our\nprovided models synthesize utterances comparable to ground-truth ones,\nachieving state-of-the-art TTS performance. The toolkit is available online at\nhttps://github.com/espnet/espnet.",
    "descriptor": "\nComments: Submitted to ICASSP2022. Demo HP: this https URL\n",
    "authors": [
      "Tomoki Hayashi",
      "Ryuichi Yamamoto",
      "Takenori Yoshimura",
      "Peter Wu",
      "Jiatong Shi",
      "Takaaki Saeki",
      "Yooncheol Ju",
      "Yusuke Yasuda",
      "Shinnosuke Takamichi",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07840"
  },
  {
    "id": "arXiv:2110.07843",
    "title": "FOLD-R++: A Toolset for Automated Inductive Learning of Default Theories  from Mixed Data",
    "abstract": "FOLD-R is an automated inductive learning algorithm for learning default\nrules with exceptions for mixed (numerical and categorical) data. It generates\nan (explainable) answer set programming (ASP) rule set for classification\ntasks. We present an improved FOLD-R algorithm, called FOLD-R++, that\nsignificantly increases the efficiency and scalability of FOLD-R. FOLD-R++\nimproves upon FOLD-R without compromising or losing information in the input\ntraining data during the encoding or feature selection phase. The FOLD-R++\nalgorithm is competitive in performance with the widely-used XGBoost algorithm,\nhowever, unlike XGBoost, the FOLD-R++ algorithm produces an explainable model.\nNext, we create a powerful tool-set by combining FOLD-R++ with s(CASP)-a\ngoal-directed ASP execution engine-to make predictions on new data samples\nusing the answer set program generated by FOLD-R++. The s(CASP) system also\nproduces a justification for the prediction. Experiments presented in this\npaper show that our improved FOLD-R++ algorithm is a significant improvement\nover the original design and that the s(CASP) system can make predictions in an\nefficient manner as well.",
    "descriptor": "",
    "authors": [
      "Huaduo Wang",
      "Gopal Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07843"
  },
  {
    "id": "arXiv:2110.07844",
    "title": "Modeling Endorsement for Multi-Document Abstractive Summarization",
    "abstract": "A crucial difference between single- and multi-document summarization is how\nsalient content manifests itself in the document(s). While such content may\nappear at the beginning of a single document, essential information is\nfrequently reiterated in a set of documents related to a particular topic,\nresulting in an endorsement effect that increases information salience. In this\npaper, we model the cross-document endorsement effect and its utilization in\nmultiple document summarization. Our method generates a synopsis from each\ndocument, which serves as an endorser to identify salient content from other\ndocuments. Strongly endorsed text segments are used to enrich a neural\nencoder-decoder model to consolidate them into an abstractive summary. The\nmethod has a great potential to learn from fewer examples to identify salient\ncontent, which alleviates the need for costly retraining when the set of\ndocuments is dynamically adjusted. Through extensive experiments on benchmark\nmulti-document summarization datasets, we demonstrate the effectiveness of our\nproposed method over strong published baselines. Finally, we shed light on\nfuture research directions and discuss broader challenges of this task using a\ncase study.",
    "descriptor": "\nComments: EMNLP 2021 Workshop on New Frontiers in Summarization\n",
    "authors": [
      "Logan Lebanoff",
      "Bingqing Wang",
      "Zhe Feng",
      "Fei Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07844"
  },
  {
    "id": "arXiv:2110.07850",
    "title": "End-to-End Segmentation-based News Summarization",
    "abstract": "In this paper, we bring a new way of digesting news content by introducing\nthe task of segmenting a news article into multiple sections and generating the\ncorresponding summary to each section. We make two contributions towards this\nnew task. First, we create and make available a dataset, SegNews, consisting of\n27k news articles with sections and aligned heading-style section summaries.\nSecond, we propose a novel segmentation-based language generation model adapted\nfrom pre-trained language models that can jointly segment a document and\nproduce the summary for each section. Experimental results on SegNews\ndemonstrate that our model can outperform several state-of-the-art\nsequence-to-sequence generation models for this new task.",
    "descriptor": "",
    "authors": [
      "Yang Liu",
      "Chenguang Zhu",
      "Michael Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07850"
  },
  {
    "id": "arXiv:2110.07855",
    "title": "Hierarchical Curriculum Learning for AMR Parsing",
    "abstract": "Abstract Meaning Representation (AMR) parsing translates sentences to the\nsemantic representation with a hierarchical structure, which is recently\nempowered by pretrained encoder-decoder models. However, the flat\nsentence-to-AMR training paradigm impedes the representation learning of\nconcepts and relations in the deeper AMR sub-graph. To make the\nsequence-to-sequence models better adapt to the inherent AMR structure, we\npropose a hierarchical curriculum learning (HCL) which consists of (1)\nstructure-level curriculum (SC) and (2) instance-level curriculum (IC). SC\nswitches progressively from shallow to deep AMR sub-graphs while IC transits\nfrom easy to hard AMR instances during training. Extensive experiments show\nthat BART trained with HCL achieves the state-of-the-art performance on the\nAMR-2.0 and AMR-3.0 benchmark, and significantly outperforms baselines on the\nstructure-dependent evaluation metrics and hard instances.",
    "descriptor": "",
    "authors": [
      "Peiyi Wang",
      "Liang Chen",
      "Tianyu Liu",
      "Baobao Chang",
      "Zhifang Sui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07855"
  },
  {
    "id": "arXiv:2110.07858",
    "title": "Understanding and Improving Robustness of Vision Transformers through  Patch-based Negative Augmentation",
    "abstract": "We investigate the robustness of vision transformers (ViTs) through the lens\nof their special patch-based architectural structure, i.e., they process an\nimage as a sequence of image patches. We find that ViTs are surprisingly\ninsensitive to patch-based transformations, even when the transformation\nlargely destroys the original semantics and makes the image unrecognizable by\nhumans. This indicates that ViTs heavily use features that survived such\ntransformations but are generally not indicative of the semantic class to\nhumans. Further investigations show that these features are useful but\nnon-robust, as ViTs trained on them can achieve high in-distribution accuracy,\nbut break down under distribution shifts. From this understanding, we ask: can\ntraining the model to rely less on these features improve ViT robustness and\nout-of-distribution performance? We use the images transformed with our\npatch-based operations as negatively augmented views and offer losses to\nregularize the training away from using non-robust features. This is a\ncomplementary view to existing research that mostly focuses on augmenting\ninputs with semantic-preserving transformations to enforce models' invariance.\nWe show that patch-based negative augmentation consistently improves robustness\nof ViTs across a wide set of ImageNet based robustness benchmarks. Furthermore,\nwe find our patch-based negative augmentation are complementary to traditional\n(positive) data augmentation, and together boost the performance further. All\nthe code in this work will be open-sourced.",
    "descriptor": "",
    "authors": [
      "Yao Qin",
      "Chiyuan Zhang",
      "Ting Chen",
      "Balaji Lakshminarayanan",
      "Alex Beutel",
      "Xuezhi Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07858"
  },
  {
    "id": "arXiv:2110.07859",
    "title": "Receptive Field Broadening and Boosting for Salient Object Detection",
    "abstract": "Salient object detection requires a comprehensive and scalable receptive\nfield to locate the visually significant objects in the image. Recently, the\nemergence of visual transformers and multi-branch modules has significantly\nenhanced the ability of neural networks to perceive objects at different\nscales. However, compared to the traditional backbone, the calculation process\nof transformers is time-consuming. Moreover, different branches of the\nmulti-branch modules could cause the same error back propagation in each\ntraining iteration, which is not conducive to extracting discriminative\nfeatures. To solve these problems, we propose a bilateral network based on\ntransformer and CNN to efficiently broaden local details and global semantic\ninformation simultaneously. Besides, a Multi-Head Boosting (MHB) strategy is\nproposed to enhance the specificity of different network branches. By\ncalculating the errors of different prediction heads, each branch can\nseparately pay more attention to the pixels that other branches predict\nincorrectly. Moreover, Unlike multi-path parallel training, MHB randomly\nselects one branch each time for gradient back propagation in a boosting way.\nAdditionally, an Attention Feature Fusion Module (AF) is proposed to fuse two\ntypes of features according to respective characteristics. Comprehensive\nexperiments on five benchmark datasets demonstrate that the proposed method can\nachieve a significant performance improvement compared with the\nstate-of-the-art methods.",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Mingcan Ma",
      "Changqun Xia",
      "Chenxi Xie",
      "Xiaowu Chen",
      "Jia Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07859"
  },
  {
    "id": "arXiv:2110.07867",
    "title": "Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning",
    "abstract": "How can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidences indicating that the adaptations of PLMs to\nvarious tasks can be reparameterized as optimizing only a few free parameters\nin a common low-dimensional intrinsic task subspace, which may help us\nunderstand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. Specifically, to find such a subspace and examine its universality, we\nresort to the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen tasks or data by only tuning parameters in the\nsubspace. We dub this pipeline as intrinsic prompt tuning (IPT). In\nexperiments, we study diverse few-shot NLP tasks and surprisingly find that in\na 5-dimensional subspace found with 100 random tasks, by only tuning 5 free\nparameters, we can recover 87% and 65% of the full prompt tuning performance\nfor 100 seen tasks (using different training data) and 20 unseen tasks,\nrespectively, showing great generalization ability of the found intrinsic task\nsubspace. Besides being an analysis tool, IPT could further bring practical\nbenefits, such as improving the prompt tuning stability.",
    "descriptor": "",
    "authors": [
      "Yujia Qin",
      "Xiaozhi Wang",
      "Yusheng Su",
      "Yankai Lin",
      "Ning Ding",
      "Zhiyuan Liu",
      "Juanzi Li",
      "Lei Hou",
      "Peng Li",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07867"
  },
  {
    "id": "arXiv:2110.07868",
    "title": "FedMe: Federated Learning via Model Exchange",
    "abstract": "Federated learning is a distributed machine learning method in which a single\nserver and multiple clients collaboratively build machine learning models\nwithout sharing datasets on clients. Numerous methods have been proposed to\ncope with the data heterogeneity issue in federated learning. Existing\nsolutions require a model architecture tuned by the central server, yet a major\ntechnical challenge is that it is difficult to tune the model architecture due\nto the absence of local data on the central server. In this paper, we propose\nFederated learning via Model exchange (FedMe), which personalizes models with\nautomatic model architecture tuning during the learning process. The novelty of\nFedMe lies in its learning process: clients exchange their models for model\narchitecture tuning and model training. First, to optimize the model\narchitectures for local data, clients tune their own personalized models by\ncomparing to exchanged models and picking the one that yields the best\nperformance. Second, clients train both personalized models and exchanged\nmodels by using deep mutual learning, in spite of different model architectures\nacross the clients. We perform experiments on three real datasets and show that\nFedMe outperforms state-of-the-art federated learning methods while tuning\nmodel architectures automatically.",
    "descriptor": "",
    "authors": [
      "Koji Matsuda",
      "Yuya Sasaki",
      "Chuan Xiao",
      "Makoto Onizuka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07868"
  },
  {
    "id": "arXiv:2110.07869",
    "title": "A Dual-Perception Graph Neural Network with Multi-hop Graph Generator",
    "abstract": "Graph neural networks (GNNs) have drawn increasing attention in recent years\nand achieved remarkable performance in many graph-based tasks, especially in\nsemi-supervised learning on graphs. However, most existing GNNs excessively\nrely on topological structures and aggregate multi-hop neighborhood information\nby simply stacking network layers, which may introduce superfluous noise\ninformation, limit the expressive power of GNNs and lead to the over-smoothing\nproblem ultimately. In light of this, we propose a novel Dual-Perception Graph\nNeural Network (DPGNN) to address these issues. In DPGNN, we utilize node\nfeatures to construct a feature graph, and perform node representations\nlearning based on the original topology graph and the constructed feature graph\nsimultaneously, which conduce to capture the structural neighborhood\ninformation and the feature-related information. Furthermore, we design a\nMulti-Hop Graph Generator (MHGG), which applies a node-to-hop attention\nmechanism to aggregate node-specific multi-hop neighborhood information\nadaptively. Finally, we apply self-ensembling to form a consistent prediction\nfor unlabeled node representations. Experimental results on five datasets with\ndifferent topological structures demonstrate that our proposed DPGNN achieves\ncompetitive performance across all datasets, four of which the results\noutperform the latest state-of-the-art models. The source code of our model is\navailable at https://github.com.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Li Zhou",
      "Wenyu Chen",
      "Dingyi Zeng",
      "Shaohuan Cheng",
      "Wanlong Liu",
      "Hong Qu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07869"
  },
  {
    "id": "arXiv:2110.07871",
    "title": "Socially Aware Bias Measurements for Hindi Language Representations",
    "abstract": "Language representations are an efficient tool used across NLP, but they are\nstrife with encoded societal biases. These biases are studied extensively, but\nwith a primary focus on English language representations and biases common in\nthe context of Western society. In this work, we investigate the biases present\nin Hindi language representations such as caste and religion associated biases.\nWe demonstrate how biases are unique to specific language representations based\non the history and culture of the region they are widely spoken in, and also\nhow the same societal bias (such as binary gender associated biases) when\ninvestigated across languages is encoded by different words and text spans.\nWith this work, we emphasize on the necessity of social-awareness along with\nlinguistic and grammatical artefacts when modeling language representations, in\norder to understand the biases encoded.",
    "descriptor": "\nComments: 11 Pages (5 Pages main content+ 1 pages for references + 5 Pages Appendix)\n",
    "authors": [
      "Vijit Malik",
      "Sunipa Dev",
      "Akihiro Nishi",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07871"
  },
  {
    "id": "arXiv:2110.07872",
    "title": "Role Similarity Metric Based on Spanning Rooted Forest",
    "abstract": "As a fundamental issue in network analysis, structural node similarity has\nreceived much attention in academia and is adopted in a wide range of\napplications. Among these proposed structural node similarity measures, role\nsimilarity stands out because of satisfying several axiomatic properties\nincluding automorphism conformation. Existing role similarity metrics cannot\nhandle top-k queries on large real-world networks due to the high time and\nspace cost. In this paper, we propose a new role similarity metric, namely\n\\textsf{ForestSim}. We prove that \\textsf{ForestSim} is an admissible role\nsimilarity metric and devise the corresponding top-k similarity search\nalgorithm, namely \\textsf{ForestSimSearch}, which is able to process a top-k\nquery in $O(k)$ time once the precomputation is finished. Moreover, we speed up\nthe precomputation by using a fast approximate algorithm to compute the\ndiagonal entries of the forest matrix, which reduces the time and space\ncomplexity of the precomputation to\n$O(\\epsilon^{-2}m\\log^5{n}\\log{\\frac{1}{\\epsilon}})$ and $O(m\\log^3{n})$,\nrespectively. Finally, we conduct extensive experiments on 26 real-world\nnetworks. The results show that \\textsf{ForestSim} works efficiently on\nmillion-scale networks and achieves comparable performance to the state-of-art\nmethods.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Qi Bao",
      "Zhongzhi Zhang"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07872"
  },
  {
    "id": "arXiv:2110.07875",
    "title": "Graph Neural Networks with Learnable Structural and Positional  Representations",
    "abstract": "Graph neural networks (GNNs) have become the standard learning architectures\nfor graphs. GNNs have been applied to numerous domains ranging from quantum\nchemistry, recommender systems to knowledge graphs and natural language\nprocessing. A major issue with arbitrary graphs is the absence of canonical\npositional information of nodes, which decreases the representation power of\nGNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An\napproach to tackle this issue is to introduce Positional Encoding (PE) of\nnodes, and inject it into the input layer, like in Transformers. Possible graph\nPE are Laplacian eigenvectors. In this work, we propose to decouple structural\nand positional representations to make easy for the network to learn these two\nessential properties. We introduce a novel generic architecture which we call\nLSPE (Learnable Structural and Positional Encodings). We investigate several\nsparse and fully-connected (Transformer-like) GNNs, and observe a performance\nincrease for molecular datasets, from 2.87% up to 64.14% when considering\nlearnable PE for both GNN classes.",
    "descriptor": "\nComments: Code at this https URL\n",
    "authors": [
      "Vijay Prakash Dwivedi",
      "Anh Tuan Luu",
      "Thomas Laurent",
      "Yoshua Bengio",
      "Xavier Bresson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07875"
  },
  {
    "id": "arXiv:2110.07879",
    "title": "Advances and Challenges in Deep Lip Reading",
    "abstract": "Driven by deep learning techniques and large-scale datasets, recent years\nhave witnessed a paradigm shift in automatic lip reading. While the main thrust\nof Visual Speech Recognition (VSR) was improving accuracy of Audio Speech\nRecognition systems, other potential applications, such as biometric\nidentification, and the promised gains of VSR systems, have motivated extensive\nefforts on developing the lip reading technology. This paper provides a\ncomprehensive survey of the state-of-the-art deep learning based VSR research\nwith a focus on data challenges, task-specific complications, and the\ncorresponding solutions. Advancements in these directions will expedite the\ntransformation of silent speech interface from theory to practice. We also\ndiscuss the main modules of a VSR pipeline and the influential datasets.\nFinally, we introduce some typical VSR application concerns and impediments to\nreal-world scenarios as well as future research directions.",
    "descriptor": "",
    "authors": [
      "Marzieh Oghbaie",
      "Arian Sabaghi",
      "Kooshan Hashemifard",
      "Mohammad Akbari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07879"
  },
  {
    "id": "arXiv:2110.07881",
    "title": "$k\\texttt{-experts}$ -- Online Policies and Fundamental Limits",
    "abstract": "This paper introduces and studies the $k\\texttt{-experts}$ problem -- a\ngeneralization of the classic Prediction with Expert's Advice (i.e., the\n$\\texttt{Experts}$) problem. Unlike the $\\texttt{Experts}$ problem, where the\nlearner chooses exactly one expert, in this problem, the learner selects a\nsubset of $k$ experts from a pool of $N$ experts at each round. The reward\nobtained by the learner at any round depends on the rewards of the selected\nexperts. The $k\\texttt{-experts}$ problem arises in many practical settings,\nincluding online ad placements, personalized news recommendations, and paging.\nOur primary goal is to design an online learning policy having a small regret.\nIn this pursuit, we propose $\\texttt{SAGE}$ ($\\textbf{Sa}$mpled\nHed$\\textbf{ge}$) - a framework for designing efficient online learning\npolicies by leveraging statistical sampling techniques. We show that, for many\nrelated problems, $\\texttt{SAGE}$ improves upon the state-of-the-art bounds for\nregret and computational complexity. Furthermore, going beyond the notion of\nregret, we characterize the mistake bounds achievable by online learning\npolicies for a class of stable loss functions. We conclude the paper by\nestablishing a tight regret lower bound for a variant of the\n$k\\texttt{-experts}$ problem and carrying out experiments with standard\ndatasets.",
    "descriptor": "",
    "authors": [
      "Samrat Mukhopadhyay",
      "Sourav Sahoo",
      "Abhishek Sinha"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07881"
  },
  {
    "id": "arXiv:2110.07882",
    "title": "PolyNet: Polynomial Neural Network for 3D Shape Recognition with  PolyShape Representation",
    "abstract": "3D shape representation and its processing have substantial effects on 3D\nshape recognition. The polygon mesh as a 3D shape representation has many\nadvantages in computer graphics and geometry processing. However, there are\nstill some challenges for the existing deep neural network (DNN)-based methods\non polygon mesh representation, such as handling the variations in the degree\nand permutations of the vertices and their pairwise distances. To overcome\nthese challenges, we propose a DNN-based method (PolyNet) and a specific\npolygon mesh representation (PolyShape) with a multi-resolution structure.\nPolyNet contains two operations; (1) a polynomial convolution (PolyConv)\noperation with learnable coefficients, which learns continuous distributions as\nthe convolutional filters to share the weights across different vertices, and\n(2) a polygonal pooling (PolyPool) procedure by utilizing the multi-resolution\nstructure of PolyShape to aggregate the features in a much lower dimension. Our\nexperiments demonstrate the strength and the advantages of PolyNet on both 3D\nshape classification and retrieval tasks compared to existing polygon\nmesh-based methods and its superiority in classifying graph representations of\nimages. The code is publicly available from\nhttps://myavartanoo.github.io/polynet/.",
    "descriptor": "",
    "authors": [
      "Mohsen Yavartanoo",
      "Shih-Hsuan Hung",
      "Reyhaneh Neshatavar",
      "Yue Zhang",
      "Kyoung Mu Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07882"
  },
  {
    "id": "arXiv:2110.07885",
    "title": "On proportional fairness of uplink spectral efficiency in cell-free  massive MIMO systems",
    "abstract": "This paper is concerned with the proportional fairness (PF) of the spectral\nefficiency (SE) maximization of uplinks in a cell-free (CF) massive\nmultiple-input multiple-output (MIMO) system in which a large number of\nsingle-antenna access points (APs) connected to a central processing unit (CPU)\nserve many single-antenna users. To detect the user signals, the APs use\nmatched filters based on the local channel state information while the CPU\ndeploys receiver filters based on knowledge of channel statistics. We devise\nthe maximization problem of the SE PF, which maximizes the sum of the logarithm\nof the achievable user rates, as a jointly nonconvex optimization problem of\nreceiver filter coefficients and user power allocation subject to user power\nconstraints. To handle the challenges associated with the nonconvexity of the\nformulated design problem, we develop an iterative algorithm by alternatively\nfinding optimal filter coefficients at the CPU and transmit powers at the\nusers. While the filter coefficient design is formulated as a generalized\neigenvalue problem, the power allocation problem is addressed by a gradient\nprojection (GP) approach. Simulation results show that the SE PF maximization\nnot only offers approximately the achievable sum rates as compared to the\nsum-rate maximization but also provides an improved trade-off between the user\nrate fairness and the achievable sum rate.",
    "descriptor": "\nComments: The article was published in International Journal of Communication Systems, volume 33, issue 14; 25 September 2020; this https URL\n",
    "authors": [
      "Viet Quoc Pham",
      "Ha Hoang Kha",
      "Le Ty Khanh"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07885"
  },
  {
    "id": "arXiv:2110.07888",
    "title": "ACE-HGNN: Adaptive Curvature Exploration Hyperbolic Graph Neural Network",
    "abstract": "Graph Neural Networks (GNNs) have been widely studied in various graph data\nmining tasks. Most existingGNNs embed graph data into Euclidean space and thus\nare less effective to capture the ubiquitous hierarchical structures in\nreal-world networks. Hyperbolic Graph Neural Networks(HGNNs) extend GNNs to\nhyperbolic space and thus are more effective to capture the hierarchical\nstructures of graphs in node representation learning. In hyperbolic geometry,\nthe graph hierarchical structure can be reflected by the curvatures of the\nhyperbolic space, and different curvatures can model different hierarchical\nstructures of a graph. However, most existing HGNNs manually set the curvature\nto a fixed value for simplicity, which achieves a suboptimal performance of\ngraph learning due to the complex and diverse hierarchical structures of the\ngraphs. To resolve this problem, we propose an Adaptive Curvature Exploration\nHyperbolic Graph NeuralNetwork named ACE-HGNN to adaptively learn the optimal\ncurvature according to the input graph and downstream tasks. Specifically,\nACE-HGNN exploits a multi-agent reinforcement learning framework and contains\ntwo agents, ACE-Agent andHGNN-Agent for learning the curvature and node\nrepresentations, respectively. The two agents are updated by a NashQ-leaning\nalgorithm collaboratively, seeking the optimal hyperbolic space indexed by the\ncurvature. Extensive experiments on multiple real-world graph datasets\ndemonstrate a significant and consistent performance improvement in model\nquality with competitive performance and good generalization ability.",
    "descriptor": "",
    "authors": [
      "Xingcheng Fu",
      "Jianxin Li",
      "Jia Wu",
      "Qingyun Sun",
      "Cheng Ji",
      "Senzhang Wang",
      "Jiajun Tan",
      "Hao Peng",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07888"
  },
  {
    "id": "arXiv:2110.07889",
    "title": "Breaking Bad? Semantic Versioning and Impact of Breaking Changes in  Maven Central",
    "abstract": "Just like any software, libraries evolve to incorporate new features, bug\nfixes, security patches, and refactorings. However, when a library evolves, it\nmay break the contract previously established with its clients by introducing\nBreaking Changes (BCs) in its API. These changes might trigger compile-time,\nlink-time, or run-time errors in client code. As a result, clients may hesitate\nto upgrade their dependencies, raising security concerns and making future\nupgrades even more difficult.Understanding how libraries evolve helps client\ndevelopers to know which changes to expect and where to expect them, and\nlibrary developers to understand how they might impact their clients. In the\nmost extensive study to date, Raemaekers et al. investigate to what extent\ndevelopers of Java libraries hosted on the Maven Central Repository (MCR)\nfollow semantic versioning conventions to signal the introduction of BCs and\nhow these changes impact client projects. Their results suggest that BCs are\nwidespread without regard for semantic versioning, with a significant impact on\nclients.In this paper, we conduct an external and differentiated replication\nstudy of their work. We identify and address some limitations of the original\nprotocol and expand the analysis to a new corpus spanning seven more years of\nthe MCR. We also present a novel static analysis tool for Java bytecode,\nMaracas, which provides us with: (i) the set of all BCs between two versions of\na library; and (ii) the set of locations in client code impacted by individual\nBCs. Our key findings, derived from the analysis of 119, 879 library upgrades\nand 293, 817 clients, contrast with the original study and show that 83.4% of\nthese upgrades do comply with semantic versioning. Furthermore, we observe that\nthe tendency to comply with semantic versioning has significantly increased\nover time. Finally, we find that most BCs affect code that is not used by any\nclient, and that only 7.9% of all clients are affected by BCs. These findings\nshould help (i) library developers to understand and anticipate the impact of\ntheir changes; (ii) library users to estimate library upgrading effort and to\npick libraries that are less likely to break; and (iii) researchers to better\nunderstand the dynamics of library-client co-evolution in Java.",
    "descriptor": "",
    "authors": [
      "Lina Ochoa",
      "Thomas Degueule",
      "Jean-R\u00e9my Falleri",
      "Jurgen Vinju"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.07889"
  },
  {
    "id": "arXiv:2110.07892",
    "title": "Combining Counterfactual Regret Minimization with Information Gain to  Solve Extensive Games with Imperfect Information",
    "abstract": "Counterfactual regret Minimization (CFR) is an effective algorithm for\nsolving extensive games with imperfect information (IIEG). However, CFR is only\nallowed to apply in a known environment such as the transition functions of the\nchance player and reward functions of the terminal nodes are aware in IIEGs.\nFor uncertain scenarios like the cases under Reinforcement Learning (RL),\nvariational information maximizing exploration (VIME) provides a useful\nframework for exploring environments using information gain. In this paper, we\npropose a method named VCFR that combines CFR with information gain to\ncalculate Nash Equilibrium (NE) in the scenario of IIEG under RL. By adding\ninformation gain to the reward, the average strategy calculated by CFR can be\ndirectly used as an interactive strategy, and the exploration efficiency of the\nalgorithm to uncertain environments has been significantly improved.\nExperimentally, The results demonstrate that this approach can not only\neffectively reduce the number of interactions with the environment, but also\nfind an approximate NE.",
    "descriptor": "",
    "authors": [
      "Chen Qiu",
      "Xuan Wang",
      "Tianzi Ma",
      "Yaojun Wen",
      "Jiajia Zhang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.07892"
  },
  {
    "id": "arXiv:2110.07895",
    "title": "A Machine Learning Approach for Delineating Similar Sound Symptoms of  Respiratory Conditions on a Smartphone",
    "abstract": "Clinical characterization and interpretation of respiratory sound symptoms\nhave remained a challenge due to the similarities in the audio properties that\nmanifest during auscultation in medical diagnosis. The misinterpretation and\nconflation of these sounds coupled with the comorbidity cases of the associated\nailments particularly, exercised-induced respiratory conditions; result in the\nunder-diagnosis and under-treatment of the conditions. Though several studies\nhave proposed computerized systems for objective classification and evaluation\nof these sounds, most of the algorithms run on desktop and backend systems. In\nthis study, we leverage the improved computational and storage capabilities of\nmodern smartphones to distinguish the respiratory sound symptoms using machine\nlearning algorithms namely: Random Forest (RF), Support Vector Machine (SVM),\nand k-Nearest Neighbour (k-NN). The appreciable performance of these\nclassifiers on a mobile phone shows smartphone as an alternate tool for\nrecognition and discrimination of respiratory symptoms in real-time scenarios.\nFurther, the objective clinical data provided by the machine learning process\ncould aid physicians in the screening and treatment of a patient during\nambulatory care where specialized medical devices may not be readily available.",
    "descriptor": "",
    "authors": [
      "Chinazunwa Uwaoma",
      "Gunjan Mansingh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07895"
  },
  {
    "id": "arXiv:2110.07897",
    "title": "Improving Unsupervised Domain Adaptive Re-Identification via  Source-Guided Selection of Pseudo-Labeling Hyperparameters",
    "abstract": "Unsupervised Domain Adaptation (UDA) for re-identification (re-ID) is a\nchallenging task: to avoid a costly annotation of additional data, it aims at\ntransferring knowledge from a domain with annotated data to a domain of\ninterest with only unlabeled data. Pseudo-labeling approaches have proven to be\neffective for UDA re-ID. However, the effectiveness of these approaches heavily\ndepends on the choice of some hyperparameters (HP) that affect the generation\nof pseudo-labels by clustering. The lack of annotation in the domain of\ninterest makes this choice non-trivial. Current approaches simply reuse the\nsame empirical value for all adaptation tasks and regardless of the target data\nrepresentation that changes through pseudo-labeling training phases. As this\nsimplistic choice may limit their performance, we aim at addressing this issue.\nWe propose new theoretical grounds on HP selection for clustering UDA re-ID as\nwell as method of automatic and cyclic HP tuning for pseudo-labeling UDA\nclustering: HyPASS. HyPASS consists in incorporating two modules in\npseudo-labeling methods: (i) HP selection based on a labeled source validation\nset and (ii) conditional domain alignment of feature discriminativeness to\nimprove HP selection based on source samples. Experiments on commonly used\nperson re-ID and vehicle re-ID datasets show that our proposed HyPASS\nconsistently improves the best state-of-the-art methods in re-ID compared to\nthe commonly used empirical HP setting.",
    "descriptor": "\nComments: Submitted to IEEE Access for review\n",
    "authors": [
      "Fabian Dubourvieux",
      "Ang\u00e9lique Loesch",
      "Romaric Audigier",
      "Samia Ainouz",
      "St\u00e9phane Canu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07897"
  },
  {
    "id": "arXiv:2110.07898",
    "title": "Certainty Modeling of a Decision Support System for Mobile Monitoring of  Exercise induced Respiratory Conditions",
    "abstract": "Mobile health systems in recent times, have notably improved the healthcare\nsector by empowering patients to actively participate in their health, and by\nfacilitating access to healthcare professionals. Effective operation of these\nmobile systems nonetheless, requires high level of intelligence and expertise\nimplemented in the form of decision support systems (DSS). However, common\nchallenges in the implementation include generalization and reliability, due to\nthe dynamics and incompleteness of information presented to the inference\nmodels. In this paper, we advance the use of ad hoc mobile decision support\nsystem to monitor and detect triggers and early symptoms of respiratory\ndistress provoked by strenuous physical exertion. The focus is on the\napplication of certainty theory to model inexact reasoning by the mobile\nmonitoring system. The aim is to develop a mobile tool to assist patients in\nmanaging their conditions, and to provide objective clinical data to aid\nphysicians in the screening, diagnosis, and treatment of the respiratory\nailments. We present the proposed model architecture and then describe an\napplication scenario in a clinical setting. We also show implementation of an\naspect of the system that enables patients in the self-management of their\nconditions.",
    "descriptor": "",
    "authors": [
      "Chinazunwa Uwaoma",
      "Gunjan. Mansingh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)",
      "Signal Processing (eess.SP)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2110.07898"
  },
  {
    "id": "arXiv:2110.07902",
    "title": "Zipping Strategies and Attribute Grammars",
    "abstract": "Strategic term rewriting and attribute grammars are two powerful programming\ntechniques widely used in language engineering. The former, relies on\nstrategies to apply term rewrite rules in defining language transformations,\nwhile the latter is suitable to express context-dependent language processing\nalgorithms. Each of these techniques, however, is usually implemented by its\nown powerful and large language processor system. As a result, it makes such\nsystems harder to extend and to combine.\nIn this paper, we present the embedding of both strategic tree rewriting and\nattribute grammars in a zipper-based, purely functional setting. Zippers\nprovide a simple, but generic tree-walk mechanism that is the building block\ntechnique we use to express the purely-functional embedding of both techniques.\nThe embedding of the two techniques in the same setting has several advantages:\nFirst, we easily combine/zip attribute grammars and strategies, thus providing\nlanguage engineers the best of the two worlds. Second, the combined embedding\nis easier to maintain and extend since it is written in a concise and uniform\nsetting. This results in a very small library which is able to express advanced\n(static) analysis and transformation tasks. We show the expressive power of our\nlibrary in optimizing |Haskell| let expressions, expressing several |Haskell|\nrefactorings and solving several language processing tasks of the LDTA Tool\nChallenge.",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 Nuno Macedo",
      "Marcos Viera",
      "Jo\u00e3o Saraiva"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07902"
  },
  {
    "id": "arXiv:2110.07904",
    "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
    "abstract": "As pre-trained language models have gotten larger, there has been growing\ninterest in parameter-efficient methods to apply these models to downstream\ntasks. Building on the PromptTuning approach of Lester et al. (2021), which\nlearns task-specific soft prompts to condition a frozen language model to\nperform downstream tasks, we propose a novel prompt-based transfer learning\napproach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one\nor more source tasks and then uses it to initialize the prompt for a target\ntask. We show that SPoT significantly boosts the performance of PromptTuning\nacross many tasks. More importantly, SPoT either matches or outperforms\nModelTuning, which fine-tunes the entire model on each individual task, across\nall model sizes while being more parameter-efficient (up to 27,000x fewer\ntask-specific parameters). We further conduct a large-scale study on task\ntransferability with 26 NLP tasks and 160 combinations of source-target tasks,\nand demonstrate that tasks can often benefit each other via prompt transfer.\nFinally, we propose a simple yet efficient retrieval approach that interprets\ntask prompts as task embeddings to identify the similarity between tasks and\npredict the most transferable source tasks for a given novel target task.",
    "descriptor": "\nComments: 20 pages, 6 figures, 5 tables\n",
    "authors": [
      "Tu Vu",
      "Brian Lester",
      "Noah Constant",
      "Rami Al-Rfou",
      "Daniel Cer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07904"
  },
  {
    "id": "arXiv:2110.07905",
    "title": "Towards Better Plasticity-Stability Trade-off in Incremental Learning: A  simple Linear Connector",
    "abstract": "Plasticity-stability dilemma is a main problem for incremental learning, with\nplasticity referring to the ability to learn new knowledge, and stability\nretaining the knowledge of previous tasks. Due to the lack of training samples\nfrom previous tasks, it is hard to balance the plasticity and stability. For\nexample, the recent null-space projection methods (e.g., Adam-NSCL) have shown\npromising performance on preserving previous knowledge, while such strong\nprojection also causes the performance degradation of the current task. To\nachieve better plasticity-stability trade-off, in this paper, we show that a\nsimple averaging of two independently optimized optima of networks, null-space\nprojection for past tasks and simple SGD for the current task, can attain a\nmeaningful balance between preserving already learned knowledge and granting\nsufficient flexibility for learning a new task. This simple linear connector\nalso provides us a new perspective and technology to control the trade-off\nbetween plasticity and stability. We evaluate the proposed method on several\nbenchmark datasets. The results indicate our simple method can achieve notable\nimprovement, and perform well on both the past and current tasks. In short, our\nmethod is an extremely simple approach and achieves a better balance model.",
    "descriptor": "",
    "authors": [
      "Guoliang Lin",
      "Hanglu Chu",
      "Hanjiang Lai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07905"
  },
  {
    "id": "arXiv:2110.07906",
    "title": "Hardware Architecture of Layered Decoders for PLDPC-Hadamard Codes",
    "abstract": "Protograph-based low-density parity-check Hadamard codes (PLDPC-HCs) are a\nnew type of ultimate-Shannon-limit-approaching codes. In this paper, we propose\na hardware architecture for the PLDPC-HC layered decoders. The decoders consist\nmainly of random address memories, Hadamard sub-decoders and control logics.\nTwo types of pipelined structures are presented and the latency and throughput\nof these two structures are derived. Implementation of the decoder design on an\nFPGA board shows that a throughput of $1.48$ Gbps is achieved with a bit error\nrate (BER) of $10^{-5}$ at around $E_b/N_0 = - 0.40$ dB. The decoder can also\nachieve the same BER at $E_b/N_0 = - 1.11$ dB with a reduced throughput of\n$0.20$ Gbps.",
    "descriptor": "\nComments: 28 pages, 11 figures, 3 tables\n",
    "authors": [
      "Peng W. Zhang",
      "Francis C.M. Lau",
      "Chiu-W. Sham"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07906"
  },
  {
    "id": "arXiv:2110.07907",
    "title": "Construction of $C^2$ cubic splines on arbitrary triangulations",
    "abstract": "In this paper, we address the problem of constructing $C^2$ cubic spline\nfunctions on a given arbitrary triangulation $\\mathcal{T}$. To this end, we\nendow every triangle of $\\mathcal{T}$ with a Wang-Shi macro-structure. The\n$C^2$ cubic space on such a refined triangulation has a stable dimension and\noptimal approximation power. Moreover, any spline function in such space can be\nlocally built on each of the macro-triangles independently via Hermite\ninterpolation. We provide a simplex spline basis for the space of $C^2$ cubics\ndefined on a single macro-triangle which behaves like a Bernstein/B-spline\nbasis over the triangle. The basis functions inherit recurrence relations and\ndifferentiation formulas from the simplex spline construction, they form a\nnonnegative partition of unity, they admit simple conditions for $C^2$ joins\nacross the edges of neighboring triangles, and they enjoy a Marsden-like\nidentity. Also, there is a single control net to facilitate control and early\nvisualization of a spline function over the macro-triangle. Thanks to these\nproperties, the complex geometry of the Wang-Shi macro-structure is transparent\nto the user. Stable global bases for the full space of $C^2$ cubics on the\nWang-Shi refined triangulation $\\mathcal{T}$ are deduced from the local simplex\nspline basis by extending the concept of minimal determining sets.",
    "descriptor": "",
    "authors": [
      "Tom Lyche",
      "Carla Manni",
      "Hendrik Speleers"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07907"
  },
  {
    "id": "arXiv:2110.07909",
    "title": "Multilingual Speech Recognition using Knowledge Transfer across Learning  Processes",
    "abstract": "Multilingual end-to-end(E2E) models have shown a great potential in the\nexpansion of the language coverage in the realm of automatic speech\nrecognition(ASR). In this paper, we aim to enhance the multilingual ASR\nperformance in two ways, 1)studying the impact of feeding a one-hot vector\nidentifying the language, 2)formulating the task with a meta-learning objective\ncombined with self-supervised learning (SSL). We associate every language with\na distinct task manifold and attempt to improve the performance by transferring\nknowledge across learning processes itself as compared to transferring through\nfinal model parameters. We employ this strategy on a dataset comprising of 6\nlanguages for an in-domain ASR task, by minimizing an objective related to\nexpected gradient path length. Experimental results reveal the best\npre-training strategy resulting in 3.55% relative reduction in overall WER. A\ncombination of LEAP and SSL yields 3.51% relative reduction in overall WER when\nusing language ID.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Rimita Lahiri",
      "Kenichi Kumatani",
      "Eric Sun",
      "Yao Qian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07909"
  },
  {
    "id": "arXiv:2110.07910",
    "title": "SaLinA: Sequential Learning of Agents",
    "abstract": "SaLinA is a simple library that makes implementing complex sequential\nlearning models easy, including reinforcement learning algorithms. It is built\nas an extension of PyTorch: algorithms coded with \\SALINA{} can be understood\nin few minutes by PyTorch users and modified easily. Moreover, SaLinA naturally\nworks with multiple CPUs and GPUs at train and test time, thus being a good fit\nfor the large-scale training use cases. In comparison to existing RL libraries,\nSaLinA has a very low adoption cost and capture a large variety of settings\n(model-based RL, batch RL, hierarchical RL, multi-agent RL, etc.). But SaLinA\ndoes not only target RL practitioners, it aims at providing sequential learning\ncapabilities to any deep learning programmer.",
    "descriptor": "",
    "authors": [
      "Ludovic Denoyer",
      "Alfredo de la Fuente",
      "Song Duong",
      "Jean-Baptiste Gaya",
      "Pierre-Alexandre Kamienny",
      "Daniel H. Thompson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07910"
  },
  {
    "id": "arXiv:2110.07911",
    "title": "Learning to Infer Kinematic Hierarchies for Novel Object Instances",
    "abstract": "Manipulating an articulated object requires perceiving itskinematic\nhierarchy: its parts, how each can move, and howthose motions are coupled.\nPrevious work has explored per-ception for kinematics, but none infers a\ncomplete kinematichierarchy on never-before-seen object instances, without\nrelyingon a schema or template. We present a novel perception systemthat\nachieves this goal. Our system infers the moving parts ofan object and the\nkinematic couplings that relate them. Toinfer parts, it uses a point cloud\ninstance segmentation neuralnetwork and to infer kinematic hierarchies, it uses\na graphneural network to predict the existence, direction, and typeof edges\n(i.e. joints) that relate the inferred parts. We trainthese networks using\nsimulated scans of synthetic 3D models.We evaluate our system on simulated\nscans of 3D objects, andwe demonstrate a proof-of-concept use of our system to\ndrivereal-world robotic manipulation.",
    "descriptor": "",
    "authors": [
      "Hameed Abdul-Rashid",
      "Miles Freeman",
      "Ben Abbatematteo",
      "George Konidaris",
      "Daniel Ritchie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.07911"
  },
  {
    "id": "arXiv:2110.07917",
    "title": "Improving overlay maps of science: combining overview and detail",
    "abstract": "Overlay maps of science are global base maps over which subsets of\npublications can be projected. Such maps can be used to monitor, explore, and\nstudy research through its publication output. Most maps of science, including\noverlay maps, are flat in the sense that they visualize research fields at one\nsingle level. Such maps generally fail to provide both overview and detail\nabout the research being analyzed. The aim of this study is to improve overlay\nmaps of science to provide both features in a single visualization. I created a\nmap based on a hierarchical classification of publications, including broad\ndisciplines for overview and more granular levels to incorporate detailed\ninformation. The classification was obtained by clustering articles in a\ncitation network of about 17 million publication records in PubMed from 1995\nonwards. The map emphasizes the hierarchical structure of the classification by\nvisualizing both disciplines and the underlying specialties. To show how the\nvisualization methodology can help getting both overview of research and\ndetailed information about its topical structure, I projected two overlay maps\nonto the base map: (1) open access publishing and (2) coronavirus/Covid-19\nresearch.",
    "descriptor": "",
    "authors": [
      "Peter Sj\u00f6g\u00e5rde"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.07917"
  },
  {
    "id": "arXiv:2110.07918",
    "title": "Estimating the Level and Direction of Phonetic Dialect Change in the  Northern Netherlands",
    "abstract": "This article reports ongoing investigations into phonetic change of dialect\ngroups in the northern Netherlandic language area, particularly the Frisian and\nLow Saxon dialect groups, which are known to differ in vitality. To achieve\nthis, we combine existing phonetically transcribed corpora with dialectometric\napproaches that allow us to quantify change among older male dialect speakers\nin a real-time framework. A multidimensional variant of the Levenshtein\ndistance, combined with methods that induce realistic phonetic distances\nbetween transcriptions, is used to estimate how much dialect groups have\nchanged between 1990 and 2010, and whether they changed towards Standard Dutch\nor away from it. Our analyses indicate that language change is a slow process\nin this geographical area. Moreover, the Frisian and Groningen dialect groups\nseem to be most stable, while the other Low Saxon varieties (excluding the\nGroningen dialect group) were shown to be most prone to change. We offer\npossible explanations for our findings, while we discuss shortcomings of the\ndata and approach in detail, as well as desiderata for future research.",
    "descriptor": "\nComments: Submitted to Taal & Tongval\n",
    "authors": [
      "Raoul Buurke",
      "Hedwig Sekeres",
      "Wilbert Heeringa",
      "Remco Knooihuizen",
      "Martijn Wieling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07918"
  },
  {
    "id": "arXiv:2110.07920",
    "title": "Data Generation using Texture Co-occurrence and Spatial Self-Similarity  for Debiasing",
    "abstract": "Classification models trained on biased datasets usually perform poorly on\nout-of-distribution samples since biased representations are embedded into the\nmodel. Recently, adversarial learning methods have been proposed to disentangle\nbiased representations, but it is challenging to discard only the biased\nfeatures without altering other relevant information. In this paper, we propose\na novel de-biasing approach that explicitly generates additional images using\ntexture representations of oppositely labeled images to enlarge the training\ndataset and mitigate the effect of biases when training a classifier. Every new\ngenerated image contains similar spatial information from a source image while\ntransferring textures from a target image of opposite label. Our model\nintegrates a texture co-occurrence loss that determines whether a generated\nimage's texture is similar to that of the target, and a spatial self-similarity\nloss that determines whether the spatial details between the generated and\nsource images are well preserved. Both generated and original training images\nare further used to train a classifier that is able to avoid learning unknown\nbias representations. We employ three distinct artificially designed datasets\nwith known biases to demonstrate the ability of our method to mitigate bias\ninformation, and report competitive performance over existing state-of-the-art\nmethods.",
    "descriptor": "",
    "authors": [
      "Myeongkyun Kang",
      "Dongkyu Won",
      "Miguel Luna",
      "Kyung Soo Hong",
      "June Hong Ahn",
      "Sang Hyun Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07920"
  },
  {
    "id": "arXiv:2110.07921",
    "title": "Diffraction Tomography, Fourier Reconstruction, and Full Waveform  Inversion",
    "abstract": "In this paper, we study the mathematical imaging problem of diffraction\ntomography (DT), which is an inverse scattering technique used to find material\nproperties of an object by illuminating it with probing waves and recording the\nscattered waves. Conventional DT relies on the Fourier diffraction theorem,\nwhich is applicable under the condition of weak scattering. However, if the\nobject has high contrasts or is too large compared to the wavelength, it tends\nto produce multiple scattering, which complicates the reconstruction. We give a\nsurvey on diffraction tomography and compare the reconstruction of low and high\ncontrast objects. We also implement and compare the reconstruction using the\nfull waveform inversion method which, contrary to the Born and Rytov\napproximations, works with the total field and is more robust to multiple\nscattering.",
    "descriptor": "\nComments: 31 pages, 21 figures\n",
    "authors": [
      "Florian Faucher",
      "Clemens Kirisits",
      "Michael Quellmalz",
      "Otmar Scherzer",
      "Eric Setterqvist"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.07921"
  },
  {
    "id": "arXiv:2110.07922",
    "title": "Anomaly Detection in Multi-Agent Trajectories for Automated Driving",
    "abstract": "Human drivers can recognise fast abnormal driving situations to avoid\naccidents. Similar to humans, automated vehicles are supposed to perform\nanomaly detection. In this work, we propose the spatio-temporal graph\nauto-encoder for learning normal driving behaviours. Our innovation is the\nability to jointly learn multiple trajectories of a dynamic number of agents.\nTo perform anomaly detection, we first estimate a density function of the\nlearned trajectory feature representation and then detect anomalies in\nlow-density regions. Due to the lack of multi-agent trajectory datasets for\nanomaly detection in automated driving, we introduce our dataset using a\ndriving simulator for normal and abnormal manoeuvres. Our evaluations show that\nour approach learns the relation between different agents and delivers\npromising results compared to the related works. The code, simulation and the\ndataset are publicly available on the project page:\nhttps://github.com/againerju/maad_highway.",
    "descriptor": "\nComments: 15 pages incl. supplementary material, 8 figures, 4 tables (accepted by CoRL 2021)\n",
    "authors": [
      "Julian Wiederer",
      "Arij Bouazizi",
      "Marco Troina",
      "Ulrich Kressel",
      "Vasileios Belagiannis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.07922"
  },
  {
    "id": "arXiv:2110.07923",
    "title": "Value Penalized Q-Learning for Recommender Systems",
    "abstract": "Scaling reinforcement learning (RL) to recommender systems (RS) is promising\nsince maximizing the expected cumulative rewards for RL agents meets the\nobjective of RS, i.e., improving customers' long-term satisfaction. A key\napproach to this goal is offline RL, which aims to learn policies from logged\ndata. However, the high-dimensional action space and the non-stationary\ndynamics in commercial RS intensify distributional shift issues, making it\nchallenging to apply offline RL methods to RS. To alleviate the action\ndistribution shift problem in extracting RL policy from static trajectories, we\npropose Value Penalized Q-learning (VPQ), an uncertainty-based offline RL\nalgorithm. It penalizes the unstable Q-values in the regression target by\nuncertainty-aware weights, without the need to estimate the behavior policy,\nsuitable for RS with a large number of items. We derive the penalty weights\nfrom the variances across an ensemble of Q-functions. To alleviate\ndistributional shift issues at test time, we further introduce the critic\nframework to integrate the proposed method with classic RS models. Extensive\nexperiments conducted on two real-world datasets show that the proposed method\ncould serve as a gain plugin for existing RS models.",
    "descriptor": "\nComments: An offline RL algorithm for recommender systems, 10 Pages\n",
    "authors": [
      "Chengqian Gao",
      "Ke Xu",
      "Peilin Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07923"
  },
  {
    "id": "arXiv:2110.07926",
    "title": "Structured Nonnegative Matrix Factorization for Traffic Flow Estimation  of Large Cloud Networks",
    "abstract": "Network traffic matrix estimation is an ill-posed linear inverse problem: it\nrequires to estimate the unobservable origin destination traffic flows, X,\ngiven the observable link traffic flows, Y, and a binary routing matrix, A,\nwhich are such that Y = AX. This is a challenging but vital problem as accurate\nestimation of OD flows is required for several network management tasks. In\nthis paper, we propose a novel model for the network traffic matrix estimation\nproblem which maps high-dimension OD flows to low-dimension latent flows with\nthe following three constraints: (1) nonnegativity constraint on the estimated\nOD flows, (2) autoregression constraint that enables the proposed model to\neffectively capture temporal patterns of the OD flows, and (3) orthogonality\nconstraint that ensures the mapping between low-dimensional latent flows and\nthe corresponding link flows to be distance preserving. The parameters of the\nproposed model are estimated with a training algorithm based on Nesterov\naccelerated gradient and generally shows fast convergence. We validate the\nproposed traffic flow estimation model on two real backbone IP network\ndatasets, namely Internet2 and G'EANT. Empirical results show that the proposed\nmodel outperforms the state-of-the-art models not only in terms of tracking the\nindividual OD flows but also in terms of standard performance metrics. The\nproposed model is also found to be highly scalable compared to the existing\nstate-of-the-art approaches.",
    "descriptor": "\nComments: 23 pages, 10 equation, 4 figures, 5 tables and 3 algorithms. The paper is accepted in Elsevier Computer Networks on Wednesday, October 13, 2021\n",
    "authors": [
      "Syed Muhammad Atif",
      "Nicolas Gillis",
      "Sameer Qazi",
      "Imran Naseem"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2110.07926"
  },
  {
    "id": "arXiv:2110.07933",
    "title": "Relation Preserving Triplet Mining for Stabilizing the Triplet Loss in  Vehicle Re-identification",
    "abstract": "Object appearances often change dramatically with pose variations. This\ncreates a challenge for embedding schemes that seek to map instances with the\nsame object ID to locations that are as close as possible. This issue becomes\nsignificantly heightened in complex computer vision tasks such as\nre-identification(re-id). In this paper, we suggest these dramatic appearance\nchanges are indications that an object ID is composed of multiple natural\ngroups and it is counter-productive to forcefully map instances from different\ngroups to a common location. This leads us to introduce Relation Preserving\nTriplet Mining (RPTM), a feature matching guided triplet mining scheme, that\nensures triplets will respect the natural sub-groupings within an object ID. We\nuse this triplet mining mechanism to establish a pose-aware, well-conditioned\ntriplet cost function. This allows a single network to be trained with fixed\nparameters across three challenging benchmarks, while still providing\nstate-of-the-art re-identification results.",
    "descriptor": "",
    "authors": [
      "Adhiraj Ghosh",
      "Kuruparan Shanmugalingam",
      "Wen-Yan Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07933"
  },
  {
    "id": "arXiv:2110.07936",
    "title": "Bridging the Gap: Cross-Lingual Summarization with Compression Rate",
    "abstract": "Cross-lingual Summarization (CLS), converting a document into a cross-lingual\nsummary, is highly related to Machine Translation (MT) task. However, MT\nresources are still underutilized for the CLS task. In this paper, we propose a\nnovel task, Cross-lingual Summarization with Compression rate (CSC), to benefit\ncross-lingual summarization through large-scale MT corpus. Through introducing\ncompression rate, we regard MT task as a special CLS task with the compression\nrate of 100%. Hence they can be trained as a unified task, sharing knowledge\nmore effectively. Moreover, to bridge these two tasks smoothly, we propose a\nsimple yet effective data augmentation method to produce document-summary pairs\nwith different compression rates. The proposed method not only improves the\nperformance of CLS task, but also provides controllability to generate\nsummaries in desired lengths. Experiments demonstrate that our method\noutperforms various strong baselines.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Yu Bai",
      "Heyan Huang",
      "Kai Fan",
      "Yang Gao",
      "Zewen Chi",
      "Boxing Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07936"
  },
  {
    "id": "arXiv:2110.07938",
    "title": "Identifying Causal Influences on Publication Trends and Behavior: A Case  Study of the Computational Linguistics Community",
    "abstract": "Drawing causal conclusions from observational real-world data is a very much\ndesired but challenging task. In this paper we present mixed-method analyses to\ninvestigate causal influences of publication trends and behavior on the\nadoption, persistence, and retirement of certain research foci --\nmethodologies, materials, and tasks that are of interest to the computational\nlinguistics (CL) community. Our key findings highlight evidence of the\ntransition to rapidly emerging methodologies in the research community (e.g.,\nadoption of bidirectional LSTMs influencing the retirement of LSTMs), the\npersistent engagement with trending tasks and techniques (e.g., deep learning,\nembeddings, generative, and language models), the effect of scientist location\nfrom outside the US, e.g., China on propensity of researching languages beyond\nEnglish, and the potential impact of funding for large-scale research programs.\nWe anticipate this work to provide useful insights about publication trends and\nbehavior and raise the awareness about the potential for causal inference in\nthe computational linguistics and a broader scientific community.",
    "descriptor": "\nComments: Accepted to First Workshop on Causal Inference & NLP at EMNLP 2021\n",
    "authors": [
      "Maria Glenski",
      "Svitlana Volkova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.07938"
  },
  {
    "id": "arXiv:2110.07940",
    "title": "Wasserstein Unsupervised Reinforcement Learning",
    "abstract": "Unsupervised reinforcement learning aims to train agents to learn a handful\nof policies or skills in environments without external reward. These\npre-trained policies can accelerate learning when endowed with external reward,\nand can also be used as primitive options in hierarchical reinforcement\nlearning. Conventional approaches of unsupervised skill discovery feed a latent\nvariable to the agent and shed its empowerment on agent's behavior by mutual\ninformation (MI) maximization. However, the policies learned by MI-based\nmethods cannot sufficiently explore the state space, despite they can be\nsuccessfully identified from each other. Therefore we propose a new framework\nWasserstein unsupervised reinforcement learning (WURL) where we directly\nmaximize the distance of state distributions induced by different policies.\nAdditionally, we overcome difficulties in simultaneously training N(N >2)\npolicies, and amortizing the overall reward to each step. Experiments show\npolicies learned by our approach outperform MI-based methods on the metric of\nWasserstein distance while keeping high discriminability. Furthermore, the\nagents trained by WURL can sufficiently explore the state space in mazes and\nMuJoCo tasks and the pre-trained policies can be applied to downstream tasks by\nhierarchical learning.",
    "descriptor": "",
    "authors": [
      "Shuncheng He",
      "Yuhang Jiang",
      "Hongchang Zhang",
      "Jianzhun Shao",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07940"
  },
  {
    "id": "arXiv:2110.07947",
    "title": "Channel Eigenvalues and Effective Degrees of Freedom of Reconfigurable  Intelligent Surfaces",
    "abstract": "As a promising candidate technology for the next-generation wireless\ncommunications, reconfigurable intelligent surface (RIS) has gained tremendous\nresearch interest in both the academia and industry in recent years. Only\nlimited knowledge, however, has been obtained about the channel eigenvlaue\ncharacteristics and degrees of freedom (DoF) of systems containing RISs. In\nthis paper, we focus on a wireless communication system where both the\ntransmitter and receiver are respectively equipped with an RIS. Features of\neigenvalues, such as their summation and individual behavior, are investigated\nfor both the correlation matrix of an RIS and the composite channel matrix\nencompassing the two RISs and the wireless channel. Furthermore, the concept of\neffective degrees of freedom (EDoF), i.e., the number of subchannels actively\ncontributing to conveying information, is revisited for RIS-enabled systems.\nAnalytical and numerical results demonstrate that the EDoF depends upon various\nfactors including the operating SNR, physical parameters of RISs, and\npropagation environment.",
    "descriptor": "\nComments: 23 pages, 11 figures\n",
    "authors": [
      "Shu Sun"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2110.07947"
  },
  {
    "id": "arXiv:2110.07950",
    "title": "Peer reviewers equally critique theory, method, and writing, with  limited effect on manuscripts' content",
    "abstract": "Peer review aims to detect flaws and deficiencies in the design and\ninterpretation of studies, and ensure the clarity and quality of their\npresentation. However, it has been questioned whether peer review fulfils this\nfunction. Studies have highlighted a stronger focus of reviewers on critiquing\nmethodological aspects of studies and the quality of writing in biomedical\nsciences, with less focus on theoretical grounding. In contrast, reviewers in\nthe social sciences appear more concerned with theoretical underpinnings. These\nstudies also found the effect of peer review on manuscripts' content to be\nvariable, but generally modest and positive. I qualitatively analysed 1,430\npeer reviewers' comments for a sample of 40 social science preprint-publication\npairs to identify the key foci of reviewers' comments. I then quantified the\neffect of peer review on manuscripts by examining differences between the\npreprint and published versions using the normalised Levenshtein distance,\ncosine similarity, and word count ratios for titles, abstracts, document\nsections and full-texts. I also examined changes in references used between\nversions and linked changes to reviewers' comments. Reviewers' comments were\nnearly equally split between issues of methodology (30.7%), theory (30.0%), and\nwriting quality (29.2%). Titles, abstracts, and the semantic content of\ndocuments remained similar, although publications were typically longer than\npreprints. Two-thirds of citations were unchanged, 20.9% were added during\nreview and 13.1% were removed. These findings indicate reviewers equally\nattended to the theoretical and methodological details and communication style\nof manuscripts, although the effect on quantitative measures of the manuscripts\nwas limited.",
    "descriptor": "",
    "authors": [
      "Dimity Stephen"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2110.07950"
  },
  {
    "id": "arXiv:2110.07953",
    "title": "Estimation and Prediction of Deterministic Human Intent Signal to  augment Haptic Glove aided Control of Robotic Hand",
    "abstract": "The paper focuses on Haptic Glove (HG) based control of a Robotic Hand (RH)\nexecuting in-hand manipulation. A control algorithm is presented to allow the\nRH relocate the object held to a goal pose. The motion signals for both the HG\nand the RH are high dimensional. The RH kinematics is usually different from\nthe HG kinematics. The variability of kinematics of the two devices, added with\nthe incomplete information about the human hand kinematics result in difficulty\nin direct mapping of the high dimensional motion signal of the HG to the RH.\nHence, a method is proposed to estimate the human intent from the high\ndimensional HG motion signal and reconstruct the signal at the RH to ensure\nobject relocation. It is also shown that the lag in synthesis of the motion\nsignal of the human hand added with the control latency of the RH leads to a\nrequirement of the prediction of the human intent signal. Then, a recurrent\nneural network (RNN) is proposed to predict the human intent signal ahead of\ntime.",
    "descriptor": "",
    "authors": [
      "Rajesh Kumar",
      "Pimmy Gandotra",
      "Brejesh Lall",
      "Arzad A. Kherani",
      "Sudipto Mukherjee"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07953"
  },
  {
    "id": "arXiv:2110.07954",
    "title": "HTTPA: HTTPS Attestable Protocol",
    "abstract": "Hypertext Transfer Protocol Secure (HTTPS) protocol has become integral part\nof the modern internet technology. It is currently the primary protocol for\ncommercialized web applications. It can provide a fast, secure connection with\na certain level of privacy and integrity, and it has become a basic assumption\non most web services on the internet. However, HTTPS cannot provide security\nassurances on the request data in compute, so the computing environment remains\nuncertain risks and vulnerabilities. A hardware-based trusted execution\nenvironment (TEE) such as Intel Software Guard Extension (SGX) provides\nin-memory encryption to help protect the runtime computation to reduce risks of\nillegal leaking or modifying private information. The central concept of SGX\nenables the computation happening inside the enclave, a protected environment\nthat encrypts the codes and data pertaining to a security-sensitive\ncomputation. In addition, SGX provides provide security assurances via remote\nattestation to the web client, including TCB identity, vendor identity and\nverification identity. Here we propose a HTTP protocol, called HTTPS Attestable\n(HTTPA), by including remote attestation process onto the HTTPS protocol to\naddress the privacy and security concerns on web and the access over the\nInternet. With HTTPA, we can provide security assurances to establish\ntrustworthiness with web services and ensure integrity of request handling for\nweb users. We expect that remote attestation will become a new trend adopted to\nreduce web services security risks, and propose the HTTPA protocol to unify the\nweb attestation and accessing services in a standard and efficient way.",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Gordon King",
      "Hans Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07954"
  },
  {
    "id": "arXiv:2110.07959",
    "title": "Low-rank Matrix Recovery With Unknown Correspondence",
    "abstract": "We study a matrix recovery problem with unknown correspondence: given the\nobservation matrix $M_o=[A,\\tilde P B]$, where $\\tilde P$ is an unknown\npermutation matrix, we aim to recover the underlying matrix $M=[A,B]$. Such\nproblem commonly arises in many applications where heterogeneous data are\nutilized and the correspondence among them are unknown, e.g., due to privacy\nconcerns. We show that it is possible to recover $M$ via solving a nuclear norm\nminimization problem under a proper low-rank condition on $M$, with provable\nnon-asymptotic error bound for the recovery of $M$. We propose an algorithm,\n$\\text{M}^3\\text{O}$ (Matrix recovery via Min-Max Optimization) which recasts\nthis combinatorial problem as a continuous minimax optimization problem and\nsolves it by proximal gradient with a Max-Oracle. $\\text{M}^3\\text{O}$ can also\nbe applied to a more general scenario where we have missing entries in $M_o$\nand multiple groups of data with distinct unknown correspondence. Experiments\non simulated data, the MovieLens 100K dataset and Yale B database show that\n$\\text{M}^3\\text{O}$ achieves state-of-the-art performance over several\nbaselines and can recover the ground-truth correspondence with high accuracy.",
    "descriptor": "",
    "authors": [
      "Zhiwei Tang",
      "Tsung-Hui Chang",
      "Xiaojing Ye",
      "Hongyuan Zha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.07959"
  },
  {
    "id": "arXiv:2110.07961",
    "title": "Tracing Origins: Coref-aware Machine Reading Comprehension",
    "abstract": "Machine reading comprehension is a heavily-studied research and test field\nfor evaluating new pre-trained models and fine-tuning strategies, and recent\nstudies have enriched the pre-trained models with syntactic, semantic and other\nlinguistic information to improve the performance of the model. In this paper,\nwe imitated the human's reading process in connecting the anaphoric expressions\nand explicitly leverage the coreference information to enhance the word\nembeddings from the pre-trained model, in order to highlight the coreference\nmentions that must be identified for coreference-intensive question answering\nin QUOREF, a relatively new dataset that is specifically designed to evaluate\nthe coreference-related performance of a model. We used an additional BERT\nlayer to focus on the coreference mentions, and a Relational Graph\nConvolutional Network to model the coreference relations. We demonstrated that\nthe explicit incorporation of the coreference information in fine-tuning stage\nperformed better than the incorporation of the coreference information in\ntraining a pre-trained language models.",
    "descriptor": "",
    "authors": [
      "Baorong Huang",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07961"
  },
  {
    "id": "arXiv:2110.07964",
    "title": "Federated Route Leak Detection in Inter-domain Routing with Privacy  Guarantee",
    "abstract": "In the inter-domain network, a route leak occurs when a routing announcement\nis propagated outside of its intended scope, which is a violation of the agreed\nrouting policy. The route leaks can disrupt the internet traffic and cause\nlarge outages. The accurately detection of route leaks requires the share of AS\nbusiness relationship information of ASes. However, the business relationship\ninformation between ASes is confidential due to economic issues. Thus, ASes are\nusually unwilling to revealing this information to the other ASes, especially\ntheir competitors. Recent advancements in federated learning make it possible\nto share data while maintaining privacy. Motivated by this, in this paper we\nstudy the route leak problem by considering the privacy of business\nrelationships between ASes, and propose a method for route leak detection with\nprivacy guarantee by using blockchain-based federated learning framework, in\nwhich ASes can train a global detection model without revealing their business\nrelationships directly. Moreover, the proposed method provides a\nself-validation scheme by labeling AS triples with local routing policies,\nwhich mitigates route leaks' lack of ground truth. We evaluate the proposed\nmethod under a variety of datasets including unbalanced and balanced datasets.\nThe different deployment strategies of the proposed method under different\ntopologies are also examined. The results show that the proposed method has a\nbetter performance in detecting route leaks than a single AS detection\nregardless of whether using balanced or unbalanced datasets. In the analysis of\nthe deployment, the results show that ASes with more peers have more possible\nroute leaks and can contribute more on the detection of route leaks with the\nproposed method.",
    "descriptor": "",
    "authors": [
      "Man Zeng",
      "Dandan Li",
      "Pei Zhang",
      "Kun Xie",
      "Xiaohong Huang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.07964"
  },
  {
    "id": "arXiv:2110.07977",
    "title": "Achievable Secrecy Rate for the Relay Eavesdropper Channel with  Non-Causal State Available at Transmitter and Relay",
    "abstract": "In this paper, we consider a more general four-terminal memoryless\nrelay-eavesdropper channel with state information (REC-SI) and derive an\nachievable perfect secrecy rate for it. We suppose that the state information\nis non-causally available at the transmitter and relay only. The transmitter\nwishes to establish a secure communication with the legitimate receiver by the\nhelp of a relay where a confidential message will be kept secret from a passive\neavesdropper. We consider active cooperation between the relay and transmitter.\nThe relay helps the transmitter by relaying the message using\ndecode-and-forward (DF) scheme. The proposed model is a generalization of some\nexisting models and the derived achievable perfect secrecy rate is compared to\nthe special cases. The results are also validated numerically for the additive\nwhite Gaussian noise (AWGN) channel.",
    "descriptor": "",
    "authors": [
      "Nematollah Zarmehi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.07977"
  },
  {
    "id": "arXiv:2110.07981",
    "title": "Reappraising Domain Generalization in Neural Networks",
    "abstract": "Domain generalization (DG) of machine learning algorithms is defined as their\nability to learn a domain agnostic hypothesis from multiple training\ndistributions, which generalizes onto data from an unseen domain. DG is vital\nin scenarios where the target domain with distinct characteristics has sparse\ndata for training. Aligning with recent work~\\cite{gulrajani2020search}, we\nfind that a straightforward Empirical Risk Minimization (ERM) baseline\nconsistently outperforms existing DG methods. We present ablation studies\nindicating that the choice of backbone, data augmentation, and optimization\nalgorithms overshadows the many tricks and trades explored in the prior art.\nOur work leads to a new state of the art on the four popular DG datasets,\nsurpassing previous methods by large margins. Furthermore, as a key\ncontribution, we propose a classwise-DG formulation, where for each class, we\nrandomly select one of the domains and keep it aside for testing. We argue that\nthis benchmarking is closer to human learning and relevant in real-world\nscenarios. We comprehensively benchmark classwise-DG on the DomainBed and\npropose a method combining ERM and reverse gradients to achieve the\nstate-of-the-art results. To our surprise, despite being exposed to all domains\nduring training, the classwise DG is more challenging than traditional DG\nevaluation and motivates more fundamental rethinking on the problem of DG.",
    "descriptor": "",
    "authors": [
      "Sarath Sivaprasad",
      "Akshay Goindani",
      "Vaibhav Garg",
      "Vineet Gandhi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07981"
  },
  {
    "id": "arXiv:2110.07982",
    "title": "Scribosermo: Fast Speech-to-Text models for German and other Languages",
    "abstract": "Recent Speech-to-Text models often require a large amount of hardware\nresources and are mostly trained in English. This paper presents Speech-to-Text\nmodels for German, as well as for Spanish and French with special features: (a)\nThey are small and run in real-time on microcontrollers like a RaspberryPi. (b)\nUsing a pretrained English model, they can be trained on consumer-grade\nhardware with a relatively small dataset. (c) The models are competitive with\nother solutions and outperform them in German. In this respect, the models\ncombine advantages of other approaches, which only include a subset of the\npresented features. Furthermore, the paper provides a new library for handling\ndatasets, which is focused on easy extension with additional datasets and shows\nan optimized way for transfer-learning new languages using a pretrained model\nfrom another language with a similar alphabet.",
    "descriptor": "",
    "authors": [
      "Daniel Bermuth",
      "Alexander Poeppel",
      "Wolfgang Reif"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.07982"
  },
  {
    "id": "arXiv:2110.07983",
    "title": "NeuroLKH: Combining Deep Learning Model with Lin-Kernighan-Helsgaun  Heuristic for Solving the Traveling Salesman Problem",
    "abstract": "We present NeuroLKH, a novel algorithm that combines deep learning with the\nstrong traditional heuristic Lin-Kernighan-Helsgaun (LKH) for solving Traveling\nSalesman Problem. Specifically, we train a Sparse Graph Network (SGN) with\nsupervised learning for edge scores and unsupervised learning for node\npenalties, both of which are critical for improving the performance of LKH.\nBased on the output of SGN, NeuroLKH creates the edge candidate set and\ntransforms edge distances to guide the searching process of LKH. Extensive\nexperiments firmly demonstrate that, by training one model on a wide range of\nproblem sizes, NeuroLKH significantly outperforms LKH and generalizes well to\nmuch larger sizes. Also, we show that NeuroLKH can be applied to other routing\nproblems such as Capacitated Vehicle Routing Problem (CVRP), Pickup and\nDelivery Problem (PDP), and CVRP with Time Windows (CVRPTW).",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Liang Xin",
      "Wen Song",
      "Zhiguang Cao",
      "Jie Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07983"
  },
  {
    "id": "arXiv:2110.07985",
    "title": "On-Policy Model Errors in Reinforcement Learning",
    "abstract": "Model-free reinforcement learning algorithms can compute policy gradients\ngiven sampled environment transitions, but require large amounts of data. In\ncontrast, model-based methods can use the learned model to generate new data,\nbut model errors and bias can render learning unstable or sub-optimal. In this\npaper, we present a novel method that combines real world data and a learned\nmodel in order to get the best of both worlds. The core idea is to exploit the\nreal world data for on-policy predictions and use the learned model only to\ngeneralize to different actions. Specifically, we use the data as\ntime-dependent on-policy correction terms on top of a learned model, to retain\nthe ability to generate data without accumulating errors over long prediction\nhorizons. We motivate this method theoretically and show that it counteracts an\nerror term for model-based policy improvement. Experiments on MuJoCo- and\nPyBullet-benchmarks show that our method can drastically improve existing\nmodel-based approaches without introducing additional tuning parameters.",
    "descriptor": "",
    "authors": [
      "Lukas P. Fr\u00f6hlich",
      "Maksym Lefarov",
      "Melanie N. Zeilinger",
      "Felix Berkenkamp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07985"
  },
  {
    "id": "arXiv:2110.07986",
    "title": "On Generating Identifiable Virtual Faces",
    "abstract": "Face anonymization with generative models have become increasingly prevalent\nsince they sanitize private information by generating virtual face images,\nensuring both privacy and image utility. Such virtual face images are usually\nnot identifiable after the removal or protection of the original identity. In\nthis paper, we formalize and tackle the problem of generating identifiable\nvirtual face images. Our virtual face images are visually different from the\noriginal ones for privacy protection. In addition, they are bound with new\nvirtual identities, which can be directly used for face recognition. We propose\nan Identifiable Virtual Face Generator (IVFG) to generate the virtual face\nimages. The IVFG projects the latent vectors of the original face images into\nvirtual ones according to a user specific key, based on which the virtual face\nimages are generated. To make the virtual face images identifiable, we propose\na multi-task learning objective as well as a triplet styled training strategy\nto learn the IVFG. Various experiments demonstrate the effectiveness of the\nIVFG for generate identifiable virtual face images.",
    "descriptor": "",
    "authors": [
      "Zhuowen Yuan",
      "Sheng Li",
      "Xinpeng Zhang",
      "Zhenxin Qian",
      "Alex Kot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07986"
  },
  {
    "id": "arXiv:2110.07993",
    "title": "Pose-guided Generative Adversarial Net for Novel View Action Synthesis",
    "abstract": "We focus on the problem of novel-view human action synthesis. Given an action\nvideo, the goal is to generate the same action from an unseen viewpoint.\nNaturally, novel view video synthesis is more challenging than image synthesis.\nIt requires the synthesis of a sequence of realistic frames with temporal\ncoherency. Besides, transferring the different actions to a novel target view\nrequires awareness of action category and viewpoint change simultaneously. To\naddress these challenges, we propose a novel framework named Pose-guided Action\nSeparable Generative Adversarial Net (PAS-GAN), which utilizes pose to\nalleviate the difficulty of this task. First, we propose a recurrent\npose-transformation module which transforms actions from the source view to the\ntarget view and generates novel view pose sequence in 2D coordinate space.\nSecond, a well-transformed pose sequence enables us to separatethe action and\nbackground in the target view. We employ a novel local-global spatial\ntransformation module to effectively generate sequential video features in the\ntarget view using these action and background features. Finally, the generated\nvideo features are used to synthesize human action with the help of a 3D\ndecoder. Moreover, to focus on dynamic action in the video, we propose a novel\nmulti-scale action-separable loss which further improves the video quality. We\nconduct extensive experiments on two large-scale multi-view human action\ndatasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN\nwhich outperforms existing approaches.",
    "descriptor": "\nComments: Accepted by WACV2022\n",
    "authors": [
      "Xianhang Li",
      "Junhao Zhang",
      "Kunchang Li",
      "Shruti Vyas",
      "Yogesh S Rawat"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07993"
  },
  {
    "id": "arXiv:2110.07994",
    "title": "Pyramid Correlation based Deep Hough Voting for Visual Object Tracking",
    "abstract": "Most of the existing Siamese-based trackers treat tracking problem as a\nparallel task of classification and regression. However, some studies show that\nthe sibling head structure could lead to suboptimal solutions during the\nnetwork training. Through experiments we find that, without regression, the\nperformance could be equally promising as long as we delicately design the\nnetwork to suit the training objective. We introduce a novel voting-based\nclassification-only tracking algorithm named Pyramid Correlation based Deep\nHough Voting (short for PCDHV), to jointly locate the top-left and bottom-right\ncorners of the target. Specifically we innovatively construct a Pyramid\nCorrelation module to equip the embedded feature with fine-grained local\nstructures and global spatial contexts; The elaborately designed Deep Hough\nVoting module further take over, integrating long-range dependencies of pixels\nto perceive corners; In addition, the prevalent discretization gap is simply\nyet effectively alleviated by increasing the spatial resolution of the feature\nmaps while exploiting channel-space relationships. The algorithm is general,\nrobust and simple. We demonstrate the effectiveness of the module through a\nseries of ablation experiments. Without bells and whistles, our tracker\nachieves better or comparable performance to the SOTA algorithms on three\nchallenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a\nreal-time speed of 80 FPS. Codes and models will be released.",
    "descriptor": "\nComments: Accepted by ACML 2021 Conference Track (Short Oral)\n",
    "authors": [
      "Ying Wang",
      "Tingfa Xu",
      "Jianan Li",
      "Shenwang Jiang",
      "Junjie Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07994"
  },
  {
    "id": "arXiv:2110.08002",
    "title": "A posteriori estimates for the stochastic total variation flow",
    "abstract": "We derive a posteriori error estimates for a fully discrete time-implicit\nfinite element approximation of the stochastic total variaton flow (STVF) with\nadditive space time noise. The estimates are first derived for an implementable\nfully discrete approximation of a regularized stochastic total variation flow.\nWe then show that the derived a posteriori estimates remain valid for the\nunregularized flow up to a perturbation term that can be controlled by the\nregularization parameter. Based on the derived a posteriori estimates we\npropose a pathwise algorithm for the adaptive space-time refinement and perform\nnumerical simulation for the regularized STVF to demonstrate the behavior of\nthe proposed algorithm.",
    "descriptor": "",
    "authors": [
      "\u013dubom\u00edr Ba\u0148as",
      "Andr\u00e9 Wilke"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08002"
  },
  {
    "id": "arXiv:2110.08003",
    "title": "A Broad-persistent Advising Approach for Deep Interactive Reinforcement  Learning in Robotic Environments",
    "abstract": "Deep Reinforcement Learning (DeepRL) methods have been widely used in\nrobotics to learn about the environment and acquire behaviors autonomously.\nDeep Interactive Reinforcement Learning (DeepIRL) includes interactive feedback\nfrom an external trainer or expert giving advice to help learners choosing\nactions to speed up the learning process. However, current research has been\nlimited to interactions that offer actionable advice to only the current state\nof the agent. Additionally, the information is discarded by the agent after a\nsingle use that causes a duplicate process at the same state for a revisit. In\nthis paper, we present Broad-persistent Advising (BPA), a broad-persistent\nadvising approach that retains and reuses the processed information. It not\nonly helps trainers to give more general advice relevant to similar states\ninstead of only the current state but also allows the agent to speed up the\nlearning process. We test the proposed approach in two continuous robotic\nscenarios, namely, a cart pole balancing task and a simulated robot navigation\ntask. The obtained results show that the performance of the agent using BPA\nimproves while keeping the number of interactions required for the trainer in\ncomparison to the DeepIRL approach.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Hung Son Nguyen",
      "Francisco Cruz",
      "Richard Dazeley"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08003"
  },
  {
    "id": "arXiv:2110.08009",
    "title": "MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without  Retraining",
    "abstract": "Deep Generative Networks (DGNs) are extensively employed in Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and their\nvariants to approximate the data manifold, and data distribution on that\nmanifold. However, training samples are often obtained based on preferences,\ncosts, or convenience producing artifacts in the empirical data distribution\ne.g., the large fraction of smiling faces in the CelebA dataset or the large\nfraction of dark-haired individuals in FFHQ. These inconsistencies will be\nreproduced when sampling from the trained DGN, which has far-reaching potential\nimplications for fairness, data augmentation, anomaly detection, domain\nadaptation, and beyond. In response, we develop a differential geometry based\nsampler -- coined MaGNET -- that, given any trained DGN, produces samples that\nare uniformly distributed on the learned manifold. We prove theoretically and\nempirically that our technique produces a uniform distribution on the manifold\nregardless of the training set distribution. We perform a range of experiments\non various datasets and DGNs. One of them considers the state-of-the-art\nStyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases\ndistribution precision and recall by 4.1% & 3.0% and decreases gender bias by\n41.2%, without requiring labels or retraining.",
    "descriptor": "\nComments: 13 pages, 14 pages Appendix, 23 figures\n",
    "authors": [
      "Ahmed Imtiaz Humayun",
      "Randall Balestriero",
      "Richard Baraniuk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08009"
  },
  {
    "id": "arXiv:2110.08010",
    "title": "Transformer-based Multi-task Learning for Disaster Tweet Categorisation",
    "abstract": "Social media has enabled people to circulate information in a timely fashion,\nthus motivating people to post messages seeking help during crisis situations.\nThese messages can contribute to the situational awareness of emergency\nresponders, who have a need for them to be categorised according to information\ntypes (i.e. the type of aid services the messages are requesting). We introduce\na transformer-based multi-task learning (MTL) technique for classifying\ninformation types and estimating the priority of these messages. We evaluate\nthe effectiveness of our approach with a variety of metrics by submitting runs\nto the TREC Incident Streams (IS) track: a research initiative specifically\ndesigned for disaster tweet classification and prioritisation. The results\ndemonstrate that our approach achieves competitive performance in most metrics\nas compared to other participating runs. Subsequently, we find that an ensemble\napproach combining disparate transformer encoders within our approach helps to\nimprove the overall effectiveness to a significant extent, achieving\nstate-of-the-art performance in almost every metric. We make the code publicly\navailable so that our work can be reproduced and used as a baseline for the\ncommunity for future work in this domain.",
    "descriptor": "\nComments: 18th International Conference on Information Systems for Crisis Response and Management (ISCRAM 2021)\n",
    "authors": [
      "Congcong Wang",
      "Paul Nulty",
      "David Lillis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08010"
  },
  {
    "id": "arXiv:2110.08011",
    "title": "Modeling Proficiency with Implicit User Representations",
    "abstract": "We introduce the problem of proficiency modeling: Given a user's posts on a\nsocial media platform, the task is to identify the subset of posts or topics\nfor which the user has some level of proficiency. This enables the filtering\nand ranking of social media posts on a given topic as per user proficiency.\nUnlike experts on a given topic, proficient users may not have received formal\ntraining and possess years of practical experience, but may be autodidacts,\nhobbyists, and people with sustained interest, enabling them to make genuine\nand original contributions to discourse. While predicting whether a user is an\nexpert on a given topic imposes strong constraints on who is a true positive,\nproficiency modeling implies a graded scoring, relaxing these constraints. Put\nanother way, many active social media users can be assumed to possess, or\neventually acquire, some level of proficiency on topics relevant to their\ncommunity. We tackle proficiency modeling in an unsupervised manner by\nutilizing user embeddings to model engagement with a given topic, as indicated\nby a user's preference for authoring related content. We investigate five\nalternative approaches to model proficiency, ranging from basic ones to an\nadvanced, tailored user modeling approach, applied within two real-world\nbenchmarks for evaluation.",
    "descriptor": "",
    "authors": [
      "Kim Breitwieser",
      "Allison Lahnala",
      "Charles Welch",
      "Lucie Flek",
      "Martin Potthast"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08011"
  },
  {
    "id": "arXiv:2110.08012",
    "title": "A Survey on State-of-the-art Techniques for Knowledge Graphs  Construction and Challenges ahead",
    "abstract": "Global datasphere is increasing fast, and it is expected to reach 175\nZettabytes by 20251 . However, most of the content is unstructured and is not\nunderstandable by machines. Structuring this data into a knowledge graph\nenables multitudes of intelligent applications such as deep question answering,\nrecommendation systems, semantic search, etc. The knowledge graph is an\nemerging technology that allows logical reasoning and uncovers new insights\nusing content along with the context. Thereby, it provides necessary syntax and\nreasoning semantics that enable machines to solve complex healthcare, security,\nfinancial institutions, economics, and business problems. As an outcome,\nenterprises are putting their effort into constructing and maintaining\nknowledge graphs to support various downstream applications. Manual approaches\nare too expensive. Automated schemes can reduce the cost of building knowledge\ngraphs up to 15-250 times. This paper critiques state-of-the-art automated\ntechniques to produce knowledge graphs of near-human quality autonomously.\nAdditionally, it highlights different research issues that need to be addressed\nto deliver high-quality knowledge graphs",
    "descriptor": "",
    "authors": [
      "Ali Hur",
      "Naeem Janjua",
      "Mohiuddin Ahmed"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.08012"
  },
  {
    "id": "arXiv:2110.08013",
    "title": "Joint Channel and Weight Pruning for Model Acceleration on Moblie  Devices",
    "abstract": "For practical deep neural network design on mobile devices, it is essential\nto consider the constraints incurred by the computational resources and the\ninference latency in various applications. Among deep network acceleration\nrelated approaches, pruning is a widely adopted practice to balance the\ncomputational resource consumption and the accuracy, where unimportant\nconnections can be removed either channel-wisely or randomly with a minimal\nimpact on model accuracy. The channel pruning instantly results in a\nsignificant latency reduction, while the random weight pruning is more flexible\nto balance the latency and accuracy. In this paper, we present a unified\nframework with Joint Channel pruning and Weight pruning (JCW), and achieves a\nbetter Pareto-frontier between the latency and accuracy than previous model\ncompression approaches. To fully optimize the trade-off between the latency and\naccuracy, we develop a tailored multi-objective evolutionary algorithm in the\nJCW framework, which enables one single search to obtain the optimal candidate\narchitectures for various deployment requirements. Extensive experiments\ndemonstrate that the JCW achieves a better trade-off between the latency and\naccuracy against various state-of-the-art pruning methods on the ImageNet\nclassification dataset. Our codes are available at\nhttps://github.com/jcw-anonymous/JCW.",
    "descriptor": "\nComments: 23 pages, 6 figures\n",
    "authors": [
      "Tianli Zhao",
      "Xi Sheryl Zhang",
      "Wentao Zhu",
      "Jiaxing Wang",
      "Ji Liu",
      "Jian Cheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08013"
  },
  {
    "id": "arXiv:2110.08015",
    "title": "Crisis Domain Adaptation Using Sequence-to-sequence Transformers",
    "abstract": "User-generated content (UGC) on social media can act as a key source of\ninformation for emergency responders in crisis situations. However, due to the\nvolume concerned, computational techniques are needed to effectively filter and\nprioritise this content as it arises during emerging events. In the literature,\nthese techniques are trained using annotated content from previous crises. In\nthis paper, we investigate how this prior knowledge can be best leveraged for\nnew crises by examining the extent to which crisis events of a similar type are\nmore suitable for adaptation to new events (cross-domain adaptation). Given the\nrecent successes of transformers in various language processing tasks, we\npropose CAST: an approach for Crisis domain Adaptation leveraging\nSequence-to-sequence Transformers. We evaluate CAST using two major\ncrisis-related message classification datasets. Our experiments show that our\nCAST-based best run without using any target data achieves the state of the art\nperformance in both in-domain and cross-domain contexts. Moreover, CAST is\nparticularly effective in one-to-one cross-domain adaptation when trained with\na larger language model. In many-to-one adaptation where multiple crises are\njointly used as the source domain, CAST further improves its performance. In\naddition, we find that more similar events are more likely to bring better\nadaptation performance whereas fine-tuning using dissimilar events does not\nhelp for adaptation. To aid reproducibility, we open source our code to the\ncommunity.",
    "descriptor": "\nComments: 18th International Conference on Information Systems for Crisis Response and Management (ISCRAM 2021)\n",
    "authors": [
      "Congcong Wang",
      "Paul Nulty",
      "David Lillis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08015"
  },
  {
    "id": "arXiv:2110.08018",
    "title": "Structural Modeling for Dialogue Disentanglement",
    "abstract": "Tangled multi-party dialogue context leads to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within the\nsame dialogue history, thus increasing difficulties in understanding a dialogue\nhistory for both human and machine. Dialogue disentanglement aims to clarify\nconversation threads in a multi-party dialogue history, thus reducing the\ndifficulty of comprehending the long disordered dialogue passage. Existing\nstudies commonly focus on utterance encoding with carefully designed feature\nengineering-based methods but pay inadequate attention to dialogue structure.\nThis work designs a novel model to disentangle multi-party history into\nthreads, by taking dialogue structure features into account. Specifically,\nbased on the fact that dialogues are constructed through successive\nparticipation of speakers and interactions between users of interest, we\nextract clues of speaker property and reference of users to model the structure\nof a long dialogue record. The novel method is evaluated on the Ubuntu IRC\ndataset and shows state-of-the-art experimental results in dialogue\ndisentanglement.",
    "descriptor": "",
    "authors": [
      "Xinbei Ma",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08018"
  },
  {
    "id": "arXiv:2110.08019",
    "title": "Reachability-based Control Synthesis under Signal Temporal Logic  Specifications",
    "abstract": "In this paper, we investigate the controller design problem for linear\ndisturbed systems under signal temporal logic (STL) specifications imposing\nboth spatial and temporal constraints on system behavior. We first implement\nzonotope-based techniques to partition the state space into finite cells, then\npropose an evaluation mechanism to rearrange the time constraints of the STL\nspecification, and finally decompose the global STL formula into finite local\nSTL formulas. In this way, each cell has a local control design problem, which\nis further formulated into a local optimization problem. To deal with each\nlocal optimization problem, we take advantage of the properties of zonotopes\nand reachability analysis to design local controller consisting of feedforward\nand feedback parts. By solving all local optimization problems, all local\ncontrollers are combined to guarantee the global STL specification. Finally, a\nnumerical example is presented to illustrate the derived results.",
    "descriptor": "\nComments: 8 pages, 4 figures, sumitted to American Control Conference 2022\n",
    "authors": [
      "Wei Ren",
      "Raphael Jungers"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08019"
  },
  {
    "id": "arXiv:2110.08020",
    "title": "Multimodal Emotion-Cause Pair Extraction in Conversations",
    "abstract": "Emotion cause analysis has received considerable attention in recent years.\nPrevious studies primarily focused on emotion cause extraction from texts in\nnews articles or microblogs. It is also interesting to discover emotions and\ntheir causes in conversations. As conversation in its natural form is\nmultimodal, a large number of studies have been carried out on multimodal\nemotion recognition in conversations, but there is still a lack of work on\nmultimodal emotion cause analysis. In this work, we introduce a new task named\nMultimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly\nextract emotions and their associated causes from conversations reflected in\nmultiple modalities (text, audio and video). We accordingly construct a\nmultimodal conversational emotion cause dataset, Emotion-Cause-in-Friends,\nwhich contains 9,272 multimodal emotion-cause pairs annotated on 13,509\nutterances in the sitcom Friends. We finally benchmark the task by establishing\na baseline system that incorporates multimodal features for emotion-cause pair\nextraction. Preliminary experimental results demonstrate the potential of\nmultimodal information fusion for discovering both emotions and causes in\nconversations.",
    "descriptor": "",
    "authors": [
      "Fanfan Wang",
      "Zixiang Ding",
      "Rui Xia",
      "Zhaoyu Li",
      "Jianfei Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08020"
  },
  {
    "id": "arXiv:2110.08021",
    "title": "StreaMulT: Streaming Multimodal Transformer for Heterogeneous and  Arbitrary Long Sequential Data",
    "abstract": "This paper tackles the problem of processing and combining efficiently\narbitrary long data streams, coming from different modalities with different\nacquisition frequencies. Common applications can be, for instance, long-time\nindustrial or real-life systems monitoring from multimodal heterogeneous data\n(sensor data, monitoring report, images, etc.). To tackle this problem, we\npropose StreaMulT, a Streaming Multimodal Transformer, relying on cross-modal\nattention and an augmented memory bank to process arbitrary long input\nsequences at training time and run in a streaming way at inference. StreaMulT\nreproduces state-of-the-art results on CMU-MOSEI dataset, while being able to\ndeal with much longer inputs than other models such as previous Multimodal\nTransformer.",
    "descriptor": "\nComments: 5 pages, 4 figures, submitted to ICASSP 2022\n",
    "authors": [
      "Victor Pellegrain",
      "Myriam Tami",
      "Michel Batteux",
      "C\u00e9line Hudelot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.08021"
  },
  {
    "id": "arXiv:2110.08024",
    "title": "New applications for the Boris Spectral Deferred Correction algorithm  for plasma simulations",
    "abstract": "The paper investigates two new use cases for the Boris Spectral Deferred\nCorrections (Boris-SDC) time integrator for plasma simulations. First, we show\nthat using Boris-SDC as a particle pusher in an electrostatic particle-in-cell\n(PIC) code can, at least in the linear regime, improve simulation accuracy\ncompared with the standard second order Boris method. In some instances, the\nhigher order of Boris-SDC even allows a much larger time step, leading to\nmodest computational gains. Second, we propose a modification of Boris-SDC for\nthe relativistic regime. Based on an implementation of Boris-SDC in the\n\\textsc{runko} PIC code, we demonstrate for a relativistic Penning trap that\nBoris-SDC retains its high order of convergence for velocities ranging from\n$0.5c$ to $>0.99c$. We also show that for the force-free case where\nacceleration from electric and magnetic field cancel, Boris-SDC produces less\nnumerical drift than Boris.",
    "descriptor": "",
    "authors": [
      "Kris Smedt",
      "Daniel Ruprecht",
      "Jitse Niesen",
      "Steven Tobias",
      "Joonas N\u00e4ttil\u00e4"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08024"
  },
  {
    "id": "arXiv:2110.08028",
    "title": "Improving Hyperparameter Optimization by Planning Ahead",
    "abstract": "Hyperparameter optimization (HPO) is generally treated as a bi-level\noptimization problem that involves fitting a (probabilistic) surrogate model to\na set of observed hyperparameter responses, e.g. validation loss, and\nconsequently maximizing an acquisition function using a surrogate model to\nidentify good hyperparameter candidates for evaluation. The choice of a\nsurrogate and/or acquisition function can be further improved via knowledge\ntransfer across related tasks. In this paper, we propose a novel transfer\nlearning approach, defined within the context of model-based reinforcement\nlearning, where we represent the surrogate as an ensemble of probabilistic\nmodels that allows trajectory sampling. We further propose a new variant of\nmodel predictive control which employs a simple look-ahead strategy as a policy\nthat optimizes a sequence of actions, representing hyperparameter candidates to\nexpedite HPO. Our experiments on three meta-datasets comparing to\nstate-of-the-art HPO algorithms including a model-free reinforcement learning\napproach show that the proposed method can outperform all baselines by\nexploiting a simple planning-based policy.",
    "descriptor": "",
    "authors": [
      "Hadi S. Jomaa",
      "Jonas Falkner",
      "Lars Schmidt-Thieme"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08028"
  },
  {
    "id": "arXiv:2110.08030",
    "title": "Identifying Incorrect Classifications with Balanced Uncertainty",
    "abstract": "Uncertainty estimation is critical for cost-sensitive deep-learning\napplications (i.e. disease diagnosis). It is very challenging partly due to the\ninaccessibility of uncertainty groundtruth in most datasets. Previous works\nproposed to estimate the uncertainty from softmax calibration, Monte Carlo\nsampling, subjective logic and so on. However, these existing methods tend to\nbe over-confident about their predictions with unreasonably low overall\nuncertainty, which originates from the imbalance between positive (correct\nclassifications) and negative (incorrect classifications) samples. For this\nissue, we firstly propose the distributional imbalance to model the imbalance\nin uncertainty estimation as two kinds of distribution biases, and secondly\npropose Balanced True Class Probability (BTCP) framework, which learns an\nuncertainty estimator with a novel Distributional Focal Loss (DFL) objective.\nFinally, we evaluate the BTCP in terms of failure prediction and\nout-of-distribution (OOD) detection on multiple datasets. The experimental\nresults show that BTCP outperforms other uncertainty estimation methods\nespecially in identifying incorrect classifications.",
    "descriptor": "",
    "authors": [
      "Bolian Li",
      "Zige Zheng",
      "Changqing Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08030"
  },
  {
    "id": "arXiv:2110.08032",
    "title": "UniDS: A Unified Dialogue System for Chit-Chat and Task-oriented  Dialogues",
    "abstract": "With the advances in deep learning, tremendous progress has been made with\nchit-chat dialogue systems and task-oriented dialogue systems. However, these\ntwo systems are often tackled separately in current methods. To achieve more\nnatural interaction with humans, a dialogue agent needs to be capable of both\nchatting and accomplishing tasks. To this end, we propose a unified dialogue\nsystem (UniDS) with the two aforementioned skills. In particular, we design a\nunified dialogue data schema, compatible for both chit-chat and task-oriented\ndialogues, and we train UniDS with mixed dialogue data from a pretrained\nchit-chat dialogue model. Without adding extra parameters to SOTA baselines,\nUniDS can alternatively handle chit-chat and task-oriented dialogues in a\nunified framework. Experimental results demonstrate that the proposed UniDS\nworks comparably well as the pure chit-chat system, and it outperforms\nstate-of-the-art task-oriented dialogue systems. More importantly, UniDS\nachieves better robustness as it is able to smoothly switch between two types\nof dialogues. These results demonstrate the feasibility and potential of\nbuilding an one-for-all dialogue system.",
    "descriptor": "",
    "authors": [
      "Xinyan Zhao",
      "Bin He",
      "Yasheng Wang",
      "Yitong Li",
      "Fei Mi",
      "Yajiao Liu",
      "Xin Jiang",
      "Qun Liu",
      "Huanhuan Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08032"
  },
  {
    "id": "arXiv:2110.08033",
    "title": "Benchmark Problems for CEC2021 Competition on Evolutionary Transfer  Multiobjectve Optimization",
    "abstract": "Evolutionary transfer multiobjective optimization (ETMO) has been becoming a\nhot research topic in the field of evolutionary computation, which is based on\nthe fact that knowledge learning and transfer across the related optimization\nexercises can improve the efficiency of others. Besides, the potential for\ntransfer optimization is deemed invaluable from the standpoint of human-like\nproblem-solving capabilities where knowledge gather and reuse are instinctive.\nTo promote the research on ETMO, benchmark problems are of great importance to\nETMO algorithm analysis, which helps designers or practitioners to understand\nthe merit and demerit better of ETMO algorithms. Therefore, a total number of\n40 benchmark functions are proposed in this report, covering diverse types and\nproperties in the case of knowledge transfer, such as various formulation\nmodels, various PS geometries and PF shapes, large-scale of variables,\ndynamically changed environment, and so on. All the benchmark functions have\nbeen implemented in JAVA code, which can be downloaded on the following\nwebsite: https://github.com/songbai-liu/etmo.",
    "descriptor": "\nComments: 20 pages, 1 figure, technical report for competition\n",
    "authors": [
      "Songbai Liu",
      "Qiuzhen Lin",
      "Kay Chen Tan",
      "Qing Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.08033"
  },
  {
    "id": "arXiv:2110.08036",
    "title": "Generating Natural Language Adversarial Examples through An Improved  Beam Search Algorithm",
    "abstract": "The research of adversarial attacks in the text domain attracts many\ninterests in the last few years, and many methods with a high attack success\nrate have been proposed. However, these attack methods are inefficient as they\nrequire lots of queries for the victim model when crafting text adversarial\nexamples. In this paper, a novel attack model is proposed, its attack success\nrate surpasses the benchmark attack methods, but more importantly, its attack\nefficiency is much higher than the benchmark attack methods. The novel method\nis empirically evaluated by attacking WordCNN, LSTM, BiLSTM, and BERT on four\nbenchmark datasets. For instance, it achieves a 100\\% attack success rate\nhigher than the state-of-the-art method when attacking BERT and BiLSTM on IMDB,\nbut the number of queries for the victim models only is 1/4 and 1/6.5 of the\nstate-of-the-art method, respectively. Also, further experiments show the novel\nmethod has a good transferability on the generated adversarial examples.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Tengfei Zhao",
      "Zhaocheng Ge",
      "Hanping Hu",
      "Dingmeng Shi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08036"
  },
  {
    "id": "arXiv:2110.08037",
    "title": "Tensor-to-Image: Image-to-Image Translation with Vision Transformers",
    "abstract": "Transformers gain huge attention since they are first introduced and have a\nwide range of applications. Transformers start to take over all areas of deep\nlearning and the Vision transformers paper also proved that they can be used\nfor computer vision tasks. In this paper, we utilized a vision\ntransformer-based custom-designed model, tensor-to-image, for the image to\nimage translation. With the help of self-attention, our model was able to\ngeneralize and apply to different problems without a single modification.",
    "descriptor": "",
    "authors": [
      "Yi\u011fit G\u00fcnd\u00fc\u00e7"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08037"
  },
  {
    "id": "arXiv:2110.08038",
    "title": "Toward Annotator Group Bias in Crowdsourcing",
    "abstract": "Crowdsourcing has emerged as a popular approach for collecting annotated data\nto train supervised machine learning models. However, annotator bias can lead\nto defective annotations. Though there are a few works investigating individual\nannotator bias, the group effects in annotators are largely overlooked. In this\nwork, we reveal that annotators within the same demographic group tend to show\nconsistent group bias in annotation tasks and thus we conduct an initial study\non annotator group bias. We first empirically verify the existence of annotator\ngroup bias in various real-world crowdsourcing datasets. Then, we develop a\nnovel probabilistic graphical framework GroupAnno to capture annotator group\nbias with a new extended Expectation Maximization (EM) training algorithm. We\nconduct experiments on both synthetic and real-world datasets. Experimental\nresults demonstrate the effectiveness of our model in modeling annotator group\nbias in label aggregation and model learning over competitive baselines.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Haochen Liu",
      "Joseph Thekinen",
      "Sinem Mollaoglu",
      "Da Tang",
      "Ji Yang",
      "Youlong Cheng",
      "Hui Liu",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08038"
  },
  {
    "id": "arXiv:2110.08042",
    "title": "Adversarial Attacks on ML Defense Models Competition",
    "abstract": "Due to the vulnerability of deep neural networks (DNNs) to adversarial\nexamples, a large number of defense techniques have been proposed to alleviate\nthis problem in recent years. However, the progress of building more robust\nmodels is usually hampered by the incomplete or incorrect robustness\nevaluation. To accelerate the research on reliable evaluation of adversarial\nrobustness of the current defense models in image classification, the TSAIL\ngroup at Tsinghua University and the Alibaba Security group organized this\ncompetition along with a CVPR 2021 workshop on adversarial machine learning\n(https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this\ncompetition is to motivate novel attack algorithms to evaluate adversarial\nrobustness more effectively and reliably. The participants were encouraged to\ndevelop stronger white-box attack algorithms to find the worst-case robustness\nof different defenses. This competition was conducted on an adversarial\nrobustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is\nheld on the TianChi platform\n(https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of\nthe series of AI Security Challengers Program. After the competition, we\nsummarized the results and established a new adversarial robustness benchmark\nat https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload\nadversarial attack algorithms and defense models for evaluation.",
    "descriptor": "\nComments: Competition Report\n",
    "authors": [
      "Yinpeng Dong",
      "Qi-An Fu",
      "Xiao Yang",
      "Wenzhao Xiang",
      "Tianyu Pang",
      "Hang Su",
      "Jun Zhu",
      "Jiayu Tang",
      "Yuefeng Chen",
      "XiaoFeng Mao",
      "Yuan He",
      "Hui Xue",
      "Chao Li",
      "Ye Liu",
      "Qilong Zhang",
      "Lianli Gao",
      "Yunrui Yu",
      "Xitong Gao",
      "Zhe Zhao",
      "Daquan Lin",
      "Jiadong Lin",
      "Chuanbiao Song",
      "Zihao Wang",
      "Zhennan Wu",
      "Yang Guo",
      "Jiequan Cui",
      "Xiaogang Xu",
      "Pengguang Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08042"
  },
  {
    "id": "arXiv:2110.08049",
    "title": "Learning Semantics: An Opportunity for Effective 6G Communications",
    "abstract": "Recently, semantic communications are envisioned as a key enabler of future\n6G networks. Back to Shannon's information theory, the goal of communication\nhas long been to guarantee the correct reception of transmitted messages\nirrespective of their meaning. However, in general, whenever communication\noccurs to convey a meaning, what matters is the receiver's understanding of the\ntransmitted message and not necessarily its correct reconstruction. Hence,\nsemantic communications introduce a new paradigm: transmitting only relevant\ninformation sufficient for the receiver to capture the meaning intended can\nsave significant communication bandwidth. Thus, this work explores the\nopportunity offered by semantic communications for beyond 5G networks. In\nparticular, we focus on the benefit of semantic compression. We refer to\nsemantic message as a sequence of well-formed symbols learned from the\n\"meaning\" underlying data, which have to be interpreted at the receiver. This\nrequires a reasoning unit, here artificial, on a knowledge base: a symbolic\nknowledge representation of the specific application. Therefore, we present and\ndetail a novel architecture that enables representation learning of semantic\nsymbols for effective semantic communications. We first discuss theoretical\naspects and successfully design objective functions, which help learn effective\nsemantic encoders and decoders. Eventually, we show promising numerical results\nfor the scenario of text transmission, especially when the sender and receiver\nspeak different languages.",
    "descriptor": "\nComments: Accepted for publication at IEEE CCNC 2021\n",
    "authors": [
      "Mohamed Sana",
      "Emilio Calvanese Strinati"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.08049"
  },
  {
    "id": "arXiv:2110.08054",
    "title": "Formation control of a leader-follower structure in three dimensional  space using bearing measurements",
    "abstract": "This paper addresses the problem of bearing leader-follower formation control\nin three-dimensional space by exploring the persistence of excitation (PE) of\nthe desired formation. Using only bearing and relative velocity measurements,\ndistributed control laws are derived for a group of agents with\ndouble-integrator dynamics. The key contribution is that the exponential\nstabilization of the actual formation to the desired one in terms of both shape\nand scale is guaranteed as long as the PE conditions on the desired formation\nare satisfied. The approach generalizes stability results provided in prior\nwork for leader-first follower (LFF) structures which are based on bearing\nrigidity and constraint consistency to ensure the exponential stabilization of\nthe actual formation to a desired static geometric pattern up to a scale\nfactor. Simulations results are provided to illustrate the performance of the\nproposed control method.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2103.06024, arXiv:2009.00209\n",
    "authors": [
      "Zhiqi Tang",
      "Rita Cunha",
      "Tarek Hamel",
      "Carlos Silvestre"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08054"
  },
  {
    "id": "arXiv:2110.08057",
    "title": "Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits",
    "abstract": "We study the optimal batch-regret tradeoff for batch linear contextual\nbandits. For any batch number $M$, number of actions $K$, time horizon $T$, and\ndimension $d$, we provide an algorithm and prove its regret guarantee, which,\ndue to technical reasons, features a two-phase expression as the time horizon\n$T$ grows. We also prove a lower bound theorem that surprisingly shows the\noptimality of our two-phase regret upper bound (up to logarithmic factors) in\nthe \\emph{full range} of the problem parameters, therefore establishing the\nexact batch-regret tradeoff.\nCompared to the recent work \\citep{ruan2020linear} which showed that $M =\nO(\\log \\log T)$ batches suffice to achieve the asymptotically minimax-optimal\nregret without the batch constraints, our algorithm is simpler and easier for\npractical implementation. Furthermore, our algorithm achieves the optimal\nregret for all $T \\geq d$, while \\citep{ruan2020linear} requires that $T$\ngreater than an unrealistically large polynomial of $d$.\nAlong our analysis, we also prove a new matrix concentration inequality with\ndependence on their dynamic upper bounds, which, to the best of our knowledge,\nis the first of its kind in literature and maybe of independent interest.",
    "descriptor": "",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "Yuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08057"
  },
  {
    "id": "arXiv:2110.08058",
    "title": "Detecting Modularity in Deep Neural Networks",
    "abstract": "A neural network is modular to the extent that parts of its computational\ngraph (i.e. structure) can be represented as performing some comprehensible\nsubtask relevant to the overall task (i.e. functionality). Are modern deep\nneural networks modular? How can this be quantified? In this paper, we consider\nthe problem of assessing the modularity exhibited by a partitioning of a\nnetwork's neurons. We propose two proxies for this: importance, which reflects\nhow crucial sets of neurons are to network performance; and coherence, which\nreflects how consistently their neurons associate with features of the inputs.\nTo measure these proxies, we develop a set of statistical methods based on\ntechniques conventionally used to interpret individual neurons. We apply the\nproxies to partitionings generated by spectrally clustering a graph\nrepresentation of the network's neurons with edges determined either by network\nweights or correlations of activations. We show that these partitionings, even\nones based only on weights (i.e. strictly from non-runtime analysis), reveal\ngroups of neurons that are important and coherent. These results suggest that\ngraph-based partitioning can reveal modularity and help us understand how deep\nneural networks function.",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Shlomi Hod",
      "Stephen Casper",
      "Daniel Filan",
      "Cody Wild",
      "Andrew Critch",
      "Stuart Russell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.08058"
  },
  {
    "id": "arXiv:2110.08059",
    "title": "FlexConv: Continuous Kernel Convolutions with Differentiable Kernel  Sizes",
    "abstract": "When designing Convolutional Neural Networks (CNNs), one must select the size\nof the convolutional kernels before training. Recent works show CNNs benefit\nfrom different kernel sizes at different layers, but exploring all possible\ncombinations is unfeasible in practice. A more efficient approach is to learn\nthe kernel size during training. However, existing works that learn the kernel\nsize have a limited bandwidth. These approaches scale kernels by dilation, and\nthus the detail they can describe is limited. In this work, we propose\nFlexConv, a novel convolutional operation with which high bandwidth\nconvolutional kernels of learnable kernel size can be learned at a fixed\nparameter cost. FlexNets model long-term dependencies without the use of\npooling, achieve state-of-the-art performance on several sequential datasets,\noutperform recent works with learned kernel sizes, and are competitive with\nmuch deeper ResNets on image benchmark datasets. Additionally, FlexNets can be\ndeployed at higher resolutions than those seen during training. To avoid\naliasing, we propose a novel kernel parameterization with which the frequency\nof the kernels can be analytically controlled. Our novel kernel\nparameterization shows higher descriptive power and faster convergence speed\nthan existing parameterizations. This leads to important improvements in\nclassification accuracy.",
    "descriptor": "\nComments: First two authors contributed equally to this work\n",
    "authors": [
      "David W. Romero",
      "Robert-Jan Bruintjes",
      "Jakub M. Tomczak",
      "Erik J. Bekkers",
      "Mark Hoogendoorn",
      "Jan C. van Gemert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08059"
  },
  {
    "id": "arXiv:2110.08062",
    "title": "Cooperative Localization in Massive Networks",
    "abstract": "Network localization is capable of providing accurate and ubiquitous position\ninformation for numerous wireless applications. This paper studies the accuracy\nof cooperative network localization in large-scale wireless networks. Based on\na decomposition of the equivalent Fisher information matrix (EFIM), we develop\na random-walk-inspired approach for the analysis of EFIM, and propose a\nposition information routing interpretation of cooperative network\nlocalization. Using this approach, we show that in large lattice and stochastic\ngeometric networks, when anchors are uniformly distributed, the average\nlocalization error of agents grows logarithmically with the reciprocal of\nanchor density in an asymptotic regime. The results are further illustrated\nusing numerical examples.",
    "descriptor": "",
    "authors": [
      "Yifeng Xiong",
      "Nan Wu",
      "Yuan Shen",
      "Moe Z. Win"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.08062"
  },
  {
    "id": "arXiv:2110.08063",
    "title": "Reliable Shot Identification for Complex Event Detection via  Visual-Semantic Embedding",
    "abstract": "Multimedia event detection is the task of detecting a specific event of\ninterest in an user-generated video on websites. The most fundamental challenge\nfacing this task lies in the enormously varying quality of the video as well as\nthe high-level semantic abstraction of event inherently. In this paper, we\ndecompose the video into several segments and intuitively model the task of\ncomplex event detection as a multiple instance learning problem by representing\neach video as a \"bag\" of segments in which each segment is referred to as an\ninstance. Instead of treating the instances equally, we associate each instance\nwith a reliability variable to indicate its importance and then select reliable\ninstances for training. To measure the reliability of the varying instances\nprecisely, we propose a visual-semantic guided loss by exploiting low-level\nfeature from visual information together with instance-event similarity based\nhigh-level semantic feature. Motivated by curriculum learning, we introduce a\nnegative elastic-net regularization term to start training the classifier with\ninstances of high reliability and gradually taking the instances with\nrelatively low reliability into consideration. An alternative optimization\nalgorithm is developed to solve the proposed challenging non-convex non-smooth\nproblem. Experimental results on standard datasets, i.e., TRECVID MEDTest 2013\nand TRECVID MEDTest 2014, demonstrate the effectiveness and superiority of the\nproposed method to the baseline algorithms.",
    "descriptor": "\nComments: 11 pages, accepted by CVIU\n",
    "authors": [
      "Minnan Luo",
      "Xiaojun Chang",
      "Chen Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08063"
  },
  {
    "id": "arXiv:2110.08066",
    "title": "Dual-Arm Adversarial Robot Learning",
    "abstract": "Robot learning is a very promising topic for the future of automation and\nmachine intelligence. Future robots should be able to autonomously acquire\nskills, learn to represent their environment, and interact with it. While these\ntopics have been explored in simulation, real-world robot learning research\nseems to be still limited. This is due to the additional challenges encountered\nin the real-world, such as noisy sensors and actuators, safe exploration,\nnon-stationary dynamics, autonomous environment resetting as well as the cost\nof running experiments for long periods of time. Unless we develop scalable\nsolutions to these problems, learning complex tasks involving hand-eye\ncoordination and rich contacts will remain an untouched vision that is only\nfeasible in controlled lab environments. We propose dual-arm settings as\nplatforms for robot learning. Such settings enable safe data collection for\nacquiring manipulation skills as well as training perception modules in a\nrobot-supervised manner. They also ease the processes of resetting the\nenvironment. Furthermore, adversarial learning could potentially boost the\ngeneralization capability of robot learning methods by maximizing the\nexploration based on game-theoretic objectives while ensuring safety based on\ncollaborative task spaces. In this paper, we will discuss the potential\nbenefits of this setup as well as the challenges and research directions that\ncan be pursued.",
    "descriptor": "\nComments: Accepted at CoRL 2021, Blue Sky Track\n",
    "authors": [
      "Elie Aljalbout"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08066"
  },
  {
    "id": "arXiv:2110.08067",
    "title": "Chaotic Fitness Dependent Optimizer for Planning and Engineering Design",
    "abstract": "Fitness Dependent Optimizer (FDO) is a recent metaheuristic algorithm that\nmimics the reproduction behavior of the bee swarm in finding better hives. This\nalgorithm is similar to Particle Swarm Optimization (PSO) but it works\ndifferently. The algorithm is very powerful and has better results compared to\nother common metaheuristic algorithms. This paper aims at improving the\nperformance of FDO, thus, the chaotic theory is used inside FDO to propose\nChaotic FDO (CFDO). Ten chaotic maps are used in the CFDO to consider which of\nthem are performing well to avoid local optima and finding global optima. New\ntechnic is used to conduct population in specific limitation since FDO technic\nhas a problem to amend population. The proposed CFDO is evaluated by using 10\nbenchmark functions from CEC2019. Finally, the results show that the ability of\nCFDO is improved. Singer map has a great impact on improving CFDO while the\nTent map is the worst. Results show that CFDO is superior to GA, FDO, and CSO.\nBoth CEC2013 and CEC2005 are used to evaluate CFDO. Finally, the proposed CFDO\nis applied to classical engineering problems, such as pressure vessel design\nand the result shows that CFDO can handle the problem better than WOA, GWO,\nFDO, and CGWO. Besides, CFDO is applied to solve the task assignment problem\nand then compared to the original FDO. The results prove that CFDO has better\ncapability to solve the problem.",
    "descriptor": "",
    "authors": [
      "Hardi M. Mohammed",
      "Tarik A. Rashid"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.08067"
  },
  {
    "id": "arXiv:2110.08068",
    "title": "SAT Encodings for Pseudo-Boolean Constraints Together With At-Most-One  Constraints",
    "abstract": "When solving a combinatorial problem using propositional satisfiability\n(SAT), the encoding of the problem is of vital importance. We study encodings\nof Pseudo-Boolean (PB) constraints, a common type of arithmetic constraint that\nappears in a wide variety of combinatorial problems such as timetabling,\nscheduling, and resource allocation. In some cases PB constraints occur\ntogether with at-most-one (AMO) constraints over subsets of their variables\n(forming PB(AMO) constraints). Recent work has shown that taking account of\nAMOs when encoding PB constraints using decision diagrams can produce a\ndramatic improvement in solver efficiency. In this paper we extend the approach\nto other state-of-the-art encodings of PB constraints, developing several new\nencodings for PB(AMO) constraints. Also, we present a more compact and\nefficient version of the popular Generalized Totalizer encoding, named Reduced\nGeneralized Totalizer. This new encoding is also adapted for PB(AMO)\nconstraints for a further gain. Our experiments show that the encodings of\nPB(AMO) constraints can be substantially smaller than those of PB constraints.\nPB(AMO) encodings allow many more instances to be solved within a time limit,\nand solving time is improved by more than one order of magnitude in some cases.\nWe also observed that there is no single overall winner among the considered\nencodings, but efficiency of each encoding may depend on PB(AMO)\ncharacteristics such as the magnitude of coefficient values.",
    "descriptor": "",
    "authors": [
      "Miquel Bofill",
      "Jordi Coll",
      "Peter Nightingale",
      "Josep Suy",
      "Felix Ulrich-Oltean",
      "Mateu Villaret"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08068"
  },
  {
    "id": "arXiv:2110.08079",
    "title": "Automated Quality Control of Vacuum Insulated Glazing by Convolutional  Neural Network Image Classification",
    "abstract": "Vacuum Insulated Glazing (VIG) is a highly thermally insulating window\ntechnology, which boasts an extremely thin profile and lower weight as compared\nto gas-filled insulated glazing units of equivalent performance. The VIG is a\ndouble-pane configuration with a submillimeter vacuum gap between the panes and\ntherefore under constant atmospheric pressure over their service life. Small\npillars are positioned between the panes to maintain the gap, which can damage\nthe glass reducing the lifetime of the VIG unit. To efficiently assess any\nsurface damage on the glass, an automated damage detection system is highly\ndesirable. For the purpose of classifying the damage, we have developed,\ntrained, and tested a deep learning computer vision system using convolutional\nneural networks. The classification model flawlessly classified the test\ndataset with an area under the curve (AUC) for the receiver operating\ncharacteristic (ROC) of 100%. We have automatically cropped the images down to\ntheir relevant information by using Faster-RCNN to locate the position of the\npillars. We employ the state-of-the-art methods Grad-CAM and Score-CAM of\nexplainable Artificial Intelligence (XAI) to provide an understanding of the\ninternal mechanisms and were able to show that our classifier outperforms\nResNet50V2 for identification of crack locations and geometry. The proposed\nmethods can therefore be used to detect systematic defects even without large\namounts of training data. Further analyses of our model's predictive\ncapabilities demonstrates its superiority over state-of-the-art models\n(ResNet50V2, ResNet101V2 and ResNet152V2) in terms of convergence speed,\naccuracy, precision at 100% recall and AUC for ROC.",
    "descriptor": "\nComments: 10 pages, 11 figures, 1 table\n",
    "authors": [
      "Henrik Riedel",
      "Sleheddine Mokdad",
      "Isabell Schulz",
      "Cenk Kocer",
      "Philipp Rosendahl",
      "Jens Schneider",
      "Michael A. Kraus",
      "Michael Drass"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08079"
  },
  {
    "id": "arXiv:2110.08084",
    "title": "Gradient Descent on Infinitely Wide Neural Networks: Global Convergence  and Generalization",
    "abstract": "Many supervised machine learning methods are naturally cast as optimization\nproblems. For prediction models which are linear in their parameters, this\noften leads to convex problems for which many mathematical guarantees exist.\nModels which are non-linear in their parameters such as neural networks lead to\nnon-convex optimization problems for which guarantees are harder to obtain. In\nthis review paper, we consider two-layer neural networks with homogeneous\nactivation functions where the number of hidden neurons tends to infinity, and\nshow how qualitative convergence guarantees may be derived.",
    "descriptor": "",
    "authors": [
      "Francis Bach",
      "Lena\u00efc Chizat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.08084"
  },
  {
    "id": "arXiv:2110.08090",
    "title": "Using DeepProbLog to perform Complex Event Processing on an Audio Stream",
    "abstract": "In this paper, we present an approach to Complex Event Processing (CEP) that\nis based on DeepProbLog. This approach has the following objectives: (i)\nallowing the use of subsymbolic data as an input, (ii) retaining the\nflexibility and modularity on the definitions of complex event rules, (iii)\nallowing the system to be trained in an end-to-end manner and (iv) being robust\nagainst noisily labelled data. Our approach makes use of DeepProbLog to create\na neuro-symbolic architecture that combines a neural network to process the\nsubsymbolic data with a probabilistic logic layer to allow the user to define\nthe rules for the complex events. We demonstrate that our approach is capable\nof detecting complex events from an audio stream. We also demonstrate that our\napproach is capable of training even with a dataset that has a moderate\nproportion of noisy data.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Marc Roig Vilamala",
      "Tianwei Xing",
      "Harrison Taylor",
      "Luis Garcia",
      "Mani Srivastava",
      "Lance Kaplan",
      "Alun Preece",
      "Angelika Kimmig",
      "Federico Cerutti"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08090"
  },
  {
    "id": "arXiv:2110.08092",
    "title": "Equivariant and Invariant Reynolds Networks",
    "abstract": "Invariant and equivariant networks are useful in learning data with symmetry,\nincluding images, sets, point clouds, and graphs. In this paper, we consider\ninvariant and equivariant networks for symmetries of finite groups. Invariant\nand equivariant networks have been constructed by various researchers using\nReynolds operators. However, Reynolds operators are computationally expensive\nwhen the order of the group is large because they use the sum over the whole\ngroup, which poses an implementation difficulty. To overcome this difficulty,\nwe consider representing the Reynolds operator as a sum over a subset instead\nof a sum over the whole group. We call such a subset a Reynolds design, and an\noperator defined by a sum over a Reynolds design a reductive Reynolds operator.\nFor example, in the case of a graph with $n$ nodes, the computational\ncomplexity of the reductive Reynolds operator is reduced to $O(n^2)$, while the\ncomputational complexity of the Reynolds operator is $O(n!)$. We construct\nlearning models based on the reductive Reynolds operator called equivariant and\ninvariant Reynolds networks (ReyNets) and prove that they have universal\napproximation property. Reynolds designs for equivariant ReyNets are derived\nfrom combinatorial observations with Young diagrams, while Reynolds designs for\ninvariant ReyNets are derived from invariants called Reynolds dimensions\ndefined on the set of invariant polynomials. Numerical experiments show that\nthe performance of our models is comparable to state-of-the-art methods.",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Akiyoshi Sannai",
      "Makoto Kawano",
      "Wataru Kumagai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08092"
  },
  {
    "id": "arXiv:2110.08094",
    "title": "Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation  for Open-Domain Dialogue",
    "abstract": "One challenge with open-domain dialogue systems is the need to produce\nhigh-quality responses on any topic. We aim to improve the quality and coverage\nof Athena, an Alexa Prize dialogue system. We utilize Athena's response\ngenerators (RGs) to create training data for two new neural Meaning-to-Text\nRGs, Athena-GPT-Neo and Athena-Jurassic, for the movies, music, TV, sports, and\nvideo game domains. We conduct few-shot experiments, both within and\ncross-domain, with different tuning set sizes (2, 3, 10), prompt formats, and\nmeaning representations (MRs) for sets of WikiData KG triples, and dialogue\nacts with 14 possible attribute combinations. Our evaluation uses BLEURT and\nhuman evaluation metrics, and shows that with 10-shot tuning, Athena-Jurassic's\nperformance is significantly better for coherence and semantic accuracy.\nExperiments with 2-shot tuning on completely novel MRs results in a huge\nperformance drop for Athena-GPT-Neo, whose semantic accuracy falls to 0.41, and\nwhose untrue hallucination rate increases to 12%. Experiments with dialogue\nacts for video games show that with 10-shot tuning, both models learn to\ncontrol dialogue acts, but Athena-Jurassic has significantly higher coherence,\nand only 4% untrue hallucinations. Our results suggest that Athena-Jurassic can\nreliably produce outputs of high-quality for live systems with real users. To\nour knowledge, these are the first results demonstrating that few-shot tuning\non a massive language model can create NLGs that generalize to new domains, and\nproduce high-quality, semantically-controlled, conversational responses\ndirectly from MRs and KG triples.",
    "descriptor": "\nComments: The 12th International Workshop on Spoken Dialog System Technology, IWSDS 2021\n",
    "authors": [
      "Lena Reed",
      "Cecilia Li",
      "Angela Ramirez",
      "Liren Wu",
      "Marilyn Walker"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08094"
  },
  {
    "id": "arXiv:2110.08100",
    "title": "A Survey of Evolutionary Multi-Objective Clustering Approaches",
    "abstract": "This article presents how the studies of the evolutionary multi-objective\nclustering have been evolving over the years, based on a mapping of the indexed\narticles in the ACM, IEEE, and Scopus. We present the most relevant approaches\nconsidering the high impact journals and conferences to provide an overview of\nthis study field. We analyzed the algorithms based on the features and\ncomponents presented in the proposed general architecture of the evolutionary\nmulti-objective clustering. These algorithms were grouped considering common\nclustering strategies and applications. Furthermore, issues regarding the\ndifficulty in defining appropriate clustering criteria applied to evolutionary\nmulti-objective clustering and the importance of the evolutionary process\nevaluation to have a clear view of the optimization efficiency are discussed.\nIt is essential to observe these aspects besides specific clustering properties\nwhen designing new approaches or selecting/using the existing ones. Finally, we\npresent other potential subjects of future research, in which this article can\ncontribute to newcomers or busy researchers who want to have a wide vision of\nthe field.",
    "descriptor": "\nComments: Submitted to ACM Computing Surveys\n",
    "authors": [
      "Cristina Y. Morimoto",
      "Aurora Pozo",
      "Marc\u00edlio C. P. de Souto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08100"
  },
  {
    "id": "arXiv:2110.08101",
    "title": "An Artificial Neural Network-Based Model Predictive Control for  Three-phase Flying Capacitor Multi-Level Inverter",
    "abstract": "Model predictive control (MPC) has been used widely in power electronics due\nto its simple concept, fast dynamic response, and good reference tracking.\nHowever, it suffers from parametric uncertainties, since it directly relies on\nthe mathematical model of the system to predict the optimal switching states to\nbe used at the next sampling time. As a result, uncertain parameters lead to an\nill-designed MPC. Thus, this paper offers a model-free control strategy on the\nbasis of artificial neural networks (ANNs), for mitigating the effects of\nparameter mismatching while having a little negative impact on the inverter's\nperformance. This method includes two related stages. First, MPC is used as an\nexpert to control the studied converter in order to provide the training data;\nwhile, in the second stage, the obtained dataset is utilized to train the\nproposed ANN which will be used directly to control the inverter without the\nrequirement for the mathematical model of the system. The case study herein is\nbased on a four-level three-cell flying capacitor inverter. In this study,\nMATLAB/Simulink is used to simulate the performance of the proposed control\nstrategy, taking into account various operating conditions. Afterward, the\nsimulation results are reported in comparison with the conventional MPC scheme,\ndemonstrating the superior performance of the proposed control strategy in\nterms of getting low total harmonic distortion (THD) and the robustness against\nparameters mismatch, especially when changes occur in the system parameters.",
    "descriptor": "\nComments: 10 pages, 16 figures, 5 tables\n",
    "authors": [
      "Parisa Boodaghi Malidarreh",
      "Abualkasim Bakeer",
      "Ihab S. Mohamed",
      "Lantao Liu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08101"
  },
  {
    "id": "arXiv:2110.08102",
    "title": "Linear maximum rank distance codes of exceptional type",
    "abstract": "Scattered polynomials of a given index over finite fields are intriguing rare\nobjects with many connections within mathematics. Of particular interest are\nthe exceptional ones, as defined in 2018 by the first author and Zhou, for\nwhich partial classification results are known. In this paper we propose a\nunified algebraic description of $\\mathbb{F}_{q^n}$-linear maximum rank\ndistance codes, introducing the notion of exceptional linear maximum rank\ndistance codes of a given index. Such a connection naturally extends the notion\nof exceptionality for a scattered polynomial in the rank metric framework and\nprovides a generalization of Moore sets in the monomial MRD context. We move\ntowards the classification of exceptional linear MRD codes, by showing that the\nones of index zero are generalized Gabidulin codes and proving that in the\npositive index case the code contains an exceptional scattered polynomial of\nthe same index.",
    "descriptor": "",
    "authors": [
      "Daniele Bartoli",
      "Giovanni Zini",
      "Ferdinando Zullo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.08102"
  },
  {
    "id": "arXiv:2110.08105",
    "title": "Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps  and Relevance Orderings",
    "abstract": "We study the effects of constrained optimization formulations and Frank-Wolfe\nalgorithms for obtaining interpretable neural network predictions.\nReformulating the Rate-Distortion Explanations (RDE) method for relevance\nattribution as a constrained optimization problem provides precise control over\nthe sparsity of relevance maps. This enables a novel multi-rate as well as a\nrelevance-ordering variant of RDE that both empirically outperform standard RDE\nin a well-established comparison test. We showcase several deterministic and\nstochastic variants of the Frank-Wolfe algorithm and their effectiveness for\nRDE.",
    "descriptor": "\nComments: 16 pages, 19 figures, 1 table\n",
    "authors": [
      "Jan Macdonald",
      "Mathieu Besan\u00e7on",
      "Sebastian Pokutta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.08105"
  },
  {
    "id": "arXiv:2110.08106",
    "title": "Compact representation for matrices of bounded twin-width",
    "abstract": "For every fixed $d \\in \\mathbb{N}$, we design a data structure that\nrepresents a binary $n \\times n$ matrix that is $d$-twin-ordered. The data\nstructure occupies $O_d(n)$ bits, which is the least one could hope for, and\ncan be queried for entries of the matrix in time $O_d(\\log \\log n)$ per query.",
    "descriptor": "\nComments: 24 pages, 2 figures\n",
    "authors": [
      "Micha\u0142 Pilipczuk",
      "Marek Soko\u0142owski",
      "Anna Zych-Pawlewicz"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.08106"
  },
  {
    "id": "arXiv:2110.08113",
    "title": "Hand Me Your PIN! Inferring ATM PINs of Users Typing with a Covered Hand",
    "abstract": "Automated Teller Machines (ATMs) represent the most used system for\nwithdrawing cash. The European Central Bank reported more than 11 billion cash\nwithdrawals and loading/unloading transactions on the European ATMs in 2019.\nAlthough ATMs have undergone various technological evolutions, Personal\nIdentification Numbers (PINs) are still the most common authentication method\nfor these devices. Unfortunately, the PIN mechanism is vulnerable to\nshoulder-surfing attacks performed via hidden cameras installed near the ATM to\ncatch the PIN pad. To overcome this problem, people get used to covering the\ntyping hand with the other hand. While such users probably believe this\nbehavior is safe enough to protect against mentioned attacks, there is no clear\nassessment of this countermeasure in the scientific literature.\nThis paper proposes a novel attack to reconstruct PINs entered by victims\ncovering the typing hand with the other hand. We consider the setting where the\nattacker can access an ATM PIN pad of the same brand/model as the target one.\nAfterward, the attacker uses that model to infer the digits pressed by the\nvictim while entering the PIN. Our attack owes its success to a carefully\nselected deep learning architecture that can infer the PIN from the typing hand\nposition and movements. We run a detailed experimental analysis including 58\nusers. With our approach, we can guess 30% of the 5-digit PINs within three\nattempts -- the ones usually allowed by ATM before blocking the card. We also\nconducted a survey with 78 users that managed to reach an accuracy of only\n7.92% on average for the same setting. Finally, we evaluate a shielding\ncountermeasure that proved to be rather inefficient unless the whole keypad is\nshielded.",
    "descriptor": "",
    "authors": [
      "Matteo Cardaioli",
      "Stefano Cecconello",
      "Mauro Conti",
      "Simone Milani",
      "Stjepan Picek",
      "Eugen Saraci"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08113"
  },
  {
    "id": "arXiv:2110.08118",
    "title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems",
    "abstract": "Learning to converse using only a few examples is a great challenge in\nconversational AI. The current best conversational models, which are either\ngood chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL),\nare language models (LMs) fine-tuned on large conversational datasets. Training\nthese models is expensive, both in terms of computational resources and time,\nand it is hard to keep them up to date with new conversational skills. A simple\nyet unexplored solution is prompt-based few-shot learning (Brown et al. 2020)\nwhich does not require gradient-based fine-tuning but instead uses a few\nexamples in the LM context as the only source of learning. In this paper, we\nexplore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of\ndifferent sizes in nine response generation tasks, which include four\nknowledge-grounded tasks, a task-oriented generations task, three open-chat\ntasks, and controlled stylistic generation, and five conversational parsing\ntasks, which include dialogue state tracking, graph path generation, persona\ninformation extraction, document retrieval, and internet query generation. The\ncurrent largest released LM (GPT-J-6B) using prompt-based few-shot learning,\nand thus requiring no training, achieves competitive performance to fully\ntrained state-of-the-art models. Moreover, we propose a novel prompt-based\nfew-shot classifier, that also does not require any fine-tuning, to select the\nmost appropriate prompt given a dialogue history. Finally, by combining the\npower of prompt-based few-shot learning and a Skill Selector, we create an\nend-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects\nthe most appropriate conversational skill, queries different knowledge bases or\nthe internet, and uses the retrieved knowledge to generate a human-like\nresponse, all using only few dialogue examples per skill.",
    "descriptor": "",
    "authors": [
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Genta Indra Winata",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08118"
  },
  {
    "id": "arXiv:2110.08122",
    "title": "Effects of Different Optimization Formulations in Evolutionary  Reinforcement Learning on Diverse Behavior Generation",
    "abstract": "Generating various strategies for a given task is challenging. However, it\nhas already proven to bring many assets to the main learning process, such as\nimproved behavior exploration. With the growth in the interest of heterogeneity\nin solution in evolutionary computation and reinforcement learning, many\npromising approaches have emerged. To better understand how one guides multiple\npolicies toward distinct strategies and benefit from diversity, we need to\nanalyze further the influence of the reward signal modulation and other\nevolutionary mechanisms on the obtained behaviors. To that effect, this paper\nconsiders an existing evolutionary reinforcement learning framework which\nexploits multi-objective optimization as a way to obtain policies that succeed\nat behavior-related tasks as well as completing the main goal. Experiments on\nthe Atari games stress that optimization formulations which do not consider\nobjectives equally fail at generating diversity and even output agents that are\nworse at solving the problem at hand, regardless of the obtained behaviors.",
    "descriptor": "",
    "authors": [
      "Victor Villin",
      "Naoki Masuyama",
      "Yusuke Nojima"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08122"
  },
  {
    "id": "arXiv:2110.08124",
    "title": "Decentralized Cooperative Lane Changing at Freeway Weaving Areas Using  Multi-Agent Deep Reinforcement Learning",
    "abstract": "Frequent lane changes during congestion at freeway bottlenecks such as merge\nand weaving areas further reduce roadway capacity. The emergence of deep\nreinforcement learning (RL) and connected and automated vehicle technology\nprovides a possible solution to improve mobility and energy efficiency at\nfreeway bottlenecks through cooperative lane changing. Deep RL is a collection\nof machine-learning methods that enables an agent to improve its performance by\nlearning from the environment. In this study, a decentralized cooperative\nlane-changing controller was developed using proximal policy optimization by\nadopting a multi-agent deep RL paradigm. In the decentralized control strategy,\npolicy learning and action reward are evaluated locally, with each agent\n(vehicle) getting access to global state information. Multi-agent deep RL\nrequires lower computational resources and is more scalable than single-agent\ndeep RL, making it a powerful tool for time-sensitive applications such as\ncooperative lane changing. The results of this study show that cooperative lane\nchanging enabled by multi-agent deep RL yields superior performance to human\ndrivers in term of traffic throughput, vehicle speed, number of stops per\nvehicle, vehicle fuel efficiency, and emissions. The trained RL policy is\ntransferable and can be generalized to uncongested, moderately congested, and\nextremely congested traffic conditions.",
    "descriptor": "",
    "authors": [
      "Yi Hou",
      "Peter Graf"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08124"
  },
  {
    "id": "arXiv:2110.08125",
    "title": "Towards a Multi-Agent System Architecture for Supply Chain Management",
    "abstract": "Individual business processes have been changing since the Internet was\ncreated, and they are now oriented towards a more distributed and collaborative\nbusiness model, in an e-commerce environment that adapts itself to the\ncompetitive and changing market conditions. This paper presents a multi-agent\nsystem architecture for supply chain management, which explores different\nstrategies and offers solutions in a distributed e-commerce environment. The\nsystem is designed to support different types of interfaces, which allow\ninteroperating with other business models already developed. In order to show\nhow the entire multi-agent system is being developed, the implementation of a\ncollaborative agent is presented and explained.",
    "descriptor": "\nComments: 2 figures, 2 snippets\n",
    "authors": [
      "Carlos R. Jaimez-Gonz\u00e1lez",
      "Wulfrano A. Luna-Ram\u00edrez"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08125"
  },
  {
    "id": "arXiv:2110.08126",
    "title": "Promoting Coordination Through Electing First-moveAgent in Multi-Agent  Reinforcement Learning",
    "abstract": "Learning to coordinate among multiple agents is an essential problem in\nmulti-agent systems. Multi-agent reinforcement learning has long been a go-to\ntool in the complicated collaborative environment. However, most existing works\nare constrained by the assumption that all agents take actions simultaneously.\nIn this paper, we endow the hierarchical order of play for the agents through\nelecting a first-move agent and other agents take the best response to the\nfirst-move agent to obtain better coordination. We propose the algorithm\nEFA-DQN to implicitly model the coordination and learn the coordinated behavior\nin multi-agent systems. To verify the feasibility and demonstrate the\neffectiveness and efficiency of our algorithm, we conduct extensive experiments\non several multi-agent tasks with different numbers of agents: Cooperative\nNavigation, Physical Deception, and The Google Football. The empirical results\nacross the various scenarios show that our method achieves competitive\nadvantages in terms of better performance and faster convergence, which\ndemonstrates that our algorithm has broad prospects for addressing many complex\nreal-world problems.",
    "descriptor": "",
    "authors": [
      "Jingqing Ruan",
      "Linghui Meng",
      "Bo Xu"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2110.08126"
  },
  {
    "id": "arXiv:2110.08127",
    "title": "Analyzing the performance of distributed conflict resolution among  autonomous vehicles",
    "abstract": "This paper presents a study on how cooperation versus non-cooperation, and\ncentralization versus distribution impact the performance of a traffic game of\nautonomous vehicles. A model using a particle-based, Lagrange representation,\nis developed, instead of a Eulerian, flow-based one, usual in routing problems\nof the game-theoretical approach. This choice allows representation of\nphenomena such as fuel exhaustion, vehicle collision, and wave propagation. The\nelements necessary to represent interactions in a multi-agent transportation\nsystem are defined, including a distributed, priority-based resource allocation\nprotocol, where resources are nodes and links in a spatial network and\nindividual routing strategies are performed. A fuel consumption dynamics is\ndeveloped in order to account for energy cost and vehicles having limited\nrange. The analysis shows that only the scenarios with cooperative resource\nallocation can achieve optimal values of either collective cost or equity\ncoefficient, corresponding respectively to the centralized and to the\ndistributed cases.",
    "descriptor": "",
    "authors": [
      "\u00cdtalo Romani de Oliveira"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08127"
  },
  {
    "id": "arXiv:2110.08128",
    "title": "Label-Wise Message Passing Graph Neural Network on Heterophilic Graphs",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable performance in modeling\ngraphs for various applications. However, most existing GNNs assume the graphs\nexhibit strong homophily in node labels, i.e., nodes with similar labels are\nconnected in the graphs. They fail to generalize to heterophilic graphs where\nlinked nodes may have dissimilar labels and attributes. Therefore, in this\npaper, we investigate a novel framework that performs well on graphs with\neither homophily or heterophily. More specifically, to address the challenge\nbrought by the heterophily in graphs, we propose a label-wise message passing\nmechanism. In label-wise message-passing, neighbors with similar pseudo labels\nwill be aggregated together, which will avoid the negative effects caused by\naggregating dissimilar node representations. We further propose a bi-level\noptimization method to automatically select the model for graphs with\nhomophily/heterophily. Extensive experiments demonstrate the effectiveness of\nour proposed framework for node classification on both homophilic and\nheterophilic graphs.",
    "descriptor": "",
    "authors": [
      "Enyan Dai",
      "Zhimeng Guo",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08128"
  },
  {
    "id": "arXiv:2110.08129",
    "title": "M\u00e9todo de Monte Carlo aplicado ao C\u00e1lculo Fracion\u00e1rio",
    "abstract": "This article analyzes and develops a method to solve fractional ordinary\ndifferential equations using the Monte Carlo Method. A numerical simulation is\nperformed for some differential equations, comparing the results with what\nexists in the mathematical literature. The Python language is used to create\ncomputational models.",
    "descriptor": "\nComments: Text in portuguese. Accepted for publication in Trends in Computational and Applied Mathematics\n",
    "authors": [
      "Luverci N. Ferreira",
      "Matheus J. Lazo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08129"
  },
  {
    "id": "arXiv:2110.08130",
    "title": "Breaking Down Multilingual Machine Translation",
    "abstract": "While multilingual training is now an essential ingredient in machine\ntranslation (MT) systems, recent work has demonstrated that it has different\neffects in different multilingual settings, such as many-to-one, one-to-many,\nand many-to-many learning. These training settings expose the encoder and the\ndecoder in a machine translation model with different data distributions. In\nthis paper, we examine how different varieties of multilingual training\ncontribute to learning these two components of the MT model. Specifically, we\ncompare bilingual models with encoders and/or decoders initialized by\nmultilingual training. We show that multilingual training is beneficial to\nencoders in general, while it only benefits decoders for low-resource languages\n(LRLs). We further find the important attention heads for each language pair\nand compare their correlations during inference. Our analysis sheds light on\nhow multilingual translation models work and also enables us to propose methods\nto improve performance by training with highly related languages. Our\nmany-to-one models for high-resource languages and one-to-many models for LRL\noutperform the best results reported by Aharoni et al. (2019).",
    "descriptor": "",
    "authors": [
      "Ting-Rui Chiang",
      "Yi-Pei Chen",
      "Yi-Ting Yeh",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08130"
  },
  {
    "id": "arXiv:2110.08131",
    "title": "Design Technology Co-Optimization for Neuromorphic Computing",
    "abstract": "We present a design-technology tradeoff analysis in implementing\nmachine-learning inference on the processing cores of a Non-Volatile Memory\n(NVM)-based many-core neuromorphic hardware. Through detailed circuit-level\nsimulations for scaled process technology nodes, we show the negative impact of\ndesign scaling on read endurance of NVMs, which directly impacts their\ninference lifetime. At a finer granularity, the inference lifetime of a core\ndepends on 1) the resistance state of synaptic weights programmed on the core\n(design) and 2) the voltage variation inside the core that is introduced by the\nparasitic components on current paths (technology). We show that such design\nand technology characteristics can be incorporated in a design flow to\nsignificantly improve the inference lifetime.",
    "descriptor": "",
    "authors": [
      "Ankita Paul",
      "Shihao Song",
      "Anup Das"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.08131"
  },
  {
    "id": "arXiv:2110.08133",
    "title": "Trade-offs of Local SGD at Scale: An Empirical Study",
    "abstract": "As datasets and models become increasingly large, distributed training has\nbecome a necessary component to allow deep neural networks to train in\nreasonable amounts of time. However, distributed training can have substantial\ncommunication overhead that hinders its scalability. One strategy for reducing\nthis overhead is to perform multiple unsynchronized SGD steps independently on\neach worker between synchronization steps, a technique known as local SGD. We\nconduct a comprehensive empirical study of local SGD and related methods on a\nlarge-scale image classification task. We find that performing local SGD comes\nat a price: lower communication costs (and thereby faster training) are\naccompanied by lower accuracy. This finding is in contrast from the\nsmaller-scale experiments in prior work, suggesting that local SGD encounters\nchallenges at scale. We further show that incorporating the slow momentum\nframework of Wang et al. (2020) consistently improves accuracy without\nrequiring additional communication, hinting at future directions for\npotentially escaping this trade-off.",
    "descriptor": "",
    "authors": [
      "Jose Javier Gonzalez Ortiz",
      "Jonathan Frankle",
      "Mike Rabbat",
      "Ari Morcos",
      "Nicolas Ballas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08133"
  },
  {
    "id": "arXiv:2110.08139",
    "title": "Chunked-Cache: On-Demand and Scalable Cache Isolation for Security  Architectures",
    "abstract": "Shared cache resources in multi-core processors are vulnerable to cache\nside-channel attacks. Recently proposed defenses have their own caveats:\nRandomization-based defenses are vulnerable to the evolving attack algorithms\nbesides relying on weak cryptographic primitives, because they do not\nfundamentally address the root cause for cache side-channel attacks. Cache\npartitioning defenses, on the other hand, provide the strict resource\npartitioning and effectively block all side-channel threats. However, they\nusually rely on way-based partitioning which is not fine-grained and cannot\nscale to support a larger number of protection domains, e.g., in trusted\nexecution environment (TEE) security architectures, besides degrading\nperformance and often resulting in cache underutilization.\nTo overcome the shortcomings of both approaches, we present a novel and\nflexible set-associative cache partitioning design for TEE architectures,\ncalled Chunked-Cache. Chunked-Cache enables an execution context to \"carve\" out\nan exclusive configurable chunk of the cache if the execution requires\nside-channel resilience. If side-channel resilience is not required, mainstream\ncache resources are freely utilized. Hence, our solution addresses the\nsecurity-performance trade-off practically by enabling selective and on-demand\nutilization of side-channel-resilient caches, while providing well-grounded\nfuture-proof security guarantees. We show that Chunked-Cache provides\nside-channel-resilient cache utilization for sensitive code execution, with\nsmall hardware overhead, while incurring no performance overhead on the OS. We\nalso show that it outperforms conventional way-based cache partitioning by 43%,\nwhile scaling significantly better to support a larger number of protection\ndomains.",
    "descriptor": "\nComments: Accepted on 3 Sept 2021 to appear at the Network and Distributed System Security Symposium (NDSS) 2022\n",
    "authors": [
      "Ghada Dessouky",
      "Alexander Gruler",
      "Pouya Mahmoody",
      "Ahmad-Reza Sadeghi",
      "Emmanuel Stapf"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.08139"
  },
  {
    "id": "arXiv:2110.08143",
    "title": "Multi-Tailed, Multi-Headed, Spatial Dynamic Memory refined Text-to-Image  Synthesis",
    "abstract": "Synthesizing high-quality, realistic images from text-descriptions is a\nchallenging task, and current methods synthesize images from text in a\nmulti-stage manner, typically by first generating a rough initial image and\nthen refining image details at subsequent stages. However, existing methods\nthat follow this paradigm suffer from three important limitations. Firstly,\nthey synthesize initial images without attempting to separate image attributes\nat a word-level. As a result, object attributes of initial images (that provide\na basis for subsequent refinement) are inherently entangled and ambiguous in\nnature. Secondly, by using common text-representations for all regions, current\nmethods prevent us from interpreting text in fundamentally different ways at\ndifferent parts of an image. Different image regions are therefore only allowed\nto assimilate the same type of information from text at each refinement stage.\nFinally, current methods generate refinement features only once at each\nrefinement stage and attempt to address all image aspects in a single shot.\nThis single-shot refinement limits the precision with which each refinement\nstage can learn to improve the prior image. Our proposed method introduces\nthree novel components to address these shortcomings: (1) An initial generation\nstage that explicitly generates separate sets of image features for each word\nn-gram. (2) A spatial dynamic memory module for refinement of images. (3) An\niterative multi-headed mechanism to make it easier to improve upon multiple\nimage aspects. Experimental results demonstrate that our Multi-Headed Spatial\nDynamic Memory image refinement with our Multi-Tailed Word-level Initial\nGeneration (MSMT-GAN) performs favourably against the previous state of the art\non the CUB and COCO datasets.",
    "descriptor": "",
    "authors": [
      "Amrit Diggavi Seshadri",
      "Balaraman Ravindran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08143"
  },
  {
    "id": "arXiv:2110.08144",
    "title": "Integrating diverse extraction pathways using iterative predictions for  Multilingual Open Information Extraction",
    "abstract": "In this paper we investigate a simple hypothesis for the Open Information\nExtraction (OpenIE) task, that it may be easier to extract some elements of an\ntriple if the extraction is conditioned on prior extractions which may be\neasier to extract. We successfully exploit this and propose a neural\nmultilingual OpenIE system that iteratively extracts triples by conditioning\nextractions on different elements of the triple leading to a rich set of\nextractions. The iterative nature of MiLIE also allows for seamlessly\nintegrating rule based extraction systems with a neural end-to-end system\nleading to improved performance. MiLIE outperforms SOTA systems on multiple\nlanguages ranging from Chinese to Galician thanks to it's ability of combining\nmultiple extraction pathways. Our analysis confirms that it is indeed true that\ncertain elements of an extraction are easier to extract than others. Finally,\nwe introduce OpenIE evaluation datasets for two low resource languages namely\nJapanese and Galician.",
    "descriptor": "",
    "authors": [
      "Bhushan Kotnis",
      "Kiril Gashteovski",
      "Carolin Lawrence",
      "Daniel O\u00f1oro Rubio",
      "Vanesa Rodriguez-Tembras",
      "Makoto Takamoto",
      "Mathias Niepert"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08144"
  },
  {
    "id": "arXiv:2110.08146",
    "title": "ACOA -- Chronological Analysis of the Exhibition of Artistic Works",
    "abstract": "We are currently prioritizing home activities, avoiding human contact, and\ncarrying out external activities mostly by necessity. Therefore, and due to the\nloss of adhesion to cultural events on the part of the population, the cultural\ndigital transformation process has been boosted, aiming to reach interested\ncommunities through digital media. The ACOA platform supports the organization\nof multiple sources of information related to creative processes behind complex\nartworks and their trajectories over time. This information is of great\ninterest to conservators and curators, as well as to the general public, as it\nallows to document changes in the artwork, from the moment it was conceived by\nthe artist, until its most recent exhibition. This platform houses a\nchronological evolution of the work, through the contextual dissemination of\nassociated multimedia content. Works by the Portuguese artist Ana Vieira\n(1940-2016) were chosen as case studies for the implementation of the platform.",
    "descriptor": "",
    "authors": [
      "Daniela Prado",
      "Armanda Rodrigues",
      "Nuno Correia",
      "Rita Macedo",
      "Sofia Gomes"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.08146"
  },
  {
    "id": "arXiv:2110.08151",
    "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained  Language Models",
    "abstract": "Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations.",
    "descriptor": "",
    "authors": [
      "Ryokan Ri",
      "Ikuya Yamada",
      "Yoshimasa Tsuruoka"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08151"
  },
  {
    "id": "arXiv:2110.08152",
    "title": "Kronecker Decomposition for GPT Compression",
    "abstract": "GPT is an auto-regressive Transformer-based pre-trained language model which\nhas attracted a lot of attention in the natural language processing (NLP)\ndomain due to its state-of-the-art performance in several downstream tasks. The\nsuccess of GPT is mostly attributed to its pre-training on huge amount of data\nand its large number of parameters (from ~100M to billions of parameters).\nDespite the superior performance of GPT (especially in few-shot or zero-shot\nsetup), this overparameterized nature of GPT can be very prohibitive for\ndeploying this model on devices with limited computational power or memory.\nThis problem can be mitigated using model compression techniques; however,\ncompressing GPT models has not been investigated much in the literature. In\nthis work, we use Kronecker decomposition to compress the linear mappings of\nthe GPT-22 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on\nthe Kronecker decomposed version of the GPT-2 model and then is undergone a\nvery light pre-training on only a small portion of the training data with\nintermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is\nfine-tuned on down-stream tasks using ILKD as well. We evaluate our model on\nboth language modeling and General Language Understanding Evaluation benchmark\ntasks and show that with more efficient pre-training and similar number of\nparameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.",
    "descriptor": "",
    "authors": [
      "Ali Edalati",
      "Marzieh Tahaei",
      "Ahmad Rashid",
      "Vahid Partovi Nia",
      "James J. Clark",
      "Mehdi Rezagholizadeh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08152"
  },
  {
    "id": "arXiv:2110.08154",
    "title": "Distributed Resource Allocation Optimization for User-Centric Cell-Free  MIMO Networks",
    "abstract": "We develop two distributed downlink resource allocation algorithms for\nuser-centric, cell-free, spatially-distributed, multiple-input multiple-output\n(MIMO) networks. In such networks, each user is served by a subset of nearby\ntransmitters that we call distributed units or DUs. The operation of the DUs in\na region is controlled by a central unit (CU). Our first scheme is implemented\nat the DUs, while the second is implemented at the CUs controlling these DUs.\nWe define a hybrid quality of service metric that enables distributed\noptimization of system resources in a proportional fair manner. Specifically,\neach of our algorithms performs user scheduling, beamforming, and power control\nwhile accounting for channel estimation errors. Importantly, our algorithm does\nnot require information exchange amongst DUs (CUs) for the DU-distributed\n(CU-distributed) system, while also smoothly converging. Our results show that\nour CU-distributed system provides 1.3- to 1.8-fold network throughput compared\nto the DU-distributed system, with minor increases in complexity and front-haul\nload - and substantial gains over benchmark schemes like local zero-forcing. We\nalso analyze the trade-offs provided by the CU-distributed system, hence\nhighlighting the significance of deploying multiple CUs in user-centric\ncell-free networks.",
    "descriptor": "\nComments: To appear in IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Hussein A. Ammar",
      "Raviraj Adve",
      "Shahram Shahbazpanahi",
      "Gary Boudreau",
      "Kothapalli Venkata Srinivas"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Performance (cs.PF)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.08154"
  },
  {
    "id": "arXiv:2110.08164",
    "title": "Accurate Fine-grained Layout Analysis for the Historical Tibetan  Document Based on the Instance Segmentation",
    "abstract": "Accurate layout analysis without subsequent text-line segmentation remains an\nongoing challenge, especially when facing the Kangyur, a kind of historical\nTibetan document featuring considerable touching components and mottled\nbackground. Aiming at identifying different regions in document images, layout\nanalysis is indispensable for subsequent procedures such as character\nrecognition. However, there was only a little research being carried out to\nperform line-level layout analysis which failed to deal with the Kangyur. To\nobtain the optimal results, a fine-grained sub-line level layout analysis\napproach is presented. Firstly, we introduced an accelerated method to build\nthe dataset which is dynamic and reliable. Secondly, enhancement had been made\nto the SOLOv2 according to the characteristics of the Kangyur. Then, we fed the\nenhanced SOLOv2 with the prepared annotation file during the training phase.\nOnce the network is trained, instances of the text line, sentence, and titles\ncan be segmented and identified during the inference stage. The experimental\nresults show that the proposed method delivers a decent 72.7% AP on our\ndataset. In general, this preliminary research provides insights into the\nfine-grained sub-line level layout analysis and testifies the SOLOv2-based\napproaches. We also believe that the proposed methods can be adopted on other\nlanguage documents with various layouts.",
    "descriptor": "\nComments: The manuscript contains 16 pages,14 figures, and 40 references in total. Now, the manuscript is undereviewed by the 'Big data research'(ISSN:2214-5796)\n",
    "authors": [
      "Penghai Zhao",
      "Weilan Wang",
      "Xiaojuan Wang",
      "Zhengqi Cai",
      "Guowei Zhang",
      "Yuqi Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08164"
  },
  {
    "id": "arXiv:2110.08166",
    "title": "Optimal Distribution Design for Irregular Repetition Slotted ALOHA with  Multi-Packet Reception",
    "abstract": "Associated with multi-packet reception at the access point, irregular\nrepetition slotted ALOHA (IRSA) holds a great potential in improving the access\ncapacity of massive machine type communication systems. Considering the\ntime-frequency resource efficiency, K = 2 (multi-packet reception capability)\nmay be the most suitable scheme for scenarios that allow smaller resource\nefficiency in exchange for greater throughput. In this paper, we analytically\nderive an optimal transmission probability distribution for IRSA with K = 2,\nwhich achieves a significant higher load threshold than the existing benchmark\ndistributions. In addition, the energy efficiency optimization in terms of the\nmaximum repetition rate is also presented.",
    "descriptor": "",
    "authors": [
      "Zhengchuan Chen",
      "Yifan Feng",
      "Chundie Feng",
      "Liang Liang",
      "Yunjian Jia",
      "Tony Q. S. Quek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.08166"
  },
  {
    "id": "arXiv:2110.08168",
    "title": "DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization",
    "abstract": "Transformer-based models have achieved state-of-the-art performance on short\ntext summarization. However, they still struggle with long-input summarization.\nIn this paper, we present a new approach for long-input summarization: Dynamic\nLatent Extraction for Abstractive Summarization. We jointly train an extractor\nwith an abstractor and treat the extracted text snippets as the latent\nvariable. We propose extractive oracles to provide the extractor with a strong\nlearning signal. We introduce consistency loss, which encourages the extractor\nto approximate the averaged dynamic weights predicted by the generator. We\nconduct extensive tests on two long-input summarization datasets, GovReport\n(document) and QMSum (dialogue). Our model significantly outperforms the\ncurrent state-of-the-art, including a 6.21 ROUGE-2 improvement on GovReport and\na 2.13 ROUGE-1 improvement on QMSum. Further analysis shows that the dynamic\nweights make our generation process highly interpretable. Our code will be\npublicly available upon publication.",
    "descriptor": "",
    "authors": [
      "Ziming Mao",
      "Chen Henry Wu",
      "Ansong Ni",
      "Yusen Zhang",
      "Rui Zhang",
      "Tao Yu",
      "Budhaditya Deb",
      "Chenguang Zhu",
      "Ahmed H. Awadallah",
      "Dragomir Radev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08168"
  },
  {
    "id": "arXiv:2110.08169",
    "title": "Containerized Distributed Value-Based Multi-Agent Reinforcement Learning",
    "abstract": "Multi-agent reinforcement learning tasks put a high demand on the volume of\ntraining samples. Different from its single-agent counterpart, distributed\nvalue-based multi-agent reinforcement learning faces the unique challenges of\ndemanding data transfer, inter-process communication management, and high\nrequirement of exploration. We propose a containerized learning framework to\nsolve these problems. We pack several environment instances, a local learner\nand buffer, and a carefully designed multi-queue manager which avoids blocking\ninto a container. Local policies of each container are encouraged to be as\ndiverse as possible, and only trajectories with highest priority are sent to a\nglobal learner. In this way, we achieve a scalable, time-efficient, and diverse\ndistributed MARL learning framework with high system throughput. To own\nknowledge, our method is the first to solve the challenging Google Research\nFootball full game $5\\_v\\_5$. On the StarCraft II micromanagement benchmark,\nour method gets $4$-$18\\times$ better results compared to state-of-the-art\nnon-distributed MARL algorithms.",
    "descriptor": "",
    "authors": [
      "Siyang Wu",
      "Tonghan Wang",
      "Chenghao Li",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08169"
  },
  {
    "id": "arXiv:2110.08170",
    "title": "Simulation of emergence in artificial societies: a practical model-based  approach with the EB-DEVS formalism",
    "abstract": "Modelling and simulation of complex systems is key to exploring and\nunderstanding social processes, benefiting from formal mechanisms to derive\nglobal-level properties from local-level interactions. In this paper we extend\nthe body of knowledge on formal methods in complex systems by applying EB-DEVS,\na novel formalism tailored for the modelling, simulation and live\nidentification of emergent properties. We guide the reader through the\nimplementation of different classical models for varied social systems to\nintroduce good modelling practices and showcase the advantages and limitations\nof modelling emergence with EB-DEVS, in particular through its live emergence\ndetection capability. This work provides case study-driven evidence for the\nneatness and compactness of the approach to modelling communication structures\nthat can be explicit or implicit, static or dynamic, with or without multilevel\ninteractions, and with weak or strong emergent behaviour. Throughout examples\nwe show that EB-DEVS permits conceptualising the analysed societies by\nincorporating emergent behaviour when required, namely by integrating as a\nmacro-level aggregate the Gini index in the Sugarscape model, Fads and Fashion\nin the Dissemination of Culture model, size-biased degree distribution in a\nPreferential Attachment model, happiness index in the Segregation model and\nquarantines in the SIR epidemic model. In each example we discuss the role of\ncommunication structures in the development of multilevel simulation models,\nand illustrate how micro-macro feedback loops enable the modelling of\nmacro-level properties. Our results stress the relevance of multilevel features\nto support a robust approach in the modelling and simulation of complex\nsystems.",
    "descriptor": "",
    "authors": [
      "Daniel Foguelman",
      "Esteban Lanzarotti",
      "Emanuel Ferreyra",
      "Rodrigo Castro"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Adaptation and Self-Organizing Systems (nlin.AO)"
    ],
    "url": "https://arxiv.org/abs/2110.08170"
  },
  {
    "id": "arXiv:2110.08172",
    "title": "MLFC: From 10 to 50 Planners in the Multi-Agent Programming Contest",
    "abstract": "In this paper, we describe the strategies used by our team, MLFC, that led us\nto achieve the 2nd place in the 15th edition of the Multi-Agent Programming\nContest. The scenario used in the contest is an extension of the previous\nedition (14th) ``Agents Assemble'' wherein two teams of agents move around a 2D\ngrid and compete to assemble complex block structures. We discuss the languages\nand tools used during the development of our team. Then, we summarise the main\nstrategies that were carried over from our previous participation in the 14th\nedition and list the limitations (if any) of using these strategies in the\nlatest contest edition. We also developed new strategies that were made\nspecifically for the extended scenario: cartography (determining the size of\nthe map); formal verification of the map merging protocol (to provide\nassurances that it works when increasing the number of agents); plan cache\n(efficiently scaling the number of planners); task achievement (forming groups\nof agents to achieve tasks); and bullies (agents that focus on stopping agents\nfrom the opposing team). Finally, we give a brief overview of our performance\nin the contest and discuss what we believe were our shortcomings.",
    "descriptor": "\nComments: Published in The Multi-Agent Programming Contest 2021: One-and-a-Half Decades of Exploring Multi-Agent Systems\n",
    "authors": [
      "Rafael C. Cardoso",
      "Angelo Ferrando",
      "Fabio Papacchini",
      "Matt Luckcuck",
      "Sven Linker",
      "Terry R. Payne"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.08172"
  },
  {
    "id": "arXiv:2110.08173",
    "title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge  of Pre-trained Language Models",
    "abstract": "Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.",
    "descriptor": "",
    "authors": [
      "Zaiqiao Meng",
      "Fangyu Liu",
      "Ehsan Shareghi",
      "Yixuan Su",
      "Charlotte Collins",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08173"
  },
  {
    "id": "arXiv:2110.08175",
    "title": "MixQG: Neural Question Generation with Mixed Answer Types",
    "abstract": "Asking good questions is an essential ability for both human and machine\nintelligence. However, existing neural question generation approaches mainly\nfocus on the short factoid type of answers. In this paper, we propose a neural\nquestion generator, MixQG, to bridge this gap. We combine 9 question answering\ndatasets with diverse answer types, including yes/no, multiple-choice,\nextractive, and abstractive answers, to train a single generative model. We\nshow with empirical results that our model outperforms existing work in both\nseen and unseen domains and can generate questions with different cognitive\nlevels when conditioned on different answer types. Our code is released and\nwell-integrated with the Huggingface library to facilitate various downstream\napplications.",
    "descriptor": "",
    "authors": [
      "Lidiya Murakhovs'ka",
      "Chien-Sheng Wu",
      "Tong Niu",
      "Wenhao Liu",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08175"
  },
  {
    "id": "arXiv:2110.08176",
    "title": "Collaborating with Humans without Human Data",
    "abstract": "Collaborating with humans requires rapidly adapting to their individual\nstrengths, weaknesses, and preferences. Unfortunately, most standard\nmulti-agent reinforcement learning techniques, such as self-play (SP) or\npopulation play (PP), produce agents that overfit to their training partners\nand do not generalize well to humans. Alternatively, researchers can collect\nhuman data, train a human model using behavioral cloning, and then use that\nmodel to train \"human-aware\" agents (\"behavioral cloning play\", or BCP). While\nsuch an approach can improve the generalization of agents to new human\nco-players, it involves the onerous and expensive step of collecting large\namounts of human data first. Here, we study the problem of how to train agents\nthat collaborate well with human partners without using human data. We argue\nthat the crux of the problem is to produce a diverse set of training partners.\nDrawing inspiration from successful multi-agent approaches in competitive\ndomains, we find that a surprisingly simple approach is highly effective. We\ntrain our agent partner as the best response to a population of self-play\nagents and their past checkpoints taken throughout training, a method we call\nFictitious Co-Play (FCP). Our experiments focus on a two-player collaborative\ncooking simulator that has recently been proposed as a challenge problem for\ncoordination with humans. We find that FCP agents score significantly higher\nthan SP, PP, and BCP when paired with novel agent and human partners.\nFurthermore, humans also report a strong subjective preference to partnering\nwith FCP agents over all baselines.",
    "descriptor": "\nComments: Accepted at NeurIPS 2021 (spotlight)\n",
    "authors": [
      "DJ Strouse",
      "Kevin R. McKee",
      "Matt Botvinick",
      "Edward Hughes",
      "Richard Everett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.08176"
  },
  {
    "id": "arXiv:2110.08177",
    "title": "The Privacy-preserving Padding Problem: Non-negative Mechanisms for  Conservative Answers with Differential Privacy",
    "abstract": "Differentially private noise mechanisms commonly use symmetric noise\ndistributions. This is attractive both for achieving the differential privacy\ndefinition, and for unbiased expectations in the noised answers. However, there\nare contexts in which a noisy answer only has utility if it is conservative,\nthat is, has known-signed error, which we call a padded answer. Seemingly, it\nis paradoxical to satisfy the DP definition with one-sided error, but we show\nhow it is possible to bury the paradox into approximate DP's delta parameter.\nWe develop a few mechanisms for one-sided padding mechanisms that always give\nconservative answers, but still achieve approximate differential privacy. We\nshow how these mechanisms can be applied in a few select areas including making\nthe cardinalities of set intersections and unions revealed in Private Set\nIntersection protocols differential private and enabling multiparty computation\nprotocols to compute on sparse data which has its exact sizes made differential\nprivate rather than performing a fully oblivious more expensive computation.",
    "descriptor": "\nComments: 20 pages, 7 figures\n",
    "authors": [
      "Benjamin M. Case",
      "James Honaker",
      "Mahnush Movahedi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.08177"
  },
  {
    "id": "arXiv:2110.08181",
    "title": "Loosely coupled, non-iterative time-splitting scheme based on  Robin-Robin coupling: unified analysis for parabolic/parabolic and  parabolic/hyperbolic problems",
    "abstract": "We present a loosely coupled, non-iterative time-splitting scheme based on\nRobin-Robin coupling conditions. We apply a novel unified analysis for this\nscheme applied to both a Parabolic/Parabolic coupled system and a\nParabolic/Hyperbolic coupled system. We show for both systems that the scheme\nis stable, and the error converges as $\\mathcal{O}\\big(\\Delta t \\sqrt{T\n+\\log{\\frac{1}{\\Delta t}}}\\big)$, where $\\Delta t$ is the time step",
    "descriptor": "",
    "authors": [
      "Erik Burman",
      "Rebecca Durst",
      "Miguel Fern\u00e1ndez",
      "Johnny Guzm\u00e1n"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08181"
  },
  {
    "id": "arXiv:2110.08182",
    "title": "The World of an Octopus: How Reporting Bias Influences a Language  Model's Perception of Color",
    "abstract": "Recent work has raised concerns about the inherent limitations of text-only\npretraining. In this paper, we first demonstrate that reporting bias, the\ntendency of people to not state the obvious, is one of the causes of this\nlimitation, and then investigate to what extent multimodal training can\nmitigate this issue. To accomplish this, we 1) generate the Color Dataset\n(CoDa), a dataset of human-perceived color distributions for 521 common\nobjects; 2) use CoDa to analyze and compare the color distribution found in\ntext, the distribution captured by language models, and a human's perception of\ncolor; and 3) investigate the performance differences between text-only and\nmultimodal models on CoDa. Our results show that the distribution of colors\nthat a language model recovers correlates more strongly with the inaccurate\ndistribution found in text than with the ground-truth, supporting the claim\nthat reporting bias negatively impacts and inherently limits text-only\ntraining. We then demonstrate that multimodal models can leverage their visual\ntraining to mitigate these effects, providing a promising avenue for future\nresearch.",
    "descriptor": "\nComments: Accepted to EMNLP 2021, 9 Pages\n",
    "authors": [
      "Cory Paik",
      "St\u00e9phane Aroca-Ouellette",
      "Alessandro Roncone",
      "Katharina Kann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08182"
  },
  {
    "id": "arXiv:2110.08185",
    "title": "Propagation on Multi-relational Graphs for Node Regression",
    "abstract": "Recent years have witnessed a rise in real-world data captured with rich\nstructural information that can be conveniently depicted by multi-relational\ngraphs. While inference of continuous node features across a simple graph is\nrather under-studied by the current relational learning research, we go one\nstep further and focus on node regression problem on multi-relational graphs.\nWe take inspiration from the well-known label propagation algorithm aiming at\ncompleting categorical features across a simple graph and propose a novel\npropagation framework for completing missing continuous features at the nodes\nof a multi-relational and directed graph. Our multi-relational propagation\nalgorithm is composed of iterative neighborhood aggregations which originate\nfrom a relational local generative model. Our findings show the benefit of\nexploiting the multi-relational structure of the data in several node\nregression scenarios in different settings.",
    "descriptor": "\nComments: Accepted to IJCLR 2021 Workshop: Statistical Relational AI (StarAI)\n",
    "authors": [
      "Eda Bayram"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08185"
  },
  {
    "id": "arXiv:2110.08186",
    "title": "Bound-Preserving Finite-Volume Schemes for Systems of Continuity  Equations with Saturation",
    "abstract": "We propose finite-volume schemes for general continuity equations which\npreserve positivity and global bounds that arise from saturation effects in the\nmobility function. In the particular case of gradient flows, the schemes\ndissipate the free energy at the fully discrete level. Moreover, these schemes\nare generalised to coupled systems of non-linear continuity equations, such as\nmultispecies models in mathematical physics or biology, preserving the bounds\nand the dissipation of the energy whenever applicable. These results are\nillustrated through extensive numerical simulations which explore known\nbehaviours in biology and showcase new phenomena not yet described by the\nliterature.",
    "descriptor": "",
    "authors": [
      "Rafael Bailo",
      "Jos\u00e9 A. Carrillo",
      "Jingwei Hu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.08186"
  },
  {
    "id": "arXiv:2110.08187",
    "title": "Crop Rotation Modeling for Deep Learning-Based Parcel Classification  from Satellite Time Series",
    "abstract": "While annual crop rotations play a crucial role for agricultural\noptimization, they have been largely ignored for automated crop type mapping.\nIn this paper, we take advantage of the increasing quantity of annotated\nsatellite data to propose the first deep learning approach modeling\nsimultaneously the inter- and intra-annual agricultural dynamics of parcel\nclassification. Along with simple training adjustments, our model provides an\nimprovement of over 6.6 mIoU points over the current state-of-the-art of crop\nclassification. Furthermore, we release the first large-scale multi-year\nagricultural dataset with over 300,000 annotated parcels.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "F\u00e9lix Quinton",
      "Loic Landrieu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08187"
  },
  {
    "id": "arXiv:2110.08188",
    "title": "Guided Point Contrastive Learning for Semi-supervised Point Cloud  Semantic Segmentation",
    "abstract": "Rapid progress in 3D semantic segmentation is inseparable from the advances\nof deep network models, which highly rely on large-scale annotated data for\ntraining. To address the high cost and challenges of 3D point-level labeling,\nwe present a method for semi-supervised point cloud semantic segmentation to\nadopt unlabeled point clouds in training to boost the model performance.\nInspired by the recent contrastive loss in self-supervised tasks, we propose\nthe guided point contrastive loss to enhance the feature representation and\nmodel generalization ability in semi-supervised setting. Semantic predictions\non unlabeled point clouds serve as pseudo-label guidance in our loss to avoid\nnegative pairs in the same category. Also, we design the confidence guidance to\nensure high-quality feature learning. Besides, a category-balanced sampling\nstrategy is proposed to collect positive and negative samples to mitigate the\nclass imbalance problem. Extensive experiments on three datasets (ScanNet V2,\nS3DIS, and SemanticKITTI) show the effectiveness of our semi-supervised method\nto improve the prediction quality with unlabeled data.",
    "descriptor": "\nComments: ICCV 2021\n",
    "authors": [
      "Li Jiang",
      "Shaoshuai Shi",
      "Zhuotao Tian",
      "Xin Lai",
      "Shu Liu",
      "Chi-Wing Fu",
      "Jiaya Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08188"
  },
  {
    "id": "arXiv:2110.08190",
    "title": "Sparse Progressive Distillation: Resolving Overfitting under  Pretrain-and-Finetune Paradigm",
    "abstract": "Various pruning approaches have been proposed to reduce the footprint\nrequirements of Transformer-based language models. Conventional wisdom is that\npruning reduces the model expressiveness and thus is more likely to underfit\nthan overfit compared to the original model. However, under the trending\npretrain-and-finetune paradigm, we argue that pruning increases the risk of\noverfitting if pruning was performed at the fine-tuning phase, as it increases\nthe amount of information a model needs to learn from the downstream task,\nresulting in relative data deficiency. In this paper, we aim to address the\noverfitting issue under the pretrain-and-finetune paradigm to improve pruning\nperformance via progressive knowledge distillation (KD) and sparse pruning.\nFurthermore, to mitigate the interference between different strategies of\nlearning rate, pruning and distillation, we propose a three-stage learning\nframework. We show for the first time that reducing the risk of overfitting can\nhelp the effectiveness of pruning under the pretrain-and-finetune paradigm.\nExperiments on multiple datasets of GLUE benchmark show that our method\nachieves highly competitive pruning performance over the state-of-the-art\ncompetitors across different pruning ratio constraints.",
    "descriptor": "",
    "authors": [
      "Shaoyi Huang",
      "Dongkuan Xu",
      "Ian E.H. Yen",
      "Sung-en Chang",
      "Bingbing Li",
      "Shiyang Chen",
      "Mimi Xie",
      "Hang Liu",
      "Caiwen Ding"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08190"
  },
  {
    "id": "arXiv:2110.08191",
    "title": "Why don't people use character-level machine translation?",
    "abstract": "We present a literature and empirical survey that critically assesses the\nstate of the art in character-level modeling for machine translation (MT).\nDespite evidence in the literature that character-level systems are comparable\nwith subword systems, they are virtually never used in competitive setups in\nWMT competitions. We empirically show that even with recent modeling\ninnovations in character-level natural language processing, character-level MT\nsystems still struggle to match their subword-based counterparts both in terms\nof translation quality and training and inference speed. Character-level MT\nsystems show neither better domain robustness, nor better morphological\ngeneralization, despite being often so motivated. On the other hand, they tend\nto be more robust towards source side noise and the translation quality does\nnot degrade with increasing beam size at decoding time.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Jind\u0159ich Libovick\u00fd",
      "Helmut Schmid",
      "Alexander Fraser"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08191"
  },
  {
    "id": "arXiv:2110.08192",
    "title": "Attention meets Geometry: Geometry Guided Spatial-Temporal Attention for  Consistent Self-Supervised Monocular Depth Estimation",
    "abstract": "Inferring geometrically consistent dense 3D scenes across a tuple of\ntemporally consecutive images remains challenging for self-supervised monocular\ndepth prediction pipelines. This paper explores how the increasingly popular\ntransformer architecture, together with novel regularized loss formulations,\ncan improve depth consistency while preserving accuracy. We propose a spatial\nattention module that correlates coarse depth predictions to aggregate local\ngeometric information. A novel temporal attention mechanism further processes\nthe local geometric information in a global context across consecutive images.\nAdditionally, we introduce geometric constraints between frames regularized by\nphotometric cycle consistency. By combining our proposed regularization and the\nnovel spatial-temporal-attention module we fully leverage both the geometric\nand appearance-based consistency across monocular frames. This yields\ngeometrically meaningful attention and improves temporal depth stability and\naccuracy compared to previous methods.",
    "descriptor": "\nComments: Accepted at 3DV 2021\n",
    "authors": [
      "Patrick Ruhkamp",
      "Daoyi Gao",
      "Hanzhi Chen",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08192"
  },
  {
    "id": "arXiv:2110.08193",
    "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering",
    "abstract": "It is well documented that NLP models learn social biases present in the\nworld, but little work has been done to show how these biases manifest in\nactual model outputs for applied tasks like question answering (QA). We\nintroduce the Bias Benchmark for QA (BBQ), a dataset consisting of\nquestion-sets constructed by the authors that highlight \\textit{attested}\nsocial biases against people belonging to protected classes along nine\ndifferent social dimensions relevant for U.S. English-speaking contexts. Our\ntask evaluates model responses at two distinct levels: (i) given an\nunder-informative context, test how strongly model answers reflect social\nbiases, and (ii) given an adequately informative context, test whether the\nmodel's biases still override a correct answer choice. We find that models\nstrongly rely on stereotypes when the context is ambiguous, meaning that the\nmodel's outputs consistently reproduce harmful biases in this setting. Though\nmodels are much more accurate when the context provides an unambiguous answer,\nthey still rely on stereotyped information and achieve an accuracy 2.5\npercentage points higher on examples where the correct answer aligns with a\nsocial bias, with this accuracy difference widening to 5 points for examples\ntargeting gender.",
    "descriptor": "\nComments: 16 pages, 9 figures\n",
    "authors": [
      "Alicia Parrish",
      "Angelica Chen",
      "Nikita Nangia",
      "Vishakh Padmakumar",
      "Jason Phang",
      "Jana Thompson",
      "Phu Mon Htut",
      "Samuel R. Bowman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08193"
  },
  {
    "id": "arXiv:2110.08196",
    "title": "The Pebble-Relation Comonad in Finite Model Theory",
    "abstract": "The pebbling comonad, introduced by Abramsky, Dawar and Wang, provides a\ncategorical interpretation for the k-pebble games from finite model theory. The\ncoKleisli category of the pebbling comonad specifies equivalences under\ndifferent fragments and extensions of infinitary k-variable logic. Moreover,\nthe coalgebras over this pebbling comonad characterise treewidth and correspond\nto tree decompositions.\nIn this paper we introduce the pebble-relation comonad that characterises\npathwidth and whose coalgebras correspond to path decompositions. We further\nshow how the coKleisli morphisms of the pebble-relation comonad provide a\ncategorical interpretation to Duplicator's winning strategies in Dalmau's\npebble-relation game. We then provide a similar treatment to the corresponding\ncoKleisli isomorphisms via a novel bijective pebble-game with a hidden pebble.\nFinally, we prove a new Lov\\'asz-type theorem relating pathwidth to the\nrestricted conjunction fragment of k-variable logic with counting quantifiers\nusing a recently developed categorical generalisation.",
    "descriptor": "",
    "authors": [
      "Yo\u00e0v Montacute",
      "Nihil Shah"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.08196"
  },
  {
    "id": "arXiv:2110.08202",
    "title": "Evaluation of Hyperparameter-Optimization Approaches in an Industrial  Federated Learning System",
    "abstract": "Federated Learning (FL) decouples model training from the need for direct\naccess to the data and allows organizations to collaborate with industry\npartners to reach a satisfying level of performance without sharing vulnerable\nbusiness information. The performance of a machine learning algorithm is highly\nsensitive to the choice of its hyperparameters. In an FL setting,\nhyperparameter optimization poses new challenges. In this work, we investigated\nthe impact of different hyperparameter optimization approaches in an FL system.\nIn an effort to reduce communication costs, a critical bottleneck in FL, we\ninvestigated a local hyperparameter optimization approach that -- in contrast\nto a global hyperparameter optimization approach -- allows every client to have\nits own hyperparameter configuration. We implemented these approaches based on\ngrid search and Bayesian optimization and evaluated the algorithms on the MNIST\ndata set using an i.i.d. partition and on an Internet of Things (IoT) sensor\nbased industrial data set using a non-i.i.d. partition.",
    "descriptor": "\nComments: This paper is accepted at the IDSC this https URL and will be published by Springer. The Version uploaded is before the peer review process. The link to the final version will be updated as soon as the paper is published\n",
    "authors": [
      "Stephanie Holly",
      "Thomas Hiessl",
      "Safoura Rezapour Lakani",
      "Daniel Schall",
      "Clemens Heitzinger",
      "Jana Kemnitz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08202"
  },
  {
    "id": "arXiv:2110.08203",
    "title": "Shared Visual Representations of Drawing for Communication: How do  different biases affect human interpretability and intent?",
    "abstract": "We present an investigation into how representational losses can affect the\ndrawings produced by artificial agents playing a communication game. Building\nupon recent advances, we show that a combination of powerful pretrained encoder\nnetworks, with appropriate inductive biases, can lead to agents that draw\nrecognisable sketches, whilst still communicating well. Further, we start to\ndevelop an approach to help automatically analyse the semantic content being\nconveyed by a sketch and demonstrate that current approaches to inducing\nperceptual biases lead to a notion of objectness being a key feature despite\nthe agent training being self-supervised.",
    "descriptor": "",
    "authors": [
      "Daniela Mihai",
      "Jonathon Hare"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08203"
  },
  {
    "id": "arXiv:2110.08207",
    "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks. It has been hypothesized that this is\na consequence of implicit multitask learning in language model training. Can\nzero-shot generalization instead be directly induced by explicit multitask\nlearning? To test this question at scale, we develop a system for easily\nmapping general natural language tasks into a human-readable prompted form. We\nconvert a large set of supervised datasets, each with multiple prompts using\nvarying natural language. These prompted datasets allow for benchmarking the\nability of a model to perform completely unseen tasks specified in natural\nlanguage. We fine-tune a pretrained encoder-decoder model on this multitask\nmixture covering a wide variety of tasks. The model attains strong zero-shot\nperformance on several standard datasets, often outperforming models 16x its\nsize. Further, our approach attains strong performance on a subset of tasks\nfrom the BIG-Bench benchmark, outperforming models 6x its size. All prompts and\ntrained models are available at github.com/bigscience-workshop/promptsource/.",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Victor Sanh",
      "Albert Webson",
      "Colin Raffel",
      "Stephen H. Bach",
      "Lintang Sutawika",
      "Zaid Alyafeai",
      "Antoine Chaffin",
      "Arnaud Stiegler",
      "Teven Le Scao",
      "Arun Raja",
      "Manan Dey",
      "M Saiful Bari",
      "Canwen Xu",
      "Urmish Thakker",
      "Shanya Sharma Sharma",
      "Eliza Szczechla",
      "Taewoon Kim",
      "Gunjan Chhablani",
      "Nihal Nayak",
      "Debajyoti Datta",
      "Jonathan Chang",
      "Mike Tian-Jian Jiang",
      "Han Wang",
      "Matteo Manica",
      "Sheng Shen",
      "Zheng Xin Yong",
      "Harshit Pandey",
      "Rachel Bawden",
      "Thomas Wang",
      "Trishala Neeraj",
      "Jos Rozen",
      "Abheesht Sharma",
      "Andrea Santilli",
      "Thibault Fevry",
      "Jason Alan Fries",
      "Ryan Teehan",
      "Stella Biderman",
      "Leo Gao",
      "Tali Bers",
      "Thomas Wolf",
      "Alexander M. Rush"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08207"
  },
  {
    "id": "arXiv:2110.08212",
    "title": "NNK-Means: Dictionary Learning using Non-Negative Kernel regression",
    "abstract": "An increasing number of systems are being designed by first gathering\nsignificant amounts of data, and then optimizing the system parameters directly\nusing the obtained data. Often this is done without analyzing the dataset\nstructure. As task complexity, data size, and parameters all increase to\nmillions or even billions, data summarization is becoming a major challenge. In\nthis work, we investigate data summarization via dictionary learning,\nleveraging the properties of recently introduced non-negative kernel regression\n(NNK) graphs. Our proposed NNK-Means, unlike competing techniques, such askSVD,\nlearns geometric dictionaries with atoms that lie in the input data space.\nExperiments show that summaries using NNK-Meanscan provide better\ndiscrimination compared to linear and kernel versions of kMeans and kSVD.\nMoreover, NNK-Means has a scalable implementation, with runtime complexity\nsimilar to that of kMeans.",
    "descriptor": "\nComments: Under review at ICASSP\n",
    "authors": [
      "Sarath Shekkizhar",
      "Antonio Ortega"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08212"
  },
  {
    "id": "arXiv:2110.08213",
    "title": "Towards Identity Preserving Normal to Dysarthric Voice Conversion",
    "abstract": "We present a voice conversion framework that converts normal speech into\ndysarthric speech while preserving the speaker identity. Such a framework is\nessential for (1) clinical decision making processes and alleviation of patient\nstress, (2) data augmentation for dysarthric speech recognition. This is an\nespecially challenging task since the converted samples should capture the\nseverity of dysarthric speech while being highly natural and possessing the\nspeaker identity of the normal speaker. To this end, we adopted a two-stage\nframework, which consists of a sequence-to-sequence model and a nonparallel\nframe-wise model. Objective and subjective evaluations were conducted on the\nUASpeech dataset, and results showed that the method was able to yield\nreasonable naturalness and capture severity aspects of the pathological speech.\nOn the other hand, the similarity to the normal source speaker's voice was\nlimited and requires further improvements.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Wen-Chin Huang",
      "Bence Mark Halpern",
      "Lester Phillip Violeta",
      "Odette Scharenborg",
      "Tomoki Toda"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.08213"
  },
  {
    "id": "arXiv:2110.08214",
    "title": "Incremental Speech Synthesis For Speech-To-Speech Translation",
    "abstract": "In a speech-to-speech translation (S2ST) pipeline, the text-to-speech (TTS)\nmodule is an important component for delivering the translated speech to users.\nTo enable incremental S2ST, the TTS module must be capable of synthesizing and\nplaying utterances while its input text is still streaming in. In this work, we\nfocus on improving the incremental synthesis performance of TTS models. With a\nsimple data augmentation strategy based on prefixes, we are able to improve the\nincremental TTS quality to approach offline performance. Furthermore, we bring\nour incremental TTS system to the practical scenario in combination with an\nupstream simultaneous speech translation system, and show the gains also carry\nover to this use-case. In addition, we propose latency metrics tailored to S2ST\napplications, and investigate methods for latency reduction in this context.",
    "descriptor": "\nComments: Work-in-progress\n",
    "authors": [
      "Danni Liu",
      "Changhan Wang",
      "Hongyu Gong",
      "Xutai Ma",
      "Yun Tang",
      "Juan Pino"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.08214"
  },
  {
    "id": "arXiv:2110.08220",
    "title": "Combining Diverse Feature Priors",
    "abstract": "To improve model generalization, model designers often restrict the features\nthat their models use, either implicitly or explicitly. In this work, we\nexplore the design space of leveraging such feature priors by viewing them as\ndistinct perspectives on the data. Specifically, we find that models trained\nwith diverse sets of feature priors have less overlapping failure modes, and\ncan thus be combined more effectively. Moreover, we demonstrate that jointly\ntraining such models on additional (unlabeled) data allows them to correct each\nother's mistakes, which, in turn, leads to better generalization and resilience\nto spurious correlations. Code available at\nhttps://github.com/MadryLab/copriors.",
    "descriptor": "",
    "authors": [
      "Saachi Jain",
      "Dimitris Tsipras",
      "Aleksander Madry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08220"
  },
  {
    "id": "arXiv:2110.08221",
    "title": "Metrics and Design of an Instruction Roofline Model for AMD GPUs",
    "abstract": "Due to the recent announcement of the Frontier supercomputer, many scientific\napplication developers are working to make their applications compatible with\nAMD architectures (CPU-GPU), which means moving away from the traditional CPU\nand NVIDIA-GPU systems. Due to the current limitations of profiling tools for\nAMD GPUs, this shift leaves a void in how to measure application performance on\nAMD GPUs. In this paper, we design an instruction roofline model for AMD GPUs\nusing AMD's ROCProfiler and a benchmarking tool, BabelStream (the HIP\nimplementation), as a way to measure an application's performance in\ninstructions and memory transactions on new AMD hardware. Specifically, we\ncreate instruction roofline models for a case study scientific application,\nPIConGPU, an open source particle-in-cell (PIC) simulations application used\nfor plasma and laser-plasma physics on the NVIDIA V100, AMD Radeon Instinct\nMI60, and AMD Instinct MI100 GPUs. When looking at the performance of multiple\nkernels of interest in PIConGPU we find that although the AMD MI100 GPU\nachieves a similar, or better, execution time compared to the NVIDIA V100 GPU,\nprofiling tool differences make comparing performance of these two\narchitectures hard. When looking at execution time, GIPS, and instruction\nintensity, the AMD MI60 achieves the worst performance out of the three GPUs\nused in this work.",
    "descriptor": "\nComments: 14 pages, 7 figures, 2 tables, 4 equations, explains how to create an instruction roofline model for an AMD GPU as of Oct. 2021\n",
    "authors": [
      "Matthew Leinhauser",
      "Ren\u00e9 Widera",
      "Sergei Bastrakov",
      "Alexander Debus",
      "Michael Bussmann",
      "Sunita Chandrasekaran"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.08221"
  },
  {
    "id": "arXiv:2110.08222",
    "title": "DialFact: A Benchmark for Fact-Checking in Dialogue",
    "abstract": "Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation, however, it has been often explored to verify formal\nsingle-sentence claims instead of casual conversational claims. To study the\nproblem, we introduce the task of fact-checking in dialogue. We construct\nDialFact, a testing benchmark dataset of 22,245 annotated conversational\nclaims, paired with pieces of evidence from Wikipedia. There are three\nsub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether\na response carries verifiable factual information; 2) Evidence retrieval task\nretrieves the most relevant Wikipedia snippets as evidence; 3) Claim\nverification task predicts a dialogue response to be supported, refuted, or not\nenough information. We found that existing fact-checking models trained on\nnon-dialogue data like FEVER fail to perform well on our task, and thus, we\npropose a simple yet data-efficient solution to effectively improve\nfact-checking performance in dialogue. We point out unique challenges in\nDialFact such as handling the colloquialisms, coreferences, and retrieval\nambiguities in the error analysis to shed light on future research in this\ndirection.",
    "descriptor": "",
    "authors": [
      "Prakhar Gupta",
      "Chien-Sheng Wu",
      "Wenhao Liu",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08222"
  },
  {
    "id": "arXiv:2110.08223",
    "title": "VICause: Simultaneous Missing Value Imputation and Causal Discovery with  Groups",
    "abstract": "Missing values constitute an important challenge in real-world machine\nlearning for both prediction and causal discovery tasks. However, existing\nimputation methods are agnostic to causality, while only few methods in\ntraditional causal discovery can handle missing data in an efficient way. In\nthis work we propose VICause, a novel approach to simultaneously tackle missing\nvalue imputation and causal discovery efficiently with deep learning.\nParticularly, we propose a generative model with a structured latent space and\na graph neural network-based architecture, scaling to large number of\nvariables. Moreover, our method can discover relationships between groups of\nvariables which is useful in many real-world applications. VICause shows\nimproved performance compared to popular and recent approaches in both missing\nvalue imputation and causal discovery.",
    "descriptor": "",
    "authors": [
      "Pablo Morales-Alvarez",
      "Angus Lamb",
      "Simon Woodhead",
      "Simon Peyton Jones",
      "Miltiadis Allamanis",
      "Cheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08223"
  },
  {
    "id": "arXiv:2110.08226",
    "title": "Guiding Visual Question Generation",
    "abstract": "In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.",
    "descriptor": "\nComments: 11 pages including references and Appendix. 3 figures and 3 tables\n",
    "authors": [
      "Nihir Vedd",
      "Zixu Wang",
      "Marek Rei",
      "Yishu Miao",
      "Lucia Specia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08226"
  },
  {
    "id": "arXiv:2110.08228",
    "title": "Cross-Domain Data Integration for Named Entity Disambiguation in  Biomedical Text",
    "abstract": "Named entity disambiguation (NED), which involves mapping textual mentions to\nstructured entities, is particularly challenging in the medical domain due to\nthe presence of rare entities. Existing approaches are limited by the presence\nof coarse-grained structural resources in biomedical knowledge bases as well as\nthe use of training datasets that provide low coverage over uncommon resources.\nIn this work, we address these issues by proposing a cross-domain data\nintegration method that transfers structural knowledge from a general text\nknowledge base to the medical domain. We utilize our integration scheme to\naugment structural resources and generate a large biomedical NED dataset for\npretraining. Our pretrained model with injected structural knowledge achieves\nstate-of-the-art performance on two benchmark medical NED datasets: MedMentions\nand BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57\naccuracy points.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2021\n",
    "authors": [
      "Maya Varma",
      "Laurel Orr",
      "Sen Wu",
      "Megan Leszczynski",
      "Xiao Ling",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08228"
  },
  {
    "id": "arXiv:2110.08229",
    "title": "Influencing Towards Stable Multi-Agent Interactions",
    "abstract": "Learning in multi-agent environments is difficult due to the non-stationarity\nintroduced by an opponent's or partner's changing behaviors. Instead of\nreactively adapting to the other agent's (opponent or partner) behavior, we\npropose an algorithm to proactively influence the other agent's strategy to\nstabilize -- which can restrain the non-stationarity caused by the other agent.\nWe learn a low-dimensional latent representation of the other agent's strategy\nand the dynamics of how the latent strategy evolves with respect to our robot's\nbehavior. With this learned dynamics model, we can define an unsupervised\nstability reward to train our robot to deliberately influence the other agent\nto stabilize towards a single strategy. We demonstrate the effectiveness of\nstabilizing in improving efficiency of maximizing the task reward in a variety\nof simulated environments, including autonomous driving, emergent\ncommunication, and robotic manipulation. We show qualitative results on our\nwebsite: https://sites.google.com/view/stable-marl/.",
    "descriptor": "\nComments: 15 pages, 5 figures, Published as an Oral at Conference on Robot Learning (CoRL) 2021\n",
    "authors": [
      "Woodrow Z. Wang",
      "Andy Shih",
      "Annie Xie",
      "Dorsa Sadigh"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.08229"
  },
  {
    "id": "arXiv:2110.08232",
    "title": "Fire Together Wire Together: A Dynamic Pruning Approach with  Self-Supervised Mask Prediction",
    "abstract": "Dynamic model pruning is a recent direction that allows for the inference of\na different sub-network for each input sample during deployment. However,\ncurrent dynamic methods rely on learning a continuous channel gating through\nregularization by inducing sparsity loss. This formulation introduces\ncomplexity in balancing different losses (e.g task loss, regularization loss).\nIn addition, regularization-based methods lack transparent tradeoff\nhyperparameter selection to realize computational budget. Our contribution is\ntwofold: 1) decoupled task and pruning training. 2) Simple hyperparameter\nselection that enables FLOPs reduction estimation before training. We propose\nto predict a mask to process k filters in a layer based on the activation of\nits previous layer. We pose the problem as a self-supervised binary\nclassification problem. Each mask predictor module is trained to predict if the\nlog-likelihood of each filter in the current layer belongs to the top-k\nactivated filters. The value k is dynamically estimated for each input based on\na novel criterion using the mass of heatmaps. We show experiments on several\nneural architectures, such as VGG, ResNet, and MobileNet on CIFAR and ImageNet\ndatasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24%\nhigher FLOPs reduction. Similarly in ImageNet, we achieve a lower drop in\naccuracy with up to 13% improvement in FLOPs reduction.",
    "descriptor": "",
    "authors": [
      "Sara Elkerdawy",
      "Mostafa Elhoushi",
      "Hong Zhang",
      "Nilanjan Ray"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08232"
  },
  {
    "id": "arXiv:2110.08239",
    "title": "Learn Proportional Derivative Controllable Latent Space from Pixels",
    "abstract": "Recent advances in latent space dynamics model from pixels show promising\nprogress in vision-based model predictive control (MPC). However, executing MPC\nin real time can be challenging due to its intensive computational cost in each\ntimestep. We propose to introduce additional learning objectives to enforce\nthat the learned latent space is proportional derivative controllable. In\nexecution time, the simple PD-controller can be applied directly to the latent\nspace encoded from pixels, to produce simple and effective control to systems\nwith visual observations. We show that our method outperforms baseline methods\nto produce robust goal reaching and trajectory tracking in various\nenvironments.",
    "descriptor": "",
    "authors": [
      "Weiyao Wang",
      "Marin Kobilarov",
      "Gregory D. Hager"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08239"
  },
  {
    "id": "arXiv:2110.08241",
    "title": "Intent-based Product Collections for E-commerce using Pretrained  Language Models",
    "abstract": "Building a shopping product collection has been primarily a human job. With\nthe manual efforts of craftsmanship, experts collect related but diverse\nproducts with common shopping intent that are effective when displayed\ntogether, e.g., backpacks, laptop bags, and messenger bags for freshman bag\ngifts. Automatically constructing a collection requires an ML system to learn a\ncomplex relationship between the customer's intent and the product's\nattributes. However, there have been challenging points, such as 1) long and\ncomplicated intent sentences, 2) rich and diverse product attributes, and 3) a\nhuge semantic gap between them, making the problem difficult. In this paper, we\nuse a pretrained language model (PLM) that leverages textual attributes of\nweb-scale products to make intent-based product collections. Specifically, we\ntrain a BERT with triplet loss by setting an intent sentence to an anchor and\ncorresponding products to positive examples. Also, we improve the performance\nof the model by search-based negative sampling and category-wise positive pair\naugmentation. Our model significantly outperforms the search-based baseline\nmodel for intent-based product matching in offline evaluations. Furthermore,\nonline experimental results on our e-commerce platform show that the PLM-based\nmethod can construct collections of products with increased CTR, CVR, and\norder-diversity compared to expert-crafted collections.",
    "descriptor": "\nComments: Accepted to IEEE International Workshop on Data Mining for Service (DMS2021)\n",
    "authors": [
      "Hiun Kim",
      "Jisu Jeong",
      "Kyung-Min Kim",
      "Dongjun Lee",
      "Hyun Dong Lee",
      "Dongpil Seo",
      "Jeeseung Han",
      "Dong Wook Park",
      "Ji Ae Heo",
      "Rak Yeong Kim"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08241"
  },
  {
    "id": "arXiv:2110.08242",
    "title": "Evolving spiking neuron cellular automata and networks to emulate in  vitro neuronal activity",
    "abstract": "Neuro-inspired models and systems have great potential for applications in\nunconventional computing. Often, the mechanisms of biological neurons are\nmodeled or mimicked in simulated or physical systems in an attempt to harness\nsome of the computational power of the brain. However, the biological\nmechanisms at play in neural systems are complicated and challenging to capture\nand engineer; thus, it can be simpler to turn to a data-driven approach to\ntransfer features of neural behavior to artificial substrates. In the present\nstudy, we used an evolutionary algorithm (EA) to produce spiking neural systems\nthat emulate the patterns of behavior of biological neurons in vitro. The aim\nof this approach was to develop a method of producing models capable of\nexhibiting complex behavior that may be suitable for use as computational\nsubstrates. Our models were able to produce a level of network-wide synchrony\nand showed a range of behaviors depending on the target data used for their\nevolution, which was from a range of neuronal culture densities and maturities.\nThe genomes of the top-performing models indicate the excitability and density\nof connections in the model play an important role in determining the\ncomplexity of the produced activity.",
    "descriptor": "\nComments: To be published in proceedings of IEEE SSCI 2021 as part of ICES symposium (International Conference on Evolvable Systems, IEEE Symposium Series on Computational Intelligence 2021)\n",
    "authors": [
      "J\u00f8rgen Jensen Farner",
      "H\u00e5kon Weydahl",
      "Ruben Jahren",
      "Ola Huse Ramstad",
      "Stefano Nichele",
      "Kristine Heiney"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.08242"
  },
  {
    "id": "arXiv:2110.08244",
    "title": "Performance, Successes and Limitations of Deep Learning Semantic  Segmentation of Multiple Defects in Transmission Electron Micrographs",
    "abstract": "In this work, we perform semantic segmentation of multiple defect types in\nelectron microscopy images of irradiated FeCrAl alloys using a deep learning\nMask Regional Convolutional Neural Network (Mask R-CNN) model. We conduct an\nin-depth analysis of key model performance statistics, with a focus on\nquantities such as predicted distributions of defect shapes, defect sizes, and\ndefect areal densities relevant to informing modeling and understanding of\nirradiated Fe-based materials properties. To better understand the performance\nand present limitations of the model, we provide examples of useful evaluation\ntests which include a suite of random splits, and dataset size-dependent and\ndomain-targeted cross validation tests. Overall, we find that the current model\nis a fast, effective tool for automatically characterizing and quantifying\nmultiple defect types in microscopy images, with a level of accuracy on par\nwith human domain expert labelers. More specifically, the model can achieve\naverage defect identification F1 scores as high as 0.8, and, based on random\ncross validation, have low overall average (+/- standard deviation) defect size\nand density percentage errors of 7.3 (+/- 3.8)% and 12.7 (+/- 5.3)%,\nrespectively. Further, our model predicts the expected material hardening to\nwithin 10-20 MPa (about 10% of total hardening), which is about the same error\nlevel as experiments. Our targeted evaluation tests also suggest the best path\ntoward improving future models is not expanding existing databases with more\nlabeled images but instead data additions that target weak points of the model\ndomain, such as images from different microscopes, imaging conditions,\nirradiation environments, and alloy types. Finally, we discuss the first phase\nof an effort to provide an easy-to-use, open-source object detection tool to\nthe broader community for identifying defects in new images.",
    "descriptor": "",
    "authors": [
      "Ryan Jacobs",
      "Mingren Shen",
      "Yuhan Liu",
      "Wei Hao",
      "Xiaoshan Li",
      "Ruoyu He",
      "Jacob RC Greaves",
      "Donglin Wang",
      "Zeming Xie",
      "Zitong Huang",
      "Chao Wang",
      "Kevin G. Field",
      "Dane Morgan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Materials Science (cond-mat.mtrl-sci)"
    ],
    "url": "https://arxiv.org/abs/2110.08244"
  },
  {
    "id": "arXiv:2110.08245",
    "title": "LPRules: Rule Induction in Knowledge Graphs Using Linear Programming",
    "abstract": "Knowledge graph (KG) completion is a well-studied problem in AI. Rule-based\nmethods and embedding-based methods form two of the solution techniques.\nRule-based methods learn first-order logic rules that capture existing facts in\nan input graph and then use these rules for reasoning about missing facts. A\nmajor drawback of such methods is the lack of scalability to large datasets. In\nthis paper, we present a simple linear programming (LP) model to choose rules\nfrom a list of candidate rules and assign weights to them. For smaller KGs, we\nuse simple heuristics to create the candidate list. For larger KGs, we start\nwith a small initial candidate list, and then use standard column generation\nideas to add more rules in order to improve the LP model objective value. To\nfoster interpretability and generalizability, we limit the complexity of the\nset of chosen rules via explicit constraints, and tune the complexity\nhyperparameter for individual datasets. We show that our method can obtain\nstate-of-the-art results for three out of four widely used KG datasets, while\ntaking significantly less computing time than other popular rule learners\nincluding some based on neuro-symbolic methods. The improved scalability of our\nmethod allows us to tackle large datasets such as YAGO3-10.",
    "descriptor": "",
    "authors": [
      "Sanjeeb Dash",
      "Joao Goncalves"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08245"
  },
  {
    "id": "arXiv:2110.08246",
    "title": "Tricks for Training Sparse Translation Models",
    "abstract": "Multi-task learning with an unbalanced data distribution skews model learning\ntowards high resource tasks, especially when model capacity is fixed and fully\nshared across all tasks. Sparse scaling architectures, such as BASELayers,\nprovide flexible mechanisms for different tasks to have a variable number of\nparameters, which can be useful to counterbalance skewed data distributions. We\nfind that that sparse architectures for multilingual machine translation can\nperform poorly out of the box, and propose two straightforward techniques to\nmitigate this - a temperature heating mechanism and dense pre-training.\nOverall, these methods improve performance on two multilingual translation\nbenchmarks compared to standard BASELayers and Dense scaling baselines, and in\ncombination, more than 2x model convergence speed.",
    "descriptor": "",
    "authors": [
      "Dheeru Dua",
      "Shruti Bhosale",
      "Vedanuj Goswami",
      "James Cross",
      "Mike Lewis",
      "Angela Fan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08246"
  },
  {
    "id": "arXiv:2110.08247",
    "title": "Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks",
    "abstract": "Backdoor attacks are a kind of emergent security threat in deep learning.\nWhen a deep neural model is injected with a backdoor, it will behave normally\non standard inputs but give adversary-specified predictions once the input\ncontains specific backdoor triggers. Current textual backdoor attacks have poor\nattack performance in some tough situations. In this paper, we find two simple\ntricks that can make existing textual backdoor attacks much more harmful. The\nfirst trick is to add an extra training task to distinguish poisoned and clean\ndata during the training of the victim model, and the second one is to use all\nthe clean training data rather than remove the original clean data\ncorresponding to the poisoned data. These two tricks are universally applicable\nto different attack models. We conduct experiments in three tough situations\nincluding clean data fine-tuning, low poisoning rate, and label-consistent\nattacks. Experimental results show that the two tricks can significantly\nimprove attack performance. This paper exhibits the great potential harmfulness\nof backdoor attacks. All the code and data will be made public to facilitate\nfurther research.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Yangyi Chen",
      "Fanchao Qi",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08247"
  },
  {
    "id": "arXiv:2110.08248",
    "title": "Transforming Autoregression: Interpretable and Expressive Time Series  Forecast",
    "abstract": "Probabilistic forecasting of time series is an important matter in many\napplications and research fields. In order to draw conclusions from a\nprobabilistic forecast, we must ensure that the model class used to approximate\nthe true forecasting distribution is expressive enough. Yet, characteristics of\nthe model itself, such as its uncertainty or its general functioning are not of\nlesser importance. In this paper, we propose Autoregressive Transformation\nModels (ATMs), a model class inspired from various research directions such as\nnormalizing flows and autoregressive models. ATMs unite expressive\ndistributional forecasts using a semi-parametric distribution assumption with\nan interpretable model specification and allow for uncertainty quantification\nbased on (asymptotic) Maximum Likelihood theory. We demonstrate the properties\nof ATMs both theoretically and through empirical evaluation on several\nsimulated and real-world forecasting datasets.",
    "descriptor": "",
    "authors": [
      "David R\u00fcgamer",
      "Philipp F.M. Baumann",
      "Thomas Kneib",
      "Torsten Hothorn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08248"
  },
  {
    "id": "arXiv:2110.08250",
    "title": "Direct simultaneous speech to speech translation",
    "abstract": "We present the first direct simultaneous speech-to-speech translation\n(Simul-S2ST) model, with the ability to start generating translation in the\ntarget speech before consuming the full source speech content and independently\nfrom intermediate text representations. Our approach leverages recent progress\non direct speech-to-speech translation with discrete units. Instead of\ncontinuous spectrogram features, a sequence of direct representations, which\nare learned in a unsupervised manner, are predicted from the model and passed\ndirectly to a vocoder for speech synthesis. The simultaneous policy then\noperates on source speech features and target discrete units. Finally, a\nvocoder synthesize the target speech from discrete units on-the-fly. We carry\nout numerical studies to compare cascaded and direct approach on Fisher\nSpanish-English dataset.",
    "descriptor": "",
    "authors": [
      "Xutai Ma",
      "Hongyu Gong",
      "Danni Liu",
      "Ann Lee",
      "Yun Tang",
      "Peng-Jen Chen",
      "Wei-Ning Hsu",
      "Kenneth Heafield",
      "Phillip Koehn",
      "Juan Pino"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.08250"
  },
  {
    "id": "arXiv:2003.04912",
    "title": "Flip-sort and combinatorial aspects of pop-stack sorting",
    "abstract": "Flip-sort is a natural sorting procedure which raises fascinating\ncombinatorial questions. It finds its roots in the seminal work of Knuth on\nstack-based sorting algorithms and leads to many links with permutation\npatterns. We present several structural, enumerative, and algorithmic results\non permutations that need few (resp. many) iterations of this procedure to be\nsorted. In particular, we give the shape of the permutations after one\niteration, and characterize several families of permutations related to the\nbest and worst cases of flip-sort. En passant, we also give some links between\npop-stack sorting, automata, and lattice paths, and introduce several tactics\nof bijective proofs which have their own interest.",
    "descriptor": "\nComments: This v3 just updates the journal reference, according to the publisher wish\n",
    "authors": [
      "Andrei Asinowski",
      "Cyril Banderier",
      "Benjamin Hackl"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2003.04912"
  },
  {
    "id": "arXiv:2110.04924",
    "title": "High-dimensional Inference for Dynamic Treatment Effects",
    "abstract": "This paper proposes a confidence interval construction for heterogeneous\ntreatment effects in the context of multi-stage experiments with $N$ samples\nand high-dimensional, $d$, confounders. Our focus is on the case of $d\\gg N$,\nbut the results obtained also apply to low-dimensional cases. We showcase that\nthe bias of regularized estimation, unavoidable in high-dimensional covariate\nspaces, is mitigated with a simple double-robust score. In this way, no\nadditional bias removal is necessary, and we obtain root-$N$ inference results\nwhile allowing multi-stage interdependency of the treatments and covariates.\nMemoryless property is also not assumed; treatment can possibly depend on all\nprevious treatment assignments and all previous multi-stage confounders. Our\nresults rely on certain sparsity assumptions of the underlying dependencies. We\ndiscover new product rate conditions necessary for robust inference with\ndynamic treatments.",
    "descriptor": "",
    "authors": [
      "Jelena Bradic",
      "Weijie Ji",
      "Yuqian Zhang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04924"
  },
  {
    "id": "arXiv:2110.07608",
    "title": "3D Structure from 2D Microscopy images using Deep Learning",
    "abstract": "Understanding the structure of a protein complex is crucial indetermining its\nfunction. However, retrieving accurate 3D structures from microscopy images is\nhighly challenging, particularly as many imaging modalities are\ntwo-dimensional. Recent advances in Artificial Intelligence have been applied\nto this problem, primarily using voxel based approaches to analyse sets of\nelectron microscopy images. Herewe present a deep learning solution for\nreconstructing the protein com-plexes from a number of 2D single molecule\nlocalization microscopy images, with the solution being completely\nunconstrained. Our convolutional neural network coupled with a differentiable\nrenderer predicts pose and derives a single structure. After training, the\nnetwork is dis-carded, with the output of this method being a structural model\nwhich fits the data-set. We demonstrate the performance of our system on two\nprotein complexes: CEP152 (which comprises part of the proximal toroid of the\ncentriole) and centrioles.",
    "descriptor": "\nComments: 32 Pages, 12 figures. Awaiting publication in 'Frontiers in Bioinformatics - Computational Bioimaging' - this https URL\n",
    "authors": [
      "Benjamin J. Blundell",
      "Christian Sieben",
      "Suliana Manley",
      "Ed Rosten",
      "QueeLim Ch'ng",
      "Susan Cox"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07608"
  },
  {
    "id": "arXiv:2110.07610",
    "title": "Non-contact Atrial Fibrillation Detection from Face Videos by Learning  Systolic Peaks",
    "abstract": "Objective: We propose a non-contact approach for atrial fibrillation (AF)\ndetection from face videos. Methods: Face videos, electrocardiography (ECG),\nand contact photoplethysmography (PPG) from 100 healthy subjects and 100 AF\npatients are recorded. All the videos in the healthy group are labeled as\nhealthy. Videos in the patient group are labeled as AF, sinus rhythm (SR), or\natrial flutter (AFL) by cardiologists. We use the 3D convolutional neural\nnetwork for remote PPG measurement and propose a novel loss function\n(Wasserstein distance) to use the timing of systolic peaks from contact PPG as\nthe label for our model training. Then a set of heart rate variability (HRV)\nfeatures are calculated from the inter-beat intervals, and a support vector\nmachine (SVM) classifier is trained with HRV features. Results: Our proposed\nmethod can accurately extract systolic peaks from face videos for AF detection.\nThe proposed method is trained with subject-independent 10-fold\ncross-validation with 30s video clips and tested on two tasks. 1)\nClassification of healthy versus AF: the accuracy, sensitivity, and specificity\nare 96.16%, 95.71%, and 96.23%. 2) Classification of SR versus AF: the\naccuracy, sensitivity, and specificity are 95.31%, 98.66%, and 91.11%.\nConclusion: We achieve good performance of non-contact AF detection by learning\nsystolic peaks. Significance: non-contact AF detection can be used for\nself-screening of AF symptom for suspectable populations at home, or\nself-monitoring of AF recurrence after treatment for the chronical patients.",
    "descriptor": "",
    "authors": [
      "Zhaodong Sun",
      "Juhani Junttila",
      "Mikko Tulppo",
      "Tapio Sepp\u00e4nen",
      "Xiaobai Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07610"
  },
  {
    "id": "arXiv:2110.07618",
    "title": "Sparse Implicit Processes for Approximate Inference",
    "abstract": "Implicit Processes (IPs) are flexible priors that can describe models such as\nBayesian neural networks, neural samplers and data generators. IPs allow for\napproximate inference in function-space. This avoids some degenerate problems\nof parameter-space approximate inference due to the high number of parameters\nand strong dependencies. For this, an extra IP is often used to approximate the\nposterior of the prior IP. However, simultaneously adjusting the parameters of\nthe prior IP and the approximate posterior IP is a challenging task. Existing\nmethods that can tune the prior IP result in a Gaussian predictive\ndistribution, which fails to capture important data patterns. By contrast,\nmethods producing flexible predictive distributions by using another IP to\napproximate the posterior process cannot fit the prior IP to the observed data.\nWe propose here a method that can carry out both tasks. For this, we rely on an\ninducing-point representation of the prior IP, as often done in the context of\nsparse Gaussian processes. The result is a scalable method for approximate\ninference with IPs that can tune the prior IP parameters to the data, and that\nprovides accurate non-Gaussian predictive distributions.",
    "descriptor": "\nComments: 10 pages for the main text (with 3 figures and 1 table), and 9 pages of supplementary material (with 6 figures and 3 tables)\n",
    "authors": [
      "Sim\u00f3n Rodr\u00edguez Santana",
      "Bryan Zaldivar",
      "Daniel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07618"
  },
  {
    "id": "arXiv:2110.07649",
    "title": "Federated learning and next generation wireless communications: A survey  on bidirectional relationship",
    "abstract": "In order to meet the extremely heterogeneous requirements of the next\ngeneration wireless communication networks, research community is increasingly\ndependent on using machine learning solutions for real-time decision-making and\nradio resource management. Traditional machine learning employs fully\ncentralized architecture in which the entire training data is collected at one\nnode e.g., cloud server, that significantly increases the communication\noverheads and also raises severe privacy concerns. Towards this end, a\ndistributed machine learning paradigm termed as Federated learning (FL) has\nbeen proposed recently. In FL, each participating edge device trains its local\nmodel by using its own training data. Then, via the wireless channels the\nweights or parameters of the locally trained models are sent to the central PS,\nthat aggregates them and updates the global model. On one hand, FL plays an\nimportant role for optimizing the resources of wireless communication networks,\non the other hand, wireless communications is crucial for FL. Thus, a\n`bidirectional' relationship exists between FL and wireless communications.\nAlthough FL is an emerging concept, many publications have already been\npublished in the domain of FL and its applications for next generation wireless\nnetworks. Nevertheless, we noticed that none of the works have highlighted the\nbidirectional relationship between FL and wireless communications. Therefore,\nthe purpose of this survey paper is to bridge this gap in literature by\nproviding a timely and comprehensive discussion on the interdependency between\nFL and wireless communications.",
    "descriptor": "\nComments: 18 pages, 6 figures\n",
    "authors": [
      "Debaditya Shome",
      "Omer Waqar",
      "Wali Ullah Khan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07649"
  },
  {
    "id": "arXiv:2110.07698",
    "title": "Directed Percolation in Random Temporal Network Models with  Heterogeneities",
    "abstract": "The event graph representation of temporal networks suggests that the\nconnectivity of temporal structures can be mapped to a directed percolation\nproblem. However, similar to percolation theory on static networks, this\nmapping is valid under the approximation that the structure and interaction\ndynamics of the temporal network are determined by its local properties, and\notherwise, it is maximally random. We challenge these conditions and\ndemonstrate the robustness of this mapping in case of more complicated systems.\nWe systematically analyze random and regular network topologies and\nheterogeneous link-activation processes driven by bursty renewal or\nself-exciting processes using numerical simulation and finite-size scaling\nmethods. We find that the critical percolation exponents characterizing the\ntemporal network are not sensitive to many structural and dynamical network\nheterogeneities, while they recover known scaling exponents characterizing\ndirected percolation on low dimensional lattices. While it is not possible to\ndemonstrate the validity of this mapping for all temporal network models, our\nresults establish the first batch of evidence supporting the robustness of the\nscaling relationships in the limited-time reachability of temporal networks.",
    "descriptor": "\nComments: Implementation available at this https URL\n",
    "authors": [
      "Arash Badie-Modiri",
      "Abbas K. Rizi",
      "M\u00e1rton Karsai",
      "Mikko Kivel\u00e4"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.07698"
  },
  {
    "id": "arXiv:2110.07711",
    "title": "Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w  MRI of Human Brain Hemispheres",
    "abstract": "Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy. However, automated\ncortical segmentation methods in ex vivo MRI are not well developed, primarily\ndue to limited availability of labeled datasets, and heterogeneity in scanner\nhardware and acquisition protocols. In this work, we present a high resolution\n7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical\nmantle segmentation performance of nine neural network architectures, trained\nand evaluated using manually-segmented 3D patches sampled from specific\ncortical regions, and show excellent generalizing capabilities across whole\nbrain hemispheres in different specimens, and also on unseen images acquired at\ndifferent magnetic field strength and imaging sequences. Finally, we provide\ncortical thickness measurements across key regions in 3D ex vivo human brain\nimages. Our code and processed datasets are publicly available at\nhttps://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.",
    "descriptor": "\nComments: Submitted to IEEE International Symposium on Biomedical Imaging (ISBI) 2022\n",
    "authors": [
      "Pulkit Khandelwal",
      "Shokufeh Sadaghiani",
      "Sadhana Ravikumar",
      "Sydney Lim",
      "Sanaz Arezoumandan",
      "Claire Peterson",
      "Eunice Chung",
      "Madigan Bedard",
      "Noah Capp",
      "Ranjit Ittyerah",
      "Elyse Migdal",
      "Grace Choi",
      "Emily Kopp",
      "Bridget Loja",
      "Eusha Hasan",
      "Jiacheng Li",
      "Karthik Prabhakaran",
      "Gabor Mizsei",
      "Marianna Gabrielyan",
      "Theresa Schuck",
      "John Robinson",
      "Daniel Ohm",
      "Edward Lee",
      "John Q. Trojanowski",
      "Corey McMillan",
      "Murray Grossman",
      "David Irwin",
      "M. Dylan Tisdall",
      "Sandhitsu R. Das",
      "Laura E.M. Wisse",
      "David A. Wolk",
      "Paul A. Yushkevich"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07711"
  },
  {
    "id": "arXiv:2110.07728",
    "title": "Pre-training Molecular Graph Representation with 3D Geometry",
    "abstract": "Molecular graph representation learning is a fundamental problem in modern\ndrug and material discovery. Molecular graphs are typically modeled by their 2D\ntopological structures, but it has been recently discovered that 3D geometric\ninformation plays a more vital role in predicting molecular functionalities.\nHowever, the lack of 3D information in real-world scenarios has significantly\nimpeded the learning of geometric graph representation. To cope with this\nchallenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework\nwhere self-supervised learning (SSL) is performed by leveraging the\ncorrespondence and consistency between 2D topological structures and 3D\ngeometric views. GraphMVP effectively learns a 2D molecular graph encoder that\nis enhanced by richer and more discriminative 3D geometry. We further provide\ntheoretical insights to justify the effectiveness of GraphMVP. Finally,\ncomprehensive experiments show that GraphMVP can consistently outperform\nexisting graph SSL methods.",
    "descriptor": "",
    "authors": [
      "Shengchao Liu",
      "Hanchen Wang",
      "Weiyang Liu",
      "Joan Lasenby",
      "Hongyu Guo",
      "Jian Tang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.07728"
  },
  {
    "id": "arXiv:2110.07739",
    "title": "Model-Change Active Learning in Graph-Based Semi-Supervised Learning",
    "abstract": "Active learning in semi-supervised classification involves introducing\nadditional labels for unlabelled data to improve the accuracy of the underlying\nclassifier. A challenge is to identify which points to label to best improve\nperformance while limiting the number of new labels. \"Model-change\" active\nlearning quantifies the resulting change incurred in the classifier by\nintroducing the additional label(s). We pair this idea with graph-based\nsemi-supervised learning methods, that use the spectrum of the graph Laplacian\nmatrix, which can be truncated to avoid prohibitively large computational and\nstorage costs. We consider a family of convex loss functions for which the\nacquisition function can be efficiently approximated using the Laplace\napproximation of the posterior distribution. We show a variety of multiclass\nexamples that illustrate improved performance over prior state-of-art.",
    "descriptor": "\nComments: Submitted to SIAM Journal on Mathematics of Data Science (SIMODS)\n",
    "authors": [
      "Kevin Miller",
      "Andrea L. Bertozzi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07739"
  },
  {
    "id": "arXiv:2110.07744",
    "title": "Constrained Covariance Steering Based Tube-MPPI",
    "abstract": "In this paper, we present a new trajectory optimization algorithm for\nstochastic linear systems which combines Model Predictive Path Integral (MPPI)\ncontrol with Constrained Covariance Steering (CSS) to achieve high performance\nwith safety guarantees (robustness). Although MPPI can be used to solve complex\nnonlinear trajectory optimization problems, it may not always handle\nconstraints effectively and its performance may degrade in the presence of\nunmodeled disturbances.\nBy contrast, CCS can handle probabilistic state and / or input constraints\n(e.g., chance constraints) and also steer the state covariance of the system to\na desired positive definite matrix (control of uncertainty) which both imply\nthat CCS can provide robustness against stochastic disturbances. CCS, however,\nsuffers from scalability issues and cannot handle complex cost functions in\ngeneral.\nWe argue that the combination of the two methods yields a class of trajectory\noptimization algorithms that can achieve high performance (a feature of MPPI)\nwhile ensuring safety with high probability (a feature of CCS).\nThe efficacy of our algorithm is demonstrated in an obstacle avoidance\nproblem and a circular track path generation problem.",
    "descriptor": "",
    "authors": [
      "Isin M. Balci",
      "Efstathios Bakolas",
      "Bogdan Vlahov",
      "Evangelos Theodorou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.07744"
  },
  {
    "id": "arXiv:2110.07745",
    "title": "A deep learning model for classification of diabetic retinopathy in eye  fundus images based on retinal lesion detection",
    "abstract": "Diabetic retinopathy (DR) is the result of a complication of diabetes\naffecting the retina. It can cause blindness, if left undiagnosed and\nuntreated. An ophthalmologist performs the diagnosis by screening each patient\nand analyzing the retinal lesions via ocular imaging. In practice, such\nanalysis is time-consuming and cumbersome to perform. This paper presents a\nmodel for automatic DR classification on eye fundus images. The approach\nidentifies the main ocular lesions related to DR and subsequently diagnoses the\nillness. The proposed method follows the same workflow as the clinicians,\nproviding information that can be interpreted clinically to support the\nprediction. A subset of the kaggle EyePACS and the Messidor-2 datasets, labeled\nwith ocular lesions, is made publicly available. The kaggle EyePACS subset is\nused as a training set and the Messidor-2 as a test set for lesions and DR\nclassification models. For DR diagnosis, our model has an area-under-the-curve,\nsensitivity, and specificity of 0.948, 0.886, and 0.875, respectively, which\ncompetes with state-of-the-art approaches.",
    "descriptor": "\nComments: 7 pages and 1 figure\n",
    "authors": [
      "Melissa delaPava",
      "Hern\u00e1n R\u00edos",
      "Francisco J. Rodr\u00edguez",
      "Oscar J. Perdomo",
      "Fabio A. Gonz\u00e1lez"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07745"
  },
  {
    "id": "arXiv:2110.07756",
    "title": "Learning Mean-Field Equations from Particle Data Using WSINDy",
    "abstract": "We develop a weak-form sparse identification method for interacting particle\nsystems (IPS) with the primary goals of reducing computational complexity for\nlarge particle number $N$ and offering robustness to either intrinsic or\nextrinsic noise. In particular, we use concepts from mean-field theory of IPS\nin combination with the weak-form sparse identification of nonlinear dynamics\nalgorithm (WSINDy) to provide a fast and reliable system identification scheme\nfor recovering the governing stochastic differential equations for an IPS when\nthe number of particles per experiment $N$ is on the order of several thousand\nand the number of experiments $M$ is less than 100. This is in contrast to\nexisting work showing that system identification for $N$ less than 100 and $M$\non the order of several thousand is feasible using strong-form methods. We\nprove that under some standard regularity assumptions the scheme converges with\nrate $\\mathcal{O}(N^{-1/2})$ in the ordinary least squares setting and we\ndemonstrate the convergence rate numerically on several systems in one and two\nspatial dimensions. Our examples include a canonical problem from\nhomogenization theory (as a first step towards learning coarse-grained models),\nthe dynamics of an attractive-repulsive swarm, and the IPS description of the\nparabolic-elliptic Keller-Segel model for chemotaxis.",
    "descriptor": "",
    "authors": [
      "Daniel A. Messenger",
      "David M. Bortz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2110.07756"
  },
  {
    "id": "arXiv:2110.07768",
    "title": "Application of Homomorphic Encryption in Medical Imaging",
    "abstract": "In this technical report, we explore the use of homomorphic encryption (HE)\nin the context of training and predicting with deep learning (DL) models to\ndeliver strict \\textit{Privacy by Design} services, and to enforce a zero-trust\nmodel of data governance. First, we show how HE can be used to make predictions\nover medical images while preventing unauthorized secondary use of data, and\ndetail our results on a disease classification task with OCT images. Then, we\ndemonstrate that HE can be used to secure the training of DL models through\nfederated learning, and report some experiments using 3D chest CT-Scans for a\nnodule detection task.",
    "descriptor": "",
    "authors": [
      "Francis Dutil",
      "Alexandre See",
      "Lisa Di Jorio",
      "Florent Chandelier"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07768"
  },
  {
    "id": "arXiv:2110.07773",
    "title": "Areas on the space of smooth probability density functions on $S^2$",
    "abstract": "We present symbolic and numerical methods for computing Poisson brackets on\nthe spaces of measures with positive densities of the plane, the 2-torus, and\nthe 2-sphere. We apply our methods to compute symplectic areas of finite\nregions for the case of the 2-sphere, including an explicit example for\nGaussian measures with positive densities.",
    "descriptor": "\nComments: 9 pages, 2 algorithms, 3 tables\n",
    "authors": [
      "J. C. Ru\u00edz-Pantale\u00f3n",
      "P. Su\u00e1rez-Serrato"
    ],
    "subjectives": [
      "Symplectic Geometry (math.SG)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.07773"
  },
  {
    "id": "arXiv:2110.07781",
    "title": "New techniques for bounding stabilizer rank",
    "abstract": "In this work, we present number-theoretic and algebraic-geometric techniques\nfor bounding the stabilizer rank of quantum states. First, we refine a\nnumber-theoretic theorem of Moulton to exhibit an explicit sequence of product\nstates with exponential stabilizer rank but constant approximate stabilizer\nrank, and to provide alternate (and simplified) proofs of the best-known\nasymptotic lower bounds on stabilizer rank and approximate stabilizer rank, up\nto a log factor. Second, we find the first non-trivial examples of quantum\nstates with multiplicative stabilizer rank under the tensor product. Third, we\nuse algebraic-geometric techniques to prove new bounds on the generic\nstabilizer rank.",
    "descriptor": "\nComments: 23 pages. Feedback welcome!\n",
    "authors": [
      "Benjamin Lovitz",
      "Vincent Steffan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2110.07781"
  },
  {
    "id": "arXiv:2110.07788",
    "title": "Gaussian Process Bandit Optimization with Few Batches",
    "abstract": "In this paper, we consider the problem of black-box optimization using\nGaussian Process (GP) bandit optimization with a small number of batches.\nAssuming the unknown function has a low norm in the Reproducing Kernel Hilbert\nSpace (RKHS), we introduce a batch algorithm inspired by batched finite-arm\nbandit algorithms, and show that it achieves the cumulative regret upper bound\n$O^\\ast(\\sqrt{T\\gamma_T})$ using $O(\\log\\log T)$ batches within time horizon\n$T$, where the $O^\\ast(\\cdot)$ notation hides dimension-independent logarithmic\nfactors and $\\gamma_T$ is the maximum information gain associated with the\nkernel. This bound is near-optimal for several kernels of interest and improves\non the typical $O^\\ast(\\sqrt{T}\\gamma_T)$ bound, and our approach is arguably\nthe simplest among algorithms attaining this improvement. In addition, in the\ncase of a constant number of batches (not depending on $T$), we propose a\nmodified version of our algorithm, and characterize how the regret is impacted\nby the number of batches, focusing on the squared exponential and Mat\\'ern\nkernels. The algorithmic upper bounds are shown to be nearly minimax optimal\nvia analogous algorithm-independent lower bounds.",
    "descriptor": "",
    "authors": [
      "Zihan Li",
      "Jonathan Scarlett"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07788"
  },
  {
    "id": "arXiv:2110.07847",
    "title": "Tight Lipschitz Hardness for Optimizing Mean Field Spin Glasses",
    "abstract": "We study the problem of algorithmically optimizing the Hamiltonian $H_N$ of a\nspherical or Ising mixed $p$-spin glass. The maximum asymptotic value\n$\\mathsf{OPT}$ of $H_N/N$ is characterized by a variational principle known as\nthe Parisi formula, proved first by Talagrand and in more generality by\nPanchenko. Recently developed approximate message passing algorithms\nefficiently optimize $H_N/N$ up to a value $\\mathsf{ALG}$ given by an extended\nParisi formula, which minimizes over a larger space of functional order\nparameters. These two objectives are equal for spin glasses exhibiting a no\noverlap gap property. However, $\\mathsf{ALG} < \\mathsf{OPT}$ can also occur,\nand no efficient algorithm producing an objective value exceeding\n$\\mathsf{ALG}$ is known.\nWe prove that for mixed even $p$-spin models, no algorithm satisfying an\noverlap concentration property can produce an objective larger than\n$\\mathsf{ALG}$ with non-negligible probability. This property holds for all\nalgorithms with suitably Lipschitz dependence on the disorder coefficients of\n$H_N$. It encompasses natural formulations of gradient descent, approximate\nmessage passing, and Langevin dynamics run for bounded time and in particular\nincludes the algorithms achieving $\\mathsf{ALG}$ mentioned above. To prove this\nresult, we substantially generalize the overlap gap property framework\nintroduced by Gamarnik and Sudan to arbitrary ultrametric forbidden structures\nof solutions.",
    "descriptor": "\nComments: 83 pages, 2 figures\n",
    "authors": [
      "Brice Huang",
      "Mark Sellke"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Computational Complexity (cs.CC)",
      "Mathematical Physics (math-ph)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.07847"
  },
  {
    "id": "arXiv:2110.07878",
    "title": "Single volume lung biomechanics from chest computed tomography using a  mode preserving generative adversarial network",
    "abstract": "Local tissue expansion of the lungs is typically derived by registering\ncomputed tomography (CT) scans acquired at multiple lung volumes. However,\nacquiring multiple scans incurs increased radiation dose, time, and cost, and\nmay not be possible in many cases, thus restricting the applicability of\nregistration-based biomechanics. We propose a generative adversarial learning\napproach for estimating local tissue expansion directly from a single CT scan.\nThe proposed framework was trained and evaluated on 2500 subjects from the\nSPIROMICS cohort. Once trained, the framework can be used as a\nregistration-free method for predicting local tissue expansion. We evaluated\nmodel performance across varying degrees of disease severity and compared its\nperformance with two image-to-image translation frameworks - UNet and Pix2Pix.\nOur model achieved an overall PSNR of 18.95 decibels, SSIM of 0.840, and\nSpearman's correlation of 0.61 at a high spatial resolution of 1 mm3.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Muhammad F. A. Chaudhary",
      "Sarah E. Gerard",
      "Di Wang",
      "Gary E. Christensen",
      "Christopher B. Cooper",
      "Joyce D. Schroeder",
      "Eric A. Hoffman",
      "Joseph M. Reinhardt"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07878"
  },
  {
    "id": "arXiv:2110.07919",
    "title": "Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor  Segmentation With Self-Supervised Pretraining",
    "abstract": "We apply an ensemble of modified TransBTS, nnU-Net, and a combination of both\nfor the segmentation task of the BraTS 2021 challenge. In fact, we change the\noriginal architecture of the TransBTS model by adding Squeeze-and-Excitation\nblocks, an increasing number of CNN layers, replacing positional encoding in\nTransformer block with a learnable Multilayer Perceptron (MLP) embeddings,\nwhich makes Transformer adjustable to any input size during inference. With\nthese modifications, we are able to largely improve TransBTS performance.\nInspired by a nnU-Net framework we decided to combine it with our modified\nTransBTS by changing the architecture inside nnU-Net to our custom model. On\nthe Validation set of BraTS 2021, the ensemble of these approaches achieves\n0.8496, 0.8698, 0.9256 Dice score and 15.72, 11.057, 3.374 HD95 for enhancing\ntumor, tumor core, and whole tumor, correspondingly. Our code is publicly\navailable.",
    "descriptor": "",
    "authors": [
      "Mariia Dobko",
      "Danylo-Ivan Kolinko",
      "Ostap Viniavskyi",
      "Yurii Yelisieiev"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07919"
  },
  {
    "id": "arXiv:2110.07932",
    "title": "A framework for the analysis of fully coupled normal and tangential  contact problems with complex interfaces",
    "abstract": "An extension to the interface finite element with eMbedded Profile for Joint\nRoughness (MPJR interface finite element) is herein proposed for solving the\nfrictional contact problem between a rigid indenter of any complex shape and an\nelastic body under generic oblique load histories. The actual shape of the\nindenter is accounted for as a correction of the gap function. A regularised\nversion of the Coulomb friction law is employed for modeling the tangential\ncontact response, while a penalty approach is introduced in the normal contact\ndirection. The development of the finite element (FE) formulation stemming from\nits variational formalism is thoroughly derived and the model is validated in\nrelation to challenging scenarios for standard (alternative) finite element\nprocedures and analytical methods, such as the contact with multi-scale rough\nprofiles. The present framework enables the comprehensive investigation of the\nsystem response due to the occurrence of tangential tractions, which are at the\norigin of important phenomena such as wear and fretting fatigue, together with\nthe analysis of the effects of coupling between normal and tangential contact\ntractions. This scenario is herein investigated in relation to challenging\nphysical problems involving arbitrary loading histories.",
    "descriptor": "\nComments: Final version available at this https URL\n",
    "authors": [
      "Jacopo Bonari",
      "Marco Paggi",
      "Jos\u00e9 Reinoso"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07932"
  },
  {
    "id": "arXiv:2110.07951",
    "title": "Stability and dynamical transition of a electrically conducting rotating  fluid",
    "abstract": "In this article, we aim to study the stability and dynamic transition of an\nelectrically conducting fluid in the presence of an external uniform horizontal\nmagnetic field and a rotation based on a Boussinesq approximation model. We\ntake a hybrid approach combining theoretical analysis with numerical\ncomputation to study the transition from a simple real eigenvalue, a pair of\ncomplex conjugate eigenvalues and a pair of real eigenvalues. The center\nmanifold reduction theory is applied to reduce the infinite dimensional system\nto the corresponding finite dimensional one together with several\nnon-dimensional transition numbers that determine the dynamic transition types.\nCareful numerical computations are performed to determine these transition\nnumbers as well as related temporal and flow patterns etc. Our results indicate\nthat both transition of continuous type and transition of jump type can occur\nat certain parameter region. For the continuous transition from a simple real\neigenvalue, the Boussinesq approximation model bifurcates to two nontrivial\nstable steady-state solutions. For the continuous transition from a pair of\ncomplex conjugate eigenvalues, the model bifurcates to a stable periodic\nsolutions. For the continuous transition from a pair of real eigenvalues, the\nmodel bifurcates to a local attractor at the critical Rayleigh number. The\nlocal attractor contains two (four) stable nodes and two (four) saddle points.",
    "descriptor": "",
    "authors": [
      "Liang Li",
      "Yanlong Fan",
      "Daozhi Han",
      "Quan Wang"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07951"
  },
  {
    "id": "arXiv:2110.07957",
    "title": "Don't speak too fast: The impact of data bias on self-supervised speech  models",
    "abstract": "Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Yen Meng",
      "Yi-Hui Chou",
      "Andy T. Liu",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07957"
  },
  {
    "id": "arXiv:2110.07992",
    "title": "BayesAoA: A Bayesian method for Computation Efficient Angle of Arrival  Estimation",
    "abstract": "The angle of Arrival (AoA) estimation is of great interest in modern\ncommunication systems. Traditional maximum likelihood-based iterative\nalgorithms are sensitive to initialization and cannot be used online. We\npropose a Bayesian method to find AoA that is insensitive towards\ninitialization. The proposed method is less complex and needs fewer computing\nresources than traditional deep learning-based methods. It has a faster\nconvergence than the brute-force methods. Further, a Hedge type solution is\nproposed that helps to deploy the method online to handle the situations where\nthe channel noise and antenna configuration in the receiver change over time.\nThe proposed method achieves $92\\%$ accuracy in a channel of noise variance\n$10^{-6}$ with $19.3\\%$ of the brute-force method's computation.",
    "descriptor": "",
    "authors": [
      "Akshay Sharma",
      "Nancy Nayak",
      "Sheetal Kalyani"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07992"
  },
  {
    "id": "arXiv:2110.07995",
    "title": "A fully conservative sharp-interface method for compressible mulitphase  flows with phase change",
    "abstract": "A fully conservative sharp-interface method is developed for multiphase flows\nwith phase change. The coupling between two phases is implemented via\nintroducing the interfacial fluxes, which are obtained by solving a general\nRiemann problem with phase change. A novel four-wave model is proposed to\nobtain an approximate Riemann solution, which simplifies the eight-dimensional\nroo-finding procedure in the exact solver to a sole iteration of the mass flux.\nUnlike in the previous research, the jump conditions of all waves are imposed\nstrictly in the present approximate Riemann solver so that conservation is\nguaranteed. Different choices of the fluid states used in the phase change\nmodel are compared, and we have shown that the adjacent states of phase\ninterface should be used to ensure numerical consistency. To the authors'\nknowledge, it has not been reported before in the open literature. With good\nagreements, various numerical examples are considered to validate the present\nmethod by comparing the results against the exact solutions or the previous\nsimulations.",
    "descriptor": "",
    "authors": [
      "Tian Long",
      "Jinsheng Cai",
      "Shucheng Pan"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.07995"
  },
  {
    "id": "arXiv:2110.07996",
    "title": "Multivariate Mean Comparison under Differential Privacy",
    "abstract": "The comparison of multivariate population means is a central task of\nstatistical inference. While statistical theory provides a variety of analysis\ntools, they usually do not protect individuals' privacy. This knowledge can\ncreate incentives for participants in a study to conceal their true data\n(especially for outliers), which might result in a distorted analysis. In this\npaper we address this problem by developing a hypothesis test for multivariate\nmean comparisons that guarantees differential privacy to users. The test\nstatistic is based on the popular Hotelling's $t^2$-statistic, which has a\nnatural interpretation in terms of the Mahalanobis distance. In order to\ncontrol the type-1-error, we present a bootstrap algorithm under differential\nprivacy that provably yields a reliable test decision. In an empirical study we\ndemonstrate the applicability of this approach.",
    "descriptor": "",
    "authors": [
      "Martin Dunsche",
      "Tim Kutta",
      "Holger Dette"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.07996"
  },
  {
    "id": "arXiv:2110.08016",
    "title": "Efficiently Solve the Max-cut Problem via a Quantum Qubit Rotation  Algorithm",
    "abstract": "Optimizing parameterized quantum circuits promises efficient use of near-term\nquantum computers to achieve the potential quantum advantage. However, there is\na notorious tradeoff between the expressibility and trainability of the\nparameter ansatz. We find that in combinatorial optimization problems, since\nthe solutions are described by bit strings, one can trade the expressiveness of\nthe ansatz for high trainability. To be specific, by focusing on the max-cut\nproblem we introduce a simple yet efficient algorithm named Quantum Qubit\nRotation Algorithm (QQRA). The quantum circuits are comprised with single-qubit\nrotation gates implementing on each qubit. The rotation angles of the gates can\nbe trained free of barren plateaus. Thus, the approximate solution of the\nmax-cut problem can be obtained with probability close to 1. To illustrate the\neffectiveness of QQRA, we compare it with the well known quantum approximate\noptimization algorithm and the classical Goemans-Williamson algorithm.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Xin Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.08016"
  },
  {
    "id": "arXiv:2110.08031",
    "title": "Multi-layer Space Information Networks: Access Design and Softwarization",
    "abstract": "In this paper, we propose an approach for constructing a multi-layer\nmulti-orbit space information network (SIN) to provide high-speed continuous\nbroadband connectivity for space missions (nanosatellite terminals) from the\nemerging space-based Internet providers. This notion has been motivated by the\nrapid developments in satellite technologies in terms of satellite\nminiaturization and reusable rocket launch, as well as the increased number of\nnanosatellite constellations in lower orbits for space downstream applications,\nsuch as earth observation, remote sensing, and Internet of Things (IoT) data\ncollection. Specifically, space-based Internet providers, such as Starlink,\nOneWeb, and SES O3b, can be utilized for broadband connectivity directly\nto/from the nanosatellites, which allows a larger degree of connectivity in\nspace network topologies. Besides, this kind of establishment is more\neconomically efficient and eliminates the need for an excessive number of\nground stations while achieving real-time and reliable space communications.\nThis objective necessitates developing suitable radio access schemes and\nefficient scalable space backhauling using inter-satellite links (ISLs) and\ninter-orbit links (IOLs). Particularly, service-oriented radio access methods\nin addition to software-defined networking (SDN)-based architecture employing\noptimal routing mechanisms over multiple ISLs and IOLs are the most essential\nenablers for this novel concept. Thus, developing this symbiotic interaction\nbetween versatile satellite nodes across different orbits will lead to a\nbreakthrough in the way that future downstream space missions and satellite\nnetworks are designed and operated.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2107.05312\n",
    "authors": [
      "Hayder Al-Hraishawi",
      "Mario Minardi",
      "Houcine Chougrani",
      "Oltjon Kodheli",
      "Jesus Fabian Mendoza Montoya",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.08031"
  },
  {
    "id": "arXiv:2110.08043",
    "title": "Phase field models for thermal fracturing and their variational  structures",
    "abstract": "It is often observed that thermal stress enhances crack propagation in\nmaterials, and conversely, crack propagation can contribute to temperature\nshifts in materials. In this study, we first consider the thermoelasticity\nmodel proposed by M. A. Biot (1956) and study its energy dissipation property.\nThe Biot thermoelasticity model takes into account the following effects.\nThermal expansion and contraction are caused by temperature changes, and\nconversely, temperatures decrease in expanding areas but increase in\ncontracting areas. In addition, we examine its thermomechanical properties\nthrough several numerical examples and observe that the stress near a singular\npoint is enhanced by the thermoelastic effect. In the second part, we propose\ntwo crack propagation models under thermal stress by coupling a phase field\nmodel for crack propagation and the Biot thermoelasticity model and show their\nvariational structures. In our numerical experiments, we investigate how\nthermal coupling affects the crack speed and shape. In particular, we observe\nthat the lowest temperature appears near the crack tip, and the crack\npropagation is accelerated by the enhanced thermal stress.",
    "descriptor": "",
    "authors": [
      "S. Alfat",
      "M. Kimura",
      "Alifian. M. M"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08043"
  },
  {
    "id": "arXiv:2110.08045",
    "title": "Compressive Independent Component Analysis: Theory and Algorithms",
    "abstract": "Compressive learning forms the exciting intersection between compressed\nsensing and statistical learning where one exploits forms of sparsity and\nstructure to reduce the memory and/or computational complexity of the learning\ntask. In this paper, we look at the independent component analysis (ICA) model\nthrough the compressive learning lens. In particular, we show that solutions to\nthe cumulant based ICA model have particular structure that induces a low\ndimensional model set that resides in the cumulant tensor space. By showing a\nrestricted isometry property holds for random cumulants e.g. Gaussian\nensembles, we prove the existence of a compressive ICA scheme. Thereafter, we\npropose two algorithms of the form of an iterative projection gradient (IPG)\nand an alternating steepest descent (ASD) algorithm for compressive ICA, where\nthe order of compression asserted from the restricted isometry property is\nrealised through empirical results. We provide analysis of the CICA algorithms\nincluding the effects of finite samples. The effects of compression are\ncharacterised by a trade-off between the sketch size and the statistical\nefficiency of the ICA estimates. By considering synthetic and real datasets, we\nshow the substantial memory gains achieved over well-known ICA algorithms by\nusing one of the proposed CICA algorithms. Finally, we conclude the paper with\nopen problems including interesting challenges from the emerging field of\ncompressive learning.",
    "descriptor": "\nComments: 27 pages, 8 figures, under review\n",
    "authors": [
      "Michael P. Sheehan",
      "Mike E. Davies"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.08045"
  },
  {
    "id": "arXiv:2110.08048",
    "title": "Multi-Layer Pseudo-Supervision for Histopathology Tissue Semantic  Segmentation using Patch-level Classification Labels",
    "abstract": "Tissue-level semantic segmentation is a vital step in computational\npathology. Fully-supervised models have already achieved outstanding\nperformance with dense pixel-level annotations. However, drawing such labels on\nthe giga-pixel whole slide images is extremely expensive and time-consuming. In\nthis paper, we use only patch-level classification labels to achieve tissue\nsemantic segmentation on histopathology images, finally reducing the annotation\nefforts. We proposed a two-step model including a classification and a\nsegmentation phases. In the classification phase, we proposed a CAM-based model\nto generate pseudo masks by patch-level labels. In the segmentation phase, we\nachieved tissue semantic segmentation by our proposed Multi-Layer\nPseudo-Supervision. Several technical novelties have been proposed to reduce\nthe information gap between pixel-level and patch-level annotations. As a part\nof this paper, we introduced a new weakly-supervised semantic segmentation\n(WSSS) dataset for lung adenocarcinoma (LUAD-HistoSeg). We conducted several\nexperiments to evaluate our proposed model on two datasets. Our proposed model\noutperforms two state-of-the-art WSSS approaches. Note that we can achieve\ncomparable quantitative and qualitative results with the fully-supervised\nmodel, with only around a 2\\% gap for MIoU and FwIoU. By comparing with manual\nlabeling, our model can greatly save the annotation time from hours to minutes.\nThe source code is available at: \\url{https://github.com/ChuHan89/WSSS-Tissue}.",
    "descriptor": "\nComments: 15 pages, 10 figures, journal\n",
    "authors": [
      "Chu Han",
      "Jiatai Lin",
      "Jinhai Mai",
      "Yi Wang",
      "Qingling Zhang",
      "Bingchao Zhao",
      "Xin Chen",
      "Xipeng Pan",
      "Zhenwei Shi",
      "Xiaowei Xu",
      "Su Yao",
      "Lixu Yan",
      "Huan Lin",
      "Zeyan Xu",
      "Xiaomei Huang",
      "Guoqiang Han",
      "Changhong Liang",
      "Zaiyi Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.08048"
  },
  {
    "id": "arXiv:2110.08065",
    "title": "Description of random level sets by polynomial chaos expansions",
    "abstract": "We present a novel approach to determine the evolution of level sets under\nuncertainties in the velocity fields. This leads to a stochastic description of\nthe level sets. To compute the quantiles of random level sets, we use the\nstochastic Galerkin method for a hyperbolic reformulation of the level-set\nequations. A novel intrusive Galerkin formulation is presented and proven\nhyperbolic. It induces a corresponding finite-volume scheme that is\nspecifically taylored for uncertain velocities.",
    "descriptor": "",
    "authors": [
      "Markus Bambach",
      "Stephan Gerster",
      "Michael Herty",
      "Aleksey Sikstel"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.08065"
  },
  {
    "id": "arXiv:2110.08078",
    "title": "The Entanglement-Assisted Communication Capacity over Quantum  Trajectories",
    "abstract": "The unique and often-weird properties of quantum mechanics allow an\ninformation carrier to propagate through multiple trajectories of quantum\nchannels simultaneously. This ultimately leads us to quantum trajectories with\nan indefinite causal order of quantum channels. It has been shown that\nindefinite causal order enables the violation of bottleneck capacity, which\nbounds the amount of the transferable classical and quantum information through\na classical trajectory with a well-defined causal order of quantum channels. In\nthis treatise, we investigate this beneficial property in the realm of both\nentanglement-assisted classical and quantum communications. To this aim, we\nderive closed-form capacity expressions of entanglement-assisted classical and\nquantum communication for arbitrary quantum Pauli channels over classical and\nquantum trajectories. We show that by exploiting the indefinite causal order of\nquantum channels, we obtain capacity gains over classical trajectory as well as\nthe violation of bottleneck capacity for various practical scenarios.\nFurthermore, we determine the operating region where entanglement-assisted\ncommunication over quantum trajectory obtains capacity gain against classical\ntrajectory and where the entanglement-assisted communication over quantum\ntrajectory violates the bottleneck capacity.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Wireless Communications (32 pages, 13 figures)\n",
    "authors": [
      "Daryus Chandra",
      "Marcello Caleffi",
      "Angela Sara Cacciapuoti"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.08078"
  },
  {
    "id": "arXiv:2110.08080",
    "title": "Multi-modal Aggregation Network for Fast MR Imaging",
    "abstract": "Magnetic resonance (MR) imaging is a commonly used scanning technique for\ndisease detection, diagnosis and treatment monitoring. Although it is able to\nproduce detailed images of organs and tissues with better contrast, it suffers\nfrom a long acquisition time, which makes the image quality vulnerable to say\nmotion artifacts. Recently, many approaches have been developed to reconstruct\nfull-sampled images from partially observed measurements in order to accelerate\nMR imaging. However, most of these efforts focus on reconstruction over a\nsingle modality or simple fusion of multiple modalities, neglecting the\ndiscovery of correlation knowledge at different feature level. In this work, we\npropose a novel Multi-modal Aggregation Network, named MANet, which is capable\nof discovering complementary representations from a fully sampled auxiliary\nmodality, with which to hierarchically guide the reconstruction of a given\ntarget modality. In our MANet, the representations from the fully sampled\nauxiliary and undersampled target modalities are learned independently through\na specific network. Then, a guided attention module is introduced in each\nconvolutional stage to selectively aggregate multi-modal features for better\nreconstruction, yielding comprehensive, multi-scale, multi-modal feature\nfusion. Moreover, our MANet follows a hybrid domain learning framework, which\nallows it to simultaneously recover the frequency signal in the $k$-space\ndomain as well as restore the image details from the image domain. Extensive\nexperiments demonstrate the superiority of the proposed approach over\nstate-of-the-art MR image reconstruction methods.",
    "descriptor": "",
    "authors": [
      "Chun-Mei Feng",
      "Huazhu Fe",
      "Tianfei Zhou",
      "Yong Xu",
      "Ling Shao",
      "David Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08080"
  },
  {
    "id": "arXiv:2110.08085",
    "title": "Prediction of Lung CT Scores of Systemic Sclerosis by Cascaded  Regression Neural Networks",
    "abstract": "Visually scoring lung involvement in systemic sclerosis from CT scans plays\nan important role in monitoring progression, but its labor intensiveness\nhinders practical application. We proposed, therefore, an automatic scoring\nframework that consists of two cascaded deep regression neural networks. The\nfirst (3D) network aims to predict the craniocaudal position of five\nanatomically defined scoring levels on the 3D CT scans. The second (2D) network\nreceives the resulting 2D axial slices and predicts the scores. We used 227 3D\nCT scans to train and validate the first network, and the resulting 1135 axial\nslices were used in the second network. Two experts scored independently a\nsubset of data to obtain intra- and interobserver variabilities and the ground\ntruth for all data was obtained in consensus. To alleviate the unbalance in\ntraining labels in the second network, we introduced a sampling technique and\nto increase the diversity of the training samples synthetic data was generated,\nmimicking ground glass and reticulation patterns. The 4-fold cross validation\nshowed that our proposed network achieved an average MAE of 5.90, 4.66 and\n4.49, weighted kappa of 0.66, 0.58 and 0.65 for total score (TOT), ground glass\n(GG) and reticular pattern (RET), respectively. Our network performed slightly\nworse than the best experts on TOT and GG prediction but it has competitive\nperformance on RET prediction and has the potential to be an objective\nalternative for the visual scoring of SSc in CT thorax studies.",
    "descriptor": "\nComments: SPIE 2022 accepted\n",
    "authors": [
      "Jingnan Jia",
      "Marius Staring",
      "Irene Hern\u00e1ndez-Gir\u00f3n",
      "Lucia J.M. Kroft",
      "Anne A. Schouffoer",
      "Berend C. Stoel"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.08085"
  },
  {
    "id": "arXiv:2110.08087",
    "title": "Causal Identification with Additive Noise Models: Quantifying the Effect  of Noise",
    "abstract": "In recent years, a lot of research has been conducted within the area of\ncausal inference and causal learning. Many methods have been developed to\nidentify the cause-effect pairs in models and have been successfully applied to\nobservational real-world data to determine the direction of causal\nrelationships. Yet in bivariate situations, causal discovery problems remain\nchallenging. One class of such methods, that also allows tackling the bivariate\ncase, is based on Additive Noise Models (ANMs). Unfortunately, one aspect of\nthese methods has not received much attention until now: what is the impact of\ndifferent noise levels on the ability of these methods to identify the\ndirection of the causal relationship. This work aims to bridge this gap with\nthe help of an empirical study. We test Regression with Subsequent Independence\nTest (RESIT) using an exhaustive range of models where the level of additive\nnoise gradually changes from 1\\% to 10000\\% of the causes' noise level (the\nlatter remains fixed). Additionally, the experiments in this work consider\nseveral different types of distributions as well as linear and non-linear\nmodels. The results of the experiments show that ANMs methods can fail to\ncapture the true causal direction for some levels of noise.",
    "descriptor": "\nComments: Presented at 10\\`emes Journ\\'ees Francophones sur les R\\'eseaux Bay\\'esiens et les Mod\\`eles Graphiques Probabilistes (JFRB-2021), this https URL\n",
    "authors": [
      "Benjamin Kap",
      "Marharyta Aleksandrova",
      "Thomas Engel"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08087"
  },
  {
    "id": "arXiv:2110.08111",
    "title": "An active learning approach for improving the performance of equilibrium  based chemical simulations",
    "abstract": "In this paper, we propose a novel sequential data-driven method for dealing\nwith equilibrium based chemical simulations, which can be seen as a specific\nmachine learning approach called active learning. The underlying idea of our\napproach is to consider the function to estimate as a sample of a Gaussian\nprocess which allows us to compute the global uncertainty on the function\nestimation. Thanks to this estimation and with almost no parameter to tune, the\nproposed method sequentially chooses the most relevant input data at which the\nfunction to estimate has to be evaluated to build a surrogate model. Hence, the\nnumber of evaluations of the function to estimate is dramatically limited. Our\nactive learning method is validated through numerical experiments and applied\nto a complex chemical system commonly used in geoscience.",
    "descriptor": "\nComments: 22 pages, 17 figures\n",
    "authors": [
      "Mary Savino",
      "C\u00e9line L\u00e9vy-Leduc",
      "Marc Leconte",
      "Benoit Cochepin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08111"
  },
  {
    "id": "arXiv:2110.08115",
    "title": "Quickest Inference of Network Cascades with Noisy Information",
    "abstract": "We study the problem of estimating the source of a network cascade given a\ntime series of noisy information about the spread. Initially, there is a single\nvertex affected by the cascade (the source) and the cascade spreads in discrete\ntime steps across the network. The cascade evolution is hidden, but one can\nobserve a time series of noisy signals from each vertex. The time series of a\nvertex is assumed to be a sequence of i.i.d. samples from a pre-change\ndistribution $Q_0$ before the cascade affects the vertex, and the time series\nis a sequence of i.i.d. samples from a post-change distribution $Q_1$ once the\ncascade has affected the vertex. Given the time series of noisy signals, which\ncan be viewed as a noisy measurement of the cascade evolution, we aim to devise\na procedure to reliably estimate the cascade source as fast as possible.\nWe investigate Bayesian and minimax formulations of the source estimation\nproblem, and derive near-optimal estimators for simple cascade dynamics and\nnetwork topologies. In the Bayesian setting, an estimator which observes\nsamples until the error of the Bayes-optimal estimator falls below a threshold\nachieves optimal performance. In the minimax setting, optimal performance is\nachieved by designing a novel multi-hypothesis sequential probability ratio\ntest (MSPRT). We find that these optimal estimators require $\\log \\log n / \\log\n(k - 1)$ observations of the noisy time series when the network topology is a\n$k$-regular tree, and $(\\log n)^{\\frac{1}{\\ell + 1}}$ observations are required\nfor $\\ell$-dimensional lattices. Finally, we discuss how our methods may be\nextended to cascades on arbitrary graphs.",
    "descriptor": "\nComments: 47 pages, 3 figures\n",
    "authors": [
      "Anirudh Sridhar",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.08115"
  },
  {
    "id": "arXiv:2110.08141",
    "title": "Data-driven Heuristics for DC optimal transmission switching problem",
    "abstract": "The goal of Optimal Transmission Switching (OTS) problem for power systems is\nto identify a topology of the power grid that minimizes the cost of the system\noperation while satisfying the operational and physical constraints. Among the\nmost popular methods to solve OTS is to construct approximation via integer\nlinear programming formulations, which often come with big-M inequalities.\nThese big-M inequalities increase, considerably, the difficulty of solving the\nresulting formulations. Moreover, choosing big-M values optimally is as hard as\nsolving OTS itself. In this paper, we devise two data-driven big-M bound\nstrengthening methods which take network structure, power demands and\ngeneration costs into account. We illustrate the robustness of our methods to\nload changes and impressive runtime improvements of mixed-integer solvers\nachieved by our methods with extensive experiments on benchmark instances. The\nspeedup by one of the proposed methods is almost 13 times with respect to the\nexact method.",
    "descriptor": "",
    "authors": [
      "Juncheng Li",
      "Trivikram Dokka",
      "Guglielmo Lulli",
      "Fabrizio Lacalandra"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.08141"
  },
  {
    "id": "arXiv:2110.08217",
    "title": "Choice functions based multi-objective Bayesian optimisation",
    "abstract": "In this work we introduce a new framework for multi-objective Bayesian\noptimisation where the multi-objective functions can only be accessed via\nchoice judgements, such as ``I pick options A,B,C among this set of five\noptions A,B,C,D,E''. The fact that the option D is rejected means that there is\nat least one option among the selected ones A,B,C that I strictly prefer over D\n(but I do not have to specify which one). We assume that there is a latent\nvector function f for some dimension $n_e$ which embeds the options into the\nreal vector space of dimension n, so that the choice set can be represented\nthrough a Pareto set of non-dominated options. By placing a Gaussian process\nprior on f and deriving a novel likelihood model for choice data, we propose a\nBayesian framework for choice functions learning. We then apply this surrogate\nmodel to solve a novel multi-objective Bayesian optimisation from choice data\nproblem.",
    "descriptor": "",
    "authors": [
      "Alessio Benavoli",
      "Dario Azzimonti",
      "Dario Piga"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08217"
  },
  {
    "id": "arXiv:2110.08233",
    "title": "The Power of Many: A Physarum Swarm Steiner Tree Algorithm",
    "abstract": "We create a novel Physarum Steiner algorithm designed to solve the Euclidean\nSteiner tree problem. Physarum is a unicellular slime mold with the ability to\nform networks and fuse with other Physarum organisms. We use the simplicity and\nfusion of Physarum to create large swarms which independently operate to solve\nthe Steiner problem. The Physarum Steiner tree algorithm then utilizes a swarm\nof Physarum organisms which gradually find terminals and fuse with each other,\nsharing intelligence. The algorithm is also highly capable of solving the\nobstacle avoidance Steiner tree problem and is a strong alternative to the\ncurrent leading algorithm. The algorithm is of particular interest due to its\nnovel approach, rectilinear properties, and ability to run on varying shapes\nand topological surfaces.",
    "descriptor": "\nComments: 25 images, 12 pages\n",
    "authors": [
      "Sheryl Hsu",
      "Fidel I. Schaposnik Massolo",
      "Laura P. Schaposnik"
    ],
    "subjectives": [
      "Biological Physics (physics.bio-ph)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.08233"
  },
  {
    "id": "arXiv:2110.08243",
    "title": "Neural Dubber: Dubbing for Silent Videos According to Scripts",
    "abstract": "Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given\nsilent video from the text. Neural Dubber is a multi-modal text-to-speech (TTS)\nmodel that utilizes the lip movement in the video to control the prosody of the\ngenerated speech. Furthermore, an image-based speaker embedding (ISE) module is\ndeveloped for the multi-speaker setting, which enables Neural Dubber to\ngenerate speech with a reasonable timbre according to the speaker's face.\nExperiments on the chemistry lecture single-speaker dataset and LRS2\nmulti-speaker dataset show that Neural Dubber can generate speech audios on par\nwith state-of-the-art TTS models in terms of speech quality. Most importantly,\nboth qualitative and quantitative evaluations show that Neural Dubber can\ncontrol the prosody of synthesized speech by the video, and generate\nhigh-fidelity speech temporally synchronized with the video.",
    "descriptor": "\nComments: Accepted by NeurIPS 2021\n",
    "authors": [
      "Chenxu Hu",
      "Qiao Tian",
      "Tingle Li",
      "Yuping Wang",
      "Yuxuan Wang",
      "Hang Zhao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.08243"
  },
  {
    "id": "arXiv:1810.00303",
    "title": "Newton-MR: Inexact Newton Method With Minimum Residual Sub-problem  Solver",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Fred Roosta",
      "Yang Liu",
      "Peng Xu",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1810.00303"
  },
  {
    "id": "arXiv:1812.00108",
    "title": "Multi-Stream Dynamic Video Summarization",
    "abstract": "Multi-Stream Dynamic Video Summarization",
    "descriptor": "",
    "authors": [
      "Mohamed Elfeki",
      "Liqiang Wang",
      "Ali Borji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1812.00108"
  },
  {
    "id": "arXiv:1907.08356",
    "title": "New Era of Deeplearning-Based Malware Intrusion Detection: The Malware  Detection and Prediction Based On Deep Learning",
    "abstract": "Comments: Unable to open due to ethical problem",
    "descriptor": "\nComments: Unable to open due to ethical problem\n",
    "authors": [
      "Shuqiang Lu",
      "Lingyun Ying",
      "Wenjie Lin",
      "Yu Wang",
      "Meining Nie",
      "Kaiwen Shen",
      "Lu Liu",
      "Haixin Duan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1907.08356"
  },
  {
    "id": "arXiv:1912.08140",
    "title": "On-the-fly Global Embeddings Using Random Projections for Extreme  Multi-label Classification",
    "abstract": "On-the-fly Global Embeddings Using Random Projections for Extreme  Multi-label Classification",
    "descriptor": "",
    "authors": [
      "Yashaswi Verma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1912.08140"
  },
  {
    "id": "arXiv:2001.01771",
    "title": "Revenue Analysis of Stationary and Transportable Battery Storage for  Power Systems with High Penetration of Renewable Sources: A Market  Participant Perspective",
    "abstract": "Comments: Submitted",
    "descriptor": "\nComments: Submitted\n",
    "authors": [
      "Zhongyang Zhao",
      "Caisheng Wang",
      "Masoud H. Nazari"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2001.01771"
  },
  {
    "id": "arXiv:2002.08101",
    "title": "The Sum of Its Parts: Analysis of Federated Byzantine Agreement Systems",
    "abstract": "The Sum of Its Parts: Analysis of Federated Byzantine Agreement Systems",
    "descriptor": "",
    "authors": [
      "Martin Florian",
      "Sebastian Henningsen",
      "Charmaine Ndolo",
      "Bj\u00f6rn Scheuermann"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2002.08101"
  },
  {
    "id": "arXiv:2002.12036",
    "title": "Complexity Measures and Features for Times Series classification",
    "abstract": "Complexity Measures and Features for Times Series classification",
    "descriptor": "",
    "authors": [
      "Francisco J. Bald\u00e1n",
      "Jos\u00e9 M. Ben\u00edtez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.12036"
  },
  {
    "id": "arXiv:2006.06592",
    "title": "The Backbone Method for Ultra-High Dimensional Sparse Machine Learning",
    "abstract": "Comments: First submission to Machine Learning: 06/2020. Revised: 10/2021",
    "descriptor": "\nComments: First submission to Machine Learning: 06/2020. Revised: 10/2021\n",
    "authors": [
      "Dimitris Bertsimas",
      "Vassilis Digalakis Jr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.06592"
  },
  {
    "id": "arXiv:2006.09773",
    "title": "Neural Ordinary Differential Equation Control of Dynamics on Graphs",
    "abstract": "Comments: Fifth version improves and clears notation",
    "descriptor": "\nComments: Fifth version improves and clears notation\n",
    "authors": [
      "Thomas Asikis",
      "Lucas B\u00f6ttcher",
      "Nino Antulov-Fantulin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.09773"
  },
  {
    "id": "arXiv:2007.05139",
    "title": "Mechanisms for Hiding Sensitive Genotypes with Information-Theoretic  Privacy",
    "abstract": "Mechanisms for Hiding Sensitive Genotypes with Information-Theoretic  Privacy",
    "descriptor": "",
    "authors": [
      "Fangwei Ye",
      "Hyunghoon Cho",
      "Salim El Rouayheb"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.05139"
  },
  {
    "id": "arXiv:2007.05578",
    "title": "Revealing the State-of-the-Art in Large-Scale Agile Development: A  Systematic Mapping Study",
    "abstract": "Revealing the State-of-the-Art in Large-Scale Agile Development: A  Systematic Mapping Study",
    "descriptor": "",
    "authors": [
      "\u00d6mer Uludag",
      "Pascal Philipp",
      "Abheeshta Putta",
      "Maria Paasivaara",
      "Casper Lassenius",
      "Florian Matthes"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2007.05578"
  },
  {
    "id": "arXiv:2007.10541",
    "title": "ZLB: A Blockchain to Tolerate Colluding Majorities",
    "abstract": "ZLB: A Blockchain to Tolerate Colluding Majorities",
    "descriptor": "",
    "authors": [
      "Alejandro Ranchal-Pedrosa",
      "Vincent Gramoli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2007.10541"
  },
  {
    "id": "arXiv:2007.12652",
    "title": "MurTree: Optimal Classification Trees via Dynamic Programming and Search",
    "abstract": "MurTree: Optimal Classification Trees via Dynamic Programming and Search",
    "descriptor": "",
    "authors": [
      "Emir Demirovi\u0107",
      "Anna Lukina",
      "Emmanuel Hebrard",
      "Jeffrey Chan",
      "James Bailey",
      "Christopher Leckie",
      "Kotagiri Ramamohanarao",
      "Peter J. Stuckey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.12652"
  },
  {
    "id": "arXiv:2009.01454",
    "title": "Say No to the Discrimination: Learning Fair Graph Neural Networks with  Limited Sensitive Attribute Information",
    "abstract": "Say No to the Discrimination: Learning Fair Graph Neural Networks with  Limited Sensitive Attribute Information",
    "descriptor": "",
    "authors": [
      "Enyan Dai",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.01454"
  },
  {
    "id": "arXiv:2009.06921",
    "title": "Optimal Decision Trees for Nonlinear Metrics",
    "abstract": "Optimal Decision Trees for Nonlinear Metrics",
    "descriptor": "",
    "authors": [
      "Emir Demirovi\u0107",
      "Peter J. Stuckey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.06921"
  },
  {
    "id": "arXiv:2009.14088",
    "title": "Task-Based Analog-to-Digital Converters",
    "abstract": "Task-Based Analog-to-Digital Converters",
    "descriptor": "",
    "authors": [
      "Peter Neuhaus",
      "Nir Shlezinger",
      "Meik D\u00f6rpinghaus",
      "Yonina C. Eldar",
      "Gerhard Fettweis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2009.14088"
  },
  {
    "id": "arXiv:2010.01663",
    "title": "KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image  and Volumetric Segmentation",
    "abstract": "Comments: Journal Extension of KiU-Net (MICCAI-2020)",
    "descriptor": "\nComments: Journal Extension of KiU-Net (MICCAI-2020)\n",
    "authors": [
      "Jeya Maria Jose Valanarasu",
      "Vishwanath A. Sindagi",
      "Ilker Hacihaliloglu",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.01663"
  },
  {
    "id": "arXiv:2010.07027",
    "title": "A Light Heterogeneous Graph Collaborative Filtering Model using Textual  Information",
    "abstract": "Comments: Accepted by Knowledge-Based Systems",
    "descriptor": "\nComments: Accepted by Knowledge-Based Systems\n",
    "authors": [
      "Chaoyang Wang",
      "Zhiqiang Guo",
      "Guohui Li",
      "Jianjun Li",
      "Peng Pan",
      "Ke Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.07027"
  },
  {
    "id": "arXiv:2010.15335",
    "title": "Learning Sampling Distributions Using Local 3D Workspace Decompositions  for Motion Planning in High Dimensions",
    "abstract": "Comments: Accepted in International Conference on Robotics and Automation (ICRA), 2021. Nominated for Best Paper in Cognitive Robotics. Supplementary video: this https URL Code: this https URL",
    "descriptor": "\nComments: Accepted in International Conference on Robotics and Automation (ICRA), 2021. Nominated for Best Paper in Cognitive Robotics. Supplementary video: this https URL Code: this https URL\n",
    "authors": [
      "Constantinos Chamzas",
      "Zachary Kingston",
      "Carlos Quintero-Pe\u00f1a",
      "Anshumali Shrivastava",
      "Lydia E. Kavraki"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.15335"
  },
  {
    "id": "arXiv:2011.03024",
    "title": "Finite element appoximation and augmented Lagrangian preconditioning for  anisothermal implicitly-constituted non-Newtonian flow",
    "abstract": "Comments: 9 figures, 39 pages",
    "descriptor": "\nComments: 9 figures, 39 pages\n",
    "authors": [
      "Patrick Farrell",
      "Pablo Alexei Gazca Orozco",
      "Endre S\u00fcli"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2011.03024"
  },
  {
    "id": "arXiv:2011.04117",
    "title": "Practical Bayesian System Identification using Hamiltonian Monte Carlo",
    "abstract": "Practical Bayesian System Identification using Hamiltonian Monte Carlo",
    "descriptor": "",
    "authors": [
      "Johannes Hendriks",
      "Adrian Wills",
      "Brett Ninness",
      "Johan Dahlin"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2011.04117"
  },
  {
    "id": "arXiv:2011.06070",
    "title": "Quantifying and Learning Linear Symmetry-Based Disentanglement",
    "abstract": "Quantifying and Learning Linear Symmetry-Based Disentanglement",
    "descriptor": "",
    "authors": [
      "Loek Tonnaer",
      "Luis A. P\u00e9rez Rey",
      "Vlado Menkovski",
      "Mike Holenderski",
      "Jacobus W. Portegies"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.06070"
  },
  {
    "id": "arXiv:2012.01954",
    "title": "A Novel Robust 3-D Path Following Control for Keplerian Orbits",
    "abstract": "Comments: 22 pages. 6 figures",
    "descriptor": "\nComments: 22 pages. 6 figures\n",
    "authors": [
      "Rodolfo Batista Negri",
      "Ant\u00f4nio Fernando Bertachini de Almeida Prado"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.01954"
  },
  {
    "id": "arXiv:2012.02957",
    "title": "Leveraging Order-Free Tag Relations for Context-Aware Recommendation",
    "abstract": "Comments: EMNLP 2021",
    "descriptor": "\nComments: EMNLP 2021\n",
    "authors": [
      "Junmo Kang",
      "Jeonghwan Kim",
      "Suwon Shin",
      "Sung-Hyon Myaeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2012.02957"
  },
  {
    "id": "arXiv:2012.13615",
    "title": "RoCUS: Robot Controller Understanding via Sampling",
    "abstract": "Comments: CoRL 2021. The project website is at this https URL",
    "descriptor": "\nComments: CoRL 2021. The project website is at this https URL\n",
    "authors": [
      "Yilun Zhou",
      "Serena Booth",
      "Nadia Figueroa",
      "Julie Shah"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2012.13615"
  },
  {
    "id": "arXiv:2012.15545",
    "title": "Vehicular Network Slicing for Reliable Access and Deadline-Constrained  Data Offloading: A Multi-Agent On-Device Learning Approach",
    "abstract": "Comments: This paper was rejected from JSAC; we need to address the reviewers' comments",
    "descriptor": "\nComments: This paper was rejected from JSAC; we need to address the reviewers' comments\n",
    "authors": [
      "Md Ferdous Pervej",
      "Shih-Chun Lin"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2012.15545"
  },
  {
    "id": "arXiv:2101.02360",
    "title": "Distributed Quantum Faithful Simulation and Function Computation Using  Algebraic Structured Measurements",
    "abstract": "Comments: 31 pages, 2 figures, 3 examples",
    "descriptor": "\nComments: 31 pages, 2 figures, 3 examples\n",
    "authors": [
      "Touheed Anwar Atif",
      "S. Sandeep Pradhan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2101.02360"
  },
  {
    "id": "arXiv:2101.03289",
    "title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual  Natural Language Processing",
    "abstract": "Comments: Camera-ready version for EACL 2021 Demo",
    "descriptor": "\nComments: Camera-ready version for EACL 2021 Demo\n",
    "authors": [
      "Minh Van Nguyen",
      "Viet Dac Lai",
      "Amir Pouran Ben Veyseh",
      "Thien Huu Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.03289"
  },
  {
    "id": "arXiv:2101.06116",
    "title": "Hyperspectral Image Classification -- Traditional to Deep Models: A  Survey for Future Prospects",
    "abstract": "Hyperspectral Image Classification -- Traditional to Deep Models: A  Survey for Future Prospects",
    "descriptor": "",
    "authors": [
      "Muhammad Ahmad",
      "Sidrah Shabbir",
      "Swalpa Kumar Roy",
      "Danfeng Hong",
      "Xin Wu",
      "Jing Yao",
      "Adil Mehmood Khan",
      "Manuel Mazzara",
      "Salvatore Distefano",
      "Jocelyn Chanussot"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.06116"
  },
  {
    "id": "arXiv:2101.09324",
    "title": "Generating Black-Box Adversarial Examples in Sparse Domain",
    "abstract": "Generating Black-Box Adversarial Examples in Sparse Domain",
    "descriptor": "",
    "authors": [
      "Hadi Zanddizari",
      "Behnam Zeinali",
      "J. Morris Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2101.09324"
  },
  {
    "id": "arXiv:2102.01881",
    "title": "Analysis and Design of Analog Fountain Codes for Short Packet  Communications",
    "abstract": "Comments: 13 pages, 15 figures",
    "descriptor": "\nComments: 13 pages, 15 figures\n",
    "authors": [
      "Wen Jun Lim",
      "Rana Abbas",
      "Yonghui Li",
      "Branka Vucetic",
      "Mahyar Shirvanimoghaddam"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.01881"
  },
  {
    "id": "arXiv:2102.03893",
    "title": "Enhancement of Distribution System State Estimation Using Pruned  Physics-Aware Neural Networks",
    "abstract": "Enhancement of Distribution System State Estimation Using Pruned  Physics-Aware Neural Networks",
    "descriptor": "",
    "authors": [
      "Minh-Quan Tran",
      "Ahmed S. Zamzam",
      "Phuong H. Nguyen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.03893"
  },
  {
    "id": "arXiv:2102.05872",
    "title": "Onoma-to-wave: Environmental sound synthesis from onomatopoeic words",
    "abstract": "Onoma-to-wave: Environmental sound synthesis from onomatopoeic words",
    "descriptor": "",
    "authors": [
      "Yuki Okamoto",
      "Keisuke Imoto",
      "Shinnosuke Takamichi",
      "Ryosuke Yamanishi",
      "Takahiro Fukumori",
      "Yoichi Yamashita"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2102.05872"
  },
  {
    "id": "arXiv:2102.10774",
    "title": "Provably Improved Context-Based Offline Meta-RL with Attention and  Contrastive Learning",
    "abstract": "Comments: 21 pages, 7 figures",
    "descriptor": "\nComments: 21 pages, 7 figures\n",
    "authors": [
      "Lanqing Li",
      "Yuanhao Huang",
      "Mingzhe Chen",
      "Siteng Luo",
      "Dijun Luo",
      "Junzhou Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.10774"
  },
  {
    "id": "arXiv:2102.11382",
    "title": "Sandwich Batch Normalization: A Drop-In Replacement for Feature  Distribution Heterogeneity",
    "abstract": "Comments: Accepted to WACV 2022",
    "descriptor": "\nComments: Accepted to WACV 2022\n",
    "authors": [
      "Xinyu Gong",
      "Wuyang Chen",
      "Tianlong Chen",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.11382"
  },
  {
    "id": "arXiv:2102.12353",
    "title": "Nonlinear Invariant Risk Minimization: A Causal Approach",
    "abstract": "Nonlinear Invariant Risk Minimization: A Causal Approach",
    "descriptor": "",
    "authors": [
      "Chaochao Lu",
      "Yuhuai Wu",
      "Jo\u015be Miguel Hern\u00e1ndez-Lobato",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.12353"
  },
  {
    "id": "arXiv:2102.13088",
    "title": "Even your Teacher Needs Guidance: Ground-Truth Targets Dampen  Regularization Imposed by Self-Distillation",
    "abstract": "Comments: To be published at NeurIPS 2021; 21 pages, 14 figures",
    "descriptor": "\nComments: To be published at NeurIPS 2021; 21 pages, 14 figures\n",
    "authors": [
      "Kenneth Borup",
      "Lars N. Andersen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.13088"
  },
  {
    "id": "arXiv:2103.00263",
    "title": "A semismooth Newton method for implicitly constituted non-Newtonian  fluids and its application to the numerical approximation of Bingham flow",
    "abstract": "Comments: 25 pages",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "P. A. Gazca-Orozco"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2103.00263"
  },
  {
    "id": "arXiv:2103.05378",
    "title": "Decentralized Non-Convex Learning with Linearly Coupled Constraints",
    "abstract": "Decentralized Non-Convex Learning with Linearly Coupled Constraints",
    "descriptor": "",
    "authors": [
      "Jiawei Zhang",
      "Songyang Ge",
      "Tsung-Hui Chang",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.05378"
  },
  {
    "id": "arXiv:2103.05456",
    "title": "Extended Tree Search for Robot Task and Motion Planning",
    "abstract": "Extended Tree Search for Robot Task and Motion Planning",
    "descriptor": "",
    "authors": [
      "Tianyu Ren",
      "Georgia Chalvatzaki",
      "Jan Peters"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.05456"
  },
  {
    "id": "arXiv:2103.09404",
    "title": "Collapsible Linear Blocks for Super-Efficient Super Resolution",
    "abstract": "Collapsible Linear Blocks for Super-Efficient Super Resolution",
    "descriptor": "",
    "authors": [
      "Kartikeya Bhardwaj",
      "Milos Milosavljevic",
      "Liam O'Neil",
      "Dibakar Gope",
      "Ramon Matas",
      "Alex Chalfin",
      "Naveen Suda",
      "Lingchuan Meng",
      "Danny Loh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.09404"
  },
  {
    "id": "arXiv:2103.12591",
    "title": "BoXHED2.0: Scalable boosting of dynamic survival analysis",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Arash Pakbin",
      "Xiaochen Wang",
      "Bobak J. Mortazavi",
      "Donald K.K. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.12591"
  },
  {
    "id": "arXiv:2103.13355",
    "title": "Bag of Tricks for Node Classification with Graph Neural Networks",
    "abstract": "Bag of Tricks for Node Classification with Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Yangkun Wang",
      "Jiarui Jin",
      "Weinan Zhang",
      "Yong Yu",
      "Zheng Zhang",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.13355"
  },
  {
    "id": "arXiv:2103.13539",
    "title": "Multi-View Fusion for Multi-Level Robotic Scene Understanding",
    "abstract": "Comments: Presented at IROS 2021. Video is at this https URL",
    "descriptor": "\nComments: Presented at IROS 2021. Video is at this https URL\n",
    "authors": [
      "Yunzhi Lin",
      "Jonathan Tremblay",
      "Stephen Tyree",
      "Patricio A. Vela",
      "Stan Birchfield"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.13539"
  },
  {
    "id": "arXiv:2103.15670",
    "title": "On the Adversarial Robustness of Vision Transformers",
    "abstract": "On the Adversarial Robustness of Vision Transformers",
    "descriptor": "",
    "authors": [
      "Rulin Shao",
      "Zhouxing Shi",
      "Jinfeng Yi",
      "Pin-Yu Chen",
      "Cho-Jui Hsieh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.15670"
  },
  {
    "id": "arXiv:2103.16701",
    "title": "Charged particle tracking via edge-classifying interaction networks",
    "abstract": "Comments: This is a post-peer-review, pre-copyedit version of this article",
    "descriptor": "\nComments: This is a post-peer-review, pre-copyedit version of this article\n",
    "authors": [
      "Gage DeZoort",
      "Savannah Thais",
      "Javier Duarte",
      "Vesal Razavimaleki",
      "Markus Atkinson",
      "Isobel Ojalvo",
      "Mark Neubauer",
      "Peter Elmer"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.16701"
  },
  {
    "id": "arXiv:2103.17138",
    "title": "SOON: Scenario Oriented Object Navigation with Graph-based Exploration",
    "abstract": "SOON: Scenario Oriented Object Navigation with Graph-based Exploration",
    "descriptor": "",
    "authors": [
      "Fengda Zhu",
      "Xiwen Liang",
      "Yi Zhu",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.17138"
  },
  {
    "id": "arXiv:2104.01538",
    "title": "Hypercorrelation Squeeze for Few-Shot Segmentation",
    "abstract": "Comments: Accepted to ICCV 2021",
    "descriptor": "\nComments: Accepted to ICCV 2021\n",
    "authors": [
      "Juhong Min",
      "Dahyun Kang",
      "Minsu Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.01538"
  },
  {
    "id": "arXiv:2104.02935",
    "title": "TSception: Capturing Temporal Dynamics and Spatial Asymmetry from EEG  for Emotion Recognition",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Yi Ding",
      "Neethu Robinson",
      "Su Zhang",
      "Qiuhao Zeng",
      "Cuntai Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.02935"
  },
  {
    "id": "arXiv:2104.07198",
    "title": "Ultra-High Dimensional Sparse Representations with Binarization for  Efficient Text Retrieval",
    "abstract": "Comments: To appear at EMNLP 2021",
    "descriptor": "\nComments: To appear at EMNLP 2021\n",
    "authors": [
      "Kyoung-Rok Jang",
      "Junmo Kang",
      "Giwon Hong",
      "Sung-Hyon Myaeng",
      "Joohee Park",
      "Taewon Yoon",
      "Heecheol Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2104.07198"
  },
  {
    "id": "arXiv:2104.07353",
    "title": "Fast Private Parameter Learning and Inference for Sum-Product Networks",
    "abstract": "Fast Private Parameter Learning and Inference for Sum-Product Networks",
    "descriptor": "",
    "authors": [
      "Ernst Althaus",
      "Mohammad Sadeq Dousti",
      "Stefan Kramer",
      "Nick Johannes Peter Rassau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.07353"
  },
  {
    "id": "arXiv:2104.08540",
    "title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages",
    "abstract": "Comments: EMNLP, 9 pages",
    "descriptor": "\nComments: EMNLP, 9 pages\n",
    "authors": [
      "Dominik Schlechtweg",
      "Nina Tahmasebi",
      "Simon Hengchen",
      "Haim Dubossarsky",
      "Barbara McGillivray"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08540"
  },
  {
    "id": "arXiv:2104.08737",
    "title": "Low-Rank Subspaces for Unsupervised Entity Linking",
    "abstract": "Comments: EMNLP 2021, 18 pages, 22 figures",
    "descriptor": "\nComments: EMNLP 2021, 18 pages, 22 figures\n",
    "authors": [
      "Akhil Arora",
      "Alberto Garc\u00eda-Dur\u00e1n",
      "Robert West"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.08737"
  },
  {
    "id": "arXiv:2104.09202",
    "title": "Monitoring Data Requests in Decentralized Data Storage Systems: A Case  Study of IPFS",
    "abstract": "Monitoring Data Requests in Decentralized Data Storage Systems: A Case  Study of IPFS",
    "descriptor": "",
    "authors": [
      "Leonhard Balduf",
      "Sebastian Henningsen",
      "Martin Florian",
      "Sebastian Rust",
      "Bj\u00f6rn Scheuermann"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2104.09202"
  },
  {
    "id": "arXiv:2104.12673",
    "title": "Joint Representation Learning and Novel Category Discovery on Single-  and Multi-modal Data",
    "abstract": "Comments: ICCV 2021",
    "descriptor": "\nComments: ICCV 2021\n",
    "authors": [
      "Xuhui Jia",
      "Kai Han",
      "Yukun Zhu",
      "Bradley Green"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.12673"
  },
  {
    "id": "arXiv:2105.01466",
    "title": "GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts",
    "abstract": "Comments: JT and LS contributed equally to this work",
    "descriptor": "\nComments: JT and LS contributed equally to this work\n",
    "authors": [
      "Lukas Stappen",
      "Jason Thies",
      "Gerhard Hagerer",
      "Bj\u00f6rn W. Schuller",
      "Georg Groh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2105.01466"
  },
  {
    "id": "arXiv:2105.04906",
    "title": "VICReg: Variance-Invariance-Covariance Regularization for  Self-Supervised Learning",
    "abstract": "VICReg: Variance-Invariance-Covariance Regularization for  Self-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Adrien Bardes",
      "Jean Ponce",
      "Yann LeCun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.04906"
  },
  {
    "id": "arXiv:2105.07610",
    "title": "Cross-Cluster Weighted Forests",
    "abstract": "Comments: 19 pages, 6 figures, 1 table",
    "descriptor": "\nComments: 19 pages, 6 figures, 1 table\n",
    "authors": [
      "Maya Ramchandran",
      "Rajarshi Mukherjee",
      "Giovanni Parmigiani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.07610"
  },
  {
    "id": "arXiv:2105.09978",
    "title": "Synthesis over Regularly Approximable Data Domains",
    "abstract": "Synthesis over Regularly Approximable Data Domains",
    "descriptor": "",
    "authors": [
      "L\u00e9o Exibard",
      "Emmanuel Filiot",
      "Ayrat Khalimov"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2105.09978"
  },
  {
    "id": "arXiv:2105.11443",
    "title": "LuvHarris: A Practical Corner Detector for Event-cameras",
    "abstract": "LuvHarris: A Practical Corner Detector for Event-cameras",
    "descriptor": "",
    "authors": [
      "Arren Glover",
      "Aiko Dinale",
      "Leandro De Souza Rosa",
      "Simeon Bamford",
      "Chiara Bartolozzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.11443"
  },
  {
    "id": "arXiv:2105.11590",
    "title": "A Quantum Hopfield Associative Memory Implemented on an Actual Quantum  Processor",
    "abstract": "Comments: 16 pages, 10 figures, 3 tables. The manuscript has been revised to improve clarity of the manuscript text, mathematics and overall argument. The presented results are unchanged",
    "descriptor": "\nComments: 16 pages, 10 figures, 3 tables. The manuscript has been revised to improve clarity of the manuscript text, mathematics and overall argument. The presented results are unchanged\n",
    "authors": [
      "Nathan Eli Miller",
      "Saibal Mukhopadhyay"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.11590"
  },
  {
    "id": "arXiv:2105.12787",
    "title": "Self-Supervised Bug Detection and Repair",
    "abstract": "Comments: Published in NeurIPS 2021",
    "descriptor": "\nComments: Published in NeurIPS 2021\n",
    "authors": [
      "Miltiadis Allamanis",
      "Henry Jackson-Flux",
      "Marc Brockschmidt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2105.12787"
  },
  {
    "id": "arXiv:2106.01178",
    "title": "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View  General-Purpose 3D Object Detection",
    "abstract": "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View  General-Purpose 3D Object Detection",
    "descriptor": "",
    "authors": [
      "Danila Rukhovich",
      "Anna Vorontsova",
      "Anton Konushin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.01178"
  },
  {
    "id": "arXiv:2106.02078",
    "title": "Improving Neural Network Robustness via Persistency of Excitation",
    "abstract": "Improving Neural Network Robustness via Persistency of Excitation",
    "descriptor": "",
    "authors": [
      "Kaustubh Sridhar",
      "Oleg Sokolsky",
      "Insup Lee",
      "James Weimer"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2106.02078"
  },
  {
    "id": "arXiv:2106.02713",
    "title": "Learning Curves for SGD on Structured Features",
    "abstract": "Comments: Added new analysis of optimal batchsize and learning rate. Provided theoretical learning curves for case where test and training measures are different and apply to predicting errors for test/train splits on real datasets. Also provided a new bound for non-Gaussian features based on a regularity condition proposed by Varre et al 2021 arXiv:2102.03183",
    "descriptor": "\nComments: Added new analysis of optimal batchsize and learning rate. Provided theoretical learning curves for case where test and training measures are different and apply to predicting errors for test/train splits on real datasets. Also provided a new bound for non-Gaussian features based on a regularity condition proposed by Varre et al 2021 arXiv:2102.03183\n",
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02713"
  },
  {
    "id": "arXiv:2106.02886",
    "title": "Context-Aware Sparse Deep Coordination Graphs",
    "abstract": "Context-Aware Sparse Deep Coordination Graphs",
    "descriptor": "",
    "authors": [
      "Tonghan Wang",
      "Liang Zeng",
      "Weijun Dong",
      "Qianlan Yang",
      "Yang Yu",
      "Chongjie Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02886"
  },
  {
    "id": "arXiv:2106.04258",
    "title": "Interpretable agent communication from scratch (with a generic visual  processor emerging on the side)",
    "abstract": "Comments: Accepted at NeurIPS 2021",
    "descriptor": "\nComments: Accepted at NeurIPS 2021\n",
    "authors": [
      "Roberto Dess\u00ec",
      "Eugene Kharitonov",
      "Marco Baroni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2106.04258"
  },
  {
    "id": "arXiv:2106.05319",
    "title": "Stein Latent Optimization for Generative Adversarial Networks",
    "abstract": "Stein Latent Optimization for Generative Adversarial Networks",
    "descriptor": "",
    "authors": [
      "Uiwon Hwang",
      "Heeseung Kim",
      "Dahuin Jung",
      "Hyemi Jang",
      "Hyungyu Lee",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.05319"
  },
  {
    "id": "arXiv:2106.06603",
    "title": "A Shuffling Framework for Local Differential Privacy",
    "abstract": "A Shuffling Framework for Local Differential Privacy",
    "descriptor": "",
    "authors": [
      "Casey Meehan",
      "Amrita Roy Chowdhury",
      "Kamalika Chaudhuri",
      "Somesh Jha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06603"
  },
  {
    "id": "arXiv:2106.06950",
    "title": "An efficient way to manage ranges of data with Wise Red-Black Trees",
    "abstract": "Comments: Added references to order-statistic trees. Corrected some terms and form. Results unchanged",
    "descriptor": "\nComments: Added references to order-statistic trees. Corrected some terms and form. Results unchanged\n",
    "authors": [
      "Alberto Boffi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.06950"
  },
  {
    "id": "arXiv:2106.07306",
    "title": "Constraining Linear-chain CRFs to Regular Languages",
    "abstract": "Constraining Linear-chain CRFs to Regular Languages",
    "descriptor": "",
    "authors": [
      "Sean Papay",
      "Roman Klinger",
      "Sebastian Pad\u00f3"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.07306"
  },
  {
    "id": "arXiv:2106.07385",
    "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP  Contributions for a Research Knowledge Graph",
    "abstract": "Comments: 13 pages, 5 figures, 8 tables",
    "descriptor": "\nComments: 13 pages, 5 figures, 8 tables\n",
    "authors": [
      "Jennifer D'Souza",
      "S\u00f6ren Auer",
      "Ted Pedersen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07385"
  },
  {
    "id": "arXiv:2106.08209",
    "title": "Can Evil IoT Twins Be Identified? Now Yes, a Hardware Behavioral  Fingerprinting Methodology",
    "abstract": "Can Evil IoT Twins Be Identified? Now Yes, a Hardware Behavioral  Fingerprinting Methodology",
    "descriptor": "",
    "authors": [
      "Pedro Miguel S\u00e1nchez S\u00e1nchez",
      "Jos\u00e9 Mar\u00eda Jorquera Valero",
      "Alberto Huertas Celdr\u00e1n",
      "G\u00e9r\u00f4me Bovet",
      "Manuel Gil P\u00e9rez",
      "Gregorio Mart\u00ednez P\u00e9rez"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.08209"
  },
  {
    "id": "arXiv:2106.08693",
    "title": "ParticleAugment: Sampling-Based Data Augmentation",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Alexander Tsaregorodtsev",
      "Vasileios Belagiannis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08693"
  },
  {
    "id": "arXiv:2106.08761",
    "title": "Toward Affective XAI: Facial Affect Analysis for Understanding  Explainable Human-AI Interactions",
    "abstract": "Toward Affective XAI: Facial Affect Analysis for Understanding  Explainable Human-AI Interactions",
    "descriptor": "",
    "authors": [
      "Luke Guerdan",
      "Alex Raymond",
      "Hatice Gunes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.08761"
  },
  {
    "id": "arXiv:2106.11609",
    "title": "Distributional Gradient Matching for Learning Uncertain Neural Dynamics  Models",
    "abstract": "Comments: Published at NeurIPS 2021",
    "descriptor": "\nComments: Published at NeurIPS 2021\n",
    "authors": [
      "Lenart Treven",
      "Philippe Wenk",
      "Florian D\u00f6rfler",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.11609"
  },
  {
    "id": "arXiv:2106.12248",
    "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To  Pyramidal Bayesian Models",
    "abstract": "Comments: Preprint submitted to ICLR 2022",
    "descriptor": "\nComments: Preprint submitted to ICLR 2022\n",
    "authors": [
      "Louis Rouillard",
      "Demian Wassermann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.12248"
  },
  {
    "id": "arXiv:2107.00594",
    "title": "Pretext Tasks selection for multitask self-supervised speech  representation learning",
    "abstract": "Pretext Tasks selection for multitask self-supervised speech  representation learning",
    "descriptor": "",
    "authors": [
      "Salah Zaiem",
      "Titouan Parcollet",
      "Slim Essid"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.00594"
  },
  {
    "id": "arXiv:2107.01510",
    "title": "Directed Percolation in Temporal Networks",
    "abstract": "Comments: Implementation available at this https URL",
    "descriptor": "\nComments: Implementation available at this https URL\n",
    "authors": [
      "Arash Badie-Modiri",
      "Abbas K. Rizi",
      "M\u00e1rton Karsai",
      "Mikko Kivel\u00e4"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2107.01510"
  },
  {
    "id": "arXiv:2107.01602",
    "title": "Graphical State Space Model",
    "abstract": "Graphical State Space Model",
    "descriptor": "",
    "authors": [
      "Shaolin L\u00fc"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2107.01602"
  },
  {
    "id": "arXiv:2107.02474",
    "title": "Viscos Flows: Variational Schur Conditional Sampling With Normalizing  Flows",
    "abstract": "Viscos Flows: Variational Schur Conditional Sampling With Normalizing  Flows",
    "descriptor": "",
    "authors": [
      "Vincent Moens",
      "Aivar Sootla",
      "Haitham Bou Ammar",
      "Jun Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.02474"
  },
  {
    "id": "arXiv:2107.02667",
    "title": "Galerkin--Chebyshev approximation of Gaussian random fields on compact  Riemannian manifolds",
    "abstract": "Comments: Version submitted to a peer-reviewed journal. Changes: fixed residual typos, new outline. 33 pages, 5 figures",
    "descriptor": "\nComments: Version submitted to a peer-reviewed journal. Changes: fixed residual typos, new outline. 33 pages, 5 figures\n",
    "authors": [
      "Annika Lang",
      "Mike Pereira"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2107.02667"
  },
  {
    "id": "arXiv:2107.05175",
    "title": "Strategyproof Mechanisms For Group-Fair Facility Location Problems",
    "abstract": "Strategyproof Mechanisms For Group-Fair Facility Location Problems",
    "descriptor": "",
    "authors": [
      "Houyu Zhou",
      "Minming Li",
      "Hau Chan"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2107.05175"
  },
  {
    "id": "arXiv:2107.05419",
    "title": "A New Approach for Active Automata Learning Based on Apartness",
    "abstract": "A New Approach for Active Automata Learning Based on Apartness",
    "descriptor": "",
    "authors": [
      "Frits Vaandrager",
      "Bharat Garhewal",
      "Jurriaan Rot",
      "Thorsten Wi\u00dfmann"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2107.05419"
  },
  {
    "id": "arXiv:2107.09088",
    "title": "Reward-Weighted Regression Converges to a Global Optimum",
    "abstract": "Comments: 7 pages in main text + 2 pages of references + 6 pages of appendices, 1 figure in main text + 1 figure in appendices; source code available at this https URL",
    "descriptor": "\nComments: 7 pages in main text + 2 pages of references + 6 pages of appendices, 1 figure in main text + 1 figure in appendices; source code available at this https URL\n",
    "authors": [
      "Miroslav \u0160trupl",
      "Francesco Faccio",
      "Dylan R. Ashley",
      "Rupesh Kumar Srivastava",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.09088"
  },
  {
    "id": "arXiv:2107.11020",
    "title": "Emotion analysis and detection during COVID-19",
    "abstract": "Emotion analysis and detection during COVID-19",
    "descriptor": "",
    "authors": [
      "Tiberiu Sosea",
      "Chau Pham",
      "Alexander Tekle",
      "Cornelia Caragea",
      "Junyi Jessy Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2107.11020"
  },
  {
    "id": "arXiv:2107.11621",
    "title": "FedLab: A Flexible Federated Learning Framework",
    "abstract": "FedLab: A Flexible Federated Learning Framework",
    "descriptor": "",
    "authors": [
      "Dun Zeng",
      "Siqi Liang",
      "Xiangjing Hu",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.11621"
  },
  {
    "id": "arXiv:2107.12079",
    "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information",
    "abstract": "Comments: 9 pages, 2 figures, Accepted at CLAR 2021",
    "descriptor": "\nComments: 9 pages, 2 figures, Accepted at CLAR 2021\n",
    "authors": [
      "Bettina Fazzinga",
      "Andrea Galassi",
      "Paolo Torroni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.12079"
  },
  {
    "id": "arXiv:2108.00825",
    "title": "A physiology-inspired framework for holistic city simulations",
    "abstract": "Comments: 34 pages (main content: 25 pages), 5 figures",
    "descriptor": "\nComments: 34 pages (main content: 25 pages), 5 figures\n",
    "authors": [
      "Irene Meta",
      "Fernando M. Cucchietti",
      "Diego Navarro",
      "Eduardo Graells-Garrido",
      "Vicente Guallart"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2108.00825"
  },
  {
    "id": "arXiv:2108.03039",
    "title": "Identifiable Energy-based Representations: An Application to Estimating  Heterogeneous Causal Effects",
    "abstract": "Comments: 19 pages, 2 figures, 9 tables",
    "descriptor": "\nComments: 19 pages, 2 figures, 9 tables\n",
    "authors": [
      "Yao Zhang",
      "Jeroen Berrevoets",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2108.03039"
  },
  {
    "id": "arXiv:2108.06096",
    "title": "SHACL: A Description Logic in Disguise",
    "abstract": "Comments: Presented at BNAIC/BeneLearn 2021",
    "descriptor": "\nComments: Presented at BNAIC/BeneLearn 2021\n",
    "authors": [
      "Bart Bogaerts",
      "Maxime Jakubowski",
      "Jan Van den Bussche"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2108.06096"
  },
  {
    "id": "arXiv:2108.06545",
    "title": "PICCOLO: Point Cloud-Centric Omnidirectional Localization",
    "abstract": "Comments: Accepted to ICCV 2021",
    "descriptor": "\nComments: Accepted to ICCV 2021\n",
    "authors": [
      "Junho Kim",
      "Changwoon Choi",
      "Hojun Jang",
      "Young Min Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.06545"
  },
  {
    "id": "arXiv:2108.07037",
    "title": "Research on Brick Schema Representation for Building Operation with  Variable Refrigerant Flow Systems",
    "abstract": "Research on Brick Schema Representation for Building Operation with  Variable Refrigerant Flow Systems",
    "descriptor": "",
    "authors": [
      "Jingming Li",
      "Nianping Li",
      "Rui Yan",
      "Kushnazarov Farruh",
      "Anbang Li",
      "Kehua Li"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2108.07037"
  },
  {
    "id": "arXiv:2108.08102",
    "title": "Affective Decoding for Empathetic Response Generation",
    "abstract": "Comments: Long paper accepted to INLG 2021",
    "descriptor": "\nComments: Long paper accepted to INLG 2021\n",
    "authors": [
      "Chengkun Zeng",
      "Guanyi Chen",
      "Chenghua Lin",
      "Ruizhe Li",
      "Zhigang Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.08102"
  },
  {
    "id": "arXiv:2108.10335",
    "title": "edge-SR: Super-Resolution For The Masses",
    "abstract": "Comments: In WACV 2022. Code available in this https URL",
    "descriptor": "\nComments: In WACV 2022. Code available in this https URL\n",
    "authors": [
      "Pablo Navarrete Michelini",
      "Yunhua Lu",
      "Xingqun Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2108.10335"
  },
  {
    "id": "arXiv:2108.10643",
    "title": "Morality-based Assertion and Homophily on Social Media: A Cultural  Comparison between English and Japanese Languages",
    "abstract": "Comments: 21 pages, 7 figures, 1 Table, 6 supplementary figures, Accepted in Frontiers in Psychology",
    "descriptor": "\nComments: 21 pages, 7 figures, 1 Table, 6 supplementary figures, Accepted in Frontiers in Psychology\n",
    "authors": [
      "Maneet Singh",
      "Rishemjit Kaur",
      "Akiko Matsuo",
      "S.R.S. Iyengar",
      "Kazutoshi Sasahara"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2108.10643"
  },
  {
    "id": "arXiv:2108.13757",
    "title": "Automatic labelling of urban point clouds using data fusion",
    "abstract": "Comments: 5 pages, 5 figures; minor changes and improved results w.r.t. v1. Presented at the 10th Intl. Workshop on Urban Computing at ACM SIGSPATIAL 2021. Code for this paper is available at this https URL",
    "descriptor": "\nComments: 5 pages, 5 figures; minor changes and improved results w.r.t. v1. Presented at the 10th Intl. Workshop on Urban Computing at ACM SIGSPATIAL 2021. Code for this paper is available at this https URL\n",
    "authors": [
      "Daan Bloembergen",
      "Chris Eijgenstein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.13757"
  },
  {
    "id": "arXiv:2109.00522",
    "title": "Conditional Extreme Value Theory for Open Set Video Domain Adaptation",
    "abstract": "Comments: Camera-ready. Accepted by ACM International Conference on Multimedia in Asia 2021 (MMAsia 2021)",
    "descriptor": "\nComments: Camera-ready. Accepted by ACM International Conference on Multimedia in Asia 2021 (MMAsia 2021)\n",
    "authors": [
      "Zhuoxiao Chen",
      "Yadan Luo",
      "Mahsa Baktashmotlagh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2109.00522"
  },
  {
    "id": "arXiv:2109.00619",
    "title": "Learning compositional programs with arguments and sampling",
    "abstract": "Comments: Accepted at the Tenth International Workshop on Statistical Relational AI (International Joint Conference on Learning & Reasoning 2021)",
    "descriptor": "\nComments: Accepted at the Tenth International Workshop on Statistical Relational AI (International Joint Conference on Learning & Reasoning 2021)\n",
    "authors": [
      "Giovanni De Toni",
      "Luca Erculiani",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.00619"
  },
  {
    "id": "arXiv:2109.00644",
    "title": "RIFLE: Robust Inference from Low Order Marginals",
    "abstract": "Comments: 32 pages, 10 figures",
    "descriptor": "\nComments: 32 pages, 10 figures\n",
    "authors": [
      "Sina Baharlouei",
      "Kelechi Ogudu",
      "Sze-chuan Suen",
      "Meisam Razaviyayn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2109.00644"
  },
  {
    "id": "arXiv:2109.04726",
    "title": "AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction",
    "abstract": "Comments: 10 pages, 12 figures, Best paper at TrustNLP@NAACL 2021 and presented at WeaSuL@ICLR 2021",
    "descriptor": "\nComments: 10 pages, 12 figures, Best paper at TrustNLP@NAACL 2021 and presented at WeaSuL@ICLR 2021\n",
    "authors": [
      "Dong-Ho Lee",
      "Ravi Kiran Selvam",
      "Sheikh Muhammad Sarwar",
      "Bill Yuchen Lin",
      "Mahak Agarwal",
      "Fred Morstatter",
      "Jay Pujara",
      "Elizabeth Boschee",
      "James Allan",
      "Xiang Ren"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2109.04726"
  },
  {
    "id": "arXiv:2109.10187",
    "title": "Oriented Object Detection in Aerial Images Based on Area Ratio of  Parallelogram",
    "abstract": "Oriented Object Detection in Aerial Images Based on Area Ratio of  Parallelogram",
    "descriptor": "",
    "authors": [
      "Xinyi Yu",
      "Mi Lin",
      "Jiangping Lu",
      "Linlin Ou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.10187"
  },
  {
    "id": "arXiv:2109.10902",
    "title": "Mixed-supervised segmentation: Confidence maximization helps knowledge  distillation",
    "abstract": "Comments: This article is a journal extension of our paper in IPMI 2021 arXiv:2012.08051 . Currently under review at Medical Image Analysis. Code available at this https URL",
    "descriptor": "\nComments: This article is a journal extension of our paper in IPMI 2021 arXiv:2012.08051 . Currently under review at Medical Image Analysis. Code available at this https URL\n",
    "authors": [
      "Bingyuan Liu",
      "Christian Desrosiers",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.10902"
  },
  {
    "id": "arXiv:2109.12290",
    "title": "Distributed Computation of Stochastic GNE with Partial Information: An  Augmented Best-Response Approach",
    "abstract": "Distributed Computation of Stochastic GNE with Partial Information: An  Augmented Best-Response Approach",
    "descriptor": "",
    "authors": [
      "Yuanhanqing Huang",
      "Jianghai Hu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.12290"
  },
  {
    "id": "arXiv:2109.12508",
    "title": "LINDA: Multi-Agent Local Information Decomposition for Awareness of  Teammates",
    "abstract": "LINDA: Multi-Agent Local Information Decomposition for Awareness of  Teammates",
    "descriptor": "",
    "authors": [
      "Jiahan Cao",
      "Lei Yuan",
      "Jianhao Wang",
      "Shaowei Zhang",
      "Chongjie Zhang",
      "Yang Yu",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2109.12508"
  },
  {
    "id": "arXiv:2109.13348",
    "title": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in  the UMLS Metathesaurus",
    "abstract": "Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in  the UMLS Metathesaurus",
    "descriptor": "",
    "authors": [
      "Goonmeet Bajaj",
      "Vinh Nguyen",
      "Thilini Wijesiriwardene",
      "Hong Yung Yip",
      "Vishesh Javangula",
      "Srinivasan Parthasarathy",
      "Amit Sheth",
      "Olivier Bodenreider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.13348"
  },
  {
    "id": "arXiv:2109.13792",
    "title": "Cluster Synchronization of Networks via a Canonical Transformation for  Simultaneous Block Diagonalization of Matrices",
    "abstract": "Comments: Accepted for publication in Chaos: An Interdisciplinary Journal of Nonlinear Science",
    "descriptor": "\nComments: Accepted for publication in Chaos: An Interdisciplinary Journal of Nonlinear Science\n",
    "authors": [
      "Shirin Panahi",
      "Isaac Klickstein",
      "Francesco Sorrentino"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.13792"
  },
  {
    "id": "arXiv:2109.14333",
    "title": "Distribution Knowledge Embedding for Graph Pooling",
    "abstract": "Comments: 8 pages, 4 figures, 4 tables",
    "descriptor": "\nComments: 8 pages, 4 figures, 4 tables\n",
    "authors": [
      "Kaixuan Chen",
      "Jie Song",
      "Shunyu Liu",
      "Na Yu",
      "Zunlei Feng",
      "Gengshi Han",
      "Mingli Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.14333"
  },
  {
    "id": "arXiv:2109.15231",
    "title": "Real-Time Tactile Grasp Force Sensing Using Fingernail Imaging via Deep  Neural Networks",
    "abstract": "Comments: pending co-authors permission to publish",
    "descriptor": "\nComments: pending co-authors permission to publish\n",
    "authors": [
      "Navid Fallahinia",
      "Stephen Mascaro"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.15231"
  },
  {
    "id": "arXiv:2110.00534",
    "title": "TEACh: Task-driven Embodied Agents that Chat",
    "abstract": "Comments: 7 pages main, 28 pages total, 29 figures; Version 2 includes information on data cleaning and experimental results use a modified data split that has been released",
    "descriptor": "\nComments: 7 pages main, 28 pages total, 29 figures; Version 2 includes information on data cleaning and experimental results use a modified data split that has been released\n",
    "authors": [
      "Aishwarya Padmakumar",
      "Jesse Thomason",
      "Ayush Shrivastava",
      "Patrick Lange",
      "Anjali Narayan-Chen",
      "Spandana Gella",
      "Robinson Piramuthu",
      "Gokhan Tur",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.00534"
  },
  {
    "id": "arXiv:2110.00678",
    "title": "Speech Technology for Everyone: Automatic Speech Recognition for  Non-Native English with Transfer Learning",
    "abstract": "Comments: All authors contributed equally. Paper accepted to International Conference on Natural Language and Speech Processing 2021 (ICNLSP 2021)",
    "descriptor": "\nComments: All authors contributed equally. Paper accepted to International Conference on Natural Language and Speech Processing 2021 (ICNLSP 2021)\n",
    "authors": [
      "Toshiko Shibano",
      "Xinyi Zhang",
      "Mia Taige Li",
      "Haejin Cho",
      "Peter Sullivan",
      "Muhammad Abdul-Mageed"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.00678"
  },
  {
    "id": "arXiv:2110.01387",
    "title": "Machine Learning with Knowledge Constraints for Process Optimization of  Open-Air Perovskite Solar Cell Manufacturing",
    "abstract": "Machine Learning with Knowledge Constraints for Process Optimization of  Open-Air Perovskite Solar Cell Manufacturing",
    "descriptor": "",
    "authors": [
      "Zhe Liu",
      "Nicholas Rolston",
      "Austin C. Flick",
      "Thomas Colburn",
      "Zekun Ren",
      "Reinhold H. Dauskardt",
      "Tonio Buonassisi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.01387"
  },
  {
    "id": "arXiv:2110.01580",
    "title": "Skew cyclic codes over $\\mathbb{Z}_4+v\\mathbb{Z}_4$ with derivation",
    "abstract": "Comments: 27 pages",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Djoko Suprijanto",
      "Hopein Christofen Tang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.01580"
  },
  {
    "id": "arXiv:2110.01660",
    "title": "HDR-cGAN: Single LDR to HDR Image Translation using Conditional GAN",
    "abstract": "Comments: Accepted in ICVGIP 2021",
    "descriptor": "\nComments: Accepted in ICVGIP 2021\n",
    "authors": [
      "Prarabdh Raipurkar",
      "Rohil Pal",
      "Shanmuganathan Raman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2110.01660"
  },
  {
    "id": "arXiv:2110.01786",
    "title": "MoEfication: Conditional Computation of Transformer Models for Efficient  Inference",
    "abstract": "MoEfication: Conditional Computation of Transformer Models for Efficient  Inference",
    "descriptor": "",
    "authors": [
      "Zhengyan Zhang",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Peng Li",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.01786"
  },
  {
    "id": "arXiv:2110.01833",
    "title": "Attaining Interpretability in Reinforcement Learning via Hierarchical  Primitive Composition",
    "abstract": "Comments: \\",
    "descriptor": "\nComments: \\\n",
    "authors": [
      "Jeong-Hoon Lee",
      "Jongeun Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.01833"
  },
  {
    "id": "arXiv:2110.02485",
    "title": "Tensor regularization by truncated iteration: a comparison of some  solution methods for large-scale linear discrete ill-posed problem with a  t-product",
    "abstract": "Tensor regularization by truncated iteration: a comparison of some  solution methods for large-scale linear discrete ill-posed problem with a  t-product",
    "descriptor": "",
    "authors": [
      "Ugochukwu O. Ugwu",
      "Lothar Reichel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.02485"
  },
  {
    "id": "arXiv:2110.03549",
    "title": "Bias-Variance Tradeoffs in Single-Sample Binary Gradient Estimators",
    "abstract": "Comments: 22 pages, GCPR 2021",
    "descriptor": "\nComments: 22 pages, GCPR 2021\n",
    "authors": [
      "Alexander Shekhovtsov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.03549"
  },
  {
    "id": "arXiv:2110.03684",
    "title": "Cross-Domain Imitation Learning via Optimal Transport",
    "abstract": "Comments: typos corrected, references added",
    "descriptor": "\nComments: typos corrected, references added\n",
    "authors": [
      "Arnaud Fickinger",
      "Samuel Cohen",
      "Stuart Russell",
      "Brandon Amos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03684"
  },
  {
    "id": "arXiv:2110.03761",
    "title": "A simple equivariant machine learning method for dynamics based on  scalars",
    "abstract": "A simple equivariant machine learning method for dynamics based on  scalars",
    "descriptor": "",
    "authors": [
      "Weichi Yao",
      "Kate Storey-Fisher",
      "David W. Hogg",
      "Soledad Villar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03761"
  },
  {
    "id": "arXiv:2110.04020",
    "title": "Pathologies in priors and inference for Bayesian transformers",
    "abstract": "Pathologies in priors and inference for Bayesian transformers",
    "descriptor": "",
    "authors": [
      "Tristan Cinquin",
      "Alexander Immer",
      "Max Horn",
      "Vincent Fortuin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.04020"
  },
  {
    "id": "arXiv:2110.04394",
    "title": "Identifying blockchain-based cryptocurrency accounts using investment  portfolios",
    "abstract": "Identifying blockchain-based cryptocurrency accounts using investment  portfolios",
    "descriptor": "",
    "authors": [
      "Amin Aghaee"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.04394"
  },
  {
    "id": "arXiv:2110.04405",
    "title": "Quantum pixel representations and compression for $N$-dimensional images",
    "abstract": "Quantum pixel representations and compression for $N$-dimensional images",
    "descriptor": "",
    "authors": [
      "Mercy G. Amankwah",
      "Daan Camps",
      "E. Wes Bethel",
      "Roel Van Beeumen",
      "Talita Perciano"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.04405"
  },
  {
    "id": "arXiv:2110.04745",
    "title": "Reinforcement Learning for Systematic FX Trading",
    "abstract": "Reinforcement Learning for Systematic FX Trading",
    "descriptor": "",
    "authors": [
      "Gabriel Borrageiro",
      "Nick Firoozye",
      "Paolo Barucca"
    ],
    "subjectives": [
      "Trading and Market Microstructure (q-fin.TR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.04745"
  },
  {
    "id": "arXiv:2110.05093",
    "title": "Consistency of the Full and Reduced Order Models for Evolve-Filter-Relax  Regularization of Convection-Dominated, Marginally-Resolved Flows",
    "abstract": "Consistency of the Full and Reduced Order Models for Evolve-Filter-Relax  Regularization of Convection-Dominated, Marginally-Resolved Flows",
    "descriptor": "",
    "authors": [
      "Maria Strazzullo",
      "Michele Girfoglio",
      "Francesco Ballarin",
      "Traian Iliescu",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.05093"
  },
  {
    "id": "arXiv:2110.05324",
    "title": "Learnable Adaptive Cosine Estimator (LACE) for Image Classification",
    "abstract": "Comments: Accepted to WACV 2022; 14 pages (including appendix), 3 figures",
    "descriptor": "\nComments: Accepted to WACV 2022; 14 pages (including appendix), 3 figures\n",
    "authors": [
      "Joshua Peeples",
      "Connor McCurley",
      "Sarah Walker",
      "Dylan Stewart",
      "Alina Zare"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.05324"
  },
  {
    "id": "arXiv:2110.05354",
    "title": "Internal Language Model Adaptation with Text-Only Data for End-to-End  Speech Recognition",
    "abstract": "Comments: 5 pages, submitted to ICASSP 2022",
    "descriptor": "\nComments: 5 pages, submitted to ICASSP 2022\n",
    "authors": [
      "Zhong Meng",
      "Yashesh Gaur",
      "Naoyuki Kanda",
      "Jinyu Li",
      "Xie Chen",
      "Yu Wu",
      "Yifan Gong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.05354"
  },
  {
    "id": "arXiv:2110.05975",
    "title": "Frame-level multi-channel speaker verification with large-scale ad-hoc  microphone arrays",
    "abstract": "Comments: 5 pages, 3 figures",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Chengdong Liang",
      "Jiadi Yao",
      "Xiao-Lei Zhang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.05975"
  },
  {
    "id": "arXiv:2110.06069",
    "title": "Generalized Memory Approximate Message Passing",
    "abstract": "Comments: This article provides a universal GMAMP framework including the existing OAMP/VAMP, GVAMP, and MAMP as instances. It gives new directions to construct low-complexity AMP algorithms for unitarily-invariant systems. BO-GMAMP is an example that overcomes the IID-matrix limitation of GAMP and avoids the high-complexity matrix inverse in GVAMP",
    "descriptor": "\nComments: This article provides a universal GMAMP framework including the existing OAMP/VAMP, GVAMP, and MAMP as instances. It gives new directions to construct low-complexity AMP algorithms for unitarily-invariant systems. BO-GMAMP is an example that overcomes the IID-matrix limitation of GAMP and avoids the high-complexity matrix inverse in GVAMP\n",
    "authors": [
      "Feiyan Tian",
      "Lei Liu",
      "Xiaoming Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.06069"
  },
  {
    "id": "arXiv:2110.06209",
    "title": "A Brief Introduction to Automatic Differentiation for Machine Learning",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Davan Harrison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Mathematical Software (cs.MS)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2110.06209"
  },
  {
    "id": "arXiv:2110.06354",
    "title": "Tell Me How to Survey: Literature Review Made Simple with Automatic  Reading Path Generation",
    "abstract": "Comments: 16 pages, 12 figures",
    "descriptor": "\nComments: 16 pages, 12 figures\n",
    "authors": [
      "Jiayuan Ding",
      "Tong Xiang",
      "Zijing Ou",
      "Wangyang Zuo",
      "Ruihui Zhao",
      "Chenghua Lin",
      "Yefeng Zheng",
      "Bang Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06354"
  },
  {
    "id": "arXiv:2110.06381",
    "title": "Meta Learning Low Rank Covariance Factors for Energy-Based Deterministic  Uncertainty",
    "abstract": "Meta Learning Low Rank Covariance Factors for Energy-Based Deterministic  Uncertainty",
    "descriptor": "",
    "authors": [
      "Jeffrey Ryan Willette",
      "Hae Beom Lee",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06381"
  },
  {
    "id": "arXiv:2110.06495",
    "title": "Cross-lingual COVID-19 Fake News Detection",
    "abstract": "Comments: Accepted by SDM at ICDM, data is available at this https URL",
    "descriptor": "\nComments: Accepted by SDM at ICDM, data is available at this https URL\n",
    "authors": [
      "Jiangshu Du",
      "Yingtong Dou",
      "Congying Xia",
      "Limeng Cui",
      "Jing Ma",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.06495"
  },
  {
    "id": "arXiv:2110.06526",
    "title": "Practice Problems for Hardware Engineers",
    "abstract": "Comments: 166 pages, Kindle Direct Publishing, Amazon 1st Edition",
    "descriptor": "\nComments: 166 pages, Kindle Direct Publishing, Amazon 1st Edition\n",
    "authors": [
      "Shahin Nazarian"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2110.06526"
  },
  {
    "id": "arXiv:2110.06536",
    "title": "NeurIPS 2021 Competition IGLU: Interactive Grounded Language  Understanding in a Collaborative Environment",
    "abstract": "NeurIPS 2021 Competition IGLU: Interactive Grounded Language  Understanding in a Collaborative Environment",
    "descriptor": "",
    "authors": [
      "Julia Kiseleva",
      "Ziming Li",
      "Mohammad Aliannejadi",
      "Shrestha Mohanty",
      "Maartje ter Hoeve",
      "Mikhail Burtsev",
      "Alexey Skrynnik",
      "Artem Zholus",
      "Aleksandr Panov",
      "Kavya Srinet",
      "Arthur Szlam",
      "Yuxuan Sun",
      "Katja Hofmann",
      "Michel Galley",
      "Ahmed Awadallah"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06536"
  },
  {
    "id": "arXiv:2110.06537",
    "title": "Well-classified Examples are Underestimated in Classification with Deep  Neural Networks",
    "abstract": "Comments: 16 pages, 11 figures, 13 tables",
    "descriptor": "\nComments: 16 pages, 11 figures, 13 tables\n",
    "authors": [
      "Guangxiang Zhao",
      "Wenkai Yang",
      "Xuancheng Ren",
      "Lei Li",
      "Xu Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06537"
  },
  {
    "id": "arXiv:2110.06565",
    "title": "Duality Temporal-channel-frequency Attention Enhanced Speaker  Representation Learning",
    "abstract": "Duality Temporal-channel-frequency Attention Enhanced Speaker  Representation Learning",
    "descriptor": "",
    "authors": [
      "Li Zhang",
      "Qing Wang",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.06565"
  },
  {
    "id": "arXiv:2110.06581",
    "title": "Averting A Crisis In Simulation-Based Inference",
    "abstract": "Averting A Crisis In Simulation-Based Inference",
    "descriptor": "",
    "authors": [
      "Joeri Hermans",
      "Arnaud Delaunoy",
      "Fran\u00e7ois Rozet",
      "Antoine Wehenkel",
      "Gilles Louppe"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06581"
  },
  {
    "id": "arXiv:2110.06736",
    "title": "Do We Need to Directly Access the Source Datasets for Domain  Generalization?",
    "abstract": "Do We Need to Directly Access the Source Datasets for Domain  Generalization?",
    "descriptor": "",
    "authors": [
      "Junkun Yuan",
      "Xu Ma",
      "Defang Chen",
      "Kun Kuang",
      "Fei Wu",
      "Lanfen Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.06736"
  },
  {
    "id": "arXiv:2110.06742",
    "title": "A Review of the Deep Sea Treasure problem as a Multi-Objective  Reinforcement Learning Benchmark",
    "abstract": "Comments: 10 pages, 4 figures; Added supplementary materials (proofs + default configs)",
    "descriptor": "\nComments: 10 pages, 4 figures; Added supplementary materials (proofs + default configs)\n",
    "authors": [
      "Thomas Cassimon",
      "Reinout Eyckerman",
      "Siegfried Mercelis",
      "Steven Latr\u00e9",
      "Peter Hellinckx"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06742"
  },
  {
    "id": "arXiv:2110.06766",
    "title": "Next-Best-View Estimation based on Deep Reinforcement Learning for  Active Object Classification",
    "abstract": "Comments: 9 pages, 11 figures, 4 tables, preprint, Github repo: this https URL",
    "descriptor": "\nComments: 9 pages, 11 figures, 4 tables, preprint, Github repo: this https URL\n",
    "authors": [
      "Christian Korbach",
      "Markus D. Solbach",
      "Raphael Memmesheimer",
      "Dietrich Paulus",
      "John K. Tsotsos"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.06766"
  },
  {
    "id": "arXiv:2110.06823",
    "title": "A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model  for Multi-turn Dialogue Generation",
    "abstract": "A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model  for Multi-turn Dialogue Generation",
    "descriptor": "",
    "authors": [
      "Zihao Wang",
      "Ming Jiang",
      "Junli Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.06823"
  },
  {
    "id": "arXiv:2110.06871",
    "title": "Two-argument activation functions learn soft XOR operations like  cortical neurons",
    "abstract": "Two-argument activation functions learn soft XOR operations like  cortical neurons",
    "descriptor": "",
    "authors": [
      "Kijung Yoon",
      "Emin Orhan",
      "Juhyun Kim",
      "Xaq Pitkow"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.06871"
  },
  {
    "id": "arXiv:2110.07098",
    "title": "Escaping Saddle Points in Nonconvex Minimax Optimization via  Cubic-Regularized Gradient Descent-Ascent",
    "abstract": "Comments: 23 pages, no figures. arXiv admin note: text overlap with arXiv:2102.04653",
    "descriptor": "\nComments: 23 pages, no figures. arXiv admin note: text overlap with arXiv:2102.04653\n",
    "authors": [
      "Ziyi Chen",
      "Qunwei Li",
      "Yi Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07098"
  },
  {
    "id": "arXiv:2110.07116",
    "title": "Auxiliary Loss of Transformer with Residual Connection for End-to-End  Speaker Diarization",
    "abstract": "Comments: Submitted to ICASSP 2022, equal contribution from first two authors",
    "descriptor": "\nComments: Submitted to ICASSP 2022, equal contribution from first two authors\n",
    "authors": [
      "Yechan Yu",
      "Dongkeon Park",
      "Hong Kook Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.07116"
  },
  {
    "id": "arXiv:2110.07225",
    "title": "Web Search via an Efficient and Effective Brain-Machine Interface",
    "abstract": "Web Search via an Efficient and Effective Brain-Machine Interface",
    "descriptor": "",
    "authors": [
      "Xuesong Chen",
      "Ziyi Ye",
      "Xiaohui Xie",
      "Yiqun Liu",
      "Weihang Su",
      "Shuqi Zhu",
      "Min Zhang",
      "Shaoping Ma"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.07225"
  },
  {
    "id": "arXiv:2110.07272",
    "title": "Relighting Humans in the Wild: Monocular Full-Body Human Relighting with  Domain Adaptation",
    "abstract": "Comments: Accepted to Pacific Graphics 2021, project page: this http URL",
    "descriptor": "\nComments: Accepted to Pacific Graphics 2021, project page: this http URL\n",
    "authors": [
      "Daichi Tajima",
      "Yoshihiro Kanamori",
      "Yuki Endo"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.07272"
  },
  {
    "id": "arXiv:2110.07276",
    "title": "Carousel Memory: Rethinking the Design of Episodic Memory for Continual  Learning",
    "abstract": "Carousel Memory: Rethinking the Design of Episodic Memory for Continual  Learning",
    "descriptor": "",
    "authors": [
      "Soobee Lee",
      "Minindu Weerakoon",
      "Jonghyun Choi",
      "Minjia Zhang",
      "Di Wang",
      "Myeongjae Jeon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.07276"
  },
  {
    "id": "arXiv:2110.07347",
    "title": "Improved Drug-target Interaction Prediction with Intermolecular Graph  Transformer",
    "abstract": "Improved Drug-target Interaction Prediction with Intermolecular Graph  Transformer",
    "descriptor": "",
    "authors": [
      "Siyuan Liu",
      "Yusong Wang",
      "Tong Wang",
      "Yifan Deng",
      "Liang He",
      "Bin Shao",
      "Jian Yin",
      "Nanning Zheng",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.07347"
  },
  {
    "id": "arXiv:2110.07402",
    "title": "Self-Supervised Learning by Estimating Twin Class Distributions",
    "abstract": "Comments: Technical report",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Feng Wang",
      "Tao Kong",
      "Rufeng Zhang",
      "Huaping Liu",
      "Hang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07402"
  },
  {
    "id": "arXiv:2110.07409",
    "title": "The Geometry of Memoryless Stochastic Policy Optimization in  Infinite-Horizon POMDPs",
    "abstract": "Comments: Preprint, 37 pages, 5 figures",
    "descriptor": "\nComments: Preprint, 37 pages, 5 figures\n",
    "authors": [
      "Johannes M\u00fcller",
      "Guido Mont\u00fafar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2110.07409"
  },
  {
    "id": "arXiv:2110.07421",
    "title": "On some batch code properties of the simplex code",
    "abstract": "On some batch code properties of the simplex code",
    "descriptor": "",
    "authors": [
      "Henk D.L. Hollmann",
      "Karan Khathuria",
      "Ago-Erik Riet",
      "Vitaly Skachek"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.07421"
  },
  {
    "id": "arXiv:2110.07509",
    "title": "TDACNN: Target-domain-free Domain Adaptation Convolutional Neural  Network for Drift Compensation in Gas Sensors",
    "abstract": "Comments: submitted to Sensors and Actuators B: Chemical",
    "descriptor": "\nComments: submitted to Sensors and Actuators B: Chemical\n",
    "authors": [
      "Yuelin Zhang",
      "Jia Yan",
      "Zehuan Wang",
      "Xiaoyan Peng",
      "Yutong Tian",
      "Shukai Duan"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07509"
  },
  {
    "id": "arXiv:2110.07604",
    "title": "NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in  the Wild",
    "abstract": "Comments: In NeurIPS 2021. v2: Fixed minor typos",
    "descriptor": "\nComments: In NeurIPS 2021. v2: Fixed minor typos\n",
    "authors": [
      "Jason Y. Zhang",
      "Gengshan Yang",
      "Shubham Tulsiani",
      "Deva Ramanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.07604"
  }
]