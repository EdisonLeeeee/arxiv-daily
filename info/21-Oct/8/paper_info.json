[
  {
    "id": "arXiv:2110.02987",
    "title": "Distributed Optimization of Graph Convolutional Network using Subgraph  Variance",
    "abstract": "In recent years, Graph Convolutional Networks (GCNs) have achieved great\nsuccess in learning from graph-structured data. With the growing tendency of\ngraph nodes and edges, GCN training by single processor cannot meet the demand\nfor time and memory, which led to a boom into distributed GCN training\nframeworks research. However, existing distributed GCN training frameworks\nrequire enormous communication costs between processors since multitudes of\ndependent nodes and edges information need to be collected and transmitted for\nGCN training from other processors. To address this issue, we propose a Graph\nAugmentation based Distributed GCN framework(GAD). In particular, GAD has two\nmain components, GAD-Partition and GAD-Optimizer. We first propose a graph\naugmentation-based partition (GAD-Partition) that can divide original graph\ninto augmented subgraphs to reduce communication by selecting and storing as\nfew significant nodes of other processors as possible while guaranteeing the\naccuracy of the training. In addition, we further design a subgraph\nvariance-based importance calculation formula and propose a novel weighted\nglobal consensus method, collectively referred to as GAD-Optimizer. This\noptimizer adaptively reduces the importance of subgraphs with large variances\nfor the purpose of reducing the effect of extra variance introduced by\nGAD-Partition on distributed GCN training. Extensive experiments on four\nlarge-scale real-world datasets demonstrate that our framework significantly\nreduces the communication overhead (50%), improves the convergence speed (2X)\nof distributed GCN training, and slight gain in accuracy (0.45%) based on\nminimal redundancy compared to the state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Taige Zhao",
      "Xiangyu Song",
      "Jianxin Li",
      "Wei Luo",
      "Imran Razzak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.02987"
  },
  {
    "id": "arXiv:2110.02991",
    "title": "NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for  Better Cause-Effect Span Detection",
    "abstract": "Automatic identification of cause-effect spans in financial documents is\nimportant for causality modelling and understanding reasons that lead to\nfinancial events. To exploit the observation that words are more connected to\nother words with the same cause-effect type in a dependency tree, we construct\nuseful graph embeddings by incorporating dependency relation features through a\ngraph neural network. Our model builds on a baseline BERT token classifier with\nViterbi decoding, and outperforms this baseline in cross-validation and during\nthe competition. In the official run of FinCausal 2021, we obtained Precision,\nRecall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place,\nand an Exact Match score of 86.05% which ranked 3rd place.",
    "descriptor": "",
    "authors": [
      "Fiona Anting Tan",
      "See-Kiong Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.02991"
  },
  {
    "id": "arXiv:2110.02994",
    "title": "Learning Canonical Embedding for Non-rigid Shape Matching",
    "abstract": "This paper provides a novel framework that learns canonical embeddings for\nnon-rigid shape matching. In contrast to prior work in this direction, our\nframework is trained end-to-end and thus avoids instabilities and constraints\nassociated with the commonly-used Laplace-Beltrami basis or sequential\noptimization schemes. On multiple datasets, we demonstrate that learning self\nsymmetry maps with a deep functional map projects 3D shapes into a low\ndimensional canonical embedding that facilitates non-rigid shape correspondence\nvia a simple nearest neighbor search. Our framework outperforms multiple recent\nlearning based methods on FAUST and SHREC benchmarks while being\ncomputationally cheaper, data-efficient, and robust.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Abhishek Sharma",
      "Maks Ovsjanikov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02994"
  },
  {
    "id": "arXiv:2110.02998",
    "title": "Federated Learning via Plurality Vote",
    "abstract": "Federated learning allows collaborative workers to solve a machine learning\nproblem while preserving data privacy. Recent studies have tackled various\nchallenges in federated learning, but the joint optimization of communication\noverhead, learning reliability, and deployment efficiency is still an open\nproblem. To this end, we propose a new scheme named federated learning via\nplurality vote (FedVote). In each communication round of FedVote, workers\ntransmit binary or ternary weights to the server with low communication\noverhead. The model parameters are aggregated via weighted voting to enhance\nthe resilience against Byzantine attacks. When deployed for inference, the\nmodel with binary or ternary weights is resource-friendly to edge devices. We\nshow that our proposed method can reduce quantization error and converges\nfaster compared with the methods directly quantizing the model updates.",
    "descriptor": "",
    "authors": [
      "Kai Yue",
      "Richeng Jin",
      "Chau-Wai Wong",
      "Huaiyu Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.02998"
  },
  {
    "id": "arXiv:2110.02999",
    "title": "Generative Modeling with Optimal Transport Maps",
    "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a\npowerful tool for large-scale generative modeling tasks. In these tasks, OT\ncost is typically used as the loss for training GANs. In contrast to this\napproach, we show that the OT map itself can be used as a generative model,\nproviding comparable performance. Previous analogous approaches consider OT\nmaps as generative models only in the latent spaces due to their poor\nperformance in the original high-dimensional ambient space. In contrast, we\napply OT maps directly in the ambient space, e.g., a space of high-dimensional\nimages. First, we derive a min-max optimization algorithm to efficiently\ncompute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we\nextend the approach to the case when the input and output distributions are\nlocated in the spaces of different dimensions and derive error bounds for the\ncomputed OT map. We evaluate the algorithm on image generation and unpaired\nimage restoration tasks. In particular, we consider denoising, colorization,\nand inpainting, where the optimality of the restoration map is a desired\nattribute, since the output (restored) image is expected to be close to the\ninput (degraded) one.",
    "descriptor": "\nComments: Preprint. Under review\n",
    "authors": [
      "Litu Rout",
      "Alexander Korotin",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02999"
  },
  {
    "id": "arXiv:2110.03006",
    "title": "Data-Centric Semi-Supervised Learning",
    "abstract": "We study unsupervised data selection for semi-supervised learning (SSL),\nwhere a large-scale unlabeled data is available and a small subset of data is\nbudgeted for label acquisition. Existing SSL methods focus on learning a model\nthat effectively integrates information from given small labeled data and large\nunlabeled data, whereas we focus on selecting the right data for SSL without\nany label or task information, in an also stark contrast to supervised data\nselection for active learning. Intuitively, instances to be labeled shall\ncollectively have maximum diversity and coverage for downstream tasks, and\nindividually have maximum information propagation utility for SSL. We formalize\nthese concepts in a three-step data-centric SSL method that improves FixMatch\nin stability and accuracy by 8% on CIFAR-10 (0.08% labeled) and 14% on\nImageNet-1K (0.2% labeled). Our work demonstrates that a small compute spent on\ncareful labeled data selection brings big annotation efficiency and model\nperformance gain without changing the learning pipeline. Our completely\nunsupervised data selection can be easily extended to other weakly supervised\nlearning settings.",
    "descriptor": "",
    "authors": [
      "Xudong Wang",
      "Long Lian",
      "Stella X. Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03006"
  },
  {
    "id": "arXiv:2110.03007",
    "title": "Unsupervised Multimodal Language Representations using Convolutional  Autoencoders",
    "abstract": "Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Panagiotis Koromilas",
      "Theodoros Giannakopoulos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.03007"
  },
  {
    "id": "arXiv:2110.03011",
    "title": "Empirical Analysis of Bi-directional Wi-Fi Network Performance on Mobile  Robots and Connected Vehicles",
    "abstract": "This paper proposes a framework to measure the important metrics (throughput,\ndelay, packet retransmits, signal strength, etc.) to determine Wi-Fi network\nperformance of mobile robots supported by the Robot Operating Systems (ROS)\nmiddleware. We analyze the bidirectional network performance of mobile robots\nand connected vehicles through an experimental setup, where a mobile robot is\ncommunicating vital sensor data such as video streaming from the camera(s) and\nLiDAR scan values to a command station while it navigates an indoor environment\nthrough teleoperated velocity commands received from the command station. The\nexperiments evaluate the performance under 2.4 GHz and 5 GHz channels with\ndifferent placement of Access Points (AP) with up to two network devices on\neach side. The discussions and insights from this study apply to the general\nvehicular networks and the field robotics community, where the wireless network\nplays a key role in enabling the success of robotic missions.",
    "descriptor": "",
    "authors": [
      "Pranav Pandey",
      "Ramviyas Parasuraman"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.03011"
  },
  {
    "id": "arXiv:2110.03014",
    "title": "Active Learning of Markov Decision Processes using Baum-Welch algorithm  (Extended)",
    "abstract": "Cyber-physical systems (CPSs) are naturally modelled as reactive systems with\nnondeterministic and probabilistic dynamics. Model-based verification\ntechniques have proved effective in the deployment of safety-critical CPSs.\nCentral for a successful application of such techniques is the construction of\nan accurate formal model for the system. Manual construction can be a\nresource-demanding and error-prone process, thus motivating the design of\nautomata learning algorithms to synthesise a system model from observed system\nbehaviours.\nThis paper revisits and adapts the classic Baum-Welch algorithm for learning\nMarkov decision processes and Markov chains. For the case of MDPs, which\ntypically demand more observations, we present a model-based active learning\nsampling strategy that choses examples which are most informative w.r.t.\\ the\ncurrent model hypothesis. We empirically compare our approach with\nstate-of-the-art tools and demonstrate that the proposed active learning\nprocedure can significantly reduce the number of observations required to\nobtain accurate models.",
    "descriptor": "\nComments: 7 pages, 7 figures, submitted and accepted (short) to ICMLA 2021\n",
    "authors": [
      "Giovanni Bacci",
      "Anna Ing\u00f3lfsd\u00f3ttir",
      "Kim Larsen",
      "Rapha\u00ebl Reynouard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.03014"
  },
  {
    "id": "arXiv:2110.03015",
    "title": "Fast and flexible preconditioners for solving multilinear systems",
    "abstract": "This paper investigates a type of fast and flexible preconditioners to solve\nmultilinear system $\\mathcal{A}\\textbf{x}^{m-1}=\\textbf{b}$ with\n$\\mathcal{M}$-tensor $\\mathcal{A}$ and obtains some important convergent\ntheorems about preconditioned Jacobi, Gauss-Seidel and SOR type iterative\nmethods. The main results theoretically prove that the preconditioners can\naccelerate the convergence of iterations. Numerical examples are presented to\nreverify the efficiency of the proposed preconditioned methods.",
    "descriptor": "",
    "authors": [
      "Eisa Khosravi Dehdezi",
      "Saeed Karimi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03015"
  },
  {
    "id": "arXiv:2110.03016",
    "title": "DeepBBS: Deep Best Buddies for Point Cloud Registration",
    "abstract": "Recently, several deep learning approaches have been proposed for point cloud\nregistration. These methods train a network to generate a representation that\nhelps finding matching points in two 3D point clouds. Finding good matches\nallows them to calculate the transformation between the point clouds\naccurately. Two challenges of these techniques are dealing with occlusions and\ngeneralizing to objects of classes unseen during training. This work proposes\nDeepBBS, a novel method for learning a representation that takes into account\nthe best buddy distance between points during training. Best Buddies (i.e.,\nmutual nearest neighbors) are pairs of points nearest to each other. The Best\nBuddies criterion is a strong indication for correct matches that, in turn,\nleads to accurate registration. Our experiments show improved performance\ncompared to previous methods. In particular, our learned representation leads\nto an accurate registration for partial shapes and in unseen categories.",
    "descriptor": "\nComments: Accepted to 3DV 2021\n",
    "authors": [
      "Itan Hezroni",
      "Amnon Drory",
      "Raja Giryes",
      "Shai Avidan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03016"
  },
  {
    "id": "arXiv:2110.03017",
    "title": "Two-Bit Aggregation for Communication Efficient and Differentially  Private Federated Learning",
    "abstract": "In federated learning (FL), a machine learning model is trained on multiple\nnodes in a decentralized manner, while keeping the data local and not shared\nwith other nodes. However, FL requires the nodes to also send information on\nthe model parameters to a central server for aggregation. However, the\ninformation sent from the nodes to the server may reveal some details about\neach node's local data, thus raising privacy concerns. Furthermore, the\nrepetitive uplink transmission from the nodes to the server may result in a\ncommunication overhead and network congestion. To address these two challenges,\nin this paper, a novel two-bit aggregation algorithm is proposed with\nguaranteed differential privacy and reduced uplink communication overhead.\nExtensive experiments demonstrate that the proposed aggregation algorithm can\nachieve the same performance as state-of-the-art approaches on datasets such as\nMNIST, Fashion MNIST, CIFAR-10, and CIFAR-100, while ensuring differential\nprivacy and improving communication efficiency.",
    "descriptor": "",
    "authors": [
      "Mohammad Aghapour",
      "Aidin Ferdowsi",
      "Walid Saad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.03017"
  },
  {
    "id": "arXiv:2110.03020",
    "title": "Efficient Methods for Online Multiclass Logistic Regression",
    "abstract": "Multiclass logistic regression is a fundamental task in machine learning with\napplications in classification and boosting. Previous work (Foster et al.,\n2018) has highlighted the importance of improper predictors for achieving \"fast\nrates\" in the online multiclass logistic regression problem without suffering\nexponentially from secondary problem parameters, such as the norm of the\npredictors in the comparison class. While Foster et al. (2018) introduced a\nstatistically optimal algorithm, it is in practice computationally intractable\ndue to its run-time complexity being a large polynomial in the time horizon and\ndimension of input feature vectors. In this paper, we develop a new algorithm,\nFOLKLORE, for the problem which runs significantly faster than the algorithm of\nFoster et al.(2018) -- the running time per iteration scales quadratically in\nthe dimension -- at the cost of a linear dependence on the norm of the\npredictors in the regret bound. This yields the first practical algorithm for\nonline multiclass logistic regression, resolving an open problem of Foster et\nal.(2018). Furthermore, we show that our algorithm can be applied to online\nbandit multiclass prediction and online multiclass boosting, yielding more\npractical algorithms for both problems compared to the ones in Foster et\nal.(2018) with similar performance guarantees. Finally, we also provide an\nonline-to-batch conversion result for our algorithm.",
    "descriptor": "",
    "authors": [
      "Naman Agarwal",
      "Satyen Kale",
      "Julian Zimmert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03020"
  },
  {
    "id": "arXiv:2110.03022",
    "title": "Tribuo: Machine Learning with Provenance in Java",
    "abstract": "Machine Learning models are deployed across a wide range of industries,\nperforming a wide range of tasks. Tracking these models and ensuring they\nbehave appropriately is becoming increasingly difficult as the number of\ndeployed models increases. There are also new regulatory burdens for ML systems\nwhich affect human lives, requiring a link between a model and its training\ndata in high-risk situations. Current ML monitoring systems often provide\nprovenance and experiment tracking as a layer on top of an ML library, allowing\nroom for imperfect tracking and skew between the tracked object and the\nmetadata. In this paper we introduce Tribuo, a Java ML library that integrates\nmodel training, inference, strong type-safety, runtime checking, and automatic\nprovenance recording into a single framework. All Tribuo's models and\nevaluations record the full processing pipeline for input data, along with the\ntraining algorithms, hyperparameters and data transformation steps\nautomatically. The provenance lives inside the model object and can be\npersisted separately using common markup formats. Tribuo implements many\npopular ML algorithms for classification, regression, clustering, multi-label\nclassification and anomaly detection, along with interfaces to XGBoost,\nTensorFlow and ONNX Runtime. Tribuo's source code is available at\nhttps://github.com/oracle/tribuo under an Apache 2.0 license with documentation\nand tutorials available at https://tribuo.org.",
    "descriptor": "",
    "authors": [
      "Adam Pocock"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03022"
  },
  {
    "id": "arXiv:2110.03024",
    "title": "A Fast Randomized Algorithm for Massive Text Normalization",
    "abstract": "Many popular machine learning techniques in natural language processing and\ndata mining rely heavily on high-quality text sources. However real-world text\ndatasets contain a significant amount of spelling errors and improperly\npunctuated variants where the performance of these models would quickly\ndeteriorate. Moreover, real-world, web-scale datasets contain hundreds of\nmillions or even billions of lines of text, where the existing text cleaning\ntools are prohibitively expensive to execute over and may require an overhead\nto learn the corrections. In this paper, we present FLAN, a scalable randomized\nalgorithm to clean and canonicalize massive text data. Our algorithm relies on\nthe Jaccard similarity between words to suggest correction results. We\nefficiently handle the pairwise word-to-word comparisons via Locality Sensitive\nHashing (LSH). We also propose a novel stabilization process to address the\nissue of hash collisions between dissimilar words, which is a consequence of\nthe randomized nature of LSH and is exacerbated by the massive scale of\nreal-world datasets. Compared with existing approaches, our method is more\nefficient, both asymptotically and in empirical evaluations, and does not rely\non additional features, such as lexical/phonetic similarity or word embedding\nfeatures. In addition, FLAN does not require any annotated data or supervised\nlearning. We further theoretically show the robustness of our algorithm with\nupper bounds on the false positive and false negative rates of corrections. Our\nexperimental results on real-world datasets demonstrate the efficiency and\nefficacy of FLAN.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Nan Jiang",
      "Chen Luo",
      "Vihan Lakshman",
      "Yesh Dattatreya",
      "Yexiang Xue"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03024"
  },
  {
    "id": "arXiv:2110.03026",
    "title": "Human Capabilities as Guiding Lights for the Field of AI-HRI: Insights  from Engineering Education",
    "abstract": "Social Justice oriented Engineering Education frameworks have been developed\nto help guide engineering students' decisions about which projects will\ngenuinely address human needs to create a better and more equitable society. In\nthis paper, we explore the role such theories might play in the field of\nAI-HRI, consider the extent to which our community is (or is not) aligned with\nthese recommendations, and envision a future in which our research community\ntakes guidance from these theories. In particular, we analyze recent AI-HRI\n(through analysis of 2020 AI-HRI papers) and consider possible futures of\nAI-HRI (through a speculative ethics exercise). Both activities are guided\nthrough the lens of the Engineering for Social Justice (E4SJ) framework, which\ncenters contextual listening and enhancement of human capabilities. Our\nanalysis suggests that current AI-HRI research is not well aligned with the\nguiding principles of Engineering for Social Justice, and as such, does not\nobviously meet the needs of the communities we could be helping most. As such,\nwe suggest that motivating future work through the E4SJ framework could help to\nensure that we as researchers are developing technologies that will actually\nlead to a more equitable world.",
    "descriptor": "\nComments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)\n",
    "authors": [
      "Tom Williams",
      "Ruchen Wen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.03026"
  },
  {
    "id": "arXiv:2110.03027",
    "title": "Dynamically Decoding Source Domain Knowledge For Unseen Domain  Generalization",
    "abstract": "Domain generalization is an important problem which has gain much attention\nrecently. While most existing studies focus on learning domain-invariant\nfeature representations, some researchers try ensemble learning of multi\nexperts and demonstrate promising performance. However, in existing\nmulti-expert learning frameworks, the source domain knowledge has not yet been\nmuch explored, resulting in sub-optimal performance. In this paper, we propose\nto adapt Transformers for the purpose of dynamically decoding source domain\nknowledge for domain generalization. Specifically, we build one domain-specific\nlocal expert per source domain, and one domain-agnostic feature branch as\nquery. Then, all local-domain features will be encoded by Transformer encoders,\nas source domain knowledge in memory. While in the Transformer decoders, the\ndomain-agnostic query will interact with the memory in the cross-attention\nmodule, where similar domains with the input will contribute more in the\nattention output. This way, the source domain knowledge will be dynamically\ndecoded for the inference of the current input from unseen domain. Therefore,\nthis mechanism makes the proposed method well generalizable to unseen domains.\nThe proposed method is evaluated on three benchmarks in the domain\ngeneralization field. The comparison with the state-of-the-art methods shows\nthat the proposed method achieves the best performance, outperforming the\nothers with a clear gap.",
    "descriptor": "",
    "authors": [
      "Cuicui Kang",
      "Karthik Nandakumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03027"
  },
  {
    "id": "arXiv:2110.03028",
    "title": "Reconsidering Optimistic Algorithms for Relational DBMS",
    "abstract": "At DBKDA 2019, we demonstrated that StrongDBMS with simple but rigorous\noptimistic algorithms, provides better performance in situations of high\nconcurrency than major commercial database management systems (DBMS). The\ndemonstration was convincing but the reasons for its success were not fully\nanalysed. There is a brief account of the results below. In this short\ncontribution, we wish to discuss the reasons for the results. The analysis\nleads to a strong criticism of all DBMS algorithms based on locking, and based\non these results, it is not fanciful to suggest that it is time to re-engineer\nexisting DBMS.",
    "descriptor": "\nComments: 4 pages, 3 figures, conference paper\n",
    "authors": [
      "Malcolm Crowe",
      "Fritz Laux"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.03028"
  },
  {
    "id": "arXiv:2110.03031",
    "title": "RieszNet and ForestRiesz: Automatic Debiased Machine Learning with  Neural Nets and Random Forests",
    "abstract": "Many causal and policy effects of interest are defined by linear functionals\nof high-dimensional or non-parametric regression functions.\n$\\sqrt{n}$-consistent and asymptotically normal estimation of the object of\ninterest requires debiasing to reduce the effects of regularization and/or\nmodel selection on the object of interest. Debiasing is typically achieved by\nadding a correction term to the plug-in estimator of the functional, that is\nderived based on a functional-specific theoretical derivation of what is known\nas the influence function and which leads to properties such as double\nrobustness and Neyman orthogonality. We instead implement an automatic\ndebiasing procedure based on automatically learning the Riesz representation of\nthe linear functional using Neural Nets and Random Forests. Our method solely\nrequires value query oracle access to the linear functional. We propose a\nmulti-tasking Neural Net debiasing method with stochastic gradient descent\nminimization of a combined Riesz representer and regression loss, while sharing\nrepresentation layers for the two functions. We also propose a Random Forest\nmethod which learns a locally linear representation of the Riesz function. Even\nthough our methodology applies to arbitrary functionals, we experimentally find\nthat it beats state of the art performance of the prior neural net based\nestimator of Shi et al. (2019) for the case of the average treatment effect\nfunctional. We also evaluate our method on the more challenging problem of\nestimating average marginal effects with continuous treatments, using\nsemi-synthetic data of gasoline price changes on gasoline demand.",
    "descriptor": "",
    "authors": [
      "Victor Chernozhukov",
      "Whitney K. Newey",
      "Victor Quintas-Martinez",
      "Vasilis Syrgkanis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03031"
  },
  {
    "id": "arXiv:2110.03032",
    "title": "Learning Multi-Objective Curricula for Deep Reinforcement Learning",
    "abstract": "Various automatic curriculum learning (ACL) methods have been proposed to\nimprove the sample efficiency and final performance of deep reinforcement\nlearning (DRL). They are designed to control how a DRL agent collects data,\nwhich is inspired by how humans gradually adapt their learning processes to\ntheir capabilities. For example, ACL can be used for subgoal generation, reward\nshaping, environment generation, or initial state generation. However, prior\nwork only considers curriculum learning following one of the aforementioned\npredefined paradigms. It is unclear which of these paradigms are complementary,\nand how the combination of them can be learned from interactions with the\nenvironment. Therefore, in this paper, we propose a unified automatic\ncurriculum learning framework to create multi-objective but coherent curricula\nthat are generated by a set of parametric curriculum modules. Each curriculum\nmodule is instantiated as a neural network and is responsible for generating a\nparticular curriculum. In order to coordinate those potentially conflicting\nmodules in unified parameter space, we propose a multi-task hyper-net learning\nframework that uses a single hyper-net to parameterize all those curriculum\nmodules. In addition to existing hand-designed curricula paradigms, we further\ndesign a flexible memory mechanism to learn an abstract curriculum, which may\notherwise be difficult to design manually. We evaluate our method on a series\nof robotic manipulation tasks and demonstrate its superiority over other\nstate-of-the-art ACL methods in terms of sample efficiency and final\nperformance.",
    "descriptor": "",
    "authors": [
      "Jikun Kang",
      "Miao Liu",
      "Abhinav Gupta",
      "Chris Pal",
      "Xue Liu",
      "Jie Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03032"
  },
  {
    "id": "arXiv:2110.03036",
    "title": "The Low-Resource Double Bind: An Empirical Study of Pruning for  Low-Resource Machine Translation",
    "abstract": "A \"bigger is better\" explosion in the number of parameters in deep neural\nnetworks has made it increasingly challenging to make state-of-the-art networks\naccessible in compute-restricted environments. Compression techniques have\ntaken on renewed importance as a way to bridge the gap. However, evaluation of\nthe trade-offs incurred by popular compression techniques has been centered on\nhigh-resource datasets. In this work, we instead consider the impact of\ncompression in a data-limited regime. We introduce the term low-resource double\nbind to refer to the co-occurrence of data limitations and compute resource\nconstraints. This is a common setting for NLP for low-resource languages, yet\nthe trade-offs in performance are poorly studied. Our work offers surprising\ninsights into the relationship between capacity and generalization in\ndata-limited regimes for the task of machine translation. Our experiments on\nmagnitude pruning for translations from English into Yoruba, Hausa, Igbo and\nGerman show that in low-resource regimes, sparsity preserves performance on\nfrequent sentences but has a disparate impact on infrequent ones. However, it\nimproves robustness to out-of-distribution shifts, especially for datasets that\nare very distinct from the training distribution. Our findings suggest that\nsparsity can play a beneficial role at curbing memorization of low frequency\nattributes, and therefore offers a promising solution to the low-resource\ndouble bind.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2021\n",
    "authors": [
      "Orevaoghene Ahia",
      "Julia Kreutzer",
      "Sara Hooker"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03036"
  },
  {
    "id": "arXiv:2110.03037",
    "title": "Reactive Locomotion Decision-Making and Robust Motion Planning for  Real-Time Perturbation Recovery",
    "abstract": "In this paper, we examine the problem of push recovery for bipedal robot\nlocomotion and present a reactive decision-making and robust planning framework\nfor locomotion resilient to external perturbations. Rejecting perturbations is\nan essential capability of bipedal robots and has been widely studied in the\nlocomotion literature. However, adversarial disturbances and aggressive turning\ncan lead to negative lateral step width (i.e., crossed-leg scenarios) with\nunstable motions and self-collision risks. These motion planning problems are\ncomputationally difficult and have not been explored under a hierarchically\nintegrated task and motion planning method. We explore a planning and\ndecision-making framework that closely ties linear-temporal-logic-based\nreactive synthesis with trajectory optimization incorporating the robot's\nfull-body dynamics, kinematics, and leg collision avoidance constraints.\nBetween the high-level discrete symbolic decision-making and the low-level\ncontinuous motion planning, behavior trees serve as a reactive interface to\nhandle perturbations occurring at any time of the locomotion process. Our\nexperimental results show the efficacy of our method in generating resilient\nrecovery behaviors in response to diverse perturbations from any direction with\nbounded magnitudes.",
    "descriptor": "",
    "authors": [
      "Zhaoyuan Gu",
      "Nathan Boyd",
      "Ye Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03037"
  },
  {
    "id": "arXiv:2110.03039",
    "title": "Optimized Recommender Systems with Deep Reinforcement Learning",
    "abstract": "Recommender Systems have been the cornerstone of online retailers.\nTraditionally they were based on rules, relevance scores, ranking algorithms,\nand supervised learning algorithms, but now it is feasible to use reinforcement\nlearning algorithms to generate meaningful recommendations. This work\ninvestigates and develops means to setup a reproducible testbed, and evaluate\ndifferent state of the art algorithms in a realistic environment. It entails a\nproposal, literature review, methodology, results, and comments.",
    "descriptor": "",
    "authors": [
      "Lucas Farris"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03039"
  },
  {
    "id": "arXiv:2110.03045",
    "title": "Iterate Averaging and Filtering Algorithms for Linear Inverse Problems",
    "abstract": "It has been proposed that classical filtering methods, like the Kalman filter\nand 3DVAR, can be used to solve linear statistical inverse problems. In the\nwork of Igelsias, Lin, Lu, & Stuart (2017), error estimates were obtained for\nthis approach. By optimally tuning a free parameter in the filters, the authors\nwere able to show that the mean squared error can be minimized. In the present\nwork, we prove that by (i) considering the problem in a weaker, weighted, space\nand (ii) applying simple iterate averaging of the filter output, 3DVAR will\nconverge in mean square, unconditionally on the parameter. Without iterate\naveraging, 3DVAR cannot converge by running additional iterations with a given,\nfixed, choice of parameter. We also establish that the Kalman filter's\nperformance cannot be improved through iterate averaging. We illustrate our\nresults with numerical experiments that suggest our convergence rates are\nsharp.",
    "descriptor": "\nComments: initial posting, 16 pages, 3 figures\n",
    "authors": [
      "Felix G. Jones",
      "Gideon Simpson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03045"
  },
  {
    "id": "arXiv:2110.03047",
    "title": "Integrating Categorical Features in End-to-End ASR",
    "abstract": "All-neural, end-to-end ASR systems gained rapid interest from the speech\nrecognition community. Such systems convert speech input to text units using a\nsingle trainable neural network model. E2E models require large amounts of\npaired speech text data that is expensive to obtain. The amount of data\navailable varies across different languages and dialects. It is critical to\nmake use of all these data so that both low resource languages and high\nresource languages can be improved. When we want to deploy an ASR system for a\nnew application domain, the amount of domain specific training data is very\nlimited. To be able to leverage data from existing domains is important for ASR\naccuracy in the new domain. In this paper, we treat all these aspects as\ncategorical information in an ASR system, and propose a simple yet effective\nway to integrate categorical features into E2E model. We perform detailed\nanalysis on various training strategies, and find that building a joint model\nthat includes categorical features can be more accurate than multiple\nindependently trained models.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Rongqing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03047"
  },
  {
    "id": "arXiv:2110.03049",
    "title": "Physics-informed neural network simulation of multiphase poroelasticity  using stress-split sequential training",
    "abstract": "Physics-informed neural networks (PINNs) have received significant attention\nas a unified framework for forward, inverse, and surrogate modeling of problems\ngoverned by partial differential equations (PDEs). Training PINNs for forward\nproblems, however, pose significant challenges, mainly because of the complex\nnon-convex and multi-objective loss function. In this work, we present a PINN\napproach to solving the equations of coupled flow and deformation in porous\nmedia for both single-phase and multiphase flow. To this end, we construct the\nsolution space using multi-layer neural networks. Due to the dynamics of the\nproblem, we find that incorporating multiple differential relations into the\nloss function results in an unstable optimization problem, meaning that\nsometimes it converges to the trivial null solution, other times it moves very\nfar from the expected solution. We report a dimensionless form of the coupled\ngoverning equations that we find most favourable to the optimizer.\nAdditionally, we propose a sequential training approach based on the\nstress-split algorithms of poromechanics. Notably, we find that sequential\ntraining based on stress-split performs well for different problems, while the\nclassical strain-split algorithm shows an unstable behaviour similar to what is\nreported in the context of finite element solvers. We use the approach to solve\nbenchmark problems of poroelasticity, including Mandel's consolidation problem,\nBarry-Mercer's injection-production problem, and a reference two-phase drainage\nproblem. The Python-SciANN codes reproducing the results reported in this\nmanuscript will be made publicly available at\nhttps://github.com/sciann/sciann-applications.",
    "descriptor": "",
    "authors": [
      "Ehsan Haghighat",
      "Danial Amini",
      "Ruben Juanes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2110.03049"
  },
  {
    "id": "arXiv:2110.03051",
    "title": "A Survey on Evidential Deep Learning For Single-Pass Uncertainty  Estimation",
    "abstract": "Popular approaches for quantifying predictive uncertainty in deep neural\nnetworks often involve a set of weights or models, for instance via ensembling\nor Monte Carlo Dropout. These techniques usually produce overhead by having to\ntrain multiple model instances or do not produce very diverse predictions. This\nsurvey aims to familiarize the reader with an alternative class of models based\non the concept of Evidential Deep Learning: For unfamiliar data, they admit\n\"what they don't know\" and fall back onto a prior belief. Furthermore, they\nallow uncertainty estimation in a single model and forward pass by\nparameterizing distributions over distributions. This survey recapitulates\nexisting works, focusing on the implementation in a classification setting.\nFinally, we survey the application of the same paradigm to regression problems.\nWe also provide a reflection on the strengths and weaknesses of the mentioned\napproaches compared to existing ones and provide the most central theoretical\nresults in order to inform future research.",
    "descriptor": "",
    "authors": [
      "Dennis Ulmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03051"
  },
  {
    "id": "arXiv:2110.03054",
    "title": "On The Vulnerability of Recurrent Neural Networks to Membership  Inference Attacks",
    "abstract": "We study the privacy implications of deploying recurrent neural networks in\nmachine learning. We consider membership inference attacks (MIAs) in which an\nattacker aims to infer whether a given data record has been used in the\ntraining of a learning agent. Using existing MIAs that target feed-forward\nneural networks, we empirically demonstrate that the attack accuracy wanes for\ndata records used earlier in the training history. Alternatively, recurrent\nnetworks are specifically designed to better remember their past experience;\nhence, they are likely to be more vulnerable to MIAs than their feed-forward\ncounterparts. We develop a pair of MIA layouts for two primary applications of\nrecurrent networks, namely, deep reinforcement learning and\nsequence-to-sequence tasks. We use the first attack to provide empirical\nevidence that recurrent networks are indeed more vulnerable to MIAs than\nfeed-forward networks with the same performance level. We use the second attack\nto showcase the differences between the effects of overtraining recurrent and\nfeed-forward networks on the accuracy of their respective MIAs. Finally, we\ndeploy a differential privacy mechanism to resolve the privacy vulnerability\nthat the MIAs exploit. For both attack layouts, the privacy mechanism degrades\nthe attack accuracy from above 80% to 50%, which is equal to guessing the data\nmembership uniformly at random, while trading off less than 10% utility.",
    "descriptor": "\nComments: Under Double-Blind Review\n",
    "authors": [
      "Yunhao Yang",
      "Parham Gohari",
      "Ufuk Topcu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03054"
  },
  {
    "id": "arXiv:2110.03060",
    "title": "Optimal Skeleton Network Reconfiguration considering Topological  Characteristics and Transmission Path",
    "abstract": "Power system restoration can be divided into three stages: black-start,\nnetwork reconfiguration, and load restoration. A skeleton-network should be\nrestored in the second stage to prepare for the subsequent large-scale\nsystem-wide load pickup. This paper proposes a novel integrated\nskeleton-network reconfiguration (ISNR) model which considers both the\ntopological characteristics of the network and the transmission path\nenergization constraints. A novel topological characteristics-based\nskeleton-network quality index (TCSNQI), an index based on the network\nimportance and distance, is proposed to evaluate the quality of the\nskeleton-network. The proposed ISNR model can attain both the target network\nand the associated restoration sequence for that network. The attained target\nnetwork reaches a certain quality level, and it requires the least restoration\ntime based on the attained sequence of ordered switching actions. The proposed\nISNR model is formulated as a mixed-integer linear programming (MILP) problem.\nNumerical simulations on the New England 39-bus test system demonstrate the\nperformance of the proposed ISNR model.",
    "descriptor": "",
    "authors": [
      "Jin Lu",
      "Xingpeng Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03060"
  },
  {
    "id": "arXiv:2110.03061",
    "title": "Automatic Tuning of Federated Learning Hyper-Parameters from System  Perspective",
    "abstract": "Federated learning (FL) is a distributed model training paradigm that\npreserves clients' data privacy. FL hyper-parameters significantly affect the\ntraining overheads in terms of time, computation, and communication. However,\nthe current practice of manually selecting FL hyper-parameters puts a high\nburden on FL practitioners since various applications prefer different training\npreferences. In this paper, we propose FedTuning, an automatic FL\nhyper-parameter tuning algorithm tailored to applications' diverse system\nrequirements of FL training. FedTuning is lightweight and flexible, achieving\nan average of 41% improvement for different training preferences on time,\ncomputation, and communication compared to fixed FL hyper-parameters. FedTuning\nis available at https://github.com/dtczhl/FedTuning.",
    "descriptor": "",
    "authors": [
      "Huanle Zhang",
      "Mi Zhang",
      "Xin Liu",
      "Prasant Mohapatra",
      "Michael DeLucia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03061"
  },
  {
    "id": "arXiv:2110.03067",
    "title": "On Neurons Invariant to Sentence Structural Changes in Neural Machine  Translation",
    "abstract": "To gain insight into the role neurons play, we study the activation patterns\ncorresponding to meaning-preserving paraphrases (e.g., active-passive). We\ncompile a dataset of controlled syntactic paraphrases in English with their\nreference German translations and demonstrate our model-agnostic approach with\nthe Transformer translation model. First, we identify neurons that correlate\nacross paraphrases and dissect the observed correlation into possible\nconfounds. Although lower-level components are found as the cause of similar\nactivations, no sentence-level semantics or syntax are detected locally. Later,\nwe manipulate neuron activations to influence translation towards a particular\nsyntactic form. We find that a simple value shift is effective, and more so\nwhen many neurons are modified. These suggest that complex syntactic\nconstructions are indeed encoded in the model. We conclude by discussing how to\nbetter manipulate it using the correlations we first obtained.",
    "descriptor": "",
    "authors": [
      "Gal Patel",
      "Leshem Choshen",
      "Omri Abend"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03067"
  },
  {
    "id": "arXiv:2110.03068",
    "title": "Learning the Optimal Recommendation from Explorative Users",
    "abstract": "We propose a new problem setting to study the sequential interactions between\na recommender system and a user. Instead of assuming the user is omniscient,\nstatic, and explicit, as the classical practice does, we sketch a more\nrealistic user behavior model, under which the user: 1) rejects recommendations\nif they are clearly worse than others; 2) updates her utility estimation based\non rewards from her accepted recommendations; 3) withholds realized rewards\nfrom the system. We formulate the interactions between the system and such an\nexplorative user in a $K$-armed bandit framework and study the problem of\nlearning the optimal recommendation on the system side. We show that efficient\nsystem learning is still possible but is more difficult. In particular, the\nsystem can identify the best arm with probability at least $1-\\delta$ within\n$O(1/\\delta)$ interactions, and we prove this is tight. Our finding contrasts\nthe result for the problem of best arm identification with fixed confidence, in\nwhich the best arm can be identified with probability $1-\\delta$ within\n$O(\\log(1/\\delta))$ interactions. This gap illustrates the inevitable cost the\nsystem has to pay when it learns from an explorative user's revealed\npreferences on its recommendations rather than from the realized rewards.",
    "descriptor": "",
    "authors": [
      "Fan Yao",
      "Chuanhao Li",
      "Denis Nekipelov",
      "Hongning Wang",
      "Haifeng Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.03068"
  },
  {
    "id": "arXiv:2110.03071",
    "title": "Two Many Cooks: Understanding Dynamic Human-Agent Team Communication and  Perception Using Overcooked 2",
    "abstract": "This paper describes a research study that aims to investigate changes in\neffective communication during human-AI collaboration with special attention to\nthe perception of competence among team members and varying levels of task load\nplaced on the team. We will also investigate differences between human-human\nteamwork and human-agent teamwork. Our project will measure differences in the\ncommunication quality, team perception and performance of a human actor playing\na Commercial Off - The Shelf game (COTS) with either a human teammate or a\nsimulated AI teammate under varying task load. We argue that the increased\ncognitive workload associated with increases task load will be negatively\nassociated with team performance and have a negative impact on communication\nquality. In addition, we argue that positive team perceptions will have a\npositive impact on the communication quality between a user and teammate in\nboth the human and AI teammate conditions. This project will offer more refined\ninsights on Human - AI relationship dynamics in collaborative tasks by\nconsidering communication quality, team perception, and performance under\nincreasing cognitive workload.",
    "descriptor": "\nComments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)\n",
    "authors": [
      "Andres Rosero",
      "Faustina Dinh",
      "Ewart J. de Visser",
      "Tyler Shaw",
      "Elizabeth Phillips"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.03071"
  },
  {
    "id": "arXiv:2110.03072",
    "title": "FOD-A: A Dataset for Foreign Object Debris in Airports",
    "abstract": "Foreign Object Debris (FOD) detection has attracted increased attention in\nthe area of machine learning and computer vision. However, a robust and\npublicly available image dataset for FOD has not been initialized. To this end,\nthis paper introduces an image dataset of FOD, named FOD in Airports (FOD-A).\nFOD-A object categories have been selected based on guidance from prior\ndocumentation and related research by the Federal Aviation Administration\n(FAA). In addition to the primary annotations of bounding boxes for object\ndetection, FOD-A provides labeled environmental conditions. As such, each\nannotation instance is further categorized into three light level categories\n(bright, dim, and dark) and two weather categories (dry and wet). Currently,\nFOD-A has released 31 object categories and over 30,000 annotation instances.\nThis paper presents the creation methodology, discusses the publicly available\ndataset extension process, and demonstrates the practicality of FOD-A with\nwidely used machine learning models for object detection.",
    "descriptor": "\nComments: This paper has been accepted for publication by 20th IEEE International Conference on Machine Learning and Applications. The copyright is with the IEEE\n",
    "authors": [
      "Travis Munyer",
      "Pei-Chi Huang",
      "Chenyu Huang",
      "Xin Zhong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03072"
  },
  {
    "id": "arXiv:2110.03073",
    "title": "DRAFT-What you always wanted to know but could not find about  block-based environments",
    "abstract": "Block-based environments are visual programming environments, which are\nbecoming more and more popular because of their ease of use. The ease of use\ncomes thanks to their intuitive graphical representation and structural\nmetaphors (jigsaw-like puzzles) to display valid combinations of language\nconstructs to the users. Part of the current popularity of block-based\nenvironments is thanks to Scratch. As a result they are often associated with\ntools for children or young learners. However, it is unclear how these types of\nprogramming environments are developed and used in general. So we conducted a\nsystematic literature review on block-based environments by studying 152 papers\npublished between 2014 and 2020, and a non-systematic tool review of 32\nblock-based environments. In particular, we provide a helpful inventory of\nblock-based editors for end-users on different topics and domains. Likewise, we\nfocused on identifying the main components of block-based environments, how\nthey are engineered, and how they are used. This survey should be equally\nhelpful for language engineering researchers and language engineers alike.",
    "descriptor": "",
    "authors": [
      "Mauricio Verano Merino",
      "Jurgen Vinju",
      "Mark van den Brand"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.03073"
  },
  {
    "id": "arXiv:2110.03080",
    "title": "Understanding and Improving Usability of Data Dashboards for Simplified  Privacy Control of Voice Assistant Data (Extended Version)",
    "abstract": "Today, intelligent voice assistant (VA) software like Amazon's Alexa,\nGoogle's Voice Assistant (GVA) and Apple's Siri have millions of users. These\nVAs often collect and analyze huge user data for improving their functionality.\nHowever, this collected data may contain sensitive information (e.g., personal\nvoice recordings) that users might not feel comfortable sharing with others and\nmight cause significant privacy concerns. To counter such concerns, service\nproviders like Google present their users with a personal data dashboard\n(called `My Activity Dashboard'), allowing them to manage all voice assistant\ncollected data. However, a real-world GVA-data driven understanding of user\nperceptions and preferences regarding this data (and data dashboards) remained\nrelatively unexplored in prior research.\nTo that end, in this work we focused on Google Voice Assistant (GVA) users\nand investigated the perceptions and preferences of GVA users regarding data\nand dashboard while grounding them in real GVA-collected user data.\nSpecifically, we conducted an 80-participant survey-based user study to collect\nboth generic perceptions regarding GVA usage as well as desired privacy\npreferences for a stratified sample of their GVA data. We show that most\nparticipants had superficial knowledge about the type of data collected by GVA.\nWorryingly, we found that participants felt uncomfortable sharing a non-trivial\n17.7% of GVA-collected data elements with Google. The current My Activity\ndashboard, although useful, did not help long-time GVA users effectively manage\ntheir data privacy. Our real-data-driven study found that showing users even\none sensitive data element can significantly improve the usability of data\ndashboards. To that end, we built a classifier that can detect sensitive data\nfor data dashboard recommendations with a 95% F1-score and shows 76%\nimprovement over baseline models.",
    "descriptor": "\nComments: This extended version of our USENIX Security '22 paper includes appendices for interested readers\n",
    "authors": [
      "Vandit Sharma",
      "Mainack Mondal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.03080"
  },
  {
    "id": "arXiv:2110.03081",
    "title": "Large-Scale Topological Radar Localization Using Learned Descriptors",
    "abstract": "In this work, we propose a method for large-scale topological localization\nbased on radar scan images using learned descriptors. We present a simple yet\nefficient deep network architecture to compute a rotationally invariant\ndiscriminative global descriptor from a radar scan image. The performance and\ngeneralization ability of the proposed method is experimentally evaluated on\ntwo large scale driving datasets: MulRan and Oxford Radar RobotCar.\nAdditionally, we present a comparative evaluation of radar-based and\nLiDAR-based localization using learned global descriptors. Our code and trained\nmodels are publicly available on the project website.",
    "descriptor": "",
    "authors": [
      "Jacek Komorowski",
      "Monika Wysoczanska",
      "Tomasz Trzcinski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03081"
  },
  {
    "id": "arXiv:2110.03083",
    "title": "Vision-based Excavator Activity Analysis and Safety Monitoring System",
    "abstract": "In this paper, we propose an excavator activity analysis and safety\nmonitoring system, leveraging recent advancements in deep learning and computer\nvision. Our proposed system detects the surrounding environment and the\nexcavators while estimating the poses and actions of the excavators. Compared\nto previous systems, our method achieves higher accuracy in object detection,\npose estimation, and action recognition tasks. In addition, we build an\nexcavator dataset using the Autonomous Excavator System (AES) on the waste\ndisposal recycle scene to demonstrate the effectiveness of our system. We also\nevaluate our method on a benchmark construction dataset. The experimental\nresults show that the proposed action recognition approach outperforms the\nstate-of-the-art approaches on top-1 accuracy by about 5.18%.",
    "descriptor": "",
    "authors": [
      "Sibo Zhang",
      "Liangjun Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03083"
  },
  {
    "id": "arXiv:2110.03085",
    "title": "A hybrid approach for dynamically training a torque prediction model for  devising a human-machine interface control strategy",
    "abstract": "Human-machine interfaces (HMI) play a pivotal role in the rehabilitation and\ndaily assistance of lower-limb amputees. The brain of such interfaces is a\ncontrol model that detects the user's intention using sensor input and\ngenerates corresponding output (control commands). With recent advances in\ntechnology, AI-based policies have gained attention as control models for HMIs.\nHowever, supervised learning techniques require affluent amounts of labeled\ntraining data from the user, which is challenging in the context of lower-limb\nrehabilitation. Moreover, a static pre-trained model does not take the temporal\nvariations in the motion of the amputee (e.g., due to speed, terrain) into\naccount. In this study, we aimed to address both of these issues by creating an\nincremental training approach for a torque prediction model using incomplete\nuser-specific training data and biologically inspired temporal patterns of\nhuman gait. To reach this goal, we created a hybrid of two distinct approaches,\na generic inter-individual and an adapting individual-specific model that\nexploits the inter-limb synergistic coupling during human gait to learn a\nfunction that predicts the torque at the ankle joint continuously based on the\nkinematic sequences of the hip, knee, and shank. An inter-individual generic\nbase model learns temporal patterns of gait from a set of able-bodied\nindividuals and predicts the gait patterns for a new individual, while the\nindividual-specific adaptation model learns and predicts the temporal patterns\nof gait specific to a particular individual. The iterative training using the\nhybrid model was validated on eight able-bodied and five transtibial amputee\nsubjects. It was found that, with the addition of estimators fitted to\nindividual-specific data, the accuracy significantly increased from the\nbaseline inter-individual model and plateaued within two to three iterations.",
    "descriptor": "\nComments: 13 pages, 9 figures\n",
    "authors": [
      "Sharmita Dey",
      "Takashi Yoshida",
      "Robert H. Foerster",
      "Michael Ernst",
      "Thomas Schmalz",
      "Rodrigo M.Carnier",
      "Arndt F. Schilling"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2110.03085"
  },
  {
    "id": "arXiv:2110.03088",
    "title": "Statistical Random Number Generator Attack against the  Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Protocol",
    "abstract": "This paper introduces and demonstrates four new statistical attacks against\nthe Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme. The attacks\nutilize compromised random number generators at Alice's/Bob's site(s). The case\nof partial correlations between Alice's/Bob's and Eve's probing noises is\nexplored, that is, Eve's knowledge of Alice's and Bob's noises is limited but\nnot zero. We explore the bilateral situation where Eve has partial knowledge of\nAlice's and Bob's random number generators. It is shown that in this situation\nEve can crack the secure key bit by taking the highest cross-correlation\nbetween her probing noises and the measured voltage noise in the wire. She can\nalso crack the secure key bit by taking the highest cross-correlation between\nher noise voltages and her evaluation of Alice's/Bob's noise voltages. We then\nexplore the unilateral situation in which Eve has partial knowledge of only\nAlice's random number generator thus only those noises (of Alice and Eve) are\ncorrelated. In this situation Eve can still crack the secure key bit, but for\nsufficiently low error probability, she needs to use the whole bit exchange\nperiod for the attack. The security of the KLJN key exchange scheme, similarly\nto other protocols, necessitates that the random number generator outputs are\ntruly random for Eve.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2005.10429, arXiv:2012.02848\n",
    "authors": [
      "Christiana Chamon",
      "Shahriar Ferdous",
      "Laszlo B. Kish"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03088"
  },
  {
    "id": "arXiv:2110.03090",
    "title": "Player Tracking and Identification in Ice Hockey",
    "abstract": "Tracking and identifying players is a fundamental step in computer\nvision-based ice hockey analytics. The data generated by tracking is used in\nmany other downstream tasks, such as game event detection and game strategy\nanalysis. Player tracking and identification is a challenging problem since the\nmotion of players in hockey is fast-paced and non-linear when compared to\npedestrians. There is also significant camera panning and zooming in hockey\nbroadcast video. Identifying players in ice hockey is challenging since the\nplayers of the same team look almost identical, with the jersey number the only\ndiscriminating factor between players. In this paper, an automated system to\ntrack and identify players in broadcast NHL hockey videos is introduced. The\nsystem is composed of three components (1) Player tracking, (2) Team\nidentification and (3) Player identification. Due to the absence of publicly\navailable datasets, the datasets used to train the three components are\nannotated manually. Player tracking is performed with the help of a state of\nthe art tracking algorithm obtaining a Multi-Object Tracking Accuracy (MOTA)\nscore of 94.5%. For team identification, the away-team jerseys are grouped into\na single class and home-team jerseys are grouped in classes according to their\njersey color. A convolutional neural network is then trained on the team\nidentification dataset. The team identification network gets an accuracy of 97%\non the test set. A novel player identification model is introduced that\nutilizes a temporal one-dimensional convolutional network to identify players\nfrom player bounding box sequences. The player identification model further\ntakes advantage of the available NHL game roster data to obtain a player\nidentification accuracy of 83%.",
    "descriptor": "",
    "authors": [
      "Kanav Vats",
      "Pascale Walters",
      "Mehrnaz Fani",
      "David A. Clausi",
      "John Zelek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03090"
  },
  {
    "id": "arXiv:2110.03091",
    "title": "Improving Fractal Pre-training",
    "abstract": "The deep neural networks used in modern computer vision systems require\nenormous image datasets to train them. These carefully-curated datasets\ntypically have a million or more images, across a thousand or more distinct\ncategories. The process of creating and curating such a dataset is a monumental\nundertaking, demanding extensive effort and labelling expense and necessitating\ncareful navigation of technical and social issues such as label accuracy,\ncopyright ownership, and content bias.\nWhat if we had a way to harness the power of large image datasets but with\nfew or none of the major issues and concerns currently faced? This paper\nextends the recent work of Kataoka et. al. (2020), proposing an improved\npre-training dataset based on dynamically-generated fractal images. Challenging\nissues with large-scale image datasets become points of elegance for fractal\npre-training: perfect label accuracy at zero cost; no need to store/transmit\nlarge image archives; no privacy/demographic bias/concerns of inappropriate\ncontent, as no humans are pictured; limitless supply and diversity of images;\nand the images are free/open-source. Perhaps surprisingly, avoiding these\ndifficulties imposes only a small penalty in performance. Leveraging a\nnewly-proposed pre-training task -- multi-instance prediction -- our\nexperiments demonstrate that fine-tuning a network pre-trained using fractals\nattains 92.7-98.1\\% of the accuracy of an ImageNet pre-trained network.",
    "descriptor": "\nComments: Accepted to WACV 2022. 15 pages, 16 figures\n",
    "authors": [
      "Connor Anderson",
      "Ryan Farrell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03091"
  },
  {
    "id": "arXiv:2110.03092",
    "title": "A Uniform Framework for Anomaly Detection in Deep Neural Networks",
    "abstract": "Deep neural networks (DNN) can achieve high performance when applied to\nIn-Distribution (ID) data which come from the same distribution as the training\nset. When presented with anomaly inputs not from the ID, the outputs of a DNN\nshould be regarded as meaningless. However, modern DNN often predict anomaly\ninputs as an ID class with high confidence, which is dangerous and misleading.\nIn this work, we consider three classes of anomaly inputs, (1) natural inputs\nfrom a different distribution than the DNN is trained for, known as\nOut-of-Distribution (OOD) samples, (2) crafted inputs generated from ID by\nattackers, often known as adversarial (AD) samples, and (3) noise (NS) samples\ngenerated from meaningless data. We propose a framework that aims to detect all\nthese anomalies for a pre-trained DNN. Unlike some of the existing works, our\nmethod does not require preprocessing of input data, nor is it dependent to any\nknown OOD set or adversarial attack algorithm. Through extensive experiments\nover a variety of DNN models for the detection of aforementioned anomalies, we\nshow that in most cases our method outperforms state-of-the-art anomaly\ndetection methods in identifying all three classes of anomalies.",
    "descriptor": "\nComments: 18 pages, 9 figures, 9 tables\n",
    "authors": [
      "Fangzhen Zhao",
      "Chenyi Zhang",
      "Naipeng Dong",
      "Zefeng You",
      "Zhenxin Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03092"
  },
  {
    "id": "arXiv:2110.03095",
    "title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space  Perspective",
    "abstract": "Deep neural networks (DNNs) often rely on easy-to-learn discriminatory\nfeatures, or cues, that are not necessarily essential to the problem at hand.\nFor example, ducks in an image may be recognized based on their typical\nbackground scenery, such as lakes or streams. This phenomenon, also known as\nshortcut learning, is emerging as a key limitation of the current generation of\nmachine learning models. In this work, we introduce a set of experiments to\ndeepen our understanding of shortcut learning and its implications. We design a\ntraining setup with several shortcut cues, named WCST-ML, where each cue is\nequally conducive to the visual recognition problem at hand. Even under equal\nopportunities, we observe that (1) certain cues are preferred to others, (2)\nsolutions biased to the easy-to-learn cues tend to converge to relatively flat\nminima on the loss surface, and (3) the solutions focusing on those preferred\ncues are far more abundant in the parameter space. We explain the abundance of\ncertain cues via their Kolmogorov (descriptional) complexity: solutions\ncorresponding to Kolmogorov-simple cues are abundant in the parameter space and\nare thus preferred by DNNs. Our studies are based on the synthetic dataset\nDSprites and the face dataset UTKFace. In our WCST-ML, we observe that the\ninborn bias of models leans toward simple cues, such as color and ethnicity.\nOur findings emphasize the importance of active human intervention to remove\nthe inborn model biases that may cause negative societal impacts.",
    "descriptor": "\nComments: First two authors have contributed equally\n",
    "authors": [
      "Luca Scimeca",
      "Seong Joon Oh",
      "Sanghyuk Chun",
      "Michael Poli",
      "Sangdoo Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03095"
  },
  {
    "id": "arXiv:2110.03097",
    "title": "SWAT Watershed Model Calibration using Deep Learning",
    "abstract": "Watershed models such as the Soil and Water Assessment Tool (SWAT) consist of\nhigh-dimensional physical and empirical parameters. These parameters need to be\naccurately calibrated for models to produce reliable predictions for\nstreamflow, evapotranspiration, snow water equivalent, and nutrient loading.\nExisting parameter estimation methods are time-consuming, inefficient, and\ncomputationally intensive, with reduced accuracy when estimating\nhigh-dimensional parameters. In this paper, we present a fast, accurate, and\nreliable methodology to calibrate the SWAT model (i.e., 21 parameters) using\ndeep learning (DL). We develop DL-enabled inverse models based on convolutional\nneural networks to ingest streamflow data and estimate the SWAT model\nparameters. Hyperparameter tuning is performed to identify the optimal neural\nnetwork architecture and the nine next best candidates. We use ensemble SWAT\nsimulations to train, validate, and test the above DL models. We estimated the\nactual parameters of the SWAT model using observational data. We test and\nvalidate the proposed DL methodology on the American River Watershed, located\nin the Pacific Northwest-based Yakima River basin. Our results show that the DL\nmodels-based calibration is better than traditional parameter estimation\nmethods, such as generalized likelihood uncertainty estimation (GLUE). The\nbehavioral parameter sets estimated by DL have narrower ranges than GLUE and\nproduce values within the sampling range even under high relative observational\nerrors. This narrow range of parameters shows the reliability of the proposed\nworkflow to estimate sensitive parameters accurately even under noise. Due to\nits fast and reasonably accurate estimations of process parameters, the\nproposed DL workflow is attractive for calibrating integrated hydrologic models\nfor large spatial-scale applications.",
    "descriptor": "\nComments: 23\n",
    "authors": [
      "M. K. Mudunuru",
      "K. Son",
      "P. Jiang",
      "X. Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Applied Physics (physics.app-ph)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2110.03097"
  },
  {
    "id": "arXiv:2110.03101",
    "title": "SPEED+: Next Generation Dataset for Spacecraft Pose Estimation across  Domain Gap",
    "abstract": "Autonomous vision-based spaceborne navigation is an enabling technology for\nfuture on-orbit servicing and space logistics missions. While computer vision\nin general has benefited from Machine Learning (ML), training and validating\nspaceborne ML models are extremely challenging due to the impracticality of\nacquiring a large-scale labeled dataset of images of the intended target in the\nspace environment. Existing datasets, such as Spacecraft PosE Estimation\nDataset (SPEED), have so far mostly relied on synthetic images for both\ntraining and validation, which are easy to mass-produce but fail to resemble\nthe visual features and illumination variability inherent to the target\nspaceborne images. In order to bridge the gap between the current practices and\nthe intended applications in future space missions, this paper introduces\nSPEED+: the next generation spacecraft pose estimation dataset with specific\nemphasis on domain gap. In addition to 60,000 synthetic images for training,\nSPEED+ includes 9,531 simulated images of a spacecraft mockup model captured\nfrom the Testbed for Rendezvous and Optical Navigation (TRON) facility. TRON is\na first-of-a-kind robotic testbed capable of capturing an arbitrary number of\ntarget images with accurate and maximally diverse pose labels and high-fidelity\nspaceborne illumination conditions. SPEED+ will be used in the upcoming\ninternational Satellite Pose Estimation Challenge co-hosted with the Advanced\nConcepts Team of the European Space Agency to evaluate and compare the\nrobustness of spaceborne ML models trained on synthetic images.",
    "descriptor": "",
    "authors": [
      "Tae Ha Park",
      "Marcus M\u00e4rtens",
      "Gurvan Lecuyer",
      "Dario Izzo",
      "Simone D'Amico"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03101"
  },
  {
    "id": "arXiv:2110.03104",
    "title": "Hybrid Pointer Networks for Traveling Salesman Problems Optimization",
    "abstract": "In this work, a novel idea is presented for combinatorial optimization\nproblems, a hybrid network, which results in a superior outcome. We applied\nthis method to graph pointer networks [1], expanding its capabilities to a\nhigher level. We proposed a hybrid pointer network (HPN) to solve the\ntravelling salesman problem trained by reinforcement learning. Furthermore, HPN\nbuilds upon graph pointer networks which is an extension of pointer networks\nwith an additional graph embedding layer. HPN outperforms the graph pointer\nnetwork in solution quality due to the hybrid encoder, which provides our model\nwith a verity encoding type, allowing our model to converge to a better policy.\nOur network significantly outperforms the original graph pointer network for\nsmall and large-scale problems increasing its performance for TSP50 from 5.959\nto 5.706 without utilizing 2opt, Pointer networks, Attention model, and a wide\nrange of models, producing results comparable to highly tuned and specialized\nalgorithms. We make our data, models, and code publicly available [2].",
    "descriptor": "",
    "authors": [
      "Ahmed Stohy",
      "Heba-Tullah Abdelhakam",
      "Sayed Ali",
      "Mohammed Elhenawy",
      "Abdallah A Hassan",
      "Mahmoud Masoud",
      "Sebastien Glaser",
      "Andry Rakotonirainy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03104"
  },
  {
    "id": "arXiv:2110.03105",
    "title": "Learning a Metacognition for Object Detection",
    "abstract": "In contrast to object recognition models, humans do not blindly trust their\nperception when building representations of the world, instead recruiting\nmetacognition to detect percepts that are unreliable or false, such as when we\nrealize that we mistook one object for another. We propose METAGEN, an\nunsupervised model that enhances object recognition models through a\nmetacognition. Given noisy output from an object-detection model, METAGEN\nlearns a meta-representation of how its perceptual system works and uses it to\ninfer the objects in the world responsible for the detections. METAGEN achieves\nthis by conditioning its inference on basic principles of objects that even\nhuman infants understand (known as Spelke principles: object permanence,\ncohesion, and spatiotemporal continuity). We test METAGEN on a variety of\nstate-of-the-art object detection neural networks. We find that METAGEN quickly\nlearns an accurate metacognitive representation of the neural network, and that\nthis improves detection accuracy by filling in objects that the detection model\nmissed and removing hallucinated objects. This approach enables generalization\nto out-of-sample data and outperforms comparison models that lack a\nmetacognition.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Marlene Berke",
      "Mario Belledonne",
      "Zhangir Azerbayez",
      "Julian Jara-Ettinger"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03105"
  },
  {
    "id": "arXiv:2110.03106",
    "title": "Multi-Trigger-Key: Towards Multi-Task Privacy Preserving In Deep  Learning",
    "abstract": "Deep learning-based Multi-Task Classification (MTC) is widely used in\napplications like facial attributes and healthcare that warrant strong privacy\nguarantees. In this work, we aim to protect sensitive information in the\ninference phase of MTC and propose a novel Multi-Trigger-Key (MTK) framework to\nachieve the privacy-preserving objective. MTK associates each secured task in\nthe multi-task dataset with a specifically designed trigger-key. The true\ninformation can be revealed by adding the trigger-key if the user is\nauthorized. We obtain such an MTK model by training it with a newly generated\ntraining set. To address the information leakage malaise resulting from\ncorrelations among different tasks, we generalize the training process by\nincorporating an MTK decoupling process with a controllable trade-off between\nthe protective efficacy and the model performance. Theoretical guarantees and\nexperimental results demonstrate the effectiveness of the privacy protection\nwithout appreciable hindering on the model performance.",
    "descriptor": "",
    "authors": [
      "Ren Wang",
      "Zhe Xu",
      "Alfred Hero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03106"
  },
  {
    "id": "arXiv:2110.03109",
    "title": "Consistent Counterfactuals for Deep Models",
    "abstract": "Counterfactual examples are one of the most commonly-cited methods for\nexplaining the predictions of machine learning models in key areas such as\nfinance and medical diagnosis. Counterfactuals are often discussed under the\nassumption that the model on which they will be used is static, but in\ndeployment models may be periodically retrained or fine-tuned. This paper\nstudies the consistency of model prediction on counterfactual examples in deep\nnetworks under small changes to initial training conditions, such as weight\ninitialization and leave-one-out variations in data, as often occurs during\nmodel deployment. We demonstrate experimentally that counterfactual examples\nfor deep models are often inconsistent across such small changes, and that\nincreasing the cost of the counterfactual, a stability-enhancing mitigation\nsuggested by prior work in the context of simpler models, is not a reliable\nheuristic in deep networks. Rather, our analysis shows that a model's local\nLipschitz continuity around the counterfactual is key to its consistency across\nrelated models. To this end, we propose Stable Neighbor Search as a way to\ngenerate more consistent counterfactual explanations, and illustrate the\neffectiveness of this approach on several benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Emily Black",
      "Zifan Wang",
      "Matt Fredrikson",
      "Anupam Datta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03109"
  },
  {
    "id": "arXiv:2110.03111",
    "title": "Cut the CARP: Fishing for zero-shot story evaluation",
    "abstract": "Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\nInformed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Shahbuland Matiana",
      "JR Smith",
      "Ryan Teehan",
      "Louis Castricato",
      "Stella Biderman",
      "Leo Gao",
      "Spencer Frazier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03111"
  },
  {
    "id": "arXiv:2110.03119",
    "title": "Adaptive Safety Margin Estimation for Safe Real-Time Replanning under  Time-Varying Disturbance",
    "abstract": "Safe navigation in real-time is challenging because engineers need to work\nwith uncertain vehicle dynamics, variable external disturbances, and imperfect\ncontrollers. A common safety strategy is to inflate obstacles by hand-defined\nmargins. However, arbitrary static margins often fail in more dynamic\nscenarios, and using worst-case assumptions is overly conservative for most\nsettings where disturbances over time. In this work, we propose a middle\nground: safety margins that adapt on-the-fly. In an offline phase, we use Monte\nCarlo simulations to pre-compute a library of safety margins for multiple\nlevels of disturbance uncertainties. Then, at runtime, our system estimates the\ncurrent disturbance level to query the associated safety margins that best\ntrades off safety and performance. We validate our approach with extensive\nsimulated and real-world flight tests. We show that our adaptive method\nsignificantly outperforms static margins, allowing the vehicle to operate up to\n1.5 times faster than worst-case static margins while maintaining safety.\nVideo: https://youtu.be/SHzKHSUjdUU",
    "descriptor": "",
    "authors": [
      "Cherie Ho",
      "Jay Patrikar",
      "Rogerio Bonatti",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03119"
  },
  {
    "id": "arXiv:2110.03120",
    "title": "Assurance Monitoring of Learning Enabled Cyber-Physical Systems Using  Inductive Conformal Prediction based on Distance Learning",
    "abstract": "Machine learning components such as deep neural networks are used extensively\nin Cyber-Physical Systems (CPS). However, such components may introduce new\ntypes of hazards that can have disastrous consequences and need to be addressed\nfor engineering trustworthy systems. Although deep neural networks offer\nadvanced capabilities, they must be complemented by engineering methods and\npractices that allow effective integration in CPS. In this paper, we proposed\nan approach for assurance monitoring of learning-enabled CPS based on the\nconformal prediction framework. In order to allow real-time assurance\nmonitoring, the approach employs distance learning to transform\nhigh-dimensional inputs into lower size embedding representations. By\nleveraging conformal prediction, the approach provides well-calibrated\nconfidence and ensures a bounded small error rate while limiting the number of\ninputs for which an accurate prediction cannot be made. We demonstrate the\napproach using three data sets of mobile robot following a wall, speaker\nrecognition, and traffic sign recognition. The experimental results demonstrate\nthat the error rates are well-calibrated while the number of alarms is very\nsmall. Further, the method is computationally efficient and allows real-time\nassurance monitoring of CPS.",
    "descriptor": "\nComments: Published in Artificial Intelligence for Engineering Design, Analysis and Manufacturing. arXiv admin note: text overlap with arXiv:2001.05014\n",
    "authors": [
      "Dimitrios Boursinos",
      "Xenofon Koutsoukos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03120"
  },
  {
    "id": "arXiv:2110.03122",
    "title": "Faster algorithm for Unique $(k,2)$-CSP",
    "abstract": "In a $(k,2)$-Constraint Satisfaction Problem we are given a set of arbitrary\nconstraints on pairs of $k$-ary variables, and are asked to find an assignment\nof values to these variables such that all constraints are satisfied. The\n$(k,2)$-CSP problem generalizes problems like $k$-coloring and\n$k$-list-coloring. In the Unique $(k,2)$-CSP problem, we add the assumption\nthat the input set of constraints has at most one satisfying assignment.\nBeigel and Eppstein gave an algorithm for $(k,2)$-CSP running in time\n$O\\left(\\left(0.4518k\\right)^n\\right)$ for $k>3$ and $O\\left(1.356^n\\right)$\nfor $k=3$, where $n$ is the number of variables. Feder and Motwani improved\nupon the Beigel-Eppstein algorithm for $k\\geq 11$. Hertli, Hurbain, Millius,\nMoser, Scheder and Szedl{\\'a}k improved these bounds for Unique $(k,2)$-CSP for\nevery $k\\geq 5$.\nWe improve the result of Hertli et al. and obtain better bounds for\nUnique~$(k,2)$-CSP for~$k\\geq 5$. In particular, we improve the running time of\nUnique~$(5,2)$-CSP from~$O\\left(2.254^n\\right)$ to~$O\\left(2.232^n\\right)$ and\nUnique~$(6,2)$-CSP from~$O\\left(2.652^n\\right)$ to~$O\\left(2.641^n\\right)$.",
    "descriptor": "",
    "authors": [
      "Or Zamir"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03122"
  },
  {
    "id": "arXiv:2110.03123",
    "title": "Improving Prediction Confidence in Learning-Enabled Autonomous Systems",
    "abstract": "Autonomous systems use extensively learning-enabled components such as deep\nneural networks (DNNs) for prediction and decision making. In this paper, we\nutilize a feedback loop between learning-enabled components used for\nclassification and the sensors of an autonomous system in order to improve the\nconfidence of the predictions. We design a classifier using Inductive Conformal\nPrediction (ICP) based on a triplet network architecture in order to learn\nrepresentations that can be used to quantify the similarity between test and\ntraining examples. The method allows computing confident set predictions with\nan error rate predefined using a selected significance level. A feedback loop\nthat queries the sensors for a new input is used to further refine the\npredictions and increase the classification accuracy. The method is\ncomputationally efficient, scalable to high-dimensional inputs, and can be\nexecuted in a feedback loop with the system in real-time. The approach is\nevaluated using a traffic sign recognition dataset and the results show that\nthe error rate is reduced.",
    "descriptor": "\nComments: Published in Dynamic Data Driven Applications Systems. DDDAS 2020\n",
    "authors": [
      "Dimitrios Boursinos",
      "Xenofon Koutsoukos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03123"
  },
  {
    "id": "arXiv:2110.03124",
    "title": "Improving Adversarial Robustness for Free with Snapshot Ensemble",
    "abstract": "Adversarial training, as one of the few certified defenses against\nadversarial attacks, can be quite complicated and time-consuming, while the\nresults might not be robust enough. To address the issue of lack of robustness,\nensemble methods were proposed, aiming to get the final output by weighting the\nselected results from repeatedly trained processes. It is proved to be very\nuseful in achieving robust and accurate results, but the computational and\nmemory costs are even higher. Snapshot ensemble, a new ensemble method that\ncombines several local minima in a single training process to make the final\nprediction, was proposed recently, which reduces the time spent on training\nmultiple networks and the memory to store the results. Based on the snapshot\nensemble, we present a new method that is easier to implement: unlike original\nsnapshot ensemble that seeks for local minima, our snapshot ensemble focuses on\nthe last few iterations of a training and stores the sets of parameters from\nthem. Our algorithm is much simpler but the results are no less accurate than\nthe original ones: based on different hyperparameters and datasets, our\nsnapshot ensemble has shown a 5% to 30% increase in accuracy when compared to\nthe traditional adversarial training.",
    "descriptor": "",
    "authors": [
      "Yihao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03124"
  },
  {
    "id": "arXiv:2110.03127",
    "title": "Reliable Probability Intervals For Classification Using Inductive Venn  Predictors Based on Distance Learning",
    "abstract": "Deep neural networks are frequently used by autonomous systems for their\nability to learn complex, non-linear data patterns and make accurate\npredictions in dynamic environments. However, their use as black boxes\nintroduces risks as the confidence in each prediction is unknown. Different\nframeworks have been proposed to compute accurate confidence measures along\nwith the predictions but at the same time introduce a number of limitations\nlike execution time overhead or inability to be used with high-dimensional\ndata. In this paper, we use the Inductive Venn Predictors framework for\ncomputing probability intervals regarding the correctness of each prediction in\nreal-time. We propose taxonomies based on distance metric learning to compute\ninformative probability intervals in applications involving high-dimensional\ninputs. Empirical evaluation on image classification and botnet attacks\ndetection in Internet-of-Things (IoT) applications demonstrates improved\naccuracy and calibration. The proposed method is computationally efficient, and\ntherefore, can be used in real-time.",
    "descriptor": "\nComments: Published in IEEE International Conference on Omni-Layer Intelligent Systems (COINS), 2021\n",
    "authors": [
      "Dimitrios Boursinos",
      "Xenofon Koutsoukos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03127"
  },
  {
    "id": "arXiv:2110.03128",
    "title": "On the Generalization of Models Trained with SGD: Information-Theoretic  Bounds and Implications",
    "abstract": "This paper follows up on a recent work of (Neu, 2021) and presents new and\ntighter information-theoretic upper bounds for the generalization error of\nmachine learning models, such as neural networks, trained with SGD. We apply\nthese bounds to analyzing the generalization behaviour of linear and two-layer\nReLU networks. Experimental study based on these bounds provide some insights\non the SGD training of neural networks. They also point to a new and simple\nregularization scheme which we show performs comparably to the current state of\nthe art.",
    "descriptor": "",
    "authors": [
      "Ziqiao Wang",
      "Yongyi Mao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03128"
  },
  {
    "id": "arXiv:2110.03134",
    "title": "Improving Robot-Centric Learning from Demonstration via Personalized  Embeddings",
    "abstract": "Learning from demonstration (LfD) techniques seek to enable novice users to\nteach robots novel tasks in the real world. However, prior work has shown that\nrobot-centric LfD approaches, such as Dataset Aggregation (DAgger), do not\nperform well with human teachers. DAgger requires a human demonstrator to\nprovide corrective feedback to the learner either in real-time, which can\nresult in degraded performance due to suboptimal human labels, or in a post hoc\nmanner which is time intensive and often not feasible. To address this problem,\nwe present Mutual Information-driven Meta-learning from Demonstration (MIND\nMELD), which meta-learns a mapping from poor quality human labels to predicted\nground truth labels, thereby improving upon the performance of prior LfD\napproaches for DAgger-based training. The key to our approach for improving\nupon suboptimal feedback is mutual information maximization via variational\ninference. Our approach learns a meaningful, personalized embedding via\nvariational inference which informs the mapping from human provided labels to\npredicted ground truth labels. We demonstrate our framework in a synthetic\ndomain and in a human-subjects experiment, illustrating that our approach\nimproves upon the corrective labels provided by a human demonstrator by 63%.",
    "descriptor": "\nComments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)\n",
    "authors": [
      "Mariah L. Schrum",
      "Erin Hedlund",
      "Matthew C. Gombolay"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03134"
  },
  {
    "id": "arXiv:2110.03135",
    "title": "Double Descent in Adversarial Training: An Implicit Label Noise  Perspective",
    "abstract": "Here, we show that the robust overfitting shall be viewed as the early part\nof an epoch-wise double descent -- the robust test error will start to decrease\nagain after training the model for a considerable number of epochs. Inspired by\nour observations, we further advance the analyses of double descent to\nunderstand robust overfitting better. In standard training, double descent has\nbeen shown to be a result of label flipping noise. However, this reasoning is\nnot applicable in our setting, since adversarial perturbations are believed not\nto change the label. Going beyond label flipping noise, we propose to measure\nthe mismatch between the assigned and (unknown) true label distributions,\ndenoted as \\emph{implicit label noise}. We show that the traditional labeling\nof adversarial examples inherited from their clean counterparts will lead to\nimplicit label noise. Towards better labeling, we show that predicted\ndistribution from a classifier, after scaling and interpolation, can provably\nreduce the implicit label noise under mild assumptions. In light of our\nanalyses, we tailored the training objective accordingly to effectively\nmitigate the double descent and verified its effectiveness on three benchmark\ndatasets.",
    "descriptor": "",
    "authors": [
      "Chengyu Dong",
      "Liyuan Liu",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03135"
  },
  {
    "id": "arXiv:2110.03141",
    "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural  Networks",
    "abstract": "Overparametrized Deep Neural Networks (DNNs) often achieve astounding\nperformances, but may potentially result in severe generalization error.\nRecently, the relation between the sharpness of the loss landscape and the\ngeneralization error has been established by Foret et al. (2020), in which the\nSharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the\ngeneralization. Unfortunately, SAM s computational cost is roughly double that\nof base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus\nproposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s\nefficiency at no cost to its generalization performance. ESAM includes two\nnovel and efficient training strategies-StochasticWeight Perturbation and\nSharpness-Sensitive Data Selection. In the former, the sharpness measure is\napproximated by perturbing a stochastically chosen set of weights in each\niteration; in the latter, the SAM loss is optimized using only a judiciously\nselected subset of data that is sensitive to the sharpness. We provide\ntheoretical explanations as to why these strategies perform well. We also show,\nvia extensive experiments on the CIFAR and ImageNet datasets, that ESAM\nenhances the efficiency over SAM from requiring 100% extra computations to 40%\nvis-a-vis base optimizers, while test accuracies are preserved or even\nimproved.",
    "descriptor": "",
    "authors": [
      "Jiawei Du",
      "Hanshu Yan",
      "Jiashi Feng",
      "Joey Tianyi Zhou",
      "Liangli Zhen",
      "Rick Siow Mong Goh",
      "Vincent Y. F. Tan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03141"
  },
  {
    "id": "arXiv:2110.03142",
    "title": "A Comparative Study of Transformer-Based Language Models on Extractive  Question Answering",
    "abstract": "Question Answering (QA) is a task in natural language processing that has\nseen considerable growth after the advent of transformers. There has been a\nsurge in QA datasets that have been proposed to challenge natural language\nprocessing models to improve human and existing model performance. Many\npre-trained language models have proven to be incredibly effective at the task\nof extractive question answering. However, generalizability remains as a\nchallenge for the majority of these models. That is, some datasets require\nmodels to reason more than others. In this paper, we train various pre-trained\nlanguage models and fine-tune them on multiple question answering datasets of\nvarying levels of difficulty to determine which of the models are capable of\ngeneralizing the most comprehensively across different datasets. Further, we\npropose a new architecture, BERT-BiLSTM, and compare it with other language\nmodels to determine if adding more bidirectionality can improve model\nperformance. Using the F1-score as our metric, we find that the RoBERTa and\nBART pre-trained models perform the best across all datasets and that our\nBERT-BiLSTM model outperforms the baseline BERT model.",
    "descriptor": "",
    "authors": [
      "Kate Pearce",
      "Tiffany Zhan",
      "Aneesh Komanduri",
      "Justin Zhan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03142"
  },
  {
    "id": "arXiv:2110.03143",
    "title": "Meta-UDA: Unsupervised Domain Adaptive Thermal Object Detection using  Meta-Learning",
    "abstract": "Object detectors trained on large-scale RGB datasets are being extensively\nemployed in real-world applications. However, these RGB-trained models suffer a\nperformance drop under adverse illumination and lighting conditions. Infrared\n(IR) cameras are robust under such conditions and can be helpful in real-world\napplications. Though thermal cameras are widely used for military applications\nand increasingly for commercial applications, there is a lack of robust\nalgorithms to robustly exploit the thermal imagery due to the limited\navailability of labeled thermal data. In this work, we aim to enhance the\nobject detection performance in the thermal domain by leveraging the labeled\nvisible domain data in an Unsupervised Domain Adaptation (UDA) setting. We\npropose an algorithm agnostic meta-learning framework to improve existing UDA\nmethods instead of proposing a new UDA strategy. We achieve this by\nmeta-learning the initial condition of the detector, which facilitates the\nadaptation process with fine updates without overfitting or getting stuck at\nlocal optima. However, meta-learning the initial condition for the detection\nscenario is computationally heavy due to long and intractable computation\ngraphs. Therefore, we propose an online meta-learning paradigm which performs\nonline updates resulting in a short and tractable computation graph. To this\nend, we demonstrate the superiority of our method over many baselines in the\nUDA setting, producing a state-of-the-art thermal detector for the KAIST and\nDSIAC datasets.",
    "descriptor": "\nComments: Accepted to WACV 2022\n",
    "authors": [
      "Vibashan VS",
      "Domenick Poster",
      "Suya You",
      "Shuowen Hu",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03143"
  },
  {
    "id": "arXiv:2110.03144",
    "title": "Conceptual Expansion Neural Architecture Search (CENAS)",
    "abstract": "Architecture search optimizes the structure of a neural network for some task\ninstead of relying on manual authoring. However, it is slow, as each potential\narchitecture is typically trained from scratch. In this paper we present an\napproach called Conceptual Expansion Neural Architecture Search (CENAS) that\ncombines a sample-efficient, computational creativity-inspired transfer\nlearning approach with neural architecture search. This approach finds models\nfaster than naive architecture search via transferring existing weights to\napproximate the parameters of the new model. It outperforms standard transfer\nlearning by allowing for the addition of features instead of only modifying\nexisting features. We demonstrate that our approach outperforms standard neural\narchitecture search and transfer learning methods in terms of efficiency,\nperformance, and parameter counts on a variety of transfer learning tasks.",
    "descriptor": "\nComments: 9 pages, 4 figures, ICCC 2021 Poster\n",
    "authors": [
      "Mohan Singamsetti",
      "Anmol Mahajan",
      "Matthew Guzdial"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03144"
  },
  {
    "id": "arXiv:2110.03147",
    "title": "PRRS Outbreak Prediction via Deep Switching Auto-Regressive  Factorization Modeling",
    "abstract": "We propose an epidemic analysis framework for the outbreak prediction in the\nlivestock industry, focusing on the study of the most costly and viral\ninfectious disease in the swine industry -- the PRRS virus. Using this\nframework, we can predict the PRRS outbreak in all farms of a swine production\nsystem by capturing the spatio-temporal dynamics of infection transmission\nbased on the intra-farm pig-level virus transmission dynamics, and inter-farm\npig shipment network. We simulate a PRRS infection epidemic based on the\nshipment network and the SEIR epidemic model using the statistics extracted\nfrom real data provided by the swine industry. We develop a hierarchical\nfactorized deep generative model that approximates high dimensional data by a\nproduct between time-dependent weights and spatially dependent low dimensional\nfactors to perform per farm time series prediction. The prediction results\ndemonstrate the ability of the model in forecasting the virus spread\nprogression with average error of NRMSE = 2.5\\%.",
    "descriptor": "",
    "authors": [
      "Mohammadsadegh Shamsabardeh",
      "Bahar Azari",
      "Beatriz Mart\u00ednez-L\u00f3pez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03147"
  },
  {
    "id": "arXiv:2110.03149",
    "title": "Data-driven behavioural biometrics for continuous and adaptive user  verification using Smartphone and Smartwatch",
    "abstract": "Recent studies have shown how motion-based biometrics can be used as a form\nof user authentication and identification without requiring any human\ncooperation. This category of behavioural biometrics deals with the features we\nlearn in our life as a result of our interaction with the environment and\nnature. This modality is related to change in human behaviour over time. The\ndevelopments in these methods aim to amplify continuous authentication such as\nbiometrics to protect their privacy on user devices. Various Continuous\nAuthentication (CA) systems have been proposed in the literature. They\nrepresent a new generation of security mechanisms that continuously monitor\nuser behaviour and use this as the basis to re-authenticate them periodically\nthroughout a login session. However, these methods usually constitute a single\nclassification model which is used to identify or verify a user. This work\nproposes an algorithm to blend behavioural biometrics with multi-factor\nauthentication (MFA) by introducing a two-step user verification algorithm that\nverifies the user's identity using motion-based biometrics and complements the\nmulti-factor authentication, thus making it more secure and flexible. This\ntwo-step user verification algorithm is also immune to adversarial attacks,\nbased on our experimental results which show how the rate of misclassification\ndrops while using this model with adversarial data.",
    "descriptor": "\nComments: 11 pages, 7 figures, 2 tables\n",
    "authors": [
      "Akriti Verma",
      "Valeh Moghaddam",
      "Adnan Anwar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03149"
  },
  {
    "id": "arXiv:2110.03152",
    "title": "Optimal (Euclidean) Metric Compression",
    "abstract": "We study the problem of representing all distances between $n$ points in\n$\\mathbb R^d$, with arbitrarily small distortion, using as few bits as\npossible. We give asymptotically tight bounds for this problem, for Euclidean\nmetrics, for $\\ell_1$ (a.k.a.~Manhattan) metrics, and for general metrics.\nOur bounds for Euclidean metrics mark the first improvement over compression\nschemes based on discretizing the classical dimensionality reduction theorem of\nJohnson and Lindenstrauss (Contemp.~Math.~1984). Since it is known that no\nbetter dimension reduction is possible, our results establish that Euclidean\nmetric compression is possible beyond dimension reduction.",
    "descriptor": "",
    "authors": [
      "Piotr Indyk",
      "Tal Wagner"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03152"
  },
  {
    "id": "arXiv:2110.03154",
    "title": "DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle  Avoidance in Autonomous Systems",
    "abstract": "Depth estimation-based obstacle avoidance has been widely adopted by\nautonomous systems (drones and vehicles) for safety purpose. It normally relies\non a stereo camera to automatically detect obstacles and make flying/driving\ndecisions, e.g., stopping several meters ahead of the obstacle in the path or\nmoving away from the detected obstacle. In this paper, we explore new security\nrisks associated with the stereo vision-based depth estimation algorithms used\nfor obstacle avoidance. By exploiting the weaknesses of the stereo matching in\ndepth estimation algorithms and the lens flare effect in optical imaging, we\npropose DoubleStar, a long-range attack that injects fake obstacle depth by\nprojecting pure light from two complementary light sources.\nDoubleStar includes two distinctive attack formats: beams attack and orbs\nattack, which leverage projected light beams and lens flare orbs respectively\nto cause false depth perception. We successfully attack two commercial stereo\ncameras designed for autonomous systems (ZED and Intel RealSense). The\nvisualization of fake depth perceived by the stereo cameras illustrates the\nfalse stereo matching induced by DoubleStar. We further use Ardupilot to\nsimulate the attack and demonstrate its impact on drones. To validate the\nattack on real systems, we perform a real-world attack towards a commercial\ndrone equipped with state-of-the-art obstacle avoidance algorithms. Our attack\ncan continuously bring a flying drone to a sudden stop or drift it away across\na long distance under various lighting conditions, even bypassing sensor fusion\nmechanisms. Specifically, our experimental results show that DoubleStar creates\nfake depth up to 15 meters in distance at night and up to 8 meters during the\ndaytime. To mitigate this newly discovered threat, we provide discussions on\npotential countermeasures to defend against DoubleStar.",
    "descriptor": "",
    "authors": [
      "Ce Zhou",
      "Qiben Yan",
      "Yan Shi",
      "Lichao Sun"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03154"
  },
  {
    "id": "arXiv:2110.03155",
    "title": "Towards Understanding Distributional Reinforcement Learning:  Regularization, Optimization, Acceleration and Sinkhorn Algorithm",
    "abstract": "Distributional reinforcement learning~(RL) is a class of state-of-the-art\nalgorithms that estimate the whole distribution of the total return rather than\nonly its expectation. Despite the remarkable performance of distributional RL,\na theoretical understanding of its advantages over expectation-based RL remains\nelusive. In this paper, we interpret distributional RL as entropy-regularized\nmaximum likelihood estimation in the \\textit{neural Z-fitted iteration}\nframework, and establish the connection of the resulting risk-aware\nregularization with maximum entropy RL. In addition, We shed light on the\nstability-promoting distributional loss with desirable smoothness properties in\ndistributional RL, which can yield stable optimization and guaranteed\ngeneralization. We also analyze the acceleration behavior while optimizing\ndistributional RL algorithms and show that an appropriate approximation to the\ntrue target distribution can speed up the convergence. From the perspective of\nrepresentation, we find that distributional RL encourages state representation\nfrom the same action class classified by the policy in tighter clusters.\nFinally, we propose a class of \\textit{Sinkhorn distributional RL} algorithm\nthat interpolates between the Wasserstein distance and maximum mean\ndiscrepancy~(MMD). Experiments on a suite of Atari games reveal the competitive\nperformance of our algorithm relative to existing state-of-the-art\ndistributional RL algorithms.",
    "descriptor": "",
    "authors": [
      "Ke Sun",
      "Yingnan Zhao",
      "Yi Liu",
      "Enze Shi",
      "Yafei Wang",
      "Aref Sadeghi",
      "Xiaodong Yan",
      "Bei Jiang",
      "Linglong Kong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03155"
  },
  {
    "id": "arXiv:2110.03156",
    "title": "StrengthNet: Deep Learning-based Emotion Strength Assessment for  Emotional Speech Synthesis",
    "abstract": "Recently, emotional speech synthesis has achieved remarkable performance.\nFurthermore, the emotion strength of synthesized speech can be controlled\nflexibly using a strength descriptor, which is obtained by an emotion attribute\nranking function. However, a trained ranking function on specific data has poor\ngeneralization, which limit its applicability for more realistic cases. In this\npaper, we propose a deep learning based emotion strength assessment network for\nstrength prediction that is referred to as StrengthNet. Our model conforms to a\nmulti-task learning framework with a structure that includes an acoustic\nencoder, a strength predictor and an auxiliary emotion predictor. A data\naugmentation strategy was utilized to improve the model generalization.\nExperiments show that the predicted emotion strength of the proposed\nStrengthNet are highly correlated with ground truth scores for seen and unseen\nspeech. Our codes are available at: https://github.com/ttslr/StrengthNet.",
    "descriptor": "\nComments: Submitted to ICASSP 2022. 5 pages, 3 figures, 1 table. Our codes are available at: this https URL\n",
    "authors": [
      "Rui Liu",
      "Berrak Sisman",
      "Haizhou Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03156"
  },
  {
    "id": "arXiv:2110.03157",
    "title": "What Should 6G Architectures Be?",
    "abstract": "The accelerated convergence of digital and real-world lifestyles has imposed\nunprecedented demands on today's wireless network architectures, as it is\nhighly desirable for such architectures to support wireless devices everywhere\nwith high capacity and minimal signaling overhead. Conventional architectures,\nsuch as cellular architectures, are not able to satisfy these requirements\nsimultaneously, and are thus no longer suitable for the 6G era. In this paper,\nwe propose a capacity-centric (C$^2$) architecture for 6G. It is designed based\non the principles of maximizing the number of non-overlapping clusters with the\naverage cluster capacity guaranteed to be higher than a certain threshold, and\nthus provides a flexible way to balance the capacity requirement against the\nsignaling overhead. Our analytical results reveal that C$^2$ has superior\ngenerality, wherein both the cellular and the fully coordinated architectures\ncan be viewed as its extreme cases. Simulation results show that the average\ncapacity of C$^2$ is at least three times higher compared to that of the\ncellular architecture. More importantly, different from the widely adopted\nconventional wisdom that basestation distributions dominate architecture\ndesigns, we find that the C$^2$ architecture is independent of base-station\ndistributions, and instead user-side information should be the focus in 6G\narchitecture designs.",
    "descriptor": "\nComments: 23 pages, 4 figures, submitted to IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Lu Yang",
      "Ping Li",
      "Miaomiao Dong",
      "Bo Bai",
      "Zaporozhets Dmitry",
      "Xiang Chen",
      "Wei Han",
      "Baochun Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03157"
  },
  {
    "id": "arXiv:2110.03163",
    "title": "Transliteration of Foreign Words in Burmese",
    "abstract": "This manuscript provides general descriptions on transliteration of foreign\nwords in the Burmese language. Phenomena caused by phonetic and orthographic\nissues are discussed. Based on this work, we expect to gradually establish\nprescriptive guidelines to normalize the transliteration in Burmese in future.",
    "descriptor": "",
    "authors": [
      "Chenchen Ding"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03163"
  },
  {
    "id": "arXiv:2110.03165",
    "title": "Offline RL With Resource Constrained Online Deployment",
    "abstract": "Offline reinforcement learning is used to train policies in scenarios where\nreal-time access to the environment is expensive or impossible. As a natural\nconsequence of these harsh conditions, an agent may lack the resources to fully\nobserve the online environment before taking an action. We dub this situation\nthe resource-constrained setting. This leads to situations where the offline\ndataset (available for training) can contain fully processed features (using\npowerful language models, image models, complex sensors, etc.) which are not\navailable when actions are actually taken online. This disconnect leads to an\ninteresting and unexplored problem in offline RL: Is it possible to use a\nrichly processed offline dataset to train a policy which has access to fewer\nfeatures in the online environment? In this work, we introduce and formalize\nthis novel resource-constrained problem setting. We highlight the performance\ngap between policies trained using the full offline dataset and policies\ntrained using limited features. We address this performance gap with a policy\ntransfer algorithm which first trains a teacher agent using the offline dataset\nwhere features are fully available, and then transfers this knowledge to a\nstudent agent that only uses the resource-constrained features. To better\ncapture the challenge of this setting, we propose a data collection procedure:\nResource Constrained-Datasets for RL (RC-D4RL). We evaluate our transfer\nalgorithm on RC-D4RL and the popular D4RL benchmarks and observe consistent\nimprovement over the baseline (TD3+BC without transfer). The code for the\nexperiments is available at\nhttps://github.com/JayanthRR/RC-OfflineRL}{github.com/RC-OfflineRL.",
    "descriptor": "",
    "authors": [
      "Jayanth Reddy Regatti",
      "Aniket Anand Deshmukh",
      "Frank Cheng",
      "Young Hun Jung",
      "Abhishek Gupta",
      "Urun Dogan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03165"
  },
  {
    "id": "arXiv:2110.03168",
    "title": "Attacks on Onion Discovery and Remedies via Self-Authenticating  Traditional Addresses",
    "abstract": "Onion addresses encode their own public key. They are thus\nself-authenticating, one of the security and privacy advantages of onion\nservices, which are typically accessed via Tor Browser. Because of the mostly\nrandom-looking appearance of onion addresses, a number of onion discovery\nmechanisms have been created to permit routing to an onion address associated\nwith a more meaningful URL, such as a registered domain name.\nWe describe novel vulnerabilities engendered by onion discovery mechanisms\nrecently introduced by Tor Browser that facilitate hijack and tracking of user\nconnections. We also recall previously known hijack and tracking\nvulnerabilities engendered by use of alternative services that are facilitated\nand rendered harder to detect if the alternative service is at an onion\naddress.\nSelf-authenticating traditional addresses (SATAs) are valid DNS addresses or\nURLs that also contain a commitment to an onion public key. We describe how the\nuse of SATAs in onion discovery counters these vulnerabilities. SATAs also\nexpand the value of onion discovery by facilitating self-authenticated access\nfrom browsers that do not connect to services via the Tor network.",
    "descriptor": "\nComments: To appear in the ACM Workshop on Privacy in the Electronic Society (WPES '21)\n",
    "authors": [
      "Paul Syverson",
      "Matthew Finkel",
      "Saba Eskandarian",
      "Dan Boneh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03168"
  },
  {
    "id": "arXiv:2110.03170",
    "title": "TreeGCN-ED: Encoding Point Cloud using a Tree-Structured Graph Network",
    "abstract": "Point cloud is an efficient way of representing and storing 3D geometric\ndata. Deep learning algorithms on point clouds are time and memory efficient.\nSeveral methods such as PointNet and FoldingNet have been proposed for\nprocessing point clouds. This work proposes an autoencoder based framework to\ngenerate robust embeddings for point clouds by utilizing hierarchical\ninformation using graph convolution. We perform multiple experiments to assess\nthe quality of embeddings generated by the proposed encoder architecture and\nvisualize the t-SNE map to highlight its ability to distinguish between\ndifferent object classes. We further demonstrate the applicability of the\nproposed framework in applications like: 3D point cloud completion and Single\nimage based 3D reconstruction.",
    "descriptor": "",
    "authors": [
      "Prajwal Singh",
      "Kaustubh Sadekar",
      "Shanmuganathan Raman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03170"
  },
  {
    "id": "arXiv:2110.03171",
    "title": "Assemblies of neurons can learn to classify well-separated distributions",
    "abstract": "Assemblies are patterns of coordinated firing across large populations of\nneurons, believed to represent higher-level information in the brain, such as\nmemories, concepts, words, and other cognitive categories. Recently, a\ncomputational system called the Assembly Calculus (AC) has been proposed, based\non a set of biologically plausible operations on assemblies. This system is\ncapable of simulating arbitrary space-bounded computation, and describes quite\nnaturally complex cognitive phenomena such as language. However, the question\nof whether assemblies can perform the brain's greatest trick -- its ability to\nlearn -- has been open. We show that the AC provides a mechanism for learning\nto classify samples from well-separated classes. We prove rigorously that for\nsimple classification problems, a new assembly that represents each class can\nbe reliably formed in response to a few stimuli from it; this assembly is\nhenceforth reliably recalled in response to new stimuli from the same class.\nFurthermore, such class assemblies will be distinguishable as long as the\nrespective classes are reasonably separated, in particular when they are\nclusters of similar assemblies, or more generally divided by a halfspace with\nmargin. Experimentally, we demonstrate the successful formation of assemblies\nwhich represent concept classes on synthetic data drawn from these\ndistributions, and also on MNIST, which lends itself to classification through\none assembly per digit. Seen as a learning algorithm, this mechanism is\nentirely online, generalizes from very few samples, and requires only mild\nsupervision -- all key attributes of learning in a model of the brain.",
    "descriptor": "",
    "authors": [
      "Max Dabagia",
      "Christos H. Papadimitriou",
      "Santosh S. Vempala"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03171"
  },
  {
    "id": "arXiv:2110.03173",
    "title": "Multi-objective Optimization by Learning Space Partitions",
    "abstract": "In contrast to single-objective optimization (SOO), multi-objective\noptimization (MOO) requires an optimizer to find the Pareto frontier, a subset\nof feasible solutions that are not dominated by other feasible solutions. In\nthis paper, we propose LaMOO, a novel multi-objective optimizer that learns a\nmodel from observed samples to partition the search space and then focus on\npromising regions that are likely to contain a subset of the Pareto frontier.\nThe partitioning is based on the dominance number, which measures \"how close\" a\ndata point is to the Pareto frontier among existing samples. To account for\npossible partition errors due to limited samples and model mismatch, we\nleverage Monte Carlo Tree Search (MCTS) to exploit promising regions while\nexploring suboptimal regions that may turn out to contain good solutions later.\nTheoretically, we prove the efficacy of learning space partitioning via LaMOO\nunder certain assumptions. Empirically, on the HyperVolume (HV) benchmark, a\npopular MOO metric, LaMOO substantially outperforms strong baselines on\nmultiple real-world MOO tasks, by up to 225% in sample efficiency for neural\narchitecture search on Nasbench201, and up to 10% for molecular design.",
    "descriptor": "",
    "authors": [
      "Yiyang Zhao",
      "Linnan Wang",
      "Kevin Yang",
      "Tianjun Zhang",
      "Tian Guo",
      "Yuandong Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03173"
  },
  {
    "id": "arXiv:2110.03174",
    "title": "Transferring Voice Knowledge for Acoustic Event Detection: An Empirical  Study",
    "abstract": "Detection of common events and scenes from audio is useful for extracting and\nunderstanding human contexts in daily life. Prior studies have shown that\nleveraging knowledge from a relevant domain is beneficial for a target acoustic\nevent detection (AED) process. Inspired by the observation that many\nhuman-centered acoustic events in daily life involve voice elements, this paper\ninvestigates the potential of transferring high-level voice representations\nextracted from a public speaker dataset to enrich an AED pipeline. Towards this\nend, we develop a dual-branch neural network architecture for the joint\nlearning of voice and acoustic features during an AED process and conduct\nthorough empirical studies to examine the performance on the public AudioSet\n[1] with different types of inputs. Our main observations are that: 1) Joint\nlearning of audio and voice inputs improves the AED performance (mean average\nprecision) for both a CNN baseline (0.292 vs 0.134 mAP) and a TALNet [2]\nbaseline (0.361 vs 0.351 mAP); 2) Augmenting the extra voice features is\ncritical to maximize the model performance with dual inputs.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Dawei Liang",
      "Yangyang Shi",
      "Yun Wang",
      "Nayan Singhal",
      "Alex Xiao",
      "Jonathan Shaw",
      "Edison Thomaz",
      "Ozlem Kalinli",
      "Mike Seltzer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03174"
  },
  {
    "id": "arXiv:2110.03175",
    "title": "Fingerprinting Multi-exit Deep Neural Network Models via Inference Time",
    "abstract": "Transforming large deep neural network (DNN) models into the multi-exit\narchitectures can overcome the overthinking issue and distribute a large DNN\nmodel on resource-constrained scenarios (e.g. IoT frontend devices and backend\nservers) for inference and transmission efficiency. Nevertheless, intellectual\nproperty (IP) protection for the multi-exit models in the wild is still an\nunsolved challenge. Previous efforts to verify DNN model ownership mainly rely\non querying the model with specific samples and checking the responses, e.g.,\nDNN watermarking and fingerprinting. However, they are vulnerable to\nadversarial settings such as adversarial training and are not suitable for the\nIP verification for multi-exit DNN models. In this paper, we propose a novel\napproach to fingerprint multi-exit models via inference time rather than\ninference predictions. Specifically, we design an effective method to generate\na set of fingerprint samples to craft the inference process with a unique and\nrobust inference time cost as the evidence for model ownership. We conduct\nextensive experiments to prove the uniqueness and robustness of our method on\nthree structures (ResNet-56, VGG-16, and MobileNet) and three datasets\n(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial\nsettings.",
    "descriptor": "",
    "authors": [
      "Tian Dong",
      "Han Qiu",
      "Tianwei Zhang",
      "Jiwei Li",
      "Hewu Li",
      "Jialiang Lu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03175"
  },
  {
    "id": "arXiv:2110.03177",
    "title": "EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits",
    "abstract": "Contextual multi-armed bandits have been studied for decades and adapted to\nvarious applications such as online advertising and personalized\nrecommendation. To solve the exploitation-exploration tradeoff in bandits,\nthere are three main techniques: epsilon-greedy, Thompson Sampling (TS), and\nUpper Confidence Bound (UCB). In recent literature, linear contextual bandits\nhave adopted ridge regression to estimate the reward function and combine it\nwith TS or UCB strategies for exploration. However, this line of works\nexplicitly assumes the reward is based on a linear function of arm vectors,\nwhich may not be true in real-world datasets. To overcome this challenge, a\nseries of neural-based bandit algorithms have been proposed, where a neural\nnetwork is assigned to learn the underlying reward function and TS or UCB are\nadapted for exploration. In this paper, we propose \"EE-Net\", a neural-based\nbandit approach with a novel exploration strategy. In addition to utilizing a\nneural network (Exploitation network) to learn the reward function, EE-Net\nadopts another neural network (Exploration network) to adaptively learn\npotential gains compared to currently estimated reward. Then, a decision-maker\nis constructed to combine the outputs from the Exploitation and Exploration\nnetworks. We prove that EE-Net achieves $\\mathcal{O}(\\sqrt{T\\log T})$ regret,\nwhich is tighter than existing state-of-the-art neural bandit algorithms\n($\\mathcal{O}(\\sqrt{T}\\log T)$ for both UCB-based and TS-based). Through\nextensive experiments on four real-world datasets, we show that EE-Net\noutperforms existing linear and neural bandit approaches.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Yikun Ban",
      "Yuchen Yan",
      "Arindam Banerjee",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03177"
  },
  {
    "id": "arXiv:2110.03179",
    "title": "HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow  Articles",
    "abstract": "We present \\textsc{HowSumm}, a novel large-scale dataset for the task of\nquery-focused multi-document summarization (qMDS), which targets the use-case\nof generating actionable instructions from a set of sources. This use-case is\ndifferent from the use-cases covered in existing multi-document summarization\n(MDS) datasets and is applicable to educational and industrial scenarios. We\nemployed automatic methods, and leveraged statistics from existing\nhuman-crafted qMDS datasets, to create \\textsc{HowSumm} from wikiHow website\narticles and the sources they cite. We describe the creation of the dataset and\ndiscuss the unique features that distinguish it from other summarization\ncorpora. Automatic and human evaluations of both extractive and abstractive\nsummarization models on the dataset reveal that there is room for improvement.\n% in existing summarization models We propose that \\textsc{HowSumm} can be\nleveraged to advance summarization research.",
    "descriptor": "\nComments: 8 pages, 4 figures, 5 tables. HowSumm dataset is publicly available at \\url{this https URL}\n",
    "authors": [
      "Odellia Boni",
      "Guy Feigenblat",
      "Guy Lev",
      "Michal Shmueli-Scheuer",
      "Benjamin Sznajder",
      "David Konopnicki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03179"
  },
  {
    "id": "arXiv:2110.03181",
    "title": "Tile Embedding: A General Representation for Procedural Level Generation  via Machine Learning",
    "abstract": "In recent years, Procedural Level Generation via Machine Learning (PLGML)\ntechniques have been applied to generate game levels with machine learning.\nThese approaches rely on human-annotated representations of game levels.\nCreating annotated datasets for games requires domain knowledge and is\ntime-consuming. Hence, though a large number of video games exist, annotated\ndatasets are curated only for a small handful. Thus current PLGML techniques\nhave been explored in limited domains, with Super Mario Bros. as the most\ncommon example. To address this problem, we present tile embeddings, a unified,\naffordance-rich representation for tile-based 2D games. To learn this\nembedding, we employ autoencoders trained on the visual and semantic\ninformation of tiles from a set of existing, human-annotated games. We evaluate\nthis representation on its ability to predict affordances for unseen tiles, and\nto serve as a PLGML representation for annotated and unannotated games.",
    "descriptor": "\nComments: 8 pages, 5 figures, AIIDE 2021\n",
    "authors": [
      "Mrunal Jadhav",
      "Matthew Guzdial"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03181"
  },
  {
    "id": "arXiv:2110.03183",
    "title": "Attention is All You Need? Good Embeddings with Statistics are enough:  Audio Understanding WITHOUT  Convolutions/Transformers/BERTs/Mixers/Attention/RNNs or ....",
    "abstract": "This paper presents a way of doing large scale audio understanding without\ntraditional state of the art neural architectures. Ever since the introduction\nof deep learning for understanding audio signals in the past decade,\nconvolutional architectures have been able to achieve state of the art results\nsurpassing traditional hand-crafted features. In the recent past, there has\nbeen a similar shift away from traditional convolutional and recurrent neural\nnetworks towards purely end-to-end Transformer architectures. We, in this work,\nexplore an approach, based on Bag-of-Words model. Our approach does not have\nany convolutions, recurrence, attention, transformers or other approaches such\nas BERT. We utilize micro and macro level clustered vanilla embeddings, and use\na MLP head for classification. We only use feed-forward encoder-decoder models\nto get the bottlenecks of spectral envelops, spectral patches and slices as\nwell as multi-resolution spectra. A classification head (a feed-forward layer),\nsimilar to the approach in SimCLR is trained on a learned representation. Using\nsimple codes learned on latent representations, we show how we surpass\ntraditional convolutional neural network architectures, and come strikingly\nclose to outperforming powerful Transformer architectures. This work hopefully\nwould pave way for exciting advancements in the field of representation\nlearning without massive, end-to-end neural architectures.",
    "descriptor": "\nComments: ICASPP 2022; Singapore\n",
    "authors": [
      "Prateek Verma"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2110.03183"
  },
  {
    "id": "arXiv:2110.03184",
    "title": "Explaining Deep Reinforcement Learning Agents In The Atari Domain  through a Surrogate Model",
    "abstract": "One major barrier to applications of deep Reinforcement Learning (RL) both\ninside and outside of games is the lack of explainability. In this paper, we\ndescribe a lightweight and effective method to derive explanations for deep RL\nagents, which we evaluate in the Atari domain. Our method relies on a\ntransformation of the pixel-based input of the RL agent to an interpretable,\npercept-like input representation. We then train a surrogate model, which is\nitself interpretable, to replicate the behavior of the target, deep RL agent.\nOur experiments demonstrate that we can learn an effective surrogate that\naccurately approximates the underlying decision making of a target agent on a\nsuite of Atari games.",
    "descriptor": "\nComments: 9 pages, 3 figures, AIIDE 2021\n",
    "authors": [
      "Alexander Sieusahai",
      "Matthew Guzdial"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03184"
  },
  {
    "id": "arXiv:2110.03187",
    "title": "On the Optimal Memorization Power of ReLU Neural Networks",
    "abstract": "We study the memorization power of feedforward ReLU neural networks. We show\nthat such networks can memorize any $N$ points that satisfy a mild separability\nassumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known\nVC-dimension upper bounds imply that memorizing $N$ samples requires\n$\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to\nlogarithmic factors. We also give a generalized construction for networks with\ndepth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using\n$\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic\nfactors. Our construction uses weights with large bit complexity. We prove that\nhaving such a large bit complexity is both necessary and sufficient for\nmemorization with a sub-linear number of parameters.",
    "descriptor": "",
    "authors": [
      "Gal Vardi",
      "Gilad Yehudai",
      "Ohad Shamir"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03187"
  },
  {
    "id": "arXiv:2110.03189",
    "title": "Pointwise Bounds for Distribution Estimation under Communication  Constraints",
    "abstract": "We consider the problem of estimating a $d$-dimensional discrete distribution\nfrom its samples observed under a $b$-bit communication constraint. In contrast\nto most previous results that largely focus on the global minimax error, we\nstudy the local behavior of the estimation error and provide \\emph{pointwise}\nbounds that depend on the target distribution $p$. In particular, we show that\nthe $\\ell_2$ error decays with $O\\left(\\frac{\\lVert p\\rVert_{1/2}}{n2^b}\\vee\n\\frac{1}{n}\\right)$ (In this paper, we use $a\\vee b$ and $a \\wedge b$ to denote\n$\\max(a, b)$ and $\\min(a,b)$ respectively.) when $n$ is sufficiently large,\nhence it is governed by the \\emph{half-norm} of $p$ instead of the ambient\ndimension $d$. For the achievability result, we propose a two-round\nsequentially interactive estimation scheme that achieves this error rate\nuniformly over all $p$. Our scheme is based on a novel local refinement idea,\nwhere we first use a standard global minimax scheme to localize $p$ and then\nuse the remaining samples to locally refine our estimate. We also develop a new\nlocal minimax lower bound with (almost) matching $\\ell_2$ error, showing that\nany interactive scheme must admit a $\\Omega\\left( \\frac{\\lVert p\n\\rVert_{{(1+\\delta)}/{2}}}{n2^b}\\right)$ $\\ell_2$ error for any $\\delta > 0$.\nThe lower bound is derived by first finding the best parametric sub-model\ncontaining $p$, and then upper bounding the quantized Fisher information under\nthis model.\nOur upper and lower bounds together indicate that the $\\mathcal{H}_{1/2}(p) =\n\\log(\\lVert p \\rVert_{{1}/{2}})$ bits of communication is both sufficient and\nnecessary to achieve the optimal (centralized) performance, where\n$\\mathcal{H}_{{1}/{2}}(p)$ is the R\\'enyi entropy of order $2$. Therefore,\nunder the $\\ell_2$ loss, the correct measure of the local communication\ncomplexity at $p$ is its R\\'enyi entropy.",
    "descriptor": "",
    "authors": [
      "Wei-Ning Chen",
      "Peter Kairouz",
      "Ayfer \u00d6zg\u00fcr"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03189"
  },
  {
    "id": "arXiv:2110.03192",
    "title": "GNN is a Counter? Revisiting GNN for Question Answering",
    "abstract": "Question Answering (QA) has been a long-standing research topic in AI and NLP\nfields, and a wealth of studies have been conducted to attempt to equip QA\nsystems with human-level reasoning capability. To approximate the complicated\nhuman reasoning process, state-of-the-art QA systems commonly use pre-trained\nlanguage models (LMs) to access knowledge encoded in LMs together with\nelaborately designed modules based on Graph Neural Networks (GNNs) to perform\nreasoning over knowledge graphs (KGs). However, many problems remain open\nregarding the reasoning functionality of these GNN-based modules. Can these\nGNN-based modules really perform a complex reasoning process? Are they under-\nor over-complicated for QA? To open the black box of GNN and investigate these\nproblems, we dissect state-of-the-art GNN modules for QA and analyze their\nreasoning capability. We discover that even a very simple graph neural counter\ncan outperform all the existing GNN modules on CommonsenseQA and OpenBookQA,\ntwo popular QA benchmark datasets which heavily rely on knowledge-aware\nreasoning. Our work reveals that existing knowledge-aware GNN modules may only\ncarry out some simple reasoning such as counting. It remains a challenging open\nproblem to build comprehensive reasoning modules for knowledge-powered QA.",
    "descriptor": "",
    "authors": [
      "Kuan Wang",
      "Yuyu Zhang",
      "Diyi Yang",
      "Le Song",
      "Tao Qin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03192"
  },
  {
    "id": "arXiv:2110.03195",
    "title": "Coresets for Decision Trees of Signals",
    "abstract": "A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix\n(2D-signal) into $k\\geq 1$ block matrices (axis-parallel rectangles, leaves)\nwhere each rectangle is assigned a real label. Its regression or classification\nloss to a given matrix $D$ of $N$ entries (labels) is the sum of squared\ndifferences over every label in $D$ and its assigned label by $t$. Given an\nerror parameter $\\varepsilon\\in(0,1)$, a $(k,\\varepsilon)$-coreset $C$ of $D$\nis a small summarization that provably approximates this loss to \\emph{every}\nsuch tree, up to a multiplicative factor of $1\\pm\\varepsilon$. In particular,\nthe optimal $k$-tree of $C$ is a $(1+\\varepsilon)$-approximation to the optimal\n$k$-tree of $D$.\nWe provide the first algorithm that outputs such a $(k,\\varepsilon)$-coreset\nfor \\emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial\nin $k\\log(N)/\\varepsilon$, and its construction takes $O(Nk)$ time. This is by\nforging a link between decision trees from machine learning -- to partition\ntrees in computational geometry.\nExperimental results on \\texttt{sklearn} and \\texttt{lightGBM} show that\napplying our coresets on real-world data-sets boosts the computation time of\nrandom forests and their parameter tuning by up to x$10$, while keeping similar\naccuracy. Full open source code is provided.",
    "descriptor": "",
    "authors": [
      "Ibrahim Jubran",
      "Ernesto Evgeniy Sanches Shayda",
      "Ilan Newman",
      "Dan Feldman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03195"
  },
  {
    "id": "arXiv:2110.03205",
    "title": "Evolutionary Computation-Assisted Brainwriting for Large-Scale Online  Ideation",
    "abstract": "Brainstorming is an effective technique for offline ideation although the\nnumber of participants able to join an ideation session and suggest ideas is\nlimited. To increase the diversity and quality of the ideas suggested, many\nparticipants with various backgrounds should be able to join the session. We\nhave devised an evolutionary computation-assisted brainwriting method for\nlarge-scale online ideation. In this method, participants not only suggest\nideas but also evaluate ideas previously suggested by other participants. The\nevaluation results are used in the evolutionary computation to identify good\nideas to which the participants can be exposed via a brainwriting-like\ninterface. We compared the performance of the proposed method with that of a\nsimple online brainwriting method for large-scale online ideation with more\nthan 30 participants. The proposed method enhanced robustness of idea quality\nimprovement due to preferentially exposing the participants to good ideas.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Nobuo Namura",
      "Tatsuya Hasebe"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.03205"
  },
  {
    "id": "arXiv:2110.03210",
    "title": "Universality of Deep Neural Network Lottery Tickets: A Renormalization  Group Perspective",
    "abstract": "Foundational work on the Lottery Ticket Hypothesis has suggested an exciting\ncorollary: winning tickets found in the context of one task can be transferred\nto similar tasks, possibly even across different architectures. While this has\nbecome of broad practical and theoretical interest, to date, there exists no\ndetailed understanding of why winning ticket universality exists, or any way of\nknowing \\textit{a priori} whether a given ticket can be transferred to a given\ntask. To address these outstanding open questions, we make use of\nrenormalization group theory, one of the most successful tools in theoretical\nphysics. We find that iterative magnitude pruning, the method used for\ndiscovering winning tickets, is a renormalization group scheme. This opens the\ndoor to a wealth of existing numerical and theoretical tools, some of which we\nleverage here to examine winning ticket universality in large scale lottery\nticket experiments, as well as sheds new light on the success iterative\nmagnitude pruning has found in the field of sparse machine learning.",
    "descriptor": "\nComments: 16 pages, 3 figures, 6 tables\n",
    "authors": [
      "William T. Redman",
      "Tianlong Chen",
      "Akshunna S. Dogra",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)"
    ],
    "url": "https://arxiv.org/abs/2110.03210"
  },
  {
    "id": "arXiv:2110.03212",
    "title": "Influence Tuning: Demoting Spurious Correlations via Instance  Attribution and Instance-Driven Updates",
    "abstract": "Among the most critical limitations of deep learning NLP models are their\nlack of interpretability, and their reliance on spurious correlations. Prior\nwork proposed various approaches to interpreting the black-box models to unveil\nthe spurious correlations, but the research was primarily used in\nhuman-computer interaction scenarios. It still remains underexplored whether or\nhow such model interpretations can be used to automatically \"unlearn\"\nconfounding features. In this work, we propose influence tuning--a procedure\nthat leverages model interpretations to update the model parameters towards a\nplausible interpretation (rather than an interpretation that relies on spurious\npatterns in the data) in addition to learning to predict the task labels. We\nshow that in a controlled setup, influence tuning can help deconfounding the\nmodel from spurious patterns in data, significantly outperforming baseline\nmethods that use adversarial training.",
    "descriptor": "\nComments: Findings of EMNLP 2021\n",
    "authors": [
      "Xiaochuang Han",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03212"
  },
  {
    "id": "arXiv:2110.03214",
    "title": "MAPA: Multi-Accelerator Pattern Allocation Policy for Multi-Tenant GPU  Servers",
    "abstract": "Multi-accelerator servers are increasingly being deployed in shared\nmulti-tenant environments (such as in cloud data centers) in order to meet the\ndemands of large-scale compute-intensive workloads. In addition, these\naccelerators are increasingly being inter-connected in complex topologies and\nworkloads are exhibiting a wider variety of inter-accelerator communication\npatterns. However, existing allocation policies are ill-suited for these\nemerging use-cases. Specifically, this work identifies that multi-accelerator\nworkloads are commonly fragmented leading to reduced bandwidth and increased\nlatency for inter-accelerator communication. We propose Multi-Accelerator\nPattern Allocation (MAPA), a graph pattern mining approach towards providing\ngeneralized allocation support for allocating multi-accelerator workloads on\nmulti-accelerator servers. We demonstrate that MAPA is able to improve the\nexecution time of multi-accelerator workloads and that MAPA is able to provide\ngeneralized benefits across various accelerator topologies. Finally, we\ndemonstrate a speedup of 12.4% for 75th percentile of jobs with the worst case\nexecution time reduced by up to 35% against baseline policy using MAPA.",
    "descriptor": "",
    "authors": [
      "Kiran Ranganath",
      "Joshua D. Suetterlein",
      "Joseph B. Manzano",
      "Shuaiwen Leon Song",
      "Daniel Wong"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.03214"
  },
  {
    "id": "arXiv:2110.03215",
    "title": "Towards Continual Knowledge Learning of Language Models",
    "abstract": "Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs.",
    "descriptor": "",
    "authors": [
      "Joel Jang",
      "Seonghyeon Ye",
      "Sohee Yang",
      "Joongbo Shin",
      "Janghoon Han",
      "Gyeonghun Kim",
      "Stanley Jungkyu Choi",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03215"
  },
  {
    "id": "arXiv:2110.03220",
    "title": "Gradient Step Denoiser for convergent Plug-and-Play",
    "abstract": "Plug-and-Play methods constitute a class of iterative algorithms for imaging\nproblems where regularization is performed by an off-the-shelf denoiser.\nAlthough Plug-and-Play methods can lead to tremendous visual performance for\nvarious image problems, the few existing convergence guarantees are based on\nunrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly\nconvex data terms. In this work, we propose a new type of Plug-and-Play\nmethods, based on half-quadratic splitting, for which the denoiser is realized\nas a gradient descent step on a functional parameterized by a deep neural\nnetwork. Exploiting convergence results for proximal gradient descent\nalgorithms in the non-convex setting, we show that the proposed Plug-and-Play\nalgorithm is a convergent iterative scheme that targets stationary points of an\nexplicit global functional. Besides, experiments show that it is possible to\nlearn such a deep denoiser while not compromising the performance in comparison\nto other state-of-the-art deep denoisers used in Plug-and-Play schemes. We\napply our proximal gradient algorithm to various ill-posed inverse problems,\ne.g. deblurring, super-resolution and inpainting. For all these applications,\nnumerical results empirically confirm the convergence results. Experiments also\nshow that this new algorithm reaches state-of-the-art performance, both\nquantitatively and qualitatively.",
    "descriptor": "",
    "authors": [
      "Samuel Hurault",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03220"
  },
  {
    "id": "arXiv:2110.03222",
    "title": "A uniformly accurate scheme for the numerical integration of penalized  Langevin dynamics",
    "abstract": "In molecular dynamics, penalized overdamped Langevin dynamics are used to\nmodel the motion of a set of particles that follow constraints up to a\nparameter $\\varepsilon$. The most used schemes for simulating these dynamics\nare the Euler integrator in $\\mathbb{R}^d$ and the constrained Euler\nintegrator. Both have weak order one of accuracy, but work properly only in\nspecific regimes depending on the size of the parameter $\\varepsilon$. We\npropose in this paper a new consistent method with an accuracy independent of\n$\\varepsilon$ for solving penalized dynamics on a manifold of any dimension.\nMoreover, this method converges to the constrained Euler scheme when\n$\\varepsilon$ goes to zero. The numerical experiments confirm the theoretical\nfindings, in the context of weak convergence and for the invariant measure, on\na torus and on the orthogonal group in high dimension and high codimension.",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Adrien Laurent"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03222"
  },
  {
    "id": "arXiv:2110.03223",
    "title": "Goal-Directed Design Agents: Integrating Visual Imitation with One-Step  Lookahead Optimization for Generative Design",
    "abstract": "Engineering design problems often involve large state and action spaces along\nwith highly sparse rewards. Since an exhaustive search of those spaces is not\nfeasible, humans utilize relevant domain knowledge to condense the search\nspace. Previously, deep learning agents (DLAgents) were introduced to use\nvisual imitation learning to model design domain knowledge. This note builds on\nDLAgents and integrates them with one-step lookahead search to develop\ngoal-directed agents capable of enhancing learned strategies for sequentially\ngenerating designs. Goal-directed DLAgents can employ human strategies learned\nfrom data along with optimizing an objective function. The visual imitation\nnetwork from DLAgents is composed of a convolutional encoder-decoder network,\nacting as a rough planning step that is agnostic to feedback. Meanwhile, the\nlookahead search identifies the fine-tuned design action guided by an\nobjective. These design agents are trained on an unconstrained truss design\nproblem that is modeled as a sequential, action-based configuration design\nproblem. The agents are then evaluated on two versions of the problem: the\noriginal version used for training and an unseen constrained version with an\nobstructed construction space. The goal-directed agents outperform the human\ndesigners used to train the network as well as the previous objective-agnostic\nversions of the agent in both scenarios. This illustrates a design agent\nframework that can efficiently use feedback to not only enhance learned design\nstrategies but also adapt to unseen design problems.",
    "descriptor": "",
    "authors": [
      "Ayush Raina",
      "Lucas Puentes",
      "Jonathan Cagan",
      "Christopher McComb"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03223"
  },
  {
    "id": "arXiv:2110.03224",
    "title": "Darts: User-Friendly Modern Machine Learning for Time Series",
    "abstract": "We present Darts, a Python machine learning library for time series, with a\nfocus on forecasting. Darts offers a variety of models, from classics such as\nARIMA to state-of-the-art deep neural networks. The emphasis of the library is\non offering modern machine learning functionalities, such as supporting\nmultidimensional series, meta-learning on multiple series, training on large\ndatasets, incorporating external data, ensembling models, and providing a rich\nsupport for probabilistic forecasting. At the same time, great care goes into\nthe API design to make it user-friendly and easy to use. For instance, all\nmodels can be used using fit()/predict(), similar to scikit-learn.",
    "descriptor": "\nComments: Darts Github repository: this https URL\n",
    "authors": [
      "Julien Herzen",
      "Francesco L\u00e4ssig",
      "Samuele Giuliano Piazzetta",
      "Thomas Neuer",
      "L\u00e9o Tafti",
      "Guillaume Raille",
      "Tomas Van Pottelbergh",
      "Marek Pasieka",
      "Andrzej Skrodzki",
      "Nicolas Huguenin",
      "Maxime Dumonal",
      "Jan Ko\u015bcisz",
      "Dennis Bader",
      "Fr\u00e9d\u00e9rick Gusset",
      "Mounir Benheddi",
      "Camila Williamson",
      "Michal Kosinski",
      "Matej Petrik",
      "Ga\u00ebl Grosch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.03224"
  },
  {
    "id": "arXiv:2110.03229",
    "title": "IaaS Signature Change Detection with Performance Noise",
    "abstract": "We propose a novel framework to detect changes in the performance behavior of\nan IaaS service. The proposed framework leverages the concept of the IaaS\nsignature to represent an IaaS service's long-term performance behavior. A new\ntype of performance signature called categorical IaaS signature is introduced\nto represent the performance behavior more accurately. A novel performance\nnoise model is proposed to accurately identify IaaS performance noise and\naccurate changes in the performance behavior of an IaaS service. A set of\nexperiments based on real-world datasets is carried out to evaluate the\neffectiveness of the proposed framework.",
    "descriptor": "\nComments: 15 pages, accepted and to be published in 19th International Conference on Service Oriented Computing. arXiv admin note: text overlap with arXiv:2007.11705\n",
    "authors": [
      "Sheik Mohammad Mostakim Fattah",
      "Athman Bouguettaya"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.03229"
  },
  {
    "id": "arXiv:2110.03232",
    "title": "Design of an Intelligent Vision Algorithm for Recognition and  Classification of Apples in an Orchard Scene",
    "abstract": "Apple is one of the remarkable fresh fruit that contains a high degree of\nnutritious and medicinal value. Hand harvesting of apples by seasonal\nfarmworkers increases physical damages on the surface of these fruits, which\ncauses a great loss in marketing quality. The main objective of this study is\nfocused on designing a robust vision algorithm for robotic apple harvesters.\nThe proposed algorithm is able to recognize and classify 4-classes of objects\nfound in an orchard scene including apples, leaves, trunk and branches, and sky\ninto two apples and non-apples classes. 100 digital images of Red Delicious\napples and 100 digital images of Golden Delicious apples were selected among\n1000 captured images of apples from 18 apple gardens in West Azerbaijan, Iran.\nAn image processing algorithm is proposed for segmentation and extraction of\nthe image classes based on the color characteristics of mentioned classes.\nInvariant-Momentums were chosen as the extracted features from the segmented\nclasses, e.g. apples. Multilayer Feedforward Neural Networks, MFNNs, were used\nas an artificial intelligence tool for the recognition and classification of\nimage classes.",
    "descriptor": "\nComments: 34 pages, 12 figures\n",
    "authors": [
      "Hamid Majidi Balanji",
      "Alaeedin Rahmani Didar",
      "Mohamadali Hadad Derafshi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03232"
  },
  {
    "id": "arXiv:2110.03234",
    "title": "Self-Supervised Depth Completion for Active Stereo",
    "abstract": "Active stereo systems are widely used in the robotics industry due to their\nlow cost and high quality depth maps. These depth sensors, however, suffer from\nstereo artefacts and do not provide dense depth estimates. In this work, we\npresent the first self-supervised depth completion method for active stereo\nsystems that predicts accurate dense depth maps. Our system leverages a\nfeature-based visual inertial SLAM system to produce motion estimates and\naccurate (but sparse) 3D landmarks. The 3D landmarks are used both as model\ninput and as supervision during training. The motion estimates are used in our\nnovel reconstruction loss that relies on a combination of passive and active\nstereo frames, resulting in significant improvements in textureless areas that\nare common in indoor environments. Due to the non-existence of publicly\navailable active stereo datasets, we release a real dataset together with\nadditional information for a publicly available synthetic dataset needed for\nactive depth completion and prediction. Through rigorous evaluations we show\nthat our method outperforms state of the art on both datasets. Additionally we\nshow how our method obtains more complete, and therefore safer, 3D maps when\nused in a robotic platform",
    "descriptor": "\nComments: Submitted to RAL-ICRA 21\n",
    "authors": [
      "Frederik Warburg",
      "Daniel Hernandez-Juarez",
      "Juan Tarrio",
      "Alexander Vakhitov",
      "Ujwal Bonde",
      "Pablo Alcantarilla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03234"
  },
  {
    "id": "arXiv:2110.03235",
    "title": "Joint Approximate Diagonalization under Orthogonality Constraints",
    "abstract": "Joint diagonalization of a set of positive (semi)-definite matrices has a\nwide range of analytical applications, such as estimation of common principal\ncomponents, estimation of multiple variance components, and blind signal\nseparation. However, when the eigenvectors of involved matrices are not the\nsame, joint diagonalization is a computationally challenging problem. To the\nbest of our knowledge, currently existing methods require at least $O(KN^3)$\ntime per iteration, when $K$ different $N \\times N$ matrices are considered. We\nreformulate this optimization problem by applying orthogonality constraints and\ndimensionality reduction techniques. In doing so, we reduce the computational\ncomplexity for joint diagonalization to $O(N^3)$ per quasi-Newton iteration.\nThis approach we refer to as JADOC: Joint Approximate Diagonalization under\nOrthogonality Constraints. We compare our algorithm to two important existing\nmethods and show JADOC has superior runtime while yielding a highly similar\ndegree of diagonalization. The JADOC algorithm is implemented as open-source\nPython code, available at https://github.com/devlaming/jadoc.",
    "descriptor": "",
    "authors": [
      "Ronald de Vlaming",
      "Eric A.W. Slob"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03235"
  },
  {
    "id": "arXiv:2110.03237",
    "title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport",
    "abstract": "We consider the fundamental problem of sampling the optimal transport\ncoupling between given source and target distributions. In certain cases, the\noptimal transport plan takes the form of a one-to-one mapping from the source\nsupport to the target support, but learning or even approximating such a map is\ncomputationally challenging for large and high-dimensional datasets due to the\nhigh cost of linear programming routines and an intrinsic curse of\ndimensionality. We study instead the Sinkhorn problem, a regularized form of\noptimal transport whose solutions are couplings between the source and the\ntarget distribution. We introduce a novel framework for learning the Sinkhorn\ncoupling between two distributions in the form of a score-based generative\nmodel. Conditioned on source data, our procedure iterates Langevin Dynamics to\nsample target data according to the regularized optimal coupling. Key to this\napproach is a neural network parametrization of the Sinkhorn problem, and we\nprove convergence of gradient descent with respect to network parameters in\nthis formulation. We demonstrate its empirical success on a variety of large\nscale optimal transport tasks.",
    "descriptor": "\nComments: Accepted to NeurIPS 2021. This v1 will soon be replaced by camera ready\n",
    "authors": [
      "Max Daniels",
      "Tyler Maunu",
      "Paul Hand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03237"
  },
  {
    "id": "arXiv:2110.03239",
    "title": "Understanding Domain Randomization for Sim-to-real Transfer",
    "abstract": "Reinforcement learning encounters many challenges when applied directly in\nthe real world. Sim-to-real transfer is widely used to transfer the knowledge\nlearned from simulation to the real world. Domain randomization -- one of the\nmost popular algorithms for sim-to-real transfer -- has been demonstrated to be\neffective in various tasks in robotics and autonomous driving. Despite its\nempirical successes, theoretical understanding on why this simple algorithm\nworks is limited. In this paper, we propose a theoretical framework for\nsim-to-real transfers, in which the simulator is modeled as a set of MDPs with\ntunable parameters (corresponding to unknown physical parameters such as\nfriction). We provide sharp bounds on the sim-to-real gap -- the difference\nbetween the value of policy returned by domain randomization and the value of\nan optimal policy for the real world. We prove that sim-to-real transfer can\nsucceed under mild conditions without any real-world training samples. Our\ntheory also highlights the importance of using memory (i.e., history-dependent\npolicies) in domain randomization. Our proof is based on novel techniques that\nreduce the problem of bounding the sim-to-real gap to the problem of designing\nefficient learning algorithms for infinite-horizon MDPs, which we believe are\nof independent interest.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Chen",
      "Jiachen Hu",
      "Chi Jin",
      "Lihong Li",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03239"
  },
  {
    "id": "arXiv:2110.03242",
    "title": "On the asymptotical regularization with convex constraints for inverse  problems",
    "abstract": "In this paper, we consider the asymptotical regularization with convex\nconstraints for nonlinear ill-posed problems. The method allows to use\nnon-smooth penalty terms, including the L1-like and the total variation-like\npenalty functionals, which are significant in reconstructing special features\nof solutions such as sparsity and piecewise constancy. Under certain conditions\nwe give convergence properties of the methods. Moreover, we propose Runge-Kutta\ntype methods to discrete the initial value problems to construct new type\niterative regularization methods.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Min Zhong",
      "Wang Wei"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03242"
  },
  {
    "id": "arXiv:2110.03243",
    "title": "Sound Event Detection Guided by Semantic Contexts of Scenes",
    "abstract": "Some studies have revealed that contexts of scenes (e.g., \"home,\" \"office,\"\nand \"cooking\") are advantageous for sound event detection (SED). Mobile devices\nand sensing technologies give useful information on scenes for SED without the\nuse of acoustic signals. However, conventional methods can employ pre-defined\ncontexts in inference stages but not undefined contexts. This is because\none-hot representations of pre-defined scenes are exploited as prior contexts\nfor such conventional methods. To alleviate this problem, we propose\nscene-informed SED where pre-defined scene-agnostic contexts are available for\nmore accurate SED. In the proposed method, pre-trained large-scale language\nmodels are utilized, which enables SED models to employ unseen semantic\ncontexts of scenes in inference stages. Moreover, we investigated the extent to\nwhich the semantic representation of scene contexts is useful for SED.\nExperimental results performed with TUT Sound Events 2016/2017 and TUT Acoustic\nScenes 2016/2017 datasets show that the proposed method improves micro and\nmacro F-scores by 4.34 and 3.13 percentage points compared with conventional\nConformer- and CNN--BiGRU-based SED, respectively.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Noriyuki Tonami",
      "Keisuke Imoto",
      "Ryotaro Nagase",
      "Yuki Okamoto",
      "Takahiro Fukumori",
      "Yoichi Yamashita"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03243"
  },
  {
    "id": "arXiv:2110.03244",
    "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with  Plug-in Solver",
    "abstract": "Although model-based reinforcement learning (RL) approaches are considered\nmore sample efficient, existing algorithms are usually relying on sophisticated\nplanning algorithm to couple tightly with the model-learning procedure. Hence\nthe learned models may lack the ability of being re-used with more specialized\nplanners. In this paper we address this issue and provide approaches to learn\nan RL model efficiently without the guidance of a reward signal. In particular,\nwe take a plug-in solver approach, where we focus on learning a model in the\nexploration phase and demand that \\emph{any planning algorithm} on the learned\nmodel can give a near-optimal policy. Specicially, we focus on the linear\nmixture MDP setting, where the probability transition matrix is a (unknown)\nconvex combination of a set of existing models. We show that, by establishing a\nnovel exploration algorithm, the plug-in approach learns a model by taking\n$\\tilde{O}(d^2H^3/\\epsilon^2)$ interactions with the environment and \\emph{any}\n$\\epsilon$-optimal planner on the model gives an $O(\\epsilon)$-optimal policy\non the original model. This sample complexity matches lower bounds for\nnon-plug-in approaches and is \\emph{statistically optimal}. We achieve this\nresult by leveraging a careful maximum total-variance bound using Bernstein\ninequality and properties specified to linear mixture MDP.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Chen",
      "Jiachen Hu",
      "Lin F. Yang",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03244"
  },
  {
    "id": "arXiv:2110.03246",
    "title": "Unprovability results for clause set cycles",
    "abstract": "The notion of clause set cycle abstracts a family of methods for automated\ninductive theorem proving based on the detection of cyclic dependencies between\nclause sets. By discerning the underlying logical features of clause set\ncycles, we are able to characterize clause set cycles by a logical theory. We\nmake use of this characterization to provide practically relevant unprovability\nresults for clause set cycles that exploit different logical features.",
    "descriptor": "",
    "authors": [
      "Stefan Hetzl",
      "Jannik Vierling"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.03246"
  },
  {
    "id": "arXiv:2110.03249",
    "title": "Colored Point Cloud to Image Alignment",
    "abstract": "Recognition and segmentation of objects in images enjoy the wealth of large\nvolume of well annotated data. At the other end, when dealing with the\nreconstruction of geometric structures of objects from images, there is a\nlimited amount of accurate data available for supervised learning. One type of\nsuch geometric data with insufficient amount required for deep learning is real\nworld accurate RGB-D images. The lack of accurate RGB-D datasets is one of the\nobstacles in the evolution of geometric scene reconstructions from images. One\nsolution to creating such a dataset is to capture RGB images while\nsimultaneously using an accurate depth scanning device that assigns a depth\nvalue to each pixel. A major challenge in acquiring such ground truth data is\nthe accurate alignment between the RGB images and the measured depth and color\nprofiles. We introduce a differential optimization method that aligns a colored\npoint cloud to a given color image via iterative geometric and color matching.\nThe proposed method enables the construction of RGB-D datasets for specific\ncamera systems. In the suggested framework, the optimization minimizes the\ndifference between the colors of the image pixels and the corresponding colors\nof the projected points to the camera plane. We assume that the colors produced\nby the geometric scanner camera and the color camera sensor are different and\nthus are characterized by different chromatic acquisition properties. We align\nthe different color spaces while compensating for their corresponding color\nappearance. Under this setup, we find the transformation between the camera\nimage and the point cloud colors by iterating between matching the relative\nlocation of the point cloud and matching colors. The successful alignments\nproduced by the proposed method are demonstrated on both synthetic data with\nquantitative evaluation and real world scenes with qualitative results.",
    "descriptor": "",
    "authors": [
      "Noam Rotstein",
      "Amit Bracha",
      "Ron Kimmel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03249"
  },
  {
    "id": "arXiv:2110.03250",
    "title": "Minimum Message Length Autoregressive Moving Average Model Order  Selection",
    "abstract": "This paper derives a Minimum Message Length (MML) criterion for the model\nselection of the Autoregressive Moving Average (ARMA) time series model. The\nMML87 performances on the ARMA model compared with other well known model\nselection criteria, Akaike Information Criterion (AIC), Corrected AIC (AICc),\nBayesian Information Criterion (BIC), and Hannan Quinn (HQ). The experimental\nresults show that the MML87 is outperformed the other model selection criteria\nas it select most of the models with lower prediction errors and the models\nselected by MML87 to have a lower mean squared error in different in-sample and\nout-sample sizes.",
    "descriptor": "",
    "authors": [
      "Zheng Fang",
      "David L. Dowe",
      "Shelton Peiris",
      "Dedi Rosadi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.03250"
  },
  {
    "id": "arXiv:2110.03251",
    "title": "A Cough-based deep learning framework for detecting COVID-19",
    "abstract": "In this paper, we propose a deep learning-based framework for detecting\nCOVID-19 positive subjects from their cough sounds. In particular, the proposed\nframework comprises two main steps. In the first step, we generate a feature\nrepresenting the cough sound by combining embedding features extracted from a\npre-trained model and handcrafted features, referred to as the front-end\nfeature extraction. Then, the combined features are fed into different back-end\nclassification models for detecting COVID-19 positive subjects. The\nexperimental results on the Second 2021 DiCOVA Challenge - Track 2 dataset\nachieve the top-3 ranking with an AUC score of 81.21 on the blind Test set,\nimproving the challenge baseline by 6.32 and showing competitive with the\nstate-of-the-art systems.",
    "descriptor": "\nComments: ICASSP-2022\n",
    "authors": [
      "Hoang Van Truong",
      "Lam Pham"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03251"
  },
  {
    "id": "arXiv:2110.03252",
    "title": "Layer-wise Pruning of Transformer Attention Heads for Efficient Language  Modeling",
    "abstract": "While Transformer-based models have shown impressive language modeling\nperformance, the large computation cost is often prohibitive for practical use.\nAttention head pruning, which removes unnecessary attention heads in the\nmultihead attention, is a promising technique to solve this problem. However,\nit does not evenly reduce the overall load because the heavy feedforward module\nis not affected by head pruning. In this paper, we apply layer-wise attention\nhead pruning on All-attention Transformer so that the entire computation and\nthe number of parameters can be reduced proportionally to the number of pruned\nheads. While the architecture has the potential to fully utilize head pruning,\nwe propose three training methods that are especially helpful to minimize\nperformance degradation and stabilize the pruning process. Our pruned model\nshows consistently lower perplexity within a comparable parameter size than\nTransformer-XL on WikiText-103 language modeling benchmark.",
    "descriptor": "",
    "authors": [
      "Kyuhong Shim",
      "Iksoo Choi",
      "Wonyong Sung",
      "Jungwook Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03252"
  },
  {
    "id": "arXiv:2110.03254",
    "title": "Palindromic linearization and numerical solution of nonsymmetric  algebraic T-Riccati equations",
    "abstract": "We identify a relationship between the solutions of a nonsymmetric algebraic\nT-Riccati equation (T-NARE) and the deflating subspaces of a palindromic matrix\npencil, obtained by arranging the coefficients of the T-NARE. The interplay\nbetween T-NARE and palindromic pencils allows one to derive both theoretical\nproperties of the solutions of the equation, and new methods for its numerical\nsolution. In particular, we propose methods based on the (palindromic) QZ\nalgorithm and the doubling algorithm, whose effectiveness is demonstrated by\nseveral numerical tests",
    "descriptor": "",
    "authors": [
      "Peter Benner",
      "Bruno Iannazzo",
      "Beatrice Meini",
      "Davide Palitta"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03254"
  },
  {
    "id": "arXiv:2110.03260",
    "title": "Improving MC-Dropout Uncertainty Estimates with Calibration Error-based  Optimization",
    "abstract": "Uncertainty quantification of machine learning and deep learning methods\nplays an important role in enhancing trust to the obtained result. In recent\nyears, a numerous number of uncertainty quantification methods have been\nintroduced. Monte Carlo dropout (MC-Dropout) is one of the most well-known\ntechniques to quantify uncertainty in deep learning methods. In this study, we\npropose two new loss functions by combining cross entropy with Expected\nCalibration Error (ECE) and Predictive Entropy (PE). The obtained results\nclearly show that the new proposed loss functions lead to having a calibrated\nMC-Dropout method. Our results confirmed the great impact of the new hybrid\nloss functions for minimising the overlap between the distributions of\nuncertainty estimates for correct and incorrect predictions without sacrificing\nthe model's overall performance.",
    "descriptor": "\nComments: 11 pages, 6 figures, 2 tables\n",
    "authors": [
      "Afshar Shamsi",
      "Hamzeh Asgharnezhad",
      "Moloud Abdar",
      "AmirReza Tajally",
      "Abbas Khosravi",
      "Saeid Nahavandi",
      "Henry Leung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03260"
  },
  {
    "id": "arXiv:2110.03262",
    "title": "Situated Dialogue Learning through Procedural Environment Generation",
    "abstract": "We teach goal-driven agents to interactively act and speak in situated\nenvironments by training on generated curriculums. Our agents operate in LIGHT\n(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure\ngame wherein an agent perceives and interacts with the world through textual\nnatural language. Goals in this environment take the form of character-based\nquests, consisting of personas and motivations. We augment LIGHT by learning to\nprocedurally generate additional novel textual worlds and quests to create a\ncurriculum of steadily increasing difficulty for training agents to achieve\nsuch goals. In particular, we measure curriculum difficulty in terms of the\nrarity of the quest in the original training distribution -- an easier\nenvironment is one that is more likely to have been found in the unaugmented\ndataset. An ablation study shows that this method of learning from the tail of\na distribution results in significantly higher generalization abilities as\nmeasured by zero-shot performance on never-before-seen quests.",
    "descriptor": "\nComments: Preprint. Under Review\n",
    "authors": [
      "Prithviraj Ammanabrolu",
      "Renee Jia",
      "Mark O. Riedl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03262"
  },
  {
    "id": "arXiv:2110.03266",
    "title": "Lagrangian Neural Network with Differential Symmetries and Relational  Inductive Bias",
    "abstract": "Realistic models of physical world rely on differentiable symmetries that, in\nturn, correspond to conservation laws. Recent works on Lagrangian and\nHamiltonian neural networks show that the underlying symmetries of a system can\nbe easily learned by a neural network when provided with an appropriate\ninductive bias. However, these models still suffer from issues such as\ninability to generalize to arbitrary system sizes, poor interpretability, and\nmost importantly, inability to learn translational and rotational symmetries,\nwhich lead to the conservation laws of linear and angular momentum,\nrespectively. Here, we present a momentum conserving Lagrangian neural network\n(MCLNN) that learns the Lagrangian of a system, while also preserving the\ntranslational and rotational symmetries. We test our approach on linear and\nnon-linear spring systems, and a gravitational system, demonstrating the energy\nand momentum conservation. We also show that the model developed can generalize\nto systems of any arbitrary size. Finally, we discuss the interpretability of\nthe MCLNN, which directly provides physical insights into the interactions of\nmulti-particle systems.",
    "descriptor": "",
    "authors": [
      "Ravinder Bhattoo",
      "Sayan Ranu",
      "N. M. Anoop Krishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Artificial Intelligence (cs.AI)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03266"
  },
  {
    "id": "arXiv:2110.03267",
    "title": "Propagating State Uncertainty Through Trajectory Forecasting",
    "abstract": "Uncertainty pervades through the modern robotic autonomy stack, with nearly\nevery component (e.g., sensors, detection, classification, tracking, behavior\nprediction) producing continuous or discrete probabilistic distributions.\nTrajectory forecasting, in particular, is surrounded by uncertainty as its\ninputs are produced by (noisy) upstream perception and its outputs are\npredictions that are often probabilistic for use in downstream planning.\nHowever, most trajectory forecasting methods do not account for upstream\nuncertainty, instead taking only the most-likely values. As a result,\nperceptual uncertainties are not propagated through forecasting and predictions\nare frequently overconfident. To address this, we present a novel method for\nincorporating perceptual state uncertainty in trajectory forecasting, a key\ncomponent of which is a new statistical distance-based loss function which\nencourages predicting uncertainties that better match upstream perception. We\nevaluate our approach both in illustrative simulations and on large-scale,\nreal-world data, demonstrating its efficacy in propagating perceptual state\nuncertainty through prediction and producing more calibrated predictions.",
    "descriptor": "\nComments: 8 pages, 6 figures, 4 tables\n",
    "authors": [
      "Boris Ivanovic",
      "Yifeng",
      "Shubham Shrivastava",
      "Punarjay Chakravarty",
      "Marco Pavone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03267"
  },
  {
    "id": "arXiv:2110.03269",
    "title": "Multi-tasking Dialogue Comprehension with Discourse Parsing",
    "abstract": "Multi-party dialogue machine reading comprehension (MRC) raises an even more\nchallenging understanding goal on dialogue with more than two involved\nspeakers, compared with the traditional plain passage style MRC. To accurately\nperform the question-answering (QA) task according to such multi-party\ndialogue, models have to handle fundamentally different discourse relationships\nfrom common non-dialogue plain text, where discourse relations are supposed to\nconnect two far apart utterances in a linguistics-motivated way.To further\nexplore the role of such unusual discourse structure on the correlated QA task\nin terms of MRC, we propose the first multi-task model for jointly performing\nQA and discourse parsing (DP) on the multi-party dialogue MRC task. Our\nproposed model is evaluated on the latest benchmark Molweni, whose results\nindicate that training with complementary tasks indeed benefits not only QA\ntask, but also DP task itself. We further find that the joint model is\ndistinctly stronger when handling longer dialogues which again verifies the\nnecessity of DP in the related MRC.",
    "descriptor": "\nComments: Accepted by PACLIC 2021\n",
    "authors": [
      "Yuchen He",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03269"
  },
  {
    "id": "arXiv:2110.03270",
    "title": "Injecting Planning-Awareness into Prediction and Detection Evaluation",
    "abstract": "Detecting other agents and forecasting their behavior is an integral part of\nthe modern robotic autonomy stack, especially in safety-critical scenarios\nentailing human-robot interaction such as autonomous driving. Due to the\nimportance of these components, there has been a significant amount of interest\nand research in perception and trajectory forecasting, resulting in a wide\nvariety of approaches. Common to most works, however, is the use of the same\nfew accuracy-based evaluation metrics, e.g., intersection-over-union,\ndisplacement error, log-likelihood, etc. While these metrics are informative,\nthey are task-agnostic and outputs that are evaluated as equal can lead to\nvastly different outcomes in downstream planning and decision making. In this\nwork, we take a step back and critically assess current evaluation metrics,\nproposing task-aware metrics as a better measure of performance in systems\nwhere they are deployed. Experiments on an illustrative simulation as well as\nreal-world autonomous driving data validate that our proposed task-aware\nmetrics are able to account for outcome asymmetry and provide a better estimate\nof a model's closed-loop performance.",
    "descriptor": "\nComments: 8 pages, 9 figures. arXiv admin note: substantial text overlap with arXiv:2107.10297\n",
    "authors": [
      "Boris Ivanovic",
      "Marco Pavone"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03270"
  },
  {
    "id": "arXiv:2110.03272",
    "title": "The Source Model Towards Maximizing The Output Signal-To-Interference  Ratio For Independent Vector Analysis",
    "abstract": "In this paper, the optimal source model for the independent vector analysis\n(IVA) algorithm towards maximizing the output signal-to-interference ratio\n(SIR) is mathematically derived, and the corresponding optimal weighted\ncovariance matrix is proved to be the covariance matrix of interference\nsignals. A new algorithm framework called minimum variance IVA (MVIVA) is\nfurther proposed, where the deep neural network-based estimation of the\ninterference covariance matrix is combined with the IVA-based estimation of the\ndemixing matrix. Experimental results show the superiority of the proposed\nsource model, and the MVIVA algorithm outperforms the original IVA algorithm by\n9.6 dB in SIR and 5.8 dB in signal-to-distortion ration (SDR) on average.",
    "descriptor": "\nComments: 5 pages, 1 figures, submitted to ICASSP2022\n",
    "authors": [
      "Jianju Gu",
      "Longbiao Cheng",
      "Junfeng Li",
      "Yonghong Yan"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03272"
  },
  {
    "id": "arXiv:2110.03273",
    "title": "AgFlow: Fast Model Selection of Penalized PCA via Implicit  Regularization Effects of Gradient Flow",
    "abstract": "Principal component analysis (PCA) has been widely used as an effective\ntechnique for feature extraction and dimension reduction. In the High Dimension\nLow Sample Size (HDLSS) setting, one may prefer modified principal components,\nwith penalized loadings, and automated penalty selection by implementing model\nselection among these different models with varying penalties. The earlier work\n[1, 2] has proposed penalized PCA, indicating the feasibility of model\nselection in $L_2$- penalized PCA through the solution path of Ridge\nregression, however, it is extremely time-consuming because of the intensive\ncalculation of matrix inverse. In this paper, we propose a fast model selection\nmethod for penalized PCA, named Approximated Gradient Flow (AgFlow), which\nlowers the computation complexity through incorporating the implicit\nregularization effect introduced by (stochastic) gradient flow [3, 4] and\nobtains the complete solution path of $L_2$-penalized PCA under varying\n$L_2$-regularization. We perform extensive experiments on real-world datasets.\nAgFlow outperforms existing methods (Oja [5], Power [6], and Shamir [7] and the\nvanilla Ridge estimators) in terms of computation costs.",
    "descriptor": "\nComments: accepted by Machine Learning\n",
    "authors": [
      "Haiyan Jiang",
      "Haoyi Xiong",
      "Dongrui Wu",
      "Ji Liu",
      "Dejing Dou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03273"
  },
  {
    "id": "arXiv:2110.03275",
    "title": "Doing Data Right: How Lessons Learned Working with Conventional Data  should Inform the Future of Synthetic Data for Recommender Systems",
    "abstract": "We present a case that the newly emerging field of synthetic data in the area\nof recommender systems should prioritize `doing data right'. We consider this\ncatchphrase to have two aspects: First, we should not repeat the mistakes of\nthe past, and, second, we should explore the full scope of opportunities\npresented by synthetic data as we move into the future. We argue that explicit\nattention to dataset design and description will help to avoid past mistakes\nwith dataset bias and evaluation. In order to fully exploit the opportunities\nof synthetic data, we point out that researchers can investigate new areas such\nas using data synthesize to support reproducibility by making data open, as\nwell as FAIR, and to push forward our understanding of data minimization.",
    "descriptor": "\nComments: Contribution to the SimuRec Workshop at RecSys 2021\n",
    "authors": [
      "Manel Slokom",
      "Martha Larson"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.03275"
  },
  {
    "id": "arXiv:2110.03276",
    "title": "Inferring Substitutable and Complementary Products with Knowledge-Aware  Path Reasoning based on Dynamic Policy Network",
    "abstract": "Inferring the substitutable and complementary products for a given product is\nan essential and fundamental concern for the recommender system. To achieve\nthis, existing approaches take advantage of the knowledge graphs to learn more\nevidences for inference, whereas they often suffer from invalid reasoning for\nlack of elegant decision making strategies. Therefore, we propose a novel\nKnowledge-Aware Path Reasoning (KAPR) model which leverages the dynamic policy\nnetwork to make explicit reasoning over knowledge graphs, for inferring the\nsubstitutable and complementary relationships. Our contributions can be\nhighlighted as three aspects. Firstly, we model this inference scenario as a\nMarkov Decision Process in order to accomplish a knowledge-aware path reasoning\nover knowledge graphs. Secondly,we integrate both structured and unstructured\nknowledge to provide adequate evidences for making accurate decision-making.\nThirdly, we evaluate our model on a series of real-world datasets, achieving\ncompetitive performance compared with state-of-the-art approaches. Our code is\nreleased on https://gitee.com/yangzijing flower/kapr/tree/master.",
    "descriptor": "",
    "authors": [
      "Zijing Yang",
      "Jiabo Ye",
      "Linlin Wang",
      "Xin Lin",
      "Liang He"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03276"
  },
  {
    "id": "arXiv:2110.03278",
    "title": "Virtual Multi-Modality Self-Supervised Foreground Matting for  Human-Object Interaction",
    "abstract": "Most existing human matting algorithms tried to separate pure human-only\nforeground from the background. In this paper, we propose a Virtual\nMulti-modality Foreground Matting (VMFM) method to learn human-object\ninteractive foreground (human and objects interacted with him or her) from a\nraw RGB image. The VMFM method requires no additional inputs, e.g. trimap or\nknown background. We reformulate foreground matting as a self-supervised\nmulti-modality problem: factor each input image into estimated depth map,\nsegmentation mask, and interaction heatmap using three auto-encoders. In order\nto fully utilize the characteristics of each modality, we first train a dual\nencoder-to-decoder network to estimate the same alpha matte. Then we introduce\na self-supervised method: Complementary Learning(CL) to predict deviation\nprobability map and exchange reliable gradients across modalities without\nlabel. We conducted extensive experiments to analyze the effectiveness of each\nmodality and the significance of different components in complementary\nlearning. We demonstrate that our model outperforms the state-of-the-art\nmethods.",
    "descriptor": "\nComments: Accepted by ICCV2021\n",
    "authors": [
      "Bo Xu",
      "Han Huang",
      "Cheng Lu",
      "Ziwen Li",
      "Yandong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03278"
  },
  {
    "id": "arXiv:2110.03279",
    "title": "Polynomial Turing Kernels for Clique with an Optimal Number of Queries",
    "abstract": "A polynomial Turing kernel for some parameterized problem $P$ is a\npolynomial-time algorithm that solves $P$ using queries to an oracle of $P$\nwhose sizes are upper-bounded by some polynomial in the parameter. Here the\nterm \"polynomial\" refers to the bound on the query sizes, as the running time\nof any kernel is required to be polynomial. One of the most important open\ngoals in parameterized complexity is to understand the applicability and\nlimitations of polynomial Turing Kernels. As any fixed-parameter tractable\nproblem admits a Turing kernel of some size, the focus has mostly being on\ndetermining which problems admit such kernels whose query sizes can be indeed\nbounded by some polynomial.\nIn this paper we take a different approach, and instead focus on the number\nof queries that a Turing kernel uses, assuming it is restricted to using only\npolynomial sized queries. Our study focuses on one the main problems studied in\nparameterized complexity, the Clique problem: Given a graph $G$ and an integer\n$k$, determine whether there are $k$ pairwise adjacent vertices in $G$. We show\nthat Clique parameterized by several structural parameters exhibits the\nfollowing phenomena:\n- It admits polynomial Turing kernels which use a sublinear number of\nqueries, namely $O(n/\\log^c n)$ queries where $n$ is the total size of the\ngraph and $c$ is any constant. This holds even for a very restrictive type of\nTuring kernels which we call OR-kernels.\n- It does not admit polynomial Turing kernels which use $O(n^{1-\\epsilon})$\nqueries, unless NP$\\subseteq$coNP/poly.\nFor proving the second item above, we develop a new framework for bounding\nthe number of queries needed by polynomial Turing kernels. This framework is\ninspired by the standard lower bounds framework for Karp kernels, and while it\nis quite similar, it still requires some novel ideas to allow its extension to\nthe Turing setting.",
    "descriptor": "",
    "authors": [
      "Till Fluschnik",
      "Klaus Heeger",
      "Danny Hermelin"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03279"
  },
  {
    "id": "arXiv:2110.03281",
    "title": "Detecting Autism Spectrum Disorders with Machine Learning Models Using  Speech Transcripts",
    "abstract": "Autism spectrum disorder (ASD) can be defined as a neurodevelopmental\ndisorder that affects how children interact, communicate and socialize with\nothers. This disorder can occur in a broad spectrum of symptoms, with varying\neffects and severity. While there is no permanent cure for ASD, early detection\nand proactive treatment can substantially improve the lives of many children.\nCurrent methods to accurately diagnose ASD are invasive, time-consuming, and\ntedious. They can also be subjective perspectives of a number of clinicians\ninvolved, including pediatricians, speech pathologists, psychologists, and\npsychiatrists. New technologies are rapidly emerging that include machine\nlearning models using speech, computer vision from facial, retinal, and brain\nMRI images of patients to accurately and timely detect this disorder. Our\nresearch focuses on computational linguistics and machine learning using speech\ndata from TalkBank, the world's largest spoken language database. We used data\nof both ASD and Typical Development (TD) in children from TalkBank to develop\nmachine learning models to accurately predict ASD. More than 50 features were\nused from specifically two datasets in TalkBank to run our experiments using\nfive different classifiers. Logistic Regression and Random Forest models were\nfound to be the most effective for each of these two main datasets, with an\naccuracy of 0.75. These experiments confirm that while significant\nopportunities exist for improving the accuracy, machine learning models can\nreliably predict ASD status in children for effective diagnosis.",
    "descriptor": "",
    "authors": [
      "Vikram Ramesh",
      "Rida Assaf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03281"
  },
  {
    "id": "arXiv:2110.03290",
    "title": "MC-LCR: Multi-modal contrastive classification by locally correlated  representations for effective face forgery detection",
    "abstract": "As the remarkable development of facial manipulation technologies is\naccompanied by severe security concerns, face forgery detection has become a\nrecent research hotspot. Most existing detection methods train a binary\nclassifier under global supervision to judge real or fake. However, advanced\nmanipulations only perform small-scale tampering, posing challenges to\ncomprehensively capture subtle and local forgery artifacts, especially in high\ncompression settings and cross-dataset scenarios. To address such limitations,\nwe propose a novel framework named Multi-modal Contrastive Classification by\nLocally Correlated Representations(MC-LCR), for effective face forgery\ndetection. Instead of specific appearance features, our MC-LCR aims to amplify\nimplicit local discrepancies between authentic and forged faces from both\nspatial and frequency domains. Specifically, we design the shallow style\nrepresentation block that measures the pairwise correlation of shallow feature\nmaps, which encodes local style information to extract more discriminative\nfeatures in the spatial domain. Moreover, we make a key observation that subtle\nforgery artifacts can be further exposed in the patch-wise phase and amplitude\nspectrum and exhibit different clues. According to the complementarity of\namplitude and phase information, we develop a patch-wise amplitude and phase\ndual attention module to capture locally correlated inconsistencies with each\nother in the frequency domain. Besides the above two modules, we further\nintroduce the collaboration of supervised contrastive loss with cross-entropy\nloss. It helps the network learn more discriminative and generalized\nrepresentations. Through extensive experiments and comprehensive studies, we\nachieve state-of-the-art performance and demonstrate the robustness and\ngeneralization of our method.",
    "descriptor": "",
    "authors": [
      "Gaojian Wang",
      "Qian Jiang",
      "Xin Jin",
      "Wei Li",
      "Xiaohui Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03290"
  },
  {
    "id": "arXiv:2110.03292",
    "title": "Robotic Lever Manipulation using Hindsight Experience Replay and Shapley  Additive Explanations",
    "abstract": "This paper deals with robotic lever control using Explainable Deep\nReinforcement Learning. First, we train a policy by using the Deep\nDeterministic Policy Gradient algorithm and the Hindsight Experience Replay\ntechnique, where the goal is to control a robotic manipulator to manipulate a\nlever. This enables us both to use continuous states and actions and to learn\nwith sparse rewards. Being able to learn from sparse rewards is especially\ndesirable for Deep Reinforcement Learning because designing a reward function\nfor complex tasks such as this is challenging. We first train in the PyBullet\nsimulator, which accelerates the training procedure, but is not accurate on\nthis task compared to the real-world environment. After completing the training\nin PyBullet, we further train in the Gazebo simulator, which runs more slowly\nthan PyBullet, but is more accurate on this task. We then transfer the policy\nto the real-world environment, where it achieves comparable performance to the\nsimulated environments for most episodes. To explain the decisions of the\npolicy we use the SHAP method to create an explanation model based on the\nepisodes done in the real-world environment. This gives us some results that\nagree with intuition, and some that do not. We also question whether the\nindependence assumption made when approximating the SHAP values influences the\naccuracy of these values for a system such as this, where there are some\ncorrelations between the states.",
    "descriptor": "\nComments: Published in proceedings of the European Control Conference 2021\n",
    "authors": [
      "Sindre Benjamin Remman",
      "Anastasios M. Lekkas"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03292"
  },
  {
    "id": "arXiv:2110.03294",
    "title": "EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern  Error Feedback",
    "abstract": "First proposed by Seide (2014) as a heuristic, error feedback (EF) is a very\npopular mechanism for enforcing convergence of distributed gradient-based\noptimization methods enhanced with communication compression strategies based\non the application of contractive compression operators. However, existing\ntheory of EF relies on very strong assumptions (e.g., bounded gradients), and\nprovides pessimistic convergence rates (e.g., while the best known rate for EF\nin the smooth nonconvex regime, and when full gradients are compressed, is\n$O(1/T^{2/3})$, the rate of gradient descent in the same regime is $O(1/T)$).\nRecently, Richt\\'{a}rik et al. (2021) proposed a new error feedback mechanism,\nEF21, based on the construction of a Markov compressor induced by a contractive\ncompressor. EF21 removes the aforementioned theoretical deficiencies of EF and\nat the same time works better in practice. In this work we propose six\npractical extensions of EF21, all supported by strong convergence theory:\npartial participation, stochastic approximation, variance reduction, proximal\nsetting, momentum and bidirectional compression. Several of these techniques\nwere never analyzed in conjunction with EF before, and in cases where they were\n(e.g., bidirectional compression), our rates are vastly superior.",
    "descriptor": "\nComments: 71 pages, 7 algorithms, 18 Lemmas, 13 Theorems, 18 Corollaries, 6 Tables, 7 Figures\n",
    "authors": [
      "Ilyas Fatkhullin",
      "Igor Sokolov",
      "Eduard Gorbunov",
      "Zhize Li",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03294"
  },
  {
    "id": "arXiv:2110.03296",
    "title": "Ranking Warnings of Static Analysis Tools Using Representation Learning",
    "abstract": "Static analysis tools are frequently used to detect potential vulnerabilities\nin software systems. However, an inevitable problem of these tools is their\nlarge number of warnings with a high false positive rate, which consumes time\nand effort for investigating. In this paper, we present DeFP, a novel method\nfor ranking static analysis warnings. Based on the intuition that warnings\nwhich have similar contexts tend to have similar labels (true positive or false\npositive), DeFP is built with two BiLSTM models to capture the patterns\nassociated with the contexts of labeled warnings. After that, for a set of new\nwarnings, DeFP can calculate and rank them according to their likelihoods to be\ntrue positives (i.e., actual vulnerabilities). Our experimental results on a\ndataset of 10 real-world projects show that using DeFP, by investigating only\n60% of the warnings, developers can find +90% of actual vulnerabilities.\nMoreover, DeFP improves the state-of-the-art approach 30% in both Precision and\nRecall.",
    "descriptor": "\nComments: Published in Proceedings of the 28th Asia-Pacific Software Engineering Conference (APSEC'21)\n",
    "authors": [
      "Kien-Tuan Ngo",
      "Dinh-Truong Do",
      "Thu-Trang Nguyen",
      "Hieu Dinh Vo"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2110.03296"
  },
  {
    "id": "arXiv:2110.03298",
    "title": "End-to-End Supermask Pruning: Learning to Prune Image Captioning Models",
    "abstract": "With the advancement of deep models, research work on image captioning has\nled to a remarkable gain in raw performance over the last decade, along with\nincreasing model complexity and computational cost. However, surprisingly works\non compression of deep networks for image captioning task has received little\nto no attention. For the first time in image captioning research, we provide an\nextensive comparison of various unstructured weight pruning methods on three\ndifferent popular image captioning architectures, namely Soft-Attention,\nUp-Down and Object Relation Transformer. Following this, we propose a novel\nend-to-end weight pruning method that performs gradual sparsification based on\nweight sensitivity to the training loss. The pruning schemes are then extended\nwith encoder pruning, where we show that conducting both decoder pruning and\ntraining simultaneously prior to the encoder pruning provides good overall\nperformance. Empirically, we show that an 80% to 95% sparse network (up to 75%\nreduction in model size) can either match or outperform its dense counterpart.\nThe code and pre-trained models for Up-Down and Object Relation Transformer\nthat are capable of achieving CIDEr scores >120 on the MS-COCO dataset but with\nonly 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94%\nrespectively against dense versions) are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.",
    "descriptor": "\nComments: Pattern Recognition; In Press\n",
    "authors": [
      "Jia Huei Tan",
      "Chee Seng Chan",
      "Joon Huang Chuah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03298"
  },
  {
    "id": "arXiv:2110.03300",
    "title": "Permutation Compressors for Provably Faster Distributed Nonconvex  Optimization",
    "abstract": "We study the MARINA method of Gorbunov et al (2021) -- the current\nstate-of-the-art distributed non-convex optimization method in terms of\ntheoretical communication complexity. Theoretical superiority of this method\ncan be largely attributed to two sources: the use of a carefully engineered\nbiased stochastic gradient estimator, which leads to a reduction in the number\nof communication rounds, and the reliance on {\\em independent} stochastic\ncommunication compression operators, which leads to a reduction in the number\nof transmitted bits within each communication round. In this paper we i) extend\nthe theory of MARINA to support a much wider class of potentially {\\em\ncorrelated} compressors, extending the reach of the method beyond the classical\nindependent compressors setting, ii) show that a new quantity, for which we\ncoin the name {\\em Hessian variance}, allows us to significantly refine the\noriginal analysis of MARINA without any additional assumptions, and iii)\nidentify a special class of correlated compressors based on the idea of {\\em\nrandom permutations}, for which we coin the term Perm$K$, the use of which\nleads to $O(\\sqrt{n})$ (resp. $O(1 + d/\\sqrt{n})$) improvement in the\ntheoretical communication complexity of MARINA in the low Hessian variance\nregime when $d\\geq n$ (resp. $d \\leq n$), where $n$ is the number of workers\nand $d$ is the number of parameters describing the model we are learning. We\ncorroborate our theoretical results with carefully engineered synthetic\nexperiments with minimizing the average of nonconvex quadratics, and on\nautoencoder training with the MNIST dataset.",
    "descriptor": "\nComments: 53 pages\n",
    "authors": [
      "Rafa\u0142 Szlendak",
      "Alexander Tyurin",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03300"
  },
  {
    "id": "arXiv:2110.03301",
    "title": "EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box  Android Malware Detection",
    "abstract": "Over the last decade, several studies have investigated the weaknesses of\nAndroid malware detectors against adversarial examples by proposing novel\nevasion attacks; however, the practicality of most studies in manipulating\nreal-world malware is arguable. The majority of studies have assumed attackers\nknow the details of the target classifiers used for malware detection, while in\nreal life, malicious actors have limited access to the target classifiers. This\npaper presents a practical evasion attack, EvadeDroid, to circumvent black-box\nAndroid malware detectors. In addition to generating real-world adversarial\nmalware, the proposed evasion attack can preserve the functionality of the\noriginal malware samples. EvadeDroid applies a set of functionality-preserving\ntransformations to morph malware instances into benign ones using an iterative\nand incremental manipulation strategy. The proposed manipulation technique is a\nnovel, query-efficient optimization algorithm with the aim of finding and\ninjecting optimal sequences of transformations into malware samples. Our\nempirical evaluation demonstrates the efficacy of EvadeDroid under hard- and\nsoft-label attacks. Moreover, EvadeDroid is capable to generate practical\nadversarial examples with only a small number of queries, with evasion rate of\n81%, 73%, and 75% for DREBIN, Sec-SVM, and MaMaDroid, respectively. Finally, we\nshow that EvadeDroid is able to preserve its stealthiness against four popular\ncommercial antivirus, thus demonstrating its feasibility in the real world.",
    "descriptor": "",
    "authors": [
      "Hamid Bostani",
      "Veelasha Moonsamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03301"
  },
  {
    "id": "arXiv:2110.03302",
    "title": "MGPSN: Motion-Guided Pseudo Siamese Network for Indoor Video Head  Detection",
    "abstract": "Head detection in real-world videos is an important research topic in\ncomputer vision. However, existing studies face some challenges in complex\nscenes. The performance of head detectors deteriorates when objects which have\nsimilar head appearance exist for indoor videos. Moreover, heads have small\nscales and diverse poses, which increases the difficulty in detection. To\nhandle these issues, we propose Motion-Guided Pseudo Siamese Network for Indoor\nVideo Head Detection (MGPSN), an end-to-end model to learn the robust head\nmotion features. MGPSN integrates spatial-temporal information on pixel level,\nguiding the model to extract effective head features. Experiments show that\nMGPSN is able to suppress static objects and enhance motion instances. Compared\nwith previous methods, it achieves state-of-the-art performance on the crowd\nBrainwash dataset. Different backbone networks and detectors are evaluated to\nverify the flexibility and generality of MGPSN.",
    "descriptor": "",
    "authors": [
      "Kailai Sun",
      "Xiaoteng Ma",
      "Qianchuan Zhao",
      "Peng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03302"
  },
  {
    "id": "arXiv:2110.03303",
    "title": "Universal Approximation Under Constraints is Possible with Transformers",
    "abstract": "Many practical problems need the output of a machine learning model to\nsatisfy a set of constraints, $K$. Nevertheless, there is no known guarantee\nthat classical neural network architectures can exactly encode constraints\nwhile simultaneously achieving universality. We provide a quantitative\nconstrained universal approximation theorem which guarantees that for any\nnon-convex compact set $K$ and any continuous function\n$f:\\mathbb{R}^n\\rightarrow K$, there is a probabilistic transformer $\\hat{F}$\nwhose randomized outputs all lie in $K$ and whose expected output uniformly\napproximates $f$. Our second main result is a \"deep neural version\" of Berge's\nMaximum Theorem (1963). The result guarantees that given an objective function\n$L$, a constraint set $K$, and a family of soft constraint sets, there is a\nprobabilistic transformer $\\hat{F}$ that approximately minimizes $L$ and whose\noutputs belong to $K$; moreover, $\\hat{F}$ approximately satisfies the soft\nconstraints. Our results imply the first universal approximation theorem for\nclassical transformers with exact convex constraint satisfaction. They also\nyield that a chart-free universal approximation theorem for Riemannian\nmanifold-valued functions subject to suitable geodesically convex constraints.",
    "descriptor": "\nComments: 9.5 Pages + 14 Page Append + References, 3 Tables, 5 Figures\n",
    "authors": [
      "Anastasis Kratsios",
      "Behnoosh Zamanlooy",
      "Tianlin Liu",
      "Ivan Dokmani\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Functional Analysis (math.FA)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2110.03303"
  },
  {
    "id": "arXiv:2110.03313",
    "title": "Distributed Methods with Compressed Communication for Solving  Variational Inequalities, with Theoretical Guarantees",
    "abstract": "Variational inequalities in general and saddle point problems in particular\nare increasingly relevant in machine learning applications, including\nadversarial learning, GANs, transport and robust optimization. With increasing\ndata and problem sizes necessary to train high performing models across these\nand other applications, it is necessary to rely on parallel and distributed\ncomputing. However, in distributed training, communication among the compute\nnodes is a key bottleneck during training, and this problem is exacerbated for\nhigh dimensional and over-parameterized models models. Due to these\nconsiderations, it is important to equip existing methods with strategies that\nwould allow to reduce the volume of transmitted information during training\nwhile obtaining a model of comparable quality. In this paper, we present the\nfirst theoretically grounded distributed methods for solving variational\ninequalities and saddle point problems using compressed communication: MASHA1\nand MASHA2. Our theory and methods allow for the use of both unbiased (such as\nRand$k$; MASHA1) and contractive (such as Top$k$; MASHA2) compressors. We\nempirically validate our conclusions using two experimental setups: a standard\nbilinear min-max problem, and large-scale distributed adversarial training of\ntransformers.",
    "descriptor": "\nComments: 30 pages, 2 algorithms (MASHA 1 and MASHA2), 2 theorems\n",
    "authors": [
      "Aleksandr Beznosikov",
      "Peter Richt\u00e1rik",
      "Michael Diskin",
      "Max Ryabinin",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03313"
  },
  {
    "id": "arXiv:2110.03315",
    "title": "On Equivalence Checking for Orthocomplemented Bisemilattices in  Log-Linear Time",
    "abstract": "We present a quasilinear time algorithm to decide the word problem on a\nnatural algebraic structures we call orthocomplemented bisemilattices, a\nsubtheory of boolean algebra. We use as a base a variation of Hopcroft, Ullman\nand Aho algorithm for tree isomorphism which we combine with a term rewriting\nsystem to decide equivalence of two terms. We prove that the rewriting system\nis terminating and confluent and hence the existence of a normal form, and that\nour algorithm is computing it. We also discuss applications and present an\neffective implementation in Scala.",
    "descriptor": "",
    "authors": [
      "Simon Guilloud",
      "Viktor Kun\u010dak"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.03315"
  },
  {
    "id": "arXiv:2110.03316",
    "title": "Correct Me if I am Wrong: Interactive Learning for Robotic Manipulation",
    "abstract": "Learning to solve complex manipulation tasks from visual observations is a\ndominant challenge for real-world robot learning. Deep reinforcement learning\nalgorithms have recently demonstrated impressive results, although they still\nrequire an impractical amount of time-consuming trial-and-error iterations. In\nthis work, we consider the promising alternative paradigm of interactive\nlearning where a human teacher provides feedback to the policy during\nexecution, as opposed to imitation learning where a pre-collected dataset of\nperfect demonstrations is used. Our proposed CEILing (Corrective and Evaluative\nInteractive Learning) framework combines both corrective and evaluative\nfeedback from the teacher to train a stochastic policy in an asynchronous\nmanner, and employs a dedicated mechanism to trade off human corrections with\nthe robot's own experience. We present results obtained with our framework in\nextensive simulation and real-world experiments that demonstrate that CEILing\ncan effectively solve complex robot manipulation tasks directly from raw images\nin less than one hour of real-world training.",
    "descriptor": "\nComments: Video, code and models available at this http URL\n",
    "authors": [
      "Eugenio Chisari",
      "Tim Welschehold",
      "Joschka Boedecker",
      "Wolfram Burgard",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03316"
  },
  {
    "id": "arXiv:2110.03318",
    "title": "On the Latent Holes of VAEs for Text Generation",
    "abstract": "In this paper, we provide the first focused study on the discontinuities\n(aka. holes) in the latent space of Variational Auto-Encoders (VAEs), a\nphenomenon which has been shown to have a detrimental effect on model capacity.\nWhen investigating latent holes, existing works are exclusively centred around\nthe encoder network and they merely explore the existence of holes. We tackle\nthese limitations by proposing a highly efficient Tree-based Decoder-Centric\n(TDC) algorithm for latent hole identification, with a focal point on the text\ndomain. In contrast to past studies, our approach pays attention to the decoder\nnetwork, as a decoder has a direct impact on the model's output quality.\nFurthermore, we provide, for the first time, in-depth empirical analysis of the\nlatent hole phenomenon, investigating several important aspects such as how the\nholes impact VAE algorithms' performance on text generation, and how the holes\nare distributed in the latent space.",
    "descriptor": "",
    "authors": [
      "Ruizhe Li",
      "Xutan Peng",
      "Chenghua Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03318"
  },
  {
    "id": "arXiv:2110.03319",
    "title": "Towards Federated Learning-Enabled Visible Light Communication in 6G  Systems",
    "abstract": "Visible light communication (VLC) technology was introduced as a key enabler\nfor the next generation of wireless networks, mainly thanks to its simple and\nlow-cost implementation. However, several challenges prohibit the realization\nof the full potentials of VLC, namely, limited modulation bandwidth, ambient\nlight interference, optical diffuse reflection effects, devices non-linearity,\nand random receiver orientation. On the contrary, centralized machine learning\n(ML) techniques have demonstrated a significant potential in handling different\nchallenges relating to wireless communication systems. Specifically, it was\nshown that ML algorithms exhibit superior capabilities in handling complicated\nnetwork tasks, such as channel equalization, estimation and modeling, resources\nallocation, and opportunistic spectrum access control, to name a few.\nNevertheless, concerns pertaining to privacy and communication overhead when\nsharing raw data of the involved clients with a server constitute major\nbottlenecks in the implementation of centralized ML techniques. This has\nmotivated the emergence of a new distributed ML paradigm, namely federated\nlearning (FL), which can reduce the cost associated with transferring raw data,\nand preserve privacy by training ML models locally and collaboratively at the\nclients' side. Hence, it becomes evident that integrating FL into VLC networks\ncan provide ubiquitous and reliable implementation of VLC systems. With this\nmotivation, this is the first in-depth review in the literature on the\napplication of FL in VLC networks. To that end, besides the different\narchitectures and related characteristics of FL, we provide a thorough overview\non the main design aspects of FL based VLC systems. Finally, we also highlight\nsome potential future research directions of FL that are envisioned to\nsubstantially enhance the performance and robustness of VLC systems.",
    "descriptor": "",
    "authors": [
      "Shimaa Naser",
      "Lina Bariah",
      "Sami Muhaidat",
      "Mahmoud Al-Qutayri",
      "Ernesto Damiani",
      "Merouane Debbah",
      "Paschalis C. Sofotasios"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03319"
  },
  {
    "id": "arXiv:2110.03320",
    "title": "Automated Testing of AI Models",
    "abstract": "The last decade has seen tremendous progress in AI technology and\napplications. With such widespread adoption, ensuring the reliability of the AI\nmodels is crucial. In past, we took the first step of creating a testing\nframework called AITEST for metamorphic properties such as fairness, robustness\nproperties for tabular, time-series, and text classification models. In this\npaper, we extend the capability of the AITEST tool to include the testing\ntechniques for Image and Speech-to-text models along with interpretability\ntesting for tabular models. These novel extensions make AITEST a comprehensive\nframework for testing AI models.",
    "descriptor": "\nComments: 5 pages, 3 Figures, 4 Tables\n",
    "authors": [
      "Swagatam Haldar",
      "Deepak Vijaykeerthy",
      "Diptikalyan Saha"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03320"
  },
  {
    "id": "arXiv:2110.03323",
    "title": "A Logic-Based Framework for Natural Language Inference in Dutch",
    "abstract": "At its core, the system is powered by two ${\\lambda}$-calculi, used as\nsyntactic and semantic theories, respectively. Sentences are first converted to\nsyntactic proofs and terms of the linear ${\\lambda}$-calculus using a choice of\ntwo parsers: an Alpino-based pipeline, and Neural Proof Nets. The syntactic\nterms are then converted to semantic terms of the simply typed\n${\\lambda}$-calculus, via a set of hand designed type- and term-level\ntransformations. Pairs of semantic terms are then fed to an automated theorem\nprover for natural logic which reasons with them while using lexical relations\nfound in the Open Dutch WordNet. We evaluate the reasoning pipeline on the\nrecently created Dutch natural language inference dataset, and achieve\npromising results, remaining only within a $1.1-3.2{\\%}$ performance margin to\nstrong neural baselines. To the best of our knowledge, the reasoning pipeline\nis the first logic-based system for Dutch.",
    "descriptor": "\nComments: 18 pages plus references. Presented in Natural Logic Meets Machine Learning (NaLoMa II) workshop at the 14th International Conference on Computational Semantics (IWCS 2021). Presented in the 31st Meeting of Computational Linguistics in The Netherlands (CLIN31). Submitted for publication in Volume 11 of the CLIN Journal. Code available at this http URL\n",
    "authors": [
      "Lasha Abzianidze",
      "Konstantinos Kogkalidis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03323"
  },
  {
    "id": "arXiv:2110.03324",
    "title": "Structured Channel Covariance Estimation from Limited Samples for Large  Antenna Arrays",
    "abstract": "In massive MIMO systems, the knowledge of channel covariance matrix is\ncrucial for MMSE channel estimation in the uplink and plays an important role\nin several downlink multiuser beamforming schemes. Due to the large number of\nbase station antennas in massive MIMO, accurate covariance estimation is\nchallenging especially in the case where the number of samples is limited and\nthus comparable to the channel vector dimension. As a result, the standard\nsample covariance estimator yields high estimation error which may yield\nsignificant system performance degradation with respect to the ideal channel\nknowledge case. To address such covariance estimation problem, we propose a\nmethod based on a parametric representation of the channel angular scattering\nfunction. The proposed parametric representation includes a discrete specular\ncomponent which is addressed using the well-known MUltiple SIgnal\nClassification (MUSIC) method, and a diffuse scattering component, which is\nmodeled as the superposition of suitable dictionary functions. To obtain the\nrepresentation parameters we propose two methods, where the first solves a\nnon-negative least-squares problem and the second maximizes the likelihood\nfunction using expectation-maximization. Our simulation results show that the\nproposed methods outperform the state of the art with respect to various\nestimation quality metrics and different sample sizes.",
    "descriptor": "\nComments: 30 pages, 20 figures\n",
    "authors": [
      "Tianyu Yang",
      "Mahdi Barzegar Khalilsarai",
      "Saeid Haghighatshoar",
      "Giuseppe Caire"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03324"
  },
  {
    "id": "arXiv:2110.03326",
    "title": "Back from the future: bidirectional CTC decoding using future  information in speech recognition",
    "abstract": "In this paper, we propose a simple but effective method to decode the output\nof Connectionist Temporal Classifier (CTC) model using a bi-directional neural\nlanguage model. The bidirectional language model uses the future as well as the\npast information in order to predict the next output in the sequence. The\nproposed method based on bi-directional beam search takes advantage of the CTC\ngreedy decoding output to represent the noisy future information. Experiments\non the Librispeechdataset demonstrate the superiority of our proposed method\ncompared to baselines using unidirectional decoding. In particular, the boost\ninaccuracy is most apparent at the start of a sequence which is the most\nerroneous part for existing systems based on unidirectional decoding.",
    "descriptor": "\nComments: submitted to ICASSP 2022\n",
    "authors": [
      "Namkyu Jung",
      "Geonmin Kim",
      "Han-Gyu Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03326"
  },
  {
    "id": "arXiv:2110.03331",
    "title": "CLEVA-Compass: A Continual Learning EValuation Assessment Compass to  Promote Research Transparency and Comparability",
    "abstract": "What is the state of the art in continual machine learning? Although a\nnatural question for predominant static benchmarks, the notion to train systems\nin a lifelong manner entails a plethora of additional challenges with respect\nto set-up and evaluation. The latter have recently sparked a growing amount of\ncritiques on prominent algorithm-centric perspectives and evaluation protocols\nbeing too narrow, resulting in several attempts at constructing guidelines in\nfavor of specific desiderata or arguing against the validity of prevalent\nassumptions. In this work, we depart from this mindset and argue that the goal\nof a precise formulation of desiderata is an ill-posed one, as diverse\napplications may always warrant distinct scenarios. Instead, we introduce the\nContinual Learning EValuation Assessment Compass, CLEVA-Compass for short. The\ncompass provides the visual means to both identify how approaches are\npractically reported and how works can simultaneously be contextualized in the\nbroader literature landscape. In addition to promoting compact specification in\nthe spirit of recent replication trends, the CLEVA-Compass thus provides an\nintuitive chart to understand the priorities of individual systems, where they\nresemble each other, and what elements are missing towards a fair comparison.",
    "descriptor": "",
    "authors": [
      "Martin Mundt",
      "Steven Lang",
      "Quentin Delfosse",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03331"
  },
  {
    "id": "arXiv:2110.03336",
    "title": "Frame Averaging for Invariant and Equivariant Network Design",
    "abstract": "Many machine learning tasks involve learning functions that are known to be\ninvariant or equivariant to certain symmetries of the input data. However, it\nis often challenging to design neural network architectures that respect these\nsymmetries while being expressive and computationally efficient. For example,\nEuclidean motion invariant/equivariant graph or point cloud neural networks. We\nintroduce Frame Averaging (FA), a general purpose and systematic framework for\nadapting known (backbone) architectures to become invariant or equivariant to\nnew symmetry types. Our framework builds on the well known group averaging\noperator that guarantees invariance or equivariance but is intractable. In\ncontrast, we observe that for many important classes of symmetries, this\noperator can be replaced with an averaging operator over a small subset of the\ngroup elements, called a frame. We show that averaging over a frame guarantees\nexact invariance or equivariance while often being much simpler to compute than\naveraging over the entire group. Furthermore, we prove that FA-based models\nhave maximal expressive power in a broad setting and in general preserve the\nexpressive power of their backbone architectures. Using frame averaging, we\npropose a new class of universal Graph Neural Networks (GNNs), universal\nEuclidean motion invariant point cloud networks, and Euclidean motion invariant\nMessage Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on\nseveral applications including point cloud normal estimation, beyond $2$-WL\ngraph separation, and $n$-body dynamics prediction, achieving state-of-the-art\nresults in all of these benchmarks.",
    "descriptor": "",
    "authors": [
      "Omri Puny",
      "Matan Atzmon",
      "Heli Ben-Hamu",
      "Edward J. Smith",
      "Ishan Misra",
      "Aditya Grover",
      "Yaron Lipman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03336"
  },
  {
    "id": "arXiv:2110.03337",
    "title": "Moment evolution equations and moment matching for stochastic image  EPDiff",
    "abstract": "Models of stochastic image deformation allow study of time-continuous\nstochastic effects transforming images by deforming the image domain.\nApplications include longitudinal medical image analysis with both population\ntrends and random subject specific variation. Focusing on a stochastic\nextension of the LDDMM models with evolutions governed by a stochastic EPDiff\nequation, we use moment approximations of the corresponding Ito diffusion to\nconstruct estimators for statistical inference in the full stochastic model. We\nshow that this approach, when efficiently implemented with automatic\ndifferentiation tools, can successfully estimate parameters encoding the\nspatial correlation of the noise fields on the image",
    "descriptor": "",
    "authors": [
      "Alexander Christgau",
      "Alexis Arnaudon",
      "Stefan Sommer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.03337"
  },
  {
    "id": "arXiv:2110.03338",
    "title": "Cross-Language Learning for Entity Matching",
    "abstract": "Transformer-based matching methods have significantly moved the\nstate-of-the-art for less-structured matching tasks involving textual entity\ndescriptions. In order to excel on these tasks, Transformer-based matching\nmethods require a decent amount of training pairs. Providing enough training\ndata can be challenging, especially if a matcher for non-English entity\ndescriptions should be learned. This paper explores along the use case of\nmatching product offers from different e-shops to which extent it is possible\nto improve the performance of Transformer-based entity matchers by\ncomplementing a small set of training pairs in the target language, German in\nour case, with a larger set of English-language training pairs. Our experiments\nusing different Transformers show that extending the German set with English\npairs is always beneficial. The impact of adding the English pairs is\nespecially high in low-resource settings in which only a rather small number of\nnon-English pairs is available. As it is often possible to automatically gather\nEnglish training pairs from the Web by using schema.org annotations, our\nresults could proof relevant for many product matching scenarios targeting\nlow-resource languages.",
    "descriptor": "",
    "authors": [
      "Ralph Peeters",
      "Christian Bizer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03338"
  },
  {
    "id": "arXiv:2110.03346",
    "title": "MSHCNet: Multi-Stream Hybridized Convolutional Networks with Mixed  Statistics in Euclidean/Non-Euclidean Spaces and Its Application to  Hyperspectral Image Classification",
    "abstract": "It is well known that hyperspectral images (HSI) contain rich\nspatial-spectral contextual information, and how to effectively combine both\nspectral and spatial information using DNN for HSI classification has become a\nnew research hotspot. Compared with CNN with square kernels, GCN have exhibited\nexciting potential to model spatial contextual structure and conduct flexible\nconvolution on arbitrarily irregular image regions. However, current GCN only\nusing first-order spectral-spatial signatures can result in boundary blurring\nand isolated misclassification. To address these, we first designed the\ngraph-based second-order pooling (GSOP) operation to obtain contextual nodes\ninformation in non-Euclidean space for GCN. Further, we proposed a novel\nmulti-stream hybridized convolutional network (MSHCNet) with combination of\nfirst and second order statistics in Euclidean/non-Euclidean spaces to learn\nand fuse multi-view complementary information to segment HSIs. Specifically,\nour MSHCNet adopted four parallel streams, which contained G-stream, utilizing\nthe irregular correlation between adjacent land covers in terms of first-order\ngraph in non-Euclidean space; C-stream, adopting convolution operator to learn\nregular spatial-spectral features in Euclidean space; N-stream, combining first\nand second order features to learn representative and discriminative regular\nspatial-spectral features of Euclidean space; S-stream, using GSOP to capture\nboundary correlations and obtain graph representations from all nodes in graphs\nof non-Euclidean space. Besides, these feature representations learned from\nfour different streams were fused to integrate the multi-view complementary\ninformation for HSI classification. Finally, we evaluated our proposed MSHCNet\non three hyperspectral datasets, and experimental results demonstrated that our\nmethod significantly outperformed state-of-the-art eight methods.",
    "descriptor": "",
    "authors": [
      "Shuang He",
      "Haitong Tang",
      "Xia Lu",
      "Hongjie Yan",
      "Nizhuan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03346"
  },
  {
    "id": "arXiv:2110.03349",
    "title": "Real-time Nonlinear MPC Strategy with Full Vehicle Validation for  Autonomous Driving",
    "abstract": "In this paper, we present the development and deployment of an embedded\noptimal control strategy for autonomous driving applications on a Ford Focus\nroad vehicle. Non-linear model predictive control (NMPC) is designed and\ndeployed on a system with hard real-time constraints. We show the properties of\nsequential quadratic programming (SQP) optimization solvers that are suitable\nfor driving tasks. Importantly, the designed algorithms are validated based on\na standard automotive development cycle: model-in-the-loop (MiL) with high\nfidelity vehicle dynamics, hardware-in-the-loop (HiL) with vehicle actuation\nand embedded platform, and vehicle-hardware-in-the-loop (VeHiL) testing using a\nfull vehicle. The autonomous driving environment contains both virtual\nsimulation and physical proving ground tracks. Throughout the process, NMPC\nalgorithms and optimal control problem (OCP) formulation are fine-tuned using a\ndeployable C code via code generation compatible with the target embedded\ntoolchains. Finally, the developed systems are applied to autonomous collision\navoidance, trajectory tracking and lane change at high speed on city/highway\nand low speed at a parking environment.",
    "descriptor": "",
    "authors": [
      "Jean Pierre Allamaa",
      "Petr Listov",
      "Herman Van der Auweraer",
      "Colin Jones",
      "Tong Duy Son"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03349"
  },
  {
    "id": "arXiv:2110.03353",
    "title": "Noisy Text Data: Achilles' Heel of popular transformer based NLP models",
    "abstract": "In the last few years, the ML community has created a number of new NLP\nmodels based on transformer architecture. These models have shown great\nperformance for various NLP tasks on benchmark datasets, often surpassing SOTA\nresults. Buoyed with this success, one often finds industry practitioners\nactively experimenting with fine-tuning these models to build NLP applications\nfor industry use cases. However, for most datasets that are used by\npractitioners to build industrial NLP applications, it is hard to guarantee the\npresence of any noise in the data. While most transformer based NLP models have\nperformed exceedingly well in transferring the learnings from one dataset to\nanother, it remains unclear how these models perform when fine-tuned on noisy\ntext. We address the open question by Kumar et al. (2020) to explore the\nsensitivity of popular transformer based NLP models to noise in the text data.\nWe continue working with the noise as defined by them -- spelling mistakes &\ntypos (which are the most commonly occurring noise). We show (via experimental\nresults) that these models perform badly on most common NLP tasks namely text\nclassification, textual similarity, NER, question answering, text summarization\non benchmark datasets. We further show that as the noise in data increases, the\nperformance degrades. Our findings suggest that one must be vary of the\npresence of noise in their datasets while fine-tuning popular transformer based\nNLP models.",
    "descriptor": "",
    "authors": [
      "Kartikay Bagla",
      "Ankit Kumar",
      "Shivam Gupta",
      "Anuj Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03353"
  },
  {
    "id": "arXiv:2110.03358",
    "title": "Uncertainty Set Prediction of Aggregated Wind Power Generation based on  Bayesian LSTM and Spatio-Temporal Analysis",
    "abstract": "Aggregated stochastic characteristics of geographically distributed wind\ngeneration will provide valuable information for secured and economical system\noperation in electricity markets. This paper focuses on the uncertainty set\nprediction of the aggregated generation of geographically distributed wind\nfarms. A Spatio-temporal model is proposed to learn the dynamic features from\npartial observation in near-surface wind fields of neighboring wind farms. We\nuse Bayesian LSTM, a probabilistic prediction model, to obtain the uncertainty\nset of the generation in individual wind farms. Then, spatial correlation\nbetween different wind farms is presented to correct the output results.\nNumerical testing results based on the actual data with 6 wind farms in\nnorthwest China show that the uncertainty set of aggregated wind generation of\ndistributed wind farms is less volatile than that of a single wind farm.",
    "descriptor": "",
    "authors": [
      "Xiaopeng Li",
      "Jiang Wu",
      "Zhanbo Xu",
      "Kun Liu",
      "Jun Yu",
      "Xiaohong Guan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03358"
  },
  {
    "id": "arXiv:2110.03360",
    "title": "Sparse MoEs meet Efficient Ensembles",
    "abstract": "Machine learning models based on the aggregated outputs of submodels, either\nat the activation or prediction levels, lead to strong performance. We study\nthe interplay of two popular classes of such models: ensembles of neural\nnetworks and sparse mixture of experts (sparse MoEs). First, we show that these\ntwo approaches have complementary features whose combination is beneficial.\nThen, we present partitioned batch ensembles, an efficient ensemble of sparse\nMoEs that takes the best of both classes of models. Extensive experiments on\nfine-tuned vision transformers demonstrate the accuracy, log-likelihood,\nfew-shot learning, robustness, and uncertainty calibration improvements of our\napproach over several challenging baselines. Partitioned batch ensembles not\nonly scale to models with up to 2.7B parameters, but also provide larger\nperformance gains for larger models.",
    "descriptor": "\nComments: 44 pages, 19 figures, 24 tables\n",
    "authors": [
      "James Urquhart Allingham",
      "Florian Wenzel",
      "Zelda E Mariet",
      "Basil Mustafa",
      "Joan Puigcerver",
      "Neil Houlsby",
      "Ghassen Jerfel",
      "Vincent Fortuin",
      "Balaji Lakshminarayanan",
      "Jasper Snoek",
      "Dustin Tran",
      "Carlos Riquelme Ruiz",
      "Rodolphe Jenatton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03360"
  },
  {
    "id": "arXiv:2110.03363",
    "title": "Evaluating model-based planning and planner amortization for continuous  control",
    "abstract": "There is a widespread intuition that model-based control methods should be\nable to surpass the data efficiency of model-free approaches. In this paper we\nattempt to evaluate this intuition on various challenging locomotion tasks. We\ntake a hybrid approach, combining model predictive control (MPC) with a learned\nmodel and model-free policy learning; the learned policy serves as a proposal\nfor MPC. We find that well-tuned model-free agents are strong baselines even\nfor high DoF control problems but MPC with learned proposals and models\n(trained on the fly or transferred from related tasks) can significantly\nimprove performance and data efficiency in hard multi-task/multi-goal settings.\nFinally, we show that it is possible to distil a model-based planner into a\npolicy that amortizes the planning computation without any loss of performance.\nVideos of agents performing different tasks can be seen at\nhttps://sites.google.com/view/mbrl-amortization/home.",
    "descriptor": "\nComments: 9 pages main text, 30 pages with references and appendix including several ablations and additional experiments. Submitted to ICLR 2022\n",
    "authors": [
      "Arunkumar Byravan",
      "Leonard Hasenclever",
      "Piotr Trochim",
      "Mehdi Mirza",
      "Alessandro Davide Ialongo",
      "Yuval Tassa",
      "Jost Tobias Springenberg",
      "Abbas Abdolmaleki",
      "Nicolas Heess",
      "Josh Merel",
      "Martin Riedmiller"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03363"
  },
  {
    "id": "arXiv:2110.03368",
    "title": "A Baseline Framework for Part-level Action Parsing and Action  Recognition",
    "abstract": "This technical report introduces our 2nd place solution to Kinetics-TPS Track\non Part-level Action Parsing in ICCV DeeperAction Workshop 2021. Our entry is\nmainly based on YOLOF for instance and part detection, HRNet for human pose\nestimation, and CSN for video-level action recognition and frame-level part\nstate parsing. We describe technical details for the Kinetics-TPS dataset,\ntogether with some experimental results. In the competition, we achieved 61.37%\nmAP on the test set of Kinetics-TPS.",
    "descriptor": "\nComments: 4 pages, 1 figures, ICCV 2021 Challenge, 2nd place solution for Kinetics-TPS Track in ICCV DeeperAction Workshop 2021\n",
    "authors": [
      "Xiaodong Chen",
      "Xinchen Liu",
      "Kun Liu",
      "Wu Liu",
      "Tao Mei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03368"
  },
  {
    "id": "arXiv:2110.03369",
    "title": "The Connection between Out-of-Distribution Generalization and Privacy of  ML Models",
    "abstract": "With the goal of generalizing to out-of-distribution (OOD) data, recent\ndomain generalization methods aim to learn \"stable\" feature representations\nwhose effect on the output remains invariant across domains. Given the\ntheoretical connection between generalization and privacy, we ask whether\nbetter OOD generalization leads to better privacy for machine learning models,\nwhere privacy is measured through robustness to membership inference (MI)\nattacks. In general, we find that the relationship does not hold. Through\nextensive evaluation on a synthetic dataset and image datasets like MNIST,\nFashion-MNIST, and Chest X-rays, we show that a lower OOD generalization gap\ndoes not imply better robustness to MI attacks. Instead, privacy benefits are\nbased on the extent to which a model captures the stable features. A model that\ncaptures stable features is more robust to MI attacks than models that exhibit\nbetter OOD generalization but do not learn stable features. Further, for the\nsame provable differential privacy guarantees, a model that learns stable\nfeatures provides higher utility as compared to others. Our results offer the\nfirst extensive empirical study connecting stable features and privacy, and\nalso have a takeaway for the domain generalization community; MI attack can be\nused as a complementary metric to measure model quality.",
    "descriptor": "\nComments: Prior version accepted at Workshop on Privacy Preserving Machine Learning, NeurIPS 2020. Code: this https URL\n",
    "authors": [
      "Divyat Mahajan",
      "Shruti Tople",
      "Amit Sharma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03369"
  },
  {
    "id": "arXiv:2110.03370",
    "title": "WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech  Recognition",
    "abstract": "In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.",
    "descriptor": "",
    "authors": [
      "Binbin Zhang",
      "Hang Lv",
      "Pengcheng Guo",
      "Qijie Shao",
      "Chao Yang",
      "Lei Xie",
      "Xin Xu",
      "Hui Bu",
      "Xiaoyu Chen",
      "Chenchen Zeng",
      "Di Wu",
      "Zhendong Peng"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03370"
  },
  {
    "id": "arXiv:2110.03374",
    "title": "Model Adaptation: Historical Contrastive Learning for Unsupervised  Domain Adaptation without Source Data",
    "abstract": "Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the source-trained and earlier-epoch models as the\nhistorical models, HCID encourages UMA to learn instance-discriminative target\nrepresentations while preserving the source hypothesis. Second, it introduces\nhistorical contrastive category discrimination (HCCD) that pseudo-labels target\nsamples to learn category-discriminative target representations. Instead of\nglobally thresholding pseudo labels, HCCD re-weights pseudo labels according to\ntheir prediction consistency across the current and historical models.\nExtensive experiments show that HCL outperforms and complements\nstate-of-the-art methods consistently across a variety of visual tasks (e.g.,\nsegmentation, classification and detection) and setups (e.g., close-set,\nopen-set and partial adaptation).",
    "descriptor": "\nComments: Accepted to NIPS 2021 (2021 Conference on Neural Information Processing Systems)\n",
    "authors": [
      "Jiaxing Huang",
      "Dayan Guan",
      "Aoran Xiao",
      "Shijian Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03374"
  },
  {
    "id": "arXiv:2110.03375",
    "title": "Learning Pessimism for Robust and Efficient Off-Policy Reinforcement  Learning",
    "abstract": "Popular off-policy deep reinforcement learning algorithms compensate for\noverestimation bias during temporal-difference learning by utilizing\npessimistic estimates of the expected target returns. In this work, we propose\na novel learnable penalty to enact such pessimism, based on a new way to\nquantify the critic's epistemic uncertainty. Furthermore, we propose to learn\nthe penalty alongside the critic with dual TD-learning, a strategy to estimate\nand minimize the bias magnitude in the target returns. Our method enables us to\naccurately counteract overestimation bias throughout training without incurring\nthe downsides of overly pessimistic targets. Empirically, by integrating our\nmethod and other orthogonal improvements with popular off-policy algorithms, we\nachieve state-of-the-art results in continuous control tasks from both\nproprioceptive and pixel observations.",
    "descriptor": "",
    "authors": [
      "Edoardo Cetin",
      "Oya Celiktutan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03375"
  },
  {
    "id": "arXiv:2110.03379",
    "title": "Hafnia Hands: A Multi-Skin Hand Texture Resource for Virtual Reality  Research",
    "abstract": "We created a hand texture resource (with different skin tone versions as well\nas non-human hands) for use in virtual reality studies. This makes it easier to\nrun lab and remote studies where the hand representation is matched to the\nparticipant's own skin tone. We validate that the virtual hands with our\ntextures align with participants view of their own real hands and allow to\ncreate VR applications where participants have an increased sense of body\nownership. These properties are critical for a range of VR studies, such as of\nimmersion.",
    "descriptor": "",
    "authors": [
      "Henning Pohl",
      "Aske Mottelson"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.03379"
  },
  {
    "id": "arXiv:2110.03380",
    "title": "Disentangled dimensionality reduction for noise-robust speaker  diarisation",
    "abstract": "The objective of this work is to train noise-robust speaker embeddings for\nspeaker diarisation. Speaker embeddings play a crucial role in the performance\nof diarisation systems, but they often capture spurious information such as\nnoise and reverberation, adversely affecting performance. Our previous work\nhave proposed an auto-encoder-based dimensionality reduction module to help\nremove the spurious information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyperparameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of a speech/non-speech indicator to\nprevent the speaker code from learning from the background noise. Through a\nrange of experiments conducted on four different datasets, our approach\nconsistently demonstrates the state-of-the-art performance among models that do\nnot adopt ensembles.",
    "descriptor": "",
    "authors": [
      "You Jin Kim",
      "Hee-Soo Heo",
      "Jee-weon Jung",
      "Youngki Kwon",
      "Bong-Jin Lee",
      "Joon Son Chung"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03380"
  },
  {
    "id": "arXiv:2110.03382",
    "title": "Analysis of the influence of political polarization in the vaccination  stance: the Brazilian COVID-19 scenario",
    "abstract": "The outbreak of COVID-19 had a huge global impact, and non-scientific beliefs\nand political polarization have significantly influenced the population's\nbehavior. In this context, COVID vaccines were made available in an\nunprecedented time, but a high level of hesitance has been observed that can\nundermine community immunization. Traditionally, anti-vaccination attitudes are\nmore related to conspiratorial thinking rather than political bias. In Brazil,\na country with an exemplar tradition in large-scale vaccination programs, all\nCOVID-related topics have also been discussed under a strong political bias. In\nthis paper, we use a multidimensional analysis framework to understand if\nanti/pro-vaccination stances expressed by Brazilians in social media are\ninfluenced by political polarization. The analysis framework incorporates\ntechniques to automatically infer from users their political orientation, topic\nmodeling to discover their concerns, network analysis to characterize their\nsocial behavior, and the characterization of information sources and external\ninfluence. Our main findings confirm that anti/pro stances are biased by\npolitical polarization, right and left, respectively. While a significant\nproportion of pro-vaxxers display haste for an immunization program and\ncriticize the government's actions, the anti-vaxxers distrust a vaccine\ndeveloped in a record time. Anti-vaccination stance is also related to\nprejudice against China (anti-sinovaxxers), revealing conspiratorial theories\nrelated to communism. All groups display an \"echo chamber behavior, revealing\nthey are not open to distinct views.",
    "descriptor": "\nComments: Accepted for the AAAI 16th International Conference on Web and Social Media (ICWSM 2022), and to be published in the ICWSM 2022 Proceedings. Please, cite the proceedings\n",
    "authors": [
      "R\u00e9gis Ebeling",
      "Carlos Abel C\u00f3rdova S\u00e1enz",
      "Jeferson Nobre",
      "Karin Becker"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2110.03382"
  },
  {
    "id": "arXiv:2110.03384",
    "title": "Deep Learning Model Explainability for Inspection Accuracy Improvement  in the Automotive Industry",
    "abstract": "The welding seams visual inspection is still manually operated by humans in\ndifferent companies, so the result of the test is still highly subjective and\nexpensive. At present, the integration of deep learning methods for welds\nclassification is a research focus in engineering applications. This work\nintends to apprehend and emphasize the contribution of deep learning model\nexplainability to the improvement of welding seams classification accuracy and\nreliability, two of the various metrics affecting the production lines and cost\nin the automotive industry. For this purpose, we implement a novel hybrid\nmethod that relies on combining the model prediction scores and visual\nexplanation heatmap of the model in order to make a more accurate\nclassification of welding seam defects and improve both its performance and its\nreliability. The results show that the hybrid model performance is relatively\nabove our target performance and helps to increase the accuracy by at least\n18%, which presents new perspectives to the developments of deep Learning\nexplainability and interpretability.",
    "descriptor": "",
    "authors": [
      "Anass El Houd",
      "Charbel El Hachem",
      "Loic Painvin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03384"
  },
  {
    "id": "arXiv:2110.03389",
    "title": "Beam Search with Bidirectional Strategies for Neural Response Generation",
    "abstract": "Sequence-to-sequence neural networks have been widely used in language-based\napplications as they have flexible capabilities to learn various language\nmodels. However, when seeking for the optimal language response through trained\nneural networks, current existing approaches such as beam-search decoder\nstrategies are still not able reaching to promising performances. Instead of\ndeveloping various decoder strategies based on a \"regular sentence order\"\nneural network (a trained model by outputting sentences from left-to-right\norder), we leveraged \"reverse\" order as additional language model (a trained\nmodel by outputting sentences from right-to-left order) which can provide\ndifferent perspectives for the path finding problems. In this paper, we propose\nbidirectional strategies in searching paths by combining two networks\n(left-to-right and right-to-left language models) making a bidirectional beam\nsearch possible. Besides, our solution allows us using any similarity measure\nin our sentence selection criterion. Our approaches demonstrate better\nperformance compared to the unidirectional beam search strategy.",
    "descriptor": "",
    "authors": [
      "Pierre Colombo",
      "Chouchang Yang",
      "Giovanna Varni",
      "Chlo\u00e9 Clavel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03389"
  },
  {
    "id": "arXiv:2110.03390",
    "title": "GANtron: Emotional Speech Synthesis with Generative Adversarial Networks",
    "abstract": "Speech synthesis is used in a wide variety of industries. Nonetheless, it\nalways sounds flat or robotic. The state of the art methods that allow for\nprosody control are very cumbersome to use and do not allow easy tuning. To\ntackle some of these drawbacks, in this work we target the implementation of a\ntext-to-speech model where the inferred speech can be tuned with the desired\nemotions. To do so, we use Generative Adversarial Networks (GANs) together with\na sequence-to-sequence model using an attention mechanism. We evaluate four\ndifferent configurations considering different inputs and training strategies,\nstudy them and prove how our best model can generate speech files that lie in\nthe same distribution as the initial training dataset. Additionally, a new\nstrategy to boost the training convergence by applying a guided attention loss\nis proposed.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Enrique Hortal",
      "Rodrigo Brechard Alarcia"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03390"
  },
  {
    "id": "arXiv:2110.03393",
    "title": "Multivariate Anomaly Detection based on Prediction Intervals Constructed  using Deep Learning",
    "abstract": "It has been shown that deep learning models can under certain circumstances\noutperform traditional statistical methods at forecasting. Furthermore, various\ntechniques have been developed for quantifying the forecast uncertainty\n(prediction intervals). In this paper, we utilize prediction intervals\nconstructed with the aid of artificial neural networks to detect anomalies in\nthe multivariate setting. Challenges with existing deep learning-based anomaly\ndetection approaches include $(i)$ large sets of parameters that may be\ncomputationally intensive to tune, $(ii)$ returning too many false positives\nrendering the techniques impractical for use, $(iii)$ requiring labeled\ndatasets for training which are often not prevalent in real life. Our approach\novercomes these challenges. We benchmark our approach against the oft-preferred\nwell-established statistical models. We focus on three deep learning\narchitectures, namely, cascaded neural networks, reservoir computing and long\nshort-term memory recurrent neural networks. Our finding is deep learning\noutperforms (or at the very least is competitive to) the latter.",
    "descriptor": "",
    "authors": [
      "Thabang Mathonsi",
      "Terence L. van Zyl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03393"
  },
  {
    "id": "arXiv:2110.03395",
    "title": "SLASH: Embracing Probabilistic Circuits into Neural Answer Set  Programming",
    "abstract": "The goal of combining the robustness of neural networks and the expressivity\nof symbolic methods has rekindled the interest in neuro-symbolic AI. Recent\nadvancements in neuro-symbolic AI often consider specifically-tailored\narchitectures consisting of disjoint neural and symbolic components, and thus\ndo not exhibit desired gains that can be achieved by integrating them into a\nunifying framework. We introduce SLASH -- a novel deep probabilistic\nprogramming language (DPPL). At its core, SLASH consists of\nNeural-Probabilistic Predicates (NPPs) and logical programs which are united\nvia answer set programming. The probability estimates resulting from NPPs act\nas the binding element between the logical program and raw input data, thereby\nallowing SLASH to answer task-dependent logical queries. This allows SLASH to\nelegantly integrate the symbolic and neural components in a unified framework.\nWe evaluate SLASH on the benchmark data of MNIST addition as well as novel\ntasks for DPPLs such as missing data prediction and set prediction with\nstate-of-the-art performance, thereby showing the effectiveness and generality\nof our method.",
    "descriptor": "\nComments: 16 pages, 7 figures and 5 tables\n",
    "authors": [
      "Arseny Skryagin",
      "Wolfgang Stammer",
      "Daniel Ochs",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03395"
  },
  {
    "id": "arXiv:2110.03399",
    "title": "What motivates people to telework? Exploratory study in a  post-confinement context",
    "abstract": "After the considerable excitement caused by COVID-19 and the first telework\nmeasures, many management issues became apparent and some questions quickly\narose, especially about efficiency of employees' work and conditions for\nsuccessful telework adoption. This study focuses on the following questions:\nwhat is the new interest to telework for employees and what are the potential\nreasons for this? How much employees feel able to do their work remotely now\nand why? To answer these questions, we conducted a survey over several weeks,\nthat involved employees coming from different industries (N=170), in order to\ncollect their experience, skills and motivations for teleworking. The results\nshow that adoption of telework is real for the respondents. Those who have\nexperienced it present a strong motivation and a real capacity to use it, with\nalmost no technological barriers. The theoretical model that we used, based on\nTechnology Acceptance Models (TAM), has highlighted an important new factor of\ntelework adoption: time saved in commuting. The study points out that adoption\nof telework could be sustainable for both public and private organizations, and\nthis requires a critical examination and discussion.",
    "descriptor": "\nComments: in French\n",
    "authors": [
      "Steve Berberat",
      "Damien Rosat",
      "Armand Kouadio"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.03399"
  },
  {
    "id": "arXiv:2110.03403",
    "title": "Disentangling deep neural networks with rectified linear units using  duality",
    "abstract": "Despite their success deep neural networks (DNNs) are still largely\nconsidered as black boxes. The main issue is that the linear and non-linear\noperations are entangled in every layer, making it hard to interpret the hidden\nlayer outputs. In this paper, we look at DNNs with rectified linear units\n(ReLUs), and focus on the gating property (`on/off' states) of the ReLUs. We\nextend the recently developed dual view in which the computation is broken\npath-wise to show that learning in the gates is more crucial, and learning the\nweights given the gates is characterised analytically via the so called neural\npath kernel (NPK) which depends on inputs and gates. In this paper, we present\nnovel results to show that convolution with global pooling and skip connection\nprovide respectively rotational invariance and ensemble structure to the NPK.\nTo address `black box'-ness, we propose a novel interpretable counterpart of\nDNNs with ReLUs namely deep linearly gated networks (DLGN): the pre-activations\nto the gates are generated by a deep linear network, and the gates are then\napplied as external masks to learn the weights in a different network. The DLGN\nis not an alternative architecture per se, but a disentanglement and an\ninterpretable re-arrangement of the computations in a DNN with ReLUs. The DLGN\ndisentangles the computations into two `mathematically' interpretable\nlinearities (i) the `primal' linearity between the input and the\npre-activations in the gating network and (ii) the `dual' linearity in the path\nspace in the weights network characterised by the NPK. We compare the\nperformance of DNN, DGN and DLGN on CIFAR-10 and CIFAR-100 to show that, the\nDLGN recovers more than $83.5\\%$ of the performance of state-of-the-art DNNs.\nThis brings us to an interesting question: `Is DLGN a universal spectral\napproximator?'",
    "descriptor": "",
    "authors": [
      "Chandrashekar Lakshminarayanan",
      "Amit Vikram Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03403"
  },
  {
    "id": "arXiv:2110.03405",
    "title": "Joint calibration and mapping of satellite altimetry data using  trainable variational models",
    "abstract": "Satellite radar altimeters are a key source of observation of ocean surface\ndynamics. However, current sensor technology and mapping techniques do not yet\nallow to systematically resolve scales smaller than 100km. With their new\nsensors, upcoming wide-swath altimeter missions such as SWOT should help\nresolve finer scales. Current mapping techniques rely on the quality of the\ninput data, which is why the raw data go through multiple preprocessing stages\nbefore being used. Those calibration stages are improved and refined over many\nyears and represent a challenge when a new type of sensor start acquiring data.\nHere we show how a data-driven variational data assimilation framework could be\nused to jointly learn a calibration operator and an interpolator from\nnon-calibrated data . The proposed framework significantly outperforms the\noperational state-of-the-art mapping pipeline and truly benefits from\nwide-swath data to resolve finer scales on the global map as well as in the\nSWOT sensor geometry.",
    "descriptor": "",
    "authors": [
      "Quentin Febvre",
      "Ronan Fablet",
      "Julien Le Sommer",
      "Cl\u00e9ment Ubelmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03405"
  },
  {
    "id": "arXiv:2110.03414",
    "title": "SERAB: A multi-lingual benchmark for speech emotion recognition",
    "abstract": "Recent developments in speech emotion recognition (SER) often leverage deep\nneural networks (DNNs). Comparing and benchmarking different DNN models can\noften be tedious due to the use of different datasets and evaluation protocols.\nTo facilitate the process, here, we present the Speech Emotion Recognition\nAdaptation Benchmark (SERAB), a framework for evaluating the performance and\ngeneralization capacity of different approaches for utterance-level SER. The\nbenchmark is composed of nine datasets for SER in six languages. Since the\ndatasets have different sizes and numbers of emotional classes, the proposed\nsetup is particularly suitable for estimating the generalization capacity of\npre-trained DNN-based feature extractors. We used the proposed framework to\nevaluate a selection of standard hand-crafted feature sets and state-of-the-art\nDNN representations. The results highlight that using only a subset of the data\nincluded in SERAB can result in biased evaluation, while compliance with the\nproposed protocol can circumvent this issue.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Neil Scheidwasser-Clow",
      "Mikolaj Kegler",
      "Pierre Beckmann",
      "Milos Cernak"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03414"
  },
  {
    "id": "arXiv:2110.03420",
    "title": "RHH-LGP: Receding Horizon And Heuristics-Based Logic-Geometric  Programming For Task And Motion Planning",
    "abstract": "Sequential decision-making and motion planning for robotic manipulation\ninduce combinatorial complexity. For long-horizon tasks, especially when the\nenvironment comprises many objects that can be interacted with, planning\nefficiency becomes even more important. To plan such long-horizon tasks, we\npresent the RHH-LGP algorithm for combined task and motion planning (TAMP).\nFirst, we propose a TAMP approach (based on Logic-Geometric Programming) that\neffectively uses geometry-based heuristics for solving long-horizon\nmanipulation tasks. We further improve the efficiency of this planner by a\nreceding horizon formulation, resulting in RHH-LGP. We demonstrate the\neffectiveness and generality of our approach on several long-horizon tasks that\nrequire reasoning about interactions with a large number of objects. Using our\nframework, we can solve tasks that require multiple robots, including a mobile\nrobot and snake-like walking robots, to form novel heterogeneous kinematic\nstructures autonomously.",
    "descriptor": "\nComments: for source code, see this https URL\n",
    "authors": [
      "Cornelius V. Braun",
      "Joaquim Ortiz-Haro",
      "Marc Toussaint",
      "Ozgur S. Oguz"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03420"
  },
  {
    "id": "arXiv:2110.03422",
    "title": "Modeling Effect of Lockdowns and Other Effects on India Covid-19  Infections Using SEIR Model and Machine Learning",
    "abstract": "The SEIR model is a widely used epidemiological model used to predict the\nrise in infections. This model has been widely used in different countries to\npredict the number of Covid-19 cases. But the original SEIR model does not take\ninto account the effect of factors such as lockdowns, vaccines, and\nre-infections. In India the first wave of Covid started in March 2020 and the\nsecond wave in April 2021. In this paper, we modify the SEIR model equations to\nmodel the effect of lockdowns and other influencers, and fit the model on data\nof the daily Covid-19 infections in India using lmfit, a python library for\nleast squares minimization for curve fitting. We modify R0 parameter in the\nstandard SEIR model as a rectangle in order to account for the effect of\nlockdowns. Our modified SEIR model accurately fits the available data of\ninfections.",
    "descriptor": "\nComments: 6 pages, 8 figures\n",
    "authors": [
      "Sathiyanarayanan Sampath",
      "Joy Bose"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03422"
  },
  {
    "id": "arXiv:2110.03423",
    "title": "Efficient GPU implementation of randomized SVD and its applications",
    "abstract": "Matrix decompositions are ubiquitous in machine learning, including\napplications in dimensionality reduction, data compression and deep learning\nalgorithms. Typical solutions for matrix decompositions have polynomial\ncomplexity which significantly increases their computational cost and time. In\nthis work, we leverage efficient processing operations that can be run in\nparallel on modern Graphical Processing Units (GPUs), predominant computing\narchitecture used e.g. in deep learning, to reduce the computational burden of\ncomputing matrix decompositions. More specifically, we reformulate the\nrandomized decomposition problem to incorporate fast matrix multiplication\noperations (BLAS-3) as building blocks. We show that this formulation, combined\nwith fast random number generators, allows to fully exploit the potential of\nparallel processing implemented in GPUs. Our extensive evaluation confirms the\nsuperiority of this approach over the competing methods and we release the\nresults of this research as a part of the official CUDA implementation\n(https://docs.nvidia.com/cuda/cusolver/index.html).",
    "descriptor": "",
    "authors": [
      "\u0141ukasz Struski",
      "Pawe\u0142 Morkisz",
      "Przemys\u0142aw Spurek",
      "Samuel Rodriguez Bernabeu",
      "Tomasz Trzci\u0144ski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.03423"
  },
  {
    "id": "arXiv:2110.03424",
    "title": "Bad-Policy Density: A Measure of Reinforcement Learning Hardness",
    "abstract": "Reinforcement learning is hard in general. Yet, in many specific\nenvironments, learning is easy. What makes learning easy in one environment,\nbut difficult in another? We address this question by proposing a simple\nmeasure of reinforcement-learning hardness called the bad-policy density. This\nquantity measures the fraction of the deterministic stationary policy space\nthat is below a desired threshold in value. We prove that this simple quantity\nhas many properties one would expect of a measure of learning hardness.\nFurther, we prove it is NP-hard to compute the measure in general, but there\nare paths to polynomial-time approximation. We conclude by summarizing\npotential directions and uses for this measure.",
    "descriptor": "\nComments: Presented at the 2021 ICML Workshop on Reinforcement Learning Theory\n",
    "authors": [
      "David Abel",
      "Cameron Allen",
      "Dilip Arumugam",
      "D. Ellis Hershkowitz",
      "Michael L. Littman",
      "Lawson L.S. Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03424"
  },
  {
    "id": "arXiv:2110.03426",
    "title": "Fast learning from label proportions with small bags",
    "abstract": "In learning from label proportions (LLP), the instances are grouped into\nbags, compared with supervised learning and the task is to learn an instance\nclassifier given relative class proportions in training bags. LLP is useful\nwhen obtaining individual instance labels is impossible or costly.\nIn this work, we focus on the case of small bags, which allows designing more\nefficient algorithms by explicitly considering all consistent label\ncombinations. In particular, we propose an EM algorithm alternating between\noptimizing a general neural network instance classifier and incorporating\nbag-level annotations. In comparison to existing deep LLP methods, our approach\nconverges faster to a comparable or better solution. Several experiments were\nperformed on two different datasets.",
    "descriptor": "\nComments: submitted to ICASSP 2022\n",
    "authors": [
      "Denis Baru\u010di\u0107",
      "Jan Kybic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03426"
  },
  {
    "id": "arXiv:2110.03427",
    "title": "Is Attention always needed? A Case Study on Language Identification from  Speech",
    "abstract": "Language Identification (LID), a recommended initial step to Automatic Speech\nRecognition (ASR), is used to detect a spoken language from audio specimens. In\nstate-of-the-art systems capable of multilingual speech processing, however,\nusers have to explicitly set one or more languages before using them. LID,\ntherefore, plays a very important role in situations where ASR based systems\ncannot parse the uttered language in multilingual contexts causing failure in\nspeech recognition. We propose an attention based convolutional recurrent\nneural network (CRNN with Attention) that works on Mel-frequency Cepstral\nCoefficient (MFCC) features of audio specimens. Additionally, we reproduce some\nstate-of-the-art approaches, namely Convolutional Neural Network (CNN) and\nConvolutional Recurrent Neural Network (CRNN), and compare them to our proposed\nmethod. We performed extensive evaluation on thirteen different Indian\nlanguages and our model achieves classification accuracy over 98%. Our LID\nmodel is robust to noise and provides 91.2% accuracy in a noisy scenario. The\nproposed model is easily extensible to new languages.",
    "descriptor": "\nComments: Submitted to ACM Transactions on Asian and Low-Resource Language Information Processing\n",
    "authors": [
      "Atanu Mandal",
      "Santanu Pal",
      "Indranil Dutta",
      "Mahidas Bhattacharya",
      "Sudip Kumar Naskar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03427"
  },
  {
    "id": "arXiv:2110.03431",
    "title": "Cloud Failure Prediction with Hierarchical Temporary Memory: An  Empirical Assessment",
    "abstract": "Hierarchical Temporary Memory (HTM) is an unsupervised learning algorithm\ninspired by the features of the neocortex that can be used to continuously\nprocess stream data and detect anomalies, without requiring a large amount of\ndata for training nor requiring labeled data. HTM is also able to continuously\nlearn from samples, providing a model that is always up-to-date with respect to\nobservations. These characteristics make HTM particularly suitable for\nsupporting online failure prediction in cloud systems, which are systems with a\ndynamically changing behavior that must be monitored to anticipate problems.\nThis paper presents the first systematic study that assesses HTM in the context\nof failure prediction. The results that we obtained considering 72\nconfigurations of HTM applied to 12 different types of faults introduced in the\nClearwater cloud system show that HTM can help to predict failures with\nsufficient effectiveness (F-measure = 0.76), representing an interesting\npractical alternative to (semi-)supervised algorithms.",
    "descriptor": "\nComments: paper submitted and accepted to the IEEE International Conference on Machine Learning and Applications (ICMLA 2021)\n",
    "authors": [
      "Oliviero Riganelli",
      "Paolo Saltarel",
      "Alessandro Tundo",
      "Marco Mobilio",
      "Leonardo Mariani"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03431"
  },
  {
    "id": "arXiv:2110.03433",
    "title": "From the Head or the Heart? An Experimental Design on the Impact of  Explanation on Cognitive and Affective Trust",
    "abstract": "Automated vehicles (AVs) are social robots that can potentially benefit our\nsociety. According to the existing literature, AV explanations can promote\npassengers' trust by reducing the uncertainty associated with the AV's\nreasoning and actions. However, the literature on AV explanations and trust has\nfailed to consider how the type of trust\n- cognitive versus affective - might alter this relationship. Yet, the\nexisting literature has shown that the implications associated with trust vary\nwidely depending on whether it is cognitive or affective. To address this\nshortcoming and better understand the impacts of explanations on trust in AVs,\nwe designed a study to investigate the effectiveness of explanations on both\ncognitive and affective trust. We expect these results to be of great\nsignificance in designing AV explanations to promote AV trust.",
    "descriptor": "\nComments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)\n",
    "authors": [
      "Qiaoning Zhang",
      "X. Jessie Yang",
      "Lionel P. Robert Jr"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03433"
  },
  {
    "id": "arXiv:2110.03440",
    "title": "Towards Robust and Transferable IIoT Sensor based Anomaly Classification  using Artificial Intelligence",
    "abstract": "The increasing deployment of low-cost industrial IoT (IIoT) sensor platforms\non industrial assets enables great opportunities for anomaly classification in\nindustrial plants. The performance of such a classification model depends\nhighly on the available training data. Models perform well when the training\ndata comes from the same machine. However, as soon as the machine is changed,\nrepaired, or put into operation in a different environment, the prediction\noften fails. For this reason, we investigate whether it is feasible to have a\nrobust and transferable method for AI based anomaly classification using\ndifferent models and pre-processing steps on centrifugal pumps which are\ndismantled and put back into operation in the same as well as in different\nenvironments. Further, we investigate the model performance on different pumps\nfrom the same type compared to those from the training data.",
    "descriptor": "\nComments: This paper is accepted at this https URL The final authenticated version is available online and this information will be updated\n",
    "authors": [
      "Jana Kemnitz",
      "Thomas Bierweiler",
      "Herbert Grieb",
      "Stefan von Dosky",
      "Daniel Schall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03440"
  },
  {
    "id": "arXiv:2110.03442",
    "title": "A Comparison of Neural Network Architectures for Data-Driven  Reduced-Order Modeling",
    "abstract": "The popularity of deep convolutional autoencoders (CAEs) has engendered\neffective reduced-order models (ROMs) for the simulation of large-scale\ndynamical systems. However, it is not known whether deep CAEs provide superior\nperformance in all ROM scenarios. To elucidate this, the effect of autoencoder\narchitecture on its associated ROM is studied through the comparison of deep\nCAEs against two alternatives: a simple fully connected autoencoder, and a\nnovel graph convolutional autoencoder. Through benchmark experiments, it is\nshown that the superior autoencoder architecture for a given ROM application is\nhighly dependent on the size of the latent space and the structure of the\nsnapshot data, with the proposed architecture demonstrating benefits on data\nwith irregular connectivity when the latent space is sufficiently large.",
    "descriptor": "",
    "authors": [
      "Anthony Gruber",
      "Max Gunzburger",
      "Lili Ju",
      "Zhu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03442"
  },
  {
    "id": "arXiv:2110.03445",
    "title": "PWG-IDS: An Intrusion Detection Model for Solving Class Imbalance in  IIoT Networks Using Generative Adversarial Networks",
    "abstract": "With the continuous development of industrial IoT (IIoT) technology, network\nsecurity is becoming more and more important. And intrusion detection is an\nimportant part of its security. However, since the amount of attack traffic is\nvery small compared to normal traffic, this imbalance makes intrusion detection\nin it very difficult. To address this imbalance, an intrusion detection system\ncalled pretraining Wasserstein generative adversarial network intrusion\ndetection system (PWG-IDS) is proposed in this paper. This system is divided\ninto two main modules: 1) In this module, we introduce the pretraining\nmechanism in the Wasserstein generative adversarial network with gradient\npenalty (WGAN-GP) for the first time, firstly using the normal network traffic\nto train the WGAN-GP, and then inputting the imbalance data into the\npre-trained WGAN-GP to retrain and generate the final required data. 2)\nIntrusion detection module: We use LightGBM as the classification algorithm to\ndetect attack traffic in IIoT networks. The experimental results show that our\nproposed PWG-IDS outperforms other models, with F1-scores of 99% and 89% on the\n2 datasets, respectively. And the pretraining mechanism we proposed can also be\nwidely used in other GANs, providing a new way of thinking for the training of\nGANs.",
    "descriptor": "",
    "authors": [
      "Lei Zhang",
      "Shuaimin Jiang",
      "Xiajiong Shen",
      "Brij B. Gupta",
      "Zhihong Tian"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03445"
  },
  {
    "id": "arXiv:2110.03446",
    "title": "A Hierarchical Variational Neural Uncertainty Model for Stochastic Video  Prediction",
    "abstract": "Predicting the future frames of a video is a challenging task, in part due to\nthe underlying stochastic real-world phenomena. Prior approaches to solve this\ntask typically estimate a latent prior characterizing this stochasticity,\nhowever do not account for the predictive uncertainty of the (deep learning)\nmodel. Such approaches often derive the training signal from the mean-squared\nerror (MSE) between the generated frame and the ground truth, which can lead to\nsub-optimal training, especially when the predictive uncertainty is high.\nTowards this end, we introduce Neural Uncertainty Quantifier (NUQ) - a\nstochastic quantification of the model's predictive uncertainty, and use it to\nweigh the MSE loss. We propose a hierarchical, variational framework to derive\nNUQ in a principled manner using a deep, Bayesian graphical model. Our\nexperiments on four benchmark stochastic video prediction datasets show that\nour proposed framework trains more effectively compared to the state-of-the-art\nmodels (especially when the training sets are small), while demonstrating\nbetter video generation quality and diversity against several evaluation\nmetrics.",
    "descriptor": "\nComments: Accepted at ICCV 2021 (Oral)\n",
    "authors": [
      "Moitreya Chatterjee",
      "Narendra Ahuja",
      "Anoop Cherian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03446"
  },
  {
    "id": "arXiv:2110.03447",
    "title": "GPS Spoofing Attacks on Phasor Measurement Units: Practical Feasibility  and Countermeasures",
    "abstract": "Prior research has demonstrated that global positioning system (GPS) spoofing\nattacks on phasor measurement units (PMUs) can cripple power system operation.\nThis paper provides an experimental evidence of the feasibility of such an\nattack using commonly available digital radios known as software defined radio\n(SDR). It also introduces a novel countermeasure against such attacks using GPS\nsignal redundancy and low power long range (LoRa) spread spectrum modulation\ntechnique. The proposed approach checks the integrity of the GPS signal at\nremote locations and compares the data with the PMUs current output. This\ncountermeasure is a ready-to-deploy system that can provide an instant solution\nto the GPS spoofing detection problem for PMUs.",
    "descriptor": "",
    "authors": [
      "Fakhri Saadedeen",
      "Anamitra Pal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03447"
  },
  {
    "id": "arXiv:2110.03448",
    "title": "Multi-Head ReLU Implicit Neural Representation Networks",
    "abstract": "In this paper, a novel multi-head multi-layer perceptron (MLP) structure is\npresented for implicit neural representation (INR). Since conventional\nrectified linear unit (ReLU) networks are shown to exhibit spectral bias\ntowards learning low-frequency features of the signal, we aim at mitigating\nthis defect by taking advantage of the local structure of the signals. To be\nmore specific, an MLP is used to capture the global features of the underlying\ngenerator function of the desired signal. Then, several heads are utilized to\nreconstruct disjoint local features of the signal, and to reduce the\ncomputational complexity, sparse layers are deployed for attaching heads to the\nbody. Through various experiments, we show that the proposed model does not\nsuffer from the special bias of conventional ReLU networks and has superior\ngeneralization capabilities. Finally, simulation results confirm that the\nproposed multi-head structure outperforms existing INR methods with\nconsiderably less computational cost.",
    "descriptor": "\nComments: ICASSP 2022 submitted, 5 pages, 5 figures\n",
    "authors": [
      "Arya Aftab",
      "Alireza Morsali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03448"
  },
  {
    "id": "arXiv:2110.03450",
    "title": "Efficient and Private Federated Learning with Partially Trainable  Networks",
    "abstract": "Federated learning is used for decentralized training of machine learning\nmodels on a large number (millions) of edge mobile devices. It is challenging\nbecause mobile devices often have limited communication bandwidth and local\ncomputation resources. Therefore, improving the efficiency of federated\nlearning is critical for scalability and usability. In this paper, we propose\nto leverage partially trainable neural networks, which freeze a portion of the\nmodel parameters during the entire training process, to reduce the\ncommunication cost with little implications on model performance. Through\nextensive experiments, we empirically show that Federated learning of Partially\nTrainable neural networks (FedPT) can result in superior communication-accuracy\ntrade-offs, with up to $46\\times$ reduction in communication cost, at a small\naccuracy cost. Our approach also enables faster training, with a smaller memory\nfootprint, and better utility for strong differential privacy guarantees. The\nproposed FedPT method can be particularly interesting for pushing the\nlimitations of overparameterization in on-device learning.",
    "descriptor": "",
    "authors": [
      "Hakim Sidahmed",
      "Zheng Xu",
      "Ankush Garg",
      "Yuan Cao",
      "Mingqing Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03450"
  },
  {
    "id": "arXiv:2110.03451",
    "title": "An Energy Management System Approach for Power System Cyber-Physical  Resilience",
    "abstract": "Power systems are large scale cyber-physical critical infrastructure that\nform the basis of modern society. The reliability and resilience of the grid is\ndependent on the correct functioning of related subsystems, including\ncomputing, communications, and control. The integration is widespread and has a\nprofound impact on the operation, reliability, and efficiency of the grid.\nTechnologies comprising these infrastructure can expose new sources of threats.\nMapping these threats to their grid resilience impacts to stop them early\nrequires a timely and detailed view of the entire cyber-physical system. Grid\nresilience must therefore be seen and addressed as a cyber-physical systems\nproblem. This short position paper presents several key preliminaries,\nsupported with evidence from experience, to enable cyber-physical situational\nawareness and intrusion response through a cyber-physical energy management\nsystem.",
    "descriptor": "\nComments: Two page position paper for Virtual Workshop on Cyber Experimentation and Science of Security (CESOS) 2021\n",
    "authors": [
      "Katherine Davis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03451"
  },
  {
    "id": "arXiv:2110.03453",
    "title": "Recurrent Multigraph Integrator Network for Predicting the Evolution of  Population-Driven Brain Connectivity Templates",
    "abstract": "Learning how to estimate a connectional brain template(CBT) from a population\nof brain multigraphs, where each graph (e.g., functional) quantifies a\nparticular relationship between pairs of brain regions of interest (ROIs),\nallows to pin down the unique connectivity patterns shared across individuals.\nSpecifically, a CBT is viewed as an integral representation of a set of highly\nheterogeneous graphs and ideally meeting the centeredness (i.e., minimum\ndistance to all graphs in the population) and discriminativeness (i.e.,\ndistinguishes the healthy from the disordered population) criteria. So far,\nexisting works have been limited to only integrating and fusing a population of\nbrain multigraphs acquired at a single timepoint. In this paper, we\nunprecedentedly tackle the question: Given a baseline multigraph population,\ncan we learn how to integrate and forecast its CBT representations at follow-up\ntimepoints? Addressing such question is of paramount in predicting common\nalternations across healthy and disordered populations. To fill this gap, we\npropose Recurrent Multigraph Integrator Network (ReMI-Net), the first graph\nrecurrent neural network which infers the baseline CBT of an input population\nt1 and predicts its longitudinal evolution over time (ti > t1). Our ReMI-Net is\ncomposed of recurrent neural blocks with graph convolutional layers using a\ncross-node message passing to first learn hidden-states embeddings of each CBT\nnode (i.e., brain region of interest) and then predict its evolution at the\nconsecutive timepoint. Moreover, we design a novel time-dependent loss to\nregularize the CBT evolution trajectory over time and further introduce a\ncyclic recursion and learnable normalization layer to generate well-centered\nCBTs from time-dependent hidden-state embeddings. Finally, we derive the CBT\nadjacency matrix from the learned hidden state graph representation.",
    "descriptor": "",
    "authors": [
      "Oytun Demirbilek",
      "Islem Rekik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.03453"
  },
  {
    "id": "arXiv:2110.03455",
    "title": "Recent Advances in Heterogeneous Relation Learning for Recommendation",
    "abstract": "Recommender systems have played a critical role in many web applications to\nmeet user's personalized interests and alleviate the information overload. In\nthis survey, we review the development of recommendation frameworks with the\nfocus on heterogeneous relational learning, which consists of different types\nof dependencies among users and items. The objective of this task is to map\nheterogeneous relational data into latent representation space, such that the\nstructural and relational properties from both user and item domain can be well\npreserved. To address this problem, recent research developments can fall into\nthree major lines: social recommendation, knowledge graph-enhanced recommender\nsystem, and multi-behavior recommendation. We discuss the learning approaches\nin each category, such as matrix factorization, attention mechanism and graph\nneural networks, for effectively distilling heterogeneous contextual\ninformation. Finally, we present an exploratory outlook to highlight several\npromising directions and opportunities in heterogeneous relational learning\nframeworks for recommendation.",
    "descriptor": "\nComments: Published as a paper in IJCAI 2021\n",
    "authors": [
      "Chao Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.03455"
  },
  {
    "id": "arXiv:2110.03461",
    "title": "Self-Evolutionary Optimization for Pareto Front Learning",
    "abstract": "Multi-task learning (MTL), which aims to improve performance by learning\nmultiple tasks simultaneously, inherently presents an optimization challenge\ndue to multiple objectives. Hence, multi-objective optimization (MOO)\napproaches have been proposed for multitasking problems. Recent MOO methods\napproximate multiple optimal solutions (Pareto front) with a single unified\nmodel, which is collectively referred to as Pareto front learning (PFL). In\nthis paper, we show that PFL can be re-formulated into another MOO problem with\nmultiple objectives, each of which corresponds to different preference weights\nfor the tasks. We leverage an evolutionary algorithm (EA) to propose a method\nfor PFL called self-evolutionary optimization (SEO) by directly maximizing the\nhypervolume. By using SEO, the neural network learns to approximate the Pareto\nfront conditioned on multiple hyper-parameters that drastically affect the\nhypervolume. Then, by generating a population of approximations simply by\ninferencing the network, the hyper-parameters of the network can be optimized\nby EA. Utilizing SEO for PFL, we also introduce self-evolutionary Pareto\nnetworks (SEPNet), enabling the unified model to approximate the entire Pareto\nfront set that maximizes the hypervolume. Extensive experimental results\nconfirm that SEPNet can find a better Pareto front than the current\nstate-of-the-art methods while minimizing the increase in model size and\ntraining cost.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Simyung Chang",
      "KiYoon Yoo",
      "Jiho Jang",
      "Nojun Kwak"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03461"
  },
  {
    "id": "arXiv:2110.03464",
    "title": "Differential Anomaly Detection for Facial Images",
    "abstract": "Due to their convenience and high accuracy, face recognition systems are\nwidely employed in governmental and personal security applications to\nautomatically recognise individuals. Despite recent advances, face recognition\nsystems have shown to be particularly vulnerable to identity attacks (i.e.,\ndigital manipulations and attack presentations). Identity attacks pose a big\nsecurity threat as they can be used to gain unauthorised access and spread\nmisinformation. In this context, most algorithms for detecting identity attacks\ngeneralise poorly to attack types that are unknown at training time. To tackle\nthis problem, we introduce a differential anomaly detection framework in which\ndeep face embeddings are first extracted from pairs of images (i.e., reference\nand probe) and then combined for identity attack detection. The experimental\nevaluation conducted over several databases shows a high generalisation\ncapability of the proposed method for detecting unknown attacks in both the\ndigital and physical domains.",
    "descriptor": "\nComments: Accepted at WIFS'21\n",
    "authors": [
      "Mathias Ibsen",
      "L\u00e1zaro J. Gonz\u00e1lez-Soler",
      "Christian Rathgeb",
      "Pawel Drozdowski",
      "Marta Gomez-Barrero",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03464"
  },
  {
    "id": "arXiv:2110.03467",
    "title": "An Event Data Extraction Approach from SAP ERP for Process Mining",
    "abstract": "The extraction, transformation, and loading of event logs from information\nsystems is the first and the most expensive step in process mining. In\nparticular, extracting event logs from popular ERP systems such as SAP poses\nmajor challenges, given the size and the structure of the data. Open-source\nsupport for ETL is scarce, while commercial process mining vendors maintain\nconnectors to ERP systems supporting ETL of a limited number of business\nprocesses in an ad-hoc manner. In this paper, we propose an approach to\nfacilitate event data extraction from SAP ERP systems. In the proposed\napproach, we store event data in the format of object-centric event logs that\nefficiently describe executions of business processes supported by ERP systems.\nTo evaluate the feasibility of the proposed approach, we have developed a tool\nimplementing it and conducted case studies with a real-life SAP ERP system.",
    "descriptor": "",
    "authors": [
      "Alessandro Berti",
      "Gyunam Park",
      "Majid Rafiei",
      "Wil van der Aalst"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.03467"
  },
  {
    "id": "arXiv:2110.03468",
    "title": "Belief Evolution Network: Probability Transformation of Basic Belief  Assignment and Fusion Conflict Probability",
    "abstract": "We give a new interpretation of basic belief assignment transformation into\nprobability distribution, and use directed acyclic network called belief\nevolution network to describe the causality between the focal elements of a\nBBA. On this basis, a new probability transformations method called full\ncausality probability transformation is proposed, and this method is superior\nto all previous method after verification from the process and the result. In\naddition, using this method combined with disjunctive combination rule, we\npropose a new probabilistic combination rule called disjunctive transformation\ncombination rule. It has an excellent ability to merge conflicts and an\ninteresting pseudo-Matthew effect, which offer a new idea to information fusion\nbesides the combination rule of Dempster.",
    "descriptor": "",
    "authors": [
      "Qianli Zhou",
      "Yusheng Huang",
      "Yong Deng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03468"
  },
  {
    "id": "arXiv:2110.03469",
    "title": "Federated Learning from Small Datasets",
    "abstract": "Federated learning allows multiple parties to collaboratively train a joint\nmodel without sharing local data. This enables applications of machine learning\nin settings of inherently distributed, undisclosable data such as in the\nmedical domain. In practice, joint training is usually achieved by aggregating\nlocal models, for which local training objectives have to be in expectation\nsimilar to the joint (global) objective. Often, however, local datasets are so\nsmall that local objectives differ greatly from the global objective, resulting\nin federated learning to fail. We propose a novel approach that intertwines\nmodel aggregations with permutations of local models. The permutations expose\neach local model to a daisy chain of local datasets resulting in more efficient\ntraining in data-sparse domains. This enables training on extremely small local\ndatasets, such as patient data across hospitals, while retaining the training\nefficiency and privacy benefits of federated learning.",
    "descriptor": "",
    "authors": [
      "Michael Kamp",
      "Jonas Fischer",
      "Jilles Vreeken"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.03469"
  },
  {
    "id": "arXiv:2110.03471",
    "title": "FaaSter Troubleshooting -- Evaluating Distributed Tracing Approaches for  Serverless Applications",
    "abstract": "Serverless applications can be particularly difficult to troubleshoot, as\nthese applications are often composed of various managed and partly managed\nservices. Faults are often unpredictable and can occur at multiple points, even\nin simple compositions. Each additional function or service in a serverless\ncomposition introduces a new possible fault source and a new layer to obfuscate\nfaults. Currently, serverless platforms offer only limited support for\nidentifying runtime faults. Developers looking to observe their serverless\ncompositions often have to rely on scattered logs and ambiguous error messages\nto pinpoint root causes. In this paper, we investigate the use of distributed\ntracing for improving the observability of faults in serverless applications.\nTo this end, we first introduce a model for characterizing fault observability,\nthen provide a prototypical tracing implementation - specifically, a\ndeveloper-driven and a platform-supported tracing approach. We compare both\napproaches with our model, measure associated trade-offs (execution latency,\nresource utilization), and contribute new insights for troubleshooting\nserverless compositions.",
    "descriptor": "\nComments: 2021 IEEE International Conference on Cloud Engineering (IC2E)\n",
    "authors": [
      "Maria C. Borges",
      "Sebastian Werner",
      "Ahmet Kilic"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.03471"
  },
  {
    "id": "arXiv:2110.03473",
    "title": "Unsupervised Image Decomposition with Phase-Correlation Networks",
    "abstract": "The ability to decompose scenes into their object components is a desired\nproperty for autonomous agents, allowing them to reason and act in their\nsurroundings. Recently, different methods have been proposed to learn\nobject-centric representations from data in an unsupervised manner. These\nmethods often rely on latent representations learned by deep neural networks,\nhence requiring high computational costs and large amounts of curated data.\nSuch models are also difficult to interpret. To address these challenges, we\npropose the Phase-Correlation Decomposition Network (PCDNet), a novel model\nthat decomposes a scene into its object components, which are represented as\ntransformed versions of a set of learned object prototypes. The core building\nblock in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the\nfrequency-domain representation of the images in order to estimate the\ntransformation between an object prototype and its transformed version in the\nimage. In our experiments, we show how PCDNet outperforms state-of-the-art\nmethods for unsupervised object discovery and segmentation on simple benchmark\ndatasets and on more challenging data, while using a small number of learnable\nparameters and being fully interpretable.",
    "descriptor": "",
    "authors": [
      "Angel Villar-Corrales",
      "Sven Behnke"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03473"
  },
  {
    "id": "arXiv:2110.03475",
    "title": "Workload-Aware Materialization of Junction Trees",
    "abstract": "Bayesian networks are popular probabilistic models that capture the\nconditional dependencies among a set of variables. Inference in Bayesian\nnetworks is a fundamental task for answering probabilistic queries over a\nsubset of variables in the data. However, exact inference in Bayesian networks\nis \\NP-hard, which has prompted the development of many practical inference\nmethods.\nIn this paper, we focus on improving the performance of the junction-tree\nalgorithm, a well-known method for exact inference in Bayesian networks. In\nparticular, we seek to leverage information in the workload of probabilistic\nqueries to obtain an optimal workload-aware materialization of junction trees,\nwith the aim to accelerate the processing of inference queries. We devise an\noptimal pseudo-polynomial algorithm to tackle this problem and discuss\napproximation schemes. Compared to state-of-the-art approaches for efficient\nprocessing of inference queries via junction trees, our methods are the first\nto exploit the information provided in query workloads. Our experimentation on\nseveral real-world Bayesian networks confirms the effectiveness of our\ntechniques in speeding-up query processing.",
    "descriptor": "",
    "authors": [
      "Martino Ciaperoni",
      "Cigdem Aslay",
      "Aristides Gionis",
      "Michael Mathioudakis"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2110.03475"
  },
  {
    "id": "arXiv:2110.03477",
    "title": "InfoSeg: Unsupervised Semantic Image Segmentation with Mutual  Information Maximization",
    "abstract": "We propose a novel method for unsupervised semantic image segmentation based\non mutual information maximization between local and global high-level image\nfeatures. The core idea of our work is to leverage recent progress in\nself-supervised image representation learning. Representation learning methods\ncompute a single high-level feature capturing an entire image. In contrast, we\ncompute multiple high-level features, each capturing image segments of one\nparticular semantic class. To this end, we propose a novel two-step learning\nprocedure comprising a segmentation and a mutual information maximization step.\nIn the first step, we segment images based on local and global features. In the\nsecond step, we maximize the mutual information between local features and\nhigh-level features of their respective class. For training, we provide solely\nunlabeled images and start from random network initialization. For quantitative\nand qualitative evaluation, we use established benchmarks, and COCO-Persons,\nwhereby we introduce the latter in this paper as a challenging novel benchmark.\nInfoSeg significantly outperforms the current state-of-the-art, e.g., we\nachieve a relative increase of 26% in the Pixel Accuracy metric on the\nCOCO-Stuff dataset.",
    "descriptor": "\nComments: GCPR 2021 - Best Paper\n",
    "authors": [
      "Robert Harb",
      "Patrick Kn\u00f6belreiter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03477"
  },
  {
    "id": "arXiv:2110.03478",
    "title": "Complex-valued deep learning with differential privacy",
    "abstract": "We present $\\zeta$-DP, an extension of differential privacy (DP) to\ncomplex-valued functions. After introducing the complex Gaussian mechanism,\nwhose properties we characterise in terms of $(\\varepsilon, \\delta)$-DP and\nR\\'enyi-DP, we present $\\zeta$-DP stochastic gradient descent ($\\zeta$-DP-SGD),\na variant of DP-SGD for training complex-valued neural networks. We\nexperimentally evaluate $\\zeta$-DP-SGD on three complex-valued tasks, i.e.\nelectrocardiogram classification, speech classification and magnetic resonance\nimaging (MRI) reconstruction. Moreover, we provide $\\zeta$-DP-SGD benchmarks\nfor a large variety of complex-valued activation functions and on a\ncomplex-valued variant of the MNIST dataset. Our experiments demonstrate that\nDP training of complex-valued neural networks is possible with rigorous privacy\nguarantees and excellent utility.",
    "descriptor": "\nComments: Submitted as conference paper to ICLR 2022\n",
    "authors": [
      "Alexander Ziller",
      "Dmitrii Usynin",
      "Moritz Knolle",
      "Kerstin Hammernik",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03478"
  },
  {
    "id": "arXiv:2110.03479",
    "title": "Camera Calibration through Camera Projection Loss",
    "abstract": "Camera calibration is a necessity in various tasks including 3D\nreconstruction, hand-eye coordination for a robotic interaction, autonomous\ndriving, etc. In this work we propose a novel method to predict extrinsic\n(baseline, pitch, and translation), intrinsic (focal length and principal point\noffset) parameters using an image pair. Unlike existing methods, instead of\ndesigning an end-to-end solution, we proposed a new representation that\nincorporates camera model equations as a neural network in multi-task learning\nframework. We estimate the desired parameters via novel \\emph{camera projection\nloss} (CPL) that uses the camera model neural network to reconstruct the 3D\npoints and uses the reconstruction loss to estimate the camera parameters. To\nthe best of our knowledge, ours is the first method to jointly estimate both\nthe intrinsic and extrinsic parameters via a multi-task learning methodology\nthat combines analytical equations in learning framework for the estimation of\ncamera parameters. We also proposed a novel dataset using CARLA Simulator.\nEmpirically, we demonstrate that our proposed approach achieves better\nperformance with respect to both deep learning-based and traditional methods on\n7 out of 10 parameters evaluated using both synthetic and real data. Our code\nand generated dataset will be made publicly available to facilitate future\nresearch.",
    "descriptor": "\nComments: 5 pages, ICASSP 2022\n",
    "authors": [
      "Talha Hanif Butt",
      "Murtaza Taj"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03479"
  },
  {
    "id": "arXiv:2110.03480",
    "title": "Learning to Regress Bodies from Images using Differentiable Semantic  Rendering",
    "abstract": "Learning to regress 3D human body shape and pose (e.g.~SMPL parameters) from\nmonocular images typically exploits losses on 2D keypoints, silhouettes, and/or\npart-segmentation when 3D training data is not available. Such losses, however,\nare limited because 2D keypoints do not supervise body shape and segmentations\nof people in clothing do not match projected minimally-clothed SMPL shapes. To\nexploit richer image information about clothed people, we introduce\nhigher-level semantic information about clothing to penalize clothed and\nnon-clothed regions of the image differently. To do so, we train a body\nregressor using a novel Differentiable Semantic Rendering - DSR loss. For\nMinimally-Clothed regions, we define the DSR-MC loss, which encourages a tight\nmatch between a rendered SMPL body and the minimally-clothed regions of the\nimage. For clothed regions, we define the DSR-C loss to encourage the rendered\nSMPL body to be inside the clothing mask. To ensure end-to-end differentiable\ntraining, we learn a semantic clothing prior for SMPL vertices from thousands\nof clothed human scans. We perform extensive qualitative and quantitative\nexperiments to evaluate the role of clothing semantics on the accuracy of 3D\nhuman pose and shape estimation. We outperform all previous state-of-the-art\nmethods on 3DPW and Human3.6M and obtain on par results on MPI-INF-3DHP. Code\nand trained models are available for research at https://dsr.is.tue.mpg.de/.",
    "descriptor": "\nComments: ICCV2021\n",
    "authors": [
      "Sai Kumar Dwivedi",
      "Nikos Athanasiou",
      "Muhammed Kocabas",
      "Michael J. Black"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03480"
  },
  {
    "id": "arXiv:2110.03484",
    "title": "Creating Training Sets via Weak Indirect Supervision",
    "abstract": "Creating labeled training sets has become one of the major roadblocks in\nmachine learning. To address this, recent Weak Supervision (WS) frameworks\nsynthesize training labels from multiple potentially noisy supervision sources.\nHowever, existing frameworks are restricted to supervision sources that share\nthe same output space as the target task. To extend the scope of usable\nsources, we formulate Weak Indirect Supervision (WIS), a new research problem\nfor automatically synthesizing training labels based on indirect supervision\nsources that have different output label spaces. To overcome the challenge of\nmismatched output spaces, we develop a probabilistic modeling approach, PLRM,\nwhich uses user-provided label relations to model and leverage indirect\nsupervision sources. Moreover, we provide a theoretically-principled test of\nthe distinguishability of PLRM for unseen labels, along with an generalization\nbound. On both image and text classification tasks as well as an industrial\nadvertising application, we demonstrate the advantages of PLRM by outperforming\nbaselines by a margin of 2%-9%.",
    "descriptor": "\nComments: 40 pages\n",
    "authors": [
      "Jieyu Zhang",
      "Bohan Wang",
      "Xiangchen Song",
      "Yujing Wang",
      "Yaming Yang",
      "Jing Bai",
      "Alexander Ratner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03484"
  },
  {
    "id": "arXiv:2110.03485",
    "title": "Cartoon Explanations of Image Classifiers",
    "abstract": "We present CartoonX (Cartoon Explanation), a novel model-agnostic explanation\nmethod tailored towards image classifiers and based on the rate-distortion\nexplanation (RDE) framework. Natural images are roughly piece-wise smooth\nsignals -- also called cartoon images -- and tend to be sparse in the wavelet\ndomain. CartoonX is the first explanation method to exploit this by requiring\nits explanations to be sparse in the wavelet domain, thus extracting the\n\\emph{relevant piece-wise smooth} part of an image instead of relevant\npixel-sparse regions. We demonstrate experimentally that CartoonX is not only\nhighly interpretable due to its piece-wise smooth nature but also particularly\napt at explaining misclassifications.",
    "descriptor": "",
    "authors": [
      "Stefan Kolek",
      "Duc Anh Nguyen",
      "Ron Levie",
      "Joan Bruna",
      "Gitta Kutyniok"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03485"
  },
  {
    "id": "arXiv:2110.03491",
    "title": "Privacy-preserving methods for smart-meter-based network simulations",
    "abstract": "Smart-meters are a key component of energy transition. The large amount of\ndata collected in near real-time allows grid operators to observe and simulate\nnetwork states. However, privacy-preserving rules forbid the use of such data\nfor any applications other than network operation and billing. Smart-meter\nmeasurements must be anonymised to transmit these sensitive data to a third\nparty to perform network simulation and analysis. This work proposes two\nmethods for data anonymisation that enable the use of raw active power\nmeasurements for network simulation and analysis. The first is based on an\nallocation of an externally sourced load database. The second consists of\ngrouping smart-meter data with similar electric characteristics, then\nperforming a random permutation of the network load-bus assignment. A benchmark\nof these two methods highlights that both provide similar results in\nbus-voltage magnitude estimation concerning ground-truth voltage.",
    "descriptor": "",
    "authors": [
      "Jordan Holweger",
      "Lionel Bloch",
      "Christophe Ballif",
      "Nicolas Wyrsch"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03491"
  },
  {
    "id": "arXiv:2110.03496",
    "title": "Scale Invariant Domain Generalization Image Recapture Detection",
    "abstract": "Recapturing and rebroadcasting of images are common attack methods in\ninsurance frauds and face identification spoofing, and an increasing number of\ndetection techniques were introduced to handle this problem. However, most of\nthem ignored the domain generalization scenario and scale variances, with an\ninferior performance on domain shift situations, and normally were exacerbated\nby intra-domain and inter-domain scale variances. In this paper, we propose a\nscale alignment domain generalization framework (SADG) to address these\nchallenges. First, an adversarial domain discriminator is exploited to minimize\nthe discrepancies of image representation distributions among different\ndomains. Meanwhile, we exploit triplet loss as a local constraint to achieve a\nclearer decision boundary. Moreover, a scale alignment loss is introduced as a\nglobal relationship regularization to force the image representations of the\nsame class across different scales to be undistinguishable. Experimental\nresults on four databases and comparison with state-of-the-art approaches show\nthat better performance can be achieved using our framework.",
    "descriptor": "",
    "authors": [
      "Jinian Luo",
      "Jie Guo",
      "Weidong Qiu",
      "Zheng Huang",
      "Hong Hui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03496"
  },
  {
    "id": "arXiv:2110.03498",
    "title": "On the relationship between disentanglement and multi-task learning",
    "abstract": "One of the main arguments behind studying disentangled representations is the\nassumption that they can be easily reused in different tasks. At the same time\nfinding a joint, adaptable representation of data is one of the key challenges\nin the multi-task learning setting. In this paper, we take a closer look at the\nrelationship between disentanglement and multi-task learning based on hard\nparameter sharing. We perform a thorough empirical study of the representations\nobtained by neural networks trained on automatically generated supervised\ntasks. Using a set of standard metrics we show that disentanglement appears\nnaturally during the process of multi-task neural network training.",
    "descriptor": "",
    "authors": [
      "\u0141ukasz Maziarka",
      "Aleksandra Nowak",
      "Maciej Wo\u0142czyk",
      "Andrzej Bedychaj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03498"
  },
  {
    "id": "arXiv:2110.03503",
    "title": "Cantilevered, Rectangular Plate Dynamics by Finite Difference Methods",
    "abstract": "In this technical note, we consider a dynamic linear, cantilevered\nrectangular plate. The evolutionary PDE model is given by the fourth order\nplate dynamics (via the spatial biharmonic operator) with\nclamped-free-free-free boundary conditions. We additionally consider\ndamping/dissipation terms, as well as non-conservative lower order terms\narising in various applications. Dynamical numerical simulations are achieved\nby way of a finite difference spatial approximation with a MATLAB time\nintegrator. The rectangular geometry allows the use of standard 2D spatial\nfinite differences, while the high spatial order of the problem and mixed\nclamped-free type boundary conditions present challenges. Dynamic energies are\nalso computed. The relevant code is presented, with discussion of the model and\ncontext.",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Benjamin Brown"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03503"
  },
  {
    "id": "arXiv:2110.03504",
    "title": "Mandarin-English Code-switching Speech Recognition with Self-supervised  Speech Representation Models",
    "abstract": "Code-switching (CS) is common in daily conversations where more than one\nlanguage is used within a sentence. The difficulties of CS speech recognition\nlie in alternating languages and the lack of transcribed data. Therefore, this\npaper uses the recently successful self-supervised learning (SSL) methods to\nleverage many unlabeled speech data without CS. We show that hidden\nrepresentations of SSL models offer frame-level language identity even if the\nmodels are trained with English speech only. Jointly training CTC and language\nidentification modules with self-supervised speech representations improves CS\nspeech recognition performance. Furthermore, using multilingual speech data for\npre-training obtains the best CS speech recognition.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Liang-Hsuan Tseng",
      "Yu-Kuan Fu",
      "Heng-Jui Chang",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03504"
  },
  {
    "id": "arXiv:2110.03506",
    "title": "Run Time Assurance for Safety-Critical Systems: An Introduction to  Safety Filtering Approaches for Complex Control Systems",
    "abstract": "Run Time Assurance (RTA) Systems are online verification mechanisms that\nfilter an unverified primary controller output to ensure system safety. The\nprimary control may come from a human operator, an advanced control approach,\nor an autonomous control approach that cannot be verified to the same level as\nsimpler control systems designs. The critical feature of RTA systems is their\nability to alter unsafe control inputs explicitly to assure safety. In many\ncases, RTA systems can functionally be described as containing a monitor that\nwatches the state of the system and output of a primary controller, and a\nbackup controller that replaces or modifies control input when necessary to\nassure safety. An important quality of an RTA system is that the assurance\nmechanism is constructed in a way that is entirely agnostic to the underlying\nstructure of the primary controller. By effectively decoupling the enforcement\nof safety constraints from performance-related objectives, RTA offers a number\nof useful advantages over traditional (offline) verification. This article\nprovides a tutorial on developing RTA systems.",
    "descriptor": "",
    "authors": [
      "Kerianne Hobbs",
      "Mark Mote",
      "Matthew Abate",
      "Samuel Coogan",
      "Eric Feron"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03506"
  },
  {
    "id": "arXiv:2110.03515",
    "title": "Use of Deterministic Transforms to Design Weight Matrices of a Neural  Network",
    "abstract": "Self size-estimating feedforward network (SSFN) is a feedforward multilayer\nnetwork. For the existing SSFN, a part of each weight matrix is trained using a\nlayer-wise convex optimization approach (a supervised training), while the\nother part is chosen as a random matrix instance (an unsupervised training). In\nthis article, the use of deterministic transforms instead of random matrix\ninstances for the SSFN weight matrices is explored. The use of deterministic\ntransforms provides a reduction in computational complexity. The use of several\ndeterministic transforms is investigated, such as discrete cosine transform,\nHadamard transform, Hartley transform, and wavelet transforms. The choice of a\ndeterministic transform among a set of transforms is made in an unsupervised\nmanner. To this end, two methods based on features' statistical parameters are\ndeveloped. The proposed methods help to design a neural net where deterministic\ntransforms can vary across its layers' weight matrices. The effectiveness of\nthe proposed approach vis-a-vis the SSFN is illustrated for object\nclassification tasks using several benchmark datasets.",
    "descriptor": "\nComments: Accepted to the 29th European Signal Processing Conference, EUSIPCO 2021, Dublin, Ireland\n",
    "authors": [
      "Pol Grau Jurado",
      "Xinyue Liang",
      "Alireza M. Javid",
      "Saikat Chatterjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03515"
  },
  {
    "id": "arXiv:2110.03522",
    "title": "Surrogate-Based Black-Box Optimization Method for Costly Molecular  Properties",
    "abstract": "AI-assisted molecular optimization is a very active research field as it is\nexpected to provide the next-generation drugs and molecular materials. An\nimportant difficulty is that the properties to be optimized rely on costly\nevaluations. Machine learning methods are investigated with success to predict\nthese properties, but show generalization issues on less known areas of the\nchemical space. We propose here a surrogate-based black box optimization\nmethod, to tackle jointly the optimization and machine learning problems. It\nconsists in optimizing the expected improvement of the surrogate of a molecular\nproperty using an evolutionary algorithm. The surrogate is defined as a\nGaussian Process Regression (GPR) model, learned on a relevant area of the\nsearch space with respect to the property to be optimized. We show that our\napproach can successfully optimize a costly property of interest much faster\nthan a purely metaheuristic approach.",
    "descriptor": "\nComments: Submitted to ICTAI 2021\n",
    "authors": [
      "Jules Leguy",
      "Thomas Cauchy",
      "Beatrice Duval",
      "Benoit Da Mota"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03522"
  },
  {
    "id": "arXiv:2110.03524",
    "title": "Data-Driven Methods for Balancing Fairness and Efficiency in  Ride-Pooling",
    "abstract": "Rideshare and ride-pooling platforms use artificial intelligence-based\nmatching algorithms to pair riders and drivers. However, these platforms can\ninduce inequality either through an unequal income distribution or disparate\ntreatment of riders. We investigate two methods to reduce forms of inequality\nin ride-pooling platforms: (1) incorporating fairness constraints into the\nobjective function and (2) redistributing income to drivers to reduce income\nfluctuation and inequality. To evaluate our solutions, we use the New York City\ntaxi data set. For the first method, we find that optimizing for driver-side\nfairness outperforms state-of-the-art models on the number of riders serviced,\nboth in the worst-off neighborhood and overall, showing that optimizing for\nfairness can assist profitability in certain circumstances. For the second\nmethod, we explore income redistribution as a way to combat income inequality\nby having drivers keep an $r$ fraction of their income, and contributing the\nrest to a redistribution pool. For certain values of $r$, most drivers earn\nnear their Shapley value, while still incentivizing drivers to maximize value,\nthereby avoiding the free-rider problem and reducing income variability. The\nfirst method can be extended to many definitions of fairness and the second\nmethod provably improves fairness without affecting profitability.",
    "descriptor": "",
    "authors": [
      "Naveen Raman",
      "Sanket Shah",
      "John Dickerson"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03524"
  },
  {
    "id": "arXiv:2110.03536",
    "title": "Prototype Learning for Interpretable Respiratory Sound Analysis",
    "abstract": "Remote screening of respiratory diseases has been widely studied as a\nnon-invasive and early instrument for diagnosis purposes, especially in the\npandemic. The respiratory sound classification task has been realized with\nnumerous deep neural network (DNN) models due to their superior performance.\nHowever, in the high-stake medical domain where decisions can have significant\nconsequences, it is desirable to develop interpretable models; thus, providing\nunderstandable reasons for physicians and patients. To address the issue, we\npropose a prototype learning framework, that jointly generates exemplar samples\nfor explanation and integrates these samples into a layer of DNNs. The\nexperimental results indicate that our method outperforms the state-of-the-art\napproaches on the largest public respiratory sound database.",
    "descriptor": "\nComments: submitted to IEEE ICASSP 2022\n",
    "authors": [
      "Zhao Ren",
      "Thanh Tam Nguyen",
      "Wolfgang Nejdl"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03536"
  },
  {
    "id": "arXiv:2110.03537",
    "title": "A Sidelink-Aided Approach for Secure Multicast Service Delivery: from  Human-Oriented Multimedia Traffic to Machine Type Communications",
    "abstract": "To date, group-oriented communications have been mainly exploited for\ndelivering multimedia services in human-oriented communications while, in\nfuture fifth generation (5G) cellular networks, objects will be the main\ntarget. Internet of Things (IoT) will undoubtedly play a key role in 5G\nnetworks, wherein massive machine-type communications (mMTC) feature a use case\nas crucial as challenging since cellular IoT connections are predicted to grow\nheavily in the next future. To boost capacity and energy efficiency, the 5G\nnetwork can leverage device-to-device (D2D) communications which are recognized\nas an effective offloading technique. This is achieved thanks to the fact that,\nin legacy D2D communications, data are directly sent from one device to\nanother, avoiding the crossing of the network. Obviously, the distributed\nnature of such a communication paradigm and the inherent broadcast nature of\nthe wireless channel make it necessary to think how to secure the so called\n\"sidelink\" transmissions. This work proposes a protocol for the efficient and\nreliable management of multicast services in a 5G-oriented IoT scenario, in\nwhich security is a crucial requirement to be met. The proposed protocol is\ntailored to Narrowband IoT (NB-IoT) and makes use of D2D communications with\nthe aim of improving network efficiency and optimizing network resource\nutilization. In addition, cyber security and social trustworthiness mechanisms\nare exploited to secure D2D communications.",
    "descriptor": "",
    "authors": [
      "S. Pizzi",
      "C. Suraci",
      "A. Iera",
      "A. Molinaro",
      "G. Araniti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.03537"
  },
  {
    "id": "arXiv:2110.03540",
    "title": "A Broad Ensemble Learning System for Drifting Stream Classification",
    "abstract": "Data stream classification has become a major research topic due to the\nincrease in temporal data. One of the biggest hurdles of data stream\nclassification is the development of algorithms that deal with evolving data,\nalso known as concept drifts. As data changes over time, static prediction\nmodels lose their validity. Adapting to concept drifts provides more robust and\nbetter performing models. The Broad Learning System (BLS) is an effective broad\nneural architecture recently developed for incremental learning. BLS cannot\nprovide instant response since it requires huge data chunks and is unable to\nhandle concept drifts. We propose a Broad Ensemble Learning System (BELS) for\nstream classification with concept drift. BELS uses a novel updating method\nthat greatly improves best-in-class model accuracy. It employs a dynamic output\nensemble layer to address the limitations of BLS. We present its mathematical\nderivation, provide comprehensive experiments with 11 datasets that demonstrate\nthe adaptability of our model, including a comparison of our model with BLS,\nand provide parameter and robustness analysis on several drifting streams,\nshowing that it statistically significantly outperforms seven state-of-the-art\nbaselines. We show that our proposed method improves on average 44% compared to\nBLS, and 29% compared to other competitive baselines.",
    "descriptor": "\nComments: Submitted to IEEE TKDE\n",
    "authors": [
      "Sepehr Bakhshi",
      "Pouya Ghahramanian",
      "Hamed Bonab",
      "Fazli Can"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03540"
  },
  {
    "id": "arXiv:2110.03542",
    "title": "A novel approach for MBSFN Area Formation aided by D2D Communications  for eMBB Service Delivery in 5G NR Systems",
    "abstract": "Forthcoming 5G New Radio (NR) systems will be asked to handle a huge number\nof devices accessing or delivering \"resource-hungry\" and high-quality services.\nIn view of this, the new 5G Radio Access Technology (RAT) aims to support, in\nnext releases, Multimedia Broadcast/Multicast Service Single Frequency Network\n(MBSFN) to enable the simultaneous delivery of the same content to a set of\nusers covered by different cells. According to MBSFN, all cells belonging to\nthe same MBSFN Area are synchronized in time and the MBSFN transmission occurs\nover the same radio resources. In such a way, the same content flow is\ndelivered by several cells to all the receivers in the MBSFN Area. A further\nmeans to enhance the network coverage and provide high data rate and low\nlatency in future 5G-enabled MBSFN networks is Device-to-Device (D2D)\nconnectivity. Along these lines, in this paper we propose a D2D-aided MBSFN\nArea Formation (D2D-MAF) algorithm to dynamically create MBSFN Areas with the\naim to improve the system aggregate data rate while satisfying all user\nrequests. The proposed D2D-MAF foresees that users could receive the service\nthrough either MBSFN, or D2D, or unicast transmissions. Performance evaluation\nresults, carried out under a wide range of conditions, testify to the high\neffectiveness of the proposed algorithm.",
    "descriptor": "",
    "authors": [
      "Federica Rinaldi",
      "Sara Pizzi",
      "Antonino Orsino",
      "Antonio Iera",
      "Antonella Molinaro",
      "Giuseppe Araniti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.03542"
  },
  {
    "id": "arXiv:2110.03544",
    "title": "RAR: Region-Aware Point Cloud Registration",
    "abstract": "This paper concerns the research problem of point cloud registration to find\nthe rigid transformation to optimally align the source point set with the\ntarget one. Learning robust point cloud registration models with deep neural\nnetworks has emerged as a powerful paradigm, offering promising performance in\npredicting the global geometric transformation for a pair of point sets.\nExisting methods firstly leverage an encoder to regress a latent shape\nembedding, which is then decoded into a shape-conditioned transformation via\nconcatenation-based conditioning. However, different regions of a 3D shape vary\nin their geometric structures which makes it more sense that we have a\nregion-conditioned transformation instead of the shape-conditioned one. In this\npaper we present a \\underline{R}egion-\\underline{A}ware point cloud\n\\underline{R}egistration, denoted as RAR, to predict transformation for\npairwise point sets in the self-supervised learning fashion. More specifically,\nwe develop a novel region-aware decoder (RAD) module that is formed with an\nimplicit neural region representation parameterized by neural networks. The\nimplicit neural region representation is learned with a self-supervised 3D\nshape reconstruction loss without the need for region labels. Consequently, the\nregion-aware decoder (RAD) module guides the training of the region-aware\ntransformation (RAT) module and region-aware weight (RAW) module, which predict\nthe transforms and weights for different regions respectively. The global\ngeometric transformation from source point set to target one is then formed by\nthe weighted fusion of region-aware transforms. Compared to the\nstate-of-the-art approaches, our experiments show that our RAR achieves\nsuperior registration performance over various benchmark datasets (e.g.\nModelNet40).",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2006.06200\n",
    "authors": [
      "Yu Hao",
      "Yi Fang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03544"
  },
  {
    "id": "arXiv:2110.03545",
    "title": "Privacy-Preserving Coded Mobile Edge Computing for Low-Latency  Distributed Inference",
    "abstract": "We consider a mobile edge computing scenario where a number of devices want\nto perform a linear inference $\\boldsymbol{W}\\boldsymbol{x}$ on some local data\n$\\boldsymbol{x}$ given a network-side matrix $\\boldsymbol{W}$. The computation\nis performed at the network edge over a number of edge servers. We propose a\ncoding scheme that provides information-theoretic privacy against $z$ colluding\n(honest-but-curious) edge servers, while minimizing the overall\nlatency\\textemdash comprising upload, computation, download, and decoding\nlatency\\textemdash in the presence of straggling servers. The proposed scheme\nexploits Shamir's secret sharing to yield data privacy and straggler\nmitigation, combined with replication to provide spatial diversity for the\ndownload. We also propose two variants of the scheme that further reduce\nlatency. For a considered scenario with $9$ edge servers, the proposed scheme\nreduces the latency by $8\\%$ compared to the nonprivate scheme recently\nintroduced by Zhang and Simeone, while providing privacy against an\nhonest-but-curious edge server.",
    "descriptor": "\nComments: 12 pages, 6 figures, submitted for publication in the Journal on Selected Areas in Communications\n",
    "authors": [
      "Reent Schlegel",
      "Siddhartha Kumar",
      "Eirik Rosnes",
      "Alexandre Graell i Amat"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.03545"
  },
  {
    "id": "arXiv:2110.03546",
    "title": "mRAT-SQL+GAP:A Portuguese Text-to-SQL Transformer",
    "abstract": "The translation of natural language questions to SQL queries has attracted\ngrowing attention, in particular in connection with transformers and similar\nlanguage models. A large number of techniques are geared towards the English\nlanguage; in this work, we thus investigated translation to SQL when input\nquestions are given in the Portuguese language. To do so, we properly adapted\nstate-of-the-art tools and resources. We changed the RAT-SQL+GAP system by\nrelying on a multilingual BART model (we report tests with other language\nmodels), and we produced a translated version of the Spider dataset. Our\nexperiments expose interesting phenomena that arise when non-English languages\nare targeted; in particular, it is better to train with original and translated\ntraining datasets together, even if a single target language is desired. This\nmultilingual BART model fine-tuned with a double-size training dataset (English\nand Portuguese) achieved 83% of the baseline, making inferences for the\nPortuguese test dataset. This investigation can help other researchers to\nproduce results in Machine Learning in a language different from English. Our\nmultilingual ready version of RAT-SQL+GAP and the data are available,\nopen-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql",
    "descriptor": "\nComments: Accepted to BRACIS 2021\n",
    "authors": [
      "Marcelo Archanjo Jos\u00e9",
      "Fabio Gagliardi Cozman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03546"
  },
  {
    "id": "arXiv:2110.03549",
    "title": "Bias-Variance Tradeoffs in Single-Sample Binary Gradient Estimators",
    "abstract": "Discrete and especially binary random variables occur in many machine\nlearning models, notably in variational autoencoders with binary latent states\nand in stochastic binary networks. When learning such models, a key tool is an\nestimator of the gradient of the expected loss with respect to the\nprobabilities of binary variables. The straight-through (ST) estimator gained\npopularity due to its simplicity and efficiency, in particular in deep networks\nwhere unbiased estimators are impractical. Several techniques were proposed to\nimprove over ST while keeping the same low computational complexity:\nGumbel-Softmax, ST-Gumbel-Softmax, BayesBiNN, FouST. We conduct a theoretical\nanalysis of Bias and Variance of these methods in order to understand tradeoffs\nand verify the originally claimed properties. The presented theoretical results\nare mainly negative, showing limitations of these methods and in some cases\nrevealing serious issues.",
    "descriptor": "\nComments: 21 pages, GCPR 2021\n",
    "authors": [
      "Alexander Shekhovtsov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.03549"
  },
  {
    "id": "arXiv:2110.03551",
    "title": "Formalizing Geometric Algebra in Lean",
    "abstract": "This paper explores formalizing Geometric (or Clifford) algebras into the\nLean 3 theorem prover, building upon the substantial body of work that is the\nLean mathematics library, mathlib. As we use Lean source code to demonstrate\nmany of our ideas, we include a brief introduction to the Lean language\ntargeted at a reader with no prior experience with Lean or theorem provers in\ngeneral.\nWe formalize the multivectors as the quotient of the tensor algebra by a\nsuitable relation, which provides the ring structure automatically, then go on\nto establish the universal property of the Clifford algebra. We show that this\nis quite different to the approach taken by existing formalizations of\nGeometric algebra in other theorem provers; most notably, our approach does not\nrequire a choice of basis.\nWe go on to show how operations and structure such as involutions, versors,\nand the $\\mathbb{Z}_2$-grading can be defined using the universal property\nalone, and how to recover an induction principle from the universal property\nsuitable for proving statements about these definitions. We outline the steps\nneeded to formalize the wedge product and $\\mathbb{N}$-grading, and some of the\ngaps in mathlib that currently make this challenging.",
    "descriptor": "\nComments: 23 pages. This is the version prior to peer review submitted to \"Advances in Applied Clifford Algebras\" on 2021-03-17, as part of the topical collection for ICCA 12\n",
    "authors": [
      "Eric Wieser",
      "Utensil Song"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2110.03551"
  },
  {
    "id": "arXiv:2110.03553",
    "title": "Shift-BNN: Highly-Efficient Probabilistic Bayesian Neural Network  Training via Memory-Friendly Pattern Retrieving",
    "abstract": "Bayesian Neural Networks (BNNs) that possess a property of uncertainty\nestimation have been increasingly adopted in a wide range of safety-critical AI\napplications which demand reliable and robust decision making, e.g.,\nself-driving, rescue robots, medical image diagnosis. The training procedure of\na probabilistic BNN model involves training an ensemble of sampled DNN models,\nwhich induces orders of magnitude larger volume of data movement than training\na single DNN model. In this paper, we reveal that the root cause for BNN\ntraining inefficiency originates from the massive off-chip data transfer by\nGaussian Random Variables (GRVs). To tackle this challenge, we propose a novel\ndesign that eliminates all the off-chip data transfer by GRVs through the\nreversed shifting of Linear Feedback Shift Registers (LFSRs) without incurring\nany training accuracy loss. To efficiently support our LFSR reversion strategy\nat the hardware level, we explore the design space of the current DNN\naccelerators and identify the optimal computation mapping scheme to best\naccommodate our strategy. By leveraging this finding, we design and prototype\nthe first highly efficient BNN training accelerator, named Shift-BNN, that is\nlow-cost and scalable. Extensive evaluation on five representative BNN models\ndemonstrates that Shift-BNN achieves an average of 4.9x (up to 10.8x) boost in\nenergy efficiency and 1.6x (up to 2.8x) speedup over the baseline DNN training\naccelerator.",
    "descriptor": "\nComments: 54th IEEE/ACM International Symposium on Microarchitecture\n",
    "authors": [
      "Qiyu Wan",
      "Haojun Xia",
      "Xingyao Zhang",
      "Lening Wang",
      "Shuaiwen Leon Song",
      "Xin Fu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03553"
  },
  {
    "id": "arXiv:2110.03555",
    "title": "Active Extrinsic Contact Sensing: Application to General Peg-in-Hole  Insertion",
    "abstract": "We propose a method that actively estimates the location of contact between a\ngrasped rigid object and its environment, and uses this as input to a\npeg-in-hole insertion policy. An estimation model and an active tactile\nfeedback controller work collaboratively to get an accurate estimate of the\nexternal contacts. The controller helps the estimation model get a better\nestimate by regulating a consistent contact mode. The better estimation makes\nit easier for the controller to regulate the contact. To utilize this contact\nestimate, we train an object-agnostic insertion policy that learns to make use\nof the series of contact estimates to guide the insertion. In contrast with\nprevious works that learn a policy directly from tactile signals, since this\npolicy is in contact configuration space, can be learned directly in\nsimulation. Lastly, we demonstrate and evaluate the active extrinsic contact\nline estimation and the trained insertion policy together in a real experiment.\nWe show that the proposed method inserts various-shaped test objects with\nhigher success rates and fewer insertion attempts than previous work with\nend-to-end approaches. See supplementary video and results at\nhttps://sites.google.com/view/active-extrinsic-contact.",
    "descriptor": "",
    "authors": [
      "Sangwoon Kim",
      "Alberto Rodriguez"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03555"
  },
  {
    "id": "arXiv:2110.03560",
    "title": "Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0",
    "abstract": "We propose a simple and effective cross-lingual transfer learning method to\nadapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in\nresource-scarce languages. We show that a monolingual wav2vec-2.0 is a good\nfew-shot ASR learner in several languages. We improve its performance further\nvia several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by\nusing a moderate-sized unlabeled speech dataset in the target language. A key\nfinding of this work is that the adapted monolingual wav2vec-2.0 achieves\nsimilar performance as the topline multilingual XLSR model, which is trained on\nfifty-three languages, on the target language ASR task.",
    "descriptor": "",
    "authors": [
      "Sameer Khurana",
      "Antoine Laurent",
      "James Glass"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03560"
  },
  {
    "id": "arXiv:2110.03562",
    "title": "Weakly Supervised Human-Object Interaction Detection in Video via  Contrastive Spatiotemporal Regions",
    "abstract": "We introduce the task of weakly supervised learning for detecting human and\nobject interactions in videos. Our task poses unique challenges as a system\ndoes not know what types of human-object interactions are present in a video or\nthe actual spatiotemporal location of the human and the object. To address\nthese challenges, we introduce a contrastive weakly supervised training loss\nthat aims to jointly associate spatiotemporal regions in a video with an action\nand object vocabulary and encourage temporal continuity of the visual\nappearance of moving objects as a form of self-supervision. To train our model,\nwe introduce a dataset comprising over 6.5k videos with human-object\ninteraction annotations that have been semi-automatically curated from sentence\ncaptions associated with the videos. We demonstrate improved performance over\nweakly supervised baselines adapted to our task on our video dataset.",
    "descriptor": "",
    "authors": [
      "Shuang Li",
      "Yilun Du",
      "Antonio Torralba",
      "Josef Sivic",
      "Bryan Russell"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03562"
  },
  {
    "id": "arXiv:2110.03567",
    "title": "GeSERA: General-domain Summary Evaluation by Relevance Analysis",
    "abstract": "We present GeSERA, an open-source improved version of SERA for evaluating\nautomatic extractive and abstractive summaries from the general domain. SERA is\nbased on a search engine that compares candidate and reference summaries\n(called queries) against an information retrieval document base (called index).\nSERA was originally designed for the biomedical domain only, where it showed a\nbetter correlation with manual methods than the widely used lexical-based ROUGE\nmethod. In this paper, we take out SERA from the biomedical domain to the\ngeneral one by adapting its content-based method to successfully evaluate\nsummaries from the general domain. First, we improve the query reformulation\nstrategy with POS Tags analysis of general-domain corpora. Second, we replace\nthe biomedical index used in SERA with two article collections from AQUAINT-2\nand Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM\ndatasets. Results show that, in most cases, GeSERA achieves higher correlations\nwith manual evaluation methods than SERA, while it reduces its gap with ROUGE\nfor general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases\nof TAC2009. Finally, we conduct extensive experiments and provide a\ncomprehensive study of the impact of human annotators and the index size on\nsummary evaluation with SERA and GeSERA.",
    "descriptor": "\nComments: Accepted in RANLP 2021 conference\n",
    "authors": [
      "Jessica L\u00f3pez Espejel",
      "Ga\u00ebl de Chalendar",
      "Jorge Garcia Flores",
      "Thierry Charnois",
      "Ivan Vladimir Meza Ruiz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.03567"
  },
  {
    "id": "arXiv:2110.03569",
    "title": "Human in the Loop for Machine Creativity",
    "abstract": "Artificial intelligence (AI) is increasingly utilized in synthesizing\nvisuals, texts, and audio. These AI-based works, often derived from neural\nnetworks, are entering the mainstream market, as digital paintings, songs,\nbooks, and others. We conceptualize both existing and future human-in-the-loop\n(HITL) approaches for creative applications and to develop more expressive,\nnuanced, and multimodal models. Particularly, how can our expertise as curators\nand collaborators be encoded in AI models in an interactive manner? We examine\nand speculate on long term implications for models, interfaces, and machine\ncreativity. Our selection, creation, and interpretation of AI art inherently\ncontain our emotional responses, cultures, and contexts. Therefore, the\nproposed HITL may help algorithms to learn creative processes that are much\nharder to codify or quantify. We envision multimodal HITL processes, where\ntexts, visuals, sounds, and other information are coupled together, with\nautomated analysis of humans and environments. Overall, these HITL approaches\nwill increase interaction between human and AI, and thus help the future AI\nsystems to better understand our own creative and emotional processes.",
    "descriptor": "\nComments: 9th AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2021), Blue Sky Ideas track\n",
    "authors": [
      "Neo Christopher Chung"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03569"
  },
  {
    "id": "arXiv:2110.03572",
    "title": "Bridge to Target Domain by Prototypical Contrastive Learning and Label  Confusion: Re-explore Zero-Shot Learning for Slot Filling",
    "abstract": "Zero-shot cross-domain slot filling alleviates the data dependence in the\ncase of data scarcity in the target domain, which has aroused extensive\nresearch. However, as most of the existing methods do not achieve effective\nknowledge transfer to the target domain, they just fit the distribution of the\nseen slot and show poor performance on unseen slot in the target domain. To\nsolve this, we propose a novel approach based on prototypical contrastive\nlearning with a dynamic label confusion strategy for zero-shot slot filling.\nThe prototypical contrastive learning aims to reconstruct the semantic\nconstraints of labels, and we introduce the label confusion strategy to\nestablish the label dependence between the source domains and the target domain\non-the-fly. Experimental results show that our model achieves significant\nimprovement on the unseen slots, while also set new state-of-the-arts on slot\nfilling task.",
    "descriptor": "\nComments: Accepted by EMNLP 2021\n",
    "authors": [
      "Liwen Wang",
      "Xuefeng Li",
      "Jiachi Liu",
      "Keqing He",
      "Yuanmeng Yan",
      "Weiran Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03572"
  },
  {
    "id": "arXiv:2110.03574",
    "title": "A New Simple Vision Algorithm for Detecting the Enzymic Browning Defects  in Golden Delicious Apples",
    "abstract": "In this work, a simple vision algorithm is designed and implemented to\nextract and identify the surface defects on the Golden Delicious apples caused\nby the enzymic browning process. 34 Golden Delicious apples were selected for\nthe experiments, of which 17 had enzymic browning defects and the other 17 were\nsound. The image processing part of the proposed vision algorithm extracted the\ndefective surface area of the apples with high accuracy of 97.15%. The area and\nmean of the segmented images were selected as the 2x1 feature vectors to feed\ninto a designed artificial neural network. The analysis based on the above\nfeatures indicated that the images with a mean less than 0.0065 did not belong\nto the defective apples; rather, they were extracted as part of the calyx and\nstem of the healthy apples. The classification accuracy of the neural network\napplied in this study was 99.19%",
    "descriptor": "\nComments: 23 pages, 11 figures, Conference\n",
    "authors": [
      "Hamid Majidi Balanji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03574"
  },
  {
    "id": "arXiv:2110.03575",
    "title": "Estimating Image Depth in the Comics Domain",
    "abstract": "Estimating the depth of comics images is challenging as such images a) are\nmonocular; b) lack ground-truth depth annotations; c) differ across different\nartistic styles; d) are sparse and noisy. We thus, use an off-the-shelf\nunsupervised image to image translation method to translate the comics images\nto natural ones and then use an attention-guided monocular depth estimator to\npredict their depth. This lets us leverage the depth annotations of existing\nnatural images to train the depth estimator. Furthermore, our model learns to\ndistinguish between text and images in the comics panels to reduce text-based\nartefacts in the depth estimates. Our method consistently outperforms the\nexisting state-ofthe-art approaches across all metrics on both the DCM and\neBDtheque images. Finally, we introduce a dataset to evaluate depth prediction\non comics.",
    "descriptor": "\nComments: WACV 2022 : Winter Conference on Applications of Computer Vision\n",
    "authors": [
      "Deblina Bhattacharjee",
      "Martin Everaert",
      "Mathieu Salzmann",
      "Sabine S\u00fcsstrunk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03575"
  },
  {
    "id": "arXiv:2110.03576",
    "title": "Training Stable Graph Neural Networks Through Constrained Learning",
    "abstract": "Graph Neural Networks (GNN) rely on graph convolutions to learn features from\nnetwork data. GNNs are stable to different types of perturbations of the\nunderlying graph, a property that they inherit from graph filters. In this\npaper we leverage the stability property of GNNs as a typing point in order to\nseek for representations that are stable within a distribution. We propose a\nnovel constrained learning approach by imposing a constraint on the stability\ncondition of the GNN within a perturbation of choice. We showcase our framework\nin real world data, corroborating that we are able to obtain more stable\nrepresentations while not compromising the overall accuracy of the predictor.",
    "descriptor": "",
    "authors": [
      "Juan Cervino",
      "Luana Ruiz",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03576"
  },
  {
    "id": "arXiv:2110.03578",
    "title": "Towards Accurate Cross-Domain In-Bed Human Pose Estimation",
    "abstract": "Human behavioral monitoring during sleep is essential for various medical\napplications. Majority of the contactless human pose estimation algorithms are\nbased on RGB modality, causing ineffectiveness in in-bed pose estimation due to\nocclusions by blankets and varying illumination conditions. Long-wavelength\ninfrared (LWIR) modality based pose estimation algorithms overcome the\naforementioned challenges; however, ground truth pose generations by a human\nannotator under such conditions are not feasible. A feasible solution to\naddress this issue is to transfer the knowledge learned from images with pose\nlabels and no occlusions, and adapt it towards real world conditions\n(occlusions due to blankets). In this paper, we propose a novel learning\nstrategy comprises of two-fold data augmentation to reduce the cross-domain\ndiscrepancy and knowledge distillation to learn the distribution of unlabeled\nimages in real world conditions. Our experiments and analysis show the\neffectiveness of our approach over multiple standard human pose estimation\nbaselines.",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Mohamed Afham",
      "Udith Haputhanthri",
      "Jathurshan Pradeepkumar",
      "Mithunjha Anandakumar",
      "Ashwin De Silva",
      "Chamira Edussooriya"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03578"
  },
  {
    "id": "arXiv:2110.03580",
    "title": "A Model Selection Approach for Corruption Robust Reinforcement Learning",
    "abstract": "We develop a model selection approach to tackle reinforcement learning with\nadversarial corruption in both transition and reward. For finite-horizon\ntabular MDPs, without prior knowledge on the total amount of corruption, our\nalgorithm achieves a regret bound of\n$\\widetilde{\\mathcal{O}}(\\min\\{\\frac{1}{\\Delta}, \\sqrt{T}\\}+C)$ where $T$ is\nthe number of episodes, $C$ is the total amount of corruption, and $\\Delta$ is\nthe reward gap between the best and the second-best policy. This is the first\nworst-case optimal bound achieved without knowledge of $C$, improving previous\nresults of Lykouris et al. (2021); Chen et al. (2021); Wu et al. (2021). For\nfinite-horizon linear MDPs, we develop a computationally efficient algorithm\nwith a regret bound of $\\widetilde{\\mathcal{O}}(\\sqrt{(1+C)T})$, and another\ncomputationally inefficient one with $\\widetilde{\\mathcal{O}}(\\sqrt{T}+C)$,\nimproving the result of Lykouris et al. (2021) and answering an open question\nby Zhang et al. (2021b). Finally, our model selection framework can be easily\napplied to other settings including linear bandits, linear contextual bandits,\nand MDPs with general function approximation, leading to several improved or\nnew results.",
    "descriptor": "",
    "authors": [
      "Chen-Yu Wei",
      "Christoph Dann",
      "Julian Zimmert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03580"
  },
  {
    "id": "arXiv:2110.03585",
    "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and  Autoencoders",
    "abstract": "Electric Vehicles (EVs) are spreading fast as they promise to provide better\nperformances and comfort, but above all, to help facing climate change. Despite\ntheir success, their cost is still a challenge. One of the most expensive\ncomponents of EVs is lithium-ion batteries, which became the standard for\nenergy storage in a wide range of applications. Precisely estimating the\nRemaining Useful Life (RUL) of battery packs can open to their reuse and thus\nhelp to reduce the cost of EVs and improve sustainability. A correct RUL\nestimation can be used to quantify the residual market value of the battery\npack. The customer can then decide to sell the battery when it still has a\nvalue, i.e., before it exceeds its end of life of the target application and\ncan still be reused in a second domain without compromising safety and\nreliability. In this paper, we propose to use a Deep Learning approach based on\nLSTMs and Autoencoders to estimate the RUL of li-ion batteries. Compared to\nwhat has been proposed so far in the literature, we employ measures to ensure\nthe applicability of the method also in the real deployed application. Such\nmeasures include (1) avoid using non-measurable variables as input, (2) employ\nappropriate datasets with wide variability and different conditions, (3) do not\nuse cycles to define the RUL.",
    "descriptor": "",
    "authors": [
      "Michael Bosello",
      "Carlo Falcomer",
      "Claudio Rossi",
      "Giovanni Pau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03585"
  },
  {
    "id": "arXiv:2110.03593",
    "title": "TranSalNet: Visual saliency prediction using transformers",
    "abstract": "Convolutional neural networks (CNNs) have significantly advanced\ncomputational modeling for saliency prediction. However, the inherent inductive\nbiases of convolutional architectures cause insufficient long-range contextual\nencoding capacity, which potentially makes a saliency model less humanlike.\nTransformers have shown great potential in encoding long-range information by\nleveraging the self-attention mechanism. In this paper, we propose a novel\nsaliency model integrating transformer components to CNNs to capture the\nlong-range contextual information. Experimental results show that the new\ncomponents make improvements, and the proposed model achieves promising results\nin predicting saliency.",
    "descriptor": "",
    "authors": [
      "Jianxun Lou",
      "Hanhe Lin",
      "David Marshall",
      "Dietmar Saupe",
      "Hantao Liu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03593"
  },
  {
    "id": "arXiv:2110.03595",
    "title": "Generalization in Deep RL for TSP Problems via Equivariance and Local  Search",
    "abstract": "Deep reinforcement learning (RL) has proved to be a competitive heuristic for\nsolving small-sized instances of traveling salesman problems (TSP), but its\nperformance on larger-sized instances is insufficient. Since training on large\ninstances is impractical, we design a novel deep RL approach with a focus on\ngeneralizability. Our proposition consisting of a simple deep learning\narchitecture that learns with novel RL training techniques, exploits two main\nideas. First, we exploit equivariance to facilitate training. Second, we\ninterleave efficient local search heuristics with the usual RL training to\nsmooth the value landscape. In order to validate the whole approach, we\nempirically evaluate our proposition on random and realistic TSP problems\nagainst relevant state-of-the-art deep RL methods. Moreover, we present an\nablation study to understand the contribution of each of its component",
    "descriptor": "\nComments: 18 pages, 2 figures\n",
    "authors": [
      "Wenbin Ouyang",
      "Yisen Wang",
      "Paul Weng",
      "Shaochen Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03595"
  },
  {
    "id": "arXiv:2110.03600",
    "title": "Fair distributions for more participants than allocations",
    "abstract": "We study the existence of fair distributions when we have more players than\npieces to allocate, focusing on envy-free distributions among those who receive\na piece. The conditions on the demand from the players can be weakened from\nthose of classic cake-cutting and rent-splitting results of Stromquist,\nWoodall, and Su. We extend existing variations of the cake-splitting problem\nwith secretive players and those that resist the removal of any sufficiently\nsmall set of players.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Pablo Sober\u00f3n"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.03600"
  },
  {
    "id": "arXiv:2110.03604",
    "title": "Online Markov Decision Processes with Non-oblivious Strategic Adversary",
    "abstract": "We study a novel setting in Online Markov Decision Processes (OMDPs) where\nthe loss function is chosen by a non-oblivious strategic adversary who follows\na no-external regret algorithm. In this setting, we first demonstrate that\nMDP-Expert, an existing algorithm that works well with oblivious adversaries\ncan still apply and achieve a policy regret bound of $\\mathcal{O}(\\sqrt{T\n\\log(L)}+\\tau^2\\sqrt{ T \\log(|A|)})$ where $L$ is the size of adversary's pure\nstrategy set and $|A|$ denotes the size of agent's action space. Considering\nreal-world games where the support size of a NE is small, we further propose a\nnew algorithm: MDP-Online Oracle Expert (MDP-OOE), that achieves a policy\nregret bound of $\\mathcal{O}(\\sqrt{T\\log(L)}+\\tau^2\\sqrt{ T k \\log(k)})$ where\n$k$ depends only on the support size of the NE. MDP-OOE leverages the key\nbenefit of Double Oracle in game theory and thus can solve games with\nprohibitively large action space. Finally, to better understand the learning\ndynamics of no-regret methods, under the same setting of no-external regret\nadversary in OMDPs, we introduce an algorithm that achieves last-round\nconvergence result to a NE. To our best knowledge, this is first work leading\nto the last iteration result in OMDPs.",
    "descriptor": "",
    "authors": [
      "Le Cong Dinh",
      "David Henry Mguni",
      "Long Tran-Thanh",
      "Jun Wang",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2110.03604"
  },
  {
    "id": "arXiv:2110.03605",
    "title": "One Thing to Fool them All: Generating Interpretable, Universal, and  Physically-Realizable Adversarial Features",
    "abstract": "It is well understood that modern deep networks are vulnerable to adversarial\nattacks. However, conventional methods fail to produce adversarial\nperturbations that are intelligible to humans, and they pose limited threats in\nthe physical world. To study feature-class associations in networks and better\nunderstand the real-world threats they face, we develop feature-level\nadversarial perturbations using deep image generators and a novel optimization\nobjective. We term these feature-fool attacks. We show that they are versatile\nand use them to generate targeted feature-level attacks at the ImageNet scale\nthat are simultaneously interpretable, universal to any source image, and\nphysically-realizable. These attacks can also reveal spurious,\nsemantically-describable feature/class associations, and we use them to guide\nthe design of \"copy/paste\" adversaries in which one natural image is pasted\ninto another to cause a targeted misclassification.",
    "descriptor": "\nComments: Code is available at: this https URL\n",
    "authors": [
      "Stephen Casper",
      "Max Nadeau",
      "Gabriel Kreiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03605"
  },
  {
    "id": "arXiv:2110.03608",
    "title": "How to Sense the World: Leveraging Hierarchy in Multimodal Perception  for Robust Reinforcement Learning Agents",
    "abstract": "This work addresses the problem of sensing the world: how to learn a\nmultimodal representation of a reinforcement learning agent's environment that\nallows the execution of tasks under incomplete perceptual conditions. To\naddress such problem, we argue for hierarchy in the design of representation\nmodels and contribute with a novel multimodal representation model, MUSE. The\nproposed model learns hierarchical representations: low-level modality-specific\nrepresentations, encoded from raw observation data, and a high-level multimodal\nrepresentation, encoding joint-modality information to allow robust state\nestimation. We employ MUSE as the sensory representation model of deep\nreinforcement learning agents provided with multimodal observations in Atari\ngames. We perform a comparative study over different designs of reinforcement\nlearning agents, showing that MUSE allows agents to perform tasks under\nincomplete perceptual experience with minimal performance loss. Finally, we\nevaluate the performance of MUSE in literature-standard multimodal scenarios\nwith higher number and more complex modalities, showing that it outperforms\nstate-of-the-art multimodal variational autoencoders in single and\ncross-modality generation.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Miguel Vasco",
      "Hang Yin",
      "Francisco S. Melo",
      "Ana Paiva"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03608"
  },
  {
    "id": "arXiv:2110.03609",
    "title": "Applying Phonological Features in Multilingual Text-To-Speech",
    "abstract": "This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech. We present a\nmapping between ARPABET/pinyin->SAMPA/SAMPA-SC->phonological features in this\npaper, and tested whether native, non-native, and code-switched speech could be\nsuccessfully generated using this mapping. We ran two experiments, one with a\nsmall dataset and one with a larger dataset. The results proved that\nphonological features can be a feasible input system, although it needs further\ninvestigation to improve model performance. The accented output generated by\nthe TTS models also helps with understanding human second language acquisition\nprocesses.",
    "descriptor": "\nComments: demo webpage: this https URL\n",
    "authors": [
      "Cong Zhang",
      "Huinan Zeng",
      "Huang Liu",
      "Jiewen Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.03609"
  },
  {
    "id": "arXiv:2110.03611",
    "title": "Adversarial Retriever-Ranker for dense text retrieval",
    "abstract": "Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.",
    "descriptor": "",
    "authors": [
      "Hang Zhang",
      "Yeyun Gong",
      "Yelong Shen",
      "Jiancheng Lv",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03611"
  },
  {
    "id": "arXiv:2110.03613",
    "title": "A Data-Centric Approach for Training Deep Neural Networks with Less Data",
    "abstract": "While the availability of large datasets is perceived to be a key requirement\nfor training deep neural networks, it is possible to train such models with\nrelatively little data. However, compensating for the absence of large datasets\ndemands a series of actions to enhance the quality of the existing samples and\nto generate new ones. This paper summarizes our winning submission to the\n\"Data-Centric AI\" competition. We discuss some of the challenges that arise\nwhile training with a small dataset, offer a principled approach for systematic\ndata quality enhancement, and propose a GAN-based solution for synthesizing new\ndata points. Our evaluations indicate that the dataset generated by the\nproposed pipeline offers 5% accuracy improvement while being significantly\nsmaller than the baseline.",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Mohammad Motamedi",
      "Nikolay Sakharnykh",
      "Tim Kaldewey"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03613"
  },
  {
    "id": "arXiv:2110.03615",
    "title": "School Virus Infection Simulator for Customizing School Schedules During  COVID-19",
    "abstract": "During the Coronavirus 2019 (the covid-19) pandemic, schools continuously\nstrive to provide consistent education to their students. Teachers and\neducation policymakers are seeking ways to re-open schools, as it is necessary\nfor community and economic development. However, in light of the pandemic,\nschools require customized schedules that can address the health concerns and\nsafety of the students considering classroom sizes, air conditioning equipment,\nclassroom systems, e.g., self-contained or compartmentalized. To solve this\nissue, we developed the School-Virus-Infection-Simulator (SVIS) for teachers\nand education policymakers. SVIS simulates the spread of infection at a school\nconsidering the students' lesson schedules, classroom volume, air circulation\nrates in classrooms, and infectability of the students. Thus, teachers and\neducation policymakers can simulate how their school schedules can impact\ncurrent health concerns. We then demonstrate the impact of several school\nschedules in self-contained and departmentalized classrooms and evaluate them\nin terms of the maximum number of students infected simultaneously and the\npercentage of face-to-face lessons. The results show that increasing classroom\nventilation rate is effective, however, the impact is not stable compared to\ncustomizing school schedules, in addition, school schedules can differently\nimpact the maximum number of students infected depending on whether classrooms\nare self-contained or compartmentalized. It was found that one of school\nschedules had a higher maximum number of students infected, compared to\nschedules with a higher percentage of face-to-face lessons. SVIS and the\nsimulation results can help teachers and education policymakers plan school\nschedules appropriately in order to reduce the maximum number of students\ninfected, while also maintaining a certain percentage of face-to-face lessons.",
    "descriptor": "\nComments: 10 pages, 2 figures, this https URL\n",
    "authors": [
      "Satoshi Takahashi",
      "Masaki Kitazawa",
      "Atsushi Yoshikawa"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.03615"
  },
  {
    "id": "arXiv:2110.03618",
    "title": "Causal Direction of Data Collection Matters: Implications of Causal and  Anticausal Learning in NLP",
    "abstract": "The principle of independent causal mechanisms (ICM) states that generative\nprocesses of real world data consist of independent modules which do not\ninfluence or inform each other. While this idea has led to fruitful\ndevelopments in the field of causal inference, it is not widely-known in the\nNLP community. In this work, we argue that the causal direction of the data\ncollection process bears nontrivial implications that can explain a number of\npublished NLP findings, such as differences in semi-supervised learning (SSL)\nand domain adaptation (DA) performance across different settings. We categorize\ncommon NLP tasks according to their causal direction and empirically assay the\nvalidity of the ICM principle for text data using minimum description length.\nWe conduct an extensive meta-analysis of over 100 published SSL and 30 DA\nstudies, and find that the results are consistent with our expectations based\non causal insights. This work presents the first attempt to analyze the ICM\nprinciple in NLP, and provides constructive suggestions for future modeling\nchoices. Code available at https://github.com/zhijing-jin/icm4nlp.",
    "descriptor": "\nComments: EMNLP 2021 (Oral)\n",
    "authors": [
      "Zhijing Jin",
      "Julius von K\u00fcgelgen",
      "Jingwei Ni",
      "Tejas Vaidhya",
      "Ayush Kaushal",
      "Mrinmaya Sachan",
      "Bernhard Schoelkopf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03618"
  },
  {
    "id": "arXiv:2110.03620",
    "title": "Hyperparameter Tuning with Renyi Differential Privacy",
    "abstract": "For many differentially private algorithms, such as the prominent noisy\nstochastic gradient descent (DP-SGD), the analysis needed to bound the privacy\nleakage of a single training run is well understood. However, few studies have\nreasoned about the privacy leakage resulting from the multiple training runs\nneeded to fine tune the value of the training algorithm's hyperparameters. In\nthis work, we first illustrate how simply setting hyperparameters based on\nnon-private training runs can leak private information. Motivated by this\nobservation, we then provide privacy guarantees for hyperparameter search\nprocedures within the framework of Renyi Differential Privacy. Our results\nimprove and extend the work of Liu and Talwar (STOC 2019). Our analysis\nsupports our previous observation that tuning hyperparameters does indeed leak\nprivate information, but we prove that, under certain assumptions, this leakage\nis modest, as long as each candidate training run needed to select\nhyperparameters is itself differentially private.",
    "descriptor": "",
    "authors": [
      "Nicolas Papernot",
      "Thomas Steinke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03620"
  },
  {
    "id": "arXiv:2110.03624",
    "title": "On the Complexity of Inductively Learning Guarded Rules",
    "abstract": "We investigate the computational complexity of mining guarded clauses from\nclausal datasets through the framework of inductive logic programming (ILP). We\nshow that learning guarded clauses is NP-complete and thus one step below the\n$\\sigma^P_2$-complete task of learning Horn clauses on the polynomial\nhierarchy. Motivated by practical applications on large datasets we identify a\nnatural tractable fragment of the problem. Finally, we also generalise all of\nour results to $k$-guarded clauses for constant $k$.",
    "descriptor": "",
    "authors": [
      "Andrei Draghici",
      "Georg Gottlob",
      "Matthias Lanzinger"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03624"
  },
  {
    "id": "arXiv:2110.03625",
    "title": "Time Series Forecasting Using Manifold Learning",
    "abstract": "We address a three-tier numerical framework based on manifold learning for\nthe forecasting of high-dimensional time series. At the first step, we embed\nthe time series into a reduced low-dimensional space using a nonlinear manifold\nlearning algorithm such as Locally Linear Embedding and Diffusion Maps. At the\nsecond step, we construct reduced-order regression models on the manifold, in\nparticular Multivariate Autoregressive (MVAR) and Gaussian Process Regression\n(GPR) models, to forecast the embedded dynamics. At the final step, we lift the\nembedded time series back to the original high-dimensional space using Radial\nBasis Functions interpolation and Geometric Harmonics. For our illustrations,\nwe test the forecasting performance of the proposed numerical scheme with four\nsets of time series: three synthetic stochastic ones resembling EEG signals\nproduced from linear and nonlinear stochastic models with different model\norders, and one real-world data set containing daily time series of 10 key\nforeign exchange rates (FOREX) spanning the time period 19/09/2001-29/10/2020.\nThe forecasting performance of the proposed numerical scheme is assessed using\nthe combinations of manifold learning, modelling and lifting approaches. We\nalso provide a comparison with the Principal Component Analysis algorithm as\nwell as with the naive random walk model and the MVAR and GPR models trained\nand implemented directly in the high-dimensional space.",
    "descriptor": "",
    "authors": [
      "Panagiotis Papaioannou",
      "Ronen Talmon",
      "Daniela di Serafino",
      "Constantinos Siettos"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03625"
  },
  {
    "id": "arXiv:2110.03628",
    "title": "Boxhead: A Dataset for Learning Hierarchical Representations",
    "abstract": "Disentanglement is hypothesized to be beneficial towards a number of\ndownstream tasks. However, a common assumption in learning disentangled\nrepresentations is that the data generative factors are statistically\nindependent. As current methods are almost solely evaluated on toy datasets\nwhere this ideal assumption holds, we investigate their performance in\nhierarchical settings, a relevant feature of real-world data. In this work, we\nintroduce Boxhead, a dataset with hierarchically structured ground-truth\ngenerative factors. We use this novel dataset to evaluate the performance of\nstate-of-the-art autoencoder-based disentanglement models and observe that\nhierarchical models generally outperform single-layer VAEs in terms of\ndisentanglement of hierarchically arranged factors.",
    "descriptor": "",
    "authors": [
      "Yukun Chen",
      "Frederik Tr\u00e4uble",
      "Andrea Dittadi",
      "Stefan Bauer",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03628"
  },
  {
    "id": "arXiv:2110.03634",
    "title": "Enabling On-Device Training of Speech Recognition Models with Federated  Dropout",
    "abstract": "Federated learning can be used to train machine learning models on the edge\non local data that never leave devices, providing privacy by default. This\npresents a challenge pertaining to the communication and computation costs\nassociated with clients' devices. These costs are strongly correlated with the\nsize of the model being trained, and are significant for state-of-the-art\nautomatic speech recognition models.\nWe propose using federated dropout to reduce the size of client models while\ntraining a full-size model server-side. We provide empirical evidence of the\neffectiveness of federated dropout, and propose a novel approach to vary the\ndropout rate applied at each layer. Furthermore, we find that federated dropout\nenables a set of smaller sub-models within the larger model to independently\nhave low word error rates, making it easier to dynamically adjust the size of\nthe model deployed for inference.",
    "descriptor": "\nComments: \\c{opyright} 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses\n",
    "authors": [
      "Dhruv Guliani",
      "Lillian Zhou",
      "Changwan Ryu",
      "Tien-Ju Yang",
      "Harry Zhang",
      "Yonghui Xiao",
      "Francoise Beaufays",
      "Giovanni Motta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.03634"
  },
  {
    "id": "arXiv:2110.03639",
    "title": "Using Contrastive Learning and Pseudolabels to learn representations for  Retail Product Image Classification",
    "abstract": "Retail product Image classification problems are often few shot\nclassification problems, given retail product classes cannot have the type of\nvariations across images like a cat or dog or tree could have. Previous works\nhave shown different methods to finetune Convolutional Neural Networks to\nachieve better classification accuracy on such datasets. In this work, we try\nto address the problem statement : Can we pretrain a Convolutional Neural\nNetwork backbone which yields good enough representations for retail product\nimages, so that training a simple logistic regression on these representations\ngives us good classifiers ? We use contrastive learning and pseudolabel based\nnoisy student training to learn representations that get accuracy in order of\nfinetuning the entire Convnet backbone for retail product image classification.",
    "descriptor": "",
    "authors": [
      "Muktabh Mayank Srivastava"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03639"
  },
  {
    "id": "arXiv:2110.03643",
    "title": "From Weighted Conditionals of Multilayer Perceptrons to a Gradual  Argumentation Semantics",
    "abstract": "A fuzzy multipreference semantics has been recently proposed for weighted\nconditional knowledge bases, and used to develop a logical semantics for\nMultilayer Perceptrons, by regarding a deep neural network (after training) as\na weighted conditional knowledge base. This semantics, in its different\nvariants, suggests some gradual argumentation semantics, which are related to\nthe family of the gradual semantics. The relationships between weighted\nconditional knowledge bases and MLPs extend to the proposed gradual semantics,\nwhich captures the stationary states of MPs, so that a dee neural network can\nas well be seen as a weighted argumentation graph.",
    "descriptor": "\nComments: 17 pages. arXiv admin note: substantial text overlap with arXiv:2106.00390\n",
    "authors": [
      "Laura Giordano"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03643"
  },
  {
    "id": "arXiv:2110.03646",
    "title": "Using Keypoint Matching and Interactive Self Attention Network to verify  Retail POSMs",
    "abstract": "Point of Sale Materials(POSM) are the merchandising and decoration items that\nare used by companies to communicate product information and offers in retail\nstores. POSMs are part of companies' retail marketing strategy and are often\napplied as stylized window displays around retail shelves. In this work, we\napply computer vision techniques to the task of verification of POSMs in\nsupermarkets by telling if all desired components of window display are present\nin a shelf image. We use Convolutional Neural Network based unsupervised\nkeypoint matching as a baseline to verify POSM components and propose a\nsupervised Neural Network based method to enhance the accuracy of baseline by a\nlarge margin. We also show that the supervised pipeline is not restricted to\nthe POSM material it is trained on and can generalize. We train and evaluate\nour model on a private dataset composed of retail shelf images.",
    "descriptor": "",
    "authors": [
      "Harshita Seth",
      "Sonaal Kant",
      "Muktabh Mayank Srivastava"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03646"
  },
  {
    "id": "arXiv:2110.03649",
    "title": "Neural Networks, Inside Out: Solving for Inputs Given Parameters (A  Preliminary Investigation)",
    "abstract": "Artificial neural network (ANN) is a supervised learning algorithm, where\nparameters are learned by several back-and-forth iterations of passing the\ninputs through the network, comparing the output with the expected labels, and\ncorrecting the parameters. Inspired by a recent work of Derian and Kramer\n(2020), we investigate a different problem: Suppose an observer can view how\nthe ANN parameters evolve over many iterations, but the dataset is oblivious to\nhim. For instance, this can be an adversary eavesdropping on a multi-party\ncomputation of an ANN parameters (where intermediate parameters are leaked).\nCan he form a system of equations, and solve it to recover the dataset?",
    "descriptor": "",
    "authors": [
      "Mohammad Sadeq Dousti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03649"
  },
  {
    "id": "arXiv:2110.03653",
    "title": "Utilizing Time-Reversibility for Shock Capturing in Nonlinear Hyperbolic  Conservation Laws",
    "abstract": "In this work, we introduce a novel approach to formulating an artificial\nviscosity for shock capturing in nonlinear hyperbolic systems by utilizing the\nproperty that the solutions of hyperbolic conservation laws are not reversible\nin time in the vicinity of shocks. The proposed approach does not require any\nadditional governing equations or a priori knowledge of the hyperbolic system\nin question, is independent of the mesh and approximation order, and requires\nthe use of only one tunable parameter. The primary novelty is that the\nresulting artificial viscosity is unique for each component of the conservation\nlaw which is advantageous for systems in which some components exhibit\ndiscontinuities while others do not. The efficacy of the method is shown in\nnumerical experiments of multi-dimensional hyperbolic conservation laws such as\nnonlinear transport, Euler equations, and ideal magnetohydrodynamics using a\nhigh-order discontinuous spectral element method on unstructured grids.",
    "descriptor": "\nComments: 20 pages, 14 figures\n",
    "authors": [
      "Tarik Dzanic",
      "Will Trojak",
      "Freddie D. Witherden"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03653"
  },
  {
    "id": "arXiv:2110.03654",
    "title": "The complexity of bidirected reachability in valence systems",
    "abstract": "We study the complexity of bidirected reachability problems arising in many\nareas of program analysis. We formulate the problem abstractly in terms of\nbidirected valence automata over graph monoids, an algebraic framework that\ngeneralizes many models of automata with storage, including CFL-reachability,\ninterleaved Dyck reachability, vector addition systems over naturals or\nintegers, and models involving complex combinations of stacks and counters. Our\nmain result is a characterization of the decidability and complexity of the\nbidirected reachability problem for different graph classes. In particular, we\ncharacterize the complexity of bidirected reachability for every graph class\nfor which reachability is known to be decidable. We show that there is a\nremarkable drop in complexity in going from the reachability to the bidirected\nreachability problems for many natural models studied previously. Our\ntechniques are algebraic, and exploit the underlying group structure of the\nbidirected problem. As a consequence of our results, we characterize the\ncomplexity of bidirectional reachability of a number of open problems in\nprogram analysis, such as the complexity of different subcases of bidirectional\ninterleaved Dyck reachability.",
    "descriptor": "\nComments: 42 pages\n",
    "authors": [
      "Moses Ganardi",
      "Rupak Majumdar",
      "Georg Zetzsche"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2110.03654"
  },
  {
    "id": "arXiv:2110.03655",
    "title": "Augmenting Reinforcement Learning with Behavior Primitives for Diverse  Manipulation Tasks",
    "abstract": "Realistic manipulation tasks require a robot to interact with an environment\nwith a prolonged sequence of motor actions. While deep reinforcement learning\nmethods have recently emerged as a promising paradigm for automating\nmanipulation behaviors, they usually fall short in long-horizon tasks due to\nthe exploration burden. This work introduces MAnipulation Primitive-augmented\nreinforcement LEarning (MAPLE), a learning framework that augments standard\nreinforcement learning algorithms with a pre-defined library of behavior\nprimitives. These behavior primitives are robust functional modules specialized\nin achieving manipulation goals, such as grasping and pushing. To use these\nheterogeneous primitives, we develop a hierarchical policy that involves the\nprimitives and instantiates their executions with input parameters. We\ndemonstrate that MAPLE outperforms baseline approaches by a significant margin\non a suite of simulated manipulation tasks. We also quantify the compositional\nstructure of the learned behaviors and highlight our method's ability to\ntransfer policies to new task variants and to physical hardware. Videos and\ncode are available at https://ut-austin-rpl.github.io/maple",
    "descriptor": "",
    "authors": [
      "Soroush Nasiriany",
      "Huihan Liu",
      "Yuke Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03655"
  },
  {
    "id": "arXiv:2110.03659",
    "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient  Agent Design",
    "abstract": "An agent's functionality is largely determined by its design, i.e., skeletal\nstructure and joint attributes (e.g., length, size, strength). However, finding\nthe optimal agent design for a given function is extremely challenging since\nthe problem is inherently combinatorial and the design space is prohibitively\nlarge. Additionally, it can be costly to evaluate each candidate design which\nrequires solving for its optimal controller. To tackle these problems, our key\nidea is to incorporate the design procedure of an agent into its\ndecision-making process. Specifically, we learn a conditional policy that, in\nan episode, first applies a sequence of transform actions to modify an agent's\nskeletal structure and joint attributes, and then applies control actions under\nthe new design. To handle a variable number of joints across designs, we use a\ngraph-based policy where each graph node represents a joint and uses message\npassing with its neighbors to output joint-specific actions. Using policy\ngradient methods, our approach enables first-order optimization of agent design\nand control as well as experience sharing across different designs, which\nimproves sample efficiency tremendously. Experiments show that our approach,\nTransform2Act, outperforms prior methods significantly in terms of convergence\nspeed and final performance. Notably, Transform2Act can automatically discover\nplausible designs similar to giraffes, squids, and spiders. Our project website\nis at https://sites.google.com/view/transform2act.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Ye Yuan",
      "Yuda Song",
      "Zhengyi Luo",
      "Wen Sun",
      "Kris Kitani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.03659"
  },
  {
    "id": "arXiv:2110.03660",
    "title": "Developing Medical AI : a cloud-native audio-visual data collection  study",
    "abstract": "Designing Artificial Intelligence (AI) solutions that can operate in\nreal-world situations is a highly complex task. Deploying such solutions in the\nmedical domain is even more challenging. The promise of using AI to improve\npatient care and reduce cost has encouraged many companies to undertake such\nendeavours. For our team, the goal has been to improve early identification of\ndeteriorating patients in the hospital. Identifying patient deterioration in\nlower acuity wards relies, to a large degree on the attention and intuition of\nclinicians, rather than on the presence of physiological monitoring devices. In\nthese care areas, an automated tool which could continuously observe patients\nand notify the clinical staff of suspected deterioration, would be extremely\nvaluable. In order to develop such an AI-enabled tool, a large collection of\npatient images and audio correlated with corresponding vital signs, past\nmedical history and clinical outcome would be indispensable. To the best of our\nknowledge, no such public or for-pay data set currently exists. This lack of\naudio-visual data led to the decision to conduct exactly such study. The main\ncontributions of this paper are, the description of a protocol for audio-visual\ndata collection study, a cloud-architecture for efficiently processing and\nconsuming such data, and the design of a specific data collection device.",
    "descriptor": "",
    "authors": [
      "Sagi Schein",
      "Greg Arutiunian",
      "Vitaly Burshtein",
      "Gal Sadeh",
      "Michelle Townshend",
      "Bruce Friedman",
      "Shada Sadr-azodi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03660"
  },
  {
    "id": "arXiv:2110.03662",
    "title": "FlowMapper.org: A web-based framework for designing origin-destination  flow maps",
    "abstract": "FlowMapper.org is a web-based framework for automated production and design\nof origin-destination flow maps (https://flowmapper.org). FlowMapper has four\nmajor features that contribute to the advancement of existing flow mapping\nsystems. First, users can upload and process their own data to design and share\ncustomized flow maps. The ability to save data, cartographic design and map\nelements in a project file allows users to easily share their data and\ncartographic design with others. Second, users can customize the flow line\nsymbology by including options to change the flow line style, width, and\ncoloring. FlowMapper includes algorithms for drawing curved line styles with\nvarying thickness along a flow line, which reduces the visual cluttering and\noverlapping by tapering flow lines at origin and destination points. The\nability to customize flow symbology supports different flow map reading tasks\nsuch as comparing flow magnitudes and directions and identifying flow and\nlocation clusters that are strongly connected with each other. Third,\nFlowMapper supports supplementary layers such as node symbol, choropleth, and\nbase maps to contextualize flow patterns with location references and\ncharacteristics such as net-flow, gross flow, net-flow ratio, or a locational\nattribute such as population density. FlowMapper also supports user\ninteractions to zoom, filter, and obtain details-on-demand functions to support\nvisual information seeking about nodes, flows and regions. Finally, the\nweb-based architecture of FlowMapper supports server side computational\ncapabilities to process, normalize and summarize large flow data to reveal\nnatural patterns of flows.",
    "descriptor": "",
    "authors": [
      "Caglar Koylu",
      "Geng Tian",
      "Mary Windsor"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2110.03662"
  },
  {
    "id": "arXiv:2110.03663",
    "title": "Quantifying the Suicidal Tendency on Social Media: A Survey",
    "abstract": "Amid lockdown period more people express their feelings over social media\nplatforms due to closed third-place and academic researchers have witnessed\nstrong associations between the mental healthcare and social media posts. The\nstress for a brief period may lead to clinical depressions and the long-lasting\ntraits of prevailing depressions can be life threatening with suicidal ideation\nas the possible outcome. The increasing concern towards the rise in number of\nsuicide cases is because it is one of the leading cause of premature but\npreventable death. Recent studies have shown that mining social media data has\nhelped in quantifying the suicidal tendency of users at risk. This potential\nmanuscript elucidates the taxonomy of mental healthcare and highlights some\nrecent attempts in examining the potential of quantifying suicidal tendency on\nsocial media data. This manuscript presents the classification of heterogeneous\nfeatures from social media data and handling feature vector representation.\nAiming to identify the new research directions and advances in the development\nof Machine Learning (ML) and Deep Learning (DL) based models, a quantitative\nsynthesis and a qualitative review was carried out with corpus of over 77\npotential research articles related to stress, depression and suicide risk from\n2013 to 2021.",
    "descriptor": "",
    "authors": [
      "Muskan Garg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.03663"
  },
  {
    "id": "arXiv:2110.03664",
    "title": "TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity,  Geo, and Gender Labels",
    "abstract": "The widespread usage of social networks during mass convergence events, such\nas health emergencies and disease outbreaks, provides instant access to\ncitizen-generated data that carry rich information about public opinions,\nsentiments, urgent needs, and situational reports. Such information can help\nauthorities understand the emergent situation and react accordingly. Moreover,\nsocial media plays a vital role in tackling misinformation and disinformation.\nThis work presents TBCOV, a large-scale Twitter dataset comprising more than\ntwo billion multilingual tweets related to the COVID-19 pandemic collected\nworldwide over a continuous period of more than one year. More importantly,\nseveral state-of-the-art deep learning models are used to enrich the data with\nimportant attributes, including sentiment labels, named-entities (e.g.,\nmentions of persons, organizations, locations), user types, and gender\ninformation. Last but not least, a geotagging method is proposed to assign\ncountry, state, county, and city information to tweets, enabling a myriad of\ndata analysis tasks to understand real-world issues. Our sentiment and trend\nanalyses reveal interesting insights and confirm TBCOV's broad coverage of\nimportant topics.",
    "descriptor": "\nComments: 20 pages, 13 figures, 8 tables\n",
    "authors": [
      "Muhammad Imran",
      "Umair Qazi",
      "Ferda Ofli"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03664"
  },
  {
    "id": "arXiv:2110.03665",
    "title": "Revisiting SVD to generate powerful Node Embeddings for Recommendation  Systems",
    "abstract": "Graph Representation Learning (GRL) is an upcoming and promising area in\nrecommendation systems. In this paper, we revisit the Singular Value\nDecomposition (SVD) of adjacency matrix for embedding generation of users and\nitems and use a two-layer neural network on top of these embeddings to learn\nrelevance between user-item pairs. Inspired by the success of higher-order\nlearning in GRL, we further propose an extension of this method to include\ntwo-hop neighbors for SVD through the second order of the adjacency matrix and\ndemonstrate improved performance compared with the simple SVD method which only\nuses one-hop neighbors. Empirical validation on three publicly available\ndatasets of recommendation system demonstrates that the proposed methods,\ndespite being simple, beat many state-of-the-art methods and for two of three\ndatasets beats all of them up to a margin of 10%. Through our research, we want\nto shed light on the effectiveness of matrix factorization approaches,\nspecifically SVD, in the deep learning era and show that these methods still\ncontribute as important baselines in recommendation systems.",
    "descriptor": "\nComments: 7 pages, 3 figures, and 4 tables\n",
    "authors": [
      "Amar Budhiraja"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03665"
  },
  {
    "id": "arXiv:2110.03666",
    "title": "Joint inference of multiple graphs with hidden variables from stationary  graph signals",
    "abstract": "Learning graphs from sets of nodal observations represents a prominent\nproblem formally known as graph topology inference. However, current approaches\nare limited by typically focusing on inferring single networks, and they assume\nthat observations from all nodes are available. First, many contemporary setups\ninvolve multiple related networks, and second, it is often the case that only a\nsubset of nodes is observed while the rest remain hidden. Motivated by these\nfacts, we introduce a joint graph topology inference method that models the\ninfluence of the hidden variables. Under the assumptions that the observed\nsignals are stationary on the sought graphs and the graphs are closely related,\nthe joint estimation of multiple networks allows us to exploit such\nrelationships to improve the quality of the learned graphs. Moreover, we\nconfront the challenging problem of modeling the influence of the hidden nodes\nto minimize their detrimental effect. To obtain an amenable approach, we take\nadvantage of the particular structure of the setup at hand and leverage the\nsimilarity between the different graphs, which affects both the observed and\nthe hidden nodes. To test the proposed method, numerical simulations over\nsynthetic and real-world graphs are provided.",
    "descriptor": "\nComments: Paper submitted to ICASSP 2022\n",
    "authors": [
      "Samuel Rey",
      "Andrei Buciulea",
      "Madeline Navarro",
      "Santiago Segarra",
      "Antonio G. Marques"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03666"
  },
  {
    "id": "arXiv:2110.03674",
    "title": "Dense Gaussian Processes for Few-Shot Segmentation",
    "abstract": "Few-shot segmentation is a challenging dense prediction task, which entails\nsegmenting a novel query image given only a small annotated support set. The\nkey problem is thus to design a method that aggregates detailed information\nfrom the support set, while being robust to large variations in appearance and\ncontext. To this end, we propose a few-shot segmentation method based on dense\nGaussian process (GP) regression. Given the support set, our dense GP learns\nthe mapping from local deep image features to mask values, capable of capturing\ncomplex appearance distributions. Furthermore, it provides a principled means\nof capturing uncertainty, which serves as another powerful cue for the final\nsegmentation, obtained by a CNN decoder. Instead of a one-dimensional mask\noutput, we further exploit the end-to-end learning capabilities of our approach\nto learn a high-dimensional output space for the GP. Our approach sets a new\nstate-of-the-art for both 1-shot and 5-shot FSS on the PASCAL-5$^i$ and\nCOCO-20$^i$ benchmarks, achieving an absolute gain of $+14.9$ mIoU in the\nCOCO-20$^i$ 5-shot setting. Furthermore, the segmentation quality of our\napproach scales gracefully when increasing the support set size, while\nachieving robust cross-dataset transfer.",
    "descriptor": "\nComments: 6 figures, 21 pages\n",
    "authors": [
      "Joakim Johnander",
      "Johan Edstedt",
      "Michael Felsberg",
      "Fahad Shahbaz Khan",
      "Martin Danelljan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03674"
  },
  {
    "id": "arXiv:2110.03675",
    "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
    "abstract": "The ability to synthesize realistic and diverse indoor furniture layouts\nautomatically or based on partial input, unlocks many applications, from better\ninteractive 3D tools to data synthesis for training and simulation. In this\npaper, we present ATISS, a novel autoregressive transformer architecture for\ncreating diverse and plausible synthetic indoor environments, given only the\nroom type and its floor plan. In contrast to prior work, which poses scene\nsynthesis as sequence generation, our model generates rooms as unordered sets\nof objects. We argue that this formulation is more natural, as it makes ATISS\ngenerally useful beyond fully automatic room layout synthesis. For example, the\nsame trained model can be used in interactive applications for general scene\ncompletion, partial room re-arrangement with any objects specified by the user,\nas well as object suggestions for any partial room. To enable this, our model\nleverages the permutation equivariance of the transformer when conditioning on\nthe partial scene, and is trained to be permutation-invariant across object\norderings. Our model is trained end-to-end as an autoregressive generative\nmodel using only labeled 3D bounding boxes as supervision. Evaluations on four\nroom types in the 3D-FRONT dataset demonstrate that our model consistently\ngenerates plausible room layouts that are more realistic than existing methods.\nIn addition, it has fewer parameters, is simpler to implement and train and\nruns up to 8 times faster than existing methods.",
    "descriptor": "\nComments: To appear in NeurIPS 2021, Project Page: this https URL\n",
    "authors": [
      "Despoina Paschalidou",
      "Amlan Kar",
      "Maria Shugrina",
      "Karsten Kreis",
      "Andreas Geiger",
      "Sanja Fidler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03675"
  },
  {
    "id": "arXiv:2110.03677",
    "title": "Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect",
    "abstract": "Recent empirical advances show that training deep models with large learning\nrate often improves generalization performance. However, theoretical\njustifications on the benefits of large learning rate are highly limited, due\nto challenges in analysis. In this paper, we consider using Gradient Descent\n(GD) with a large learning rate on a homogeneous matrix factorization problem,\ni.e., $\\min_{X, Y} \\|A - XY^\\top\\|_{\\sf F}^2$. We prove a convergence theory\nfor constant large learning rates well beyond $2/L$, where $L$ is the largest\neigenvalue of Hessian at the initialization. Moreover, we rigorously establish\nan implicit bias of GD induced by such a large learning rate, termed\n'balancing', meaning that magnitudes of $X$ and $Y$ at the limit of GD\niterations will be close even if their initialization is significantly\nunbalanced. Numerical experiments are provided to support our theory.",
    "descriptor": "\nComments: Questions and comments are welcome\n",
    "authors": [
      "Yuqing Wang",
      "Minshuo Chen",
      "Tuo Zhao",
      "Molei Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03677"
  },
  {
    "id": "arXiv:2110.03680",
    "title": "Burst Image Restoration and Enhancement",
    "abstract": "Modern handheld devices can acquire burst image sequence in a quick\nsuccession. However, the individual acquired frames suffer from multiple\ndegradations and are misaligned due to camera shake and object motions. The\ngoal of Burst Image Restoration is to effectively combine complimentary cues\nacross multiple burst frames to generate high-quality outputs. Towards this\ngoal, we develop a novel approach by solely focusing on the effective\ninformation exchange between burst frames, such that the degradations get\nfiltered out while the actual scene details are preserved and enhanced. Our\ncentral idea is to create a set of \\emph{pseudo-burst} features that combine\ncomplimentary information from all the input burst frames to seamlessly\nexchange information. The pseudo-burst representations encode channel-wise\nfeatures from the original burst images, thus making it easier for the model to\nlearn distinctive information offered by multiple burst frames. However, the\npseudo-burst cannot be successfully created unless the individual burst frames\nare properly aligned to discount inter-frame movements. Therefore, our approach\ninitially extracts preprocessed features from each burst frame and matches them\nusing an edge-boosting burst alignment module. The pseudo-burst features are\nthen created and enriched using multi-scale contextual information. Our final\nstep is to adaptively aggregate information from the pseudo-burst features to\nprogressively increase resolution in multiple stages while merging the\npseudo-burst features. In comparison to existing works that usually follow a\nlate fusion scheme with single-stage upsampling, our approach performs\nfavorably, delivering state of the art performance on burst super-resolution\nand low-light image enhancement tasks. Our codes and models will be released\npublicly.",
    "descriptor": "",
    "authors": [
      "Akshay Dudhane",
      "Syed Waqas Zamir",
      "Salman Khan",
      "Fahad Khan",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03680"
  },
  {
    "id": "arXiv:2110.03681",
    "title": "Neural Tangent Kernel Empowered Federated Learning",
    "abstract": "Federated learning (FL) is a privacy-preserving paradigm where multiple\nparticipants jointly solve a machine learning problem without sharing raw data.\nUnlike traditional distributed learning, a unique characteristic of FL is\nstatistical heterogeneity, namely, data distributions across participants are\ndifferent from each other. Meanwhile, recent advances in the interpretation of\nneural networks have seen a wide use of neural tangent kernel (NTK) for\nconvergence and generalization analyses. In this paper, we propose a novel FL\nparadigm empowered by the NTK framework. The proposed paradigm addresses the\nchallenge of statistical heterogeneity by transmitting update data that are\nmore expressive than those of the traditional FL paradigms. Specifically,\nsample-wise Jacobian matrices, rather than model weights/gradients, are\nuploaded by participants. The server then constructs an empirical kernel matrix\nto update a global model without explicitly performing gradient descent. We\nfurther develop a variant with improved communication efficiency and enhanced\nprivacy. Numerical results show that the proposed paradigm can achieve the same\naccuracy while reducing the number of communication rounds by an order of\nmagnitude compared to federated averaging.",
    "descriptor": "",
    "authors": [
      "Kai Yue",
      "Richeng Jin",
      "Ryan Pilgrim",
      "Chau-Wai Wong",
      "Dror Baron",
      "Huaiyu Dai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.03681"
  },
  {
    "id": "arXiv:2110.03682",
    "title": "Learning invariance preserving moment closure model for Boltzmann-BGK  equation",
    "abstract": "As one of the main governing equations in kinetic theory, the Boltzmann\nequation is widely utilized in aerospace, microscopic flow, etc. Its\nhigh-resolution simulation is crucial in these related areas. However, due to\nthe Boltzmann equation's high dimensionality, high-resolution simulations are\noften difficult to achieve numerically. The moment method which Grad first\nproposed in 1949 [12] is among popular numerical methods to achieve efficient\nhigh-resolution simulations. We can derive the governing equations in the\nmoment method by taking moments on both sides of the Boltzmann equation, which\neffectively reduces the dimensionality of the problem. However, one of the main\nchallenges is that it leads to an unclosed moment system, and closure is needed\nto obtain a closed moment system. It is truly an art in designing closures for\nmoment systems and has been a significant research field in kinetic theory.\nOther than the traditional human designs of closures, the machine\nlearning-based approach has attracted much attention lately [13, 19]. In this\nwork, we propose a machine learning-based method to derive a moment closure\nmodel for the Boltzmann-BGK equation. In particular, the closure relation is\napproximated by a carefully designed deep neural network that possesses\ndesirable physical invariances, i.e., the Galilean invariance, reflecting\ninvariance, and scaling invariance, inherited from the original Boltzmann-BGK\nequation. Numerical simulations on the smooth and discontinuous initial\ncondition problem, Sod shock tube problem, and the shock structure problems\ndemonstrate satisfactory numerical performances of the proposed invariance\npreserving neural closure method.",
    "descriptor": "",
    "authors": [
      "Zhengyi Li",
      "Bin Dong",
      "Yanli Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03682"
  },
  {
    "id": "arXiv:2110.03684",
    "title": "Cross-Domain Imitation Learning via Optimal Transport",
    "abstract": "Cross-domain imitation learning studies how to leverage expert demonstrations\nof one agent to train an imitation agent with a different embodiment or\nmorphology. Comparing trajectories and stationary distributions between the\nexpert and imitation agents is challenging because they live on different\nsystems that may not even have the same dimensionality. We propose\nGromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain\nimitation that uses the Gromov-Wasserstein distance to align and compare states\nbetween the different spaces of the agents. Our theory formally characterizes\nthe scenarios where GWIL preserves optimality, revealing its possibilities and\nlimitations. We demonstrate the effectiveness of GWIL in non-trivial continuous\ncontrol domains ranging from simple rigid transformation of the expert domain\nto arbitrary transformation of the state-action space.",
    "descriptor": "",
    "authors": [
      "Arnaud Fickinger",
      "Samuel Cohen",
      "Stuart Russell",
      "Brandon Amos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03684"
  },
  {
    "id": "arXiv:2110.02954",
    "title": "A Stochastic Newton Algorithm for Distributed Convex Optimization",
    "abstract": "We propose and analyze a stochastic Newton algorithm for homogeneous\ndistributed stochastic convex optimization, where each machine can calculate\nstochastic gradients of the same population objective, as well as stochastic\nHessian-vector products (products of an independent unbiased estimator of the\nHessian of the population objective with arbitrary vectors), with many such\nstochastic computations performed between rounds of communication. We show that\nour method can reduce the number, and frequency, of required communication\nrounds compared to existing methods without hurting performance, by proving\nconvergence guarantees for quasi-self-concordant objectives (e.g., logistic\nregression), alongside empirical evidence.",
    "descriptor": "",
    "authors": [
      "Brian Bullins",
      "Kumar Kshitij Patel",
      "Ohad Shamir",
      "Nathan Srebro",
      "Blake Woodworth"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.02954"
  },
  {
    "id": "arXiv:2110.03001",
    "title": "Predictability and Fairness in Load Aggregation and Operations of  Virtual Power Plants",
    "abstract": "In power systems, one wishes to regulate the aggregate demand of an ensemble\nof distributed energy resources (DERs), such as controllable loads and battery\nenergy storage systems. We suggest a notion of predictability and fairness,\nwhich suggests that the long-term averages of prices or incentives offered\nshould be independent of the initial states of the operators of the DER, the\naggregator, and the power grid. We show that this notion cannot be guaranteed\nwith many traditional controllers used by the load aggregator, including the\nusual proportional-integral (PI) controller. We show that even considering the\nnon-linearity of the alternating-current model, this notion of predictability\nand fairness can be guaranteed for incrementally input-to-state stable (iISS)\ncontrollers, under mild assumptions.",
    "descriptor": "",
    "authors": [
      "Jakub Marecek",
      "Michal Roubalik",
      "Ramen Ghosh",
      "Robert N. Shorten",
      "Fabian R. Wirth"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03001"
  },
  {
    "id": "arXiv:2110.03002",
    "title": "Multi-Scale Convolutional Neural Network for Automated AMD  Classification using Retinal OCT Images",
    "abstract": "Age-related macular degeneration (AMD) is the most common cause of blindness\nin developed countries, especially in people over 60 years of age. The workload\nof specialists and the healthcare system in this field has increased in recent\nyears mainly dues to three reasons: 1) increased use of retinal optical\ncoherence tomography (OCT) imaging technique, 2) prevalence of population aging\nworldwide, and 3) chronic nature of AMD. Recent developments in deep learning\nhave provided a unique opportunity for the development of fully automated\ndiagnosis frameworks. Considering the presence of AMD-related retinal\npathologies in varying sizes in OCT images, our objective was to propose a\nmulti-scale convolutional neural network (CNN) capable of distinguishing\npathologies using receptive fields with various sizes. The multi-scale CNN was\ndesigned based on the feature pyramid network (FPN) structure and was used to\ndiagnose normal and two common clinical characteristics of dry and wet AMD,\nnamely drusen and choroidal neovascularization (CNV). The proposed method was\nevaluated on a national dataset gathered at Noor Eye Hospital (NEH), consisting\nof 12649 retinal OCT images from 441 patients, and a UCSD public dataset,\nconsisting of 108312 OCT images. The results show that the multi-scale\nFPN-based structure was able to improve the base model's overall accuracy by\n0.4% to 3.3% for different backbone models. In addition, gradual learning\nimproved the performance in two phases from 87.2%+-2.5% to 93.4%+-1.4% by\npre-training the base model on ImageNet weights in the first phase and\nfine-tuning the resulting model on a dataset of OCT images in the second phase.\nThe promising quantitative and qualitative results of the proposed architecture\nprove the suitability of the proposed method to be used as a screening tool in\nhealthcare centers assisting ophthalmologists in making better diagnostic\ndecisions.",
    "descriptor": "",
    "authors": [
      "Saman Sotoudeh-Paima",
      "Ata Jodeiri",
      "Fedra Hajizadeh",
      "Hamid Soltanian-Zadeh"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03002"
  },
  {
    "id": "arXiv:2110.03012",
    "title": "Emphasis control for parallel neural TTS",
    "abstract": "The semantic information conveyed by a speech signal is strongly influenced\nby local variations in prosody. Recent parallel neural text-to-speech (TTS)\nsynthesis methods are able to generate speech with high fidelity while\nmaintaining high performance. However, these systems often lack simple control\nover the output prosody, thus restricting the semantic information conveyable\nfor a given text. This paper proposes a hierarchical parallel neural TTS system\nfor prosodic emphasis control by learning a latent space that directly\ncorresponds to a change in emphasis. Three candidate features for the latent\nspace are compared: 1) Variance of pitch and duration within words in a\nsentence, 2) a wavelet based feature computed from pitch, energy, and duration\nand 3) a learned combination of the above features. Objective measures reveal\nthat the proposed methods are able to achieve a wide range of emphasis\nmodification, and subjective evaluations on the degree of emphasis and the\noverall quality indicate that they show promise for real-world applications.",
    "descriptor": "\nComments: 5 pages, 6 figures, preprint will be submitted to ICASSP 2022\n",
    "authors": [
      "Shreyas Seshadri",
      "Tuomo Raitio",
      "Dan Castellani",
      "Jiangchuan Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03012"
  },
  {
    "id": "arXiv:2110.03040",
    "title": "Approximate Quantiles for Stochastic Optimal Control of LTI Systems with  Arbitrary Disturbances",
    "abstract": "We propose a method for open-loop stochastic optimal control of LTI systems\nbased in approximations of a quantile function. This approach enables efficient\ncomputation of quantile functions that arise in chance constraints. We are\nmotivated by multi-vehicle planning problems in LTI systems, with norm-based\ncollision avoidance constraints and polytopic feasibility constraints. These\nconstraints can be posed as reverse-convex and convex chance constraints,\nrespectively, that are affine in the control and in the disturbance. We show\nthat for constraints of this form, piecewise affine approximations of the\nquantile function can be embedded in a difference-of-convex program that\nenables use of conic solvers. We demonstrate our method on multi-satellite\ncoordination with Gaussian and Cauchy disturbances, and provide a comparison\nwith particle control.",
    "descriptor": "\nComments: Initial submission to American Control Conference (ACC) 2022\n",
    "authors": [
      "Shawn Priore",
      "Christopher Petersen",
      "Meeko Oishi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03040"
  },
  {
    "id": "arXiv:2110.03070",
    "title": "Robust Algorithms for GMM Estimation: A Finite Sample Viewpoint",
    "abstract": "For many inference problems in statistics and econometrics, the unknown\nparameter is identified by a set of moment conditions. A generic method of\nsolving moment conditions is the Generalized Method of Moments (GMM). However,\nclassical GMM estimation is potentially very sensitive to outliers. Robustified\nGMM estimators have been developed in the past, but suffer from several\ndrawbacks: computational intractability, poor dimension-dependence, and no\nquantitative recovery guarantees in the presence of a constant fraction of\noutliers. In this work, we develop the first computationally efficient GMM\nestimator (under intuitive assumptions) that can tolerate a constant $\\epsilon$\nfraction of adversarially corrupted samples, and that has an $\\ell_2$ recovery\nguarantee of $O(\\sqrt{\\epsilon})$. To achieve this, we draw upon and extend a\nrecent line of work on algorithmic robust statistics for related but simpler\nproblems such as mean estimation, linear regression and stochastic\noptimization. As two examples of the generality of our algorithm, we show how\nour estimation algorithm and assumptions apply to instrumental variables linear\nand logistic regression. Moreover, we experimentally validate that our\nestimator outperforms classical IV regression and two-stage Huber regression on\nsynthetic and semi-synthetic datasets with corruption.",
    "descriptor": "\nComments: 24 pages, 1 figure\n",
    "authors": [
      "Dhruv Rohatgi",
      "Vasilis Syrgkanis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03070"
  },
  {
    "id": "arXiv:2110.03094",
    "title": "Improving Pneumonia Localization via Cross-Attention on Medical Images  and Reports",
    "abstract": "Localization and characterization of diseases like pneumonia are primary\nsteps in a clinical pipeline, facilitating detailed clinical diagnosis and\nsubsequent treatment planning. Additionally, such location annotated datasets\ncan provide a pathway for deep learning models to be used for downstream tasks.\nHowever, acquiring quality annotations is expensive on human resources and\nusually requires domain expertise. On the other hand, medical reports contain a\nplethora of information both about pneumonia characteristics and its location.\nIn this paper, we propose a novel weakly-supervised attention-driven deep\nlearning model that leverages encoded information in medical reports during\ntraining to facilitate better localization. Our model also performs\nclassification of attributes that are associated to pneumonia and extracted\nfrom medical reports for supervision. Both the classification and localization\nare trained in conjunction and once trained, the model can be utilized for both\nthe localization and characterization of pneumonia using only the input image.\nIn this paper, we explore and analyze the model using chest X-ray datasets and\ndemonstrate qualitatively and quantitatively that the introduction of textual\ninformation improves pneumonia localization. We showcase quantitative results\non two datasets, MIMIC-CXR and Chest X-ray-8, and we also showcase severity\ncharacterization on the COVID-19 dataset.",
    "descriptor": "\nComments: Published at MICCAI 2021\n",
    "authors": [
      "Riddhish Bhalodia",
      "Ali Hatamizadeh",
      "Leo Tam",
      "Ziyue Xu",
      "Xiaosong Wang",
      "Evrim Turkbey",
      "Daguang Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03094"
  },
  {
    "id": "arXiv:2110.03098",
    "title": "CTC Variations Through New WFST Topologies",
    "abstract": "This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n<epsilon> back-off transitions; (2) the \"minimal-CTC\", that only adds <blank>\nself-loops when used in WFST-composition; and (3) \"selfless-CTC\", that\ndisallows self-loop for non-blank units. The new CTC variants have several\nbenefits, such as reducing decoding graph size and GPU memory required for\ntraining while keeping model accuracy.",
    "descriptor": "\nComments: Submitted to ICASSP 2022, 5 pages, 2 figures, 7 tables\n",
    "authors": [
      "Aleksandr Laptev",
      "Somshubra Majumdar",
      "Boris Ginsburg"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03098"
  },
  {
    "id": "arXiv:2110.03103",
    "title": "Lightweight Speech Enhancement in Unseen Noisy and Reverberant  Conditions using KISS-GEV Beamforming",
    "abstract": "This paper introduces a new method referred to as KISS-GEV (for Keep It Super\nSimple Generalized eigenvalue) beamforming. While GEV beamforming usually\nrelies on deep neural network for estimating target and noise time-frequency\nmasks, this method uses a signal processing approach based on the direction of\narrival (DoA) of the target. This considerably reduces the amount of\ncomputations involved at test time, and works for speech enhancement in unseen\nconditions as there is no need to train a neural network with noisy speech. The\nproposed method can also be used to separate speech from a mixture, provided\nthe speech sources come from different directions. Results also show that the\nproposed method uses the same minimal DoA assumption as Delay-and-Sum\nbeamforming, yet outperforms this traditional approach.",
    "descriptor": "",
    "authors": [
      "Thomas Bernard",
      "Cem Subakan",
      "Fran\u00e7ois Grondin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2110.03103"
  },
  {
    "id": "arXiv:2110.03114",
    "title": "On audio enhancement via online non-negative matrix factorization",
    "abstract": "We propose a method for noise reduction, the task of producing a clean audio\nsignal from a recording corrupted by additive noise. Many common approaches to\nthis problem are based upon applying non-negative matrix factorization to\nspectrogram measurements. These methods use a noiseless recording, which is\nbelieved to be similar in structure to the signal of interest, and a pure-noise\nrecording to learn dictionaries for the true signal and the noise.\nOne may then construct an approximation of the true signal by projecting the\ncorrupted recording on to the clean dictionary. In this work, we build upon\nthese methods by proposing the use of \\emph{online} non-negative matrix\nfactorization for this problem. This method is more memory efficient than\ntraditional non-negative matrix factorization and also has potential\napplications to real-time denoising.",
    "descriptor": "",
    "authors": [
      "Andrew Sack",
      "Wenzhao Jiang",
      "Michael Perlmutter",
      "Palina Salanevich",
      "Deanna Needell"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03114"
  },
  {
    "id": "arXiv:2110.03130",
    "title": "Generic tool for numerical simulation of transformation-diffusion  processes in complex volume geometric shapes: application to microbial  decomposition of organic matter",
    "abstract": "This paper presents a generic framework for the numerical simulation of\ntransformation-diffusion processes in complex volume geometric shapes. This\nwork follows a previous one devoted to the simulation of microbial degradation\nof organic matter in porous system at microscopic scale. We generalized and\nimproved the MOSAIC method significantly and thus yielding a much more generic\nand efficient numerical simulation scheme. In particular, regarding the\nsimulation of diffusion processes from the graph, in this study we proposed a\ncompletely explicit and semi-implicit numerical scheme that can significantly\nreduce the computational complexity. We validated our method by comparing the\nresults to the one provided by classical Lattice Boltzmann Method (LBM) within\nthe context of microbial decomposition simulation. For the same datasets, we\nobtained similar results in a significantly shorter computing time (i.e., 10-15\nminutes) than the prior work (several hours). Besides the classical LBM method\ntakes around 3 weeks computing time.",
    "descriptor": "\nComments: This paper represents, in my opinion, a breakthrough and then is worthing to be online before the end of the review process\n",
    "authors": [
      "Olivier Monga",
      "Fr\u00e9d\u00e9ric Hecht",
      "Serge Moto",
      "Bruno Mbe",
      "Patricia Garnier",
      "Val\u00e9rie Pot"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Physics (physics.comp-ph)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03130"
  },
  {
    "id": "arXiv:2110.03136",
    "title": "The Gromov-Hausdorff distance between ultrametric spaces: its structure  and computation",
    "abstract": "The Gromov-Hausdorff distance ($d_\\mathrm{GH}$) provides a natural way of\nquantifying the dissimilarity between two given metric spaces. It is known that\ncomputing $d_\\mathrm{GH}$ between two finite metric spaces is NP-hard, even in\nthe case of finite ultrametric spaces which are highly structured metric spaces\nin the sense that they satisfy the so-called \\emph{strong triangle inequality}.\nUltrametric spaces naturally arise in many applications such as hierarchical\nclustering, phylogenetics, genomics, and even linguistics. By exploiting the\nspecial structures of ultrametric spaces, (1) we identify a one parameter\nfamily $\\{d_\\mathrm{GH}^{(p)}\\}_{p\\in[1,\\infty]}$ of distances defined in a\nflavor similar to the Gromov-Hausdorff distance on the collection of finite\nultrametric spaces, and in particular $d_\\mathrm{GH}^{(1)} =d_\\mathrm{GH}$. The\nextreme case when $p=\\infty$, which we also denote by $u_\\mathrm{GH}$, turns\nout to be an ultrametric on the collection of ultrametric spaces. Whereas for\nall $p\\in[1,\\infty)$, $d_\\mathrm{GH}^{(p)}$ yields NP-hard problems, we prove\nthat surprisingly $u_\\mathrm{GH}$ can be computed in polynomial time. The proof\nis based on a structural theorem for $u_\\mathrm{GH}$ established in this paper;\n(2) inspired by the structural theorem for $u_\\mathrm{GH}$, and by carefully\nleveraging properties of ultrametric spaces, we also establish a structural\ntheorem for $d_\\mathrm{GH}$ when restricted to ultrametric spaces. This\nstructural theorem allows us to identify special families of ultrametric spaces\non which $d_\\mathrm{GH}$ is computationally tractable. These families are\ndetermined by properties related to the doubling constant of metric space.\nBased on these families, we devise a fixed-parameter tractable (FPT) algorithm\nfor computing the exact value of $d_\\mathrm{GH}$ between ultrametric spaces. We\nbelieve ours is the first such algorithm to be identified.",
    "descriptor": "",
    "authors": [
      "Facundo M\u00e9moli",
      "Zane Smith",
      "Zhengchao Wan"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2110.03136"
  },
  {
    "id": "arXiv:2110.03146",
    "title": "Solving Multistage Stochastic Linear Programming via Regularized Linear  Decision Rules: An Application to Hydrothermal Dispatch Planning",
    "abstract": "The solution of multistage stochastic linear problems (MSLP) represents a\nchallenge for many applications. Long-term hydrothermal dispatch planning\n(LHDP) materializes this challenge in a real-world problem that affects\nelectricity markets, economies, and natural resources worldwide. No closed-form\nsolutions are available for MSLP and the definition of non-anticipative\npolicies with high-quality out-of-sample performance is crucial. Linear\ndecision rules (LDR) provide an interesting simulation-based framework for\nfinding high-quality policies to MSLP through two-stage stochastic models. In\npractical applications, however, the number of parameters to be estimated when\nusing an LDR may be close or higher than the number of scenarios, thereby\ngenerating an in-sample overfit and poor performances in out-of-sample\nsimulations. In this paper, we propose a novel regularization scheme for LDR\nbased on the AdaLASSO (adaptive least absolute shrinkage and selection\noperator). The goal is to use the parsimony principle as largely studied in\nhigh-dimensional linear regression models to obtain better out-of-sample\nperformance for an LDR applied to MSLP. Computational experiments show that the\noverfit threat is non-negligible when using the classical non-regularized LDR\nto solve MSLP. For the LHDP problem, our analysis highlights the following\nbenefits of the proposed framework in comparison to the non-regularized\nbenchmark: 1) significant reductions in the number of non-zero coefficients\n(model parsimony), 2) substantial cost reductions in out-of-sample evaluations,\nand 3) improved spot-price profiles.",
    "descriptor": "",
    "authors": [
      "Felipe Nazare",
      "Alexandre Street"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03146"
  },
  {
    "id": "arXiv:2110.03151",
    "title": "Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number  of Speakers using End-to-End Speaker-Attributed ASR",
    "abstract": "This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Naoyuki Kanda",
      "Xiong Xiao",
      "Yashesh Gaur",
      "Xiaofei Wang",
      "Zhong Meng",
      "Zhuo Chen",
      "Takuya Yoshioka"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03151"
  },
  {
    "id": "arXiv:2110.03196",
    "title": "Explicitly Multi-Modal Benchmarks for Multi-Objective Optimization",
    "abstract": "We model multi-modality in multi-objective optimization problems and apply\nthis to generate benchmarking problems. In the model, the mode is based on the\nsingularity of the objective functions.",
    "descriptor": "",
    "authors": [
      "Reiya Hagiwara",
      "Takahiro Yamamoto",
      "Naoki Hamada",
      "Daisuke Sakurai"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.03196"
  },
  {
    "id": "arXiv:2110.03199",
    "title": "An optimal control approach to particle filtering",
    "abstract": "We present a novel particle filtering framework for continuous-time dynamical\nsystems with continuous-time measurements. Our approach is based on the duality\nbetween estimation and optimal control, which allows reformulating the\nestimation problem over a fixed time window into an optimal control problem.\nThe resulting optimal control problem has a cost function that depends on the\nmeasurements and the closed-loop dynamics under optimal control coincides with\nthe posterior distribution over the trajectories for the corresponding\nestimation problem. This type of stochastic optimal control problem can be\nsolved using a remarkable technique known as path integral control. By\nrecursively solving these optimal control problems using path integral control\nas new measurements become available we obtain an optimal control-based\nparticle filtering algorithm. A distinguishing feature of the proposed method\nis that it uses the measurements over a finite-length time window instead of a\nsingle measurement for the estimation at each time step, resembling the batch\nmethods of filtering, and improving fault tolerance. The efficacy of our\nalgorithm is illustrated with several numerical examples.",
    "descriptor": "",
    "authors": [
      "Qinsheng Zhang",
      "Amirhossein Taghvaei",
      "Yongxin Chen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03199"
  },
  {
    "id": "arXiv:2110.03218",
    "title": "Joint optimization of system design and reconstruction in MIMO radar  imaging",
    "abstract": "Multiple-input multiple-output (MIMO) radar is one of the leading depth\nsensing modalities. However, the usage of multiple receive channels lead to\nrelative high costs and prevent the penetration of MIMOs in many areas such as\nthe automotive industry. Over the last years, few studies concentrated on\ndesigning reduced measurement schemes and image reconstruction schemes for MIMO\nradars, however these problems have been so far addressed separately. On the\nother hand, recent works in optical computational imaging have demonstrated\ngrowing success of simultaneous learning-based design of the acquisition and\nreconstruction schemes, manifesting significant improvement in the\nreconstruction quality. Inspired by these successes, in this work, we propose\nto learn MIMO acquisition parameters in the form of receive (Rx) antenna\nelements locations jointly with an image neural-network based reconstruction.\nTo this end, we propose an algorithm for training the combined\nacquisition-reconstruction pipeline end-to-end in a differentiable way. We\ndemonstrate the significance of using our learned acquisition parameters with\nand without the neural-network reconstruction.",
    "descriptor": "",
    "authors": [
      "Tomer Weiss",
      "Nissim Peretz",
      "Sanketh Vedula",
      "Arie Feuer",
      "Alex Bronstein"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03218"
  },
  {
    "id": "arXiv:2110.03265",
    "title": "Optical secret sharing with cascaded metasurface holography",
    "abstract": "Secret sharing is a well-established cryptographic primitive for storing\nhighly sensitive information like encryption keys for encoded data. It\ndescribes the problem of splitting a secret into different shares, without\nrevealing any information about the secret to its shareholders. Here, we\ndemonstrate an all-optical solution for secret sharing based on metasurface\nholography. In our concept, metasurface holograms are used as spatially\nseparable shares that carry an encrypted message in form of a holographic\nimage. Two of these shares can be recombined by bringing them close together.\nLight passing through this stack of metasurfaces accumulates the phase shift of\nboth holograms and can optically reconstruct the secret with high fidelity. On\nthe other hand, the holograms generated by the single metasurfaces can be used\nfor identifying each shareholder. Furthermore, we demonstrate that the inherent\ntranslational alignment sensitivity between the two stacked metasurface\nholograms can be used for spatial multiplexing, which can be further extended\nto realize optical rulers.",
    "descriptor": "",
    "authors": [
      "Philip Georgi",
      "Qunshuo Wei",
      "Basudeb Sain",
      "Christian Schlickriede",
      "Yongtian Wang",
      "Lingling Huang",
      "Thomas Zentgraf"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.03265"
  },
  {
    "id": "arXiv:2110.03299",
    "title": "End-to-end label uncertainty modeling for speech emotion recognition  using Bayesian neural networks",
    "abstract": "Emotions are subjective constructs. Recent end-to-end speech emotion\nrecognition systems are typically agnostic to the subjective nature of\nemotions, despite their state-of-the-art performances. In this work, we\nintroduce an end-to-end Bayesian neural network architecture to capture the\ninherent subjectivity in emotions. To the best of our knowledge, this work is\nthe first to use Bayesian neural networks for speech emotion recognition. At\ntraining, the network learns a distribution of weights to capture the inherent\nuncertainty related to subjective emotion annotations. For this, we introduce a\nloss term which enables the model to be explicitly trained on a distribution of\nemotion annotations, rather than training them exclusively on mean or\ngold-standard labels. We evaluate the proposed approach on the AVEC'16 emotion\nrecognition dataset. Qualitative and quantitative analysis of the results\nreveal that the proposed model can aptly capture the distribution of subjective\nemotion annotations with a compromise between mean and standard deviation\nestimations.",
    "descriptor": "\nComments: (c) 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Navin Raj Prabhu",
      "Guillaume Carbajal",
      "Nale Lehmann-Willenbrock",
      "Timo Gerkmann"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03299"
  },
  {
    "id": "arXiv:2110.03305",
    "title": "A spatio-temporal adaptive phase-field fracture method",
    "abstract": "We present an energy-preserving mechanic formulation for dynamic\nquasi-brittle fracture in an Eulerian-Lagrangian formulation, where a\nsecond-order phase-field equation controls the damage evolution. The numerical\nformulation adapts in space and time to bound the errors, solving the mesh-bias\nissues these models typically suffer. The time-step adaptivity estimates the\ntemporal truncation error of the partial differential equation that governs the\nsolid equilibrium. The second-order generalized-$\\alpha$ time-marching scheme\nevolves the dynamic system. We estimate the temporal error by extrapolating a\nfirst-order approximation of the present time-step solution using previous ones\nwith backward difference formulas; the estimate compares the extrapolation with\nthe time-marching solution. We use an adaptive scheme built on a residual\nminimization formulation in space. We estimate the spatial error by enriching\nthe discretization with elemental bubbles; then, we localize an error indicator\nnorm to guide the mesh refinement as the fracture propagates. The combined\nspace and time adaptivity allows us to use low-order linear elements in\nproblems involving complex stress paths. We efficiently and robustly use\nlow-order spatial discretizations while avoiding mesh bias in structured and\nunstructured meshes. We demonstrate the method's efficiency with numerical\nexperiments that feature dynamic crack branching, where the capacity of the\nadaptive space-time scheme is apparent. The adaptive method delivers accurate\nand reproducible crack paths on meshes with fewer elements.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Nicolas A. Labanda",
      "Luis Espath",
      "Victor Manuel Calo"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03305"
  },
  {
    "id": "arXiv:2110.03310",
    "title": "Solving the Dirichlet problem for the Monge-Amp\u00e8re equation using  neural networks",
    "abstract": "The Monge-Amp\\`ere equation is a fully nonlinear partial differential\nequation (PDE) of fundamental importance in analysis, geometry and in the\napplied sciences. In this paper we solve the Dirichlet problem associated with\nthe Monge-Amp\\`ere equation using neural networks and we show that an ansatz\nusing deep input convex neural networks can be used to find the unique convex\nsolution. As part of our analysis we study the effect of singularities and\nnoise in the source function, we consider nontrivial domains, and we\ninvestigate how the method performs in higher dimensions. We also compare this\nmethod to an alternative approach in which standard feed-forward networks are\nused together with a loss function which penalizes lack of convexity.",
    "descriptor": "\nComments: 20 pages, 5 figures, 3 tables\n",
    "authors": [
      "Kaj Nystr\u00f6m",
      "Matias Vestberg"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03310"
  },
  {
    "id": "arXiv:2110.03321",
    "title": "Robustness and reliability when training with noisy labels",
    "abstract": "Labelling of data for supervised learning can be costly and time-consuming\nand the risk of incorporating label noise in large data sets is imminent. If\ntraining a flexible discriminative model using a strictly proper loss, such\nnoise will inevitably shift the solution towards the conditional distribution\nover noisy labels. Nevertheless, while deep neural networks have proved capable\nof fitting random labels, regularisation and the use of robust loss functions\nempirically mitigate the effects of label noise. However, such observations\nconcern robustness in accuracy, which is insufficient if reliable uncertainty\nquantification is critical. We demonstrate this by analysing the properties of\nthe conditional distribution over noisy labels for an input-dependent noise\nmodel. In addition, we evaluate the set of robust loss functions characterised\nby an overlap in asymptotic risk minimisers under the clean and noisy data\ndistributions. We find that strictly proper and robust loss functions both\noffer asymptotic robustness in accuracy, but neither guarantee that the\nresulting model is calibrated. Moreover, overfitting is an issue in practice.\nWith these results, we aim to explain inherent robustness of algorithms to\nlabel noise and to give guidance in the development of new noise-robust\nalgorithms.",
    "descriptor": "",
    "authors": [
      "Amanda Olmin",
      "Fredrik Lindsten"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03321"
  },
  {
    "id": "arXiv:2110.03327",
    "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End  Speech Recognition",
    "abstract": "As end-to-end automatic speech recognition (ASR) models reach promising\nperformance, various downstream tasks rely on good confidence estimators for\nthese systems. Recent research has shown that model-based confidence estimators\nhave a significant advantage over using the output softmax probabilities. If\nthe input data to the speech recogniser is from mismatched acoustic and\nlinguistic conditions, the ASR performance and the corresponding confidence\nestimators may exhibit severe degradation. Since confidence models are often\ntrained on the same in-domain data as the ASR, generalising to out-of-domain\n(OOD) scenarios is challenging. By keeping the ASR model untouched, this paper\nproposes two approaches to improve the model-based confidence estimators on OOD\ndata: using pseudo transcriptions and an additional OOD language model. With an\nASR model trained on LibriSpeech, experiments show that the proposed methods\ncan significantly improve the confidence metrics on TED-LIUM and Switchboard\ndatasets while preserving in-domain performance. Furthermore, the improved\nconfidence estimators are better calibrated on OOD data and can provide a much\nmore reliable criterion for data selection.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Qiujia Li",
      "Yu Zhang",
      "David Qiu",
      "Yanzhang He",
      "Liangliang Cao",
      "Philip C. Woodland"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03327"
  },
  {
    "id": "arXiv:2110.03329",
    "title": "Towards Universal Neural Vocoding with a Multi-band Excited WaveNet",
    "abstract": "This paper introduces the Multi-Band Excited WaveNet a neural vocoder for\nspeaking and singing voices. It aims to advance the state of the art towards an\nuniversal neural vocoder, which is a model that can generate voice signals from\narbitrary mel spectrograms extracted from voice signals. Following the success\nof the DDSP model and following the development of the recently proposed\nexcitation vocoders we propose a vocoder structure consisting of multiple\nspecialized DNN that are combined with dedicated signal processing components.\nAll components are implemented as differentiable operators and therefore allow\njoined optimization of the model parameters. To prove the capacity of the model\nto reproduce high quality voice signals we evaluate the model on single and\nmulti speaker/singer datasets. We conduct a subjective evaluation demonstrating\nthat the models support a wide range of domain variations (unseen voices,\nlanguages, expressivity) achieving perceptive quality that compares with a\nstate of the art universal neural vocoder, however using significantly smaller\ntraining datasets and significantly less parameters. We also demonstrate\nremaining limits of the universality of neural vocoders e.g. the creation of\nsaturated singing voices.",
    "descriptor": "",
    "authors": [
      "Axel Roebel",
      "Frederik Bous"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03329"
  },
  {
    "id": "arXiv:2110.03341",
    "title": "An Improved Penalty Algorithm using Model Order Reduction for MIPDECO  problems with partial observations",
    "abstract": "This work addresses optimal control problems governed by a linear\ntime-dependent partial differential equation (PDE) as well as integer\nconstraints on the control. Moreover, partial observations are assumed in the\nobjective function. The resulting problem poses several numerical challenges\ndue to the mixture of combinatorial aspects, induced by integer variables, and\nlarge scale linear algebra issues, arising from the PDE discretization. Since\nclassical solution approaches such as the branch-and-bound framework are\ntypically overwhelmed by such large-scale problems, this work extends an\nimproved penalty algorithm proposed by the authors, to the time-dependent\nsetting. The main contribution is a novel combination of an interior point\nmethod, preconditioning, and model order reduction yielding a tailored local\noptimization solver at the heart of the overall solution procedure. A thorough\nnumerical investigation is carried out both for a Poisson problem as well as a\nconvection-diffusion problem demonstrating the versatility of the approach.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1907.06462\n",
    "authors": [
      "Dominik Garmatter",
      "Margherita Porcelli",
      "Francesco Rinaldi",
      "Martin Stoll"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.03341"
  },
  {
    "id": "arXiv:2110.03342",
    "title": "VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic  Voice Over",
    "abstract": "In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Junchen Lu",
      "Berrak Sisman",
      "Rui Liu",
      "Mingyang Zhang",
      "Haizhou Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03342"
  },
  {
    "id": "arXiv:2110.03343",
    "title": "Uncertainty-aware GAN with Adaptive Loss for Robust MRI Image  Enhancement",
    "abstract": "Image-to-image translation is an ill-posed problem as unique one-to-one\nmapping may not exist between the source and target images. Learning-based\nmethods proposed in this context often evaluate the performance on test data\nthat is similar to the training data, which may be impractical. This demands\nrobust methods that can quantify uncertainty in the prediction for making\ninformed decisions, especially for critical areas such as medical imaging.\nRecent works that employ conditional generative adversarial networks (GANs)\nhave shown improved performance in learning photo-realistic image-to-image\nmappings between the source and the target images. However, these methods do\nnot focus on (i)~robustness of the models to out-of-distribution (OOD)-noisy\ndata and (ii)~uncertainty quantification. This paper proposes a GAN-based\nframework that (i)~models an adaptive loss function for robustness to OOD-noisy\ndata that automatically tunes the spatially varying norm for penalizing the\nresiduals and (ii)~estimates the per-voxel uncertainty in the predictions. We\ndemonstrate our method on two key applications in medical imaging:\n(i)~undersampled magnetic resonance imaging (MRI) reconstruction (ii)~MRI\nmodality propagation. Our experiments with two different real-world datasets\nshow that the proposed method (i)~is robust to OOD-noisy test data and provides\nimproved accuracy and (ii)~quantifies voxel-level uncertainty in the\npredictions.",
    "descriptor": "\nComments: Accepted at IEEE ICCV-2021 workshop on Computer Vision for Automated Medical Diagnosis\n",
    "authors": [
      "Uddeshya Upadhyay",
      "Viswanath P. Sudarshan",
      "Suyash P. Awate"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03343"
  },
  {
    "id": "arXiv:2110.03347",
    "title": "Cloning one's voice using very limited data in the wild",
    "abstract": "With the increasing popularity of speech synthesis products, the industry has\nput forward more requirements for personalized speech synthesis: (1) How to use\nlow-resource, easily accessible data to clone a person's voice. (2) How to\nclone a person's voice while controlling the style and prosody. To solve the\nabove two problems, we proposed the Hieratron model framework in which the\nprosody and timbre are modeled separately using two modules, therefore, the\nindependent control of timbre and the other characteristics of audio can be\nachieved while generating speech. The practice shows that, for very limited\ntarget speaker data in the wild, Hieratron has obvious advantages over the\ntraditional method, in addition to controlling the style and language of the\ngenerated speech, the mean opinion score on speech quality of the generated\nspeech has also been improved by more than 0.2 points.",
    "descriptor": "",
    "authors": [
      "Dongyang Dai",
      "Yuanzhe Chen",
      "Li Chen",
      "Ming Tu",
      "Lu Liu",
      "Rui Xia",
      "Qiao Tian",
      "Yuping Wang",
      "Yuxuan Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Human-Computer Interaction (cs.HC)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03347"
  },
  {
    "id": "arXiv:2110.03352",
    "title": "Optimized U-Net for Brain Tumor Segmentation",
    "abstract": "We propose an optimized U-Net architecture for a brain \\mbox{tumor}\nsegmentation task in the BraTS21 Challenge. To find the \\mbox{optimal} model\narchitecture and learning schedule we ran an extensive ablation study to test:\ndeep supervision loss, Focal loss, decoder attention, drop block, and residual\nconnections. Additionally, we have searched for the optimal depth of the U-Net\nand number of convolutional channels. Our solution was the winner of the\nchallenge validation phase, with the normalized statistical ranking score of\n0.267 and mean Dice score of 0.8855",
    "descriptor": "\nComments: 13 pages, 3 figures, MICCAI submission, BraTS21 submission\n",
    "authors": [
      "Micha\u0142 Futrega",
      "Alexandre Milesi",
      "Michal Marcinkiewicz",
      "Pablo Ribalta"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03352"
  },
  {
    "id": "arXiv:2110.03354",
    "title": "$\\bar{G}_{mst}$:An Unbiased Stratified Statistic and a Fast Gradient  Optimization Algorithm Based on It",
    "abstract": "-The fluctuation effect of gradient expectation and variance caused by\nparameter update between consecutive iterations is neglected or confusing by\ncurrent mainstream gradient optimization algorithms. The work in this paper\nremedy this issue by introducing a novel unbiased stratified statistic \\\n$\\bar{G}_{mst}$\\ , a sufficient condition of fast convergence for \\\n$\\bar{G}_{mst}$\\ also is established. A novel algorithm named MSSG designed\nbased on \\ $\\bar{G}_{mst}$\\ outperforms other sgd-like algorithms. Theoretical\nconclusions and experimental evidence strongly suggest to employ MSSG when\ntraining deep model.",
    "descriptor": "",
    "authors": [
      "Aixiang Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03354"
  },
  {
    "id": "arXiv:2110.03372",
    "title": "Unifying Likelihood-free Inference with Black-box Sequence Design and  Beyond",
    "abstract": "Black-box optimization formulations for biological sequence design have drawn\nrecent attention due to their promising potential impact on the pharmaceutical\nindustry. In this work, we propose to unify two seemingly distinct worlds:\nlikelihood-free inference and black-box sequence design, under one\nprobabilistic framework. In tandem, we provide a recipe for constructing\nvarious sequence design methods based on this framework. We show how previous\ndrug discovery approaches can be \"reinvented\" in our framework, and further\npropose new probabilistic sequence design algorithms. Extensive experiments\nillustrate the benefits of the proposed methodology.",
    "descriptor": "",
    "authors": [
      "Dinghuai Zhang",
      "Jie Fu",
      "Yoshua Bengio",
      "Aaron Courville"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03372"
  },
  {
    "id": "arXiv:2110.03396",
    "title": "AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning",
    "abstract": "Anomaly segmentation, which localizes defective areas, is an important\ncomponent in large-scale industrial manufacturing. However, most recent\nresearches have focused on anomaly detection. This paper proposes a novel\nanomaly segmentation network (AnoSeg) that can directly generate an accurate\nanomaly map using self-supervised learning. For highly accurate anomaly\nsegmentation, the proposed AnoSeg considers three novel techniques: Anomaly\ndata generation based on hard augmentation, self-supervised learning with\npixel-wise and adversarial losses, and coordinate channel concatenation. First,\nto generate synthetic anomaly images and reference masks for normal data, the\nproposed method uses hard augmentation to change the normal sample\ndistribution. Then, the proposed AnoSeg is trained in a self-supervised\nlearning manner from the synthetic anomaly data and normal data. Finally, the\ncoordinate channel, which represents the pixel location information, is\nconcatenated to an input of AnoSeg to consider the positional relationship of\neach pixel in the image. The estimated anomaly map can also be utilized to\nimprove the performance of anomaly detection. Our experiments show that the\nproposed method outperforms the state-of-the-art anomaly detection and anomaly\nsegmentation methods for the MVTec AD dataset. In addition, we compared the\nproposed method with the existing methods through the intersection over union\n(IoU) metric commonly used in segmentation tasks and demonstrated the\nsuperiority of our method for anomaly segmentation.",
    "descriptor": "\nComments: 10 pages, 17 figures\n",
    "authors": [
      "Jouwon Song",
      "Kyeongbo Kong",
      "Ye-In Park",
      "Seong-Gyun Kim",
      "Suk-Ju Kang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.03396"
  },
  {
    "id": "arXiv:2110.03413",
    "title": "Curved Markov Chain Monte Carlo for Network Learning",
    "abstract": "We present a geometrically enhanced Markov chain Monte Carlo sampler for\nnetworks based on a discrete curvature measure defined on graphs. Specifically,\nwe incorporate the concept of graph Forman curvature into sampling procedures\non both the nodes and edges of a network explicitly, via the transition\nprobability of the Markov chain, as well as implicitly, via the target\nstationary distribution, which gives a novel, curved Markov chain Monte Carlo\napproach to learning networks. We show that integrating curvature into the\nsampler results in faster convergence to a wide range of network statistics\ndemonstrated on deterministic networks drawn from real-world data.",
    "descriptor": "\nComments: 12 pages, 5 figures. To appear in Studies in Computational Intelligence: Proceedings of The 10th International Conference on Complex Networks and Their Applications (2021)\n",
    "authors": [
      "John Sigbeku",
      "Emil Saucan",
      "Anthea Monod"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.03413"
  },
  {
    "id": "arXiv:2110.03443",
    "title": "Unpacking the Black Box: Regulating Algorithmic Decisions",
    "abstract": "We characterize optimal oversight of algorithms in a world where an agent\ndesigns a complex prediction function but a principal is limited in the amount\nof information she can learn about the prediction function. We show that\nlimiting agents to prediction functions that are simple enough to be fully\ntransparent is inefficient as long as the bias induced by misalignment between\nprincipal's and agent's preferences is small relative to the uncertainty about\nthe true state of the world. Algorithmic audits can improve welfare, but the\ngains depend on the design of the audit tools. Tools that focus on minimizing\noverall information loss, the focus of many post-hoc explainer tools, will\ngenerally be inefficient since they focus on explaining the average behavior of\nthe prediction function rather than sources of mis-prediction, which matter for\nwelfare-relevant outcomes. Targeted tools that focus on the source of incentive\nmisalignment, e.g., excess false positives or racial disparities, can provide\nfirst-best solutions. We provide empirical support for our theoretical findings\nusing an application in consumer lending.",
    "descriptor": "",
    "authors": [
      "Laura Blattner",
      "Scott Nelson",
      "Jann Spiess"
    ],
    "subjectives": [
      "General Economics (econ.GN)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03443"
  },
  {
    "id": "arXiv:2110.03452",
    "title": "Inter-Domain Alignment for Predicting High-Resolution Brain Networks  Using Teacher-Student Learning",
    "abstract": "Accurate and automated super-resolution image synthesis is highly desired\nsince it has the great potential to circumvent the need for acquiring high-cost\nmedical scans and a time-consuming preprocessing pipeline of neuroimaging data.\nHowever, existing deep learning frameworks are solely designed to predict\nhigh-resolution (HR) image from a low-resolution (LR) one, which limits their\ngeneralization ability to brain graphs (i.e., connectomes). A small body of\nworks has focused on superresolving brain graphs where the goal is to predict a\nHR graph from a single LR graph. Although promising, existing works mainly\nfocus on superresolving graphs belonging to the same domain (e.g., functional),\noverlooking the domain fracture existing between multimodal brain data\ndistributions (e.g., morphological and structural). To this aim, we propose a\nnovel inter-domain adaptation framework namely, Learn to SuperResolve Brain\nGraphs with Knowledge Distillation Network (L2S-KDnet), which adopts a\nteacher-student paradigm to superresolve brain graphs. Our teacher network is a\ngraph encoder-decoder that firstly learns the LR brain graph embeddings, and\nsecondly learns how to align the resulting latent representations to the HR\nground truth data distribution using an adversarial regularization. Ultimately,\nit decodes the HR graphs from the aligned embeddings. Next, our student network\nlearns the knowledge of the aligned brain graphs as well as the topological\nstructure of the predicted HR graphs transferred from the teacher. We further\nleverage the decoder of the teacher to optimize the student network. L2S-KDnet\npresents the first TS architecture tailored for brain graph super-resolution\nsynthesis that is based on inter-domain alignment. Our experimental results\ndemonstrate substantial performance gains over benchmark methods.",
    "descriptor": "",
    "authors": [
      "Basar Demir",
      "Alaa Bessadok",
      "Islem Rekik"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2110.03452"
  },
  {
    "id": "arXiv:2110.03460",
    "title": "Finding popular branchings in vertex-weighted digraphs",
    "abstract": "Popular matchings have been intensively studied recently as a relaxed concept\nof stable matchings. By applying the concept of popular matchings to branchings\nin directed graphs, Kavitha et al.\\ (2020) introduced popular branchings. In a\ndirected graph $G=(V_G,E_G)$, each vertex has preferences over its incoming\nedges. For branchings $B_1$ and $B_2$ in $G$, a vertex $v\\in V_G$ prefers $B_1$\nto $B_2$ if $v$ prefers its incoming edge of $B_1$ to that of $B_2$, where\nhaving an arbitrary incoming edge is preferred to having none, and $B_1$ is\nmore popular than $B_2$ if the number of vertices that prefer $B_1$ is greater\nthan the number of vertices that prefer $B_2$. A branching $B$ is called a\npopular branching if there is no branching more popular than $B$. Kavitha et\nal.\\ (2020) proposed an algorithm for finding a popular branching when the\npreferences of each vertex are given by a strict partial order. The validity of\nthis algorithm is proved by utilizing classical theorems on the duality of\nweighted arborescences. In this paper, we generalize popular branchings to\nweighted popular branchings in vertex-weighted directed graphs in the same\nmanner as weighted popular matchings by Mestre (2014). We give an algorithm for\nfinding a weighted popular branching, which extends the algorithm of Kavitha et\nal., when the preferences of each vertex are given by a total preorder and the\nweights satisfy certain conditions. Our algorithm includes elaborated\nprocedures resulting from the vertex-weights, and its validity is proved by\nextending the argument of the duality of weighted arborescences.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Kei Natsui",
      "Kenjiro Takazawa"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.03460"
  },
  {
    "id": "arXiv:2110.03501",
    "title": "Pretrained Language Models are Symbolic Mathematics Solvers too!",
    "abstract": "Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npretrain our model with different pairs of language translations. Our results\nshow language bias in solving symbolic mathematics tasks. Finally, we study the\nrobustness of the fine-tuned model on symbolic math tasks against distribution\nshift, and our approach generalizes better in distribution shift scenarios for\nthe function integration.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Kimia Noorbakhsh",
      "Modar Sulaiman",
      "Mahdi Sharifi",
      "Kallol Roy",
      "Pooyan Jamshidi"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03501"
  },
  {
    "id": "arXiv:2110.03511",
    "title": "Peer Collaborative Learning for Polyphonic Sound Event Detection",
    "abstract": "This paper describes that semi-supervised learning called peer collaborative\nlearning (PCL) can be applied to the polyphonic sound event detection (PSED)\ntask, which is one of the tasks in the Detection and Classification of Acoustic\nScenes and Events (DCASE) challenge. Many deep learning models have been\nstudied to find out what kind of sound events occur where and for how long in a\ngiven audio clip. The characteristic of PCL used in this paper is the\ncombination of ensemble-based knowledge distillation into sub-networks and\nstudent-teacher model-based knowledge distillation, which can train a robust\nPSED model from a small amount of strongly labeled data, weakly labeled data,\nand a large amount of unlabeled data. We evaluated the proposed PCL model using\nthe DCASE 2019 Task 4 datasets and achieved an F1-score improvement of about\n10% compared to the baseline model.",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Hayato Endo",
      "Hiromitsu Nishizaki"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03511"
  },
  {
    "id": "arXiv:2110.03513",
    "title": "Accelerated Componentwise Gradient Boosting using Efficient Data  Representation and Momentum-based Optimization",
    "abstract": "Componentwise boosting (CWB), also known as model-based boosting, is a\nvariant of gradient boosting that builds on additive models as base learners to\nensure interpretability. CWB is thus often used in research areas where models\nare employed as tools to explain relationships in data. One downside of CWB is\nits computational complexity in terms of memory and runtime. In this paper, we\npropose two techniques to overcome these issues without losing the properties\nof CWB: feature discretization of numerical features and incorporating Nesterov\nmomentum into functional gradient descent. As the latter can be prone to early\noverfitting, we also propose a hybrid approach that prevents a possibly\ndiverging gradient descent routine while ensuring faster convergence. We\nperform extensive benchmarks on multiple simulated and real-world data sets to\ndemonstrate the improvements in runtime and memory consumption while\nmaintaining state-of-the-art estimation and prediction performance.",
    "descriptor": "",
    "authors": [
      "Daniel Schalk",
      "Bernd Bischl",
      "David R\u00fcgamer"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03513"
  },
  {
    "id": "arXiv:2110.03523",
    "title": "Range and Bearing Data Fusion for Precise Convex Network Localization",
    "abstract": "Hybrid localization in GNSS-challenged environments using measured ranges and\nangles is becoming increasingly popular, in particular with the advent of\nmultimodal communication systems. Here, we address the hybrid network\nlocalization problem using ranges and bearings to jointly determine the\npositions of a number of agents through a single maximum-likelihood (ML)\noptimization problem that seamlessly fuses all the available pairwise range and\nangle measurements. We propose a tight convex surrogate to the ML estimator, we\nexamine practical measures for the accuracy of the relaxation, and we\ncomprehensively characterize its behavior in simulation. We found that our\nrelaxation outperforms a state of the art SDP relaxation by one order of\nmagnitude in terms of localization error, and is amenable to much more\nlightweight solution algorithms.",
    "descriptor": "",
    "authors": [
      "Claudia Soares",
      "Filipa Valdeira",
      "Joao Gomes"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2110.03523"
  },
  {
    "id": "arXiv:2110.03528",
    "title": "Decoding ECoG signal into 3D hand translation using deep learning",
    "abstract": "Motor brain-computer interfaces (BCIs) are a promising technology that may\nenable motor-impaired people to interact with their environment. Designing\nreal-time and accurate BCI is crucial to make such devices useful, safe, and\neasy to use by patients in a real-life environment. Electrocorticography\n(ECoG)-based BCIs emerge as a good compromise between invasiveness of the\nrecording device and good spatial and temporal resolution of the recorded\nsignal. However, most ECoG signal decoders used to predict continuous hand\nmovements are linear models. These models have a limited representational\ncapacity and may fail to capture the relationship between ECoG signal and\ncontinuous hand movements. Deep learning (DL) models, which are\nstate-of-the-art in many problems, could be a solution to better capture this\nrelationship. In this study, we tested several DL-based architectures to\npredict imagined 3D continuous hand translation using time-frequency features\nextracted from ECoG signals. The dataset used in the analysis is a part of a\nlong-term clinical trial (ClinicalTrials.gov identifier: NCT02550522) and was\nacquired during a closed-loop experiment with a tetraplegic subject. The\nproposed architectures include multilayer perceptron (MLP), convolutional\nneural networks (CNN), and long short-term memory networks (LSTM). The accuracy\nof the DL-based and multilinear models was compared offline using cosine\nsimilarity. Our results show that CNN-based architectures outperform the\ncurrent state-of-the-art multilinear model. The best architecture exploited the\nspatial correlation between neighboring electrodes with CNN and benefited from\nthe sequential character of the desired hand trajectory by using LSTMs.\nOverall, DL increased the average cosine similarity, compared to the\nmultilinear model, by up to 60%, from 0.189 to 0.302 and from 0.157 to 0.249\nfor the left and right hand, respectively.",
    "descriptor": "",
    "authors": [
      "Maciej \u015aliwowski",
      "Matthieu Martin",
      "Antoine Souloumiac",
      "Pierre Blanchart",
      "Tetiana Aksenova"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03528"
  },
  {
    "id": "arXiv:2110.03529",
    "title": "Using Single-Trial Representational Similarity Analysis with EEG to  track semantic similarity in emotional word processing",
    "abstract": "Electroencephalography (EEG) is a powerful non-invasive brain imaging\ntechnique with a high temporal resolution that has seen extensive use across\nmultiple areas of cognitive science research. This thesis adapts\nrepresentational similarity analysis (RSA) to single-trial EEG datasets and\nintroduces its principles to EEG researchers unfamiliar with multivariate\nanalyses. We have two separate aims: 1. we want to explore the effectiveness of\nsingle-trial RSA on EEG datasets; 2. we want to utilize single-trial RSA and\ncomputational semantic models to investigate the role of semantic meaning in\nemotional word processing. We report two primary findings: 1. single-trial RSA\non EEG datasets can produce meaningful and interpretable results given a high\nnumber of trials and subjects; 2. single-trial RSA reveals that emotional\nprocessing in the 500-800ms time window is associated with additional semantic\nanalysis.",
    "descriptor": "",
    "authors": [
      "Feng Cheng"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.03529"
  },
  {
    "id": "arXiv:2110.03535",
    "title": "A Few-shot Learning Graph Multi-Trajectory Evolution Network for  Forecasting Multimodal Baby Connectivity Development from a Baseline  Timepoint",
    "abstract": "Charting the baby connectome evolution trajectory during the first year after\nbirth plays a vital role in understanding dynamic connectivity development of\nbaby brains. Such analysis requires acquisition of longitudinal connectomic\ndatasets. However, both neonatal and postnatal scans are rarely acquired due to\nvarious difficulties. A small body of works has focused on predicting baby\nbrain evolution trajectory from a neonatal brain connectome derived from a\nsingle modality. Although promising, large training datasets are essential to\nboost model learning and to generalize to a multi-trajectory prediction from\ndifferent modalities (i.e., functional and morphological connectomes). Here, we\nunprecedentedly explore the question: Can we design a few-shot learning-based\nframework for predicting brain graph trajectories across different modalities?\nTo this aim, we propose a Graph Multi-Trajectory Evolution Network (GmTE-Net),\nwhich adopts a teacher-student paradigm where the teacher network learns on\npure neonatal brain graphs and the student network learns on simulated brain\ngraphs given a set of different timepoints. To the best of our knowledge, this\nis the first teacher-student architecture tailored for brain graph\nmulti-trajectory growth prediction that is based on few-shot learning and\ngeneralized to graph neural networks (GNNs). To boost the performance of the\nstudent network, we introduce a local topology-aware distillation loss that\nforces the predicted graph topology of the student network to be consistent\nwith the teacher network. Experimental results demonstrate substantial\nperformance gains over benchmark methods. Hence, our GmTE-Net can be leveraged\nto predict atypical brain connectivity trajectory evolution across various\nmodalities. Our code is available at https: //github.com/basiralab/GmTE-Net.",
    "descriptor": "",
    "authors": [
      "Alaa Bessadok",
      "Ahmed Nebli",
      "Mohamed Ali Mahjoub",
      "Gang Li",
      "Weili Lin",
      "Dinggang Shen",
      "Islem Rekik"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03535"
  },
  {
    "id": "arXiv:2110.03557",
    "title": "Group synchrony, parameter mismatches, and intragroup connections",
    "abstract": "Group synchronization arises when two or more synchronization patterns\ncoexist in a network formed of oscillators of different types, with the systems\nin each group synchronizing on the same time-evolution, but systems in\ndifferent groups synchronizing on distinct time-evolutions. Group\nsynchronization has been observed and characterized when the systems in each\ngroup are identical and the couplings between the systems satisfy specific\nconditions. By relaxing these constraints and allowing them to be satisfied in\nan approximate rather than exact way, we observe that stable group\nsynchronization may still occur in the presence of small deviations of the\nparameters of the individual systems and of the couplings from their nominal\nvalues. We analyze this case and provide necessary and sufficient conditions\nfor stability through a master stability function approach, which also allows\nus to quantify the synchronization error. We also investigate the stability of\ngroup synchronization in the presence of intra-group connections and for this\ncase, extend some of the existing results in the literature. Our analysis\npoints out a broader class of matrices describing intra-group connections for\nwhich the stability problem can be reduced in a low-dimensional form.",
    "descriptor": "",
    "authors": [
      "Shirin Panahi",
      "Francesco Sorrentino"
    ],
    "subjectives": [
      "Chaotic Dynamics (nlin.CD)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03557"
  },
  {
    "id": "arXiv:2110.03588",
    "title": "A transformer-based deep learning approach for classifying brain  metastases into primary organ sites using clinical whole brain MRI images",
    "abstract": "The treatment decisions for brain metastatic disease are driven by knowledge\nof the primary organ site cancer histology, often requiring invasive biopsy.\nThis study aims to develop a novel deep learning approach for accurate and\nrapid non-invasive identification of brain metastatic tumor histology with\nconventional whole-brain MRI. The use of clinical whole-brain data and the\nend-to-end pipeline obviate external human intervention. This IRB-approved\nsingle-site retrospective study was comprised of patients (n=1,293) referred\nfor MRI treatment-planning and gamma knife radiosurgery from July 2000 to May\n2019. Contrast-enhanced T1-weighted contrast enhanced and\nT2-weighted-Fluid-Attenuated Inversion Recovery brain MRI exams (n=1,428) were\nminimally preprocessed (voxel resolution unification and signal-intensity\nrescaling/normalization), requiring only seconds per an MRI scan, and input\ninto the proposed deep learning workflow for tumor segmentation, modality\ntransfer, and primary site classification associated with brain metastatic\ndisease in one of four classes (lung, melanoma, renal, and other). Ten-fold\ncross-validation generated the overall AUC of 0.941, lung class AUC of 0.899,\nmelanoma class AUC of 0.882, renal class AUC of 0.870, and other class AUC of\n0.885. It is convincingly established that whole-brain imaging features would\nbe sufficiently discriminative to allow accurate diagnosis of the primary organ\nsite of malignancy. Our end-to-end deep learning-based radiomic method has a\ngreat translational potential for classifying metastatic tumor types using\nwhole-brain MRI images, without additional human intervention. Further\nrefinement may offer invaluable tools to expedite primary organ site cancer\nidentification for treatment of brain metastatic disease and improvement of\npatient outcomes and survival.",
    "descriptor": "",
    "authors": [
      "Qing Lyu",
      "Sanjeev V. Namjoshi",
      "Emory McTyre",
      "Umit Topaloglu",
      "Richard Barcus",
      "Michael D. Chan",
      "Christina K. Cramer",
      "Waldemar Debinski",
      "Metin N. Gurcan",
      "Glenn J. Lesser",
      "Hui-Kuan Lin",
      "Reginald F. Munden",
      "Boris C. Pasche",
      "Kiran Kumar Solingapuram Sai",
      "Roy E. Strowd",
      "Stephen B. Tatter",
      "Kounosuke Watabe",
      "Wei Zhang",
      "Ge Wang",
      "Christopher T. Whitlow"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03588"
  },
  {
    "id": "arXiv:2110.03594",
    "title": "Ship Performance Monitoring using Machine-learning",
    "abstract": "The hydrodynamic performance of a sea-going ship varies over its lifespan due\nto factors like marine fouling and the condition of the anti-fouling paint\nsystem. In order to accurately estimate the power demand and fuel consumption\nfor a planned voyage, it is important to assess the hydrodynamic performance of\nthe ship. The current work uses machine-learning (ML) methods to estimate the\nhydrodynamic performance of a ship using the onboard recorded in-service data.\nThree ML methods, NL-PCR, NL-PLSR and probabilistic ANN, are calibrated using\nthe data from two sister ships. The calibrated models are used to extract the\nvarying trend in ship's hydrodynamic performance over time and predict the\nchange in performance through several propeller and hull cleaning events. The\npredicted change in performance is compared with the corresponding values\nestimated using the fouling friction coefficient ($\\Delta C_F$). The ML methods\nare found to be performing well while modelling the hydrodynamic state\nvariables of the ships with probabilistic ANN model performing the best, but\nthe results from NL-PCR and NL-PLSR are not far behind, indicating that it may\nbe possible to use simple methods to solve such problems with the help of\ndomain knowledge.",
    "descriptor": "\nComments: Preprint submitted to Journal of Ocean Engineering\n",
    "authors": [
      "Prateek Gupta",
      "Adil Rasheed",
      "Sverre Steen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.03594"
  },
  {
    "id": "arXiv:2110.03623",
    "title": "From Contraction Theory to Fixed Point Algorithms on Riemannian and  Non-Euclidean Spaces",
    "abstract": "The design of fixed point algorithms is at the heart of monotone operator\ntheory, convex analysis, and of many modern optimization problems arising in\nmachine learning and control. This tutorial reviews recent advances in\nunderstanding the relationship between Demidovich conditions, one-sided\nLipschitz conditions, and contractivity theorems. We review the standard\ncontraction theory on Euclidean spaces as well as little-known results for\nRiemannian manifolds. Special emphasis is placed on the setting of\nnon-Euclidean norms and the recently introduced weak pairings for the $\\ell_1$\nand $\\ell_\\infty$ norms. We highlight recent results on explicit and implicit\nfixed point schemes for non-Euclidean contracting systems.",
    "descriptor": "\nComments: Paper in the invited tutorial session \"Contraction Theory for Machine Learning\" at 60th IEEE Conference on Decision and Control, 2021\n",
    "authors": [
      "Francesco Bullo",
      "Pedro Cisneros-Velarde",
      "Alexander Davydov",
      "Saber Jafarpour"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03623"
  },
  {
    "id": "arXiv:2110.03630",
    "title": "Towards Faster Continuous Multi-Channel HRTF Measurements Based on  Learning System Models",
    "abstract": "Measuring personal head-related transfer functions (HRTFs) is essential in\nbinaural audio. Personal HRTFs are not only required for binaural rendering and\nfor loudspeaker-based binaural reproduction using crosstalk cancellation, but\nthey also serve as a basis for data-driven HRTF individualization techniques\nand psychoacoustic experiments. Although many attempts have been made to\nexpedite HRTF measurements, the rotational velocities in today's measurement\nsystems remain lower than those in natural head movements. To cope with faster\nrotations, we present a novel continuous HRTF measurement method. This method\nestimates the HRTFs offline using a Kalman smoother and learns state-space\nparameters, including the system model, on short signal segments, utilizing the\nexpectation maximization algorithm. We evaluated our method in simulated\nsingle-channel and multi-channel measurements using a rigid sphere HRTF model.\nComparing with conventional methods, we found that the system distances are\nimproved by up to 30 dB.",
    "descriptor": "\nComments: 5 pages, 4 figures, submitted to ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n",
    "authors": [
      "Tobias Kabzinski",
      "Peter Jax"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2110.03630"
  },
  {
    "id": "arXiv:2110.03633",
    "title": "Regression markets and application to energy forecasting",
    "abstract": "Energy forecasting has attracted enormous attention over the last few\ndecades, with novel proposals related to the use of heterogeneous data sources,\nprobabilistic forecasting, online learn-ing, etc. A key aspect that emerged is\nthat learning and forecasting may highly benefit from distributed data, though\nnot only in the geographical sense. That is, various agents collect and own\ndata that may be useful to others. In contrast to recent proposals that look\ninto distributed and privacy-preserving learning (incentive-free), we explore\nhere a framework called regression markets. There, agents aiming to improve\ntheir forecasts post a regression task, for which other agents may contribute\nby sharing their data for their features and get monetarily rewarded for it.The\nmarket design is for regression models that are linear in their parameters, and\npossibly sep-arable, with estimation performed based on either batch or online\nlearning. Both in-sample and out-of-sample aspects are considered, with markets\nfor fitting models in-sample, and then for improving genuine forecasts\nout-of-sample. Such regression markets rely on recent concepts within\ninterpretability of machine learning approaches and cooperative game theory,\nwith Shapley additive explanations. Besides introducing the market design and\nproving its desirable properties, application results are shown based on\nsimulation studies (to highlight the salient features of the proposal) and with\nreal-world case studies.",
    "descriptor": "",
    "authors": [
      "Pierre Pinson",
      "Liyang Han",
      "Jalal Kazempour"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.03633"
  },
  {
    "id": "arXiv:2110.03636",
    "title": "A Hybrid Direct-Iterative Method for Solving KKT Linear Systems",
    "abstract": "We propose a solution strategy for linear systems arising in interior method\noptimization, which is suitable for implementation on hardware accelerators\nsuch as graphical processing units (GPUs). The current gold standard for\nsolving these systems is the LDL^T factorization. However, LDL^T requires\npivoting during factorization, which substantially increases communication cost\nand degrades performance on GPUs. Our novel approach solves a large indefinite\nsystem by solving multiple smaller positive definite systems, using an\niterative solve for the Schur complement and an inner direct solve (via\nCholesky factorization) within each iteration. Cholesky is stable without\npivoting, thereby reducing communication and allowing reuse of the symbolic\nfactorization. We demonstrate the practicality of our approach and show that on\nlarge systems it can efficiently utilize GPUs and outperform LDL^T\nfactorization of the full system.",
    "descriptor": "\nComments: 22 pages, 9 figures, 7 tables\n",
    "authors": [
      "Shaked Regev",
      "Nai-Yuan Chiang",
      "Eric Darve",
      "Cosmin G. Petra",
      "Michael A. Saunders",
      "Kasia \u015awirydowicz",
      "Slaven Pele\u0161"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.03636"
  },
  {
    "id": "arXiv:2110.03647",
    "title": "A criterion for critical junctions in elastic-plastic adhesive wear",
    "abstract": "We investigate elastic-plastic adhesive wear via a continuum variational\nphase-field approach. The model seamlessly captures the transition from\nperfectly brittle, over quasi-brittle to elastic-plastic wear regimes, as the\nductility of the contacting material increases. Simulation results highlight\nthe existence of a critical condition that morphological features and material\nductility need to satisfy for the adhesive junction to detach a wear debris. We\npropose a new criterion to discriminate between non-critical and critical\nasperity contacts, where the former produce negligible wear while the latter\nlead to significant debris formation.",
    "descriptor": "",
    "authors": [
      "Stella Brach",
      "Sylvain Collet"
    ],
    "subjectives": [
      "Other Condensed Matter (cond-mat.other)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2110.03647"
  },
  {
    "id": "arXiv:2110.03673",
    "title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks",
    "abstract": "A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron,\n2018) provides bounds on the width $n$ of a ReLU two-layer neural network\nneeded to approximate a function $f$ over the ball $\\mathcal{B}_R(\\R^d)$ up to\nerror $\\epsilon$, when the Fourier based quantity $C_f = \\int_{\\R^d} \\|\\xi\\|^2\n|\\hat{f}(\\xi)| \\ d\\xi$ is finite. More recently Ongie et al. (2019) used the\nRadon transform as a tool for analysis of infinite-width ReLU two-layer\nnetworks. In particular, they introduce the concept of Radon-based\n$\\mathcal{R}$-norms and show that a function defined on $\\R^d$ can be\nrepresented as an infinite-width two-layer neural network if and only if its\n$\\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et\nal. (2019) and define similar Radon-based semi-norms ($\\mathcal{R},\n\\mathcal{U}$-norms) such that a function admits an infinite-width neural\nnetwork representation on a bounded open set $\\mathcal{U} \\subseteq \\R^d$ when\nits $\\mathcal{R}, \\mathcal{U}$-norm is finite. Building on this, we derive\nsparse (finite-width) neural network approximation bounds that refine those of\nBreiman (1993); Klusowski & Barron (2018). Finally, we show that infinite-width\nneural network representations on bounded open sets are not unique and study\ntheir structure, providing a functional view of mode connectivity.",
    "descriptor": "",
    "authors": [
      "Carles Domingo-Enrich",
      "Youssef Mroueh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.03673"
  },
  {
    "id": "arXiv:1807.02925",
    "title": "Vehicle Image Generation Going Well with The Surroundings",
    "abstract": "Vehicle Image Generation Going Well with The Surroundings",
    "descriptor": "",
    "authors": [
      "Jeesoo Kim",
      "Jangho Kim",
      "Jaeyoung Yoo",
      "Daesik Kim",
      "Nojun Kwak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/1807.02925"
  },
  {
    "id": "arXiv:1812.06519",
    "title": "A standard and linear fan-beam Fourier backprojection theorem",
    "abstract": "A standard and linear fan-beam Fourier backprojection theorem",
    "descriptor": "",
    "authors": [
      "Patricio Guerrero",
      "Matheus Bernardi",
      "Eduardo X. Miqueles"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1812.06519"
  },
  {
    "id": "arXiv:1812.09234",
    "title": "A Primal-dual Learning Algorithm for Personalized Dynamic Pricing with  an Inventory Constraint",
    "abstract": "A Primal-dual Learning Algorithm for Personalized Dynamic Pricing with  an Inventory Constraint",
    "descriptor": "",
    "authors": [
      "Ningyuan Chen",
      "Guillermo Gallego"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1812.09234"
  },
  {
    "id": "arXiv:1812.11479",
    "title": "Abelian varieties with prescribed embedding and embedding degrees",
    "abstract": "Comments: Typos fixed",
    "descriptor": "\nComments: Typos fixed\n",
    "authors": [
      "Steve Thakur"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/1812.11479"
  },
  {
    "id": "arXiv:1901.03371",
    "title": "Optimal Asynchronous Dynamic Policies in Energy-Efficient Data Centers",
    "abstract": "Comments: There are several mistakes and errors in sections 5 and 6 of the paper, now we are more busy not to be able to process this paper. Once we complete the reprocessing, we submit it to arXive again. Many thanks for your great help",
    "descriptor": "\nComments: There are several mistakes and errors in sections 5 and 6 of the paper, now we are more busy not to be able to process this paper. Once we complete the reprocessing, we submit it to arXive again. Many thanks for your great help\n",
    "authors": [
      "Jing-Yu Ma",
      "Quan-Lin Li",
      "Li Xia"
    ],
    "subjectives": [
      "Performance (cs.PF)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/1901.03371"
  },
  {
    "id": "arXiv:1907.07175",
    "title": "Measuring Scientific Brain Drain with Hubs and Authorities: a Dual  Perspective",
    "abstract": "Comments: Paper published on Online Social Networks and Media, doi: this https URL",
    "descriptor": "\nComments: Paper published on Online Social Networks and Media, doi: this https URL\n",
    "authors": [
      "Alessandra Urbinati",
      "Edoardo Galimberti",
      "Giancarlo Ruffo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/1907.07175"
  },
  {
    "id": "arXiv:1910.01931",
    "title": "Sparse Popularity Adjusted Stochastic Block Model",
    "abstract": "Comments: 4 figures. arXiv admin note: text overlap with arXiv:1902.00431",
    "descriptor": "\nComments: 4 figures. arXiv admin note: text overlap with arXiv:1902.00431\n",
    "authors": [
      "Majid Noroozi",
      "Marianna Pensky",
      "Ramchandra Rimal"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/1910.01931"
  },
  {
    "id": "arXiv:1912.09160",
    "title": "On the stability of Scott-Zhang type operators and application to  multilevel preconditioning in fractional diffusion",
    "abstract": "On the stability of Scott-Zhang type operators and application to  multilevel preconditioning in fractional diffusion",
    "descriptor": "",
    "authors": [
      "Markus Faustmann",
      "Jens Markus Melenk",
      "Maryam Parvizi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1912.09160"
  },
  {
    "id": "arXiv:2002.05954",
    "title": "Learning Functionally Decomposed Hierarchies for Continuous Control  Tasks with Path Planning",
    "abstract": "Learning Functionally Decomposed Hierarchies for Continuous Control  Tasks with Path Planning",
    "descriptor": "",
    "authors": [
      "Sammy Christen",
      "Lukas Jendele",
      "Emre Aksan",
      "Otmar Hilliges"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2002.05954"
  },
  {
    "id": "arXiv:2003.06701",
    "title": "A Kogbetliantz-type algorithm for the hyperbolic SVD",
    "abstract": "Comments: Accepted for publication in Numerical Algorithms. This version slightly differs from the accepted one, with several small corrections and an alternative (hopefully more stable) data server listed",
    "descriptor": "\nComments: Accepted for publication in Numerical Algorithms. This version slightly differs from the accepted one, with several small corrections and an alternative (hopefully more stable) data server listed\n",
    "authors": [
      "Vedran Novakovi\u0107",
      "Sanja Singer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2003.06701"
  },
  {
    "id": "arXiv:2003.10166",
    "title": "Constrained Control and Observer Design by Inverse Optimality",
    "abstract": "Comments: Submitted for publication",
    "descriptor": "\nComments: Submitted for publication\n",
    "authors": [
      "Mario Zanon",
      "Alberto Bemporad"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2003.10166"
  },
  {
    "id": "arXiv:2006.06320",
    "title": "Hypernetwork-Based Augmentation",
    "abstract": "Hypernetwork-Based Augmentation",
    "descriptor": "",
    "authors": [
      "Chih-Yang Chen",
      "Che-Han Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2006.06320"
  },
  {
    "id": "arXiv:2006.06880",
    "title": "Reintroducing Straight-Through Estimators as Principled Methods for  Stochastic Binary Networks",
    "abstract": "Comments: 34 pages, DAGM 2021 version (published)",
    "descriptor": "\nComments: 34 pages, DAGM 2021 version (published)\n",
    "authors": [
      "Alexander Shekhovtsov",
      "Viktor Yanush"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2006.06880"
  },
  {
    "id": "arXiv:2006.10670",
    "title": "On a subdiffusive tumour growth model with fractional time derivative",
    "abstract": "On a subdiffusive tumour growth model with fractional time derivative",
    "descriptor": "",
    "authors": [
      "Marvin Fritz",
      "Christina Kuttler",
      "Mabel L. Rajendran",
      "Barbara Wohlmuth",
      "Laura Scarabosio"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Cell Behavior (q-bio.CB)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2006.10670"
  },
  {
    "id": "arXiv:2006.12000",
    "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
    "abstract": "Comments: Accepted at ICCV 2021 (oral presentation)",
    "descriptor": "\nComments: Accepted at ICCV 2021 (oral presentation)\n",
    "authors": [
      "Kyungyul Kim",
      "ByeongMoon Ji",
      "Doyoung Yoon",
      "Sangheum Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.12000"
  },
  {
    "id": "arXiv:2006.14774",
    "title": "Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User  MIMO Communications",
    "abstract": "Comments: 19 pages, 8 figures",
    "descriptor": "\nComments: 19 pages, 8 figures\n",
    "authors": [
      "Jiawei Liu",
      "Kumar Vijay Mishra",
      "Mohammad Saquib"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2006.14774"
  },
  {
    "id": "arXiv:2007.04911",
    "title": "GAMA: a General Automated Machine learning Assistant",
    "abstract": "Comments: Accepted at ECML-PKDD 2020 Demo Track",
    "descriptor": "\nComments: Accepted at ECML-PKDD 2020 Demo Track\n",
    "authors": [
      "Pieter Gijsbers",
      "Joaquin Vanschoren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.04911"
  },
  {
    "id": "arXiv:2008.00149",
    "title": "Hybridization and postprocessing in finite element exterior calculus",
    "abstract": "Comments: 30 pages; v2: major revisions to reduce length (cut review material from Section 2, eliminated Sections 3.4-3.5, fewer numerical experiments in Section 7), new results on trace norms (Lemma 2.3) and well-posedness (Theorem 3.4), and minor edits",
    "descriptor": "\nComments: 30 pages; v2: major revisions to reduce length (cut review material from Section 2, eliminated Sections 3.4-3.5, fewer numerical experiments in Section 7), new results on trace norms (Lemma 2.3) and well-posedness (Theorem 3.4), and minor edits\n",
    "authors": [
      "Gerard Awanou",
      "Maurice Fabien",
      "Johnny Guzm\u00e1n",
      "Ari Stern"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2008.00149"
  },
  {
    "id": "arXiv:2009.01636",
    "title": "Multi-domain semantic segmentation with pyramidal fusion",
    "abstract": "Comments: 2 pages, 2 tables, no figures",
    "descriptor": "\nComments: 2 pages, 2 tables, no figures\n",
    "authors": [
      "Petra Bevandi\u0107",
      "Marin Or\u0161i\u0107",
      "Ivan Grubi\u0161i\u0107",
      "Josip \u0160ari\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.01636"
  },
  {
    "id": "arXiv:2009.06516",
    "title": "Justicia: A Stochastic SAT Approach to Formally Verify Fairness",
    "abstract": "Comments: 21 pages, 4 figures, 4 theorems",
    "descriptor": "\nComments: 21 pages, 4 figures, 4 theorems\n",
    "authors": [
      "Bishwamittra Ghosh",
      "Debabrota Basu",
      "Kuldeep S. Meel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2009.06516"
  },
  {
    "id": "arXiv:2009.12368",
    "title": "Generate Novel Molecules With Target Properties Using Conditional  Generative Models",
    "abstract": "Comments: The assumptions used while carrying out experiments was found to be incorrect",
    "descriptor": "\nComments: The assumptions used while carrying out experiments was found to be incorrect\n",
    "authors": [
      "Abhinav Sagar"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.12368"
  },
  {
    "id": "arXiv:2010.01196",
    "title": "End-to-End Differentiable Molecular Mechanics Force Field Construction",
    "abstract": "End-to-End Differentiable Molecular Mechanics Force Field Construction",
    "descriptor": "",
    "authors": [
      "Yuanqing Wang",
      "Josh Fass",
      "John D. Chodera"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2010.01196"
  },
  {
    "id": "arXiv:2010.03316",
    "title": "Batch Normalization Increases Adversarial Vulnerability and Decreases  Adversarial Transferability: A Non-Robust Feature Perspective",
    "abstract": "Comments: Accepted to ICCV 2021",
    "descriptor": "\nComments: Accepted to ICCV 2021\n",
    "authors": [
      "Philipp Benz",
      "Chaoning Zhang",
      "In So Kweon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.03316"
  },
  {
    "id": "arXiv:2010.11852",
    "title": "Efficient Robust Optimal Transport with Application to Multi-Label  Classification",
    "abstract": "Comments: Accepted to IEEE CDC 2021",
    "descriptor": "\nComments: Accepted to IEEE CDC 2021\n",
    "authors": [
      "Pratik Jawanpuria",
      "N T V Satyadev",
      "Bamdev Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2010.11852"
  },
  {
    "id": "arXiv:2011.00832",
    "title": "Sparse Multilevel Roadmaps for High-Dimensional Robot Motion Planning",
    "abstract": "Comments: 7 pages, 4 figures, Accepted at ICRA 2021",
    "descriptor": "\nComments: 7 pages, 4 figures, Accepted at ICRA 2021\n",
    "authors": [
      "Andreas Orthey",
      "Marc Toussaint"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2011.00832"
  },
  {
    "id": "arXiv:2011.01129",
    "title": "Multi-Agent Reinforcement Learning for Visibility-based Persistent  Monitoring",
    "abstract": "Comments: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021",
    "descriptor": "\nComments: Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021\n",
    "authors": [
      "Jingxi Chen",
      "Amrish Baskaran",
      "Zhongshun Zhang",
      "Pratap Tokekar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2011.01129"
  },
  {
    "id": "arXiv:2011.14048",
    "title": "Is Support Set Diversity Necessary for Meta-Learning?",
    "abstract": "Is Support Set Diversity Necessary for Meta-Learning?",
    "descriptor": "",
    "authors": [
      "Amrith Setlur",
      "Oscar Li",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.14048"
  },
  {
    "id": "arXiv:2012.00807",
    "title": "On the robustness of minimum norm interpolators and regularized  empirical risk minimizers",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Geoffrey Chinot",
      "Matthias L\u00f6ffler",
      "Sara van de Geer"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.00807"
  },
  {
    "id": "arXiv:2012.05846",
    "title": "Full-Glow: Fully conditional Glow for more realistic image generation",
    "abstract": "Comments: Accepted to DAGM GCPR 2021",
    "descriptor": "\nComments: Accepted to DAGM GCPR 2021\n",
    "authors": [
      "Moein Sorkhei",
      "Gustav Eje Henter",
      "Hedvig Kjellstr\u00f6m"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.05846"
  },
  {
    "id": "arXiv:2012.05896",
    "title": "Encoding Classical Information in Gauge Subsystems of Quantum Codes",
    "abstract": "Comments: 22 pages, 2 figures",
    "descriptor": "\nComments: 22 pages, 2 figures\n",
    "authors": [
      "Andrew Nemec",
      "Andreas Klappenecker"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.05896"
  },
  {
    "id": "arXiv:2012.15297",
    "title": "Trapping Sets of Quantum LDPC Codes",
    "abstract": "Comments: Revised version - 19 pages, 12 figures - Accepted for publication in Quantum",
    "descriptor": "\nComments: Revised version - 19 pages, 12 figures - Accepted for publication in Quantum\n",
    "authors": [
      "Nithin Raveendran",
      "Bane Vasi\u0107"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2012.15297"
  },
  {
    "id": "arXiv:2101.01673",
    "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons",
    "abstract": "Comments: Accepted to the 2nd Affinity Group Workshop on Diversity in Artificial Intelligence: Diversity, Belonging, Equity, and Inclusion (AIDBEI) at AAAI 2021 - PMLR version",
    "descriptor": "\nComments: Accepted to the 2nd Affinity Group Workshop on Diversity in Artificial Intelligence: Diversity, Belonging, Equity, and Inclusion (AIDBEI) at AAAI 2021 - PMLR version\n",
    "authors": [
      "Avijit Ghosh",
      "Lea Genuit",
      "Mary Reagan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2101.01673"
  },
  {
    "id": "arXiv:2101.09903",
    "title": "A Two-stage Framework for Compound Figure Separation",
    "abstract": "A Two-stage Framework for Compound Figure Separation",
    "descriptor": "",
    "authors": [
      "Weixin Jiang",
      "Eric Schwenker",
      "Trevor Spreadbury",
      "Nicola Ferrier",
      "Maria K.Y. Chan",
      "Oliver Cossairt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2101.09903"
  },
  {
    "id": "arXiv:2101.10539",
    "title": "Arabic aspect based sentiment analysis using bidirectional GRU based  models",
    "abstract": "Arabic aspect based sentiment analysis using bidirectional GRU based  models",
    "descriptor": "",
    "authors": [
      "Mohammed M.Abdelgwad",
      "Taysir Hassan A Soliman",
      "Ahmed I.Taloba",
      "Mohamed Fawzy Farghaly"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2101.10539"
  },
  {
    "id": "arXiv:2101.11246",
    "title": "A Survey on 5G Radio Access Network Energy Efficiency: Massive MIMO,  Lean Carrier Design, Sleep Modes, and Machine Learning",
    "abstract": "A Survey on 5G Radio Access Network Energy Efficiency: Massive MIMO,  Lean Carrier Design, Sleep Modes, and Machine Learning",
    "descriptor": "",
    "authors": [
      "David Lopez-Perez",
      "Antonio De Domenico",
      "Nicola Piovesan",
      "Harvey Bao",
      "Geng Xinli",
      "Song Qitao",
      "Merouane Debbah"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2101.11246"
  },
  {
    "id": "arXiv:2102.00457",
    "title": "MultiRocket: Multiple pooling operators and transformations for fast and  effective time series classification",
    "abstract": "MultiRocket: Multiple pooling operators and transformations for fast and  effective time series classification",
    "descriptor": "",
    "authors": [
      "Chang Wei Tan",
      "Angus Dempster",
      "Christoph Bergmeir",
      "Geoffrey I. Webb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.00457"
  },
  {
    "id": "arXiv:2102.03082",
    "title": "Achieving Explainability for Plant Disease Classification with  Disentangled Variational Autoencoders",
    "abstract": "Comments: 54 pages, 24 figures, 2 tables",
    "descriptor": "\nComments: 54 pages, 24 figures, 2 tables\n",
    "authors": [
      "Harshana Habaragamuwa",
      "Yu Oishi",
      "Kenichi Tanaka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.03082"
  },
  {
    "id": "arXiv:2102.04877",
    "title": "Noisy Recurrent Neural Networks",
    "abstract": "Comments: 38 pages",
    "descriptor": "\nComments: 38 pages\n",
    "authors": [
      "Soon Hoe Lim",
      "N. Benjamin Erichson",
      "Liam Hodgkinson",
      "Michael W. Mahoney"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2102.04877"
  },
  {
    "id": "arXiv:2102.06004",
    "title": "Fairness-Aware PAC Learning from Corrupted Data",
    "abstract": "Comments: 57 pages; New upper bounds results, significantly extended discussion",
    "descriptor": "\nComments: 57 pages; New upper bounds results, significantly extended discussion\n",
    "authors": [
      "Nikola Konstantinov",
      "Christoph H. Lampert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.06004"
  },
  {
    "id": "arXiv:2102.07437",
    "title": "Data Quality Matters For Adversarial Training: An Empirical Study",
    "abstract": "Data Quality Matters For Adversarial Training: An Empirical Study",
    "descriptor": "",
    "authors": [
      "Chengyu Dong",
      "Liyuan Liu",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07437"
  },
  {
    "id": "arXiv:2102.07824",
    "title": "A Koopman Approach to Understanding Sequence Neural Models",
    "abstract": "A Koopman Approach to Understanding Sequence Neural Models",
    "descriptor": "",
    "authors": [
      "Ilan Naiman",
      "Omri Azencot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.07824"
  },
  {
    "id": "arXiv:2102.09310",
    "title": "VAE Approximation Error: ELBO and Conditional Independence",
    "abstract": "VAE Approximation Error: ELBO and Conditional Independence",
    "descriptor": "",
    "authors": [
      "Dmitrij Schlesinger",
      "Alexander Shekhovtsov",
      "Boris Flach"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.09310"
  },
  {
    "id": "arXiv:2102.11901",
    "title": "Construction of arbitrary order finite element degree-of-freedom maps on  polygonal and polyhedral cell meshes",
    "abstract": "Construction of arbitrary order finite element degree-of-freedom maps on  polygonal and polyhedral cell meshes",
    "descriptor": "",
    "authors": [
      "Matthew W. Scroggs",
      "J\u00f8rgen S. Dokken",
      "Chris N. Richardson",
      "Garth N. Wells"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2102.11901"
  },
  {
    "id": "arXiv:2102.12274",
    "title": "A Multi-Objective Optimization Framework for URLLC with Decoding  Complexity Constraints",
    "abstract": "Comments: To appear in IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: To appear in IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Hasan Basri Celebi",
      "Antonios Pitarokoilis",
      "Mikael Skoglund"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2102.12274"
  },
  {
    "id": "arXiv:2102.12695",
    "title": "Orbital dynamics of binary black hole systems can be learned from  gravitational wave measurements",
    "abstract": "Orbital dynamics of binary black hole systems can be learned from  gravitational wave measurements",
    "descriptor": "",
    "authors": [
      "Brendan Keith",
      "Akshay Khadse",
      "Scott E. Field"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2102.12695"
  },
  {
    "id": "arXiv:2102.13635",
    "title": "Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel  Channel UT Scans",
    "abstract": "Using Deep Learning to Automate the Detection of Flaws in Nuclear Fuel  Channel UT Scans",
    "descriptor": "",
    "authors": [
      "Issam Hammad",
      "Ryan Simpson",
      "Hippolyte Djonon Tsague",
      "Sarah Hall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2102.13635"
  },
  {
    "id": "arXiv:2103.00241",
    "title": "Neural Architecture Search From Task Similarity Measure",
    "abstract": "Neural Architecture Search From Task Similarity Measure",
    "descriptor": "",
    "authors": [
      "Cat P. Le",
      "Mohammadreza Soltani",
      "Robert Ravier",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.00241"
  },
  {
    "id": "arXiv:2103.00713",
    "title": "Lower Bounds and Improved Algorithms for Asymmetric Streaming Edit  Distance and Longest Common Subsequence",
    "abstract": "Comments: This paper is accepted to Foundations of Software Technology and Theoretical Computer Science (FSTTCS) 2021",
    "descriptor": "\nComments: This paper is accepted to Foundations of Software Technology and Theoretical Computer Science (FSTTCS) 2021\n",
    "authors": [
      "Xin Li",
      "Yu Zheng"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2103.00713"
  },
  {
    "id": "arXiv:2103.00737",
    "title": "Meta-Learning an Inference Algorithm for Probabilistic Programs",
    "abstract": "Meta-Learning an Inference Algorithm for Probabilistic Programs",
    "descriptor": "",
    "authors": [
      "Gwonsoo Che",
      "Hongseok Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.00737"
  },
  {
    "id": "arXiv:2103.02303",
    "title": "Domain and View-point Agnostic Hand Action Recognition",
    "abstract": "Domain and View-point Agnostic Hand Action Recognition",
    "descriptor": "",
    "authors": [
      "Alberto Sabater",
      "I\u00f1igo Alonso",
      "Luis Montesano",
      "Ana C. Murillo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.02303"
  },
  {
    "id": "arXiv:2103.02631",
    "title": "RotoGrad: Gradient Homogenization in Multitask Learning",
    "abstract": "Comments: 23 pages, 9 figures",
    "descriptor": "\nComments: 23 pages, 9 figures\n",
    "authors": [
      "Adri\u00e1n Javaloy",
      "Isabel Valera"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2103.02631"
  },
  {
    "id": "arXiv:2103.04250",
    "title": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing",
    "abstract": "Greedy Approximation Algorithms for Active Sequential Hypothesis Testing",
    "descriptor": "",
    "authors": [
      "Kyra Gan",
      "Su Jia",
      "Andrew Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2103.04250"
  },
  {
    "id": "arXiv:2103.05036",
    "title": "Random 2-cell embeddings of multistars",
    "abstract": "Comments: 15 pages, 2 figures. Accepted to European conference on combinatorics, graph theory and applications (EUROCOMB) 2021",
    "descriptor": "\nComments: 15 pages, 2 figures. Accepted to European conference on combinatorics, graph theory and applications (EUROCOMB) 2021\n",
    "authors": [
      "Jesse Campion Loth",
      "Kevin Halasz",
      "Tom\u00e1\u0161 Masa\u0159\u00edk",
      "Bojan Mohar",
      "Robert \u0160\u00e1mal"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2103.05036"
  },
  {
    "id": "arXiv:2103.09708",
    "title": "What's in My LiDAR Odometry Toolbox?",
    "abstract": "Comments: This work was realised in the context of the PhD thesis of Pierre Dellenbach, financed by Kitware (this https URL), under the supervision of Bastien Jacquet (Kitware), Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette (Mines ParisTech)",
    "descriptor": "\nComments: This work was realised in the context of the PhD thesis of Pierre Dellenbach, financed by Kitware (this https URL), under the supervision of Bastien Jacquet (Kitware), Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette (Mines ParisTech)\n",
    "authors": [
      "Pierre Dellenbach",
      "Jean-Emmanuel Deschaud",
      "Bastien Jacquet",
      "Fran\u00e7ois Goulette"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.09708"
  },
  {
    "id": "arXiv:2103.10796",
    "title": "CoordiNet: uncertainty-aware pose regressor for reliable vehicle  localization",
    "abstract": "Comments: 8 pages, 8 figures. Accepted at WACV 2022",
    "descriptor": "\nComments: 8 pages, 8 figures. Accepted at WACV 2022\n",
    "authors": [
      "Arthur Moreau",
      "Nathan Piasco",
      "Dzmitry Tsishkou",
      "Bogdan Stanciulescu",
      "Arnaud de La Fortelle"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.10796"
  },
  {
    "id": "arXiv:2103.12306",
    "title": "GISE-51: A scalable isolated sound events dataset",
    "abstract": "Comments: Technical Report",
    "descriptor": "\nComments: Technical Report\n",
    "authors": [
      "Sarthak Yadav",
      "Mary Ellen Foster"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2103.12306"
  },
  {
    "id": "arXiv:2103.13971",
    "title": "Scaled relative graphs for system analysis",
    "abstract": "Comments: Accepted to the 2021 IEEE Conference on Decision and Control",
    "descriptor": "\nComments: Accepted to the 2021 IEEE Conference on Decision and Control\n",
    "authors": [
      "Thomas Chaffey",
      "Fulvio Forni",
      "Rodolphe Sepulchre"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2103.13971"
  },
  {
    "id": "arXiv:2103.14023",
    "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent  Forecasting",
    "abstract": "Comments: ICCV 2021. Code: this https URL Project page: this https URL",
    "descriptor": "\nComments: ICCV 2021. Code: this https URL Project page: this https URL\n",
    "authors": [
      "Ye Yuan",
      "Xinshuo Weng",
      "Yanglan Ou",
      "Kris Kitani"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2103.14023"
  },
  {
    "id": "arXiv:2103.15812",
    "title": "LatentKeypointGAN: Controlling GANs via Latent Keypoints",
    "abstract": "LatentKeypointGAN: Controlling GANs via Latent Keypoints",
    "descriptor": "",
    "authors": [
      "Xingzhe He",
      "Bastian Wandt",
      "Helge Rhodin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.15812"
  },
  {
    "id": "arXiv:2103.16748",
    "title": "Dual Contrastive Loss and Attention for GANs",
    "abstract": "Comments: Accepted to ICCV'21",
    "descriptor": "\nComments: Accepted to ICCV'21\n",
    "authors": [
      "Ning Yu",
      "Guilin Liu",
      "Aysegul Dundar",
      "Andrew Tao",
      "Bryan Catanzaro",
      "Larry Davis",
      "Mario Fritz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2103.16748"
  },
  {
    "id": "arXiv:2104.00098",
    "title": "Balancing Fairness and Efficiency in Traffic Routing via Interpolated  Traffic Assignment",
    "abstract": "Balancing Fairness and Efficiency in Traffic Routing via Interpolated  Traffic Assignment",
    "descriptor": "",
    "authors": [
      "Devansh Jalota",
      "Kiril Solovey",
      "Matthew Tsao",
      "Stephen Zoepf",
      "Marco Pavone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2104.00098"
  },
  {
    "id": "arXiv:2104.00322",
    "title": "Domain Invariant Adversarial Learning",
    "abstract": "Domain Invariant Adversarial Learning",
    "descriptor": "",
    "authors": [
      "Matan Levi",
      "Idan Attias",
      "Aryeh Kontorovich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.00322"
  },
  {
    "id": "arXiv:2104.02369",
    "title": "Classification with Runge-Kutta networks and feature space augmentation",
    "abstract": "Classification with Runge-Kutta networks and feature space augmentation",
    "descriptor": "",
    "authors": [
      "Elisa Giesecke",
      "Axel Kr\u00f6ner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2104.02369"
  },
  {
    "id": "arXiv:2104.03963",
    "title": "InfinityGAN: Towards Infinite-Pixel Image Synthesis",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Chieh Hubert Lin",
      "Hsin-Ying Lee",
      "Yen-Chi Cheng",
      "Sergey Tulyakov",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.03963"
  },
  {
    "id": "arXiv:2104.06619",
    "title": "An Integrated Optimization-Learning Framework for Online Combinatorial  Computation Offloading in MEC Networks",
    "abstract": "Comments: This paper has been accepted by IEEE Wireless Communications Magzine",
    "descriptor": "\nComments: This paper has been accepted by IEEE Wireless Communications Magzine\n",
    "authors": [
      "Xian Li",
      "Liang Huang",
      "Hui Wang",
      "Suzhi Bi",
      "Ying-Jun Angela Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2104.06619"
  },
  {
    "id": "arXiv:2104.07412",
    "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation",
    "abstract": "Comments: EMNLP 2021 camera-ready",
    "descriptor": "\nComments: EMNLP 2021 camera-ready\n",
    "authors": [
      "Sebastian Ruder",
      "Noah Constant",
      "Jan Botha",
      "Aditya Siddhant",
      "Orhan Firat",
      "Jinlan Fu",
      "Pengfei Liu",
      "Junjie Hu",
      "Dan Garrette",
      "Graham Neubig",
      "Melvin Johnson"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.07412"
  },
  {
    "id": "arXiv:2104.07684",
    "title": "Homomorphic Encryption-Enabled Distance-Based Distributed Formation  Control with Distance Mismatch Estimators",
    "abstract": "Comments: 8 pages, 6 figures, conference",
    "descriptor": "\nComments: 8 pages, 6 figures, conference\n",
    "authors": [
      "Mariano Perez Chaher",
      "Bayu Jayawardhana",
      "Junsoo Kim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.07684"
  },
  {
    "id": "arXiv:2104.08171",
    "title": "Safe Exploration in Model-based Reinforcement Learning using Control  Barrier Functions",
    "abstract": "Safe Exploration in Model-based Reinforcement Learning using Control  Barrier Functions",
    "descriptor": "",
    "authors": [
      "Max H. Cohen",
      "Calin Belta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2104.08171"
  },
  {
    "id": "arXiv:2104.08230",
    "title": "Point-Based Modeling of Human Clothing",
    "abstract": "Comments: Accepted for ICCV 2021",
    "descriptor": "\nComments: Accepted for ICCV 2021\n",
    "authors": [
      "Ilya Zakharkin",
      "Kirill Mazur",
      "Artur Grigorev",
      "Victor Lempitsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.08230"
  },
  {
    "id": "arXiv:2104.08259",
    "title": "Context-Adaptive Document-Level Neural Machine Translation",
    "abstract": "Context-Adaptive Document-Level Neural Machine Translation",
    "descriptor": "",
    "authors": [
      "Linlin Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08259"
  },
  {
    "id": "arXiv:2104.08401",
    "title": "Enriching a Model's Notion of Belief using a Persistent Memory",
    "abstract": "Comments: This is an old and now obsolete draft. See arXiv:2109.14723 (\"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief\") for the final paper",
    "descriptor": "\nComments: This is an old and now obsolete draft. See arXiv:2109.14723 (\"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief\") for the final paper\n",
    "authors": [
      "Nora Kassner",
      "Oyvind Tafjord",
      "Hinrich Schutze",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.08401"
  },
  {
    "id": "arXiv:2104.14548",
    "title": "With a Little Help from My Friends: Nearest-Neighbor Contrastive  Learning of Visual Representations",
    "abstract": "Comments: Accepted at ICCV 2021",
    "descriptor": "\nComments: Accepted at ICCV 2021\n",
    "authors": [
      "Debidatta Dwibedi",
      "Yusuf Aytar",
      "Jonathan Tompson",
      "Pierre Sermanet",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.14548"
  },
  {
    "id": "arXiv:2105.00925",
    "title": "Hyperspherically Regularized Networks for BYOL Improves Feature  Uniformity and Separability",
    "abstract": "Comments: 9 pages, 4 figures",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Aiden Durrant",
      "Georgios Leontidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.00925"
  },
  {
    "id": "arXiv:2105.01535",
    "title": "Fourier Plane-Wave Series Expansion for Holographic MIMO Communications",
    "abstract": "Comments: 32 pages, 9 figures, submitted to IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: 32 pages, 9 figures, submitted to IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Andrea Pizzo",
      "Luca Sanguinetti",
      "Thomas L. Marzetta"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2105.01535"
  },
  {
    "id": "arXiv:2105.02872",
    "title": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies",
    "abstract": "Comments: Accepted to ICCV 2021. The first two authors contributed equally to this paper. Project page: this https URL",
    "descriptor": "\nComments: Accepted to ICCV 2021. The first two authors contributed equally to this paper. Project page: this https URL\n",
    "authors": [
      "Sida Peng",
      "Junting Dong",
      "Qianqian Wang",
      "Shangzhan Zhang",
      "Qing Shuai",
      "Xiaowei Zhou",
      "Hujun Bao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.02872"
  },
  {
    "id": "arXiv:2105.04112",
    "title": "ROBI: A Multi-View Dataset for Reflective Objects in Robotic Bin-Picking",
    "abstract": "ROBI: A Multi-View Dataset for Reflective Objects in Robotic Bin-Picking",
    "descriptor": "",
    "authors": [
      "Jun Yang",
      "Yizhou Gao",
      "Dong Li",
      "Steven L. Waslander"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2105.04112"
  },
  {
    "id": "arXiv:2105.05016",
    "title": "Noise-Tolerant Quantum Tokens for MAC",
    "abstract": "Noise-Tolerant Quantum Tokens for MAC",
    "descriptor": "",
    "authors": [
      "Amit Behera",
      "Or Sattath",
      "Uriel Shinar"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.05016"
  },
  {
    "id": "arXiv:2105.10135",
    "title": "Unified Expression of Achievable Region in Privacy-Constrained Source  Coding",
    "abstract": "Comments: 13 pages, 3 figure, 4 tables, An extended version of the paper submitted to SITA2021",
    "descriptor": "\nComments: 13 pages, 3 figure, 4 tables, An extended version of the paper submitted to SITA2021\n",
    "authors": [
      "Naruki Shinohara",
      "Hideki Yagi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2105.10135"
  },
  {
    "id": "arXiv:2105.10292",
    "title": "Minimization and Synthesis of the Tail in Sequential Compositions of  Mealy Machines",
    "abstract": "Minimization and Synthesis of the Tail in Sequential Compositions of  Mealy Machines",
    "descriptor": "",
    "authors": [
      "Alberto Larrauri",
      "Roderick Bloem"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2105.10292"
  },
  {
    "id": "arXiv:2105.10832",
    "title": "Spectral Pruning for Recurrent Neural Networks",
    "abstract": "Comments: 26 pages, 2 figures",
    "descriptor": "\nComments: 26 pages, 2 figures\n",
    "authors": [
      "Takashi Furuya",
      "Kazuma Suetake",
      "Koichi Taniguchi",
      "Hiroyuki Kusumoto",
      "Ryuji Saiin",
      "Tomohiro Daimon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.10832"
  },
  {
    "id": "arXiv:2105.11084",
    "title": "Unsupervised Speech Recognition",
    "abstract": "Unsupervised Speech Recognition",
    "descriptor": "",
    "authors": [
      "Alexei Baevski",
      "Wei-Ning Hsu",
      "Alexis Conneau",
      "Michael Auli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2105.11084"
  },
  {
    "id": "arXiv:2105.13327",
    "title": "Encoders and Ensembles for Task-Free Continual Learning",
    "abstract": "Encoders and Ensembles for Task-Free Continual Learning",
    "descriptor": "",
    "authors": [
      "Murray Shanahan",
      "Christos Kaplanis",
      "Jovana Mitrovi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2105.13327"
  },
  {
    "id": "arXiv:2105.13396",
    "title": "Comparing Alternatives to the Fixed Degree Sequence Model for Extracting  the Backbone of Bipartite Projections",
    "abstract": "Comparing Alternatives to the Fixed Degree Sequence Model for Extracting  the Backbone of Bipartite Projections",
    "descriptor": "",
    "authors": [
      "Zachary P. Neal",
      "Rachel Domagalski",
      "Bruce Sagan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2105.13396"
  },
  {
    "id": "arXiv:2105.15183",
    "title": "Efficient and Modular Implicit Differentiation",
    "abstract": "Comments: V3: added more related work and Jacobian precision figure",
    "descriptor": "\nComments: V3: added more related work and Jacobian precision figure\n",
    "authors": [
      "Mathieu Blondel",
      "Quentin Berthet",
      "Marco Cuturi",
      "Roy Frostig",
      "Stephan Hoyer",
      "Felipe Llinares-L\u00f3pez",
      "Fabian Pedregosa",
      "Jean-Philippe Vert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.15183"
  },
  {
    "id": "arXiv:2106.01487",
    "title": "LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes",
    "abstract": "Comments: NeurIPS 2021 Camera Ready. 19 pages, 6 figures",
    "descriptor": "\nComments: NeurIPS 2021 Camera Ready. 19 pages, 6 figures\n",
    "authors": [
      "Aditya Kusupati",
      "Matthew Wallingford",
      "Vivek Ramanujan",
      "Raghav Somani",
      "Jae Sung Park",
      "Krishna Pillutla",
      "Prateek Jain",
      "Sham Kakade",
      "Ali Farhadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.01487"
  },
  {
    "id": "arXiv:2106.02914",
    "title": "Feature Flow Regularization: Improving Structured Sparsity in Deep  Neural Networks",
    "abstract": "Feature Flow Regularization: Improving Structured Sparsity in Deep  Neural Networks",
    "descriptor": "",
    "authors": [
      "Yue Wu",
      "Yuan Lan",
      "Luchan Zhang",
      "Yang Xiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.02914"
  },
  {
    "id": "arXiv:2106.03156",
    "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via  Random Scaling",
    "abstract": "Comments: 29 pages, 8 figures, 8 tables",
    "descriptor": "\nComments: 29 pages, 8 figures, 8 tables\n",
    "authors": [
      "Sokbae Lee",
      "Yuan Liao",
      "Myung Hwan Seo",
      "Youngki Shin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.03156"
  },
  {
    "id": "arXiv:2106.03157",
    "title": "Self-Supervision is All You Need for Solving Rubik's Cube",
    "abstract": "Comments: 8 pages, 5 figures. This update v2 restructures the paper, includes additional theoretical background, and adjusts figures",
    "descriptor": "\nComments: 8 pages, 5 figures. This update v2 restructures the paper, includes additional theoretical background, and adjusts figures\n",
    "authors": [
      "Kyo Takano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03157"
  },
  {
    "id": "arXiv:2106.03790",
    "title": "Multi-armed Bandit Requiring Monotone Arm Sequences",
    "abstract": "Multi-armed Bandit Requiring Monotone Arm Sequences",
    "descriptor": "",
    "authors": [
      "Ningyuan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.03790"
  },
  {
    "id": "arXiv:2106.04362",
    "title": "DIPS-Plus: The Enhanced Database of Interacting Protein Structures for  Interface Prediction",
    "abstract": "Comments: 19 pages, 1 figure, and 4 tables. Updated URLs",
    "descriptor": "\nComments: 19 pages, 1 figure, and 4 tables. Updated URLs\n",
    "authors": [
      "Alex Morehead",
      "Chen Chen",
      "Ada Sedova",
      "Jianlin Cheng"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2106.04362"
  },
  {
    "id": "arXiv:2106.06127",
    "title": "Differentially Private Federated Learning via Inexact ADMM",
    "abstract": "Comments: 20 pages, 6 figures",
    "descriptor": "\nComments: 20 pages, 6 figures\n",
    "authors": [
      "Minseok Ryu",
      "Kibaek Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06127"
  },
  {
    "id": "arXiv:2106.06210",
    "title": "Learning to Pool in Graph Neural Networks for Extrapolation",
    "abstract": "Learning to Pool in Graph Neural Networks for Extrapolation",
    "descriptor": "",
    "authors": [
      "Jihoon Ko",
      "Taehyung Kwon",
      "Kijung Shin",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.06210"
  },
  {
    "id": "arXiv:2106.08365",
    "title": "Predicting Unreliable Predictions by Shattering a Neural Network",
    "abstract": "Comments: Updated version (Oct 2021)",
    "descriptor": "\nComments: Updated version (Oct 2021)\n",
    "authors": [
      "Xu Ji",
      "Razvan Pascanu",
      "Devon Hjelm",
      "Andrea Vedaldi",
      "Balaji Lakshminarayanan",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08365"
  },
  {
    "id": "arXiv:2106.08417",
    "title": "Scene Transformer: A unified architecture for predicting multiple agent  trajectories",
    "abstract": "Scene Transformer: A unified architecture for predicting multiple agent  trajectories",
    "descriptor": "",
    "authors": [
      "Jiquan Ngiam",
      "Benjamin Caine",
      "Vijay Vasudevan",
      "Zhengdong Zhang",
      "Hao-Tien Lewis Chiang",
      "Jeffrey Ling",
      "Rebecca Roelofs",
      "Alex Bewley",
      "Chenxi Liu",
      "Ashish Venugopal",
      "David Weiss",
      "Ben Sapp",
      "Zhifeng Chen",
      "Jonathon Shlens"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.08417"
  },
  {
    "id": "arXiv:2106.08928",
    "title": "Recursive Construction of Stable Assemblies of Recurrent Neural Networks",
    "abstract": "Comments: 23 pages, 3 figures",
    "descriptor": "\nComments: 23 pages, 3 figures\n",
    "authors": [
      "Leo Kozachkov",
      "Michaela Ennis",
      "Jean-Jacques Slotine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.08928"
  },
  {
    "id": "arXiv:2106.09847",
    "title": "Disinformation, Stochastic Harm, and Costly Effort: A Principal-Agent  Analysis of Regulating Social Media Platforms",
    "abstract": "Disinformation, Stochastic Harm, and Costly Effort: A Principal-Agent  Analysis of Regulating Social Media Platforms",
    "descriptor": "",
    "authors": [
      "Shehroze Khan",
      "James R. Wright"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2106.09847"
  },
  {
    "id": "arXiv:2106.11257",
    "title": "Secure Distributed Training at Scale",
    "abstract": "Comments: 62 pages, 8 figures. Code: this https URL v2 has slightly more general assumptions, contains additional clarifications on them, extra experiments, improved discussion of the algorithms and the related work, and corrected typos",
    "descriptor": "\nComments: 62 pages, 8 figures. Code: this https URL v2 has slightly more general assumptions, contains additional clarifications on them, extra experiments, improved discussion of the algorithms and the related work, and corrected typos\n",
    "authors": [
      "Eduard Gorbunov",
      "Alexander Borzunov",
      "Michael Diskin",
      "Max Ryabinin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.11257"
  },
  {
    "id": "arXiv:2106.12496",
    "title": "Threaded Code Generation with a Meta-tracing JIT Compiler",
    "abstract": "Comments: 10 pages, 6 figures, and 8 listings",
    "descriptor": "\nComments: 10 pages, 6 figures, and 8 listings\n",
    "authors": [
      "Yusuke Izawa",
      "Hidehiko Masuhara",
      "Carl Friedrich Bolz-Tereick",
      "Youyou Cong"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2106.12496"
  },
  {
    "id": "arXiv:2107.01496",
    "title": "A Data-Driven Method for Recognizing Automated Negotiation Strategies",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Ming Li",
      "Pradeep K.Murukannaiah",
      "Catholijn M.Jonker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2107.01496"
  },
  {
    "id": "arXiv:2107.01777",
    "title": "Statistical Theory for Imbalanced Binary Classification",
    "abstract": "Comments: Parts of this paper have been revised from arXiv:2004.04715v2 [math.ST]",
    "descriptor": "\nComments: Parts of this paper have been revised from arXiv:2004.04715v2 [math.ST]\n",
    "authors": [
      "Shashank Singh",
      "Justin Khim"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.01777"
  },
  {
    "id": "arXiv:2107.01983",
    "title": "Gradient Importance Learning for Incomplete Observations",
    "abstract": "Gradient Importance Learning for Incomplete Observations",
    "descriptor": "",
    "authors": [
      "Qitong Gao",
      "Dong Wang",
      "Joshua D. Amason",
      "Siyang Yuan",
      "Chenyang Tao",
      "Ricardo Henao",
      "Majda Hadziahmetovic",
      "Lawrence Carin",
      "Miroslav Pajic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.01983"
  },
  {
    "id": "arXiv:2107.02045",
    "title": "Understanding the Security of Deepfake Detection",
    "abstract": "Comments: To appear in ICDF2C 2021",
    "descriptor": "\nComments: To appear in ICDF2C 2021\n",
    "authors": [
      "Xiaoyu Cao",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.02045"
  },
  {
    "id": "arXiv:2107.05610",
    "title": "The Fundamental Theorem of Natural Selection",
    "abstract": "Comments: 6 pages",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "John C. Baez"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2107.05610"
  },
  {
    "id": "arXiv:2107.07651",
    "title": "Align before Fuse: Vision and Language Representation Learning with  Momentum Distillation",
    "abstract": "Align before Fuse: Vision and Language Representation Learning with  Momentum Distillation",
    "descriptor": "",
    "authors": [
      "Junnan Li",
      "Ramprasaath R. Selvaraju",
      "Akhilesh Deepak Gotmare",
      "Shafiq Joty",
      "Caiming Xiong",
      "Steven Hoi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.07651"
  },
  {
    "id": "arXiv:2107.08249",
    "title": "Gait-learning with morphologically evolving robots generated by L-system",
    "abstract": "Comments: 9 pages, 11 figures, IEEE SSCI conference",
    "descriptor": "\nComments: 9 pages, 11 figures, IEEE SSCI conference\n",
    "authors": [
      "Jie Luo",
      "Daan Zeeuwe",
      "Agoston E. Eiben"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.08249"
  },
  {
    "id": "arXiv:2107.10483",
    "title": "Efficient Neural Causal Discovery without Acyclicity Constraints",
    "abstract": "Comments: 8th Causal Inference Workshop at UAI 2021 (contributed talk). 34 pages, 12 figures",
    "descriptor": "\nComments: 8th Causal Inference Workshop at UAI 2021 (contributed talk). 34 pages, 12 figures\n",
    "authors": [
      "Phillip Lippe",
      "Taco Cohen",
      "Efstratios Gavves"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.10483"
  },
  {
    "id": "arXiv:2107.12168",
    "title": "Exploiting Language Model for Efficient Linguistic Steganalysis",
    "abstract": "Comments: this https URL&hl=en",
    "descriptor": "\nComments: this https URL&hl=en\n",
    "authors": [
      "Biao Yi",
      "Hanzhou Wu",
      "Guorui Feng",
      "Xinpeng Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2107.12168"
  },
  {
    "id": "arXiv:2107.12850",
    "title": "Guidelines on Minimum Standards for Developer Verification of Software",
    "abstract": "Comments: 33 pages, 2 figures",
    "descriptor": "\nComments: 33 pages, 2 figures\n",
    "authors": [
      "Paul E. Black",
      "Barbara Guttman",
      "Vadim Okun"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2107.12850"
  },
  {
    "id": "arXiv:2107.13268",
    "title": "A Distributed Intelligence Architecture for B5G Network Automation",
    "abstract": "Comments: 6 pages, 4 figures. This work has been submitted to the IEEE Networking Letters for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: 6 pages, 4 figures. This work has been submitted to the IEEE Networking Letters for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Sayantini Majumdar",
      "Riccardo Trivisonno",
      "Georg Carle"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.13268"
  },
  {
    "id": "arXiv:2107.13349",
    "title": "Self-Supervised Inference in State-Space Models",
    "abstract": "Self-Supervised Inference in State-Space Models",
    "descriptor": "",
    "authors": [
      "David Ruhe",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.13349"
  },
  {
    "id": "arXiv:2108.00439",
    "title": "Transformer-based Map Matching Model with Limited Ground-Truth Data  using Transfer-Learning Approach",
    "abstract": "Comments: 25 pages, 9 figures, 4 tables",
    "descriptor": "\nComments: 25 pages, 9 figures, 4 tables\n",
    "authors": [
      "Zhixiong Jin",
      "Jiwon Kim",
      "Hwasoo Yeo",
      "Seongjin Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.00439"
  },
  {
    "id": "arXiv:2108.02312",
    "title": "Gohberg-Kaashoek Numbers and Stability of the Schur Canonical Form",
    "abstract": "Comments: 23 pages",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Anastasiia Minenkova",
      "Evelyn Nitch-Griffin",
      "Vadim Olshevsky"
    ],
    "subjectives": [
      "Classical Analysis and ODEs (math.CA)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.02312"
  },
  {
    "id": "arXiv:2108.03533",
    "title": "Improving Similar Language Translation With Transfer Learning",
    "abstract": "Comments: Submitted to WMT 2021 Similar Language Task",
    "descriptor": "\nComments: Submitted to WMT 2021 Similar Language Task\n",
    "authors": [
      "Ife Adebara",
      "Muhammad Abdul-Mageed"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.03533"
  },
  {
    "id": "arXiv:2108.04766",
    "title": "Falling for Phishing: An Empirical Investigation into People's Email  Response Behaviors",
    "abstract": "Comments: The 42nd International Conference on Information Systems (ICIS'21), Austin, Texas, USA, 2021, 17",
    "descriptor": "\nComments: The 42nd International Conference on Information Systems (ICIS'21), Austin, Texas, USA, 2021, 17\n",
    "authors": [
      "Asangi Jayatilaka",
      "Nalin Asanka Gamagedara Arachchilage",
      "Muhammad Ali Babar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2108.04766"
  },
  {
    "id": "arXiv:2108.05198",
    "title": "Natural Language-Guided Programming",
    "abstract": "Natural Language-Guided Programming",
    "descriptor": "",
    "authors": [
      "Geert Heyman",
      "Rafael Huysegems",
      "Pascal Justen",
      "Tom Van Cutsem"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2108.05198"
  },
  {
    "id": "arXiv:2108.05971",
    "title": "Ergonomically Intelligent Physical Human-Robot Interaction: Postural  Estimation, Assessment, and Optimization",
    "abstract": "Comments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)",
    "descriptor": "\nComments: Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836)\n",
    "authors": [
      "Amir Yazdani",
      "Roya Sabbagh Novin",
      "Andrew Merryweather",
      "Tucker Hermans"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.05971"
  },
  {
    "id": "arXiv:2108.06455",
    "title": "PTT: Point-Track-Transformer Module for 3D Single Object Tracking in  Point Clouds",
    "abstract": "Comments: final version, which is accepted by IROS 2021",
    "descriptor": "\nComments: final version, which is accepted by IROS 2021\n",
    "authors": [
      "Jiayao Shan",
      "Sifan Zhou",
      "Zheng Fang",
      "Yubo Cui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.06455"
  },
  {
    "id": "arXiv:2108.06797",
    "title": "Deep Adversarially-Enhanced k-Nearest Neighbors",
    "abstract": "Deep Adversarially-Enhanced k-Nearest Neighbors",
    "descriptor": "",
    "authors": [
      "Ren Wang",
      "Tianqi Chen",
      "Alfred Hero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.06797"
  },
  {
    "id": "arXiv:2108.07362",
    "title": "A Game-Theoretic Approach to Self-Stabilization with Selfish Agents",
    "abstract": "A Game-Theoretic Approach to Self-Stabilization with Selfish Agents",
    "descriptor": "",
    "authors": [
      "Amir Reza Ramtin",
      "Don Towsley"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2108.07362"
  },
  {
    "id": "arXiv:2108.08350",
    "title": "Data-driven Modeling for Distribution Grids Under Partial Observability",
    "abstract": "Data-driven Modeling for Distribution Grids Under Partial Observability",
    "descriptor": "",
    "authors": [
      "Shanny Lin",
      "Hao Zhu"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.08350"
  },
  {
    "id": "arXiv:2108.11018",
    "title": "A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your  Pre-training Effective?",
    "abstract": "A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your  Pre-training Effective?",
    "descriptor": "",
    "authors": [
      "Hiroaki Mikami",
      "Kenji Fukumizu",
      "Shogo Murai",
      "Shuji Suzuki",
      "Yuta Kikuchi",
      "Taiji Suzuki",
      "Shin-ichi Maeda",
      "Kohei Hayashi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.11018"
  },
  {
    "id": "arXiv:2108.12628",
    "title": "Planar Straight-line Realizations of 2-Trees with Prescribed Edge  Lengths",
    "abstract": "Comments: Appears in the Proceedings of the 29th International Symposium on Graph Drawing and Network Visualization (GD 2021)",
    "descriptor": "\nComments: Appears in the Proceedings of the 29th International Symposium on Graph Drawing and Network Visualization (GD 2021)\n",
    "authors": [
      "Carlos Alegr\u00eda",
      "Manuel Borrazzo",
      "Giordano Da Lozzo",
      "Giuseppe Di Battista",
      "Fabrizio Frati",
      "Maurizio Patrignani"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2108.12628"
  },
  {
    "id": "arXiv:2109.00116",
    "title": "Universality, criticality and complexity of information propagation in  social media",
    "abstract": "Comments: 10 pages, 5 figures, 7 pages of bibliography, 28 pages of supplemental material",
    "descriptor": "\nComments: 10 pages, 5 figures, 7 pages of bibliography, 28 pages of supplemental material\n",
    "authors": [
      "Daniele Notarmuzi",
      "Claudio Castellano",
      "Alessandro Flammini",
      "Dario Mazzilli",
      "Filippo Radicchi"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2109.00116"
  },
  {
    "id": "arXiv:2109.00162",
    "title": "Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces",
    "abstract": "Comments: Version 2, 6 pages",
    "descriptor": "\nComments: Version 2, 6 pages\n",
    "authors": [
      "Hui Guo",
      "Shu Hu",
      "Xin Wang",
      "Ming-Ching Chang",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.00162"
  },
  {
    "id": "arXiv:2109.00698",
    "title": "An Empirical Exploration in Quality Filtering of Text Data",
    "abstract": "Comments: corrected typo in citation",
    "descriptor": "\nComments: corrected typo in citation\n",
    "authors": [
      "Leo Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2109.00698"
  },
  {
    "id": "arXiv:2109.02791",
    "title": "Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic  through Gaussian Processes and Control Barrier Functions",
    "abstract": "Comments: Under Review. arXiv admin note: text overlap with arXiv:2102.12855",
    "descriptor": "\nComments: Under Review. arXiv admin note: text overlap with arXiv:2102.12855\n",
    "authors": [
      "Mingyu Cai",
      "Cristian-Ioan Vasile"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.02791"
  },
  {
    "id": "arXiv:2109.02929",
    "title": "Brand Label Albedo Extraction of eCommerce Products using Generative  Adversarial Network",
    "abstract": "Comments: 5 pages, 5 figures",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Suman Sapkota",
      "Manish Juneja",
      "Laurynas Keleras",
      "Pranav Kotwal",
      "Binod Bhattarai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.02929"
  },
  {
    "id": "arXiv:2109.03041",
    "title": "Only Six Passive Circuit Elements Are Existent",
    "abstract": "Comments: 10 pages, 12 figures",
    "descriptor": "\nComments: 10 pages, 12 figures\n",
    "authors": [
      "Frank Z. Wang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2109.03041"
  },
  {
    "id": "arXiv:2109.05490",
    "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via  Hybrid Action Representation",
    "abstract": "Comments: Preprint. Work in progress",
    "descriptor": "\nComments: Preprint. Work in progress\n",
    "authors": [
      "Boyan Li",
      "Hongyao Tang",
      "Yan Zheng",
      "Jianye Hao",
      "Pengyi Li",
      "Zhen Wang",
      "Zhaopeng Meng",
      "Li Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.05490"
  },
  {
    "id": "arXiv:2109.06697",
    "title": "Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions",
    "abstract": "Comments: Accepted to the 5th Annual Conference on Robot Learning (CoRL 21)",
    "descriptor": "\nComments: Accepted to the 5th Annual Conference on Robot Learning (CoRL 21)\n",
    "authors": [
      "Charles Dawson",
      "Zengyi Qin",
      "Sicun Gao",
      "Chuchu Fan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2109.06697"
  },
  {
    "id": "arXiv:2109.09948",
    "title": "Neural networks with trainable matrix activation functions",
    "abstract": "Neural networks with trainable matrix activation functions",
    "descriptor": "",
    "authors": [
      "Yuwen Li",
      "Zhengqi Liu",
      "Ludmil Zikatanov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.09948"
  },
  {
    "id": "arXiv:2109.10203",
    "title": "Generalized minimum 0-extension problem and discrete convexity",
    "abstract": "Generalized minimum 0-extension problem and discrete convexity",
    "descriptor": "",
    "authors": [
      "Martin Dvorak",
      "Vladimir Kolmogorov"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Metric Geometry (math.MG)"
    ],
    "url": "https://arxiv.org/abs/2109.10203"
  },
  {
    "id": "arXiv:2109.12380",
    "title": "A Compositional Feature Embedding and Similarity Metric for  Ultra-Fine-Grained Visual Categorization",
    "abstract": "Comments: Accepted by Digital Image Computing Techniques and Applications (DICTA) 2021",
    "descriptor": "\nComments: Accepted by Digital Image Computing Techniques and Applications (DICTA) 2021\n",
    "authors": [
      "Yajie Sun",
      "Miaohua Zhang",
      "Xiaohan Yu",
      "Yi Liao",
      "Yongsheng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.12380"
  },
  {
    "id": "arXiv:2109.13346",
    "title": "Computational phase transition in Quantum Approximate Optimization  Algorithm -- the difference between hard and easy",
    "abstract": "Comments: 10 pages, 9 figures",
    "descriptor": "\nComments: 10 pages, 9 figures\n",
    "authors": [
      "Bingzhi Zhang",
      "Akira Sone",
      "Quntao Zhuang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2109.13346"
  },
  {
    "id": "arXiv:2109.13863",
    "title": "A First-Occupancy Representation for Reinforcement Learning",
    "abstract": "A First-Occupancy Representation for Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Ted Moskovitz",
      "Spencer R. Wilson",
      "Maneesh Sahani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.13863"
  },
  {
    "id": "arXiv:2109.14501",
    "title": "Towards a theory of out-of-distribution learning",
    "abstract": "Towards a theory of out-of-distribution learning",
    "descriptor": "",
    "authors": [
      "Ali Geisa",
      "Ronak Mehta",
      "Hayden S. Helm",
      "Jayanta Dey",
      "Eric Eaton",
      "Jeffery Dick",
      "Carey E. Priebe",
      "Joshua T. Vogelstein"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.14501"
  },
  {
    "id": "arXiv:2109.14516",
    "title": "On Assessing the Usefulness of Proxy Domains for Developing and  Evaluating Embodied Agents",
    "abstract": "Comments: 8 pages, 6 figures Accepted & Presented at IROS2021 For associated code, see this https URL",
    "descriptor": "\nComments: 8 pages, 6 figures Accepted & Presented at IROS2021 For associated code, see this https URL\n",
    "authors": [
      "Anthony Courchesne",
      "Andrea Censi",
      "Liam Paull"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.14516"
  },
  {
    "id": "arXiv:2109.14530",
    "title": "Deep Spatio-Temporal Wind Power Forecasting",
    "abstract": "Deep Spatio-Temporal Wind Power Forecasting",
    "descriptor": "",
    "authors": [
      "Jiangyuan Li",
      "Mohammadreza Armandpour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2109.14530"
  },
  {
    "id": "arXiv:2109.14920",
    "title": "On the Kullback-Leibler divergence between discrete normal distributions",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Frank Nielsen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2109.14920"
  },
  {
    "id": "arXiv:2110.00329",
    "title": "Student Helping Teacher: Teacher Evolution via Self-Knowledge  Distillation",
    "abstract": "Student Helping Teacher: Teacher Evolution via Self-Knowledge  Distillation",
    "descriptor": "",
    "authors": [
      "Zheng Li",
      "Xiang Li",
      "Lingfeng Yang",
      "Jian Yang",
      "Zhigeng Pan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.00329"
  },
  {
    "id": "arXiv:2110.00981",
    "title": "SecFL: Confidential Federated Learning using TEEs",
    "abstract": "SecFL: Confidential Federated Learning using TEEs",
    "descriptor": "",
    "authors": [
      "Do Le Quoc",
      "Christof Fetzer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.00981"
  },
  {
    "id": "arXiv:2110.00984",
    "title": "Adaptive Unfolding Total Variation Network for Low-Light Image  Enhancement",
    "abstract": "Comments: Accepted by ICCV 2021",
    "descriptor": "\nComments: Accepted by ICCV 2021\n",
    "authors": [
      "Chuanjun Zheng",
      "Daming Shi",
      "Wentian Shi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.00984"
  },
  {
    "id": "arXiv:2110.01210",
    "title": "Audio Captioning Using Sound Event Detection",
    "abstract": "Comments: Submitted to DCASE 2021 Challenge",
    "descriptor": "\nComments: Submitted to DCASE 2021 Challenge\n",
    "authors": [
      "Ay\u015feg\u00fcl \u00d6zkaya Eren",
      "Mustafa Sert"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.01210"
  },
  {
    "id": "arXiv:2110.01467",
    "title": "HyperTeNet: Hypergraph and Transformer-based Neural Network for  Personalized List Continuation",
    "abstract": "Comments: 11 pages, 5 figures, The IEEE International Conference on Data Mining (ICDM) 2021",
    "descriptor": "\nComments: 11 pages, 5 figures, The IEEE International Conference on Data Mining (ICDM) 2021\n",
    "authors": [
      "Vijaikumar M",
      "Deepesh Hada",
      "Shirish Shevade"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2110.01467"
  },
  {
    "id": "arXiv:2110.01476",
    "title": "Learning Online Visual Invariances for Novel Objects via Supervised and  Self-Supervised Training",
    "abstract": "Learning Online Visual Invariances for Novel Objects via Supervised and  Self-Supervised Training",
    "descriptor": "",
    "authors": [
      "Valerio Biscione",
      "Jeffrey S. Bowers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.01476"
  },
  {
    "id": "arXiv:2110.01500",
    "title": "Factorized Neural Transducer for Efficient Language Model Adaptation",
    "abstract": "Factorized Neural Transducer for Efficient Language Model Adaptation",
    "descriptor": "",
    "authors": [
      "Xie Chen",
      "Zhong Meng",
      "Sarangarajan Parthasarathy",
      "Jinyu Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2110.01500"
  },
  {
    "id": "arXiv:2110.01661",
    "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and  Enhancement Prediction",
    "abstract": "Comments: Journal of Data Mining and Digital Humanities; Small title adjustment",
    "descriptor": "\nComments: Journal of Data Mining and Digital Humanities; Small title adjustment\n",
    "authors": [
      "Pit Schneider"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01661"
  },
  {
    "id": "arXiv:2110.01757",
    "title": "Detecting Timing Attack on PMU Data utilizing Unwrapped Phase Angle and  Low-Rank Henkel Matrix Properties",
    "abstract": "Comments: 7 pages",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Imtiaj Khan",
      "Virgilio Centeno"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.01757"
  },
  {
    "id": "arXiv:2110.01758",
    "title": "Quantified Facial Expressiveness for Affective Behavior Analytics",
    "abstract": "Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2022",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 2022\n",
    "authors": [
      "Md Taufeeq Uddin",
      "Shaun Canavan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.01758"
  },
  {
    "id": "arXiv:2110.01990",
    "title": "SMProbLog: Stable Model Semantics in ProbLog and its Applications in  Argumentation",
    "abstract": "SMProbLog: Stable Model Semantics in ProbLog and its Applications in  Argumentation",
    "descriptor": "",
    "authors": [
      "Pietro Totis",
      "Angelika Kimmig",
      "Luc De Raedt"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.01990"
  },
  {
    "id": "arXiv:2110.01999",
    "title": "Federating for Learning Group Fair Models",
    "abstract": "Federating for Learning Group Fair Models",
    "descriptor": "",
    "authors": [
      "Afroditi Papadaki",
      "Natalia Martinez",
      "Martin Bertran",
      "Guillermo Sapiro",
      "Miguel Rodrigues"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2110.01999"
  },
  {
    "id": "arXiv:2110.02058",
    "title": "Interactively Generating Explanations for Transformer Language Models",
    "abstract": "Interactively Generating Explanations for Transformer Language Models",
    "descriptor": "",
    "authors": [
      "Patrick Schramowski",
      "Felix Friedrich",
      "Christopher Tauchmann",
      "Kristian Kersting"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02058"
  },
  {
    "id": "arXiv:2110.02069",
    "title": "OPAD: An Optimized Policy-based Active Learning Framework for Document  Content Analysis",
    "abstract": "OPAD: An Optimized Policy-based Active Learning Framework for Document  Content Analysis",
    "descriptor": "",
    "authors": [
      "Sumit Shekhar",
      "Bhanu Prakash Reddy Guda",
      "Ashutosh Chaubey",
      "Ishan Jindal",
      "Avneet Jain"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.02069"
  },
  {
    "id": "arXiv:2110.02192",
    "title": "Reducing Gaze Distraction for Real-time Vibration Monitoring Using  Augmented Reality",
    "abstract": "Comments: 23 pages, 21 figures, 2 tables",
    "descriptor": "\nComments: 23 pages, 21 figures, 2 tables\n",
    "authors": [
      "Elijah Wyckoff",
      "Marlan Ball",
      "Fernando Moreu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.02192"
  },
  {
    "id": "arXiv:2110.02220",
    "title": "Fast Contextual Adaptation with Neural Associative Memory for On-Device  Personalized Speech Recognition",
    "abstract": "Comments: 5 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 5 pages, 3 figures, 3 tables\n",
    "authors": [
      "Tsendsuren Munkhdalai",
      "Khe Chai Sim",
      "Angad Chandorkar",
      "Fan Gao",
      "Mason Chua",
      "Trevor Strohman",
      "Fran\u00e7oise Beaufays"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2110.02220"
  },
  {
    "id": "arXiv:2110.02281",
    "title": "A Rate Splitting Strategy for Uplink CR-NOMA Systems",
    "abstract": "Comments: 13 pages, 5 figures, submitted to IEEE Journal",
    "descriptor": "\nComments: 13 pages, 5 figures, submitted to IEEE Journal\n",
    "authors": [
      "Hongwu Liu",
      "Zhiquan Bai",
      "Hongjiang Lei",
      "Gaofeng Pan",
      "Kyeong Jin Kim",
      "Theodoros A. Tsiftsis"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.02281"
  },
  {
    "id": "arXiv:2110.02442",
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "abstract": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "descriptor": "",
    "authors": [
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Zhen-Hua Ling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02442"
  },
  {
    "id": "arXiv:2110.02483",
    "title": "Detecting and Quantifying Malicious Activity with Simulation-based  Inference",
    "abstract": "Comments: Short version, appeared at ICML workshop on Socially Responsible Machine Learning 2021",
    "descriptor": "\nComments: Short version, appeared at ICML workshop on Socially Responsible Machine Learning 2021\n",
    "authors": [
      "Andrew Gambardella",
      "Bogdan State",
      "Naeemullah Khan",
      "Leo Tsourides",
      "Philip H. S. Torr",
      "At\u0131l\u0131m G\u00fcne\u015f Baydin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.02483"
  },
  {
    "id": "arXiv:2110.02491",
    "title": "Data-Centric AI Requires Rethinking Data Notion",
    "abstract": "Data-Centric AI Requires Rethinking Data Notion",
    "descriptor": "",
    "authors": [
      "Mustafa Hajij",
      "Ghada Zamzmi",
      "Karthikeyan Natesan Ramamurthy",
      "Aldo Guzman Saenz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Category Theory (math.CT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.02491"
  },
  {
    "id": "arXiv:2110.02597",
    "title": "Cookie Banners, What's the Purpose? Analyzing Cookie Banner Text Through  a Legal Lens",
    "abstract": "Cookie Banners, What's the Purpose? Analyzing Cookie Banner Text Through  a Legal Lens",
    "descriptor": "",
    "authors": [
      "Cristiana Santos",
      "Arianna Rossi",
      "Lorena S\u00e1nchez Chamorro",
      "Kerstin Bongard-Blanchy",
      "Ruba Abu-Salma"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2110.02597"
  },
  {
    "id": "arXiv:2110.02638",
    "title": "2nd Place Solution to Google Landmark Recognition Competition 2021",
    "abstract": "2nd Place Solution to Google Landmark Recognition Competition 2021",
    "descriptor": "",
    "authors": [
      "Shubin Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.02638"
  },
  {
    "id": "arXiv:2110.02732",
    "title": "On Margin Maximization in Linear and ReLU Networks",
    "abstract": "On Margin Maximization in Linear and ReLU Networks",
    "descriptor": "",
    "authors": [
      "Gal Vardi",
      "Ohad Shamir",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.02732"
  },
  {
    "id": "arXiv:2110.02768",
    "title": "Posture Recognition in the Critical Care Settings using Wearable Devices",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Anis Davoudi",
      "Patrick J. Tighe",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2110.02768"
  },
  {
    "id": "arXiv:2110.02794",
    "title": "3rd Place Solution to Google Landmark Recognition Competition 2021",
    "abstract": "Comments: Corrected typos",
    "descriptor": "\nComments: Corrected typos\n",
    "authors": [
      "Cheng Xu",
      "Weimin Wang",
      "Shuai Liu",
      "Yong Wang",
      "Yuxiang Tang",
      "Tianling Bian",
      "Yanyu Yan",
      "Qi She",
      "Cheng Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.02794"
  },
  {
    "id": "arXiv:2110.02852",
    "title": "PSG@HASOC-Dravidian CodeMixFIRE2021: Pretrained Transformers for  Offensive Language Identification in Tanglish",
    "abstract": "Comments: Under review for FIRE 2021",
    "descriptor": "\nComments: Under review for FIRE 2021\n",
    "authors": [
      "Sean Benhur",
      "Kanchana Sivanraju"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.02852"
  },
  {
    "id": "arXiv:2110.02862",
    "title": "RevASIDE: Assignment of Suitable Reviewer Sets for Publications from  Fixed Candidate Pools",
    "abstract": "RevASIDE: Assignment of Suitable Reviewer Sets for Publications from  Fixed Candidate Pools",
    "descriptor": "",
    "authors": [
      "Christin Katharina Kreutz",
      "Ralf Schenkel"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2110.02862"
  },
  {
    "id": "arXiv:2110.02869",
    "title": "Sequence-to-Sequence Lexical Normalization with Multilingual  Transformers",
    "abstract": "Comments: Accepted to Proceedings of the 7th Workshop on Noisy User-generated Text (WNUT 2021), EMNLP 2021",
    "descriptor": "\nComments: Accepted to Proceedings of the 7th Workshop on Noisy User-generated Text (WNUT 2021), EMNLP 2021\n",
    "authors": [
      "Ana-Maria Bucur",
      "Adrian Cosma",
      "Liviu P. Dinu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.02869"
  },
  {
    "id": "arXiv:2110.02892",
    "title": "Probabilistic Metamodels for an Efficient Characterization of Complex  Driving Scenarios",
    "abstract": "Comments: 10 pages, 15 figures, 1 table, associated dataset at this https URL",
    "descriptor": "\nComments: 10 pages, 15 figures, 1 table, associated dataset at this https URL\n",
    "authors": [
      "Max Winkelmann",
      "Mike Kohlhoff",
      "Hadj Hamma Tadjine",
      "Steffen M\u00fcller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.02892"
  },
  {
    "id": "arXiv:2110.02912",
    "title": "Generative Optimization Networks for Memory Efficient Data Generation",
    "abstract": "Generative Optimization Networks for Memory Efficient Data Generation",
    "descriptor": "",
    "authors": [
      "Shreshth Tuli",
      "Shikhar Tuli",
      "Giuliano Casale",
      "Nicholas R. Jennings"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02912"
  },
  {
    "id": "arXiv:2110.02933",
    "title": "On Cropped versus Uncropped Training Sets in Tabular Structure Detection",
    "abstract": "On Cropped versus Uncropped Training Sets in Tabular Structure Detection",
    "descriptor": "",
    "authors": [
      "Yakup Akkaya",
      "Murat Simsek",
      "Burak Kantarci",
      "Shahzad Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.02933"
  }
]