[
  {
    "id": "arXiv:2210.14225",
    "title": "Flexible Android Malware Detection Model based on Generative Adversarial  Networks with Code Tensor",
    "abstract": "The behavior of malware threats is gradually increasing, heightened the need\nfor malware detection. However, existing malware detection methods only target\nat the existing malicious samples, the detection of fresh malicious code and\nvariants of malicious code is limited. In this paper, we propose a novel scheme\nthat detects malware and its variants efficiently. Based on the idea of the\ngenerative adversarial networks (GANs), we obtain the `true' sample\ndistribution that satisfies the characteristics of the real malware, use them\nto deceive the discriminator, thus achieve the defense against malicious code\nattacks and improve malware detection. Firstly, a new Android malware APK to\nimage texture feature extraction segmentation method is proposed, which is\ncalled segment self-growing texture segmentation algorithm. Secondly, tensor\nsingular value decomposition (tSVD) based on the low-tubal rank transforms\nmalicious features with different sizes into a fixed third-order tensor\nuniformly, which is entered into the neural network for training and learning.\nFinally, a flexible Android malware detection model based on GANs with code\ntensor (MTFD-GANs) is proposed. Experiments show that the proposed model can\ngenerally surpass the traditional malware detection model, with a maximum\nimprovement efficiency of 41.6\\%. At the same time, the newly generated samples\nof the GANs generator greatly enrich the sample diversity. And retraining\nmalware detector can effectively improve the detection efficiency and\nrobustness of traditional models.",
    "descriptor": "",
    "authors": [
      "Zhao Yang",
      "Fengyang Deng",
      "Linxi Han"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14225"
  },
  {
    "id": "arXiv:2210.14226",
    "title": "FedClassAvg: Local Representation Learning for Personalized Federated  Learning on Heterogeneous Neural Networks",
    "abstract": "Personalized federated learning is aimed at allowing numerous clients to\ntrain personalized models while participating in collaborative training in a\ncommunication-efficient manner without exchanging private data. However, many\npersonalized federated learning algorithms assume that clients have the same\nneural network architecture, and those for heterogeneous models remain\nunderstudied. In this study, we propose a novel personalized federated learning\nmethod called federated classifier averaging (FedClassAvg). Deep neural\nnetworks for supervised learning tasks consist of feature extractor and\nclassifier layers. FedClassAvg aggregates classifier weights as an agreement on\ndecision boundaries on feature spaces so that clients with not independently\nand identically distributed (non-iid) data can learn about scarce labels. In\naddition, local feature representation learning is applied to stabilize the\ndecision boundaries and improve the local feature extraction capabilities for\nclients. While the existing methods require the collection of auxiliary data or\nmodel weights to generate a counterpart, FedClassAvg only requires clients to\ncommunicate with a couple of fully connected layers, which is highly\ncommunication-efficient. Moreover, FedClassAvg does not require extra\noptimization problems such as knowledge transfer, which requires intensive\ncomputation overhead. We evaluated FedClassAvg through extensive experiments\nand demonstrated it outperforms the current state-of-the-art algorithms on\nheterogeneous personalized federated learning tasks.",
    "descriptor": "\nComments: Accepted to ICPP 2022. Code: this https URL\n",
    "authors": [
      "Jaehee Jang",
      "Heonseok Ha",
      "Dahuin Jung",
      "Sungroh Yoon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14226"
  },
  {
    "id": "arXiv:2210.14228",
    "title": "'A net for everyone': fully personalized and unsupervised neural  networks trained with longitudinal data from a single patient",
    "abstract": "With the rise in importance of personalized medicine, we trained personalized\nneural networks to detect tumor progression in longitudinal datasets. The model\nwas evaluated on two datasets with a total of 64 scans from 32 patients\ndiagnosed with glioblastoma multiforme (GBM). Contrast-enhanced T1w sequences\nof brain magnetic resonance imaging (MRI) images were used in this study. For\neach patient, we trained their own neural network using just two images from\ndifferent timepoints. Our approach uses a Wasserstein-GAN (generative\nadversarial network), an unsupervised network architecture, to map the\ndifferences between the two images. Using this map, the change in tumor volume\ncan be evaluated. Due to the combination of data augmentation and the network\narchitecture, co-registration of the two images is not needed. Furthermore, we\ndo not rely on any additional training data, (manual) annotations or\npre-training neural networks. The model received an AUC-score of 0.87 for tumor\nchange. We also introduced a modified RANO criteria, for which an accuracy of\n66% can be achieved. We show that using data from just one patient can be used\nto train deep neural networks to monitor tumor change.",
    "descriptor": "",
    "authors": [
      "Christian Strack",
      "Kelsey L. Pomykala",
      "Heinz-Peter Schlemmer",
      "Jan Egger",
      "Jens Kleesiek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14228"
  },
  {
    "id": "arXiv:2210.14229",
    "title": "Causal Information Bottleneck Boosts Adversarial Robustness of Deep  Neural Network",
    "abstract": "The information bottleneck (IB) method is a feasible defense solution against\nadversarial attacks in deep learning. However, this method suffers from the\nspurious correlation, which leads to the limitation of its further improvement\nof adversarial robustness. In this paper, we incorporate the causal inference\ninto the IB framework to alleviate such a problem. Specifically, we divide the\nfeatures obtained by the IB method into robust features (content information)\nand non-robust features (style information) via the instrumental variables to\nestimate the causal effects. With the utilization of such a framework, the\ninfluence of non-robust features could be mitigated to strengthen the\nadversarial robustness. We make an analysis of the effectiveness of our\nproposed method. The extensive experiments in MNIST, FashionMNIST, and CIFAR-10\nshow that our method exhibits the considerable robustness against multiple\nadversarial attacks. Our code would be released.",
    "descriptor": "",
    "authors": [
      "Huan Hua",
      "Jun Yan",
      "Xi Fang",
      "Weiquan Huang",
      "Huilin Yin",
      "Wancheng Ge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14229"
  },
  {
    "id": "arXiv:2210.14250",
    "title": "Exploring Document-Level Literary Machine Translation with Parallel  Paragraphs from World Literature",
    "abstract": "Literary translation is a culturally significant task, but it is bottlenecked\nby the small number of qualified literary translators relative to the many\nuntranslated works published around the world. Machine translation (MT) holds\npotential to complement the work of human translators by improving both\ntraining procedures and their overall efficiency. Literary translation is less\nconstrained than more traditional MT settings since translators must balance\nmeaning equivalence, readability, and critical interpretability in the target\nlanguage. This property, along with the complex discourse-level context present\nin literary texts, also makes literary MT more challenging to computationally\nmodel and evaluate. To explore this task, we collect a dataset (Par3) of\nnon-English language novels in the public domain, each aligned at the paragraph\nlevel to both human and automatic English translations. Using Par3, we discover\nthat expert literary translators prefer reference human translations over\nmachine-translated paragraphs at a rate of 84%, while state-of-the-art\nautomatic MT metrics do not correlate with those preferences. The experts note\nthat MT outputs contain not only mistranslations, but also discourse-disrupting\nerrors and stylistic inconsistencies. To address these problems, we train a\npost-editing model whose output is preferred over normal MT output at a rate of\n69% by experts. We publicly release Par3 at\nhttps://github.com/katherinethai/par3/ to spur future research into literary\nMT.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Katherine Thai",
      "Marzena Karpinska",
      "Kalpesh Krishna",
      "Bill Ray",
      "Moira Inghilleri",
      "John Wieting",
      "Mohit Iyyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14250"
  },
  {
    "id": "arXiv:2210.14252",
    "title": "Dynamic Speech Endpoint Detection with Regression Targets",
    "abstract": "Interactive voice assistants have been widely used as input interfaces in\nvarious scenarios, e.g. on smart homes devices, wearables and on AR devices.\nDetecting the end of a speech query, i.e. speech end-pointing, is an important\ntask for voice assistants to interact with users. Traditionally, speech\nend-pointing is based on pure classification methods along with arbitrary\nbinary targets. In this paper, we propose a novel regression-based speech\nend-pointing model, which enables an end-pointer to adjust its detection\nbehavior based on context of user queries. Specifically, we present a pause\nmodeling method and show its effectiveness for dynamic end-pointing. Based on\nour experiments with vendor-collected smartphone and wearables speech queries,\nour strategy shows a better trade-off between endpointing latency and accuracy,\ncompared to the traditional classification-based method. We further discuss the\nbenefits of this model and generalization of the framework in the paper.",
    "descriptor": "\nComments: Manuscript submitted to ICASSP 2023\n",
    "authors": [
      "Dawei Liang",
      "Hang Su",
      "Tarun Singh",
      "Jay Mahadeokar",
      "Shanil Puri",
      "Jiedan Zhu",
      "Edison Thomaz",
      "Mike Seltzer"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14252"
  },
  {
    "id": "arXiv:2210.14253",
    "title": "Classification and Self-Supervised Regression of Arrhythmic ECG Signals  Using Convolutional Neural Networks",
    "abstract": "Interpretation of electrocardiography (ECG) signals is required for\ndiagnosing cardiac arrhythmia. Recently, machine learning techniques have been\napplied for automated computer-aided diagnosis. Machine learning tasks can be\ndivided into regression and classification. Regression can be used for noise\nand artifacts removal as well as resolve issues of missing data from low\nsampling frequency. Classification task concerns the prediction of output\ndiagnostic classes according to expert-labeled input classes. In this work, we\npropose a deep neural network model capable of solving regression and\nclassification tasks. Moreover, we combined the two approaches, using unlabeled\nand labeled data, to train the model. We tested the model on the MIT-BIH\nArrhythmia database. Our method showed high effectiveness in detecting cardiac\narrhythmia based on modified Lead II ECG records, as well as achieved high\nquality of ECG signal approximation. For the former, our method attained\noverall accuracy of 87:33% and balanced accuracy of 80:54%, on par with\nreference approaches. For the latter, application of self-supervised learning\nallowed for training without the need for expert labels. The regression model\nyielded satisfactory performance with fairly accurate prediction of QRS\ncomplexes. Transferring knowledge from regression to the classification task,\nour method attained higher overall accuracy of 87:78%.",
    "descriptor": "",
    "authors": [
      "Bartosz Grabowski",
      "Przemys\u0142aw G\u0142omb",
      "Wojciech Masarczyk",
      "Pawe\u0142 P\u0142awiak",
      "\u00d6zal Y\u0131ld\u0131r\u0131m",
      "U Rajendra Acharya",
      "Ru-San Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14253"
  },
  {
    "id": "arXiv:2210.14254",
    "title": "Leveraging Open Data and Task Augmentation to Automated Behavioral  Coding of Psychotherapy Conversations in Low-Resource Scenarios",
    "abstract": "In psychotherapy interactions, the quality of a session is assessed by\ncodifying the communicative behaviors of participants during the conversation\nthrough manual observation and annotation. Developing computational approaches\nfor automated behavioral coding can reduce the burden on human coders and\nfacilitate the objective evaluation of the intervention. In the real world,\nhowever, implementing such algorithms is associated with data sparsity\nchallenges since privacy concerns lead to limited available in-domain data. In\nthis paper, we leverage a publicly available conversation-based dataset and\ntransfer knowledge to the low-resource behavioral coding task by performing an\nintermediate language model training via meta-learning. We introduce a task\naugmentation method to produce a large number of \"analogy tasks\" - tasks\nsimilar to the target one - and demonstrate that the proposed framework\npredicts target behaviors more accurately than all the other baseline models.",
    "descriptor": "\nComments: Accepted to appear at Findings of EMNLP 2022\n",
    "authors": [
      "Zhuohao Chen",
      "Nikolaos Flemotomos",
      "Zac E. Imel",
      "David C. Atkins",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14254"
  },
  {
    "id": "arXiv:2210.14257",
    "title": "Revision for Concision: A Constrained Paraphrase Generation Task",
    "abstract": "Academic writing should be concise as concise sentences better keep the\nreaders' attention and convey meaning clearly. Writing concisely is\nchallenging, for writers often struggle to revise their drafts. We introduce\nand formulate revising for concision as a natural language processing task at\nthe sentence level. Revising for concision requires algorithms to use only\nnecessary words to rewrite a sentence while preserving its meaning. The revised\nsentence should be evaluated according to its word choice, sentence structure,\nand organization. The revised sentence also needs to fulfil semantic retention\nand syntactic soundness. To aide these efforts, we curate and make available a\nbenchmark parallel dataset that can depict revising for concision. The dataset\ncontains 536 pairs of sentences before and after revising, and all pairs are\ncollected from college writing centres. We also present and evaluate the\napproaches to this problem, which may assist researchers in this area.",
    "descriptor": "",
    "authors": [
      "Wenchuan Mu",
      "Kwan Hui Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14257"
  },
  {
    "id": "arXiv:2210.14259",
    "title": "Net Separation-Oriented Printed Circuit Board Placement via Margin  Maximization",
    "abstract": "Packaging has become a crucial process due to the paradigm shift of More than\nMoore. Addressing manufacturing and yield issues is a significant challenge for\nmodern layout algorithms.\nWe propose to use printed circuit board (PCB) placement as a benchmark for\nthe packaging problem. A maximum-margin formulation is devised to improve the\nseparation between nets. Our framework includes seed layout proposals, a\ncoordinate descent-based procedure to optimize routability, and a mixed-integer\nlinear programming method to legalize the layout. We perform an extensive study\nwith 14 PCB designs and an open-source router. We show that the placements\nproduced by NS-place improve routed wirelength by up to 25\\%, reduce the number\nof vias by up to 50\\%, and reduce the number of DRVs by 79\\% compared to manual\nand wirelength-minimal placements.",
    "descriptor": "",
    "authors": [
      "Chung-Kuan Cheng",
      "Chia-Tung Ho",
      "Chester Holtz"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.14259"
  },
  {
    "id": "arXiv:2210.14260",
    "title": "Universal Evasion Attacks on Summarization Scoring",
    "abstract": "The automatic scoring of summaries is important as it guides the development\nof summarizers. Scoring is also complex, as it involves multiple aspects such\nas fluency, grammar, and even textual entailment with the source text. However,\nsummary scoring has not been considered a machine learning task to study its\naccuracy and robustness. In this study, we place automatic scoring in the\ncontext of regression machine learning tasks and perform evasion attacks to\nexplore its robustness. Attack systems predict a non-summary string from each\ninput, and these non-summary strings achieve competitive scores with good\nsummarizers on the most popular metrics: ROUGE, METEOR, and BERTScore. Attack\nsystems also \"outperform\" state-of-the-art summarization methods on ROUGE-1 and\nROUGE-L, and score the second-highest on METEOR. Furthermore, a BERTScore\nbackdoor is observed: a simple trigger can score higher than any automatic\nsummarization method. The evasion attacks in this work indicate the low\nrobustness of current scoring systems at the system level. We hope that our\nhighlighting of these proposed attacks will facilitate the development of\nsummary scores.",
    "descriptor": "",
    "authors": [
      "Wenchuan Mu",
      "Kwan Hui Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14260"
  },
  {
    "id": "arXiv:2210.14267",
    "title": "A Survey on 3D-aware Image Synthesis",
    "abstract": "Recent years have seen remarkable progress in deep learning powered visual\ncontent creation. This includes 3D-aware generative image synthesis, which\nproduces high-fidelity images in a 3D-consistent manner while simultaneously\ncapturing compact surfaces of objects from pure image collections without the\nneed for any 3D supervision, thus bridging the gap between 2D imagery and 3D\nreality. The 3D-aware generative models have shown that the introduction of 3D\ninformation can lead to more controllable image generation. The task of\n3D-aware image synthesis has taken the field of computer vision by storm, with\nhundreds of papers accepted to top-tier journals and conferences in recent year\n(mainly the past two years), but there lacks a comprehensive survey of this\nremarkable and swift progress. Our survey aims to introduce new researchers to\nthis topic, provide a useful reference for related works, and stimulate future\nresearch directions through our discussion section. Apart from the presented\npapers, we aim to constantly update the latest relevant papers along with\ncorresponding implementations at\nhttps://weihaox.github.io/projects/awesome-3d-aware.",
    "descriptor": "\nComments: Project: this https URL\n",
    "authors": [
      "Weihao Xia",
      "Jing-Hao Xue"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.14267"
  },
  {
    "id": "arXiv:2210.14270",
    "title": "Guiding Users to Where to Give Color Hints for Efficient Interactive  Sketch Colorization via Unsupervised Region Prioritization",
    "abstract": "Existing deep interactive colorization models have focused on ways to utilize\nvarious types of interactions, such as point-wise color hints, scribbles, or\nnatural-language texts, as methods to reflect a user's intent at runtime.\nHowever, another approach, which actively informs the user of the most\neffective regions to give hints for sketch image colorization, has been\nunder-explored. This paper proposes a novel model-guided deep interactive\ncolorization framework that reduces the required amount of user interactions,\nby prioritizing the regions in a colorization model. Our method, called\nGuidingPainter, prioritizes these regions where the model most needs a color\nhint, rather than just relying on the user's manual decision on where to give a\ncolor hint. In our extensive experiments, we show that our approach outperforms\nexisting interactive colorization methods in terms of the conventional metrics,\nsuch as PSNR and FID, and reduces required amount of interactions.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Youngin Cho",
      "Junsoo Lee",
      "Soyoung Yang",
      "Juntae Kim",
      "Yeojeong Park",
      "Haneol Lee",
      "Mohammad Azam Khan",
      "Daesik Kim",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14270"
  },
  {
    "id": "arXiv:2210.14271",
    "title": "Learning to Augment via Implicit Differentiation for Domain  Generalization",
    "abstract": "Machine learning models are intrinsically vulnerable to domain shift between\ntraining and testing data, resulting in poor performance in novel domains.\nDomain generalization (DG) aims to overcome the problem by leveraging multiple\nsource domains to learn a domain-generalizable model. In this paper, we propose\na novel augmentation-based DG approach, dubbed AugLearn. Different from\nexisting data augmentation methods, our AugLearn views a data augmentation\nmodule as hyper-parameters of a classification model and optimizes the module\ntogether with the model via meta-learning. Specifically, at each training step,\nAugLearn (i) divides source domains into a pseudo source and a pseudo target\nset, and (ii) trains the augmentation module in such a way that the augmented\n(synthetic) images can make the model generalize well on the pseudo target set.\nMoreover, to overcome the expensive second-order gradient computation during\nmeta-learning, we formulate an efficient joint training algorithm, for both the\naugmentation module and the classification model, based on the implicit\nfunction theorem. With the flexibility of augmenting data in both time and\nfrequency spaces, AugLearn shows effectiveness on three standard DG benchmarks,\nPACS, Office-Home and Digits-DG.",
    "descriptor": "\nComments: BMVC 2022 (Oral)\n",
    "authors": [
      "Tingwei Wang",
      "Da Li",
      "Kaiyang Zhou",
      "Tao Xiang",
      "Yi-Zhe Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14271"
  },
  {
    "id": "arXiv:2210.14275",
    "title": "Similarity between Units of Natural Language: The Transition from Coarse  to Fine Estimation",
    "abstract": "Capturing the similarities between human language units is crucial for\nexplaining how humans associate different objects, and therefore its\ncomputation has received extensive attention, research, and applications. With\nthe ever-increasing amount of information around us, calculating similarity\nbecomes increasingly complex, especially in many cases, such as legal or\nmedical affairs, measuring similarity requires extra care and precision, as\nsmall acts within a language unit can have significant real-world effects. My\nresearch goal in this thesis is to develop regression models that account for\nsimilarities between language units in a more refined way.\nComputation of similarity has come a long way, but approaches to debugging\nthe measures are often based on continually fitting human judgment values. To\nthis end, my goal is to develop an algorithm that precisely catches loopholes\nin a similarity calculation. Furthermore, most methods have vague definitions\nof the similarities they compute and are often difficult to interpret. The\nproposed framework addresses both shortcomings. It constantly improves the\nmodel through catching different loopholes. In addition, every refinement of\nthe model provides a reasonable explanation. The regression model introduced in\nthis thesis is called progressively refined similarity computation, which\ncombines attack testing with adversarial training. The similarity regression\nmodel of this thesis achieves state-of-the-art performance in handling edge\ncases.",
    "descriptor": "\nComments: PhD thesis\n",
    "authors": [
      "Wenchuan Mu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14275"
  },
  {
    "id": "arXiv:2210.14277",
    "title": "A distributed blossom algorithm for minimum-weight perfect matching",
    "abstract": "We describe a distributed, asynchronous variant of Edmonds's exact algorithm\nfor producing perfect matchings of minimum weight. The development of this\nalgorithm is driven by an application to online error correction in quantum\ncomputing, first envisioned by Fowler; we analyze the performance of our\nalgorithm as applied to this domain in a sequel.",
    "descriptor": "",
    "authors": [
      "Eric C. Peterson",
      "Peter J. Karalekas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14277"
  },
  {
    "id": "arXiv:2210.14280",
    "title": "Learning in Multi-Player Stochastic Games",
    "abstract": "We consider the problem of simultaneous learning in stochastic games with\nmany players in the finite-horizon setting. While the typical target solution\nfor a stochastic game is a Nash equilibrium, this is intractable with many\nplayers. We instead focus on variants of {\\it correlated equilibria}, such as\nthose studied for extensive-form games. We begin with a hardness result for the\nadversarial MDP problem: even for a horizon of 3, obtaining sublinear regret\nagainst the best non-stationary policy is \\textsf{NP}-hard when both rewards\nand transitions are adversarial. This implies that convergence to even the\nweakest natural solution concept -- normal-form coarse correlated equilbrium --\nis not possible via black-box reduction to a no-regret algorithm even in\nstochastic games with constant horizon (unless\n$\\textsf{NP}\\subseteq\\textsf{BPP}$). Instead, we turn to a different target:\nalgorithms which {\\it generate} an equilibrium when they are used by all\nplayers. Our main result is algorithm which generates an {\\it extensive-form}\ncorrelated equilibrium, whose runtime is exponential in the horizon but\npolynomial in all other parameters. We give a similar algorithm which is\npolynomial in all parameters for \"fast-mixing\" stochastic games. We also show a\nmethod for efficiently reaching normal-form coarse correlated equilibria in\n\"single-controller\" stochastic games which follows the traditional no-regret\napproach. When shared randomness is available, the two generative algorithms\ncan be extended to give simultaneous regret bounds and converge in the\ntraditional sense.",
    "descriptor": "\nComments: 24 pages. Presented at UAI 2021\n",
    "authors": [
      "William Brown"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14280"
  },
  {
    "id": "arXiv:2210.14283",
    "title": "Accelerating Certified Robustness Training via Knowledge Transfer",
    "abstract": "Training deep neural network classifiers that are certifiably robust against\nadversarial attacks is critical to ensuring the security and reliability of\nAI-controlled systems. Although numerous state-of-the-art certified training\nmethods have been developed, they are computationally expensive and scale\npoorly with respect to both dataset and network complexity. Widespread usage of\ncertified training is further hindered by the fact that periodic retraining is\nnecessary to incorporate new data and network improvements. In this paper, we\npropose Certified Robustness Transfer (CRT), a general-purpose framework for\nreducing the computational overhead of any certifiably robust training method\nthrough knowledge transfer. Given a robust teacher, our framework uses a novel\ntraining loss to transfer the teacher's robustness to the student. We provide\ntheoretical and empirical validation of CRT. Our experiments on CIFAR-10 show\nthat CRT speeds up certified robustness training by $8 \\times$ on average\nacross three different architecture generations while achieving comparable\nrobustness to state-of-the-art methods. We also show that CRT can scale to\nlarge-scale datasets like ImageNet.",
    "descriptor": "\nComments: NeurIPS '22 Camera Ready version (with appendix)\n",
    "authors": [
      "Pratik Vaishnavi",
      "Kevin Eykholt",
      "Amir Rahmati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14283"
  },
  {
    "id": "arXiv:2210.14284",
    "title": "Refining Action Boundaries for One-stage Detection",
    "abstract": "Current one-stage action detection methods, which simultaneously predict\naction boundaries and the corresponding class, do not estimate or use a measure\nof confidence in their boundary predictions, which can lead to inaccurate\nboundaries. We incorporate the estimation of boundary confidence into one-stage\nanchor-free detection, through an additional prediction head that predicts the\nrefined boundaries with higher confidence. We obtain state-of-the-art\nperformance on the challenging EPIC-KITCHENS-100 action detection as well as\nthe standard THUMOS14 action detection benchmarks, and achieve improvement on\nthe ActivityNet-1.3 benchmark.",
    "descriptor": "\nComments: Accepted to AVSS 2022. Our code is available at this https URL\n",
    "authors": [
      "Hanyuan Wang",
      "Majid Mirmehdi",
      "Dima Damen",
      "Toby Perrett"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14284"
  },
  {
    "id": "arXiv:2210.14287",
    "title": "Combined Data and Deep Learning Model Uncertainties: An Application to  the Measurement of Solid Fuel Regression Rate",
    "abstract": "In complex physical process characterization, such as the measurement of the\nregression rate for solid hybrid rocket fuels, where both the observation data\nand the model used have uncertainties originating from multiple sources,\ncombining these in a systematic way for quantities of interest(QoI) remains a\nchallenge. In this paper, we present a forward propagation uncertainty\nquantification (UQ) process to produce a probabilistic distribution for the\nobserved regression rate $\\dot{r}$. We characterized two input data uncertainty\nsources from the experiment (the distortion from the camera $U_c$ and the\nnon-zero angle fuel placement $U_\\gamma$), the prediction and model form\nuncertainty from the deep neural network ($U_m$), as well as the variability\nfrom the manually segmented images used for training it ($U_s$). We conducted\nseven case studies on combinations of these uncertainty sources with the model\nform uncertainty. The main contribution of this paper is the investigation and\ninclusion of the experimental image data uncertainties involved, and how to\ninclude them in a workflow when the QoI is the result of multiple sequential\nprocesses.",
    "descriptor": "",
    "authors": [
      "Georgios Georgalis",
      "Kolos Retfalvi",
      "Paul E. DesJardin",
      "Abani Patra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14287"
  },
  {
    "id": "arXiv:2210.14290",
    "title": "Parallel Order-Based Core Maintenance in Dynamic Graphs",
    "abstract": "The core numbers of vertices in a graph are one of the most well-studied\ncohesive subgraph models because of the linear running time. In practice, many\ndata graphs are dynamic graphs that are continuously changing by inserting or\nremoving edges. The core numbers are updated in dynamic graphs with edge\ninsertions and deletions, which is called core maintenance. When a burst of a\nlarge number of inserted or removed edges come in, we have to handle these\nedges on time to keep up with the data stream. There are two main sequential\nalgorithms for core maintenance, \\textsc{Traversal} and \\textsc{Order}. It is\nproved that the \\textsc{Order} algorithm significantly outperforms the\n\\alg{Traversal} algorithm over all tested graphs with up to 2,083 times\nspeedups.\nTo the best of our knowledge, all existing parallel approaches are based on\nthe \\alg{Traversal} algorithm; also, their parallelism exists only for affected\nvertices with different core numbers, which will reduce to sequential when all\nvertices have the same core numbers. In this paper, we propose a new parallel\ncore maintenance algorithm based on the \\alg{Order} algorithm. Importantly, our\nnew approach always has parallelism, even for the graphs where all vertices\nhave the same core numbers. Extensive experiments are conducted over\nreal-world, temporal, and synthetic graphs on a 64-core machine. The results\nshow that for inserting and removing 100,000 edges using 16-worker, our method\nachieves up to 289x and 10x times speedups compared with the most efficient\nexisting method, respectively.",
    "descriptor": "\nComments: 15 pages, 6 figures\n",
    "authors": [
      "Bin Guo",
      "Emil Sekerinski"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14290"
  },
  {
    "id": "arXiv:2210.14295",
    "title": "Cross-View Image Sequence Geo-localization",
    "abstract": "Cross-view geo-localization aims to estimate the GPS location of a query\nground-view image by matching it to images from a reference database of\ngeo-tagged aerial images. To address this challenging problem, recent\napproaches use panoramic ground-view images to increase the range of\nvisibility. Although appealing, panoramic images are not readily available\ncompared to the videos of limited Field-Of-View (FOV) images. In this paper, we\npresent the first cross-view geo-localization method that works on a sequence\nof limited FOV images. Our model is trained end-to-end to capture the temporal\nstructure that lies within the frames using the attention-based temporal\nfeature aggregation module. To robustly tackle different sequences length and\nGPS noises during inference, we propose to use a sequential dropout scheme to\nsimulate variant length sequences. To evaluate the proposed approach in\nrealistic settings, we present a new large-scale dataset containing ground-view\nsequences along with the corresponding aerial-view images. Extensive\nexperiments and comparisons demonstrate the superiority of the proposed\napproach compared to several competitive baselines.",
    "descriptor": "",
    "authors": [
      "Xiaohan Zhang",
      "Waqas Sultani",
      "Safwan Wshah"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14295"
  },
  {
    "id": "arXiv:2210.14297",
    "title": "Progressively refined deep joint registration segmentation (ProRSeg) of  gastrointestinal organs at risk: Application to MRI and cone-beam CT",
    "abstract": "Method: ProRSeg was trained using 5-fold cross-validation with 110\nT2-weighted MRI acquired at 5 treatment fractions from 10 different patients,\ntaking care that same patient scans were not placed in training and testing\nfolds. Segmentation accuracy was measured using Dice similarity coefficient\n(DSC) and Hausdorff distance at 95th percentile (HD95). Registration\nconsistency was measured using coefficient of variation (CV) in displacement of\nOARs. Ablation tests and accuracy comparisons against multiple methods were\ndone. Finally, applicability of ProRSeg to segment cone-beam CT (CBCT) scans\nwas evaluated on 80 scans using 5-fold cross-validation. Results: ProRSeg\nprocessed 3D volumes (128 $\\times$ 192 $\\times$ 128) in 3 secs on a NVIDIA\nTesla V100 GPU. It's segmentations were significantly more accurate ($p<0.001$)\nthan compared methods, achieving a DSC of 0.94 $\\pm$0.02 for liver,\n0.88$\\pm$0.04 for large bowel, 0.78$\\pm$0.03 for small bowel and 0.82$\\pm$0.04\nfor stomach-duodenum from MRI. ProRSeg achieved a DSC of 0.72$\\pm$0.01 for\nsmall bowel and 0.76$\\pm$0.03 for stomach-duodenum from CBCT. ProRSeg\nregistrations resulted in the lowest CV in displacement (stomach-duodenum\n$CV_{x}$: 0.75\\%, $CV_{y}$: 0.73\\%, and $CV_{z}$: 0.81\\%; small bowel $CV_{x}$:\n0.80\\%, $CV_{y}$: 0.80\\%, and $CV_{z}$: 0.68\\%; large bowel $CV_{x}$: 0.71\\%,\n$CV_{y}$ : 0.81\\%, and $CV_{z}$: 0.75\\%). ProRSeg based dose accumulation\naccounting for intra-fraction (pre-treatment to post-treatment MRI scan) and\ninter-fraction motion showed that the organ dose constraints were violated in 4\npatients for stomach-duodenum and for 3 patients for small bowel. Study\nlimitations include lack of independent testing and ground truth phantom\ndatasets to measure dose accumulation accuracy.",
    "descriptor": "\nComments: This manuscript is currently under review at Medical Physics\n",
    "authors": [
      "Jue Jiang",
      "Jun Hong",
      "Kathryn Tringale",
      "Marsha Reyngold",
      "Christopher Crane",
      "Neelam Tyagi",
      "Harini Veeraraghavan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14297"
  },
  {
    "id": "arXiv:2210.14299",
    "title": "OpenStance: Real-world Zero-shot Stance Detection",
    "abstract": "Prior studies of zero-shot stance detection identify the attitude of texts\ntowards unseen topics occurring in the same document corpus. Such task\nformulation has three limitations: (i) Single domain/dataset. A system is\noptimized on a particular dataset from a single domain; therefore, the\nresulting system cannot work well on other datasets; (ii) the model is\nevaluated on a limited number of unseen topics; (iii) it is assumed that part\nof the topics has rich annotations, which might be impossible in real-world\napplications. These drawbacks will lead to an impractical stance detection\nsystem that fails to generalize to open domains and open-form topics. This work\ndefines OpenStance: open-domain zero-shot stance detection, aiming to handle\nstance detection in an open world with neither domain constraints nor\ntopic-specific annotations. The key challenge of OpenStance lies in the\nopen-domain generalization: learning a system with fully unspecific supervision\nbut capable of generalizing to any dataset. To solve OpenStance, we propose to\ncombine indirect supervision, from textual entailment datasets, and weak\nsupervision, from data generated automatically by pre-trained Language Models.\nOur single system, without any topic-specific supervision, outperforms the\nsupervised method on three popular datasets. To our knowledge, this is the\nfirst work that studies stance detection under the open-domain zero-shot\nsetting. All data and code are publicly released.",
    "descriptor": "\nComments: CoNLL 2022 Camera-ready version\n",
    "authors": [
      "Hanzi Xu",
      "Slobodan Vucetic",
      "Wenpeng Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14299"
  },
  {
    "id": "arXiv:2210.14303",
    "title": "WaveBound: Dynamic Error Bounds for Stable Time Series Forecasting",
    "abstract": "Time series forecasting has become a critical task due to its high\npracticality in real-world applications such as traffic, energy consumption,\neconomics and finance, and disease analysis. Recent deep-learning-based\napproaches have shown remarkable success in time series forecasting.\nNonetheless, due to the dynamics of time series data, deep networks still\nsuffer from unstable training and overfitting. Inconsistent patterns appearing\nin real-world data lead the model to be biased to a particular pattern, thus\nlimiting the generalization. In this work, we introduce the dynamic error\nbounds on training loss to address the overfitting issue in time series\nforecasting. Consequently, we propose a regularization method called WaveBound\nwhich estimates the adequate error bounds of training loss for each time step\nand feature at each iteration. By allowing the model to focus less on\nunpredictable data, WaveBound stabilizes the training process, thus\nsignificantly improving generalization. With the extensive experiments, we show\nthat WaveBound consistently improves upon the existing models in large margins,\nincluding the state-of-the-art model.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Youngin Cho",
      "Daejin Kim",
      "Dongmin Kim",
      "Mohammad Azam Khan",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14303"
  },
  {
    "id": "arXiv:2210.14304",
    "title": "Learning Better Intent Representations for Financial Open Intent  Classification",
    "abstract": "With the recent surge of NLP technologies in the financial domain, banks and\nother financial entities have adopted virtual agents (VA) to assist customers.\nA challenging problem for VAs in this domain is determining a user's reason or\nintent for contacting the VA, especially when the intent was unseen or open\nduring the VA's training. One method for handling open intents is adaptive\ndecision boundary (ADB) post-processing, which learns tight decision boundaries\nfrom intent representations to separate known and open intents. We propose\nincorporating two methods for supervised pre-training of intent\nrepresentations: prefix-tuning and fine-tuning just the last layer of a large\nlanguage model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher\nthan the prior state-of-the-art ADB method for open intent classification on\nthe banking77 benchmark amongst others. Notably, we only supplement the\noriginal ADB model with 0.1% additional trainable parameters. Ablation studies\nalso determine that our method yields better results than full fine-tuning the\nentire model. We hypothesize that our findings could stimulate a new optimal\nmethod of downstream tuning that combines parameter efficient tuning modules\nwith fine-tuning a subset of the base model's layers.",
    "descriptor": "\nComments: Accepted to FinNLP-2022, in conjunction with EMNLP-2022\n",
    "authors": [
      "Xianzhi Li",
      "Will Aitken",
      "Xiaodan Zhu",
      "Stephen W. Thomas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2210.14304"
  },
  {
    "id": "arXiv:2210.14306",
    "title": "Reading Between the Lines: Modeling User Behavior and Costs in  AI-Assisted Programming",
    "abstract": "AI code-recommendation systems (CodeRec), such as Copilot, can assist\nprogrammers inside an IDE by suggesting and autocompleting arbitrary code;\npotentially improving their productivity. To understand how these AI improve\nprogrammers in a coding session, we need to understand how they affect\nprogrammers' behavior. To make progress, we studied GitHub Copilot, and\ndeveloped CUPS -- a taxonomy of 12 programmer activities common to AI code\ncompletion systems. We then conducted a study with 21 programmers who completed\ncoding tasks and used our labeling tool to retrospectively label their sessions\nwith CUPS. We analyze over 3000 label instances, and visualize the results with\ntimelines and state machines to profile programmer-CodeRec interaction. This\nreveals novel insights into the distribution and patterns of programmer\nbehavior, as well as inefficiencies and time costs. Finally, we use these\ninsights to inform future interventions to improve AI-assisted programming and\nhuman-AI interaction.",
    "descriptor": "",
    "authors": [
      "Hussein Mozannar",
      "Gagan Bansal",
      "Adam Fourney",
      "Eric Horvitz"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14306"
  },
  {
    "id": "arXiv:2210.14307",
    "title": "On Robust Incremental Learning over Many Multilingual Steps",
    "abstract": "Recent work in incremental learning has introduced diverse approaches to\ntackle catastrophic forgetting from data augmentation to optimized training\nregimes. However, most of them focus on very few training steps. We propose a\nmethod for robust incremental learning over dozens of fine-tuning steps using\ndata from a variety of languages. We show that a combination of\ndata-augmentation and an optimized training regime allows us to continue\nimproving the model even for as many as fifty training steps. Crucially, our\naugmentation strategy does not require retaining access to previous training\ndata and is suitable in scenarios with privacy constraints.",
    "descriptor": "\nComments: Accepted for publication at the IncrLearn Workshop at the 22nd IEEE International Conference on Data Mining\n",
    "authors": [
      "Karan Praharaj",
      "Irina Matveeva"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14307"
  },
  {
    "id": "arXiv:2210.14309",
    "title": "Empowering Long-tail Item Recommendation through Cross Decoupling  Network (CDN)",
    "abstract": "Recommenders provide personalized content recommendations to users. They\noften suffer from highly skewed long-tail item distributions, with a small\nfraction of the items receiving most of the user feedback. This hurts model\nquality especially for the slices without much supervision. Existing work in\nboth academia and industry mainly focuses on re-balancing strategies (e.g.,\nup-sampling and up-weighting), leveraging content features, and transfer\nlearning. However, there still lacks of a deeper understanding of how the\nlong-tail distribution influences the recommendation performance.\nIn this work, we theoretically demonstrate that the prediction of user\npreference is biased under the long-tail distributions. This bias comes from\nthe discrepancy of both the prior and conditional probabilities between\ntraining data and test data. Most existing methods mainly attempt to reduce the\nbias from the prior perspective, which ignores the discrepancy in the\nconditional probability. This leads to a severe forgetting issue and results in\nsuboptimal performance. To address the problem, we design a novel Cross\nDecoupling Network (CDN) to reduce the differences in both prior and\nconditional probabilities. Specifically, CDN (i) decouples the learning process\nof memorization and generalization on the item side through a mixture-of-expert\nstructure; (ii) decouples the user samples from different distributions through\na regularized bilateral branch network. Finally, a novel adapter is introduced\nto aggregate the decoupled vectors, and softly shift the training attention to\ntail items. Extensive experimental results show that CDN significantly\noutperforms state-of-the-art approaches on popular benchmark datasets, leading\nto an improvement in HR@50 (hit ratio) of 8.7\\% for overall recommendation and\n12.4\\% for tail items.",
    "descriptor": "",
    "authors": [
      "Yin Zhang",
      "Ruoxi Wang",
      "Derek Zhiyuan Cheng",
      "Tiansheng Yao",
      "Xinyang Yi",
      "Lichan Hong",
      "James Caverlee",
      "Ed H. Chi"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.14309"
  },
  {
    "id": "arXiv:2210.14312",
    "title": "JAX-DIPS: Neural bootstrapping of finite discretization methods and  application to elliptic problems with discontinuities",
    "abstract": "We present a scalable strategy for development of mesh-free hybrid\nneuro-symbolic partial differential equation solvers based on existing\nmesh-based numerical discretization methods. Particularly, this strategy can be\nused to efficiently train neural network surrogate models for the solution\nfunctions and operators of partial differential equations while retaining the\naccuracy and convergence properties of the state-of-the-art numerical solvers.\nThe presented neural bootstrapping method (hereby dubbed NBM) is based on\nevaluation of the finite discretization residuals of the PDE system obtained on\nimplicit Cartesian cells centered on a set of random collocation points with\nrespect to trainable parameters of the neural network. We apply NBM to the\nimportant class of elliptic problems with jump conditions across irregular\ninterfaces in three spatial dimensions. We show the method is convergent such\nthat model accuracy improves by increasing number of collocation points in the\ndomain. The algorithms presented here are implemented and released in a\nsoftware package named JAX-DIPS (https://github.com/JAX-DIPS/JAX-DIPS),\nstanding for differentiable interfacial PDE solver. JAX-DIPS is purely\ndeveloped in JAX, offering end-to-end differentiability from mesh generation to\nthe higher level discretization abstractions, geometric integrations, and\ninterpolations, thus facilitating research into use of differentiable\nalgorithms for developing hybrid PDE solvers.",
    "descriptor": "\nComments: Library release is pending paper acceptance\n",
    "authors": [
      "Pouria Mistani",
      "Samira Pakravan",
      "Rajesh Ilango",
      "Frederic Gibou"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.14312"
  },
  {
    "id": "arXiv:2210.14313",
    "title": "An efficient and fast sparse grid algorithm for high-dimensional  numerical integration",
    "abstract": "This paper is concerned with developing an efficient numerical algorithm for\nfast implementation of the sparse grid method for computing the $d$-dimensional\nintegral of a given function. The new algorithm, called the MDI-SG ({\\em\nmultilevel dimension iteration sparse grid}) method, implements the sparse grid\nmethod based on a dimension iteration/reduction procedure, it does not need to\nstore the integration points, neither does it compute the function values\nindependently at each integration point, instead, it re-uses the computation\nfor function evaluations as much as possible by performing the function\nevaluations at all integration points in a cluster and iteratively along\ncoordinate directions. It is shown numerically that the computational\ncomplexity (in terms of CPU time) of the proposed MDI-SG method is of\npolynomial order $O(Nd^3 )$ or better, compared to the exponential order\n$O(N(\\log N)^{d-1})$ for the standard sparse grid method, where $N$ denotes the\nmaximum number of integration points in each coordinate direction. As a result,\nthe proposed MDI-SG method effectively circumvents the curse of dimensionality\nsuffered by the standard sparse grid method for high-dimensional numerical\nintegration.",
    "descriptor": "\nComments: 28 pages, 12 tables, 8 figures. arXiv admin note: text overlap with arXiv:2210.13658\n",
    "authors": [
      "Huicong Zhong",
      "Xiaobing Feng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14313"
  },
  {
    "id": "arXiv:2210.14315",
    "title": "Streaming Submodular Maximization with Differential Privacy",
    "abstract": "In this work, we study the problem of privately maximizing a submodular\nfunction in the streaming setting. Extensive work has been done on privately\nmaximizing submodular functions in the general case when the function depends\nupon the private data of individuals. However, when the size of the data stream\ndrawn from the domain of the objective function is large or arrives very fast,\none must privately optimize the objective within the constraints of the\nstreaming setting. We establish fundamental differentially private baselines\nfor this problem and then derive better trade-offs between privacy and utility\nfor the special case of decomposable submodular functions. A submodular\nfunction is decomposable when it can be written as a sum of submodular\nfunctions; this structure arises naturally when each summand function models\nthe utility of an individual and the goal is to study the total utility of the\nwhole population as in the well-known Combinatorial Public Projects Problem.\nFinally, we complement our theoretical analysis with experimental\ncorroboration.",
    "descriptor": "",
    "authors": [
      "Anamay Chaturvedi",
      "Huy L\u00ea Nguyen",
      "Thy Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14315"
  },
  {
    "id": "arXiv:2210.14318",
    "title": "Object recognition in atmospheric turbulence scenes",
    "abstract": "The influence of atmospheric turbulence on acquired surveillance imagery\nmakes image interpretation and scene analysis extremely difficult. It also\nreduces the effectiveness of conventional approaches for classifying, and\ntracking targets in the scene. Whilst deep-learning based object detection is\nhighly successful in normal conditions, these methods cannot directly be\napplied to the atmospheric turbulence sequences. This paper hence proposes a\nnovel framework learning the distorted features to detect and classify object\ntypes. Specifically, deformable convolutions are exploited to deal with spatial\nturbulent displacement. The features are extracted via a feature pyramid\nnetwork and Faster R-CNN is employed as a detector. Testing with synthetic VOC\ndataset, the results show that the proposed framework outperforms the benchmark\nwith mean Average Precision (mAP) score of >30%. Subjective results on the real\ndata are also significantly improved.",
    "descriptor": "",
    "authors": [
      "Disen Hu",
      "Nantheera Anantrasirichai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14318"
  },
  {
    "id": "arXiv:2210.14319",
    "title": "Explicitly Increasing Input Information Density for Vision Transformers  on Small Datasets",
    "abstract": "Vision Transformers have attracted a lot of attention recently since the\nsuccessful implementation of Vision Transformer (ViT) on vision tasks. With\nvision Transformers, specifically the multi-head self-attention modules,\nnetworks can capture long-term dependencies inherently. However, these\nattention modules normally need to be trained on large datasets, and vision\nTransformers show inferior performance on small datasets when training from\nscratch compared with widely dominant backbones like ResNets. Note that the\nTransformer model was first proposed for natural language processing, which\ncarries denser information than natural images. To boost the performance of\nvision Transformers on small datasets, this paper proposes to explicitly\nincrease the input information density in the frequency domain. Specifically,\nwe introduce selecting channels by calculating the channel-wise heatmaps in the\nfrequency domain using Discrete Cosine Transform (DCT), reducing the size of\ninput while keeping most information and hence increasing the information\ndensity. As a result, 25% fewer channels are kept while better performance is\nachieved compared with previous work. Extensive experiments demonstrate the\neffectiveness of the proposed approach on five small-scale datasets, including\nCIFAR-10/100, SVHN, Flowers-102, and Tiny ImageNet. The accuracy has been\nboosted up to 17.05% with Swin and Focal Transformers. Codes are available at\nhttps://github.com/xiangyu8/DenseVT.",
    "descriptor": "\nComments: Accepted to NeurIPS workshop (VTTA) 2022\n",
    "authors": [
      "Xiangyu Chen",
      "Ying Qin",
      "Wenju Xu",
      "Andr\u00e9s M. Bur",
      "Cuncong Zhong",
      "Guanghui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14319"
  },
  {
    "id": "arXiv:2210.14320",
    "title": "FO-PINNs: A First-Order formulation for Physics Informed Neural Networks",
    "abstract": "We present FO-PINNs, physics-informed neural networks that are trained using\nthe first-order formulation of the Partial Differential Equation (PDE) losses.\nWe show that FO-PINNs offer significantly higher accuracy in solving\nparameterized systems compared to traditional PINNs, and reduce\ntime-per-iteration by removing the extra backpropagations needed to compute the\nsecond or higher-order derivatives. Additionally, unlike standard PINNs,\nFO-PINNs can be used with exact imposition of boundary conditions using\napproximate distance functions, and can be trained using Automatic Mixed\nPrecision (AMP) to further speed up the training. Through two Helmholtz and\nNavier-Stokes examples, we demonstrate the advantages of FO-PINNs over\ntraditional PINNs in terms of accuracy and training speedup.",
    "descriptor": "\nComments: 6 pages, 3 figures, Selected for ML4PS workshop at NeurIPS 2022\n",
    "authors": [
      "Rini J. Gladstone",
      "Mohammad A. Nabian",
      "Hadi Meidani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14320"
  },
  {
    "id": "arXiv:2210.14322",
    "title": "ANACONDA: An Improved Dynamic Regret Algorithm for Adaptive  Non-Stationary Dueling Bandits",
    "abstract": "We study the problem of non-stationary dueling bandits and provide the first\nadaptive dynamic regret algorithm for this problem. The only two existing\nattempts in this line of work fall short across multiple dimensions, including\npessimistic measures of non-stationary complexity and non-adaptive parameter\ntuning that requires knowledge of the number of preference changes. We develop\nan elimination-based rescheduling algorithm to overcome these shortcomings and\nshow a near-optimal $\\tilde{O}(\\sqrt{S^{\\texttt{CW}} T})$ dynamic regret bound,\nwhere $S^{\\texttt{CW}}$ is the number of times the Condorcet winner changes in\n$T$ rounds. This yields the first near-optimal dynamic regret algorithm for\nunknown $S^{\\texttt{CW}}$. We further study other related notions of\nnon-stationarity for which we also prove near-optimal dynamic regret guarantees\nunder additional assumptions on the underlying preference model.",
    "descriptor": "",
    "authors": [
      "Thomas Kleine Buening",
      "Aadirupa Saha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14322"
  },
  {
    "id": "arXiv:2210.14324",
    "title": "The Championship Simulator: Architectural Simulation for Education and  Competition",
    "abstract": "Recent years have seen a dramatic increase in the microarchitectural\ncomplexity of processors. This increase in complexity presents a twofold\nchallenge for the field of computer architecture. First, no individual\narchitect can fully comprehend the complexity of the entire microarchitecture\nof the core. This leads to increasingly specialized architects, who treat parts\nof the core outside their particular expertise as black boxes. Second, with\nincreasing complexity, the field becomes decreasingly accessible to new\nstudents of the field. When learning core microarchitecture, new students must\nfirst learn the big picture of how the system works in order to understand how\nthe pieces all fit together. The tools used to study microarchitecture\nexperience a similar struggle. As with the microarchitectures they simulate, an\nincrease in complexity reduces accessibility to new users.\nIn this work, we present ChampSim. ChampSim uses a modular design and\nconfigurable structure to achieve a low barrier to entry into the field of\nmicroarchitecural simulation. ChampSim has shown itself to be useful in\nmultiple areas of research, competition, and education. In this way, we seek to\npromote access and inclusion despite the increasing complexity of the field of\ncomputer architecture.",
    "descriptor": "",
    "authors": [
      "Nathan Gober",
      "Gino Chacon",
      "Lei Wang",
      "Paul V. Gratz",
      "Daniel A. Jimenez",
      "Elvira Teran",
      "Seth Pugsley",
      "Jinchun Kim"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.14324"
  },
  {
    "id": "arXiv:2210.14325",
    "title": "Obtrusive Subtleness and Why We Should Focus on Meaning, not Form, in  Social Acceptability Studies",
    "abstract": "Nowadays, interactive technologies are used almost everywhere. As a result,\ndesigners need to increasingly make them \"socially acceptable\". Previous work\nrecommends \"subtle\" forms of interaction to increase social acceptability and\navoid negative experiences. Although often appropriate, such uniform\nrecommendations neglect the variety of social situations. We demonstrate this\nlimitation in an experiment (N=35), by comparing the observer experience of\ndifferent forms of interaction in \"face-to-face conversations\", a social\nsituation rarely studied. Here, the typically recommended form of interaction\n(\"subtle\") led to a more negative observer experience than the usually\ndeprecated form (\"suspenseful\"), in terms of affective experience and product\nperception. It also made the user appear less extraverted. We conclude by\npositioning interactions with technology not as separate from the social\nsituation in which they are performed, but as a constitutive part of it that\nmeaningfully relates to other situated activities.",
    "descriptor": "\nComments: 11 pages, 4 figures\n",
    "authors": [
      "Alarith Uhde",
      "Tim zum Hoff",
      "Marc Hassenzahl"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.14325"
  },
  {
    "id": "arXiv:2210.14326",
    "title": "Band selection and classification of hyperspectral images by minimizing  normalized mutual information",
    "abstract": "Hyperspectral images (HSI) classification is a high technical remote sensing\ntool. The main goal is to classify the point of a region. The HIS contains more\nthan a hundred bidirectional measures, called bands (or simply images), of the\nsame region called Ground Truth Map (GT). Unfortunately, some bands contain\nredundant information, others are affected by the noise, and the high\ndimensionalities of features make the accuracy of classification lower. All\nthese bands can be important for some applications, but for the classification\na small subset of these is relevant. In this paper we use mutual information\n(MI) to select the relevant bands; and the Normalized Mutual Information\ncoefficient to avoid and control redundant ones. This is a feature selection\nscheme and a Filter strategy. We establish this study on HSI AVIRIS 92AV3C.\nThis is effectiveness, and fast scheme to control redundancy. Index Terms:\nHyperspectral images, Classification, Feature Selection, Normalized Mutual\nInformation, Redundancy.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2210.13456, arXiv:2210.12296, arXiv:1211.0613, arXiv:1210.0528; text overlap with arXiv:1210.0052, arXiv:1211.0055\n",
    "authors": [
      "E.Sarhrouni",
      "A. Hammouch",
      "D. Aboutajdine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14326"
  },
  {
    "id": "arXiv:2210.14327",
    "title": "Smart cloud collocation: geometry-aware adaptivity directly from CAD",
    "abstract": "Computer Aided Design (CAD) is widely used in the creation and optimization\nof various industrial systems and processes. Transforming a CAD geometry into a\ncomputational discretization that be used to solve PDEs requires care and a\ndeep knowledge of the selected computational method. In this article, we\npresent a novel integrated collocation scheme based on smart clouds. It allows\nus to transform a CAD geometry into a complete point collocation model, aware\nof the base geometry, with minimum effort. For this process, only the geometry\nof the domain, in the form of a STEP file, and the boundary conditions are\nneeded. We also introduce an adaptive refinement process for the resultant\nsmart cloud using an \\textit{a posteriori} error indication. The scheme can be\napplied to any 2D or 3D geometry, to any PDE and can be applied to most point\ncollocation approaches. We illustrate this with the meshfree Generalized Finite\nDifference (GFD) method applied to steady linear elasticity problems. We\nfurther show that each step of this process, from the initial discretization to\nthe refinement strategy, is connected and is affected by the approach selected\nin the previous step, thus requiring an integrated scheme where the whole\nsolution process should be considered at once.",
    "descriptor": "",
    "authors": [
      "Thibault Jacquemin",
      "Pratik Suchde",
      "St\u00e9phane P.A. Bordas"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14327"
  },
  {
    "id": "arXiv:2210.14328",
    "title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language  Models",
    "abstract": "Structural probing work has found evidence for latent syntactic information\nin pre-trained language models. However, much of this analysis has focused on\nmonolingual models, and analyses of multilingual models have employed\ncorrelational methods that are confounded by the choice of probing tasks. In\nthis study, we causally probe multilingual language models (XGLM and\nmultilingual BERT) as well as monolingual BERT-based models across various\nlanguages; we do this by performing counterfactual perturbations on neuron\nactivations and observing the effect on models' subject-verb agreement\nprobabilities. We observe where in the model and to what extent syntactic\nagreement is encoded in each language. We find significant neuron overlap\nacross languages in autoregressive multilingual language models, but not masked\nlanguage models. We also find two distinct layer-wise effect patterns and two\ndistinct sets of neurons used for syntactic agreement, depending on whether the\nsubject and verb are separated by other tokens. Finally, we find that\nbehavioral analyses of language models are likely underestimating how sensitive\nmasked language models are to syntactic information.",
    "descriptor": "\nComments: Accepted to CoNLL 2022\n",
    "authors": [
      "Aaron Mueller",
      "Yu Xia",
      "Tal Linzen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14328"
  },
  {
    "id": "arXiv:2210.14333",
    "title": "On multiscale quasi-interpolation of scattered scalar- and  manifold-valued functions",
    "abstract": "We address the problem of approximating an unknown function from its discrete\nsamples given at arbitrarily scattered sites. This problem is essential in\nnumerical sciences, where modern applications also highlight the need for a\nsolution to the case of functions with manifold values. In this paper, we\nintroduce and analyze a combination of kernel-based quasi-interpolation and\nmultiscale approximations for both scalar- and manifold-valued functions. While\nquasi-interpolation provides a powerful tool for approximation problems if the\ndata is defined on infinite grids, the situation is more complicated when it\ncomes to scattered data. Hence, this paper is particularly interested in\nstudying the improvement achieved by combining quasi-interpolation with a\nmultiscale technique. The main contributions of this paper are as follows.\nFirst, we introduce the multiscale quasi-interpolation technique for\nscalar-valued functions. Second, we show how this technique can be carried over\nto the manifold-valued setting. Third, we give mathematical proof that\nconverging quasi-interpolation will also lead to converging multiscale\nquasi-interpolation. Fourth, we provide ample numerical evidence that\nmultiscale quasi-interpolation has superior convergence to quasi-interpolation.\nIn addition, we will provide examples showing that the multiscale\nquasi-interpolation approach offers a powerful tool for many data analysis\ntasks, such as denoising and anomaly detection. It is especially attractive for\ncases of massive data points and high-dimensionality.",
    "descriptor": "\nComments: 22 pages, 12 figures\n",
    "authors": [
      "Nir Sharon",
      "Rafael Sherbu Cohen",
      "Holger Wendland"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14333"
  },
  {
    "id": "arXiv:2210.14346",
    "title": "New wrapper method based on normalized mutual information for dimension  reduction and classification of hyperspectral images",
    "abstract": "Feature selection is one of the most important problems in hyperspectral\nimages classification. It consists to choose the most informative bands from\nthe entire set of input datasets and discard the noisy, redundant and\nirrelevant ones. In this context, we propose a new wrapper method based on\nnormalized mutual information (NMI) and error probability (PE) using support\nvector machine (SVM) to reduce the dimensionality of the used hyperspectral\nimages and increase the classification efficiency. The experiments have been\nperformed on two challenging hyperspectral benchmarks datasets captured by the\nNASA's Airborne Visible/Infrared Imaging Spectrometer Sensor (AVIRIS). Several\nmetrics had been calculated to evaluate the performance of the proposed\nalgorithm. The obtained results prove that our method can increase the\nclassification performance and provide an accurate thematic map in comparison\nwith other reproduced algorithms. This method may be improved for more\nclassification efficiency. Keywords-Feature selection, hyperspectral images,\nclassification, wrapper, normalized mutual information, support vector machine.",
    "descriptor": "",
    "authors": [
      "Hasna Nhaila",
      "Asma Elmaizi",
      "Elkebir Sarhrouni",
      "Ahmed Hammouch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14346"
  },
  {
    "id": "arXiv:2210.14348",
    "title": "Synthetic Text Generation with Differential Privacy: A Simple and  Practical Recipe",
    "abstract": "Privacy concerns have attracted increasing attention in data-driven products\nand services. Existing legislation forbids arbitrary processing of personal\ndata collected from individuals. Generating synthetic versions of such data\nwith a formal privacy guarantee such as differential privacy (DP) is considered\nto be a solution to address privacy concerns. In this direction, we show a\nsimple, practical, and effective recipe in the text domain: simply fine-tuning\na generative language model with DP allows us to generate useful synthetic text\nwhile mitigating privacy concerns. Through extensive empirical analyses, we\ndemonstrate that our method produces synthetic data that is competitive in\nterms of utility with its non-private counterpart and meanwhile provides strong\nprotection against potential privacy leakages.",
    "descriptor": "",
    "authors": [
      "Xiang Yue",
      "Huseyin A. Inan",
      "Xuechen Li",
      "Girish Kumar",
      "Julia McAnallen",
      "Huan Sun",
      "David Levitan",
      "Robert Sim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14348"
  },
  {
    "id": "arXiv:2210.14349",
    "title": "A DirectX-Based DICOM Viewer for Multi-User Surgical Planning in  Augmented Reality",
    "abstract": "Preoperative medical imaging is an essential part of surgical planning. The\ndata from medical imaging devices, such as CT and MRI scanners, consist of\nstacks of 2D images in DICOM format. Conversely, advances in 3D data\nvisualization provide further information by assembling cross-sections into 3D\nvolumetric datasets. As Microsoft unveiled the HoloLens 2 (HL2), which is\nconsidered one of the best Mixed Reality (XR) headsets in the market, it\npromised to enhance visualization in 3D by providing an immersive experience to\nusers. This paper introduces a prototype holographic XR DICOM Viewer for the 3D\nvisualization of DICOM image sets on HL2 for surgical planning. We first\ndeveloped a standalone graphical C++ engine using the native DirectX11 API and\nHLSL shaders. Based on that, the prototype further applies the OpenXR API for\npotential deployment on a wide range of devices from vendors across the XR\nspectrum. With native access to the device, our prototype unravels the\nlimitation of hardware capabilities on HL2 for 3D volume rendering and\ninteraction. Moreover, smartphones can act as input devices to provide another\nuser interaction method by connecting to our server. In this paper, we present\na holographic DICOM viewer for the HoloLens 2 and contribute (i) a prototype\nthat renders the DICOM image stacks in real-time on HL2, (ii) three types of\nuser interactions in XR, and (iii) a preliminary qualitative evaluation of our\nprototype.",
    "descriptor": "",
    "authors": [
      "Menghe Zhang",
      "Weichen Liu",
      "Nadir Weibel",
      "Jurgen Schulze"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.14349"
  },
  {
    "id": "arXiv:2210.14350",
    "title": "Detection and estimation of spacecraft maneuvers for catalog maintenance",
    "abstract": "Building and maintaining a catalog of resident space objects involves several\ntasks, ranging from observations to data analysis. Once acquired, the knowledge\nof a space object needs to be updated following a dedicated observing schedule.\nDynamics mismodeling and unknown maneuvers can alter the catalog's accuracy,\nresulting in uncorrelated observations originating from the same object.\nStarting from two independent orbits, this work presents a novel approach to\ndetect and estimate maneuvers of resident space objects, which allows for\ncorrelation recovery. The estimation is performed with successive convex\noptimization without a-priori assumption on the thrust arcs structure and\nthrust direction.",
    "descriptor": "\nComments: 14 pages, 9 figures, 1 table\n",
    "authors": [
      "Laura Pirovano",
      "Roberto Armellin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)"
    ],
    "url": "https://arxiv.org/abs/2210.14350"
  },
  {
    "id": "arXiv:2210.14353",
    "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question  Answering",
    "abstract": "We introduce RoMQA, the first benchmark for robust, multi-evidence,\nmulti-answer question answering (QA). RoMQA contains clusters of questions that\nare derived from related constraints mined from the Wikidata knowledge graph.\nRoMQA evaluates robustness of QA models to varying constraints by measuring\nworst-case performance within each question cluster. Compared to prior QA\ndatasets, RoMQA has more human-written questions that require reasoning over\nmore evidence text and have, on average, many more correct answers. In\naddition, human annotators rate RoMQA questions as more natural or likely to be\nasked by people. We evaluate state-of-the-art large language models in\nzero-shot, few-shot, and fine-tuning settings, and find that RoMQA is\nchallenging: zero-shot and few-shot models perform similarly to naive\nbaselines, while supervised retrieval methods perform well below gold evidence\nupper bounds. Moreover, existing models are not robust to variations in\nquestion constraints, but can be made more robust by tuning on clusters of\nrelated questions. Our results show that RoMQA is a challenging benchmark for\nlarge language models, and provides a quantifiable test to build more robust QA\nmethods.",
    "descriptor": "",
    "authors": [
      "Victor Zhong",
      "Weijia Shi",
      "Wen-tau Yih",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14353"
  },
  {
    "id": "arXiv:2210.14358",
    "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled  Representations",
    "abstract": "There is an inescapable long-tailed class-imbalance issue in many real-world\nclassification problems. Existing long-tailed classification methods focus on\nthe single-domain setting, where all examples are drawn from the same\ndistribution. However, real-world scenarios often involve multiple domains with\ndistinct imbalanced class distributions. We study this multi-domain long-tailed\nlearning problem and aim to produce a model that generalizes well across all\nclasses and domains. Towards that goal, we introduce TALLY, which produces\ninvariant predictors by balanced augmenting hidden representations over domains\nand classes. Built upon a proposed selective balanced sampling strategy, TALLY\nachieves this by mixing the semantic representation of one example with the\ndomain-associated nuisances of another, producing a new representation for use\nas data augmentation. To improve the disentanglement of semantic\nrepresentations, TALLY further utilizes a domain-invariant class prototype that\naverages out domain-specific effects. We evaluate TALLY on four long-tailed\nvariants of classical domain generalization benchmarks and two real-world\nimbalanced multi-domain datasets. The results indicate that TALLY consistently\noutperforms other state-of-the-art methods in both subpopulation shift and\ndomain shift.",
    "descriptor": "",
    "authors": [
      "Huaxiu Yao",
      "Xinyu Yang",
      "Allan Zhou",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14358"
  },
  {
    "id": "arXiv:2210.14360",
    "title": "LaundroGraph: Self-Supervised Graph Representation Learning for  Anti-Money Laundering",
    "abstract": "Anti-money laundering (AML) regulations mandate financial institutions to\ndeploy AML systems based on a set of rules that, when triggered, form the basis\nof a suspicious alert to be assessed by human analysts. Reviewing these cases\nis a cumbersome and complex task that requires analysts to navigate a large\nnetwork of financial interactions to validate suspicious movements.\nFurthermore, these systems have very high false positive rates (estimated to be\nover 95\\%). The scarcity of labels hinders the use of alternative systems based\non supervised learning, reducing their applicability in real-world\napplications.\nIn this work we present LaundroGraph, a novel self-supervised graph\nrepresentation learning approach to encode banking customers and financial\ntransactions into meaningful representations. These representations are used to\nprovide insights to assist the AML reviewing process, such as identifying\nanomalous movements for a given customer. LaundroGraph represents the\nunderlying network of financial interactions as a customer-transaction\nbipartite graph and trains a graph neural network on a fully self-supervised\nlink prediction task. We empirically demonstrate that our approach outperforms\nother strong baselines on self-supervised link prediction using a real-world\ndataset, improving the best non-graph baseline by $12$ p.p. of AUC. The goal is\nto increase the efficiency of the reviewing process by supplying these\nAI-powered insights to the analysts upon review. To the best of our knowledge,\nthis is the first fully self-supervised system within the context of AML\ndetection.",
    "descriptor": "\nComments: Accepted at ACM International Conference on AI in Finance 2022 (ICAIF'22)\n",
    "authors": [
      "M\u00e1rio Cardoso",
      "Pedro Saleiro",
      "Pedro Bizarro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.14360"
  },
  {
    "id": "arXiv:2210.14361",
    "title": "Auxiliary task discovery through generate-and-test",
    "abstract": "In this paper, we explore an approach to auxiliary task discovery in\nreinforcement learning based on ideas from representation learning. Auxiliary\ntasks tend to improve data efficiency by forcing the agent to learn auxiliary\nprediction and control objectives in addition to the main task of maximizing\nreward, and thus producing better representations. Typically these tasks are\ndesigned by people. Meta-learning offers a promising avenue for automatic task\ndiscovery; however, these methods are computationally expensive and challenging\nto tune in practice. In this paper, we explore a complementary approach to the\nauxiliary task discovery: continually generating new auxiliary tasks and\npreserving only those with high utility. We also introduce a new measure of\nauxiliary tasks usefulness based on how useful the features induced by them are\nfor the main task. Our discovery algorithm significantly outperforms random\ntasks, hand-designed tasks, and learning without auxiliary tasks across a suite\nof environments.",
    "descriptor": "",
    "authors": [
      "Banafsheh Rafiee",
      "Sina Ghiassian",
      "Jun Jin",
      "Richard Sutton",
      "Jun Luo",
      "Adam White"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14361"
  },
  {
    "id": "arXiv:2210.14362",
    "title": "Federated Learning Using Variance Reduced Stochastic Gradient for  Probabilistically Activated Agents",
    "abstract": "This paper proposes an algorithm for Federated Learning (FL) with a two-layer\nstructure that achieves both variance reduction and a faster convergence rate\nto an optimal solution in the setting where each agent has an arbitrary\nprobability of selection in each iteration. In distributed machine learning,\nwhen privacy matters, FL is a functional tool. Placing FL in an environment\nwhere it has some irregular connections of agents (devices), reaching a trained\nmodel in both an economical and quick way can be a demanding job. The first\nlayer of our algorithm corresponds to the model parameter propagation across\nagents done by the server. In the second layer, each agent does its local\nupdate with a stochastic and variance-reduced technique called Stochastic\nVariance Reduced Gradient (SVRG). We leverage the concept of variance reduction\nfrom stochastic optimization when the agents want to do their local update step\nto reduce the variance caused by stochastic gradient descent (SGD). We provide\na convergence bound for our algorithm which improves the rate from\n$O(\\frac{1}{\\sqrt{K}})$ to $O(\\frac{1}{K})$ by using a constant step-size. We\ndemonstrate the performance of our algorithm using numerical examples.",
    "descriptor": "",
    "authors": [
      "M. R. Rostami",
      "S. S. Kia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14362"
  },
  {
    "id": "arXiv:2210.14363",
    "title": "Enhancing Product Safety in E-Commerce with NLP",
    "abstract": "Ensuring safety of the products offered to the customers is of paramount\nimportance to any e- commerce platform. Despite stringent quality and safety\nchecking of products listed on these platforms, occasionally customers might\nreceive a product that can pose a safety issue arising out of its use. In this\npaper, we present an innovative mechanism of how a large scale multinational\ne-commerce platform, Zalando, uses Natural Language Processing techniques to\nassist timely investigation of the potentially unsafe products mined directly\nfrom customer written claims in unstructured plain text. We systematically\ndescribe the types of safety issues that concern Zalando customers. We\ndemonstrate how we map this core business problem into a supervised text\nclassification problem with highly imbalanced, noisy, multilingual data in a\nAI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven\nevaluation. Finally, we present detailed ablation studies to show a\ncomprehensive comparison between different classification techniques. We\nconclude the work with how this NLP model was deployed.",
    "descriptor": "",
    "authors": [
      "Kishaloy Halder",
      "Josip Krapac",
      "Dmitry Goryunov",
      "Anthony Brew",
      "Matti Lyra",
      "Alsida Dizdari",
      "William Gillett",
      "Adrien Renahy",
      "Sinan Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14363"
  },
  {
    "id": "arXiv:2210.14367",
    "title": "The Monitor Model and its Misconceptions: A Clarification",
    "abstract": "Horizontal (automatic) and vertical (control) processes have long been\nreported in human translation production (e.g., Konig 1987, Lorscher 1991,\nJaaskelainen 1996, de Groot 1997, Tirkkonen-Condit 2005, Macizo and Bajo 2006).\nThe Monitor Model (Schaeffer and Carl 2013, 2015) integrates horizontal and\nvertical processes, assuming priming mechanisms underlie horizontal/automatic\nprocesses, while vertical/monitoring processes implement consciously accessible\ncontrol mechanisms. Carl (2021a) argues that priming processes in translation\nare part of perception-action loops, interpretable in an embodied/enactivist\nframework. Carl (2022) develops a post-humanist view on translator-technology\ninteraction facilitated by priming mechanisms which enable representationally\nunmediated translator-environment coupling. I substantiate these claims,\narguing that translation priming results in basic, non-representational\ncontent. I update the Monitor Model with additional evidence and address an\naccumulation of misconceptions.",
    "descriptor": "",
    "authors": [
      "Michael Carl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14367"
  },
  {
    "id": "arXiv:2210.14369",
    "title": "Adaptive Experimental Design and Counterfactual Inference",
    "abstract": "Adaptive experimental design methods are increasingly being used in industry\nas a tool to boost testing throughput or reduce experimentation cost relative\nto traditional A/B/N testing methods. This paper shares lessons learned\nregarding the challenges and pitfalls of naively using adaptive experimentation\nsystems in industrial settings where non-stationarity is prevalent, while also\nproviding perspectives on the proper objectives and system specifications in\nthese settings. We developed an adaptive experimental design framework for\ncounterfactual inference based on these experiences, and tested it in a\ncommercial environment.",
    "descriptor": "\nComments: In Workshops of the Conference on Recommender Systems (RecSys), 2022\n",
    "authors": [
      "Tanner Fiez",
      "Sergio Gamez",
      "Arick Chen",
      "Houssam Nassif",
      "Lalit Jain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.14369"
  },
  {
    "id": "arXiv:2210.14373",
    "title": "Shared Autonomous Vehicle Mobility for a Transportation Underserved City",
    "abstract": "This paper proposes the use of an on-demand, ride hailed and ride-Shared\nAutonomous Vehicle (SAV) service as a feasible solution to serve the mobility\nneeds of a small city where fixed route, circulator type public transportation\nmay be too expensive to operate. The presented work builds upon our earlier\nwork that modeled the city of Marysville, Ohio as an example of such a city,\nwith realistic traffic behavior, and trip requests. A simple SAV dispatcher is\nimplemented to model the behavior of the proposed on-demand mobility service.\nThe goal of the service is to optimally distribute SAVs along the network to\nallocate passengers and shared rides. The pickup and drop-off locations are\nstrategically placed along the network to provide mobility from affordable\nhousing, which are also transit deserts, to locations corresponding to jobs and\nother opportunities. The study is carried out by varying the behaviors of the\nSAV driving system from cautious to aggressive along with the size of the SAV\nfleet and analyzing their corresponding performance. It is found that the size\nof the network and behavior of AV driving system behavior results in an optimal\nnumber of SAVs after which increasing the number of SAVs does not improve\noverall mobility. For the Marysville network, which is a 9 mile by 8 mile\nnetwork, this happens at the mark of a fleet of 8 deployed SAVs. The results\nshow that the introduction of the proposed SAV service with a simple optimal\nshared scheme can provide access to services and jobs to hundreds of people in\na small sized city.",
    "descriptor": "",
    "authors": [
      "Karina Meneses-Cime",
      "Bilin Aksun-Guvenc",
      "Levent Guvenc"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14373"
  },
  {
    "id": "arXiv:2210.14376",
    "title": "Robustness of Locally Differentially Private Graph Analysis Against  Poisoning",
    "abstract": "Locally differentially private (LDP) graph analysis allows private analysis\non a graph that is distributed across multiple users. However, such\ncomputations are vulnerable to data poisoning attacks where an adversary can\nskew the results by submitting malformed data. In this paper, we formally study\nthe impact of poisoning attacks for graph degree estimation protocols under\nLDP. We make two key technical contributions. First, we observe LDP makes a\nprotocol more vulnerable to poisoning -- the impact of poisoning is worse when\nthe adversary can directly poison their (noisy) responses, rather than their\ninput data. Second, we observe that graph data is naturally redundant -- every\nedge is shared between two users. Leveraging this data redundancy, we design\nrobust degree estimation protocols under LDP that can significantly reduce the\nimpact of data poisoning and compute degree estimates with high accuracy. We\nevaluate our proposed robust degree estimation protocols under poisoning\nattacks on real-world datasets to demonstrate their efficacy in practice.",
    "descriptor": "\nComments: 22 pages, 6 figures\n",
    "authors": [
      "Jacob Imola",
      "Amrita Roy Chowdhury",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14376"
  },
  {
    "id": "arXiv:2210.14377",
    "title": "Fusing Modalities by Multiplexed Graph Neural Networks for Outcome  Prediction in Tuberculosis",
    "abstract": "In a complex disease such as tuberculosis, the evidence for the disease and\nits evolution may be present in multiple modalities such as clinical, genomic,\nor imaging data. Effective patient-tailored outcome prediction and therapeutic\nguidance will require fusing evidence from these modalities. Such multimodal\nfusion is difficult since the evidence for the disease may not be uniform\nacross all modalities, not all modality features may be relevant, or not all\nmodalities may be present for all patients. All these nuances make simple\nmethods of early, late, or intermediate fusion of features inadequate for\noutcome prediction. In this paper, we present a novel fusion framework using\nmultiplexed graphs and derive a new graph neural network for learning from such\ngraphs. Specifically, the framework allows modalities to be represented through\ntheir targeted encodings, and models their relationship explicitly via\nmultiplexed graphs derived from salient features in a combined latent space. We\npresent results that show that our proposed method outperforms state-of-the-art\nmethods of fusing modalities for multi-outcome prediction on a large\nTuberculosis (TB) dataset.",
    "descriptor": "\nComments: Accepted into MICCAI 2022\n",
    "authors": [
      "Niharika S. D'Souza",
      "Hongzhi Wang",
      "Andrea Giovannini",
      "Antonio Foncubierta-Rodriguez",
      "Kristen L. Beck",
      "Orest Boyko",
      "Tanveer Syeda-Mahmood"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.14377"
  },
  {
    "id": "arXiv:2210.14378",
    "title": "Bilingual Lexicon Induction for Low-Resource Languages using Graph  Matching via Optimal Transport",
    "abstract": "Bilingual lexicons form a critical component of various natural language\nprocessing applications, including unsupervised and semisupervised machine\ntranslation and crosslingual information retrieval. We improve bilingual\nlexicon induction performance across 40 language pairs with a graph-matching\nmethod based on optimal transport. The method is especially strong with low\namounts of supervision.",
    "descriptor": "\nComments: EMNLP 2022 Camera-Ready\n",
    "authors": [
      "Kelly Marchisio",
      "Ali Saad-Eldin",
      "Kevin Duh",
      "Carey Priebe",
      "Philipp Koehn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14378"
  },
  {
    "id": "arXiv:2210.14379",
    "title": "Deploying a Retrieval based Response Model for Task Oriented Dialogues",
    "abstract": "Task-oriented dialogue systems in industry settings need to have high\nconversational capability, be easily adaptable to changing situations and\nconform to business constraints. This paper describes a 3-step procedure to\ndevelop a conversational model that satisfies these criteria and can\nefficiently scale to rank a large set of response candidates. First, we provide\na simple algorithm to semi-automatically create a high-coverage template set\nfrom historic conversations without any annotation. Second, we propose a neural\narchitecture that encodes the dialogue context and applicable business\nconstraints as profile features for ranking the next turn. Third, we describe a\ntwo-stage learning strategy with self-supervised training, followed by\nsupervised fine-tuning on limited data collected through a human-in-the-loop\nplatform. Finally, we describe offline experiments and present results of\ndeploying our model with human-in-the-loop to converse with live customers\nonline.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Lahari Poddar",
      "Gy\u00f6rgy Szarvas",
      "Cheng Wang",
      "Jorge Balazs",
      "Pavel Danchenko",
      "Patrick Ernst"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14379"
  },
  {
    "id": "arXiv:2210.14380",
    "title": "Progressive Sentiment Analysis for Code-Switched Text Data",
    "abstract": "Multilingual transformer language models have recently attracted much\nattention from researchers and are used in cross-lingual transfer learning for\nmany NLP tasks such as text classification and named entity recognition.\nHowever, similar methods for transfer learning from monolingual text to\ncode-switched text have not been extensively explored mainly due to the\nfollowing challenges: (1) Code-switched corpus, unlike monolingual corpus,\nconsists of more than one language and existing methods can't be applied\nefficiently, (2) Code-switched corpus is usually made of resource-rich and\nlow-resource languages and upon using multilingual pre-trained language models,\nthe final model might bias towards resource-rich language. In this paper, we\nfocus on code-switched sentiment analysis where we have a labelled\nresource-rich language dataset and unlabelled code-switched data. We propose a\nframework that takes the distinction between resource-rich and low-resource\nlanguage into account. Instead of training on the entire code-switched corpus\nat once, we create buckets based on the fraction of words in the resource-rich\nlanguage and progressively train from resource-rich language dominated samples\nto low-resource language dominated samples. Extensive experiments across\nmultiple language pairs demonstrate that progressive training helps\nlow-resource language dominated samples.",
    "descriptor": "\nComments: To appear in Findings of EMNLP 2022\n",
    "authors": [
      "Sudhanshu Ranjan",
      "Dheeraj Mekala",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14380"
  },
  {
    "id": "arXiv:2210.14383",
    "title": "CLIP-FLow: Contrastive Learning by {s}emi-supervised Iterative Pseudo  {l}abeling for Optical Flow Estimation",
    "abstract": "Synthetic datasets are often used to pretrain end-to-end optical flow\nnetworks, due to the lack of a large amount of labeled, real-scene data. But\nmajor drops in accuracy occur when moving from synthetic to real scenes. How do\nwe better transfer the knowledge learned from synthetic to real domains? To\nthis end, we propose CLIP-FLow, a semi-supervised iterative pseudo-labeling\nframework to transfer the pretraining knowledge to the target real domain. We\nleverage large-scale, unlabeled real data to facilitate transfer learning with\nthe supervision of iteratively updated pseudo-ground truth labels, bridging the\ndomain gap between the synthetic and the real. In addition, we propose a\ncontrastive flow loss on reference features and the warped features by pseudo\nground truth flows, to further boost the accurate matching and dampen the\nmismatching due to motion, occlusion, or noisy pseudo labels. We adopt RAFT as\nthe backbone and obtain an F1-all error of 4.11\\%, i.e. a 19\\% error reduction\nfrom RAFT (5.10\\%) and ranking 2$^{nd}$ place at submission on KITTI 2015\nbenchmark. Our framework can also be extended to other models, e.g. CRAFT,\nreducing the F1-all error from 4.79\\% to 4.66\\% on KITTI 2015 benchmark.",
    "descriptor": "",
    "authors": [
      "Zhiqi Zhang",
      "Pan Ji",
      "Nitin Bansal",
      "Changjiang Cai",
      "Qingan Yan",
      "Xiangyu Xu",
      "Yi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14383"
  },
  {
    "id": "arXiv:2210.14386",
    "title": "Moment Estimation for Nonparametric Mixture Models Through Implicit  Tensor Decomposition",
    "abstract": "We present an alternating least squares type numerical optimization scheme to\nestimate conditionally-independent mixture models in $\\mathbb{R}^n$, with\nminimal additional distributional assumptions. Following the method of moments,\nwe tackle a coupled system of low-rank tensor decomposition problems. The steep\ncosts associated with high-dimensional tensors are avoided, through the\ndevelopment of specialized tensor-free operations. Numerical experiments\nillustrate the performance of the algorithm and its applicability to various\nmodels and applications. In many cases the results exhibit improved reliability\nover the expectation-maximization algorithm, with similar time and storage\ncosts. We also provide some supporting theory, establishing identifiability and\nlocal linear convergence.",
    "descriptor": "\nComments: 35 pages, 5 figures, 5 tables\n",
    "authors": [
      "Yifan Zhang",
      "Joe Kileel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14386"
  },
  {
    "id": "arXiv:2210.14389",
    "title": "Towards standardizing Korean Grammatical Error Correction: Datasets and  Annotation",
    "abstract": "Research on Korean grammatical error correction (GEC) is limited compared to\nother major languages such as English and Chinese. We attribute this\nproblematic circumstance to the lack of a carefully designed evaluation\nbenchmark for Korean. Thus, in this work, we first collect three datasets from\ndifferent sources (Kor-Lang8, Kor-Native, and Kor-Learner) to cover a wide\nrange of error types and annotate them using our newly proposed tool called\nKorean Automatic Grammatical error Annotation System (KAGAS). KAGAS is a\ncarefully designed edit alignment & classification tool that considers the\nnature of Korean on generating an alignment between a source sentence and a\ntarget sentence, and identifies error types on each aligned edit. We also\npresent baseline models fine-tuned over our datasets. We show that the model\ntrained with our datasets significantly outperforms the public statistical GEC\nsystem (Hanspell) on a wider range of error types, demonstrating the diversity\nand usefulness of the datasets.",
    "descriptor": "",
    "authors": [
      "Soyoung Yoon",
      "Sungjoon Park",
      "Gyuwan Kim",
      "Junhee Cho",
      "Kihyo Park",
      "Gyu Tae Kim",
      "Minjoon Seo",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14389"
  },
  {
    "id": "arXiv:2210.14391",
    "title": "Can Transformer Attention Spread Give Insights Into Uncertainty of  Detected and Tracked Objects?",
    "abstract": "Transformers have recently been utilized to perform object detection and\ntracking in the context of autonomous driving. One unique characteristic of\nthese models is that attention weights are computed in each forward pass,\ngiving insights into the model's interior, in particular, which part of the\ninput data it deemed interesting for the given task. Such an attention matrix\nwith the input grid is available for each detected (or tracked) object in every\ntransformer decoder layer. In this work, we investigate the distribution of\nthese attention weights: How do they change through the decoder layers and\nthrough the lifetime of a track? Can they be used to infer additional\ninformation about an object, such as a detection uncertainty? Especially in\nunstructured environments, or environments that were not common during\ntraining, a reliable measure of detection uncertainty is crucial to decide\nwhether the system can still be trusted or not.",
    "descriptor": "\nComments: Accepted for publication at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) PNARUDE workshop, Oct 27, 2022, in Kyoto, Japan\n",
    "authors": [
      "Felicia Ruppel",
      "Florian Faion",
      "Claudius Gl\u00e4ser",
      "Klaus Dietmayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14391"
  },
  {
    "id": "arXiv:2210.14392",
    "title": "Zero-Shot Learning of a Conditional Generative Adversarial Network for  Data-Free Network Quantization",
    "abstract": "We propose a novel method for training a conditional generative adversarial\nnetwork (CGAN) without the use of training data, called zero-shot learning of a\nCGAN (ZS-CGAN). Zero-shot learning of a conditional generator only needs a\npre-trained discriminative (classification) model and does not need any\ntraining data. In particular, the conditional generator is trained to produce\nlabeled synthetic samples whose characteristics mimic the original training\ndata by using the statistics stored in the batch normalization layers of the\npre-trained model. We show the usefulness of ZS-CGAN in data-free quantization\nof deep neural networks. We achieved the state-of-the-art data-free network\nquantization of the ResNet and MobileNet classification models trained on the\nImageNet dataset. Data-free quantization using ZS-CGAN showed a minimal loss in\naccuracy compared to that obtained by conventional data-dependent quantization.",
    "descriptor": "\nComments: IEEE ICIP 2021\n",
    "authors": [
      "Yoojin Choi",
      "Mostafa El-Khamy",
      "Jungwon Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.14392"
  },
  {
    "id": "arXiv:2210.14393",
    "title": "Federated Fuzzy Neural Network with Evolutionary Rule Learning",
    "abstract": "Distributed fuzzy neural networks (DFNNs) have attracted increasing attention\nrecently due to their learning abilities in handling data uncertainties in\ndistributed scenarios. However, it is challenging for DFNNs to handle cases in\nwhich the local data are non-independent and identically distributed (non-IID).\nIn this paper, we propose a federated fuzzy neural network (FedFNN) with\nevolutionary rule learning (ERL) to cope with non-IID issues as well as data\nuncertainties. The FedFNN maintains a global set of rules in a server and a\npersonalized subset of these rules for each local client. ERL is inspired by\nthe theory of biological evolution; it encourages rule variations while\nactivating superior rules and deactivating inferior rules for local clients\nwith non-IID data. Specifically, ERL consists of two stages in an iterative\nprocedure: a rule cooperation stage that updates global rules by aggregating\nlocal rules based on their activation statuses and a rule evolution stage that\nevolves the global rules and updates the activation statuses of the local\nrules. This procedure improves both the generalization and personalization of\nthe FedFNN for dealing with non-IID issues and data uncertainties. Extensive\nexperiments conducted on a range of datasets demonstrate the superiority of the\nFedFNN over state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Leijie Zhang",
      "Ye Shi",
      "Yu-Cheng Chang",
      "Chin-Teng Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.14393"
  },
  {
    "id": "arXiv:2210.14395",
    "title": "IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from  Egocentric Videos and Text",
    "abstract": "We present IMU2CLIP, a novel pre-training approach to align Inertial\nMeasurement Unit (IMU) motion sensor recordings with video and text, by\nprojecting them into the joint representation space of Contrastive\nLanguage-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to\ntranslate human motions (as measured by IMU sensors) into their corresponding\ntextual descriptions and videos -- while preserving the transitivity across\nthese modalities.\nWe explore several new IMU-based applications that IMU2CLIP enables, such as\nmotion-based media retrieval and natural language reasoning tasks with motion\ndata. In addition, we show that IMU2CLIP can significantly improve the\ndownstream performance when fine-tuned for each application (e.g. activity\nrecognition), demonstrating the universal usage of IMU2CLIP as a new\npre-trained resource. Our code will be made publicly available.",
    "descriptor": "",
    "authors": [
      "Seungwhan Moon",
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Alireza Dirafzoon",
      "Aparajita Saraf",
      "Amy Bearman",
      "Babak Damavandi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14395"
  },
  {
    "id": "arXiv:2210.14396",
    "title": "FedX: Federated Learning for Compositional Pairwise Risk Optimization",
    "abstract": "In this paper, we tackle a novel federated learning (FL) problem for\noptimizing a family of compositional pairwise risks, to which no existing FL\nalgorithms are applicable. In particular, the objective has the form of\n$\\mathbb E_{\\mathbf z\\sim \\mathcal S_1} f(\\mathbb E_{\\mathbf z'\\sim\\mathcal\nS_2} \\ell(\\mathbf w, \\mathbf z, \\mathbf z'))$, where two sets of data $\\mathcal\nS_1, \\mathcal S_2$ are distributed over multiple machines, $\\ell(\\cdot;\n\\cdot,\\cdot)$ is a pairwise loss that only depends on the prediction outputs of\nthe input data pairs $(\\mathbf z, \\mathbf z')$, and $f(\\cdot)$ is possibly a\nnon-linear non-convex function. This problem has important applications in\nmachine learning, e.g., AUROC maximization with a pairwise loss, and partial\nAUROC maximization with a compositional loss. The challenges for designing an\nFL algorithm lie in the non-decomposability of the objective over multiple\nmachines and the interdependency between different machines. We propose two\nprovable FL algorithms (FedX) for handling linear and nonlinear $f$,\nrespectively. To address the challenges, we decouple the gradient's components\nwith two types, namely active parts and lazy parts, where the active parts\ndepend on local data that are computed with the local model and the lazy parts\ndepend on other machines that are communicated/computed based on historical\nmodels and samples. We develop a novel theoretical analysis to combat the\nlatency of the lazy parts and the interdependency between the local model\nparameters and the involved data for computing local gradient estimators. We\nestablish both iteration and communication complexities and show that using the\nhistorical samples and models for computing the lazy parts do not degrade the\ncomplexities. We conduct empirical studies of FedX for deep AUROC and partial\nAUROC maximization, and demonstrate their performance compared with several\nbaselines.",
    "descriptor": "",
    "authors": [
      "Zhishuai Guo",
      "Rong Jin",
      "Jiebo Luo",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14396"
  },
  {
    "id": "arXiv:2210.14401",
    "title": "A Crank-Nicolson leap-frog scheme for the unsteady incompressible  magnetohydrodynamics equations",
    "abstract": "This paper presents a Crank-Nicolson leap-frog (CNLF) scheme for the unsteady\nincompressible magnetohydrodynamics (MHD) equations. The spatial discretization\nadopts the Galerkin finite element method (FEM), and the temporal\ndiscretization employs the CNLF method for linear terms and the semi-implicit\nmethod for nonlinear terms. The first step uses Stokes style's scheme, the\nsecond step employs the Crank-Nicolson extrapolation scheme, and others apply\nthe CNLF scheme. We testify that the fully discrete scheme is stable and\nconvergent when the time step is less than or equal to a positive constant. The\nsecond order $L^{2}$ error estimates can be derived by a novel negative norm\ntechnique. The numerical results are consistent with our theoretical analysis,\nwhich indicates that the method has an optimal convergence order. Therefore,\nthe scheme is effective for different parameters.",
    "descriptor": "",
    "authors": [
      "Zhiyong Si",
      "Mingyi Wang",
      "Yunxia Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14401"
  },
  {
    "id": "arXiv:2210.14402",
    "title": "Adaptive deep density approximation for fractional Fokker-Planck  equations",
    "abstract": "In this work, we propose adaptive deep learning approaches based on\nnormalizing flows for solving fractional Fokker-Planck equations (FPEs). The\nsolution of a FPE is a probability density function (PDF). Traditional\nmesh-based methods are ineffective because of the unbounded computation domain,\na large number of dimensions and the nonlocal fractional operator. To this end,\nwe represent the solution with an explicit PDF model induced by a flow-based\ndeep generative model, simplified KRnet, which constructs a transport map from\na simple distribution to the target distribution. We consider two methods to\napproximate the fractional Laplacian. One method is the Monte Carlo\napproximation. The other method is to construct an auxiliary model with\nGaussian radial basis functions (GRBFs) to approximate the solution such that\nwe may take advantage of the fact that the fractional Laplacian of a Gaussian\nis known analytically. Based on these two different ways for the approximation\nof the fractional Laplacian, we propose two models, MCNF and GRBFNF, to\napproximate stationary FPEs and MCTNF to approximate time-dependent FPEs. To\nfurther improve the accuracy, we refine the training set and the approximate\nsolution alternately. A variety of numerical examples is presented to\ndemonstrate the effectiveness of our adaptive deep density approaches.",
    "descriptor": "\nComments: 25 pages, 22 figures\n",
    "authors": [
      "Li Zeng",
      "Xiaoliang Wan",
      "Tao Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14402"
  },
  {
    "id": "arXiv:2210.14403",
    "title": "Stealthy Measurement-Aided Pole-Dynamics Attacks with Nominal Models",
    "abstract": "When traditional pole-dynamics attacks (TPDAs) are implemented with nominal\nmodels, model mismatch between exact and nominal models often affects their\nstealthiness, or even makes the stealthiness lost. To solve this problem, our\ncurrent paper presents a novel stealthy measurement-aided pole-dynamics attacks\n(MAPDAs) method with model mismatch. Firstly, the limitations of TPDAs using\nexact models are revealed, where exact models help ensure the stealthiness of\nTPDAs but model mismatch severely influences stealthiness of TPDAs. Secondly,\nto handle the model mismatch, the proposed MAPDAs method is designed using a\nmodel reference adaptive control strategy, which can keep stealthiness.\nMoreover, it is easier to implement as only the measurements are needed in\ncomparison with the existing methods requiring both the measurements and\ncontrol inputs. Thirdly, we explore the performance of the proposed MAPDAs\nmethod using convergence of multivariate measurements, and MAPDAs with model\nmismatch have the same stealthiness and similar destructiveness as TPDAs.\nSpecifically, MAPDAs with adaptive gains will remain stealthy at an acceptable\ndetection threshold till destructiveness occurs. Finally, experimental results\nfrom a networked inverted pendulum system validate the proposed system.",
    "descriptor": "",
    "authors": [
      "Dajun Du",
      "Changda Zhang",
      "Dakui Wu",
      "Chen Peng",
      "Minrui Fei",
      "Huiyu Zhou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14403"
  },
  {
    "id": "arXiv:2210.14404",
    "title": "Adaptive Test-Time Defense with the Manifold Hypothesis",
    "abstract": "In this work, we formulate a novel framework of adversarial robustness using\nthe manifold hypothesis. Our framework provides sufficient conditions for\ndefending against adversarial examples. We develop a test-time defense method\nwith our formulation and variational inference. The developed approach combines\nmanifold learning with the Bayesian framework to provide adversarial robustness\nwithout the need for adversarial training. We show that our proposed approach\ncan provide adversarial robustness even if attackers are aware of existence of\ntest-time defense. In additions, our approach can also serve as a test-time\ndefense mechanism for variational autoencoders.",
    "descriptor": "",
    "authors": [
      "Zhaoyuan Yang",
      "Zhiwei Xu",
      "Jing Zhang",
      "Richard Hartley",
      "Peter Tu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14404"
  },
  {
    "id": "arXiv:2210.14405",
    "title": "Adversarially Robust Medical Classification via Attentive Convolutional  Neural Networks",
    "abstract": "Convolutional neural network-based medical image classifiers have been shown\nto be especially susceptible to adversarial examples. Such instabilities are\nlikely to be unacceptable in the future of automated diagnoses. Though\nstatistical adversarial example detection methods have proven to be effective\ndefense mechanisms, additional research is necessary that investigates the\nfundamental vulnerabilities of deep-learning-based systems and how best to\nbuild models that jointly maximize traditional and robust accuracy. This paper\npresents the inclusion of attention mechanisms in CNN-based medical image\nclassifiers as a reliable and effective strategy for increasing robust accuracy\nwithout sacrifice. This method is able to increase robust accuracy by up to 16%\nin typical adversarial scenarios and up to 2700% in extreme cases.",
    "descriptor": "",
    "authors": [
      "Isaac Wasserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.14405"
  },
  {
    "id": "arXiv:2210.14408",
    "title": "An Attention-based Long Short-Term Memory Framework for Detection of  Bitcoin Scams",
    "abstract": "Bitcoin is the most common cryptocurrency involved in cyber scams.\nCybercriminals often utilize pseudonymity and privacy protection mechanism\nassociated with Bitcoin transactions to make their scams virtually untraceable.\nThe Ponzi scheme has attracted particularly significant attention among Bitcoin\nfraudulent activities. This paper considers a multi-class classification\nproblem to determine whether a transaction is involved in Ponzi schemes or\nother cyber scams, or is a non-scam transaction. We design a specifically\ndesigned crawler to collect data and propose a novel Attention-based Long\nShort-Term Memory (A-LSTM) method for the classification problem. The\nexperimental results show that the proposed model has better efficiency and\naccuracy than existing approaches, including Random Forest, Extra Trees,\nGradient Boosting, and classical LSTM. With correctly identified scam features,\nour proposed A-LSTM achieves an F1-score over 82% for the original data and\noutperforms the existing approaches.",
    "descriptor": "",
    "authors": [
      "Puyang Zhao",
      "Wei Tian",
      "Lefu Xiao",
      "Xinhui Liu",
      "Jingjin Wu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14408"
  },
  {
    "id": "arXiv:2210.14409",
    "title": "Modeling the Graphotactics of Low-Resource Languages Using Sequential  GANs",
    "abstract": "Generative Adversarial Networks (GANs) have been shown to aid in the creation\nof artificial data in situations where large amounts of real data are difficult\nto come by. This issue is especially salient in the computational linguistics\nspace, where researchers are often tasked with modeling the complex morphologic\nand grammatical processes of low-resource languages. This paper will discuss\nthe implementation and testing of a GAN that attempts to model and reproduce\nthe graphotactics of a language using only 100 example strings. These\nartificial, yet graphotactically compliant, strings are meant to aid in\nmodeling the morphological inflection of low-resource languages.",
    "descriptor": "",
    "authors": [
      "Isaac Wasserman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14409"
  },
  {
    "id": "arXiv:2210.14410",
    "title": "Improving Adversarial Robustness via Joint Classification and Multiple  Explicit Detection Classes",
    "abstract": "This work concerns the development of deep networks that are certifiably\nrobust to adversarial attacks. Joint robust classification-detection was\nrecently introduced as a certified defense mechanism, where adversarial\nexamples are either correctly classified or assigned to the \"abstain\" class. In\nthis work, we show that such a provable framework can benefit by extension to\nnetworks with multiple explicit abstain classes, where the adversarial examples\nare adaptively assigned to those. We show that naively adding multiple abstain\nclasses can lead to \"model degeneracy\", then we propose a regularization\napproach and a training method to counter this degeneracy by promoting full use\nof the multiple abstain classes. Our experiments demonstrate that the proposed\napproach consistently achieves favorable standard vs. robust verified accuracy\ntradeoffs, outperforming state-of-the-art algorithms for various choices of\nnumber of abstain classes.",
    "descriptor": "\nComments: 21 pages, 6 figures\n",
    "authors": [
      "Sina Baharlouei",
      "Fatemeh Sheikholeslami",
      "Meisam Razaviyayn",
      "Zico Kolter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14410"
  },
  {
    "id": "arXiv:2210.14413",
    "title": "InterSim: Interactive Traffic Simulation via Explicit Relation Modeling",
    "abstract": "Interactive traffic simulation is crucial to autonomous driving systems by\nenabling testing for planners in a more scalable and safe way compared to\nreal-world road testing. Existing approaches learn an agent model from\nlarge-scale driving data to simulate realistic traffic scenarios, yet it\nremains an open question to produce consistent and diverse multi-agent\ninteractive behaviors in crowded scenes. In this work, we present InterSim, an\ninteractive traffic simulator for testing autonomous driving planners. Given a\ntest plan trajectory from the ego agent, InterSim reasons about the interaction\nrelations between the agents in the scene and generates realistic trajectories\nfor each environment agent that are consistent with the relations. We train and\nvalidate our model on a large-scale interactive driving dataset. Experiment\nresults show that InterSim achieves better simulation realism and reactivity in\ntwo simulation tasks compared to a state-of-the-art learning-based traffic\nsimulator.",
    "descriptor": "\nComments: Accepted at IROS 2022. Author version with 8 pages, 4 figures, and 2 tables. Code and demo available at paper website: this https URL\n",
    "authors": [
      "Qiao Sun",
      "Xin Huang",
      "Brian C. Williams",
      "Hang Zhao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14413"
  },
  {
    "id": "arXiv:2210.14417",
    "title": "From Obstacle Avoidance To Motion Learning Using Local Rotation of  Dynamical Systems",
    "abstract": "In robotics motion is often described from an external perspective, i.e., we\ngive information on the obstacle motion in a mathematical manner with respect\nto a specific (often inertial) reference frame. In the current work, we propose\nto describe the robotic motion with respect to the robot itself. Similar to how\nwe give instructions to each other (go straight, and then after multiple meters\nmove left, and then a sharp turn right.), we give the instructions to a robot\nas a relative rotation. We first introduce an obstacle avoidance framework that\nallows avoiding star-shaped obstacles while trying to stay close to an initial\n(linear or nonlinear) dynamical system. The framework of the local rotation is\nextended to motion learning. Automated clustering defines regions of local\nstability, for which the precise dynamics are individually learned. The\nframework has been applied to the LASA-handwriting dataset and shows promising\nresults.",
    "descriptor": "",
    "authors": [
      "Lukas Huber",
      "Jean-Jacques Slotine",
      "Aude Billard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14417"
  },
  {
    "id": "arXiv:2210.14419",
    "title": "Discourse-Aware Emotion Cause Extraction in Conversations",
    "abstract": "Emotion Cause Extraction in Conversations (ECEC) aims to extract the\nutterances which contain the emotional cause in conversations. Most prior\nresearch focuses on modelling conversational contexts with sequential encoding,\nignoring the informative interactions between utterances and\nconversational-specific features for ECEC. In this paper, we investigate the\nimportance of discourse structures in handling utterance interactions and\nconversationspecific features for ECEC. To this end, we propose a\ndiscourse-aware model (DAM) for this task. Concretely, we jointly model ECEC\nwith discourse parsing using a multi-task learning (MTL) framework and\nexplicitly encode discourse structures via gated graph neural network (gated\nGNN), integrating rich utterance interaction information to our model. In\naddition, we use gated GNN to further enhance our ECEC model with\nconversation-specific features. Results on the benchmark corpus show that DAM\noutperform the state-of-theart (SOTA) systems in the literature. This suggests\nthat the discourse structure may contain a potential link between emotional\nutterances and their corresponding cause expressions. It also verifies the\neffectiveness of conversationalspecific features. The codes of this paper will\nbe available on GitHub.",
    "descriptor": "",
    "authors": [
      "Dexin Kong",
      "Nan Yu",
      "Yun Yuan",
      "Guohong Fu",
      "Chen Gong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14419"
  },
  {
    "id": "arXiv:2210.14424",
    "title": "Geographic Citation Gaps in NLP Research",
    "abstract": "In a fair world, people have equitable opportunities to education, to conduct\nscientific research, to publish, and to get credit for their work, regardless\nof where they live. However, it is common knowledge among researchers that a\nvast number of papers accepted at top NLP venues come from a handful of western\ncountries and (lately) China; whereas, very few papers from Africa and South\nAmerica get published. Similar disparities are also believed to exist for paper\ncitation counts. In the spirit of \"what we do not measure, we cannot improve\",\nthis work asks a series of questions on the relationship between geographical\nlocation and publication success (acceptance in top NLP venues and citation\nimpact). We first created a dataset of 70,000 papers from the ACL Anthology,\nextracted their meta-information, and generated their citation network. We then\nshow that not only are there substantial geographical disparities in paper\nacceptance and citation but also that these disparities persist even when\ncontrolling for a number of variables such as venue of publication and\nsub-field of NLP. Further, despite some steps taken by the NLP community to\nimprove geographical diversity, we show that the disparity in publication\nmetrics across locations is still on an increasing trend since the early 2000s.\nWe release our code and dataset here:\nhttps://github.com/iamjanvijay/acl-cite-net",
    "descriptor": "\nComments: EMNLP 2022 Main Conference\n",
    "authors": [
      "Mukund Rungta",
      "Janvijay Singh",
      "Saif M. Mohammad",
      "Diyi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14424"
  },
  {
    "id": "arXiv:2210.14427",
    "title": "ReSel: N-ary Relation Extraction from Scientific Text and Tables by  Learning to Retrieve and Select",
    "abstract": "We study the problem of extracting N-ary relation tuples from scientific\narticles. This task is challenging because the target knowledge tuples can\nreside in multiple parts and modalities of the document. Our proposed method\nReSel decomposes this task into a two-stage procedure that first retrieves the\nmost relevant paragraph/table and then selects the target entity from the\nretrieved component. For the high-level retrieval stage, ReSel designs a simple\nand effective feature set, which captures multi-level lexical and semantic\nsimilarities between the query and components. For the low-level selection\nstage, ReSel designs a cross-modal entity correlation graph along with a\nmulti-view architecture, which models both semantic and document-structural\nrelations between entities. Our experiments on three scientific information\nextraction datasets show that ReSel outperforms state-of-the-art baselines\nsignificantly.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Yuchen Zhuang",
      "Yinghao Li",
      "Jerry Junyang Cheung",
      "Yue Yu",
      "Yingjun Mou",
      "Xiang Chen",
      "Le Song",
      "Chao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14427"
  },
  {
    "id": "arXiv:2210.14428",
    "title": "D-Shape: Demonstration-Shaped Reinforcement Learning via Goal  Conditioning",
    "abstract": "While combining imitation learning (IL) and reinforcement learning (RL) is a\npromising way to address poor sample efficiency in autonomous behavior\nacquisition, methods that do so typically assume that the requisite behavior\ndemonstrations are provided by an expert that behaves optimally with respect to\na task reward. If, however, suboptimal demonstrations are provided, a\nfundamental challenge appears in that the demonstration-matching objective of\nIL conflicts with the return-maximization objective of RL. This paper\nintroduces D-Shape, a new method for combining IL and RL that uses ideas from\nreward shaping and goal-conditioned RL to resolve the above conflict. D-Shape\nallows learning from suboptimal demonstrations while retaining the ability to\nfind the optimal policy with respect to the task reward. We experimentally\nvalidate D-Shape in sparse-reward gridworld domains, showing that it both\nimproves over RL in terms of sample efficiency and converges consistently to\nthe optimal policy in the presence of suboptimal demonstrations.",
    "descriptor": "",
    "authors": [
      "Caroline Wang",
      "Garrett Warnell",
      "Peter Stone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14428"
  },
  {
    "id": "arXiv:2210.14431",
    "title": "Residual Learning of Neural Text Generation with $n$-gram Language Model",
    "abstract": "$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.",
    "descriptor": "\nComments: Accepted to findings of EMNLP 2022\n",
    "authors": [
      "Huayang Li",
      "Deng Cai",
      "Jin Xu",
      "Taro Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14431"
  },
  {
    "id": "arXiv:2210.14434",
    "title": "A formal process of hierarchical functional requirements development for  Set-Based Design",
    "abstract": "The design of complex systems is typically uncertain and ambiguous at early\nstages. Set-Based Design is a promising approach to complex systems design as\nit supports alternative exploration and gradual uncertainty reduction. When\ndesigning a complex system, functional requirements decomposition is a common\nand effective approach to progress the design incrementally. However, the\ncurrent literature on Set-Based Design lacks formal guidance in functional\nrequirements decomposition. To bridge the gap, we propose a formal process to\nhierarchically decompose the functional requirements for Set-Based Design. A\nfour-step formal process is proposed to systematically define, reason, and\nnarrow the sets, and eventually decompose the functional requirement into the\nsub-requirements. Such a process can be used by the individual suppliers\nworking in parallel at multiple levels of abstraction and guarantee that the\nresulting system will eventually satisfy the top-level functional requirements.\nAn example of designing a cruise control system is applied to demonstrate the\nfeasibility of the proposed process.",
    "descriptor": "",
    "authors": [
      "Minghui Sun",
      "Zhaoyang Chen",
      "Georgios Bakirtzis",
      "Hassan Jafarzadeh",
      "Cody Fleming"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14434"
  },
  {
    "id": "arXiv:2210.14436",
    "title": "Hybrid Inlining: A Compositional and Context Sensitive Static Analysis  Framework",
    "abstract": "Context sensitivity is essential for achieving the precision in\ninter-procedural static analysis. To be (fully) context sensitive, top-down\nanalysis needs to fully inline all statements of the callees at each callsite,\nleading to statement explosion. Compositional analysis, which inlines summaries\nof the callees, scales up but often loses precision, as it is not strictly\ncontext sensitive. We propose a compositional and strictly context sensitive\nframework for static analysis. This framework is based on one key observation:\na compositional static analysis often loses precision only on some critical\nstatements that need to be analyzed context sensitively. Our approach hybridly\ninlines the critical statements and the summaries of non-critical statements of\neach callee, thus avoiding the re-analysis of non-critical ones. In addition,\nour analysis lazily summarizes the critical statements, by stopping propagating\nthe critical statements once the calling context accumulated is adequate.\nHybrid Inlining can be as precise as context sensitive top-down analysis. We\nhave designed and implemented a pointer analysis based on this framework. It\ncan analyze large Java programs from the Dacapo benchmark suite and industry in\nminutes. In our evaluation, compared to context insensitive analysis, Hybrid\nInlining just brings 65% and 1% additional time overhead on Dacapo and\nindustrial applications respectively.",
    "descriptor": "",
    "authors": [
      "Jiangchao Liu",
      "Jierui Liu",
      "Peng Di",
      "Diyu Wu",
      "Hengjie Zheng",
      "Alex Liu",
      "Jingling Xue"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.14436"
  },
  {
    "id": "arXiv:2210.14441",
    "title": "Towards Automatically Extracting UML Class Diagrams from Natural  Language Specifications",
    "abstract": "In model-driven engineering (MDE), UML class diagrams serve as a way to plan\nand communicate between developers. However, it is complex and\nresource-consuming. We propose an automated approach for the extraction of UML\nclass diagrams from natural language software specifications. To develop our\napproach, we create a dataset of UML class diagrams and their English\nspecifications with the help of volunteers. Our approach is a pipeline of steps\nconsisting of the segmentation of the input into sentences, the classification\nof the sentences, the generation of UML class diagram fragments from sentences,\nand the composition of these fragments into one UML class diagram. We develop a\nquantitative testing framework specific to UML class diagram extraction. Our\napproach yields low precision and recall but serves as a benchmark for future\nresearch.",
    "descriptor": "\nComments: 8 pages, 7 tables, 9 figures, 2 algorithms, to be published in MODELS '22 Companion\n",
    "authors": [
      "S. Yang",
      "H. Sahraoui"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.14441"
  },
  {
    "id": "arXiv:2210.14446",
    "title": "Smart Speech Segmentation using Acousto-Linguistic Features with  look-ahead",
    "abstract": "Segmentation for continuous Automatic Speech Recognition (ASR) has\ntraditionally used silence timeouts or voice activity detectors (VADs), which\nare both limited to acoustic features. This segmentation is often overly\naggressive, given that people naturally pause to think as they speak.\nConsequently, segmentation happens mid-sentence, hindering both punctuation and\ndownstream tasks like machine translation for which high-quality segmentation\nis critical. Model-based segmentation methods that leverage acoustic features\nare powerful, but without an understanding of the language itself, these\napproaches are limited. We present a hybrid approach that leverages both\nacoustic and language information to improve segmentation. Furthermore, we show\nthat including one word as a look-ahead boosts segmentation quality. On\naverage, our models improve segmentation-F0.5 score by 9.8% over baseline. We\nshow that this approach works for multiple languages. For the downstream task\nof machine translation, it improves the translation BLEU score by an average of\n1.05 points.",
    "descriptor": "",
    "authors": [
      "Piyush Behre",
      "Naveen Parihar",
      "Sharman Tan",
      "Amy Shah",
      "Eva Sharma",
      "Geoffrey Liu",
      "Shuangyu Chang",
      "Hosam Khalil",
      "Chris Basoglu",
      "Sayan Pathak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14446"
  },
  {
    "id": "arXiv:2210.14448",
    "title": "The NPU-ASLP System for The ISCSLP 2022 Magichub Code-Swiching ASR  Challenge",
    "abstract": "This paper describes our NPU-ASLP system submitted to the ISCSLP 2022\nMagichub Code-Switching ASR Challenge. In this challenge, we first explore\nseveral popular end-to-end ASR architectures and training strategies, including\nbi-encoder, language-aware encoder (LAE) and mixture of experts (MoE). To\nimprove our system's language modeling ability, we further attempt the internal\nlanguage model as well as the long context language model. Given the limited\ntraining data in the challenge, we further investigate the effects of data\naugmentation, including speed perturbation, pitch shifting, speech codec,\nSpecAugment and synthetic data from text-to-speech (TTS). Finally, we explore\nROVER-based score fusion to make full use of complementary hypotheses from\ndifferent models. Our submitted system achieves 16.87% on mix error rate (MER)\non the test set and comes to the 2nd place in the challenge ranking.",
    "descriptor": "\nComments: Submitted to ISCSLP 2022\n",
    "authors": [
      "Yuhao Liang",
      "Peikun Chen",
      "Fan Yu",
      "Xinfa Zhu",
      "Tianyi Xu",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14448"
  },
  {
    "id": "arXiv:2210.14449",
    "title": "Modeling of dendritic solidification and numerical analysis of the  phase-field approach to model complex morphologies in alloys",
    "abstract": "Dendrites are one of the most widely observed patterns in nature and occur\nacross a wide spectrum of physical phenomena. In solidification and growth\npatterns in metals and crystals, the multi-level branching structures of\ndendrites pose a modeling challenge, and a full resolution of these structures\nis computationally demanding. In the literature, theoretical models of\ndendritic formation and evolution, essentially as extensions of the classical\nmoving boundary Stefan problem exist. Much of this understanding is from the\nanalysis of dendrites occurring during the solidification of metallic alloys.\nMotivated by the problem of modeling microstructure evolution from liquid melts\nof pure metals and alloys during MAM, we developed a comprehensive numerical\nframework for modeling a large variety of dendritic structures that are\nrelevant to metal solidification. In this work, we present a numerical\nframework encompassing the modeling of Stefan problem formulations relevant to\ndendritic evolution using a phase-field approach and a finite element method\nimplementation. Using this framework, we model numerous complex dendritic\nmorphologies that are physically relevant to the solidification of pure melts\nand binary alloys. The distinguishing aspects of this work are - a unified\ntreatment of both pure metals and alloys; novel numerical error estimates of\ndendritic tip velocity; and the convergence of error for the primal fields of\ntemperature and the order parameter with respect to numerical discretization.\nTo the best of our knowledge, this is a first-of-its-kind study of numerical\nconvergence of the phase-field equations of dendritic growth in a finite\nelement method setting. Further, we modeled various types of physically\nrelevant dendritic solidification patterns in 2D and 3D computational domains.",
    "descriptor": "\nComments: under review\n",
    "authors": [
      "Kunal Bhagat",
      "Shiva Rudraraju"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14449"
  },
  {
    "id": "arXiv:2210.14451",
    "title": "Discovering Design Concepts for CAD Sketches",
    "abstract": "Sketch design concepts are recurring patterns found in parametric CAD\nsketches. Though rarely explicitly formalized by the CAD designers, these\nconcepts are implicitly used in design for modularity and regularity. In this\npaper, we propose a learning based approach that discovers the modular concepts\nby induction over raw sketches. We propose the dual implicit-explicit\nrepresentation of concept structures that allows implicit detection and\nexplicit generation, and the separation of structure generation and parameter\ninstantiation for parameterized concept generation, to learn modular concepts\nby end-to-end training. We demonstrate the design concept learning on a large\nscale CAD sketch dataset and show its applications for design intent\ninterpretation and auto-completion.",
    "descriptor": "",
    "authors": [
      "Yuezhi Yang",
      "Hao Pan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14451"
  },
  {
    "id": "arXiv:2210.14452",
    "title": "Short Paper: Static and Microarchitectural ML-Based Approaches For  Detecting Spectre Vulnerabilities and Attacks",
    "abstract": "Spectre intrusions exploit speculative execution design vulnerabilities in\nmodern processors. The attacks violate the principles of isolation in programs\nto gain unauthorized private user information. Current state-of-the-art\ndetection techniques utilize micro-architectural features or vulnerable\nspeculative code to detect these threats. However, these techniques are\ninsufficient as Spectre attacks have proven to be more stealthy with recently\ndiscovered variants that bypass current mitigation mechanisms. Side-channels\ngenerate distinct patterns in processor cache, and sensitive information\nleakage is dependent on source code vulnerable to Spectre attacks, where an\nadversary uses these vulnerabilities, such as branch prediction, which causes a\ndata breach. Previous studies predominantly approach the detection of Spectre\nattacks using the microarchitectural analysis, a reactive approach. Hence, in\nthis paper, we present the first comprehensive evaluation of static and\nmicroarchitectural analysis-assisted machine learning approaches to detect\nSpectre vulnerable code snippets (preventive) and Spectre attacks (reactive).\nWe evaluate the performance trade-offs in employing classifiers for detecting\nSpectre vulnerabilities and attacks.",
    "descriptor": "\nComments: 5 pages, 2 figures. Accepted to the Hardware and Architectural Support for Security and Privacy (HASP'22), in conjunction with the 55th IEEE/ACM International Symposium on Microarchitecture (MICRO'22)\n",
    "authors": [
      "Chidera Biringa",
      "Gaspard Baye",
      "G\u00f6khan Kul"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14452"
  },
  {
    "id": "arXiv:2210.14453",
    "title": "Scale-free linear protocol design for global regulated state  synchronization of discrete-time double-integrator multi-agent systems  subject to actuator saturation",
    "abstract": "This paper studies global regulated state synchronization of discrete-time\ndouble-integrator multi-agent systems subject to actuator saturation by\nutilizing localized information exchange. We propose a scale-free linear\nprotocol that achieves global regulated state synchronization for any network\nwith arbitrary number of agents and arbitrarily directed communication graph\nthat has a path between each agent and exosystem which generates the reference\ntrajectory.",
    "descriptor": "\nComments: This paper was submitted to 2023 Chinese Control and Decision Conference (CCDC), Yichang, China. arXiv admin note: substantial text overlap with arXiv:2204.02129\n",
    "authors": [
      "Zhenwei Liu",
      "Ali Saberi",
      "Anton A. Stoorvogel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14453"
  },
  {
    "id": "arXiv:2210.14456",
    "title": "Question-Interlocutor Scope Realized Graph Modeling over Key Utterances  for Dialogue Reading Comprehension",
    "abstract": "In this work, we focus on dialogue reading comprehension (DRC), a task\nextracting answer spans for questions from dialogues. Dialogue context modeling\nin DRC is tricky due to complex speaker information and noisy dialogue context.\nTo solve the two problems, previous research proposes two self-supervised tasks\nrespectively: guessing who a randomly masked speaker is according to the\ndialogue and predicting which utterance in the dialogue contains the answer.\nAlthough these tasks are effective, there are still urging problems: (1)\nrandomly masking speakers regardless of the question cannot map the speaker\nmentioned in the question to the corresponding speaker in the dialogue, and\nignores the speaker-centric nature of utterances. This leads to wrong answer\nextraction from utterances in unrelated interlocutors' scopes; (2) the single\nutterance prediction, preferring utterances similar to the question, is limited\nin finding answer-contained utterances not similar to the question. To\nalleviate these problems, we first propose a new key utterances extracting\nmethod. It performs prediction on the unit formed by several contiguous\nutterances, which can realize more answer-contained utterances. Based on\nutterances in the extracted units, we then propose Question-Interlocutor Scope\nRealized Graph (QuISG) modeling. As a graph constructed on the text of\nutterances, QuISG additionally involves the question and question-mentioning\nspeaker names as nodes. To realize interlocutor scopes, speakers in the\ndialogue are connected with the words in their corresponding utterances.\nExperiments on the benchmarks show that our method can achieve better and\ncompetitive results against previous works.",
    "descriptor": "",
    "authors": [
      "Jiangnan Li",
      "Mo Yu",
      "Fandong Meng",
      "Zheng Lin",
      "Peng Fu",
      "Weiping Wang",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14456"
  },
  {
    "id": "arXiv:2210.14457",
    "title": "Towards A Robust Deepfake Detector:Common Artifact Deepfake Detection  Model",
    "abstract": "Existing deepfake detection methods perform poorly on face forgeries\ngenerated by unseen face manipulation algorithms. The generalization ability of\nprevious methods is mainly improved by modeling hand-crafted artifact features.\nSuch properties, on the other hand, impede their further improvement. In this\npaper, we propose a novel deepfake detection method named Common Artifact\nDeepfake Detection Model, which aims to learn common artifact features in\ndifferent face manipulation algorithms. To this end, we find that the main\nobstacle to learning common artifact features is that models are easily misled\nby the identity representation feature. We call this phenomenon Implicit\nIdentity Leakage (IIL). Extensive experimental results demonstrate that, by\nlearning the binary classifiers with the guidance of the Artifact Detection\nModule, our method effectively reduces the influence of IIL and outperforms the\nstate-of-the-art by a large margin, proving that hand-crafted artifact feature\ndetectors are not indispensable when tackling deepfake problems.",
    "descriptor": "\nComments: 27 pages; 12 figures\n",
    "authors": [
      "Shichao Dong",
      "Jin Wang",
      "Renhe Ji",
      "Jiajun Liang",
      "Haoqiang Fan",
      "Zheng Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14457"
  },
  {
    "id": "arXiv:2210.14458",
    "title": "Joint Waveform and Passive Beamformer Design in Multi-IRS Aided Radar",
    "abstract": "Intelligent reflecting surface (IRS) technology has recently attracted a\nsignificant interest in non-light-of-sight radar remote sensing. Prior works\nhave largely focused on designing single IRS beamformers for this problem. For\nthe first time in the literature, this paper considers multi-IRS-aided\nmultiple-input multiple-output (MIMO) radar and jointly designs the transmit\nunimodular waveforms and optimal IRS beamformers. To this end, we derive the\nCramer-Rao lower bound (CRLB) of target direction-of-arrival (DoA) as a\nperformance metric. Unimodular transmit sequences are the preferred waveforms\nfrom a hardware perspective. We show that, through suitable transformations,\nthe joint design problem can be reformulated as two unimodular quadratic\nprograms (UQP). To deal with the NP-hard nature of both UQPs, we propose\nunimodular waveform and beamforming design for multi-IRS radar (UBeR) algorithm\nthat takes advantage of the low-cost power method-like iterations. Numerical\nexperiments illustrate that the MIMO waveforms and phase shifts obtained from\nour UBeR algorithm are effective in improving the CRLB of DoA estimation.",
    "descriptor": "",
    "authors": [
      "Zahra Esmaeilbeig",
      "Arian Eamaz",
      "Kumar Vijay Mishra",
      "Mojtaba Soltanalian"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14458"
  },
  {
    "id": "arXiv:2210.14460",
    "title": "PredNAS: A Universal and Sample Efficient Neural Architecture Search  Framework",
    "abstract": "In this paper, we present a general and effective framework for Neural\nArchitecture Search (NAS), named PredNAS. The motivation is that given a\ndifferentiable performance estimation function, we can directly optimize the\narchitecture towards higher performance by simple gradient ascent.\nSpecifically, we adopt a neural predictor as the performance predictor.\nSurprisingly, PredNAS can achieve state-of-the-art performances on NAS\nbenchmarks with only a few training samples (less than 100). To validate the\nuniversality of our method, we also apply our method on large-scale tasks and\ncompare our method with RegNet on ImageNet and YOLOX on MSCOCO. The results\ndemonstrate that our PredNAS can explore novel architectures with competitive\nperformances under specific computational complexity constraints.",
    "descriptor": "\nComments: 11 Pages,4 figures\n",
    "authors": [
      "Liuchun Yuan",
      "Zehao Huang",
      "Naiyan Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14460"
  },
  {
    "id": "arXiv:2210.14461",
    "title": "TPFNet: A Novel Text In-painting Transformer for Text Removal",
    "abstract": "Text erasure from an image is helpful for various tasks such as image editing\nand privacy preservation. In this paper, we present TPFNet, a novel one-stage\n(end-toend) network for text removal from images. Our network has two parts:\nfeature synthesis and image generation. Since noise can be more effectively\nremoved from low-resolution images, part 1 operates on low-resolution images.\nThe output of part 1 is a low-resolution text-free image. Part 2 uses the\nfeatures learned in part 1 to predict a high-resolution text-free image. In\npart 1, we use \"pyramidal vision transformer\" (PVT) as the encoder. Further, we\nuse a novel multi-headed decoder that generates a high-pass filtered image and\na segmentation map, in addition to a text-free image. The segmentation branch\nhelps locate the text precisely, and the high-pass branch helps in learning the\nimage structure. To precisely locate the text, TPFNet employs an adversarial\nloss that is conditional on the segmentation map rather than the input image.\nOn Oxford, SCUT, and SCUT-EnsText datasets, our network outperforms recently\nproposed networks on nearly all the metrics. For example, on SCUT-EnsText\ndataset, TPFNet has a PSNR (higher is better) of 39.0 and text-detection\nprecision (lower is better) of 21.1, compared to the best previous technique,\nwhich has a PSNR of 32.3 and precision of 53.2. The source code can be obtained\nfrom https://github.com/CandleLabAI/TPFNet",
    "descriptor": "\nComments: 10 pages, 5 figures, 5 tables, Neurips Proceedings\n",
    "authors": [
      "Onkar Susladkar",
      "Dhruv Makwana",
      "Gayatri Deshmukh",
      "Sparsh Mittal",
      "Sai Chandra Teja R",
      "Rekha Singhal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.14461"
  },
  {
    "id": "arXiv:2210.14463",
    "title": "Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive  Learning of Transformers and Prompts",
    "abstract": "Inductive knowledge graph completion requires models to comprehend the\nunderlying semantics and logic patterns of relations. With the advance of\npretrained language models, recent research have designed transformers for link\nprediction tasks. However, empirical studies show that linearizing triples\naffects the learning of relational patterns, such as inversion and symmetry. In\nthis paper, we propose Bi-Link, a contrastive learning framework with\nprobabilistic syntax prompts for link predictions. Using grammatical knowledge\nof BERT, we efficiently search for relational prompts according to learnt\nsyntactical patterns that generalize to large knowledge graphs. To better\nexpress symmetric relations, we design a symmetric link prediction model,\nestablishing bidirectional linking between forward prediction and backward\nprediction. This bidirectional linking accommodates flexible self-ensemble\nstrategies at test time. In our experiments, Bi-Link outperforms recent\nbaselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).\nFurthermore, we construct Zeshel-Ind as an in-domain inductive entity linking\nthe environment to evaluate Bi-Link. The experimental results demonstrate that\nour method yields robust representations which can generalize under domain\nshift.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Bohua Peng",
      "Shihao Liang",
      "Mobarakol Islam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14463"
  },
  {
    "id": "arXiv:2210.14465",
    "title": "Eeny, meeny, miny, moe. How to choose data for morphological inflection",
    "abstract": "Data scarcity is a widespread problem in numerous natural language processing\n(NLP) tasks for low-resource languages. Within morphology, the labour-intensive\nwork of tagging/glossing data is a serious bottleneck for both NLP and language\ndocumentation. Active learning (AL) aims to reduce the cost of data annotation\nby selecting data that is most informative for improving the model. In this\npaper, we explore four sampling strategies for the task of morphological\ninflection using a Transformer model: a pair of oracle experiments where data\nis chosen based on whether the model already can or cannot inflect the test\nforms correctly, as well as strategies based on high/low model confidence,\nentropy, as well as random selection. We investigate the robustness of each\nstrategy across 30 typologically diverse languages. We also perform a more\nin-depth case study of Nat\\\"ugu. Our results show a clear benefit to selecting\ndata based on model confidence and entropy. Unsurprisingly, the oracle\nexperiment, where only incorrectly handled forms are chosen for further\ntraining, which is presented as a proxy for linguist/language consultant\nfeedback, shows the most improvement. This is followed closely by choosing\nlow-confidence and high-entropy predictions. We also show that despite the\nconventional wisdom of larger data sets yielding better accuracy, introducing\nmore instances of high-confidence or low-entropy forms, or forms that the model\ncan already inflect correctly, can reduce model performance.",
    "descriptor": "\nComments: EMNLP2022\n",
    "authors": [
      "Saliha Muradoglu",
      "Mans Hulden"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14465"
  },
  {
    "id": "arXiv:2210.14469",
    "title": "Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning",
    "abstract": "Prefix-tuning, or more generally continuous prompt tuning, has become an\nessential paradigm of parameter-efficient transfer learning. Using a large\npre-trained language model (PLM), prefix-tuning can obtain strong performance\nby training only a small portion of parameters. In this paper, we propose to\nunderstand and further develop prefix-tuning through the kernel lens.\nSpecifically, we make an analogy between \\textit{prefixes} and \\textit{inducing\nvariables} in kernel methods and hypothesize that \\textit{prefixes} serving as\n\\textit{inducing variables} would improve their overall mechanism. From the\nkernel estimator perspective, we suggest a new variant of prefix-tuning --\n\\textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning\nwhile leveraging the residual form found in adapter-tuning. This mitigates the\ninitialization issue in prefix-tuning. Through comprehensive empirical\nexperiments on natural language understanding and generation tasks, we\ndemonstrate that inducer-tuning can close the performance gap between\nprefix-tuning and fine-tuning.",
    "descriptor": "\nComments: To appear in EMNLP 2022. Code is available at this https URL\n",
    "authors": [
      "Yifan Chen",
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Yang Liu",
      "Di Jin",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14469"
  },
  {
    "id": "arXiv:2210.14472",
    "title": "Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource  Languages",
    "abstract": "In the process of numerically modeling natural languages, developing language\nembeddings is a vital step. However, it is challenging to develop functional\nembeddings for resource-poor languages such as Sinhala, for which sufficiently\nlarge corpora, effective language parsers, and any other required resources are\ndifficult to find. In such conditions, the exploitation of existing models to\ncome up with an efficacious embedding methodology to numerically represent text\ncould be quite fruitful. This paper explores the effectivity of several\none-tiered and two-tiered embedding architectures in representing Sinhala text\nin the sentiment analysis domain. With our findings, the two-tiered embedding\narchitecture where the lower-tier consists of a word embedding and the\nupper-tier consists of a sentence embedding has been proven to perform better\nthan one-tier word embeddings, by achieving a maximum F1 score of 88.04% in\ncontrast to the 83.76% achieved by word embedding models. Furthermore,\nembeddings in the hyperbolic space are also developed and compared with\nEuclidean embeddings in terms of performance. A sentiment data set consisting\nof Facebook posts and associated reactions have been used for this research. To\neffectively compare the performance of different embedding systems, the same\ndeep neural network structure has been trained on sentiment data with each of\nthe embedding systems used to encode the text associated.",
    "descriptor": "",
    "authors": [
      "Gihan Weeraprameshwara",
      "Vihanga Jayawickrama",
      "Nisansa de Silva",
      "Yudhanjaya Wijeratne"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14472"
  },
  {
    "id": "arXiv:2210.14473",
    "title": "Benchmarking Language Models for Code Syntax Understanding",
    "abstract": "Pre-trained language models have demonstrated impressive performance in both\nnatural language processing and program understanding, which represent the\ninput as a token sequence without explicitly modeling its structure. Some prior\nworks show that pre-trained language models can capture the syntactic rules of\nnatural languages without finetuning on syntax understanding tasks. However,\nthere is limited understanding of how well pre-trained models understand the\ncode structure so far. In this work, we perform the first thorough benchmarking\nof the state-of-the-art pre-trained models for identifying the syntactic\nstructures of programs. Specifically, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syntactic relationships in their\ncorresponding abstract syntax trees. Our key observation is that existing\nlanguage models pretrained on code still lack the understanding of code syntax.\nIn fact, these pre-trained programming language models fail to match the\nperformance of simple baselines based on positional offsets and keywords. We\nalso present a natural language benchmark to highlight the differences between\nnatural languages and programming languages in terms of syntactic structure\nunderstanding. Our findings point out key limitations of existing pre-training\nmethods for programming languages, and suggest the importance of modeling code\nsyntactic structures.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Da Shen",
      "Xinyun Chen",
      "Chenguang Wang",
      "Koushik Sen",
      "Dawn Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14473"
  },
  {
    "id": "arXiv:2210.14474",
    "title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training  Consistency Preserving Metric GAN on Speech Enhancement Tasks",
    "abstract": "In recent years, Generative Adversarial Networks (GANs) have produced\nsignificantly improved results in speech enhancement (SE) tasks. They are\ndifficult to train, however. In this work, we introduce several improvements to\nthe GAN training schemes, which can be applied to most GAN-based SE models. We\npropose using consistency loss functions, which target the inconsistency in\ntime and time-frequency domains caused by Fourier and Inverse Fourier\nTransforms. We also present self-correcting optimization for training a GAN\ndiscriminator on SE tasks, which helps avoid \"harmful\" training directions for\nparts of the discriminator loss function. We have tested our proposed methods\non several state-of-the-art GAN-based SE models and obtained consistent\nimprovements, including new state-of-the-art results for the Voice Bank+DEMAND\ndataset.",
    "descriptor": "\nComments: 5 pages (4 - manuscript and 1 - references), 2 figures, 2 tables\n",
    "authors": [
      "Vasily Zadorozhnyy",
      "Qiang Ye",
      "Kazuhito Koishida"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14474"
  },
  {
    "id": "arXiv:2210.14480",
    "title": "Meta-node: A Concise Approach to Effectively Learn Complex Relationships  in Heterogeneous Graphs",
    "abstract": "Existing message passing neural networks for heterogeneous graphs rely on the\nconcepts of meta-paths or meta-graphs due to the intrinsic nature of\nheterogeneous graphs. However, the meta-paths and meta-graphs need to be\npre-configured before learning and are highly dependent on expert knowledge to\nconstruct them. To tackle this challenge, we propose a novel concept of\nmeta-node for message passing that can learn enriched relational knowledge from\ncomplex heterogeneous graphs without any meta-paths and meta-graphs by\nexplicitly modeling the relations among the same type of nodes. Unlike\nmeta-paths and meta-graphs, meta-nodes do not require any pre-processing steps\nthat require expert knowledge. Going one step further, we propose a meta-node\nmessage passing scheme and apply our method to a contrastive learning model. In\nthe experiments on node clustering and classification tasks, the proposed\nmeta-node message passing method outperforms state-of-the-arts that depend on\nmeta-paths. Our results demonstrate that effective heterogeneous graph learning\nis possible without the need for meta-paths that are frequently used in this\nfield.",
    "descriptor": "",
    "authors": [
      "Jiwoong Park",
      "Jisu Jeong",
      "Kyungmin Kim",
      "Jin Young Choi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.14480"
  },
  {
    "id": "arXiv:2210.14483",
    "title": "Robust Contextual Linear Bandits",
    "abstract": "Model misspecification is a major consideration in applications of\nstatistical methods and machine learning. However, it is often neglected in\ncontextual bandits. This paper studies a common form of misspecification, an\ninter-arm heterogeneity that is not captured by context. To address this issue,\nwe assume that the heterogeneity arises due to arm-specific random variables,\nwhich can be learned. We call this setting a robust contextual bandit. The\narm-specific variables explain the unknown inter-arm heterogeneity, and we\nincorporate them in the robust contextual estimator of the mean reward and its\nuncertainty. We develop two efficient bandit algorithms for our setting: a UCB\nalgorithm called RoLinUCB and a posterior-sampling algorithm called RoLinTS. We\nanalyze both algorithms and bound their $n$-round Bayes regret. Our experiments\nshow that RoLinTS is comparably statistically efficient to the classic methods\nwhen the misspecification is low, more robust when the misspecification is\nhigh, and significantly more computationally efficient than its naive\nimplementation.",
    "descriptor": "",
    "authors": [
      "Rong Zhu",
      "Branislav Kveton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14483"
  },
  {
    "id": "arXiv:2210.14485",
    "title": "Reconstruction from edge image combined with color and gradient  difference for industrial surface anomaly detection",
    "abstract": "Reconstruction-based methods are widely explored in industrial visual anomaly\ndetection. Such methods commonly require the model to well reconstruct the\nnormal patterns but fail in the anomalies, and thus the anomalies can be\ndetected by evaluating the reconstruction errors. However, in practice, it's\nusually difficult to control the generalization boundary of the model. The\nmodel with an overly strong generalization capability can even well reconstruct\nthe abnormal regions, making them less distinguishable, while the model with a\npoor generalization capability can not reconstruct those changeable\nhigh-frequency components in the normal regions, which ultimately leads to\nfalse positives. To tackle the above issue, we propose a new reconstruction\nnetwork where we reconstruct the original RGB image from its gray value edges\n(EdgRec). Specifically, this is achieved by an UNet-type denoising autoencoder\nwith skip connections. The input edge and skip connections can well preserve\nthe high-frequency information in the original image. Meanwhile, the proposed\nrestoration task can force the network to memorize the normal low-frequency and\ncolor information. Besides, the denoising design can prevent the model from\ndirectly copying the original high-frequent components. To evaluate the\nanomalies, we further propose a new interpretable hand-crafted evaluation\nfunction that considers both the color and gradient differences. Our method\nachieves competitive results on the challenging benchmark MVTec AD (97.8\\% for\ndetection and 97.7\\% for localization, AUROC). In addition, we conduct\nexperiments on the MVTec 3D-AD dataset and show convincing results using RGB\nimages only. Our code will be available at\nhttps://github.com/liutongkun/EdgRec.",
    "descriptor": "\nComments: 11 pages, 8 figures\n",
    "authors": [
      "Tongkun Liu",
      "Bing Li",
      "Zhuo Zhao",
      "Xiao Du",
      "Bingke Jiang",
      "Leqi Geng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14485"
  },
  {
    "id": "arXiv:2210.14486",
    "title": "Leveraging Affirmative Interpretations from Negation Improves Natural  Language Understanding",
    "abstract": "Negation poses a challenge in many natural language understanding tasks.\nInspired by the fact that understanding a negated statement often requires\nhumans to infer affirmative interpretations, in this paper we show that doing\nso benefits models for three natural language understanding tasks. We present\nan automated procedure to collect pairs of sentences with negation and their\naffirmative interpretations, resulting in over 150,000 pairs. Experimental\nresults show that leveraging these pairs helps (a) T5 generate affirmative\ninterpretations from negations in a previous benchmark, and (b) a RoBERTa-based\nclassifier solve the task of natural language inference. We also leverage our\npairs to build a plug-and-play neural generator that given a negated statement\ngenerates an affirmative interpretation. Then, we incorporate the pretrained\ngenerator into a RoBERTa-based classifier for sentiment analysis and show that\ndoing so improves the results. Crucially, our proposal does not require any\nmanual effort.",
    "descriptor": "\nComments: To appear at the main conference of EMNLP 2022\n",
    "authors": [
      "Md Mosharaf Hossain",
      "Eduardo Blanco"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14486"
  },
  {
    "id": "arXiv:2210.14487",
    "title": "Synchronization of Online Social Rhythms via Avatar Communications",
    "abstract": "In this study, we consider users' online communication rhythms (online social\nrhythms) as coupled oscillators in a complex social network. Users' rhythms may\nbe entrained onto those of their friends, and macro-scale pattern of such\nrhythms can emerge. We investigated the entrainment in online social rhythms\nand long-range correlations of the rhythms using an avatar communication\ndataset. We indicated entrainment in online social rhythms to emerge if the\nstrength of a new connection reaches a threshold. This entrainment spread via\ndensely-connected clusters. Consequently, long-range correlations of online\nsocial rhythms extended to about 36% of the network, although offline social\nlife naturally restricts online social rhythms. This research supports an\nunderstanding of human social dynamics in terms of systems of coupled\noscillators.",
    "descriptor": "\nComments: 6 pages, 9 figures, and appendixes (7 figures)\n",
    "authors": [
      "Masanori Takano"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14487"
  },
  {
    "id": "arXiv:2210.14489",
    "title": "Differentially Private Stochastic Convex Optimization for Network  Routing Applications",
    "abstract": "Network routing problems are common across many engineering applications.\nComputing optimal routing policies requires knowledge about network demand,\ni.e., the origin and destination (OD) of all requests in the network. However,\nprivacy considerations make it challenging to share individual OD data that\nwould be required for computing optimal policies. Privacy can be particularly\nchallenging in standard network routing problems because sources and sinks can\nbe easily identified from flow conservation constraints, making feasibility and\nprivacy mutually exclusive.\nIn this paper, we present a differentially private algorithm for network\nrouting problems. The main ingredient is a reformulation of network routing\nwhich moves all user data-dependent parameters out of the constraint set and\ninto the objective function. We then present an algorithm for solving this\nformulation based on a differentially private variant of stochastic gradient\ndescent. In this algorithm, differential privacy is achieved by injecting\nnoise, and one may wonder if this noise injection compromises solution quality.\nWe prove that our algorithm is both differentially private and asymptotically\noptimal as the size of the training set goes to infinity. We corroborate the\ntheoretical results with numerical experiments on a road traffic network which\nshow that our algorithm provides differentially private and near-optimal\nsolutions in practice.",
    "descriptor": "",
    "authors": [
      "Matthew Tsao",
      "Karthik Gopalakrishnan",
      "Kaidi Yang",
      "Marco Pavone"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14489"
  },
  {
    "id": "arXiv:2210.14492",
    "title": "Provable Safe Reinforcement Learning with Binary Feedback",
    "abstract": "Safety is a crucial necessity in many applications of reinforcement learning\n(RL), whether robotic, automotive, or medical. Many existing approaches to safe\nRL rely on receiving numeric safety feedback, but in many cases this feedback\ncan only take binary values; that is, whether an action in a given state is\nsafe or unsafe. This is particularly true when feedback comes from human\nexperts. We therefore consider the problem of provable safe RL when given\naccess to an offline oracle providing binary feedback on the safety of state,\naction pairs. We provide a novel meta algorithm, SABRE, which can be applied to\nany MDP setting given access to a blackbox PAC RL algorithm for that setting.\nSABRE applies concepts from active learning to reinforcement learning to\nprovably control the number of queries to the safety oracle. SABRE works by\niteratively exploring the state space to find regions where the agent is\ncurrently uncertain about safety. Our main theoretical results shows that,\nunder appropriate technical assumptions, SABRE never takes unsafe actions\nduring training, and is guaranteed to return a near-optimal safe policy with\nhigh probability. We provide a discussion of how our meta-algorithm may be\napplied to various settings studied in both theoretical and empirical\nframeworks.",
    "descriptor": "",
    "authors": [
      "Andrew Bennett",
      "Dipendra Misra",
      "Nathan Kallus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14492"
  },
  {
    "id": "arXiv:2210.14493",
    "title": "AVES: Animal Vocalization Encoder based on Self-Supervision",
    "abstract": "The lack of annotated training data in bioacoustics hinders the use of\nlarge-scale neural network models trained in a supervised way. In order to\nleverage a large amount of unannotated audio data, we propose AVES (Animal\nVocalization Encoder based on Self-Supervision), a self-supervised,\ntransformer-based audio representation model for encoding animal vocalizations.\nWe pretrain AVES on a diverse set of unannotated audio datasets and fine-tune\nthem for downstream bioacoustics tasks. Comprehensive experiments with a suite\nof classification and detection tasks have shown that AVES outperforms all the\nstrong baselines and even the supervised \"topline\" models trained on annotated\naudio classification datasets. The results also suggest that curating a small\ntraining subset related to downstream tasks is an efficient way to train\nhigh-quality audio representation models. We open-source our models at\n\\url{https://github.com/earthspecies/aves}.",
    "descriptor": "",
    "authors": [
      "Masato Hagiwara"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14493"
  },
  {
    "id": "arXiv:2210.14494",
    "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an  Introductory Programming Course",
    "abstract": "We introduce CS1QA, a dataset for code-based question answering in the\nprogramming education domain. CS1QA consists of 9,237 question-answer pairs\ngathered from chat logs in an introductory programming class using Python, and\n17,698 unannotated chat data with code. Each question is accompanied with the\nstudent's code, and the portion of the code relevant to answering the question.\nWe carefully design the annotation process to construct CS1QA, and analyze the\ncollected dataset in detail. The tasks for CS1QA are to predict the question\ntype, the relevant code snippet given the question and the code and retrieving\nan answer from the annotated corpus. Results for the experiments on several\nbaseline models are reported and thoroughly analyzed. The tasks for CS1QA\nchallenge models to understand both the code and natural language. This unique\ndataset can be used as a benchmark for source code comprehension and question\nanswering in the educational setting.",
    "descriptor": "",
    "authors": [
      "Changyoon Lee",
      "Yeon Seonwoo",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14494"
  },
  {
    "id": "arXiv:2210.14495",
    "title": "Two-stage dimensional emotion recognition by fusing predictions of  acoustic and text networks using SVM",
    "abstract": "Automatic speech emotion recognition (SER) by a computer is a critical\ncomponent for more natural human-machine interaction. As in human-human\ninteraction, the capability to perceive emotion correctly is essential to take\nfurther steps in a particular situation. One issue in SER is whether it is\nnecessary to combine acoustic features with other data such as facial\nexpressions, text, and motion capture. This research proposes to combine\nacoustic and text information by applying a late-fusion approach consisting of\ntwo steps. First, acoustic and text features are trained separately in deep\nlearning systems. Second, the prediction results from the deep learning systems\nare fed into a support vector machine (SVM) to predict the final regression\nscore. Furthermore, the task in this research is dimensional emotion modeling\nbecause it can enable a deeper analysis of affective states. Experimental\nresults show that this two-stage, late-fusion approach, obtains higher\nperformance than that of any one-stage processing, with a linear correlation\nfrom one-stage to two-stage processing. This late-fusion approach improves\nprevious early fusion results measured in concordance correlation coefficients\nscore.",
    "descriptor": "\nComments: Published in Speech Communications\n",
    "authors": [
      "Bagus Tris Atmaja",
      "Masato Akagi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14495"
  },
  {
    "id": "arXiv:2210.14496",
    "title": "Visually Improved Erosion Algorithm for the Procedural Generation of  Tile-based Terrain",
    "abstract": "Procedural terrain generation is the process of generating a digital\nrepresentation of terrain using a computer program or procedure, with little to\nno human guidance. This paper proposes a procedural terrain generation\nalgorithm based on a graph representation of fluvial erosion that offers\nseveral novel improvements over existing algorithms. Namely, the use of a\nheight constraint map with two types of locally defined constraint strengths;\nthe ability to specify a realistic erosion strength via level of rainfall; and\nthe ability to carve realistic gorges. These novelties allow it to generate\nmore varied and realistic terrain by integrating additional parameters and\nsimulation processes, while being faster and offering more flexibility and ease\nof use to terrain designers due to the nature and intuitiveness of these new\nparameters and processes. This paper additionally reviews some common metrics\nused to evaluate terrain generators, and suggests a completely new one that\ncontributes to a more holistic evaluation.",
    "descriptor": "",
    "authors": [
      "Fong Yuan Lim",
      "Yu Wei Tan",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.14496"
  },
  {
    "id": "arXiv:2210.14498",
    "title": "The Biscari Archive. A case study of the application of Transkribus tool",
    "abstract": "The Paterno' Castello Principi di Biscari Archive, preserved at the State\nArchives of Catania, amongst one of the most crucial family archives, is, in\nthe light of a digital historical methodology, the best computable historical\nheritage for demonstrating the applicability of applying an HTR tool, such as\nTranskribus, to digitised historical documents.",
    "descriptor": "",
    "authors": [
      "Salvatore Spina"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.14498"
  },
  {
    "id": "arXiv:2210.14500",
    "title": "Impact and analysis of space-time coupling on slotted MAC in UANs",
    "abstract": "The propagation delay is non-negligible in underwater acoustic networks\n(UANs) since the propagation speed is five orders of magnitude smaller than the\nspeed of light. In this case, space and time factors are strongly coupled to\ndetermine the collisions of packet transmissions. To this end, this paper\nanalyzes the impact of spatial-time coupling on slotted medium access control\n(MAC). We find that both inter-slot and intra-slot collisions may exist, and\nthe inter-slot collision may span multiple slots. The sending slot dependent\ninterference regions could be an annulus inside the whole transmission range.\nIt is pointed out that there exist collision-free regions when a guard interval\nlarger than a packet duration is used in the slot setting. In this sense, the\nlong slot brings spatial reuse in a transmission range. However, we further\nfind that the successful transmission probabilities and throughput are the same\nfor the slot lengths of one packet duration and two packet durations.\nSimulation results show that the maximum successful transmission probability\nand throughput can be achieved by a guard interval less than a packet duration,\nwhich is much shorter than the existing slot setting in typical Slotted-ALOHA.\nSimulations also show that the spatial impact is greater for vertical\ntransmission than for horizontal transmissions due to the longer vertical\ntransmission range in three-dimensional UANs.",
    "descriptor": "",
    "authors": [
      "Yan Wang",
      "Quansheng Guan",
      "Fei Ji",
      "Weiqi Chen"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14500"
  },
  {
    "id": "arXiv:2210.14502",
    "title": "SentBS: Sentence-level Beam Search for Controllable Summarization",
    "abstract": "A wide range of control perspectives have been explored in controllable text\ngeneration. Structure-controlled summarization is recently proposed as a useful\nand interesting research direction. However, current structure-controlling\nmethods have limited effectiveness in enforcing the desired structure. To\naddress this limitation, we propose a sentence-level beam search generation\nmethod (SentBS), where evaluation is conducted throughout the generation\nprocess to select suitable sentences for subsequent generations. We experiment\nwith different combinations of decoding methods to be used as subcomponents by\nSentBS and evaluate results on the structure-controlled dataset MReD.\nExperiments show that all explored combinations for SentBS can improve the\nagreement between the generated text and the desired structure, with the best\nmethod significantly reducing the structural discrepancies suffered by the\nexisting model, by approximately 68%.",
    "descriptor": "\nComments: 10 pages, 1 figure, accepted by EMNLP 2022\n",
    "authors": [
      "Chenhui Shen",
      "Liying Cheng",
      "Lidong Bing",
      "Yang You",
      "Luo Si"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14502"
  },
  {
    "id": "arXiv:2210.14505",
    "title": "Constructions of entanglement-assisted quantum MDS from generalized  Reed-Solomon codes",
    "abstract": "Entanglement-assisted quantum error-correcting (EAQEC) codes are a\ngeneralization of standard stabilizer quantum error-correcting codes, which can\nbe possibly constructed from any classical codes by relaxing self-orthogonal\ncondition with the help of pre-shared entanglement between the sender and the\nreceiver. In this paper, by using generalized Reed-Solomon codes, we construct\ntwo families of entanglement-assisted quantum error-correcting MDS (EAQMDS)\ncodes with parameters $[[\\frac{b({q^2}-1)}{a}+\\frac{{q^2} - 1}{a},\n\\frac{b({q^2}-1)}{a}+\\frac{{q^2}-1}{a}-2d+c+2,d;c]]_q$, where $q$ is a prime\npower and $a| (q+1)$. Among our constructions, the EAQMDS codes have much\nlarger minimum distance than the known EAQMDS codes with the same length and\nconsume the same number of ebits. Moreover, some of the lengths of ours EAQMDS\ncodes may not be divisors of $q^2\\pm 1$, which are new and different from all\nthe previously known ones.",
    "descriptor": "\nComments: 17 pages. 6 table\n",
    "authors": [
      "Xiujing Zheng"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14505"
  },
  {
    "id": "arXiv:2210.14507",
    "title": "SimpleDG: Simple Domain Generalization Baseline without Bells and  Whistles",
    "abstract": "We present a simple domain generalization baseline, which wins second place\nin both the common context generalization track and the hybrid context\ngeneralization track respectively in NICO CHALLENGE 2022. We verify the\nfounding in recent literature, domainbed, that ERM is a strong baseline\ncompared to recent state-of-the-art domain generalization methods and propose\nSimpleDG which includes several simple yet effective designs that further boost\ngeneralization performance. Code is available at\nhttps://github.com/megvii-research/SimpleDG",
    "descriptor": "",
    "authors": [
      "Zhi Lv",
      "Bo Lin",
      "Siyuan Liang",
      "Lihua Wang",
      "Mochen Yu",
      "Yao Tang",
      "Jiajun Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14507"
  },
  {
    "id": "arXiv:2210.14509",
    "title": "Parallel Gated Neural Network With Attention Mechanisim For Speech  Enhancement",
    "abstract": "Deep learning algorithm are increasingly used for speech enhancement (SE). In\nsupervised methods, global and local information is required for accurate\nspectral mapping. A key restriction is often poor capture of key contextual\ninformation. To leverage long-term for target speakers and compensate\ndistortions of cleaned speech, this paper adopts a sequence-to-sequence (S2S)\nmapping structure and proposes a novel monaural speech enhancement system,\nconsisting of a Feature Extraction Block (FEB), a Compensation Enhancement\nBlock (ComEB) and a Mask Block (MB). In the FEB a U-net block is used to\nextract abstract features using complex-valued spectra with one path to\nsuppress the background noise in the magnitude domain using masking methods and\nthe MB takes magnitude features from the FEBand compensates the lost\ncomplex-domain features produced from ComEB to restore the final cleaned\nspeech. Experiments are conducted on the Librispeech dataset and results show\nthat the proposed model obtains better performance than recent models in terms\nof ESTOI and PESQ scores.",
    "descriptor": "\nComments: 5 pages, 6 figures, references added\n",
    "authors": [
      "Jianqiao Cui",
      "Stefan Bleeck"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14509"
  },
  {
    "id": "arXiv:2210.14512",
    "title": "End-to-End Multimodal Representation Learning for Video Dialog",
    "abstract": "Video-based dialog task is a challenging multimodal learning task that has\nreceived increasing attention over the past few years with state-of-the-art\nobtaining new performance records. This progress is largely powered by the\nadaptation of the more powerful transformer-based language encoders. Despite\nthis progress, existing approaches do not effectively utilize visual features\nto help solve tasks. Recent studies show that state-of-the-art models are\nbiased toward textual information rather than visual cues. In order to better\nleverage the available visual information, this study proposes a new framework\nthat combines 3D-CNN network and transformer-based networks into a single\nvisual encoder to extract more robust semantic representations from videos. The\nvisual encoder is jointly trained end-to-end with other input modalities such\nas text and audio. Experiments on the AVSD task show significant improvement\nover baselines in both generative and retrieval tasks.",
    "descriptor": "",
    "authors": [
      "Huda Alamri",
      "Anthony Bilic",
      "Michael Hu",
      "Apoorva Beedu",
      "Irfan Essa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14512"
  },
  {
    "id": "arXiv:2210.14514",
    "title": "Improving Speech-to-Speech Translation Through Unlabeled Text",
    "abstract": "Direct speech-to-speech translation (S2ST) is among the most challenging\nproblems in the translation paradigm due to the significant scarcity of S2ST\ndata. While effort has been made to increase the data size from unlabeled\nspeech by cascading pretrained speech recognition (ASR), machine translation\n(MT) and text-to-speech (TTS) models; unlabeled text has remained relatively\nunder-utilized to improve S2ST. We propose an effective way to utilize the\nmassive existing unlabeled text from different languages to create a large\namount of S2ST data to improve S2ST performance by applying various acoustic\neffects to the generated synthetic data. Empirically our method outperforms the\nstate of the art in Spanish-English translation by up to 2 BLEU. Significant\ngains by the proposed method are demonstrated in extremely low-resource\nsettings for both Spanish-English and Russian-English translations.",
    "descriptor": "",
    "authors": [
      "Xuan-Phi Nguyen",
      "Sravya Popuri",
      "Changhan Wang",
      "Yun Tang",
      "Ilia Kulikov",
      "Hongyu Gong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14514"
  },
  {
    "id": "arXiv:2210.14520",
    "title": "Adaptive scaling of the learning rate by second order automatic  differentiation",
    "abstract": "In the context of the optimization of Deep Neural Networks, we propose to\nrescale the learning rate using a new technique of automatic differentiation.\nThis technique relies on the computation of the {\\em curvature}, a second order\ninformation whose computational complexity is in between the computation of the\ngradient and the one of the Hessian-vector product. If (1C,1M) represents\nrespectively the computational time and memory footprint of the gradient\nmethod, the new technique increase the overall cost to either (1.5C,2M) or\n(2C,1M). This rescaling has the appealing characteristic of having a natural\ninterpretation, it allows the practitioner to choose between exploration of the\nparameters set and convergence of the algorithm. The rescaling is adaptive, it\ndepends on the data and on the direction of descent. The numerical experiments\nhighlight the different exploration/convergence regimes.",
    "descriptor": "",
    "authors": [
      "Fr\u00e9d\u00e9ric de Gournay",
      "Alban Gossard"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.14520"
  },
  {
    "id": "arXiv:2210.14523",
    "title": "OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for  Extreme Multi-label Text Classification",
    "abstract": "Extreme multi-label text classification (XMTC) is the task of finding the\nmost relevant subset labels from an extremely large-scale label collection.\nRecently, some deep learning models have achieved state-of-the-art results in\nXMTC tasks. These models commonly predict scores for all labels by a fully\nconnected layer as the last layer of the model. However, such models can't\npredict a relatively complete and variable-length label subset for each\ndocument, because they select positive labels relevant to the document by a\nfixed threshold or take top k labels in descending order of scores. A less\npopular type of deep learning models called sequence-to-sequence (Seq2Seq)\nfocus on predicting variable-length positive labels in sequence style. However,\nthe labels in XMTC tasks are essentially an unordered set rather than an\nordered sequence, the default order of labels restrains Seq2Seq models in\ntraining. To address this limitation in Seq2Seq, we propose an autoregressive\nsequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates\npredictions in student-forcing scheme and is trained by a loss function based\non bipartite matching which enables permutation-invariance. Meanwhile, we use\nthe optimal transport distance as a measurement to force the model to focus on\nthe closest labels in semantic label space. Experiments show that OTSeq2Set\noutperforms other competitive baselines on 4 benchmark datasets. Especially, on\nthe Wikipedia dataset with 31k labels, it outperforms the state-of-the-art\nSeq2Seq method by 16.34% in micro-F1 score. The code is available at\nhttps://github.com/caojie54/OTSeq2Set.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Jie Cao",
      "Yin Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14523"
  },
  {
    "id": "arXiv:2210.14524",
    "title": "A Bibliometric Analysis and Review on Reinforcement Learning for  Transportation Applications",
    "abstract": "Transportation is the backbone of the economy and urban development.\nImproving the efficiency, sustainability, resilience, and intelligence of\ntransportation systems is critical and also challenging. The constantly\nchanging traffic conditions, the uncertain influence of external factors (e.g.,\nweather, accidents), and the interactions among multiple travel modes and\nmulti-type flows result in the dynamic and stochastic natures of transportation\nsystems. The planning, operation, and control of transportation systems require\nflexible and adaptable strategies in order to deal with uncertainty,\nnon-linearity, variability, and high complexity. In this context, Reinforcement\nLearning (RL) that enables autonomous decision-makers to interact with the\ncomplex environment, learn from the experiences, and select optimal actions has\nbeen rapidly emerging as one of the most useful approaches for smart\ntransportation. This paper conducts a bibliometric analysis to identify the\ndevelopment of RL-based methods for transportation applications, typical\njournals/conferences, and leading topics in the field of intelligent\ntransportation in recent ten years. Then, this paper presents a comprehensive\nliterature review on applications of RL in transportation by categorizing\ndifferent methods with respect to the specific application domains. The\npotential future research directions of RL applications and developments are\nalso discussed.",
    "descriptor": "",
    "authors": [
      "Can Li",
      "Lei Bai",
      "Lina Yao",
      "S. Travis Waller",
      "Wei Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14524"
  },
  {
    "id": "arXiv:2210.14529",
    "title": "Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with  User Simulator",
    "abstract": "Task-Oriented Dialogue (TOD) systems are drawing more and more attention in\nrecent studies. Current methods focus on constructing pre-trained models or\nfine-tuning strategies while the evaluation of TOD is limited by a policy\nmismatch problem. That is, during evaluation, the user utterances are from the\nannotated dataset while these utterances should interact with previous\nresponses which can have many alternatives besides annotated texts. Therefore,\nin this work, we propose an interactive evaluation framework for TOD. We first\nbuild a goal-oriented user simulator based on pre-trained models and then use\nthe user simulator to interact with the dialogue system to generate dialogues.\nBesides, we introduce a sentence-level and a session-level score to measure the\nsentence fluency and session coherence in the interactive evaluation.\nExperimental results show that RL-based TOD systems trained by our proposed\nuser simulator can achieve nearly 98% inform and success rates in the\ninteractive evaluation of MultiWOZ dataset and the proposed scores measure the\nresponse quality besides the inform and success rates. We are hoping that our\nwork will encourage simulator-based interactive evaluations in the TOD task.",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Qinyuan Cheng",
      "Linyang Li",
      "Guofeng Quan",
      "Feng Gao",
      "Xiaofeng Mou",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14529"
  },
  {
    "id": "arXiv:2210.14530",
    "title": "RGB-T Semantic Segmentation with Location, Activation, and Sharpening",
    "abstract": "Semantic segmentation is important for scene understanding. To address the\nscenes of adverse illumination conditions of natural images, thermal infrared\n(TIR) images are introduced. Most existing RGB-T semantic segmentation methods\nfollow three cross-modal fusion paradigms, i.e. encoder fusion, decoder fusion,\nand feature fusion. Some methods, unfortunately, ignore the properties of RGB\nand TIR features or the properties of features at different levels. In this\npaper, we propose a novel feature fusion-based network for RGB-T semantic\nsegmentation, named \\emph{LASNet}, which follows three steps of location,\nactivation, and sharpening. The highlight of LASNet is that we fully consider\nthe characteristics of cross-modal features at different levels, and\naccordingly propose three specific modules for better segmentation. Concretely,\nwe propose a Collaborative Location Module (CLM) for high-level semantic\nfeatures, aiming to locate all potential objects. We propose a Complementary\nActivation Module for middle-level features, aiming to activate exact regions\nof different objects. We propose an Edge Sharpening Module (ESM) for low-level\ntexture features, aiming to sharpen the edges of objects. Furthermore, in the\ntraining phase, we attach a location supervision and an edge supervision after\nCLM and ESM, respectively, and impose two semantic supervisions in the decoder\npart to facilitate network convergence. Experimental results on two public\ndatasets demonstrate that the superiority of our LASNet over relevant\nstate-of-the-art methods. The code and results of our method are available at\nhttps://github.com/MathLee/LASNet.",
    "descriptor": "\nComments: 12 pages, 7 figures, Accepted by IEEE Transactions on Circuits and Systems for Video Technology 2022\n",
    "authors": [
      "Gongyang Li",
      "Yike Wang",
      "Zhi Liu",
      "Xinpeng Zhang",
      "Dan Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14530"
  },
  {
    "id": "arXiv:2210.14531",
    "title": "Unifying Data Perspectivism and Personalization: An Application to  Social Norms",
    "abstract": "Instead of using a single ground truth for language processing tasks, several\nrecent studies have examined how to represent and predict the labels of the set\nof annotators. However, often little or no information about annotators is\nknown, or the set of annotators is small. In this work, we examine a corpus of\nsocial media posts about conflict from a set of 13k annotators and 210k\njudgements of social norms. We provide a novel experimental setup that applies\npersonalization methods to the modeling of annotators and compare their\neffectiveness for predicting the perception of social norms. We further provide\nan analysis of performance across subsets of social situations that vary by the\ncloseness of the relationship between parties in conflict, and assess where\npersonalization helps the most.",
    "descriptor": "",
    "authors": [
      "Joan Plepi",
      "B\u00e9la Neuendorf",
      "Lucie Flek",
      "Charles Welch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14531"
  },
  {
    "id": "arXiv:2210.14532",
    "title": "Uncertainty-based Meta-Reinforcement Learning for Robust Radar Tracking",
    "abstract": "Nowadays, Deep Learning (DL) methods often overcome the limitations of\ntraditional signal processing approaches. Nevertheless, DL methods are barely\napplied in real-life applications. This is mainly due to limited robustness and\ndistributional shift between training and test data. To this end, recent work\nhas proposed uncertainty mechanisms to increase their reliability. Besides,\nmeta-learning aims at improving the generalization capability of DL models. By\ntaking advantage of that, this paper proposes an uncertainty-based\nMeta-Reinforcement Learning (Meta-RL) approach with Out-of-Distribution (OOD)\ndetection. The presented method performs a given task in unseen environments\nand provides information about its complexity. This is done by determining\nfirst and second-order statistics on the estimated reward. Using information\nabout its complexity, the proposed algorithm is able to point out when tracking\nis reliable. To evaluate the proposed method, we benchmark it on a\nradar-tracking dataset. There, we show that our method outperforms related\nMeta-RL approaches on unseen tracking scenarios in peak performance by 16% and\nthe baseline by 35% while detecting OOD data with an F1-Score of 72%. This\nshows that our method is robust to environmental changes and reliably detects\nOOD scenarios.",
    "descriptor": "\nComments: accepted at ICMLA 2022\n",
    "authors": [
      "Julius Ott",
      "Lorenzo Servadei",
      "Gianfranco Mauro",
      "Thomas Stadelmayer",
      "Avik Santra",
      "Robert Wille"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14532"
  },
  {
    "id": "arXiv:2210.14533",
    "title": "A robust GMRES algorithm in Tensor Train format",
    "abstract": "We consider the solution of linear systems with tensor product structure\nusing a GMRES algorithm. In order to cope with the computational complexity in\nlarge dimension both in terms of floating point operations and memory\nrequirement, our algorithm is based on low-rank tensor representation, namely\nthe Tensor Train format. In a backward error analysis framework, we show how\nthe tensor approximation affects the accuracy of the computed solution. With\nthe bacwkward perspective, we investigate the situations where the\n$(d+1)$-dimensional problem to be solved results from the concatenation of a\nsequence of $d$-dimensional problems (like parametric linear operator or\nparametric right-hand side problems), we provide backward error bounds to\nrelate the accuracy of the $(d+1)$-dimensional computed solution with the\nnumerical quality of the sequence of $d$-dimensional solutions that can be\nextracted form it. This enables to prescribe convergence threshold when solving\nthe $(d+1)$-dimensional problem that ensures the numerical quality of the\n$d$-dimensional solutions that will be extracted from the $(d+1)$-dimensional\ncomputed solution once the solver has converged. The above mentioned features\nare illustrated on a set of academic examples of varying dimensions and sizes.",
    "descriptor": "",
    "authors": [
      "Olivier Coulaud",
      "Luc Giraud",
      "Martina Iannacito"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14533"
  },
  {
    "id": "arXiv:2210.14534",
    "title": "Efficient Quantized Constant Envelope Precoding for Multiuser Downlink  Massive MIMO Systems",
    "abstract": "Quantized constant envelope (QCE) precoding, a new transmission scheme that\nonly discrete QCE transmit signals are allowed at each antenna, has gained\ngrowing research interests due to its ability of reducing the hardware cost and\nthe energy consumption of massive multiple-input multiple-output (MIMO)\nsystems. However, the discrete nature of QCE transmit signals greatly\ncomplicates the precoding design. In this paper, we consider the QCE precoding\nproblem for a massive MIMO system with phase shift keying (PSK) modulation and\ndevelop an efficient approach for solving the constructive interference (CI)\nbased problem formulation. Our approach is based on a custom-designed\n(continuous) penalty model that is equivalent to the original discrete problem.\nSpecifically, the penalty model relaxes the discrete QCE constraint and\npenalizes it in the objective with a negative $\\ell_2$-norm term, which leads\nto a non-smooth non-convex optimization problem. To tackle it, we resort to our\nrecently proposed alternating optimization (AO) algorithm. We show that the AO\nalgorithm admits closed-form updates at each iteration when applied to our\nproblem and thus can be efficiently implemented. Simulation results demonstrate\nthe superiority of the proposed approach over the existing algorithms.",
    "descriptor": "\nComments: 5 pages, 5 figures, submitted for possible publication\n",
    "authors": [
      "Zheyu Wu",
      "Bo Jiang",
      "Ya-Feng Liu",
      "Yu-Hong Dai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14534"
  },
  {
    "id": "arXiv:2210.14541",
    "title": "Look to the Right: Mitigating Relative Position Bias in Extractive  Question Answering",
    "abstract": "Extractive question answering (QA) models tend to exploit spurious\ncorrelations to make predictions when a training set has unintended biases.\nThis tendency results in models not being generalizable to examples where the\ncorrelations do not hold. Determining the spurious correlations QA models can\nexploit is crucial in building generalizable QA models in real-world\napplications; moreover, a method needs to be developed that prevents these\nmodels from learning the spurious correlations even when a training set is\nbiased. In this study, we discovered that the relative position of an answer,\nwhich is defined as the relative distance from an answer span to the closest\nquestion-context overlap word, can be exploited by QA models as superficial\ncues for making predictions. Specifically, we find that when the relative\npositions in a training set are biased, the performance on examples with\nrelative positions unseen during training is significantly degraded. To\nmitigate the performance degradation for unseen relative positions, we propose\nan ensemble-based debiasing method that does not require prior knowledge about\nthe distribution of relative positions. We demonstrate that the proposed method\nmitigates the models' reliance on relative positions using the biased and full\nSQuAD dataset. We hope that this study can help enhance the generalization\nability of QA models in real-world applications.",
    "descriptor": "\nComments: Accepted to BlackboxNLP 2022\n",
    "authors": [
      "Kazutoshi Shinoda",
      "Saku Sugawara",
      "Akiko Aizawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14541"
  },
  {
    "id": "arXiv:2210.14543",
    "title": "Diversity Order Analysis for Quantized Constant Envelope Transmission",
    "abstract": "Quantized constant envelope (QCE) transmission is a popular and effective\ntechnique to reduce the hardware cost and improve the power efficiency of 5G\nand beyond systems equipped with large antenna arrays. It has been widely\nobserved that the number of quantization levels has a substantial impact on the\nsystem performance. This paper aims to quantify the impact of the number of\nquantization levels on the system performance. Specifically, we consider a\ndownlink single-user multiple-input-single-output (MISO) system with M-phase\nshift keying (PSK) constellation under the Rayleigh fading channel. We first\nderive a novel bound on the system symbol error probability (SEP). Based on the\nderived SEP bound, we characterize the achievable diversity order of the\nquantized matched filter (MF) precoding strategy. Our results show that full\ndiversity order can be achieved when the number of quantization levels L is\ngreater than the PSK constellation order M, i.e., L>M, only half diversity\norder is achievable when L=M, and the achievable diversity order is 0 when L<M.\nSimulation results verify our theoretical analysis.",
    "descriptor": "\nComments: 9 pages, 3 figures, submitted for possible publication\n",
    "authors": [
      "Zheyu Wu",
      "Jiageng Wu",
      "Wei-Kun Chen",
      "Ya-Feng Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14543"
  },
  {
    "id": "arXiv:2210.14545",
    "title": "Towards Practical Few-Shot Query Sets: Transductive Minimum Description  Length Inference",
    "abstract": "Standard few-shot benchmarks are often built upon simplifying assumptions on\nthe query sets, which may not always hold in practice. In particular, for each\ntask at testing time, the classes effectively present in the unlabeled query\nset are known a priori, and correspond exactly to the set of classes\nrepresented in the labeled support set. We relax these assumptions and extend\ncurrent benchmarks, so that the query-set classes of a given task are unknown,\nbut just belong to a much larger set of possible classes. Our setting could be\nviewed as an instance of the challenging yet practical problem of extremely\nimbalanced K-way classification, K being much larger than the values typically\nused in standard benchmarks, and with potentially irrelevant supervision from\nthe support set. Expectedly, our setting incurs drops in the performances of\nstate-of-the-art methods. Motivated by these observations, we introduce a\nPrimAl Dual Minimum Description LEngth (PADDLE) formulation, which balances\ndata-fitting accuracy and model complexity for a given few-shot task, under\nsupervision constraints from the support set. Our constrained MDL-like\nobjective promotes competition among a large set of possible classes,\npreserving only effective classes that befit better the data of a few-shot\ntask. It is hyperparameter free, and could be applied on top of any base-class\ntraining. Furthermore, we derive a fast block coordinate descent algorithm for\noptimizing our objective, with convergence guarantee, and a linear\ncomputational complexity at each iteration. Comprehensive experiments over the\nstandard few-shot datasets and the more realistic and challenging i-Nat dataset\nshow highly competitive performances of our method, more so when the numbers of\npossible classes in the tasks increase. Our code is publicly available at\nhttps://github.com/SegoleneMartin/PADDLE.",
    "descriptor": "",
    "authors": [
      "S\u00e9gol\u00e8ne Martin",
      "Malik Boudiaf",
      "Emilie Chouzenoux",
      "Jean-Christophe Pesquet",
      "Ismail Ben Ayed"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14545"
  },
  {
    "id": "arXiv:2210.14547",
    "title": "Tracking-based distributed equilibrium seeking for aggregative games",
    "abstract": "We propose fully-distributed algorithms for Nash equilibrium seeking in\naggregative games over networks. We first consider the case where only local\nconstraints are present and we design an algorithm combining, for each agent,\n(i) the projected pseudo-gradient descent and (ii) a tracking mechanism to\nlocally reconstruct the aggregative variable. To handle coupling constraints\narising in generalized settings, we propose another distributed algorithm based\non (i) a recently emerged augmented primal-dual scheme and (ii) two tracking\nmechanisms to reconstruct, for each agent, both the aggregative variable and\nthe coupling constraint satisfaction. Leveraging tools from singular\nperturbations analysis, we prove linear convergence to the Nash equilibrium for\nboth schemes. Finally, we run extensive numerical simulations to confirm the\neffectiveness of our methods, also showing that they outperform the current\nstate-of-the-art distributed equilibrium seeking algorithms.",
    "descriptor": "",
    "authors": [
      "Guido Carnevale",
      "Filippo Fabiani",
      "Filiberto Fele",
      "Kostas Margellos",
      "Giuseppe Notarstefano"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14547"
  },
  {
    "id": "arXiv:2210.14549",
    "title": "Binary optimal linear codes with various hull dimensions and  entanglement-assisted QECC",
    "abstract": "The hull of a linear code $C$ is the intersection of $C$ with its dual. To\nthe best of our knowledge, there are very few constructions of binary linear\ncodes with the hull dimension $\\ge 2$ except for self-orthogonal codes. We\npropose a building-up construction to obtain a plenty of binary $[n+2, k+1]$\ncodes with hull dimension $\\ell, \\ell +1$, or $\\ell +2$ from a given binary\n$[n,k]$ code with hull dimension $\\ell$. In particular, with respect to hull\ndimensions 1 and 2, we construct all binary optimal $[n, k]$ codes of lengths\nup to 13. With respect to hull dimensions 3, 4, and 5, we construct all binary\noptimal $[n,k]$ codes of lengths up to 12 and the best possible minimum\ndistances of $[13,k]$ codes for $3 \\le k \\le 10$. As an application, we apply\nour binary optimal codes with a given hull dimension to construct several\nentanglement-assisted quantum error-correcting codes(EAQECC) with the best\nknown parameters.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Jon-Lark Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14549"
  },
  {
    "id": "arXiv:2210.14552",
    "title": "A Robust Bias Mitigation Procedure Based on the Stereotype Content Model",
    "abstract": "The Stereotype Content model (SCM) states that we tend to perceive minority\ngroups as cold, incompetent or both. In this paper we adapt existing work to\ndemonstrate that the Stereotype Content model holds for contextualised word\nembeddings, then use these results to evaluate a fine-tuning process designed\nto drive a language model away from stereotyped portrayals of minority groups.\nWe find the SCM terms are better able to capture bias than demographic agnostic\nterms related to pleasantness. Further, we were able to reduce the presence of\nstereotypes in the model through a simple fine-tuning procedure that required\nminimal human and computer resources, without harming downstream performance.\nWe present this work as a prototype of a debiasing procedure that aims to\nremove the need for a priori knowledge of the specifics of bias in the model.",
    "descriptor": "",
    "authors": [
      "Eddie L. Ungless",
      "Amy Rafferty",
      "Hrichika Nag",
      "Bj\u00f6rn Ross"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14552"
  },
  {
    "id": "arXiv:2210.14556",
    "title": "Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal  Prediction for Multimodal Sentiment Analysis",
    "abstract": "Multimodal representation learning is a challenging task in which previous\nwork mostly focus on either uni-modality pre-training or cross-modality fusion.\nIn fact, we regard modeling multimodal representation as building a skyscraper,\nwhere laying stable foundation and designing the main structure are equally\nessential. The former is like encoding robust uni-modal representation while\nthe later is like integrating interactive information among different\nmodalities, both of which are critical to learning an effective multimodal\nrepresentation. Recently, contrastive learning has been successfully applied in\nrepresentation learning, which can be utilized as the pillar of the skyscraper\nand benefit the model to extract the most important features contained in the\nmultimodal data. In this paper, we propose a novel framework named MultiModal\nContrastive Learning (MMCL) for multimodal representation to capture intra- and\ninter-modality dynamics simultaneously. Specifically, we devise uni-modal\ncontrastive coding with an efficient uni-modal feature augmentation strategy to\nfilter inherent noise contained in acoustic and visual modality and acquire\nmore robust uni-modality representations. Besides, a pseudo siamese network is\npresented to predict representation across different modalities, which\nsuccessfully captures cross-modal dynamics. Moreover, we design two contrastive\nlearning tasks, instance- and sentiment-based contrastive learning, to promote\nthe process of prediction and learn more interactive information related to\nsentiment. Extensive experiments conducted on two public datasets demonstrate\nthat our method surpasses the state-of-the-art methods.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Ronghao Lin",
      "Haifeng Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14556"
  },
  {
    "id": "arXiv:2210.14558",
    "title": "Compressing And Debiasing Vision-Language Pre-Trained Models for Visual  Question Answering",
    "abstract": "Despite the excellent performance of large-scale vision-language pre-trained\nmodels (VLPs) on conventional visual question answering task, they still suffer\nfrom two problems: First, VLPs tend to rely on language biases in datasets and\nfail to generalize to out-of-distribution (OOD) data. Second, they are\ninefficient in terms of memory footprint and computation. Although promising\nprogress has been made in both problems, most existing works tackle them\nindependently. To facilitate the application of VLP to VQA tasks, it is\nimperative to jointly study VLP compression and OOD robustness, which, however,\nhas not yet been explored. In this paper, we investigate whether a VLP can be\ncompressed and debiased simultaneously by searching sparse and robust\nsubnetworks. To this end, we conduct extensive experiments with LXMERT, a\nrepresentative VLP, on the OOD dataset VQA-CP v2. We systematically study the\ndesign of a training and compression pipeline to search the subnetworks, as\nwell as the assignment of sparsity to different modality-specific modules. Our\nresults show that there indeed exist sparse and robust LXMERT subnetworks,\nwhich significantly outperform the full model (without debiasing) with much\nfewer parameters. These subnetworks also exceed the current SoTA debiasing\nmodels with comparable or fewer parameters. We will release the codes on\npublication.",
    "descriptor": "",
    "authors": [
      "Qingyi Si",
      "Yuanxin Liu",
      "Zheng Lin",
      "Peng Fu",
      "Weiping Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14558"
  },
  {
    "id": "arXiv:2210.14560",
    "title": "Hierarchical Federated Learning with Momentum Acceleration in Multi-Tier  Networks",
    "abstract": "In this paper, we propose Hierarchical Federated Learning with Momentum\nAcceleration (HierMo), a three-tier worker-edge-cloud federated learning\nalgorithm that applies momentum for training acceleration. Momentum is\ncalculated and aggregated in the three tiers. We provide convergence analysis\nfor HierMo, showing a convergence rate of O(1/T). In the analysis, we develop a\nnew approach to characterize model aggregation, momentum aggregation, and their\ninteractions. Based on this result, {we prove that HierMo achieves a tighter\nconvergence upper bound compared with HierFAVG without momentum}. We also\npropose HierOPT, which optimizes the aggregation periods (worker-edge and\nedge-cloud aggregation periods) to minimize the loss given a limited training\ntime.",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Zhengjie Yang",
      "Sen Fu",
      "Wei Bao",
      "Dong Yuan",
      "Albert Y. Zomaya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14560"
  },
  {
    "id": "arXiv:2210.14562",
    "title": "FairCLIP: Social Bias Elimination based on Attribute Prototype Learning  and Representation Neutralization",
    "abstract": "The Vision-Language Pre-training (VLP) models like CLIP have gained\npopularity in recent years. However, many works found that the social biases\nhidden in CLIP easily manifest in downstream tasks, especially in image\nretrieval, which can have harmful effects on human society. In this work, we\npropose FairCLIP to eliminate the social bias in CLIP-based image retrieval\nwithout damaging the retrieval performance achieving the compatibility between\nthe debiasing effect and the retrieval performance. FairCLIP is divided into\ntwo steps: Attribute Prototype Learning (APL) and Representation Neutralization\n(RN). In the first step, we extract the concepts needed for debiasing in CLIP.\nWe use the query with learnable word vector prefixes as the extraction\nstructure. In the second step, we first divide the attributes into target and\nbias attributes. By analysis, we find that both attributes have an impact on\nthe bias. Therefore, we try to eliminate the bias by using Re-Representation\nMatrix (RRM) to achieve the neutralization of the representation. We compare\nthe debiasing effect and retrieval performance with other methods, and\nexperiments demonstrate that FairCLIP can achieve the best compatibility.\nAlthough FairCLIP is used to eliminate bias in image retrieval, it achieves the\nneutralization of the representation which is common to all CLIP downstream\ntasks. This means that FairCLIP can be applied as a general debiasing method\nfor other fairness issues related to CLIP.",
    "descriptor": "",
    "authors": [
      "Junyang Wang",
      "Yi Zhang",
      "Jitao Sang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14562"
  },
  {
    "id": "arXiv:2210.14566",
    "title": "Access Service Records Based Trust Management Scheme for Internet of  Things",
    "abstract": "The distributed structure of the Internet of things has gradually replaced\nthe centralized structure because of its scalability, security, and single\npoint of failure. The huge scale of information recording of the Internet of\nthings brings challenges and opportunities to the trust management of the\nInternet of things. Through the analysis of a variety of existing trust\nmanagement schemes, this paper proposes a unified data structure for\ndistributed access service records, TokenChain, in which triple DES data\nencryption is used to ensure privacy and traceability, to achieve a unified\ntrust management scheme. Based on TokenChain, a three-tier trust management\narchitecture (TokenChain-Based Trust Management, TBTM) is implemented in the\ndata layer, computing layer, and control layer. Trust evaluation is affected by\nfour statistics and finally converges to a real value under certain conditions.\nBased on TBTM, we carry out theoretical analysis, malicious attack resistance,\nsimulation, and performance evaluation in various complex scenarios. The\nresults show that TBTM satisfies service prediction, global trust analysis,\nhigh security, and excellent performance in multi-domain complex scenarios.\nCompared with the existing trust management schemes, TBTM realizes\ncross-scenario interaction and two-way trust management, and simultaneously\nmeets the characteristics of traceability, tamper-proof, attack resistance, and\ndistribution. Finally, this paper is summarized.",
    "descriptor": "",
    "authors": [
      "Xuefei Li",
      "Ru Li"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14566"
  },
  {
    "id": "arXiv:2210.14571",
    "title": "Towards the Detection of Diffusion Model Deepfakes",
    "abstract": "Diffusion models (DMs) have recently emerged as a promising method in image\nsynthesis. They have surpassed generative adversarial networks (GANs) in both\ndiversity and quality, and have achieved impressive results in text-to-image\nand image-to-image modeling. However, to date, only little attention has been\npaid to the detection of DM-generated images, which is critical to prevent\nadverse impacts on our society. Although prior work has shown that\nGAN-generated images can be reliably detected using automated methods, it is\nunclear whether the same methods are effective against DMs. In this work, we\naddress this challenge and take a first look at detecting DM-generated images.\nWe approach the problem from two different angles: First, we evaluate the\nperformance of state-of-the-art detectors on a variety of DMs. Second, we\nanalyze DM-generated images in the frequency domain and study different factors\nthat influence the spectral properties of these images. Most importantly, we\ndemonstrate that GANs and DMs produce images with different characteristics,\nwhich requires adaptation of existing classifiers to ensure reliable detection.\nWe believe this work provides the foundation and starting point for further\nresearch to detect DM deepfakes effectively.",
    "descriptor": "\nComments: 24 pages, 18 figures\n",
    "authors": [
      "Jonas Ricker",
      "Simon Damm",
      "Thorsten Holz",
      "Asja Fischer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14571"
  },
  {
    "id": "arXiv:2210.14572",
    "title": "Quantifying the Loss of Acyclic Join Dependencies",
    "abstract": "Acyclic schemas possess known benefits for database design, speeding up\nqueries, and reducing space requirements. An acyclic join dependency (AJD) is\nlossless with respect to a universal relation if joining the projections\nassociated with the schema results in the original universal relation. An\nintuitive and standard measure of loss entailed by an AJD is the number of\nredundant tuples generated by the acyclic join. Recent work has shown that the\nloss of an AJD can also be characterized by an information-theoretic measure.\nMotivated by the problem of automatically fitting an acyclic schema to a\nuniversal relation, we investigate the connection between these two\ncharacterizations of loss. We first show that the loss of an AJD is captured\nusing the notion of KL-Divergence. We then show that the KL-divergence can be\nused to bound the number of redundant tuples. We prove a deterministic lower\nbound on the percentage of redundant tuples. For an upper bound, we propose a\n\\e{random database model}, and establish a bound that holds in expectation over\na random choice of relation, which coincides with the lower bound for large\nrelation instances.",
    "descriptor": "\nComments: Accepted to PODS 2023\n",
    "authors": [
      "Batya Kenig",
      "Nir Weinberger"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14572"
  },
  {
    "id": "arXiv:2210.14576",
    "title": "Uncertainty Sentence Sampling by Virtual Adversarial Perturbation",
    "abstract": "Active learning for sentence understanding attempts to reduce the annotation\ncost by identifying the most informative examples. Common methods for active\nlearning use either uncertainty or diversity sampling in the pool-based\nscenario. In this work, to incorporate both predictive uncertainty and sample\ndiversity, we propose Virtual Adversarial Perturbation for Active Learning\n(VAPAL) , an uncertainty-diversity combination framework, using virtual\nadversarial perturbation (Miyato et al., 2019) as model uncertainty\nrepresentation. VAPAL consistently performs equally well or even better than\nthe strong baselines on four sentence understanding datasets: AGNEWS, IMDB,\nPUBMED, and SST-2, offering a potential option for active learning on sentence\nunderstanding tasks.",
    "descriptor": "",
    "authors": [
      "Hanshan Zhang",
      "Zhen Zhang",
      "Hongfei Jiang",
      "Yang Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14576"
  },
  {
    "id": "arXiv:2210.14577",
    "title": "Disentangling Past-Future Modeling in Sequential Recommendation via Dual  Networks",
    "abstract": "Sequential recommendation (SR) plays an important role in personalized\nrecommender systems because it captures dynamic and diverse preferences from\nusers' real-time increasing behaviors. Unlike the standard autoregressive\ntraining strategy, future data (also available during training) has been used\nto facilitate model training as it provides richer signals about user's current\ninterests and can be used to improve the recommendation quality. However, these\nmethods suffer from a severe training-inference gap, i.e., both past and future\ncontexts are modeled by the same encoder when training, while only historical\nbehaviors are available during inference. This discrepancy leads to potential\nperformance degradation. To alleviate the training-inference gap, we propose a\nnew framework DualRec, which achieves past-future disentanglement and\npast-future mutual enhancement by a novel dual network. Specifically, a dual\nnetwork structure is exploited to model the past and future context separately.\nAnd a bi-directional knowledge transferring mechanism enhances the knowledge\nlearnt by the dual network. Extensive experiments on four real-world datasets\ndemonstrate the superiority of our approach over baseline methods. Besides, we\ndemonstrate the compatibility of DualRec by instantiating using RNN,\nTransformer, and filter-MLP as backbones. Further empirical analysis verifies\nthe high utility of modeling future contexts under our DualRec framework.",
    "descriptor": "\nComments: Accept by CIKM 2022\n",
    "authors": [
      "Hengyu Zhang",
      "Enming Yuan",
      "Wei Guo",
      "Zhicheng He",
      "Jiarui Qin",
      "Huifeng Guo",
      "Bo Chen",
      "Xiu Li",
      "Ruiming Tang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.14577"
  },
  {
    "id": "arXiv:2210.14582",
    "title": "WebCrack: Dynamic Dictionary Adjustment for Web Weak Password Detection  based on Blasting Response Event Discrimination",
    "abstract": "The feature diversity of different web systems in page elements, submission\ncontents and return information makes it difficult to detect weak password\nautomatically. To solve this problem, multi-factor correlation detection method\nas integrated in the DBKER algorithm is proposed to achieve automatic detection\nof web weak passwords and universal passwords. It generates password\ndictionaries based on PCFG algorithm, proposes to judge blasting result via 4\nsteps with traditional static keyword features and dynamic page feature\ninformation. Then the blasting failure events are discriminated and the\nusernames are blasted based on response time. Thereafter the weak password\ndictionary is dynamically adjusted according to the hints provided by the\nresponse failure page. Based on the algorithm, this paper implements a\ndetection system named WebCrack. Experimental results of two blasting tests on\nDedeCMS and Discuz! systems as well as a random backend test show that the\nproposed method can detect weak passwords and universal passwords of various\nweb systems with an average accuracy rate of about 93.75%, providing security\nadvisories for users' password settings with strong practicability.",
    "descriptor": "\nComments: 22 pages, 6 figures, 4 tables\n",
    "authors": [
      "Xiang Long",
      "Yan Huang",
      "Zhendong Liu",
      "Lansheng Han",
      "Haili Sun",
      "Jingyuan He"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.14582"
  },
  {
    "id": "arXiv:2210.14583",
    "title": "ADR-Lite: A Low-Complexity Adaptive Data Rate Scheme for the LoRa  Network",
    "abstract": "The long-range and low energy consumption requirements in Internet of Things\n(IoT) applications have led to a new wireless communication technology known as\nLow Power Wide Area Network (LPWANs). In recent years, the Long Range (LoRa)\nprotocol has gained a lot of attention as one of the most promising\ntechnologies in LPWAN. Choosing the right combination of transmission\nparameters is a major challenge in the LoRa networks. In LoRa, an Adaptive Data\nRate (ADR) mechanism is executed to configure each End Device's (ED)\ntransmission parameters, resulting in improved performance metrics. In this\npaper, we propose a link-based ADR approach that aims to configure the\ntransmission parameters of EDs by making a decision without taking into account\nthe history of the last received packets, resulting in a relatively low space\ncomplexity approach. In this study, we present four different scenarios for\nassessing performance, including a scenario where mobile EDs are considered.\nOur simulation results show that in a mobile scenario with high channel noise,\nour proposed algorithm's Packet Delivery Ratio (PDR) is 2.8 times outperforming\nthe original ADR and 1.35 times that of other relevant algorithms.",
    "descriptor": "\nComments: 6 Pages, 6 Figures, 2022 18th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)\n",
    "authors": [
      "Reza Serati",
      "Benyamin Teymuri",
      "Nikolaos Athanasios Anagnostopoulos",
      "Mehdi Rasti"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14583"
  },
  {
    "id": "arXiv:2210.14584",
    "title": "Planning with Occluded Traffic Agents using Bi-Level Variational  Occlusion Models",
    "abstract": "Reasoning with occluded traffic agents is a significant open challenge for\nplanning for autonomous vehicles. Recent deep learning models have shown\nimpressive results for predicting occluded agents based on the behaviour of\nnearby visible agents; however, as we show in experiments, these models are\ndifficult to integrate into downstream planning. To this end, we propose\nBi-level Variational Occlusion Models (BiVO), a two-step generative model that\nfirst predicts likely locations of occluded agents, and then generates likely\ntrajectories for the occluded agents. In contrast to existing methods, BiVO\noutputs a trajectory distribution which can then be sampled from and integrated\ninto standard downstream planning. We evaluate the method in closed-loop replay\nsimulation using the real-world nuScenes dataset. Our results suggest that BiVO\ncan successfully learn to predict occluded agent trajectories, and these\npredictions lead to better subsequent motion plans in critical scenarios.",
    "descriptor": "\nComments: 7 pages, 6 figures\n",
    "authors": [
      "Filippos Christianos",
      "Peter Karkus",
      "Boris Ivanovic",
      "Stefano V. Albrecht",
      "Marco Pavone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14584"
  },
  {
    "id": "arXiv:2210.14593",
    "title": "Scaling Laws Beyond Backpropagation",
    "abstract": "Alternatives to backpropagation have long been studied to better understand\nhow biological brains may learn. Recently, they have also garnered interest as\na way to train neural networks more efficiently. By relaxing constraints\ninherent to backpropagation (e.g., symmetric feedforward and feedback weights,\nsequential updates), these methods enable promising prospects, such as local\nlearning. However, the tradeoffs between different methods in terms of final\ntask performance, convergence speed, and ultimately compute and data\nrequirements are rarely outlined. In this work, we use scaling laws to study\nthe ability of Direct Feedback Alignment~(DFA) to train causal decoder-only\nTransformers efficiently. Scaling laws provide an overview of the tradeoffs\nimplied by a modeling decision, up to extrapolating how it might transfer to\nincreasingly large models. We find that DFA fails to offer more efficient\nscaling than backpropagation: there is never a regime for which the degradation\nin loss incurred by using DFA is worth the potential reduction in compute\nbudget. Our finding comes at variance with previous beliefs in the alternative\ntraining methods community, and highlights the need for holistic empirical\napproaches to better understand modeling decisions.",
    "descriptor": "\nComments: I Can't Believe It's Not Better Workshop, NeurIPS 2022\n",
    "authors": [
      "Matthew J. Filipovich",
      "Alessandro Cappelli",
      "Daniel Hesslow",
      "Julien Launay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14593"
  },
  {
    "id": "arXiv:2210.14595",
    "title": "Safe and Efficient Switching Mechanism Design for Uncertified Linear  Controller",
    "abstract": "Sustained research efforts have been devoted to learning optimal controllers\nfor linear stochastic dynamical systems with unknown parameters, but due to the\ncorruption of noise, learned controllers are usually uncertified in the sense\nthat they may destabilize the system. To address this potential instability, we\npropose a \"plug-and-play\" modification to the uncertified controller which\nfalls back to a known stabilizing controller when the norm of the difference\nbetween the uncertified and the fall-back control input exceeds a certain\nthreshold. We show that the switching strategy is both safe and efficient, in\nthe sense that: 1) the linear-quadratic cost of the system is always bounded\neven if original uncertified controller is destabilizing; 2) in case the\nuncertified controller is stabilizing, the performance loss caused by switching\nconverges super-exponentially to $0$ for Gaussian noise, while the converging\npolynomially for general heavy-tailed noise. Finally, we demonstrate the\neffectiveness of the proposed switching strategy via numerical simulation on\nthe Tennessee Eastman Process.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2205.08817\n",
    "authors": [
      "Yiwen Lu",
      "Yilin Mo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14595"
  },
  {
    "id": "arXiv:2210.14596",
    "title": "Scaling Knowledge Graphs for Automating AI of Digital Twins",
    "abstract": "Digital Twins are digital representations of systems in the Internet of\nThings (IoT) that are often based on AI models that are trained on data from\nthose systems. Semantic models are used increasingly to link these datasets\nfrom different stages of the IoT systems life-cycle together and to\nautomatically configure the AI modelling pipelines. This combination of\nsemantic models with AI pipelines running on external datasets raises unique\nchallenges particular if rolled out at scale. Within this paper we will discuss\nthe unique requirements of applying semantic graphs to automate Digital Twins\nin different practical use cases. We will introduce the benchmark dataset DTBM\nthat reflects these characteristics and look into the scaling challenges of\ndifferent knowledge graph technologies. Based on these insights we will propose\na reference architecture that is in-use in multiple products in IBM and derive\nlessons learned for scaling knowledge graphs for configuring AI models for\nDigital Twins.",
    "descriptor": "\nComments: Accepted at ISWC 2022\n",
    "authors": [
      "Joern Ploennigs",
      "Konstantinos Semertzidis",
      "Fabio Lorenzi",
      "Nandana Mihindukulasooriya"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.14596"
  },
  {
    "id": "arXiv:2210.14599",
    "title": "RMLStreamer-SISO: an RDF stream generator from streaming heterogeneous  data",
    "abstract": "Stream-reasoning query languages such as CQELS and C-SPARQL enable query\nanswering over RDF streams. Unfortunately, there currently is a lack of\nefficient RDF stream generators to feed RDF stream reasoners. State-of-the-art\nRDF stream generators are limited with regard to the velocity and volume of\nstreaming data they can handle. To efficiently generate RDF streams in a\nscalable way, we extended the RMLStreamer to also generate RDF streams from\ndynamic heterogeneous data streams. This paper introduces a scalable solution\nthat relies on a dynamic window approach to generate RDF streams with low\nlatency and high throughput from multiple heterogeneous data streams. Our\nevaluation shows that our solution outperforms the state-of-the-art by\nachieving millisecond latency (compared to seconds that state-of-the-art\nsolutions need), constant memory usage for all workloads, and sustainable\nthroughput of around 70,000 records/s (compared to 10,000 records/s that\nstate-of-the-art solutions take). This opens up the access to numerous data\nstreams for integration with the semantic web.",
    "descriptor": "",
    "authors": [
      "Sitt Min Oo",
      "Gerald Haesendonck",
      "Ben De Meester",
      "Anastasia Dimou"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.14599"
  },
  {
    "id": "arXiv:2210.14600",
    "title": "MIMA -- Multifunctional IoT Integrated Menstrual Aid",
    "abstract": "Menstruation is the monthly shedding of the endometrium lining of a woman's\nuterus. The average age when girls start menstruating is around the age of 12\nyears (menarche), and the cycle continues until they attain menopause (about\nthe age of 51). Medical research and analysis in this field reveal that most\nwomen have to go through a painful cycle of abdominal cramps along with\nsanitary pad rashes, while painkillers or endurance ability are their go-to\nsolution. Heat pads or hot water bags also help in pain reduction. Currently,\nthe concept of period pants revolves around pad-free and hassle-free periods\nfor women, whereas most women still prefer sanitary pads during their period\ncycle. MIMA aims at the development of IoT integrated smart, functional\nintimate wear for women that would help women comfort during menstruation by\ncatering to issues of menstrual cramps, rashes, leakage and stains, malodor,\netc. The proposed methodology has been implemented by referring to the online\nsurvey conducted from Indian women (17-58 years old). MIMA can provide comfort\nduring the menstruation cycle with IoT integrated Heat-Pad and functional\nalterations in the garment for a rash-free, anti-odor, and leak-proof period.",
    "descriptor": "\nComments: 5 Pages, For associated code - this https URL, Project Website - this https URL\n",
    "authors": [
      "Jyothish Kumar J",
      "Subhankar Mishra",
      "Amish Bibhu",
      "Shreya Shivangi",
      "Sulagna Saha"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14600"
  },
  {
    "id": "arXiv:2210.14601",
    "title": "End-to-end Tracking with a Multi-query Transformer",
    "abstract": "Multiple-object tracking (MOT) is a challenging task that requires\nsimultaneous reasoning about location, appearance, and identity of the objects\nin the scene over time. Our aim in this paper is to move beyond\ntracking-by-detection approaches, that perform well on datasets where the\nobject classes are known, to class-agnostic tracking that performs well also\nfor unknown object classes.To this end, we make the following three\ncontributions: first, we introduce {\\em semantic detector queries} that enable\nan object to be localized by specifying its approximate position, or its\nappearance, or both; second, we use these queries within an auto-regressive\nframework for tracking, and propose a multi-query tracking transformer\n(\\textit{MQT}) model for simultaneous tracking and appearance-based\nre-identification (reID) based on the transformer architecture with deformable\nattention. This formulation allows the tracker to operate in a class-agnostic\nmanner, and the model can be trained end-to-end; finally, we demonstrate that\n\\textit{MQT} performs competitively on standard MOT benchmarks, outperforms all\nbaselines on generalised-MOT, and generalises well to a much harder tracking\nproblems such as tracking any object on the TAO dataset.",
    "descriptor": "",
    "authors": [
      "Bruno Korbar",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14601"
  },
  {
    "id": "arXiv:2210.14602",
    "title": "Audio Mosaicing with Simulation-based Inference",
    "abstract": "Mosaics and collages have been an integral part of art for decades.\nParticularly important in contemporary media art is the audio mosaic, in which\nan artist manually combines several audio sources in order to construct one\nsingle coherent sound, combining elements from disparate sources. Here we\npropose an algorithm to automatically create audio mosaics using the\nsimulation-based inference paradigm. Our algorithm takes as input an audio file\nthat one wishes to approximate, and a list of audio files one can use for\napproximation, finding a posterior distribution from which one can sample\nreconstructions of the original audio file, using the sources in an\ninterpretable and disentangled manner. We validate our approach by creating an\naudio mosaic which reconstructs the sound of a traditional Korean funeral using\n100 K-pop songs rearranged and overlapped.",
    "descriptor": "",
    "authors": [
      "Andrew Gambardella",
      "Youngjun Choi",
      "Doyo Choi",
      "Jinjoon Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.14602"
  },
  {
    "id": "arXiv:2210.14606",
    "title": "Analyzing Multi-Task Learning for Abstractive Text Summarization",
    "abstract": "Despite the recent success of multi-task learning and pre-finetuning for\nnatural language understanding, few works have studied the effects of task\nfamilies on abstractive text summarization. Task families are a form of task\ngrouping during the pre-finetuning stage to learn common skills, such as\nreading comprehension. To close this gap, we analyze the influence of\nmulti-task learning strategies using task families for the English abstractive\ntext summarization task. We group tasks into one of three strategies, i.e.,\nsequential, simultaneous, and continual multi-task learning, and evaluate\ntrained models through two downstream tasks. We find that certain combinations\nof task families (e.g., advanced reading comprehension and natural language\ninference) positively impact downstream performance. Further, we find that\nchoice and combinations of task families influence downstream performance more\nthan the training scheme, supporting the use of task families for abstractive\ntext summarization.",
    "descriptor": "",
    "authors": [
      "Frederic Kirstein",
      "Jan Philip Wahle",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14606"
  },
  {
    "id": "arXiv:2210.14607",
    "title": "A practical method for occupational skills detection in Vietnamese job  listings",
    "abstract": "Vietnamese labor market has been under an imbalanced development. The number\nof university graduates is growing, but so is the unemployment rate. This\nsituation is often caused by the lack of accurate and timely labor market\ninformation, which leads to skill miss-matches between worker supply and the\nactual market demands. To build a data monitoring and analytic platform for the\nlabor market, one of the main challenges is to be able to automatically detect\noccupational skills from labor-related data, such as resumes and job listings.\nTraditional approaches rely on existing taxonomy and/or large annotated data to\nbuild Named Entity Recognition (NER) models. They are expensive and require\nhuge manual efforts. In this paper, we propose a practical methodology for\nskill detection in Vietnamese job listings. Rather than viewing the task as a\nNER task, we consider the task as a ranking problem. We propose a pipeline in\nwhich phrases are first extracted and ranked in semantic similarity with the\nphrases' contexts. Then we employ a final classification to detect skill\nphrases. We collected three datasets and conducted extensive experiments. The\nresults demonstrated that our methodology achieved better performance than a\nNER model in scarce datasets.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Viet-Trung Tran",
      "Hai-Nam Cao",
      "Tuan-Dung Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14607"
  },
  {
    "id": "arXiv:2210.14609",
    "title": "A novel filter based on three variables mutual information for  dimensionality reduction and classification of hyperspectral images",
    "abstract": "The high dimensionality of hyperspectral images (HSI) that contains more than\nhundred bands (images) for the same region called Ground Truth Map, often\nimposes a heavy computational burden for image processing and complicates the\nlearning process. In fact, the removal of irrelevant, noisy and redundant bands\nhelps increase the classification accuracy. Band selection filter based on\n\"Mutual Information\" is a common technique for dimensionality reduction. In\nthis paper, a categorization of dimensionality reduction methods according to\nthe evaluation process is presented. Moreover, a new filter approach based on\nthree variables mutual information is developed in order to measure band\ncorrelation for classification, it considers not only bands relevance but also\nbands interaction. The proposed approach is compared to a reproduced filter\nalgorithm based on mutual information. Experimental results on HSI AVIRIS\n92AV3C have shown that the proposed approach is very competitive, effective and\noutperforms the reproduced filter strategy performance.\nKeywords - Hyperspectral images, Classification, band Selection, Three\nvariables Mutual Information, information gain.",
    "descriptor": "",
    "authors": [
      "Asma Elmaizi",
      "Elkebir Sarhrouni",
      "Ahmed hammouch",
      "Chafik Nacir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14609"
  },
  {
    "id": "arXiv:2210.14611",
    "title": "Automatic Diagnosis of Myocarditis Disease in Cardiac MRI Modality using  Deep Transformers and Explainable Artificial Intelligence",
    "abstract": "Myocarditis is among the most important cardiovascular diseases (CVDs),\nendangering the health of many individuals by damaging the myocardium. Microbes\nand viruses, such as HIV, play a vital role in myocarditis disease (MCD)\nincidence. Lack of MCD diagnosis in the early stages is associated with\nirreversible complications. Cardiac magnetic resonance imaging (CMRI) is highly\npopular among cardiologists to diagnose CVDs. In this paper, a deep learning\n(DL) based computer-aided diagnosis system (CADS) is presented for the\ndiagnosis of MCD using CMRI images. The proposed CADS includes dataset,\npreprocessing, feature extraction, classification, and post-processing steps.\nFirst, the Z-Alizadeh dataset was selected for the experiments. The\npreprocessing step included noise removal, image resizing, and data\naugmentation (DA). In this step, CutMix, and MixUp techniques were used for the\nDA. Then, the most recent pre-trained and transformers models were used for\nfeature extraction and classification using CMRI images. Our results show high\nperformance for the detection of MCD using transformer models compared with the\npre-trained architectures. Among the DL architectures, Turbulence Neural\nTransformer (TNT) architecture achieved an accuracy of 99.73% with 10-fold\ncross-validation strategy. Explainable-based Grad Cam method is used to\nvisualize the MCD suspected areas in CMRI images.",
    "descriptor": "",
    "authors": [
      "Mahboobeh Jafari",
      "Afshin Shoeibi",
      "Navid Ghassemi",
      "Jonathan Heras",
      "Abbas Khosravi",
      "Sai Ho Ling",
      "Roohallah Alizadehsani",
      "Amin Beheshti",
      "Yu-Dong Zhang",
      "Shui-Hua Wang",
      "Juan M. Gorriz",
      "U. Rajendra Acharya",
      "Hamid Alinejad Rokny"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14611"
  },
  {
    "id": "arXiv:2210.14612",
    "title": "Analyzing Deep Learning Representations of Point Clouds for Real-Time  In-Vehicle LiDAR Perception",
    "abstract": "LiDAR sensors are an integral part of modern autonomous vehicles as they\nprovide an accurate, high-resolution 3D representation of the vehicle's\nsurroundings. However, it is computationally difficult to make use of the\never-increasing amounts of data from multiple high-resolution LiDAR sensors. As\nframe-rates, point cloud sizes and sensor resolutions increase, real-time\nprocessing of these point clouds must still extract semantics from this\nincreasingly precise picture of the vehicle's environment. One deciding factor\nof the run-time performance and accuracy of deep neural networks operating on\nthese point clouds is the underlying data representation and the way it is\ncomputed. In this work, we examine the relationship between the computational\nrepresentations used in neural networks and their performance characteristics.\nTo this end, we propose a novel computational taxonomy of LiDAR point cloud\nrepresentations used in modern deep neural networks for 3D point cloud\nprocessing. Using this taxonomy, we perform a structured analysis of different\nfamilies of approaches. Thereby, we uncover common advantages and limitations\nin terms of computational efficiency, memory requirements, and representational\ncapacity as measured by semantic segmentation performance. Finally, we provide\nsome insights and guidance for future developments in neural point cloud\nprocessing methods.",
    "descriptor": "\nComments: Accepted at the NeurIPS 2022 Workshop on Machine Learning for Autonomous Driving (ML4AD)\n",
    "authors": [
      "Marc Uecker",
      "Tobias Fleck",
      "Marcel Pflugfelder",
      "J. Marius Z\u00f6llner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14612"
  },
  {
    "id": "arXiv:2210.14616",
    "title": "A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail",
    "abstract": "In recent years, spammers are now trying to obfuscate their intents by\nintroducing hybrid spam e-mail combining both image and text parts, which is\nmore challenging to detect in comparison to e-mails containing text or image\nonly. The motivation behind this research is to design an effective approach\nfiltering out hybrid spam e-mails to avoid situations where traditional\ntext-based or image-baesd only filters fail to detect hybrid spam e-mails. To\nthe best of our knowledge, a few studies have been conducted with the goal of\ndetecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR)\ntechnology is used to eliminate the image parts of spam by transforming images\ninto text. However, the research questions are that although OCR scanning is a\nvery successful technique in processing text-and-image hybrid spam, it is not\nan effective solution for dealing with huge quantities due to the CPU power\nrequired and the execution time it takes to scan e-mail files. And the OCR\ntechniques are not always reliable in the transformation processes. To address\nsuch problems, we propose new late multi-modal fusion training frameworks for a\ntext-and-image hybrid spam e-mail filtering system compared to the classical\nearly fusion detection frameworks based on the OCR method. Convolutional Neural\nNetwork (CNN) and Continuous Bag of Words were implemented to extract features\nfrom image and text parts of hybrid spam respectively, whereas generated\nfeatures were fed to sigmoid layer and Machine Learning based classifiers\nincluding Random Forest (RF), Decision Tree (DT), Naive Bayes (NB) and Support\nVector Machine (SVM) to determine the e-mail ham or spam.",
    "descriptor": "\nComments: Under review by the 17th International Conference on Information Security Practice and Experience (ISPEC 2022)\n",
    "authors": [
      "Zhibo Zhang",
      "Ernesto Damiani",
      "Hussam Al Hamadi",
      "Chan Yeob Yeun",
      "Fatma Taher"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14616"
  },
  {
    "id": "arXiv:2210.14618",
    "title": "SemFormer: Semantic Guided Activation Transformer for Weakly Supervised  Semantic Segmentation",
    "abstract": "Recent mainstream weakly supervised semantic segmentation (WSSS) approaches\nare mainly based on Class Activation Map (CAM) generated by a CNN\n(Convolutional Neural Network) based image classifier. In this paper, we\npropose a novel transformer-based framework, named Semantic Guided Activation\nTransformer (SemFormer), for WSSS. We design a transformer-based Class-Aware\nAutoEncoder (CAAE) to extract the class embeddings for the input image and\nlearn class semantics for all classes of the dataset. The class embeddings and\nlearned class semantics are then used to guide the generation of activation\nmaps with four losses, i.e., class-foreground, class-background, activation\nsuppression, and activation complementation loss. Experimental results show\nthat our SemFormer achieves \\textbf{74.3}\\% mIoU and surpasses many recent\nmainstream WSSS approaches by a large margin on PASCAL VOC 2012 dataset. Code\nwill be available at \\url{https://github.com/JLChen-C/SemFormer}.",
    "descriptor": "",
    "authors": [
      "Junliang Chen",
      "Xiaodong Zhao",
      "Cheng Luo",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14618"
  },
  {
    "id": "arXiv:2210.14619",
    "title": "Environment-Aware AUV Trajectory Design and Resource Management for  Multi-Tier Underwater Computing",
    "abstract": "The Internet of underwater things (IoUT) is envisioned to be an essential\npart of maritime activities. Given the IoUT devices' wide-area distribution and\nconstrained transmit power, autonomous underwater vehicles (AUVs) have been\nwidely adopted for collecting and forwarding the data sensed by IoUT devices to\nthe surface-stations. In order to accommodate the diverse requirements of IoUT\napplications, it is imperative to conceive a multi-tier underwater computing\n(MTUC) framework by carefully harnessing both the computing and the\ncommunications as well as the storage resources of both the surface-station and\nof the AUVs as well as of the IoUT devices. Furthermore, to meet the stringent\nenergy constraints of the IoUT devices and to reduce the operating cost of the\nMTUC framework, a joint environment-aware AUV trajectory design and resource\nmanagement problem is formulated, which is a high-dimensional NP-hard problem.\nTo tackle this challenge, we first transform the problem into a Markov decision\nprocess (MDP) and solve it with the aid of the asynchronous advantage\nactor-critic (A3C) algorithm. Our simulation results demonstrate the\nsuperiority of our scheme.",
    "descriptor": "\nComments: Accepted by IEEE Journal on Selected Areas in Communications\n",
    "authors": [
      "Xiangwang Hou",
      "Jingjing Wang",
      "Tong Bai",
      "Yansha Deng",
      "Yong Ren",
      "Lajos Hanzo"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14619"
  },
  {
    "id": "arXiv:2210.14621",
    "title": "A new band selection approach based on information theory and support  vector machine for hyperspectral images reduction and classification",
    "abstract": "The high dimensionality of hyperspectral images consisting of several bands\noften imposes a big computational challenge for image processing. Therefore,\nspectral band selection is an essential step for removing the irrelevant, noisy\nand redundant bands. Consequently increasing the classification accuracy.\nHowever, identification of useful bands from hundreds or even thousands of\nrelated bands is a nontrivial task. This paper aims at identifying a small set\nof highly discriminative bands, for improving computational speed and\nprediction accuracy. Hence, we proposed a new strategy based on joint mutual\ninformation to measure the statistical dependence and correlation between the\nselected bands and evaluate the relative utility of each one to classification.\nThe proposed filter approach is compared to an effective reproduced filters\nbased on mutual information. Simulations results on the hyperpectral image HSI\nAVIRIS 92AV3C using the SVM classifier have shown that the effective proposed\nalgorithm outperforms the reproduced filters strategy performance.\nKeywords-Hyperspectral images, Classification, band Selection, Joint Mutual\nInformation, dimensionality reduction ,correlation, SVM.",
    "descriptor": "",
    "authors": [
      "A. Elmaizi",
      "E. Sarhrouni",
      "A. Hammouch",
      "C. Nacir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14621"
  },
  {
    "id": "arXiv:2210.14622",
    "title": "DEMIS: A Threat Model for Selectively Encrypted Visual Surveillance Data",
    "abstract": "The monitoring of individuals/objects has become increasingly possible in\nrecent years due to the convenience of integrated cameras in many devices. Due\nto the important moments or activities of people captured by these devices, it\nhas made it a great asset for attackers to launch attacks against by exploiting\nthe weaknesses in these devices. Different studies proposed na\\\"ive/selective\nencryption of the captured visual data for safety but despite the encryption,\nan attacker can still access or manipulate such data. This paper proposed a\nnovel threat model, DEMIS which helps analyse the threats against such\nencrypted videos. The paper also examines the attack vectors that can be used\nfor threats and the mitigation that will reduce or prevent the attack. For\nexperiments, firstly the data set is generated by applying selective encryption\non the Regions-of-interests (ROI) of the tested videos using the image\nsegmentation technique and Chacha20 cipher. Secondly, different types of\nattacks, such as inverse, lowercase, uppercase, random insertion, and\nmalleability attacks were simulated in experiments to show the effects of the\nattacks, the risk matrix, and the severity of these attacks. Our developed data\nset with the original, selective encrypted, and attacked videos are available\non git-repository(https://github.com/Ifeoluwapoo/video-datasets) for future\nresearchers.",
    "descriptor": "\nComments: 17 pages, 7 figures, 7 tables\n",
    "authors": [
      "Ifeoluwapo Aribilola",
      "Mamoona Naveed Asghar",
      "Brian Lee"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14622"
  },
  {
    "id": "arXiv:2210.14624",
    "title": "RapidAI4EO: Mono- and Multi-temporal Deep Learning models for Updating  the CORINE Land Cover Product",
    "abstract": "In the remote sensing community, Land Use Land Cover (LULC) classification\nwith satellite imagery is a main focus of current research activities. Accurate\nand appropriate LULC classification, however, continues to be a challenging\ntask. In this paper, we evaluate the performance of multi-temporal (monthly\ntime series) compared to mono-temporal (single time step) satellite images for\nmulti-label classification using supervised learning on the RapidAI4EO dataset.\nAs a first step, we trained our CNN model on images at a single time step for\nmulti-label classification, i.e. mono-temporal. We incorporated time-series\nimages using a LSTM model to assess whether or not multi-temporal signals from\nsatellites improves CLC classification. The results demonstrate an improvement\nof approximately 0.89% in classifying satellite imagery on 15 classes using a\nmulti-temporal approach on monthly time series images compared to the\nmono-temporal approach. Using features from multi-temporal or mono-temporal\nimages, this work is a step towards an efficient change detection and land\nmonitoring approach.",
    "descriptor": "\nComments: Published in IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium\n",
    "authors": [
      "Priyash Bhugra",
      "Benjamin Bischke",
      "Christoph Werner",
      "Robert Syrnicki",
      "Carolin Packbier",
      "Patrick Helber",
      "Caglar Senaras",
      "Akhil Singh Rana",
      "Tim Davis",
      "Wanda De Keersmaecker",
      "Daniele Zanaga",
      "Annett Wania",
      "Ruben Van De Kerchove",
      "Giovanni Marchisio"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14624"
  },
  {
    "id": "arXiv:2210.14627",
    "title": "Channel-Aware Ordered Successive Relaying with Finite-Blocklength Coding",
    "abstract": "Successive relaying can improve the transmission rate by allowing the source\nand relays to transmit messages simultaneously, but it may cause severe\ninter-relay interference (IRI). IRI cancellation schemes have been proposed to\nmitigate IRI. However, interference cancellation methods have a high risk of\nerror propagation, resulting in a severe transmission rate loss in finite\nblocklength regimes. Thus, jointly decoding for successive relaying with\nfinite-blocklength coding (FBC) remains a challenge. In this paper, we present\nan optimized channel-aware ordered successive relaying protocol with\nfinite-blocklength coding (CAO-SIR-FBC), which can recover the rate loss by\ncarefully adapting the relay transmission order and rate. We analyze the\naverage throughput of the CAO-SIR-FBC method, based on which a closed-form\nexpression in a high signal-to-noise regime (SNR) is presented. Average\nthroughput analysis and simulations show that CAO-SIR-FBC outperforms\nconventional two-timeslot half-duplex relaying in terms of spectral efficiency.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Lingrui Zhang",
      "Yuxing Han",
      "Qiong Wang",
      "Wei Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14627"
  },
  {
    "id": "arXiv:2210.14628",
    "title": "Provable Sample-Efficient Sparse Phase Retrieval Initialized by  Truncated Power Method",
    "abstract": "We study the sparse phase retrieval problem, recovering an $s$-sparse\nlength-$n$ signal from $m$ magnitude-only measurements. Two-stage non-convex\napproaches have drawn much attention in recent studies for this problem.\nDespite non-convexity, many two-stage algorithms provably converge to the\nunderlying solution linearly when appropriately initialized. However, in terms\nof sample complexity, the bottleneck of those algorithms often comes from the\ninitialization stage. Although the refinement stage usually needs only\n$m=\\Omega(s\\log n)$ measurements, the widely used spectral initialization in\nthe initialization stage requires $m=\\Omega(s^2\\log n)$ measurements to produce\na desired initial guess, which causes the total sample complexity order-wisely\nmore than necessary. To reduce the number of measurements, we propose a\ntruncated power method to replace the spectral initialization for non-convex\nsparse phase retrieval algorithms. We prove that $m=\\Omega(\\bar{s} s\\log n)$\nmeasurements, where $\\bar{s}$ is the stable sparsity of the underlying signal,\nare sufficient to produce a desired initial guess. When the underlying signal\ncontains only very few significant components, the sample complexity of the\nproposed algorithm is $m=\\Omega(s\\log n)$ and optimal. Numerical experiments\nillustrate that the proposed method is more sample-efficient than\nstate-of-the-art algorithms.",
    "descriptor": "",
    "authors": [
      "Jian-Feng Cai",
      "Jingyang Li",
      "Juntao You"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14628"
  },
  {
    "id": "arXiv:2210.14632",
    "title": "Cover Reproducible Steganography via Deep Generative Models",
    "abstract": "Whereas cryptography easily arouses attacks by means of encrypting a secret\nmessage into a suspicious form, steganography is advantageous for its\nresilience to attacks by concealing the message in an innocent-looking cover\nsignal. Minimal distortion steganography, one of the mainstream steganography\nframeworks, embeds messages while minimizing the distortion caused by the\nmodification on the cover elements. Due to the unavailability of the original\ncover signal for the receiver, message embedding is realized by finding the\ncoset leader of the syndrome function of steganographic codes migrated from\nchannel coding, which is complex and has limited performance. Fortunately, deep\ngenerative models and the robust semantic of generated data make it possible\nfor the receiver to perfectly reproduce the cover signal from the stego signal.\nWith this advantage, we propose cover-reproducible steganography where the\nsource coding, e.g., arithmetic coding, serves as the steganographic code.\nSpecifically, the decoding process of arithmetic coding is used for message\nembedding and its encoding process is regarded as message extraction. Taking\ntext-to-speech and text-to-image synthesis tasks as two examples, we illustrate\nthe feasibility of cover-reproducible steganography. Steganalysis experiments\nand theoretical analysis are conducted to demonstrate that the proposed methods\noutperform the existing methods in most cases.",
    "descriptor": "\nComments: Accepted by IEEE TDSC\n",
    "authors": [
      "Kejiang Chen",
      "Hang Zhou",
      "Yaofei Wang",
      "Menghan Li",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.14632"
  },
  {
    "id": "arXiv:2210.14636",
    "title": "Fast Yet Effective Speech Emotion Recognition with Self-distillation",
    "abstract": "Speech emotion recognition (SER) is the task of recognising human's emotional\nstates from speech. SER is extremely prevalent in helping dialogue systems to\ntruly understand our emotions and become a trustworthy human conversational\npartner. Due to the lengthy nature of speech, SER also suffers from the lack of\nabundant labelled data for powerful models like deep neural networks.\nPre-trained complex models on large-scale speech datasets have been\nsuccessfully applied to SER via transfer learning. However, fine-tuning complex\nmodels still requires large memory space and results in low inference\nefficiency. In this paper, we argue achieving a fast yet effective SER is\npossible with self-distillation, a method of simultaneously fine-tuning a\npretrained model and training shallower versions of itself. The benefits of our\nself-distillation framework are threefold: (1) the adoption of\nself-distillation method upon the acoustic modality breaks through the limited\nground-truth of speech data, and outperforms the existing models' performance\non an SER dataset; (2) executing powerful models at different depth can achieve\nadaptive accuracy-efficiency trade-offs on resource-limited edge devices; (3) a\nnew fine-tuning process rather than training from scratch for self-distillation\nleads to faster learning time and the state-of-the-art accuracy on data with\nsmall quantities of label information.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Zhao Ren",
      "Thanh Tam Nguyen",
      "Yi Chang",
      "Bj\u00f6rn W. Schuller"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14636"
  },
  {
    "id": "arXiv:2210.14638",
    "title": "Fixed-parameter tractability of Graph Isomorphism in graphs with an  excluded minor",
    "abstract": "We prove that Graph Isomorphism and Canonization in graphs excluding a fixed\ngraph $H$ as a minor can be solved by an algorithm working in time $f(H)\\cdot\nn^{O(1)}$, where $f$ is some function. In other words, we show that these\nproblems are fixed-parameter tractable when parameterized by the size of the\nexcluded minor, with the caveat that the bound on the running time is not\nnecessarily computable. The underlying approach is based on decomposing the\ngraph in a canonical way into unbreakable (intuitively, well-connected) parts,\nwhich essentially provides a reduction to the case where the given\n$H$-minor-free graph is unbreakable itself. This is complemented by an analysis\nof unbreakable $H$-minor-free graphs, performed in a second subordinate\nmanuscript, which reveals that every such graph can be canonically decomposed\ninto a part that admits few automorphisms and a part that has bounded\ntreewidth.",
    "descriptor": "\nComments: Part I of a full version of a paper accepted at STOC 2022\n",
    "authors": [
      "Daniel Lokshtanov",
      "Marcin Pilipczuk",
      "Micha\u0142 Pilipczuk",
      "Saket Saurabh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.14638"
  },
  {
    "id": "arXiv:2210.14640",
    "title": "HSVI can solve zero-sum Partially Observable Stochastic Games",
    "abstract": "State-of-the-art methods for solving 2-player zero-sum imperfect information\ngames rely on linear programming or regret minimization, though not on dynamic\nprogramming (DP) or heuristic search (HS), while the latter are often at the\ncore of state-of-the-art solvers for other sequential decision-making problems.\nIn partially observable or collaborative settings (e.g., POMDPs and Dec-\nPOMDPs), DP and HS require introducing an appropriate statistic that induces a\nfully observable problem as well as bounding (convex) approximators of the\noptimal value function. This approach has succeeded in some subclasses of\n2-player zero-sum partially observable stochastic games (zs- POSGs) as well,\nbut how to apply it in the general case still remains an open question. We\nanswer it by (i) rigorously defining an equivalent game to work with, (ii)\nproving mathematical properties of the optimal value function that allow\nderiving bounds that come with solution strategies, (iii) proposing for the\nfirst time an HSVI-like solver that provably converges to an $\\epsilon$-optimal\nsolution in finite time, and (iv) empirically analyzing it. This opens the door\nto a novel family of promising approaches complementing those relying on linear\nprogramming or iterative methods.",
    "descriptor": "\nComments: 42 pages, 2 algorithms. arXiv admin note: substantial text overlap with arXiv:2110.14529\n",
    "authors": [
      "Aur\u00e9lien Delage",
      "Olivier Buffet",
      "Jilles S. Dibangoye",
      "Abdallah Saffidine"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14640"
  },
  {
    "id": "arXiv:2210.14644",
    "title": "Speaker Diarization Based on Multi-channel Microphone Array in  Small-scale Meeting",
    "abstract": "In the task of speaker diarization, the number of small-scale meetings\naccounts for a large proportion. When microphone arrays are employed as a\nrecording device, its spatial information is usually ignored by most\nresearchers. In this paper, inspired by the clustering method combining\nd-vector and microphone array spatial vector, we proposed a diarization method\nwhich using multi-channel microphone arrays for a meeting with no more than 4\nspeakers. We utilize speech enhancement to preprocess the audio from the\nmicrophone array. The Steered-Response Power Phase Transform (SRP-PHAT)\nalgorithm are employed to get more accurate speakers, and apply the number of\nspeakers to recluster the speech segments to achieve better performance.\nFinally, we fuse our system by DOVER-LAP to get the best result. We evaluated\nour system on the AMI corpus. Compared with the best experimental results so\nfar, our system has achieved largely improvement in the diarization error rate\n(DER).",
    "descriptor": "",
    "authors": [
      "Yuxuan Du",
      "Ruohua Zhou"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14644"
  },
  {
    "id": "arXiv:2210.14646",
    "title": "Singularity-free Formation Path Following of Underactuated AUVs:  Extended Version",
    "abstract": "This paper proposes a method for formation path following control of a fleet\nof underactuated autonomous underwater vehicles. The proposed method combines\nseveral hierarchic tasks in a null space-based behavioral algorithm to safely\nguide the vehicles. Compared to the existing literature, the algorithm includes\nboth inter-vehicle and obstacle collision avoidance, and employs a scheme that\nkeeps the vehicles within given operation limits. The algorithm is applied to a\nsix degree-of-freedom model, using rotation matrices to describe the attitude\nto avoid singularities. Using the results of cascaded systems theory, we prove\nthat the closed-loop system is uniformly semiglobally exponentially stable. We\nuse numerical simulations to validate the results.",
    "descriptor": "\nComments: Extended version of a paper submitted to 2023 IFAC World Congress, 13 pages (9p + 4p appendices), 5 figures\n",
    "authors": [
      "Josef Matou\u0161",
      "Kristin Y. Pettersen",
      "Damiano Varagnolo",
      "Claudio Paliotta"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14646"
  },
  {
    "id": "arXiv:2210.14649",
    "title": "Higher-Order MSL Horn Constraints",
    "abstract": "The monadic shallow linear (MSL) class is a decidable fragment of first-order\nHorn clauses that was discovered and rediscovered around the turn of the\ncentury, with applications in static analysis and verification. We propose a\nnew class of higher-order Horn constraints which extend MSL to higher-order\nlogic and develop a resolution-based decision procedure. Higher-order MSL Horn\nconstraints can quite naturally capture the complex patterns of call and return\nthat are possible in higher-order programs, which make them well suited to\nhigher-order program verification. In fact, we show that the higher-order MSL\nsatisfiability problem and the HORS model checking problem are interreducible,\nso that higher-order MSL can be seen as a constraint-based approach to\nhigher-order model checking. Finally, we describe an implementation of our\ndecision procedure and its application to verified socket programming.",
    "descriptor": "\nComments: Revision of conditionally accepted submission to POPL 2023\n",
    "authors": [
      "Jerome Jochems",
      "Eddie Jones",
      "Steven Ramsay"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.14649"
  },
  {
    "id": "arXiv:2210.14650",
    "title": "MOCHA: A Multi-Task Training Approach for Coherent Text Generation from  Cognitive Perspective",
    "abstract": "Teaching neural models to generate narrative coherent texts is a critical\nproblem. Recent pre-trained language models have achieved promising results,\nbut there is still a gap between human written texts and machine-generated\noutputs. In this work, we propose a novel multi-task training strategy for\ncoherent text generation grounded on the cognitive theory of writing, which\nempowers the model to learn essential subskills needed for writing including\nplanning and reviewing besides end-to-end generation. We extensively evaluate\nour model on three open-ended generation tasks including story generation, news\narticle writing and argument generation. Experiments show that our model\nachieves better results on both few-shot and fully-supervised settings than\nstrong baselines, and human evaluations confirm that our model can generate\nmore coherent outputs.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Zhe Hu",
      "Hou Pong Chan",
      "Lifu Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14650"
  },
  {
    "id": "arXiv:2210.14653",
    "title": "TSUP Speaker Diarization System for Conversational Short-phrase Speaker  Diarization Challenge",
    "abstract": "This paper describes the TSUP team's submission to the ISCSLP 2022\nconversational short-phrase speaker diarization (CSSD) challenge which\nparticularly focuses on short-phrase conversations with a new evaluation metric\ncalled conversational diarization error rate (CDER). In this challenge, we\nexplore three kinds of typical speaker diarization systems, which are spectral\nclustering(SC) based diarization, target-speaker voice activity\ndetection(TS-VAD) and end-to-end neural diarization(EEND) respectively. Our\nmajor findings are summarized as follows. First, the SC approach is more\nfavored over the other two approaches under the new CDER metric. Second, tuning\non hyperparameters is essential to CDER for all three types of speaker\ndiarization systems. Specifically, CDER becomes smaller when the length of\nsub-segments setting longer. Finally, multi-system fusion through DOVER-LAP\nwill worsen the CDER metric on the challenge data. Our submitted SC system\neventually ranks the third place in the challenge.",
    "descriptor": "",
    "authors": [
      "Bowen Pang",
      "Huan Zhao",
      "Gaosheng Zhang",
      "Xiaoyue Yang",
      "Yang Sun",
      "Li Zhang",
      "Qing Wang",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14653"
  },
  {
    "id": "arXiv:2210.14657",
    "title": "Multi-Objective Hardware-Mapping Co-Optimisation for Multi-Tenant DNN  Accelerators",
    "abstract": "To meet the ever-increasing computation demand from emerging workloads, a\nscalable design paradigm combines multiple Deep Neural Network (DNN)\naccelerators to build a large multi-accelerator system. They are mainly\nproposed for data centers, where workload varies across vision, language,\nrecommendation, etc. Existing works independently explore their hardware\nconfiguration and mapping strategies due to the extremely large cross-coupled\ndesign space. However, hardware and mapping are interdependent and, if not\nexplored together, may lead to sub-optimal performance when workload changes.\nMoreover, even though a data center accelerator has multiple objectives, almost\nall the existing works prefer aggregating them into one (mono-objective). But\naggregation does not help if the objectives are conflicting, as improving one\nwill worsen the other.\nThis work proposes MOHaM, a multi-objective hardware-mapping co-optimisation\nframework for multi-tenant DNN accelerators. Specifically, given an application\nmodel and a library of heterogeneous, parameterised and reconfigurable\nsub-accelerator templates, MOHaM returns a Pareto-optimal set of\nmulti-accelerator systems with an optimal schedule for each one of them to\nminimise the overall system latency, energy and area. MOHaM is evaluated for\ndiverse workload scenarios with state-of-the-art sub-accelerators. The\nPareto-optimal set of competitive design choices enables selecting the best one\nas per the requirement.",
    "descriptor": "",
    "authors": [
      "Abhijit Das",
      "Enrico Russo",
      "Maurizio Palesi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.14657"
  },
  {
    "id": "arXiv:2210.14660",
    "title": "Simulated Bifurcation Algorithm for MIMO Detection",
    "abstract": "We study the performance of the simulated bifurcation (SB) algorithm for\nsignal detection in multiple-input multiple-output (MIMO) system, a problem of\nkey interest in modern wireless communication systems. Our results show that SB\nalgorithm can achieve significant performance improvement over the widely used\nlinear minimum-mean square error decoder in terms of the bit error rate versus\nthe signal-to-noise ratio, as well as performance improvement over the coherent\nIsing machine based MIMO detection method.",
    "descriptor": "\nComments: 4pages, 2figures\n",
    "authors": [
      "Wen Zhang",
      "Yu-Lin Zheng"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14660"
  },
  {
    "id": "arXiv:2210.14661",
    "title": "Full-band General Audio Synthesis with Score-based Diffusion",
    "abstract": "Recent works have shown the capability of deep generative models to tackle\ngeneral audio synthesis from a single label, producing a variety of impulsive,\ntonal, and environmental sounds. Such models operate on band-limited signals\nand, as a result of an autoregressive approach, they are typically conformed by\npre-trained latent encoders and/or several cascaded modules. In this work, we\npropose a diffusion-based generative model for general audio synthesis, named\nDAG, which deals with full-band signals end-to-end in the waveform domain.\nResults show the superiority of DAG over existing label-conditioned generators\nin terms of both quality and diversity. More specifically, when compared to the\nstate of the art, the band-limited and full-band versions of DAG achieve\nrelative improvements that go up to 40 and 65%, respectively. We believe DAG is\nflexible enough to accommodate different conditioning schemas while providing\ngood quality synthesis.",
    "descriptor": "",
    "authors": [
      "Santiago Pascual",
      "Gautam Bhattacharya",
      "Chunghsin Yeh",
      "Jordi Pons",
      "Joan Serr\u00e0"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14661"
  },
  {
    "id": "arXiv:2210.14664",
    "title": "Coresets for Vertical Federated Learning: Regularized Linear Regression  and $K$-Means Clustering",
    "abstract": "Vertical federated learning (VFL), where data features are stored in multiple\nparties distributively, is an important area in machine learning. However, the\ncommunication complexity for VFL is typically very high. In this paper, we\npropose a unified framework by constructing coresets in a distributed fashion\nfor communication-efficient VFL. We study two important learning tasks in the\nVFL setting: regularized linear regression and $k$-means clustering, and apply\nour coreset framework to both problems. We theoretically show that using\ncoresets can drastically alleviate the communication complexity, while nearly\nmaintain the solution quality. Numerical experiments are conducted to\ncorroborate our theoretical findings.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022, 32 pages\n",
    "authors": [
      "Lingxiao Huang",
      "Zhize Li",
      "Jialin Sun",
      "Haoyu Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.14664"
  },
  {
    "id": "arXiv:2210.14665",
    "title": "Desiderata for next generation of ML model serving",
    "abstract": "Inference is a significant part of ML software infrastructure. Despite the\nvariety of inference frameworks available, the field as a whole can be\nconsidered in its early days. This paper puts forth a range of important\nqualities that next generation of inference platforms should be aiming for. We\npresent our rationale for the importance of each quality, and discuss ways to\nachieve it in practice. An overarching design pattern is data-centricity, which\nenables smarter monitoring in ML system operation.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 Workshop on Challenges in Deploying and Monitoring Machine Learning Systems\n",
    "authors": [
      "Sherif Akoush",
      "Andrei Paleyes",
      "Arnaud Van Looveren",
      "Clive Cox"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14665"
  },
  {
    "id": "arXiv:2210.14667",
    "title": "A Bilingual Parallel Corpus with Discourse Annotations",
    "abstract": "Machine translation (MT) has almost achieved human parity at sentence-level\ntranslation. In response, the MT community has, in part, shifted its focus to\ndocument-level translation. However, the development of document-level MT\nsystems is hampered by the lack of parallel document corpora. This paper\ndescribes BWB, a large parallel corpus first introduced in Jiang et al. (2022),\nalong with an annotated test set. The BWB corpus consists of Chinese novels\ntranslated by experts into English, and the annotated test set is designed to\nprobe the ability of machine translation systems to model various discourse\nphenomena. Our resource is freely available, and we hope it will serve as a\nguide and inspiration for more work in document-level machine translation.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Yuchen Eleanor Jiang",
      "Tianyu Liu",
      "Shuming Ma",
      "Dongdong Zhang",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14667"
  },
  {
    "id": "arXiv:2210.14670",
    "title": "Boosting Semi-Supervised Semantic Segmentation with Probabilistic  Representations",
    "abstract": "Recent breakthroughs in semi-supervised semantic segmentation have been\ndeveloped through contrastive learning. In prevalent pixel-wise contrastive\nlearning solutions, the model maps pixels to deterministic representations and\nregularizes them in the latent space. However, there exist inaccurate\npseudo-labels which map the ambiguous representations of pixels to the wrong\nclasses due to the limited cognitive ability of the model. In this paper, we\ndefine pixel-wise representations from a new perspective of probability theory\nand propose a Probabilistic Representation Contrastive Learning (PRCL)\nframework that improves representation quality by taking its probability into\nconsideration. Through modeling the mapping from pixels to representations as\nthe probability via multivariate Gaussian distributions, we can tune the\ncontribution of the ambiguous representations to tolerate the risk of\ninaccurate pseudo-labels. Furthermore, we define prototypes in the form of\ndistributions, which indicates the confidence of a class, while the point\nprototype cannot. Moreover, we propose to regularize the distribution variance\nto enhance the reliability of representations. Taking advantage of these\nbenefits, high-quality feature representations can be derived in the latent\nspace, thereby the performance of semantic segmentation can be further\nimproved. We conduct sufficient experiment to evaluate PRCL on Pascal VOC and\nCityScapes. The comparisons with state-of-the-art approaches demonstrate the\nsuperiority of proposed PRCL.",
    "descriptor": "",
    "authors": [
      "Haoyu Xie",
      "Changqi Wang",
      "Mingkai Zheng",
      "Minjing Dong",
      "Shan You",
      "Chang Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14670"
  },
  {
    "id": "arXiv:2210.14672",
    "title": "Sparsity in Continuous-Depth Neural Networks",
    "abstract": "Neural Ordinary Differential Equations (NODEs) have proven successful in\nlearning dynamical systems in terms of accurately recovering the observed\ntrajectories. While different types of sparsity have been proposed to improve\nrobustness, the generalization properties of NODEs for dynamical systems beyond\nthe observed data are underexplored. We systematically study the influence of\nweight and feature sparsity on forecasting as well as on identifying the\nunderlying dynamical laws. Besides assessing existing methods, we propose a\nregularization technique to sparsify \"input-output connections\" and extract\nrelevant features during training. Moreover, we curate real-world datasets\nconsisting of human motion capture and human hematopoiesis single-cell RNA-seq\ndata to realistically analyze different levels of out-of-distribution (OOD)\ngeneralization in forecasting and dynamics identification respectively. Our\nextensive empirical evaluation on these challenging benchmarks suggests that\nweight sparsity improves generalization in the presence of noise or irregular\nsampling. However, it does not prevent learning spurious feature dependencies\nin the inferred dynamics, rendering them impractical for predictions under\ninterventions, or for inferring the true underlying dynamics. Instead, feature\nsparsity can indeed help with recovering sparse ground-truth dynamics compared\nto unregularized NODEs.",
    "descriptor": "\nComments: Neurips 2022\n",
    "authors": [
      "Hananeh Aliee",
      "Till Richter",
      "Mikhail Solonin",
      "Ignacio Ibarra",
      "Fabian Theis",
      "Niki Kilbertus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14672"
  },
  {
    "id": "arXiv:2210.14675",
    "title": "Comparison of neural closure models for discretised PDEs",
    "abstract": "Neural closure models have recently been proposed as a method for efficiently\napproximating small scales in multiscale systems with neural networks. The\nchoice of loss function and associated training procedure has a large effect on\nthe accuracy and stability of the resulting neural closure model. In this work,\nwe systematically compare three distinct procedures: \"derivative fitting\",\n\"trajectory fitting\" with discretise-then-optimise, and \"trajectory fitting\"\nwith optimise-then-discretise. Derivative fitting is conceptually the simplest\nand computationally the most efficient approach and is found to perform\nreasonably well on one of the test problems (Kuramoto-Sivashinsky) but poorly\non the other (Burgers). Trajectory fitting is computationally more expensive\nbut is more robust and is therefore the preferred approach. Of the two\ntrajectory fitting procedures, the discretise-then-optimise approach produces\nmore accurate models than the optimise-then-discretise approach. While the\noptimise-then-discretise approach can still produce accurate models, care must\nbe taken in choosing the length of the trajectories used for training, in order\nto train the models on long-term behaviour while still producing reasonably\naccurate gradients during training. Two existing theorems are interpreted in a\nnovel way that gives insight into the long-term accuracy of a neural closure\nmodel based on how accurate it is in the short term.",
    "descriptor": "\nComments: 24 pages and 9 figures. Submitted to Computers and Mathematics with Applications. For associated code, see this https URL\n",
    "authors": [
      "Hugo Melchers",
      "Daan Crommelin",
      "Barry Koren",
      "Vlado Menkovski",
      "Benjamin Sanderse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14675"
  },
  {
    "id": "arXiv:2210.14677",
    "title": "How precise are performance estimates for typical medical image  segmentation tasks?",
    "abstract": "An important issue in medical image processing is to be able to estimate not\nonly the performances of algorithms but also the precision of the estimation of\nthese performances. Reporting precision typically amounts to reporting\nstandard-error of the mean (SEM) or equivalently confidence intervals. However,\nthis is rarely done in medical image segmentation studies. In this paper, we\naim to estimate what is the typical confidence that can be expected in such\nstudies. To that end, we first perform experiments for Dice metric estimation\nusing a standard deep learning model (U-net) and a classical task from the\nMedical Segmentation Decathlon. We extensively study precision estimation using\nboth Gaussian assumption and bootstrapping (which does not require any\nassumption on the distribution). We then perform simulations for other test set\nsizes and performance spreads. Overall, our work shows that small test sets\nlead to wide confidence intervals ($\\sim$6 points of Dice for 20 samples) and\nthat, in order to obtain a confidence interval narrower than 2, it is necessary\nto have at least 200 test samples.",
    "descriptor": "",
    "authors": [
      "Rosana El Jurdi",
      "Olivier Colliot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Quantitative Methods (q-bio.QM)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.14677"
  },
  {
    "id": "arXiv:2210.14678",
    "title": "Investigating the Role of Centering Theory in the Context of Neural  Coreference Resolution Systems",
    "abstract": "Centering theory (CT; Grosz et al., 1995) provides a linguistic analysis of\nthe structure of discourse. According to the theory, local coherence of\ndiscourse arises from the manner and extent to which successive utterances make\nreference to the same entities. In this paper, we investigate the connection\nbetween centering theory and modern coreference resolution systems. We provide\nan operationalization of centering and systematically investigate if neural\ncoreference resolvers adhere to the rules of centering theory by defining\nvarious discourse metrics and developing a search-based methodology. Our\ninformation-theoretic analysis reveals a positive dependence between\ncoreference and centering; but also shows that high-quality neural coreference\nresolvers may not benefit much from explicitly modeling centering ideas. Our\nanalysis further shows that contextualized embeddings contain much of the\ncoherence information, which helps explain why CT can only provide little gains\nto modern neural coreference resolvers which make use of pretrained\nrepresentations. Finally, we discuss factors that contribute to coreference\nwhich are not modeled by CT such as world knowledge and recency bias. We\nformulate a version of CT that also models recency and show that it captures\ncoreference information better compared to vanilla CT.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Yuchen Eleanor Jiang",
      "Ryan Cotterell",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14678"
  },
  {
    "id": "arXiv:2210.14679",
    "title": "On the epidemic threshold of a network",
    "abstract": "The graph invariant examined in this paper is the largest eigenvalue of the\nadjacency matrix of a graph. Previous work demonstrates the tight relationship\nbetween this invariant, the birth and death rate of a contagion spreading on\nthe graph, and the trajectory of the contagion over time. We begin by\nconducting a simulation confirming this and explore bounds on the birth and\ndeath rate in terms of well-known graph invariants. As a result, the change in\nthe largest eigenvalue resulting from removal of a vertex in the network is the\nbest measure of effectiveness of interventions that slow the spread of a\ncontagion. We define the spread centrality of a vertex $v$ in a graph $G$ as\nthe difference between the largest eigenvalues of $G$ and $G-v$. While the\nspread centrality is a distinct centrality measure and serves as another graph\ninvariant for distinguishing graphs, we found experimental evidence that\nvertices ranked by the spread centrality and those ranked by eigenvector\ncentrality are strongly correlated. Since eigenvector centrality is easier to\ncompute than the spread centrality, this justifies using eigenvector centrality\nas a measure of spread, especially in large networks with unknown portions. We\nalso examine two strategies for selecting members of a population to vaccinate.",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "V. Cherniavskyi",
      "G. Dennis",
      "S. R. Kingan"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.14679"
  },
  {
    "id": "arXiv:2210.14680",
    "title": "An adaptive finite element/finite difference domain decomposition method  for applications in microwave imaging",
    "abstract": "A new domain decomposition method for Maxwell's equations in conductive media\nis presented. Using this method reconstruction algorithms are developed for\ndetermination of dielectric permittivity function using time-dependent\nscattered data of electric field. All reconstruction algorithms are based on\noptimization approach to find stationary point of the Lagrangian. Adaptive\nreconstruction algorithms and space mesh refinement indicators are also\npresented. Our computational tests show qualitative reconstruction of\ndielectric permittivity function using anatomically realistic breast phantom.",
    "descriptor": "",
    "authors": [
      "Larisa Beilina",
      "Eric Lindstr\u00f6m"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14680"
  },
  {
    "id": "arXiv:2210.14682",
    "title": "In search of strong embedding extractors for speaker diarisation",
    "abstract": "Speaker embedding extractors (EEs), which map input audio to a speaker\ndiscriminant latent space, are of paramount importance in speaker diarisation.\nHowever, there are several challenges when adopting EEs for diarisation, from\nwhich we tackle two key problems. First, the evaluation is not straightforward\nbecause the features required for better performance differ between speaker\nverification and diarisation. We show that better performance on widely adopted\nspeaker verification evaluation protocols does not lead to better diarisation\nperformance. Second, embedding extractors have not seen utterances in which\nmultiple speakers exist. These inputs are inevitably present in speaker\ndiarisation because of overlapped speech and speaker changes; they degrade the\nperformance. To mitigate the first problem, we generate speaker verification\nevaluation protocols that mimic the diarisation scenario better. We propose two\ndata augmentation techniques to alleviate the second problem, making embedding\nextractors aware of overlapped speech or speaker change input. One technique\ngenerates overlapped speech segments, and the other generates segments where\ntwo speakers utter sequentially. Extensive experimental results using three\nstate-of-the-art speaker embedding extractors demonstrate that both proposed\napproaches are effective.",
    "descriptor": "\nComments: 5pages, 1 figure, 2 tables, submitted to ICASSP\n",
    "authors": [
      "Jee-weon Jung",
      "Hee-Soo Heo",
      "Bong-Jin Lee",
      "Jaesung Huh",
      "Andrew Brown",
      "Youngki Kwon",
      "Shinji Watanabe",
      "Joon Son Chung"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14682"
  },
  {
    "id": "arXiv:2210.14685",
    "title": "Leveraging Demonstrations with Latent Space Priors",
    "abstract": "Demonstrations provide insight into relevant state or action space regions,\nbearing great potential to boost the efficiency and practicality of\nreinforcement learning agents. In this work, we propose to leverage\ndemonstration datasets by combining skill learning and sequence modeling.\nStarting with a learned joint latent space, we separately train a generative\nmodel of demonstration sequences and an accompanying low-level policy. The\nsequence model forms a latent space prior over plausible demonstration\nbehaviors to accelerate learning of high-level policies. We show how to acquire\nsuch priors from state-only motion capture demonstrations and explore several\nmethods for integrating them into policy learning on transfer tasks. Our\nexperimental results confirm that latent space priors provide significant gains\nin learning speed and final performance in a set of challenging sparse-reward\nenvironments with a complex, simulated humanoid. Videos, source code and\npre-trained models are available at the corresponding project website at\nhttps://facebookresearch.github.io/latent-space-priors .",
    "descriptor": "",
    "authors": [
      "Jonas Gehring",
      "Deepak Gopinath",
      "Jungdam Won",
      "Andreas Krause",
      "Gabriel Synnaeve",
      "Nicolas Usunier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14685"
  },
  {
    "id": "arXiv:2210.14687",
    "title": "Which is the best model for my data?",
    "abstract": "In this paper, we tackle the problem of selecting the optimal model for a\ngiven structured pattern classification dataset. In this context, a model can\nbe understood as a classifier and a hyperparameter configuration. The proposed\nmeta-learning approach purely relies on machine learning and involves four\nmajor steps. Firstly, we present a concise collection of 62 meta-features that\naddress the problem of information cancellation when aggregation measure values\ninvolving positive and negative measurements. Secondly, we describe two\ndifferent approaches for synthetic data generation intending to enlarge the\ntraining data. Thirdly, we fit a set of pre-defined classification models for\neach classification problem while optimizing their hyperparameters using grid\nsearch. The goal is to create a meta-dataset such that each row denotes a\nmultilabel instance describing a specific problem. The features of these\nmeta-instances denote the statistical properties of the generated datasets,\nwhile the labels encode the grid search results as binary vectors such that\nbest-performing models are positively labeled. Finally, we tackle the model\nselection problem with several multilabel classifiers, including a\nConvolutional Neural Network designed to handle tabular data. The simulation\nresults show that our meta-learning approach can correctly predict an optimal\nmodel for 91% of the synthetic datasets and for 87% of the real-world datasets.\nFurthermore, we noticed that most meta-classifiers produced better results when\nusing our meta-features. Overall, our proposal differs from other meta-learning\napproaches since it tackles the algorithm selection and hyperparameter tuning\nproblems in a single step. Toward the end, we perform a feature importance\nanalysis to determine which statistical features drive the model selection\nmechanism.",
    "descriptor": "",
    "authors": [
      "Gonzalo N\u00e1poles",
      "Isel Grau",
      "\u00c7i\u00e7ek G\u00fcven",
      "Or\u00e7un \u00d6zdemir",
      "Yamisleydi Salgueiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14687"
  },
  {
    "id": "arXiv:2210.14688",
    "title": "Scaling Law Analysis for Covariance Based Activity Detection in  Cooperative Multi-Cell Massive MIMO",
    "abstract": "This paper studies the covariance based activity detection problem in a\nmulti-cell massive multiple-input multiple-output (MIMO) system, where the\nactive devices transmit their signature sequences to multiple base stations\n(BSs), and the BSs cooperatively detect the active devices based on the\nreceived signals. The scaling law of covariance based activity detection in the\nsingle-cell scenario has been thoroughly analyzed in the literature. This paper\naims to analyze the scaling law of covariance based activity detection in the\nmulti-cell massive MIMO system. In particular, this paper shows a quadratic\nscaling law in the multi-cell system under the assumption that the exponent in\nthe classical path-loss model is greater than 2, which demonstrates that in the\nmulti-cell MIMO system the maximum number of active devices that can be\ncorrectly detected in each cell increases quadratically as the length of the\nsignature sequence and decreases logarithmically as the number of cells (as the\nnumber of antennas tends to infinity). Moreover, this paper also characterizes\nthe distribution of the estimation error in the multi-cell scenario.",
    "descriptor": "\nComments: 5 pages, 2 figures, submitted for possible publication\n",
    "authors": [
      "Ziyue Wang",
      "Ya-Feng Liu",
      "Zhaorui Wang",
      "Wei Yu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14688"
  },
  {
    "id": "arXiv:2210.14691",
    "title": "Pronunciation Generation for Foreign Language Words in Intra-Sentential  Code-Switching Speech Recognition",
    "abstract": "Code-Switching refers to the phenomenon of switching languages within a\nsentence or discourse. However, limited code-switching , different language\nphoneme-sets and high rebuilding costs throw a challenge to make the\nspecialized acoustic model for code-switching speech recognition. In this\npaper, we make use of limited code-switching data as driving materials and\nexplore a shortcut to quickly develop intra-sentential code-switching\nrecognition skill on the commissioned native language acoustic model, where we\npropose a data-driven method to make the seed lexicon which is used to train\ngrapheme-to-phoneme model to predict mapping pronunciations for foreign\nlanguage word in code-switching sentences. The core work of the data-driven\ntechnology in this paper consists of a phonetic decoding method and different\nselection methods. And for imbalanced word-level driving materials problem, we\nhave an internal assistance inspiration that learning the good pronunciation\nrules in the words that possess sufficient materials using the\ngrapheme-to-phoneme model to help the scarce. Our experiments show that the\nMixed Error Rate in intra-sentential Chinese-English code-switching recognition\nreduced from 29.15\\%, acquired on the pure Chinese recognizer, to 12.13\\% by\nadding foreign language words' pronunciation through our data-driven approach,\nand finally get the best result 11.14\\% with the combination of different\nselection methods and internal assistance tactic.",
    "descriptor": "",
    "authors": [
      "Wei Wang",
      "Chao Zhang",
      "Xiaopei Wu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14691"
  },
  {
    "id": "arXiv:2210.14692",
    "title": "Identifying Threats, Cybercrime and Digital Forensic Opportunities in  Smart City Infrastructure via Threat Modeling",
    "abstract": "Technological advances have enabled multiple countries to consider\nimplementing Smart City Infrastructure to provide in-depth insights into\ndifferent data points and enhance the lives of citizens. Unfortunately, these\nnew technological implementations also entice adversaries and cybercriminals to\nexecute cyber-attacks and commit criminal acts on these modern infrastructures.\nGiven the borderless nature of cyber attacks, varying levels of understanding\nof smart city infrastructure and ongoing investigation workloads, law\nenforcement agencies and investigators would be hard-pressed to respond to\nthese kinds of cybercrime. Without an investigative capability by\ninvestigators, these smart infrastructures could become new targets favored by\ncybercriminals.\nTo address the challenges faced by investigators, we propose a common\ndefinition of smart city infrastructure. Based on the definition, we utilize\nthe STRIDE threat modeling methodology and the Microsoft Threat Modeling Tool\nto identify threats present in the infrastructure and create a threat model\nwhich can be further customized or extended by interested parties. Next, we map\noffences, possible evidence sources and types of threats identified to help\ninvestigators understand what crimes could have been committed and what\nevidence would be required in their investigation work. Finally, noting that\nSmart City Infrastructure investigations would be a global multi-faceted\nchallenge, we discuss technical and legal opportunities in digital forensics on\nSmart City Infrastructure.",
    "descriptor": "",
    "authors": [
      "Yee Ching Tok",
      "Sudipta Chattopadhyay"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14692"
  },
  {
    "id": "arXiv:2210.14698",
    "title": "Autoregressive Structured Prediction with Language Models",
    "abstract": "Recent years have seen a paradigm shift in NLP towards using pretrained\nlanguage models ({PLM}) for a wide range of tasks.\nHowever, there are many difficult design decisions to represent structures\n(e.g. tagged text, coreference chains) in a way such that they can be captured\nby PLMs.\nPrior work on structured prediction with PLMs typically flattens the\nstructured output into a sequence, which limits the quality of structural\ninformation being learned and leads to inferior performance compared to classic\ndiscriminative models.\nIn this work, we describe an approach to model structures as sequences of\nactions in an autoregressive manner with PLMs, allowing in-structure\ndependencies to be learned without any loss.\nOur approach achieves the new state-of-the-art on all the structured\nprediction tasks we looked at, namely, named entity recognition, end-to-end\nrelation extraction, and coreference resolution.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Tianyu Liu",
      "Yuchen Jiang",
      "Nicholas Monath",
      "Ryan Cotterell",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14698"
  },
  {
    "id": "arXiv:2210.14699",
    "title": "Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black  Magic?",
    "abstract": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently attracted attention in code\nassistants, with programs automatically written in a given programming language\nfrom a programming task description in natural language. They have the\npotential to save time and effort when writing code. However, these systems are\ncurrently poorly understood, preventing them from being used optimally. In this\npaper, we investigate the various input parameters of two language models, and\nconduct a study to understand if variations of these input parameters (e.g.\nprogramming task description and the surrounding context, creativity of the\nlanguage model, number of generated solutions) can have a significant impact on\nthe quality of the generated programs. We design specific operators for varying\ninput parameters and apply them over two code assistants (Copilot and Codex)\nand two benchmarks representing algorithmic problems (HumanEval and LeetCode).\nOur results showed that varying the input parameters can significantly improve\nthe performance of language models. However, there is a tight dependency when\nvarying the temperature, the prompt and the number of generated solutions,\nmaking potentially hard for developers to properly control the parameters to\nobtain an optimal result. This work opens opportunities to propose (automated)\nstrategies for improving performance.",
    "descriptor": "\nComments: 13 pages, 3 Figures (not counted the subfigures), 10 Tables\n",
    "authors": [
      "Jean-Baptiste D\u00f6derlein",
      "Mathieu Acher",
      "Djamel Eddine Khelladi",
      "Benoit Combemale"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.14699"
  },
  {
    "id": "arXiv:2210.14700",
    "title": "Low-latency Federated Learning with DNN Partition in Distributed  Industrial IoT Networks",
    "abstract": "Federated Learning (FL) empowers Industrial Internet of Things (IIoT) with\ndistributed intelligence of industrial automation thanks to its capability of\ndistributed machine learning without any raw data exchange. However, it is\nrather challenging for lightweight IIoT devices to perform\ncomputation-intensive local model training over large-scale deep neural\nnetworks (DNNs). Driven by this issue, we develop a communication-computation\nefficient FL framework for resource-limited IIoT networks that integrates DNN\npartition technique into the standard FL mechanism, wherein IIoT devices\nperform local model training over the bottom layers of the objective DNN, and\noffload the top layers to the edge gateway side. Considering imbalanced data\ndistribution, we derive the device-specific participation rate to involve the\ndevices with better data distribution in more communication rounds. Upon\nderiving the device-specific participation rate, we propose to minimize the\ntraining delay under the constraints of device-specific participation rate,\nenergy consumption and memory usage. To this end, we formulate a joint\noptimization problem of device scheduling and resource allocation (i.e. DNN\npartition point, channel assignment, transmit power, and computation\nfrequency), and solve the long-term min-max mixed integer non-linear\nprogramming based on the Lyapunov technique. In particular, the proposed\ndynamic device scheduling and resource allocation (DDSRA) algorithm can achieve\na trade-off to balance the training delay minimization and FL performance. We\nalso provide the FL convergence bound for the DDSRA algorithm with both convex\nand non-convex settings. Experimental results demonstrate the derived\ndevice-specific participation rate in terms of feasibility, and show that the\nDDSRA algorithm outperforms baselines in terms of test accuracy and convergence\ntime.",
    "descriptor": "",
    "authors": [
      "Xiumei Deng",
      "Jun Li",
      "Chuan Ma",
      "Kang Wei",
      "Long Shi",
      "Ming Ding",
      "Wen Chen"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14700"
  },
  {
    "id": "arXiv:2210.14702",
    "title": "Privacy Analysis of Samsung's Crowd-Sourced Bluetooth Location Tracking  System",
    "abstract": "We present a detailed privacy analysis of Samsung's Offline Finding (OF)\nprotocol, which is part of Samsung's Find My Mobile (FMM) location tracking\nsystem for locating Samsung mobile devices, such as Samsung smartphones and\nBluetooth trackers (Galaxy SmartTags). The OF protocol uses Bluetooth Low\nEnergy (BLE) to broadcast a unique beacon for a lost device. This beacon is\nthen picked up by nearby Samsung phones or tablets (the {\\em finder} devices),\nwhich then forward the unique beacon, along with the location it was detected\nat, to a Samsung managed server. The owner of a lost device can then query the\nserver to locate their device. We examine several security and privacy related\nproperties of the OF protocol and its implementation, from the perspectives of\nthe owner, the finder and the vendor. These include examining: the possibility\nof identifying the owner of a device through the Bluetooth data obtained from\nthe device, the possibility for a malicious actor to perform unwanted tracking\nagainst a person by exploiting the OF network, the possibility for the vendor\nto de-anonymise location reports to determine the locations of the owners or\nthe finders of lost devices, and the possibility for an attacker to compromise\nthe integrity of the location reports. Our findings suggest that there are\nprivacy risks on all accounts, arising from issues in the design and the\nimplementation of the OF protocol.",
    "descriptor": "",
    "authors": [
      "Tingfeng Yu",
      "James Henderson",
      "Alwen Tiu",
      "Thomas Haines"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14702"
  },
  {
    "id": "arXiv:2210.14703",
    "title": "ClipBot: an educational, physically impaired robot that learns to walk  via genetic algorithm optimization",
    "abstract": "Educational robots allow experimenting with a variety of principles from\nmechanics, electronics, and informatics. Here we propose ClipBot, a low-cost,\ndo-it-yourself, robot whose skeleton is made of two paper clips. An Arduino\nnano microcontroller actuates two servo motors that move the paper clips.\nHowever, such mechanical configuration confers physical impairments to\nmovement. This creates the need for and allows experimenting with artificial\nintelligence methods to overcome hardware limitations. We report our experience\nin the usage of this robot during the study week 'fascinating informatics',\norganized by the Swiss Foundation Schweizer Jugend Forscht (www.sjf.ch).\nStudents at the high school level were asked to implement a genetic algorithm\nto optimize the movements of the robot until it learned to walk. Such a\nmethodology allowed the robot to learn the motor actuation scheme yielding\nstraight movement in the forward direction using less than 20 iterations.",
    "descriptor": "\nComments: 5 pages, 3 figures, brief report\n",
    "authors": [
      "Diego Ulisse Pizzagalli",
      "Ilaria Arini",
      "Mauro Prevostini"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14703"
  },
  {
    "id": "arXiv:2210.14706",
    "title": "Rhino: Deep Causal Temporal Relationship Learning With History-dependent  Noise",
    "abstract": "Discovering causal relationships between different variables from time series\ndata has been a long-standing challenge for many domains such as climate\nscience, finance, and healthcare. Given the complexity of real-world\nrelationships and the nature of observations in discrete time, causal discovery\nmethods need to consider non-linear relations between variables, instantaneous\neffects and history-dependent noise (the change of noise distribution due to\npast actions). However, previous works do not offer a solution addressing all\nthese problems together. In this paper, we propose a novel causal relationship\nlearning framework for time-series data, called Rhino, which combines vector\nauto-regression, deep learning and variational inference to model non-linear\nrelationships with instantaneous effects while allowing the noise distribution\nto be modulated by historical observations. Theoretically, we prove the\nstructural identifiability of Rhino. Our empirical results from extensive\nsynthetic experiments and two real-world benchmarks demonstrate better\ndiscovery performance compared to relevant baselines, with ablation studies\nrevealing its robustness under model misspecification.",
    "descriptor": "\nComments: 28 pages, 8 figures, 5 tables\n",
    "authors": [
      "Wenbo Gong",
      "Joel Jennings",
      "Cheng Zhang",
      "Nick Pawlowski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14706"
  },
  {
    "id": "arXiv:2210.14707",
    "title": "Is Out-of-Distribution Detection Learnable?",
    "abstract": "Supervised learning aims to train a classifier under the assumption that\ntraining and test data are from the same distribution. To ease the above\nassumption, researchers have studied a more realistic setting:\nout-of-distribution (OOD) detection, where test data may come from classes that\nare unknown during training (i.e., OOD data). Due to the unavailability and\ndiversity of OOD data, good generalization ability is crucial for effective OOD\ndetection algorithms. To study the generalization of OOD detection, in this\npaper, we investigate the probably approximately correct (PAC) learning theory\nof OOD detection, which is proposed by researchers as an open problem. First,\nwe find a necessary condition for the learnability of OOD detection. Then,\nusing this condition, we prove several impossibility theorems for the\nlearnability of OOD detection under some scenarios. Although the impossibility\ntheorems are frustrating, we find that some conditions of these impossibility\ntheorems may not hold in some practical scenarios. Based on this observation,\nwe next give several necessary and sufficient conditions to characterize the\nlearnability of OOD detection in some practical scenarios. Lastly, we also\noffer theoretical supports for several representative OOD detection works based\non our OOD theory.",
    "descriptor": "",
    "authors": [
      "Zhen Fang",
      "Yixuan Li",
      "Jie Lu",
      "Jiahua Dong",
      "Bo Han",
      "Feng Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14707"
  },
  {
    "id": "arXiv:2210.14709",
    "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
    "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node\nis associated with a text description. An ideal solution for such a problem\nwould be integrating both the text and graph structure information with large\nlanguage models and graph neural networks (GNNs). However, the problem becomes\nvery challenging when graphs are large due to the high computational complexity\nbrought by large language models and training GNNs on big graphs. In this\npaper, we propose an efficient and effective solution to learning on large\ntext-attributed graphs by fusing graph structure and language learning with a\nvariational Expectation-Maximization (EM) framework, called GLEM. Instead of\nsimultaneously training large language models and GNNs on big graphs, GLEM\nproposes to alternatively update the two modules in the E-step and M-step. Such\na procedure allows to separately train the two modules but at the same time\nallows the two modules to interact and mutually enhance each other. Extensive\nexperiments on multiple data sets demonstrate the efficiency and effectiveness\nof the proposed approach.",
    "descriptor": "",
    "authors": [
      "Jianan Zhao",
      "Meng Qu",
      "Chaozhuo Li",
      "Hao Yan",
      "Qian Liu",
      "Rui Li",
      "Xing Xie",
      "Jian Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14709"
  },
  {
    "id": "arXiv:2210.14712",
    "title": "Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of  Downstream Tasks",
    "abstract": "We present Bloom Library, a linguistically diverse set of multimodal and\nmultilingual datasets for language modeling, image captioning, visual\nstorytelling, and speech synthesis/recognition. These datasets represent either\nthe most, or among the most, multilingual datasets for each of the included\ndownstream tasks. In total, the initial release of the Bloom Library datasets\ncovers 363 languages across 32 language families. We train downstream task\nmodels for various languages represented in the data, showing the viability of\nthe data for future work in low-resource, multimodal NLP and establishing the\nfirst known baselines for these downstream tasks in certain languages (e.g.,\nBisu [bzi], with an estimated population of 700 users). Some of these\nfirst-of-their-kind baselines are comparable to state-of-the-art performance\nfor higher-resourced languages. The Bloom Library datasets are released under\nCreative Commons licenses on the Hugging Face datasets hub to catalyze more\nlinguistically diverse research in the included downstream tasks.",
    "descriptor": "\nComments: 14 pages, 1 figure, 3 tables, accepted to and presented at EMNLP 2022\n",
    "authors": [
      "Colin Leong",
      "Joshua Nemecek",
      "Jacob Mansdorfer",
      "Anna Filighera",
      "Abraham Owodunni",
      "Daniel Whitenack"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14712"
  },
  {
    "id": "arXiv:2210.14714",
    "title": "TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early  Intent Prediction",
    "abstract": "Human intention prediction is a growing area of research where an activity in\na video has to be anticipated by a vision-based system. To this end, the model\ncreates a representation of the past, and subsequently, it produces future\nhypotheses about upcoming scenarios. In this work, we focus on pedestrians'\nearly intention prediction in which, from a current observation of an urban\nscene, the model predicts the future activity of pedestrians that approach the\nstreet. Our method is based on a multi-modal transformer that encodes past\nobservations and produces multiple predictions at different anticipation times.\nMoreover, we propose to learn the attention masks of our transformer-based\nmodel (Temporal Adaptive Mask Transformer) in order to weigh differently\npresent and past temporal dependencies. We investigate our method on several\npublic benchmarks for early intention prediction, improving the prediction\nperformances at different anticipation times compared to the previous works.",
    "descriptor": "",
    "authors": [
      "Nada Osman",
      "Guglielmo Camporese",
      "Lamberto Ballan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.14714"
  },
  {
    "id": "arXiv:2210.14716",
    "title": "Pretrained audio neural networks for Speech emotion recognition in  Portuguese",
    "abstract": "The goal of speech emotion recognition (SER) is to identify the emotional\naspects of speech. The SER challenge for Brazilian Portuguese speech was\nproposed with short snippets of Portuguese which are classified as neutral,\nnon-neutral female and non-neutral male according to paralinguistic elements\n(laughing, crying, etc). This dataset contains about $50$ minutes of Brazilian\nPortuguese speech. As the dataset leans on the small side, we investigate\nwhether a combination of transfer learning and data augmentation techniques can\nproduce positive results. Thus, by combining a data augmentation technique\ncalled SpecAugment, with the use of Pretrained Audio Neural Networks (PANNs)\nfor transfer learning we are able to obtain interesting results. The PANNs\n(CNN6, CNN10 and CNN14) are pretrained on a large dataset called AudioSet\ncontaining more than $5000$ hours of audio. They were finetuned on the SER\ndataset and the best performing model (CNN10) on the validation set was\nsubmitted to the challenge, achieving an $F1$ score of $0.73$ up from $0.54$\nfrom the baselines provided by the challenge. Moreover, we also tested the use\nof Transformer neural architecture, pretrained on about $600$ hours of\nBrazilian Portuguese audio data. Transformers, as well as more complex models\nof PANNs (CNN14), fail to generalize to the test set in the SER dataset and do\nnot beat the baseline. Considering the limitation of the dataset sizes,\ncurrently the best approach for SER is using PANNs (specifically, CNN6 and\nCNN10).",
    "descriptor": "",
    "authors": [
      "Marcelo Matheus Gauy",
      "Marcelo Finger"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14716"
  },
  {
    "id": "arXiv:2210.14721",
    "title": "Sim-to-Real via Sim-to-Seg: End-to-end Off-road Autonomous Driving  Without Real Data",
    "abstract": "Autonomous driving is complex, requiring sophisticated 3D scene\nunderstanding, localization, mapping, and control. Rather than explicitly\nmodelling and fusing each of these components, we instead consider an\nend-to-end approach via reinforcement learning (RL). However, collecting\nexploration driving data in the real world is impractical and dangerous. While\ntraining in simulation and deploying visual sim-to-real techniques has worked\nwell for robot manipulation, deploying beyond controlled workspace viewpoints\nremains a challenge. In this paper, we address this challenge by presenting\nSim2Seg, a re-imagining of RCAN that crosses the visual reality gap for\noff-road autonomous driving, without using any real-world data. This is done by\nlearning to translate randomized simulation images into simulated segmentation\nand depth maps, subsequently enabling real-world images to also be translated.\nThis allows us to train an end-to-end RL policy in simulation, and directly\ndeploy in the real-world. Our approach, which can be trained in 48 hours on 1\nGPU, can perform equally as well as a classical perception and control stack\nthat took thousands of engineering hours over several months to build. We hope\nthis work motivates future end-to-end autonomous driving research.",
    "descriptor": "\nComments: CoRL 2022 Paper\n",
    "authors": [
      "John So",
      "Amber Xie",
      "Sunggoo Jung",
      "Jeffrey Edlund",
      "Rohan Thakker",
      "Ali Agha-mohammadi",
      "Pieter Abbeel",
      "Stephen James"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14721"
  },
  {
    "id": "arXiv:2210.14722",
    "title": "Online TSP with Known Locations",
    "abstract": "In this paper, we consider the Online Traveling Salesperson Problem (OLTSP)\nwhere the locations of the requests are known in advance, but not their arrival\ntimes. We study both the open variant, in which the algorithm is not required\nto return to the origin when all the requests are served, as well as the closed\nvariant, in which the algorithm has to return to the origin after serving all\nthe requests. Our aim is to measure the impact of the extra knowledge of the\nlocations on the competitiveness of the problem. We present an online\n3/2-competitive algorithm for the general case and a matching lower bound for\nboth the open and the closed variant. Then, we focus on some interesting metric\nspaces (ring, star, semi-line), providing both lower bounds and polynomial time\nonline algorithms for the problem.",
    "descriptor": "",
    "authors": [
      "Evripidis Bampis",
      "Bruno Escoffier",
      "Niklas Hahn",
      "Michalis Xefteris"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.14722"
  },
  {
    "id": "arXiv:2210.14723",
    "title": "Semi-Supervised Learning Based on Reference Model for Low-resource TTS",
    "abstract": "Most previous neural text-to-speech (TTS) methods are mainly based on\nsupervised learning methods, which means they depend on a large training\ndataset and hard to achieve comparable performance under low-resource\nconditions. To address this issue, we propose a semi-supervised learning method\nfor neural TTS in which labeled target data is limited, which can also resolve\nthe problem of exposure bias in the previous auto-regressive models.\nSpecifically, we pre-train the reference model based on Fastspeech2 with much\nsource data, fine-tuned on a limited target dataset. Meanwhile, pseudo labels\ngenerated by the original reference model are used to guide the fine-tuned\nmodel's training further, achieve a regularization effect, and reduce the\noverfitting of the fine-tuned model during training on the limited target data.\nExperimental results show that our proposed semi-supervised learning scheme\nwith limited target data significantly improves the voice quality for test data\nto achieve naturalness and robustness in speech synthesis.",
    "descriptor": "\nComments: Accepted by NMIC2022, The Fourth International Workshop on Network Meets Intelligent Computations\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14723"
  },
  {
    "id": "arXiv:2210.14724",
    "title": "Improving Imbalanced Text Classification with Dynamic Curriculum  Learning",
    "abstract": "Recent advances in pre-trained language models have improved the performance\nfor text classification tasks. However, little attention is paid to the\npriority scheduling strategy on the samples during training. Humans acquire\nknowledge gradually from easy to complex concepts, and the difficulty of the\nsame material can also vary significantly in different learning stages.\nInspired by this insights, we proposed a novel self-paced dynamic curriculum\nlearning (SPDCL) method for imbalanced text classification, which evaluates the\nsample difficulty by both linguistic character and model capacity. Meanwhile,\nrather than using static curriculum learning as in the existing research, our\nSPDCL can reorder and resample training data by difficulty criterion with an\nadaptive from easy to hard pace. The extensive experiments on several\nclassification tasks show the effectiveness of SPDCL strategy, especially for\nthe imbalanced dataset.",
    "descriptor": "\nComments: Accepted by UEIoT2022, The 3rd International Workshop on Ubiquitous Electric Internet of Things\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14724"
  },
  {
    "id": "arXiv:2210.14725",
    "title": "Linguistic-Enhanced Transformer with CTC Embedding for Speech  Recognition",
    "abstract": "The recent emergence of joint CTC-Attention model shows significant\nimprovement in automatic speech recognition (ASR). The improvement largely lies\nin the modeling of linguistic information by decoder. The decoder\njoint-optimized with an acoustic encoder renders the language model from\nground-truth sequences in an auto-regressive manner during training. However,\nthe training corpus of the decoder is limited to the speech transcriptions,\nwhich is far less than the corpus needed to train an acceptable language model.\nThis leads to poor robustness of decoder. To alleviate this problem, we propose\nlinguistic-enhanced transformer, which introduces refined CTC information to\ndecoder during training process, so that the decoder can be more robust. Our\nexperiments on AISHELL-1 speech corpus show that the character error rate (CER)\nis relatively reduced by up to 7%. We also find that in joint CTC-Attention ASR\nmodel, decoder is more sensitive to linguistic information than acoustic\ninformation.",
    "descriptor": "\nComments: Accepted by ECAISS2022, The Fourth International Workshop on Edge Computing and Artificial Intelligence based Sensor-Cloud System\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Mengyuan Zhao",
      "Zhiyong Zhang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14725"
  },
  {
    "id": "arXiv:2210.14730",
    "title": "Watch Your Step: Real-Time Adaptive Character Stepping",
    "abstract": "An effective 3D stepping control algorithm that is computationally fast,\nrobust, and easy to implement is extremely important and valuable to character\nanimation research. In this paper, we present a novel technique for generating\ndynamic, interactive, and controllable biped stepping motions. Our approach\nuses a low-dimensional physics-based model to create balanced humanoid avatars\nthat can handle a wide variety of interactive situations, such as terrain\nheight shifting and push exertions, while remaining upright and balanced. We\naccomplish this by combining the popular inverted-pendulum model with an\nankle-feedback torque and variable leg-length mechanism to create a\ncontrollable solution that can adapt to unforeseen circumstances in real-time\nwithout key-framed data, any offline pre-processing, or on-line optimizations\njoint torque computations. We explain and address oversimplifications and\nlimitations with the basic IP model and the reasons for extending the model by\nmeans of additional control mechanisms. We demonstrate a simple and fast\napproach for extending the IP model based on an ankle-torque and variable leg\nlengths approximation without hindering the extremely attractive properties\n(i.e., computational speed, robustness, and simplicity) that make the IP model\nso ideal for generating upright responsive balancing biped movements. Finally,\nwhile our technique focuses on lower body motions, it can, nevertheless, handle\nboth small and large push forces even during terrain height variations.\nMoreover, our model effectively creates human-like motions that synthesize\nlow-level upright stepping movements, and can be combined with additional\ncontroller techniques to produce whole body autonomous agents.",
    "descriptor": "",
    "authors": [
      "Ben Kenwright"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14730"
  },
  {
    "id": "arXiv:2210.14736",
    "title": "An Optimal Lower Bound for Simplex Range Reporting",
    "abstract": "We give a simplified and improved lower bound for the simplex range reporting\nproblem. We show that given a set $P$ of $n$ points in $\\mathbb{R}^d$, any data\nstructure that uses $S(n)$ space to answer such queries must have\n$Q(n)=\\Omega((n^2/S(n))^{(d-1)/d}+k)$ query time, where $k$ is the output size.\nFor near-linear space data structures, i.e., $S(n)=O(n\\log^{O(1)}n)$, this\nimproves the previous lower bounds by Chazelle and Rosenberg [CR96] and Afshani\n[A12] but perhaps more importantly, it is the first ever tight lower bound for\nany variant of simplex range searching for $d\\ge 3$ dimensions.\nWe obtain our lower bound by making a simple connection to well-studied\nproblems in incident geometry which allows us to use known constructions in the\narea. We observe that a small modification of a simple already existing\nconstruction can lead to our lower bound. We believe that our proof is\naccessible to a much wider audience, at least compared to the previous\nintricate probabilistic proofs based on measure arguments by Chazelle and\nRosenberg [CR96] and Afshani [A12].\nThe lack of tight or almost-tight (up to polylogarithmic factor) lower bounds\nfor near-linear space data structures is a major bottleneck in making progress\non problems such as proving lower bounds for multilevel data structures. It is\nour hope that this new line of attack based on incidence geometry can lead to\nfurther progress in this area.",
    "descriptor": "\nComments: To appear in SOSA'23\n",
    "authors": [
      "Peyman Afshani",
      "Pingan Cheng"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.14736"
  },
  {
    "id": "arXiv:2210.14738",
    "title": "Optimization based coordination of autonomous vehicles in confined areas",
    "abstract": "Confined areas present an opportunity for early deployment of autonomous\nvehicles (AV) due to the absence of non-controlled traffic participants. In\nthis paper, we present an approach for coordination of multiple AVs in confined\nsites. The method computes speed-profiles for the AVs such that collisions are\navoided in cross-intersection and merge crossings. Specifically, this is done\nthrough the solution of an optimal control problem where the motion of all\nvehicles is optimized jointly. The order in which the vehicles pass the\ncrossings is determined through the solution of a Mixed Integer Quadratic\nProgram (MIQP). Through simulation results, we demonstrate the capability of\nthe algorithm in terms of performance and satisfaction of collision avoidance\nconstraints.",
    "descriptor": "\nComments: To be published in the 25th IEEE International Conference on Intelligent Transportation Systems (ITSC 2022)\n",
    "authors": [
      "Stefan Kojchev",
      "Robert Hult",
      "Jonas Fredriksson"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14738"
  },
  {
    "id": "arXiv:2210.14739",
    "title": "A Case for Business Process-Specific Foundation Models",
    "abstract": "The inception of large language models has helped advance state-of-the-art\nperformance on numerous natural language tasks. This has also opened the door\nfor the development of foundation models for other domains and data modalities\nsuch as images, code, and music. In this paper, we argue that business process\ndata representations have unique characteristics that warrant the development\nof a new class of foundation models to handle tasks like process mining,\noptimization, and decision making. These models should also tackle the unique\nchallenges of applying AI to business processes which include data scarcity,\nmulti-modal representations, domain specific terminology, and privacy concerns.",
    "descriptor": "",
    "authors": [
      "Yara Rizk",
      "Praveen Venkateswaran",
      "Vatche Isahagian",
      "Vinod Muthusamy"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.14739"
  },
  {
    "id": "arXiv:2210.14742",
    "title": "Monotonic segmental attention for automatic speech recognition",
    "abstract": "We introduce a novel segmental-attention model for automatic speech\nrecognition. We restrict the decoder attention to segments to avoid quadratic\nruntime of global attention, better generalize to long sequences, and\neventually enable streaming. We directly compare global-attention and different\nsegmental-attention modeling variants. We develop and compare two separate\ntime-synchronous decoders, one specifically taking the segmental nature into\naccount, yielding further improvements. Using time-synchronous decoding for\nsegmental models is novel and a step towards streaming applications. Our\nexperiments show the importance of a length model to predict the segment\nboundaries. The final best segmental-attention model using segmental decoding\nperforms better than global-attention, in contrast to other monotonic attention\napproaches in the literature. Further, we observe that the segmental model\ngeneralizes much better to long sequences of up to several minutes.",
    "descriptor": "\nComments: accepted at SLT: this https URL\n",
    "authors": [
      "Albert Zeyer",
      "Robin Schmitt",
      "Wei Zhou",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14742"
  },
  {
    "id": "arXiv:2210.14743",
    "title": "DEEPFAKE CLI: Accelerated Deepfake Detection using FPGAs",
    "abstract": "Because of the availability of larger datasets and recent improvements in the\ngenerative model, more realistic Deepfake videos are being produced each day.\nPeople consume around one billion hours of video on social media platforms\nevery day, and thats why it is very important to stop the spread of fake videos\nas they can be damaging, dangerous, and malicious. There has been a significant\nimprovement in the field of deepfake classification, but deepfake detection and\ninference have remained a difficult task. To solve this problem in this paper,\nwe propose a novel DEEPFAKE C-L-I (Classification-Localization-Inference) in\nwhich we have explored the idea of accelerating Quantized Deepfake Detection\nModels using FPGAs due to their ability of maximum parallelism and energy\nefficiency compared to generalized GPUs. In this paper, we have used light\nMesoNet with EFF-YNet structure and accelerated it on VCK5000 FPGA, powered by\nstate-of-the-art VC1902 Versal Architecture which uses AI, DSP, and Adaptable\nEngines for acceleration. We have benchmarked our inference speed with other\nstate-of-the-art inference nodes, got 316.8 FPS on VCK5000 while maintaining\n93\\% Accuracy.",
    "descriptor": "\nComments: This preprint has not undergone peer review or any post-submission improvement or corrections. The Version of Record of this contribution is published in LNCS [13798], PDCAT 2022 , and is available online at [this https URL ]\n",
    "authors": [
      "Omkar Bhilare",
      "Rahul Singh",
      "Vedant Paranjape",
      "Sravan Chittupalli",
      "Shraddha Suratkar",
      "Faruk Kazi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14743"
  },
  {
    "id": "arXiv:2210.14748",
    "title": "Long-tailed Food Classification",
    "abstract": "Food classification serves as the basic step of image-based dietary\nassessment to predict the types of foods in each input image. However, food\nimage predictions in a real world scenario are usually long-tail distributed\namong different food classes, which cause heavy class-imbalance problems and a\nrestricted performance. In addition, none of the existing long-tailed\nclassification methods focus on food data, which can be more challenging due to\nthe lower inter-class and higher intra-class similarity among foods. In this\nwork, we first introduce two new benchmark datasets for long-tailed food\nclassification including Food101-LT and VFN-LT where the number of samples in\nVFN-LT exhibits the real world long-tailed food distribution. Then we propose a\nnovel 2-Phase framework to address the problem of class-imbalance by (1)\nundersampling the head classes to remove redundant samples along with\nmaintaining the learned information through knowledge distillation, and (2)\noversampling the tail classes by performing visual-aware data augmentation. We\nshow the effectiveness of our method by comparing with existing\nstate-of-the-art long-tailed classification methods and show improved\nperformance on both Food101-LT and VFN-LT benchmarks. The results demonstrate\nthe potential to apply our method to related real life applications.",
    "descriptor": "",
    "authors": [
      "Jiangpeng He",
      "Luotao Lin",
      "Heather Eicher-Miller",
      "Fengqing Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14748"
  },
  {
    "id": "arXiv:2210.14749",
    "title": "Hybrid HMM Decoder For Convolutional Codes By Joint Trellis-Like  Structure and Channel Prior",
    "abstract": "The anti-interference capability of wireless links is a physical layer\nproblem for edge computing. Although convolutional codes have inherent error\ncorrection potential due to the redundancy introduced in the data, the\nperformance of the convolutional code is drastically degraded due to multipath\neffects on the channel. In this paper, we propose the use of a Hidden Markov\nModel (HMM) for the reconstruction of convolutional codes and decoding by the\nViterbi algorithm. Furthermore, to implement soft-decision decoding, the\nobservation of HMM is replaced by Gaussian mixture models (GMM). Our method\nprovides superior error correction potential than the standard method because\nthe model parameters contain channel state information (CSI). We evaluated the\nperformance of the method compared to standard Viterbi decoding by numerical\nsimulation. In the multipath channel, the hybrid HMM decoder can achieve a\nperformance gain of 4.7 dB and 2 dB when using hard-decision and soft-decision\ndecoding, respectively. The HMM decoder also achieves significant performance\ngains for the RSC code, suggesting that the method could be extended to turbo\ncodes.",
    "descriptor": "\nComments: 21 pages, 8 figures\n",
    "authors": [
      "Haoyu Li",
      "Xuan Wang",
      "Tong Liu",
      "Dingyi Fang",
      "Baoying Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14749"
  },
  {
    "id": "arXiv:2210.14756",
    "title": "Maximum Likelihood Learning of Energy-Based Models for Simulation-Based  Inference",
    "abstract": "We introduce two synthetic likelihood methods for Simulation-Based Inference\n(SBI), to conduct either amortized or targeted inference from experimental\nobservations when a high-fidelity simulator is available. Both methods learn a\nconditional energy-based model (EBM) of the likelihood using synthetic data\ngenerated by the simulator, conditioned on parameters drawn from a proposal\ndistribution. The learned likelihood can then be combined with any prior to\nobtain a posterior estimate, from which samples can be drawn using MCMC. Our\nmethods uniquely combine a flexible Energy-Based Model and the minimization of\na KL loss: this is in contrast to other synthetic likelihood methods, which\neither rely on normalizing flows, or minimize score-based objectives; choices\nthat come with known pitfalls. Our first method, Amortized Unnormalized Neural\nLikelihood Estimation (AUNLE), introduces a tilting trick during training that\nallows to significantly lower the computational cost of inference by enabling\nthe use of efficient MCMC techniques. Our second method, Sequential UNLE\n(SUNLE), employs a robust doubly intractable approach in order to re-use\nsimulation data and improve posterior accuracy on a specific dataset. We\ndemonstrate the properties of both methods on a range of synthetic datasets,\nand apply them to a neuroscience model of the pyloric network in the crab\nCancer Borealis, matching the performance of other synthetic likelihood methods\nat a fraction of the simulation budget.",
    "descriptor": "",
    "authors": [
      "Pierre Glaser",
      "Michael Arbel",
      "Arnaud Doucet",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14756"
  },
  {
    "id": "arXiv:2210.14763",
    "title": "ProSiT! Latent Variable Discovery with PROgressive SImilarity Thresholds",
    "abstract": "The most common ways to explore latent document dimensions are topic models\nand clustering methods. However, topic models have several drawbacks: e.g.,\nthey require us to choose the number of latent dimensions a priori, and the\nresults are stochastic. Most clustering methods have the same issues and lack\nflexibility in various ways, such as not accounting for the influence of\ndifferent topics on single documents, forcing word-descriptors to belong to a\nsingle topic (hard-clustering) or necessarily relying on word representations.\nWe propose PROgressive SImilarity Thresholds - ProSiT, a deterministic and\ninterpretable method, agnostic to the input format, that finds the optimal\nnumber of latent dimensions and only has two hyper-parameters, which can be set\nefficiently via grid search. We compare this method with a wide range of topic\nmodels and clustering methods on four benchmark data sets. In most setting,\nProSiT matches or outperforms the other methods in terms six metrics of topic\ncoherence and distinctiveness, producing replicable, deterministic results.",
    "descriptor": "",
    "authors": [
      "Tommaso Fornaciari",
      "Dirk Hovy",
      "Federico Bianchi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14763"
  },
  {
    "id": "arXiv:2210.14764",
    "title": "Towards a machine learning pipeline in reduced order modelling for  inverse problems: neural networks for boundary parametrization,  dimensionality reduction and solution manifold approximation",
    "abstract": "In this work, we propose a model order reduction framework to deal with\ninverse problems in a non-intrusive setting. Inverse problems, especially in a\npartial differential equation context, require a huge computational load due to\nthe iterative optimization process. To accelerate such a procedure, we apply a\nnumerical pipeline that involves artificial neural networks to parametrize the\nboundary conditions of the problem in hand, compress the dimensionality of the\n(full-order) snapshots, and approximate the parametric solution manifold. It\nderives a general framework capable to provide an ad-hoc parametrization of the\ninlet boundary and quickly converges to the optimal solution thanks to model\norder reduction. We present in this contribution the results obtained by\napplying such methods to two different CFD test cases.",
    "descriptor": "",
    "authors": [
      "Anna Ivagnes",
      "Nicola Demo",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14764"
  },
  {
    "id": "arXiv:2210.14767",
    "title": "Stabilization of Energy-Conserving Gaits for Point-Foot Planar Bipeds",
    "abstract": "The problem of designing and stabilizing impact-free, energy-conserving gaits\nis considered for underactuated, point-foot planar bipeds. Virtual holonomic\nconstraints are used to design energy-conserving gaits. A desired gait\ncorresponds to a periodic hybrid orbit and is stabilized using the Impulse\nControlled Poincar\\'e Map approach. Numerical simulations for the case of a\nfive-link biped demonstrate convergence to a desired gait from arbitrary\ninitial conditions.",
    "descriptor": "\nComments: 6 pages, 6 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Aakash Khandelwal",
      "Nilay Kant",
      "Ranjan Mukherjee"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14767"
  },
  {
    "id": "arXiv:2210.14771",
    "title": "Rapid and robust endoscopic content area estimation: A lean GPU-based  pipeline and curated benchmark dataset",
    "abstract": "Endoscopic content area refers to the informative area enclosed by the dark,\nnon-informative, border regions present in most endoscopic footage. The\nestimation of the content area is a common task in endoscopic image processing\nand computer vision pipelines. Despite the apparent simplicity of the problem,\nseveral factors make reliable real-time estimation surprisingly challenging.\nThe lack of rigorous investigation into the topic combined with the lack of a\ncommon benchmark dataset for this task has been a long-lasting issue in the\nfield. In this paper, we propose two variants of a lean GPU-based computational\npipeline combining edge detection and circle fitting. The two variants differ\nby relying on handcrafted features, and learned features respectively to\nextract content area edge point candidates. We also present a first-of-its-kind\ndataset of manually annotated and pseudo-labelled content areas across a range\nof surgical indications. To encourage further developments, the curated\ndataset, and an implementation of both algorithms, has been made public\n(https://doi.org/10.7303/syn32148000,\nhttps://github.com/charliebudd/torch-content-area). We compare our proposed\nalgorithm with a state-of-the-art U-Net-based approach and demonstrate\nsignificant improvement in terms of both accuracy (Hausdorff distance: 6.3 px\nversus 118.1 px) and computational time (Average runtime per frame: 0.13 ms\nversus 11.2 ms).",
    "descriptor": "\nComments: Presented at AE-CAI MICCAI workshop\n",
    "authors": [
      "Charlie Budd",
      "Luis C. Garcia-Peraza-Herrera",
      "Martin Huber",
      "Sebastien Ourselin",
      "Tom Vercauteren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14771"
  },
  {
    "id": "arXiv:2210.14772",
    "title": "Multiscale multimesh finite element method | $\\text{M}^2$-FEM:  Hierarchical mesh-decoupling for integral structural theories",
    "abstract": "This study presents a generalized multiscale multimesh finite element method\n($\\text{M}^2$-FEM) that addresses several long-standing challenges in the\nnumerical simulation of integral structural theories, often used to model\nmultiscale and nonlocal effects. The major challenges in the numerical\nsimulation of integral boundary value problems are primarily rooted in the\ncoupling of the spatial discretization of the global (parent) and integral\n(child) domains which severely restricts the computational efficiency of\nexisting algorithms by imposing an implicit trade-off in the accuracy achieved\nby the child domain and in the resources dedicated to the simulation of the\noverall parent domain. One of the most defining contributions of this study\nconsists in the development of a mesh-decoupling technique that generates\nisolated sets of meshes such that the parent and child domains can be\ndiscretized and approximated independently. This mesh-decoupling has a\nmulti-fold impact on the simulation of integral theories such that, when\ncompared to existing state-of-the-art techniques, the proposed algorithm\nachieves simultaneously better numerical accuracy and efficiency (hence\nallowing a greater flexibility in both mesh size and computational cost\ntrade-off decisions), greater ability to adopt generalized integral kernel\nfunctions, and the ability to handle non-regular (non-rectangular) domains via\nunstructured meshing. In this study, we choose a benchmark problem based on an\nextended version of the Eringen's nonlocal elasticity theory (implicitly, a\nmultiscale theory) that leverages the use of generalized attenuation kernels\nand non-constant horizons of nonlocality. Nonetheless, the proposed\n$\\text{M}^2$-FEM algorithm is very general and it can be applied to a variety\nof integral theories, even beyond structural elasticity.",
    "descriptor": "",
    "authors": [
      "Wei Ding",
      "Sansit Patnaik",
      "Fabio Semperlotti"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.14772"
  },
  {
    "id": "arXiv:2210.14774",
    "title": "Unknown area exploration for robots with energy constraints using a  modified Butterfly Optimization Algorithm",
    "abstract": "Butterfly Optimization Algorithm (BOA) is a recent metaheuristic that has\nbeen used in several optimization problems. In this paper, we propose a new\nversion of the algorithm (xBOA) based on the crossover operator and compare its\nresults to the original BOA and 3 other variants recently introduced in the\nliterature. We also proposed a framework for solving the unknown area\nexploration problem with energy constraints using metaheuristics in both\nsingle- and multi-robot scenarios. This framework allowed us to benchmark the\nperformances of different metaheuristics for the robotics exploration problem.\nWe conducted several experiments to validate this framework and used it to\ncompare the effectiveness of xBOA with wellknown metaheuristics used in the\nliterature through 5 evaluation criteria. Although BOA and xBOA are not optimal\nin all these criteria, we found that BOA can be a good alternative to many\nmetaheuristics in terms of the exploration time, while xBOA is more robust to\nlocal optima; has better fitness convergence; and achieves better exploration\nrates than the original BOA and its other variants.",
    "descriptor": "",
    "authors": [
      "Amine Bendahmane",
      "Redouane Tlemsani"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.14774"
  },
  {
    "id": "arXiv:2210.14782",
    "title": "Development of linear functional arithmetic and its application to  solving problems of interval analysis",
    "abstract": "The work is devoted to the construction of a new type of intervals --\nfunctional intervals. These intervals are built on the idea of expanding\nboundaries from numbers to functions. Functional intervals have shown\nthemselves to be promising for further study and use, since they have more rich\nalgebraic properties compared to classical intervals lamy. In the work, linear\nfunctional arithmetic was constructed from one variable. This arithmetic was\napplied to solve such problems of interval analysis, as minimization of a\nfunction on an interval and finding zeros of a function on an interval. Results\nof numerical experiments for linear functional arithmetic showed a high order\nof convergence and a higher speed the growth of algorithms when using intervals\nof a new type, despite the fact that the calculations did not use information\nabout derivative function. Also in the work, a modification of the minimization\nalgorithms functions of several variables, based on the use of the function\nrational intervals of several variables. As a result, it was Improved speedup\nof algorithms, but only up to a certain number of unknowns.",
    "descriptor": "\nComments: in Russian language\n",
    "authors": [
      "Dmitry A. Skorik"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14782"
  },
  {
    "id": "arXiv:2210.14783",
    "title": "Decoupled Mixup for Generalized Visual Recognition",
    "abstract": "Convolutional neural networks (CNN) have demonstrated remarkable performance\nwhen the training and testing data are from the same distribution. However,\nsuch trained CNN models often largely degrade on testing data which is unseen\nand Out-Of-the-Distribution (OOD). To address this issue, we propose a novel\n\"Decoupled-Mixup\" method to train CNN models for OOD visual recognition.\nDifferent from previous work combining pairs of images homogeneously, our\nmethod decouples each image into discriminative and noise-prone regions, and\nthen heterogeneously combines these regions of image pairs to train CNN models.\nSince the observation is that noise-prone regions such as textural and clutter\nbackgrounds are adverse to the generalization ability of CNN models during\ntraining, we enhance features from discriminative regions and suppress\nnoise-prone ones when combining an image pair. To further improve the\ngeneralization ability of trained models, we propose to disentangle\ndiscriminative and noise-prone regions in frequency-based and context-based\nfashions. Experiment results show the high generalization performance of our\nmethod on testing data that are composed of unseen contexts, where our method\nachieves 85.76\\% top-1 accuracy in Track-1 and 79.92\\% in Track-2 in the NICO\nChallenge. The source code is available at\nhttps://github.com/HaozheLiu-ST/NICOChallenge-OOD-Classification.",
    "descriptor": "\nComments: Accepted by ECCV'2022 Workshop: Causality in Vision\n",
    "authors": [
      "Haozhe Liu",
      "Wentian Zhang",
      "Jinheng Xie",
      "Haoqian Wu",
      "Bing Li",
      "Ziqi Zhang",
      "Yuexiang Li",
      "Yawen Huang",
      "Bernard Ghanem",
      "Yefeng Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14783"
  },
  {
    "id": "arXiv:2210.14784",
    "title": "Categorical SDEs with Simplex Diffusion",
    "abstract": "Diffusion models typically operate in the standard framework of generative\nmodelling by producing continuously-valued datapoints. To this end, they rely\non a progressive Gaussian smoothing of the original data distribution, which\nadmits an SDE interpretation involving increments of a standard Brownian\nmotion. However, some applications such as text generation or reinforcement\nlearning might naturally be better served by diffusing categorical-valued data,\ni.e., lifting the diffusion to a space of probability distributions. To this\nend, this short theoretical note proposes Simplex Diffusion, a means to\ndirectly diffuse datapoints located on an n-dimensional probability simplex. We\nshow how this relates to the Dirichlet distribution on the simplex and how the\nanalogous SDE is realized thanks to a multi-dimensional Cox-Ingersoll-Ross\nprocess (abbreviated as CIR), previously used in economics and mathematical\nfinance. Finally, we make remarks as to the numerical implementation of\ntrajectories of the CIR process, and discuss some limitations of our approach.",
    "descriptor": "",
    "authors": [
      "Pierre H. Richemond",
      "Sander Dieleman",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14784"
  },
  {
    "id": "arXiv:2210.14789",
    "title": "Extracting Unique Information Through Markov Relations",
    "abstract": "We propose two new measures for extracting the unique information in $X$ and\nnot $Y$ about a message $M$, when $X, Y$ and $M$ are joint random variables\nwith a given joint distribution. We take a Markov based approach, motivated by\nquestions in fair machine learning, and inspired by similar Markov-based\noptimization problems that have been used in the Information Bottleneck and\nCommon Information frameworks. We obtain a complete characterization of our\ndefinitions in the Gaussian case (namely, when $X, Y$ and $M$ are jointly\nGaussian), under the assumption of Gaussian optimality. We also examine the\nconsistency of our definitions with the partial information decomposition (PID)\nframework, and show that these Markov based definitions achieve non-negativity,\nbut not symmetry, within the PID framework.",
    "descriptor": "",
    "authors": [
      "Keerthana Gurushankar",
      "Praveen Venkatesh",
      "Pulkit Grover"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14789"
  },
  {
    "id": "arXiv:2210.14791",
    "title": "ViNL: Visual Navigation and Locomotion Over Obstacles",
    "abstract": "We present Visual Navigation and Locomotion over obstacles (ViNL), which\nenables a quadrupedal robot to navigate unseen apartments while stepping over\nsmall obstacles that lie in its path (e.g., shoes, toys, cables), similar to\nhow humans and pets lift their feet over objects as they walk. ViNL consists\nof: (1) a visual navigation policy that outputs linear and angular velocity\ncommands that guides the robot to a goal coordinate in unfamiliar indoor\nenvironments; and (2) a visual locomotion policy that controls the robot's\njoints to avoid stepping on obstacles while following provided velocity\ncommands. Both the policies are entirely \"model-free\", i.e. sensors-to-actions\nneural networks trained end-to-end. The two are trained independently in two\nentirely different simulators and then seamlessly co-deployed by feeding the\nvelocity commands from the navigator to the locomotor, entirely \"zero-shot\"\n(without any co-training). While prior works have developed learning methods\nfor visual navigation or visual locomotion, to the best of our knowledge, this\nis the first fully learned approach that leverages vision to accomplish both\n(1) intelligent navigation in new environments, and (2) intelligent visual\nlocomotion that aims to traverse cluttered environments without disrupting\nobstacles. On the task of navigation to distant goals in unknown environments,\nViNL using just egocentric vision significantly outperforms prior work on\nrobust locomotion using privileged terrain maps (+32.8% success and -4.42\ncollisions per meter). Additionally, we ablate our locomotion policy to show\nthat each aspect of our approach helps reduce obstacle collisions. Videos and\ncode at this http URL",
    "descriptor": "",
    "authors": [
      "Simar Kareer",
      "Naoki Yokoyama",
      "Dhruv Batra",
      "Sehoon Ha",
      "Joanne Truong"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14791"
  },
  {
    "id": "arXiv:2210.14793",
    "title": "M$^3$ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task  Learning with Model-Accelerator Co-design",
    "abstract": "Multi-task learning (MTL) encapsulates multiple learned tasks in a single\nmodel and often lets those tasks learn better jointly. However, when deploying\nMTL onto those real-world systems that are often resource-constrained or\nlatency-sensitive, two prominent challenges arise: (i) during training,\nsimultaneously optimizing all tasks is often difficult due to gradient\nconflicts across tasks; (ii) at inference, current MTL regimes have to activate\nnearly the entire model even to just execute a single task. Yet most real\nsystems demand only one or two tasks at each moment, and switch between tasks\nas needed: therefore such all tasks activated inference is also highly\ninefficient and non-scalable. In this paper, we present a model-accelerator\nco-design framework to enable efficient on-device MTL. Our framework, dubbed\nM$^3$ViT, customizes mixture-of-experts (MoE) layers into a vision transformer\n(ViT) backbone for MTL, and sparsely activates task-specific experts during\ntraining. Then at inference with any task of interest, the same design allows\nfor activating only the task-corresponding sparse expert pathway, instead of\nthe full model. Our new model design is further enhanced by hardware-level\ninnovations, in particular, a novel computation reordering scheme tailored for\nmemory-constrained MTL that achieves zero-overhead switching between tasks and\ncan scale to any number of experts. When executing single-task inference,\nM$^{3}$ViT achieves higher accuracies than encoder-focused MTL methods, while\nsignificantly reducing 88% inference FLOPs. When implemented on a hardware\nplatform of one Xilinx ZCU104 FPGA, our co-design framework reduces the memory\nrequirement by 2.4 times, while achieving energy efficiency up to 9.23 times\nhigher than a comparable FPGA baseline. Code is available at:\nhttps://github.com/VITA-Group/M3ViT.",
    "descriptor": "",
    "authors": [
      "Hanxue Liang",
      "Zhiwen Fan",
      "Rishov Sarkar",
      "Ziyu Jiang",
      "Tianlong Chen",
      "Kai Zou",
      "Yu Cheng",
      "Cong Hao",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14793"
  },
  {
    "id": "arXiv:2210.14795",
    "title": "Enforcing Dirichlet boundary conditions in physics-informed neural  networks and variational physics-informed neural networks",
    "abstract": "In this paper, we present and compare four methods to enforce Dirichlet\nboundary conditions in Physics-Informed Neural Networks (PINNs) and Variational\nPhysics-Informed Neural Networks (VPINNs). Such conditions are usually imposed\nby adding penalization terms in the loss function and properly choosing the\ncorresponding scaling coefficients; however, in practice, this requires an\nexpensive tuning phase. We show through several numerical tests that modifying\nthe output of the neural network to exactly match the prescribed values leads\nto more efficient and accurate solvers. The best results are achieved by\nexactly enforcing the Dirichlet boundary conditions by means of an approximate\ndistance function. We also show that variationally imposing the Dirichlet\nboundary conditions via Nitsche's method leads to suboptimal solvers.",
    "descriptor": "\nComments: 18 pages, 36 figures\n",
    "authors": [
      "S. Berrone",
      "C. Canuto",
      "M. Pintore",
      "N. Sukumar"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14795"
  },
  {
    "id": "arXiv:2210.14796",
    "title": "AD-DMKDE: Anomaly Detection through Density Matrices and Fourier  Features",
    "abstract": "This paper presents a novel density estimation method for anomaly detection\nusing density matrices (a powerful mathematical formalism from quantum\nmechanics) and Fourier features. The method can be seen as an efficient\napproximation of Kernel Density Estimation (KDE). A systematic comparison of\nthe proposed method with eleven state-of-the-art anomaly detection methods on\nvarious data sets is presented, showing competitive performance on different\nbenchmark data sets. The method is trained efficiently and it uses optimization\nto find the parameters of data embedding. The prediction phase complexity of\nthe proposed algorithm is constant relative to the training data size, and it\nperforms well in data sets with different anomaly rates. Its architecture\nallows vectorization and can be implemented on GPU/TPU hardware.",
    "descriptor": "\nComments: 10 pages, 1 figure\n",
    "authors": [
      "Oscar Bustos-Brinez",
      "Joseph Gallego-Mejia",
      "Fabio A. Gonz\u00e1lez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14796"
  },
  {
    "id": "arXiv:2210.14797",
    "title": "Is Multi-Task Learning an Upper Bound for Continual Learning?",
    "abstract": "Continual and multi-task learning are common machine learning approaches to\nlearning from multiple tasks. The existing works in the literature often assume\nmulti-task learning as a sensible performance upper bound for various continual\nlearning algorithms. While this assumption is empirically verified for\ndifferent continual learning benchmarks, it is not rigorously justified.\nMoreover, it is imaginable that when learning from multiple tasks, a small\nsubset of these tasks could behave as adversarial tasks reducing the overall\nlearning performance in a multi-task setting. In contrast, continual learning\napproaches can avoid the performance drop caused by such adversarial tasks to\npreserve their performance on the rest of the tasks, leading to better\nperformance than a multi-task learner. This paper proposes a novel continual\nself-supervised learning setting, where each task corresponds to learning an\ninvariant representation for a specific class of data augmentations. In this\nsetting, we show that continual learning often beats multi-task learning on\nvarious benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100.",
    "descriptor": "",
    "authors": [
      "Zihao Wu",
      "Huy Tran",
      "Hamed Pirsiavash",
      "Soheil Kolouri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14797"
  },
  {
    "id": "arXiv:2210.14803",
    "title": "Don't Prompt, Search! Mining-based Zero-Shot Learning with Language  Models",
    "abstract": "Masked language models like BERT can perform text classification in a\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\nthis approach is highly sensitive to the template used to prompt the model, yet\npractitioners are blind when designing them in strict zero-shot settings. In\nthis paper, we propose an alternative mining-based approach for zero-shot\nlearning. Instead of prompting language models, we use regular expressions to\nmine labeled examples from unlabeled corpora, which can optionally be filtered\nthrough prompting, and used to finetune a pretrained model. Our method is more\nflexible and interpretable than prompting, and outperforms it on a wide range\nof tasks when using comparable templates. Our results suggest that the success\nof prompting can partly be explained by the model being exposed to similar\nexamples during pretraining, which can be directly retrieved through regular\nexpressions.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Mozes van de Kar",
      "Mengzhou Xia",
      "Danqi Chen",
      "Mikel Artetxe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14803"
  },
  {
    "id": "arXiv:2210.14808",
    "title": "Search for Concepts: Discovering Visual Concepts Using Direct  Optimization",
    "abstract": "Finding an unsupervised decomposition of an image into individual objects is\na key step to leverage compositionality and to perform symbolic reasoning.\nTraditionally, this problem is solved using amortized inference, which does not\ngeneralize beyond the scope of the training data, may sometimes miss correct\ndecompositions, and requires large amounts of training data. We propose finding\na decomposition using direct, unamortized optimization, via a combination of a\ngradient-based optimization for differentiable object properties and global\nsearch for non-differentiable properties. We show that using direct\noptimization is more generalizable, misses fewer correct decompositions, and\ntypically requires less data than methods based on amortized inference. This\nhighlights a weakness of the current prevalent practice of using amortized\ninference that can potentially be improved by integrating more direct\noptimization elements.",
    "descriptor": "",
    "authors": [
      "Pradyumna Reddy",
      "Paul Guerrero",
      "Niloy J. Mitra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14808"
  },
  {
    "id": "arXiv:2210.14813",
    "title": "HyperEF: Spectral Hypergraph Coarsening by Effective-Resistance  Clustering",
    "abstract": "This paper introduces a scalable algorithmic framework (HyperEF) for spectral\ncoarsening (decomposition) of large-scale hypergraphs by exploiting hyperedge\neffective resistances. Motivated by the latest theoretical framework for\nlow-resistance-diameter decomposition of simple graphs, HyperEF aims at\ndecomposing large hypergraphs into multiple node clusters with only a few\ninter-cluster hyperedges. The key component in HyperEF is a nearly-linear time\nalgorithm for estimating hyperedge effective resistances, which allows\nincorporating the latest diffusion-based non-linear quadratic operators defined\non hypergraphs. To achieve good runtime scalability, HyperEF searches within\nthe Krylov subspace (or approximate eigensubspace) for identifying the\nnearly-optimal vectors for approximating the hyperedge effective resistances.\nIn addition, a node weight propagation scheme for multilevel spectral\nhypergraph decomposition has been introduced for achieving even greater node\ncoarsening ratios. When compared with state-of-the-art hypergraph partitioning\n(clustering) methods, extensive experiment results on real-world VLSI designs\nshow that HyperEF can more effectively coarsen (decompose) hypergraphs without\nlosing key structural (spectral) properties of the original hypergraphs, while\nachieving over $70\\times$ runtime speedups over hMetis and $20\\times$ speedups\nover HyperSF.",
    "descriptor": "\nComments: Accepted on ICCAD 2022\n",
    "authors": [
      "Ali Aghdaei",
      "Zhuo Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14813"
  },
  {
    "id": "arXiv:2210.14814",
    "title": "BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic  Constraints for Adversarial Examples",
    "abstract": "Natural language inference (NLI) is critical for complex decision-making in\nbiomedical domain. One key question, for example, is whether a given biomedical\nmechanism is supported by experimental evidence. This can be seen as an NLI\nproblem but there are no directly usable datasets to address this. The main\nchallenge is that manually creating informative negative examples for this task\nis difficult and expensive. We introduce a novel semi-supervised procedure that\nbootstraps an NLI dataset from existing biomedical dataset that pairs\nmechanisms with experimental evidence in abstracts. We generate a range of\nnegative examples using nine strategies that manipulate the structure of the\nunderlying mechanisms both with rules, e.g., flip the roles of the entities in\nthe interaction, and, more importantly, as perturbations via logical\nconstraints in a neuro-logical decoding system. We use this procedure to create\na novel dataset for NLI in the biomedical domain, called BioNLI and benchmark\ntwo state-of-the-art biomedical classifiers. The best result we obtain is\naround mid 70s in F1, suggesting the difficulty of the task. Critically, the\nperformance on the different classes of negative examples varies widely, from\n97% F1 on the simple role change negative examples, to barely better than\nchance on the negative examples generated using neuro-logic decoding.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022, Data and evaluation suite available at this https URL\n",
    "authors": [
      "Mohaddeseh Bastan",
      "Mihai Surdeanu",
      "Niranjan Balasubramanian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14814"
  },
  {
    "id": "arXiv:2210.14815",
    "title": "On the Curious Case of $\\ell_2$ norm of Sense Embeddings",
    "abstract": "We show that the $\\ell_2$ norm of a static sense embedding encodes\ninformation related to the frequency of that sense in the training corpus used\nto learn the sense embeddings. This finding can be seen as an extension of a\npreviously known relationship for word embeddings to sense embeddings. Our\nexperimental results show that, in spite of its simplicity, the $\\ell_2$ norm\nof sense embeddings is a surprisingly effective feature for several word sense\nrelated tasks such as (a) most frequent sense prediction, (b) Word-in-Context\n(WiC), and (c) Word Sense Disambiguation (WSD). In particular, by simply\nincluding the $\\ell_2$ norm of a sense embedding as a feature in a classifier,\nwe show that we can improve WiC and WSD methods that use static sense\nembeddings.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Yi Zhou",
      "Danushka Bollegala"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14815"
  },
  {
    "id": "arXiv:2210.14816",
    "title": "Deep Subspace Encoders for Nonlinear System Identification",
    "abstract": "Using Artificial Neural Networks (ANN) for nonlinear system identification\nhas proven to be a promising approach, but despite of all recent research\nefforts, many practical and theoretical problems still remain open.\nSpecifically, noise handling and models, issues of consistency and reliable\nestimation under minimisation of the prediction error are the most severe\nproblems. The latter comes with numerous practical challenges such as explosion\nof the computational cost in terms of the number of data samples and the\noccurrence of instabilities during optimization. In this paper, we aim to\novercome these issues by proposing a method which uses a truncated prediction\nloss and a subspace encoder for state estimation. The truncated prediction loss\nis computed by selecting multiple truncated subsections from the time series\nand computing the average prediction loss. To obtain a computationally\nefficient estimation method that minimizes the truncated prediction loss, a\nsubspace encoder represented by an artificial neural network is introduced.\nThis encoder aims to approximate the state reconstructability map of the\nestimated model to provide an initial state for each truncated subsection given\npast inputs and outputs. By theoretical analysis, we show that, under mild\nconditions, the proposed method is locally consistent, increases optimization\nstability, and achieves increased data efficiency by allowing for overlap\nbetween the subsections. Lastly, we provide practical insights and user\nguidelines employing a numerical example and state-of-the-art benchmark\nresults.",
    "descriptor": "\nComments: Submitted to Automatica\n",
    "authors": [
      "Gerben I. Beintema",
      "Maarten Schoukens",
      "Roland T\u00f3th"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14816"
  },
  {
    "id": "arXiv:2210.14818",
    "title": "A Bioinspired Stiffness Tunable Sucker for Passive Adaptation and Firm  Attachment to Angular Substrates",
    "abstract": "The ability to adapt and conform to angular and uneven surfaces improves the\nsuction cup's performance in grasping and manipulation. However, in most cases,\nthe adaptation costs lack of required stiffness for manipulation after surface\nattachment; thus, the ideal scenario is to have compliance during adaptation\nand stiffness after attachment to the surface. Nevertheless, most stiffness\nmodulation techniques in suction cups require additional actuation. This\narticle presents a new stiffness tunable suction cup that adapts to steep\nangular surfaces. Using granular jamming as a vacuum driven stiffness\nmodulation provides a sensorless for activating the mechanism. Thus, the design\nis composed of a conventional active suction pad connected to a granular stalk,\nemulating a hinge behavior that is compliant during adaptation and has high\nstiffness after attachment is ensured. During the experiment, the suction cup\ncan adapt to angles up to 85 degrees with force lower than 0.5 N. We also\ninvestigated the effect of granular stalk's length on the adaptation and how\nthis design performs compared to passive adaptation without stiffness\nmodulation.",
    "descriptor": "",
    "authors": [
      "Arman Goshtasbi",
      "Ali Sadeghi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14818"
  },
  {
    "id": "arXiv:2210.14819",
    "title": "Network Functional Compression for Control Applications",
    "abstract": "The trend of future communication systems is to aim for the steering and\ncontrol of cyber physical systems. These systems can quickly become congested\nin environments like those presented in Industry 4.0. In these scenarios, a\nplethora of sensor data is transmitted wirelessly to multiple in network\ncontrollers that compute the control functions of the cyber physical systems.\nIn this paper, we show an implementation of network Functional Compression (FC)\nas a proof of concept to drastically reduce the data traffic in these\nscenarios. FC is a form of goal-oriented communication scheme in which the\nobjective of the sender receiver pair is to transmit the minimum amount of\ninformation to compute a function at the receiver end. In our scenario, the\nsenders transmit an encoded and compressed version of the sensor data to a\ndestination, an in-network controller interested in computing as its target\nfunction, a PID controller. We show that it is possible to achieve compression\nrates of over 50% in some cases by employing FC. We also show that using FC in\na distributed cascade fashion can achieve more significant compression rates\nwhile reducing computational costs.",
    "descriptor": "",
    "authors": [
      "Sifat Rezwan",
      "Juan A. Cabrera",
      "Frank H. P. Fitzek"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14819"
  },
  {
    "id": "arXiv:2210.14823",
    "title": "Visual Answer Localization with Cross-modal Mutual Knowledge Transfer",
    "abstract": "The goal of visual answering localization (VAL) in the video is to obtain a\nrelevant and concise time clip from a video as the answer to the given natural\nlanguage question. Early methods are based on the interaction modeling between\nvideo and text to predict the visual answer by the visual predictor. Later,\nusing textual predictor with subtitles for the VAL proves to be more precise.\nHowever, these existing methods still have cross-modal knowledge deviations\nfrom visual frames or textual subtitles. In this paper, we propose a\ncross-modal mutual knowledge transfer span localization (MutualSL) method to\nreduce the knowledge deviation. MutualSL has both visual predictor and textual\npredictor, where we expect the prediction results of these both to be\nconsistent, so as to promote semantic knowledge understanding between\ncross-modalities. On this basis, we design a one-way dynamic loss function to\ndynamically adjust the proportion of knowledge transferring. We have conducted\nextensive experiments on three public datasets for evaluation. The experimental\nresults show that our method outperforms other competitive state-of-the-art\n(SOTA) methods, demonstrating its effectiveness.",
    "descriptor": "\nComments: 4 pages, 3 figures, 2 tables\n",
    "authors": [
      "Yixuan Weng",
      "Bin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14823"
  },
  {
    "id": "arXiv:2210.14824",
    "title": "Analyzing Distribution Transformer Degradation with Increased Power  Electronic Loads",
    "abstract": "The influx of non-linear power electronic loads into the distribution network\nhas the potential to disrupt the existing distribution transformer operations.\nThey were not designed to mediate the excessive heating losses generated from\nthe harmonics. To have a good understanding of current standing challenges, a\nknowledge of the generation and load mix as well as the current harmonic\nestimations are essential for designing transformers and evaluating their\nperformance. In this paper, we investigate a mixture of essential power\nelectronic loads for a household designed in PSCAD/EMTdc and their potential\nimpacts on transformer eddy current losses and derating using harmonic\nanalysis. The various scenarios have been studied with increasing PV\npenetrations. The peak load conditions are chosen for each scenario to perform\na transformer derating analysis. Our findings reveal that in the presence of\nhigh power electronic loads (especially third harmonics), along with increasing\nPV generation may worsen transformer degradation. However, with a low amount of\npower electronic loads, additional PV generation helps to reduce the harmonic\ncontent in the current and improve transformer performance.",
    "descriptor": "\nComments: The paper has been accepted at ISGT NA 2023 and will be presented there\n",
    "authors": [
      "Bhaskar Mitra",
      "Ankit Singhal",
      "Soumya Kundu",
      "James P. Ogle"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14824"
  },
  {
    "id": "arXiv:2210.14826",
    "title": "A case for disaggregation of ML data processing",
    "abstract": "Machine Learning (ML) computation requires feeding input data for the models\nto ingest. Traditionally, input data processing happens on the same host as the\nML computation. The input data processing can however become a bottleneck of\nthe ML computation if there are insufficient resources to process data quickly\nenough. This slows down the ML computation and wastes valuable and scarce ML\nhardware (e.g. GPUs and TPUs) used by the ML computation.\nIn this paper, we present tf.data service, a disaggregated input data\nprocessing service built on top of tf.data. Our work goes beyond describing the\ndesign and implementation of a new system which disaggregates preprocessing\nfrom ML computation and presents: (1) empirical evidence based on production\nworkloads for the need of disaggregation, as well as quantitative evaluation of\nthe impact disaggregation has on the performance and cost of production\nworkloads, (2) benefits of disaggregation beyond horizontal scaling, (3)\nanalysis of tf.data service's adoption at Google, the lessons learned during\nbuilding and deploying the system and potential future lines of research opened\nup by our work.\nWe demonstrate that horizontally scaling data processing using tf.data\nservice helps remove input bottlenecks, achieving speedups of up to 110x and\njob cost reductions of up to 89x. We further show that tf.data service can\nsupport computation reuse through data sharing across ML jobs with identical\ndata processing pipelines (e.g. hyperparameter tuning jobs), incurring no\nperformance penalty and reducing overall resource cost. Finally, we show that\ntf.data service advanced features can benefit performance of non-input bound\njobs; in particular, coordinated data reads through tf.data service can yield\nup to 2x speedups and job cost savings for NLP jobs.",
    "descriptor": "",
    "authors": [
      "Andrew Audibert",
      "Yang Chen",
      "Dan Graur",
      "Ana Klimovic",
      "Jiri Simsa",
      "Chandramohan A. Thekkath"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14826"
  },
  {
    "id": "arXiv:2210.14827",
    "title": "A Nonlinear Sum of Squares Search for CAZAC Sequences",
    "abstract": "We report on a search for CAZAC sequences by using nonlinear sum of squares\noptimization. Up to equivalence, we found all length 7 CAZAC sequences. We\nobtained evidence suggesting there are finitely many length 10 CAZAC sequences\nwith a total of 3040 sequences. Last, we compute longer sequences and compare\ntheir aperiodic autocorrelation properties to known sequences. The code and\nresults of this search are publicly available through GitHub.",
    "descriptor": "\nComments: 6 pages, 4 figures, submitted to IEEE Radar Conference 2023\n",
    "authors": [
      "Mark Magsino",
      "Yixin Xu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14827"
  },
  {
    "id": "arXiv:2210.14830",
    "title": "Personalized Federated Learning via Heterogeneous Modular Networks",
    "abstract": "Personalized Federated Learning (PFL) which collaboratively trains a\nfederated model while considering local clients under privacy constraints has\nattracted much attention. Despite its popularity, it has been observed that\nexisting PFL approaches result in sub-optimal solutions when the joint\ndistribution among local clients diverges. To address this issue, we present\nFederated Modular Network (FedMN), a novel PFL approach that adaptively selects\nsub-modules from a module pool to assemble heterogeneous neural architectures\nfor different clients. FedMN adopts a light-weighted routing hypernetwork to\nmodel the joint distribution on each client and produce the personalized\nselection of the module blocks for each client. To reduce the communication\nburden in existing FL, we develop an efficient way to interact between the\nclients and the server. We conduct extensive experiments on the real-world test\nbeds and the results show both the effectiveness and efficiency of the proposed\nFedMN over the baselines.",
    "descriptor": "",
    "authors": [
      "Tianchun Wan",
      "Wei Cheng",
      "Dongsheng Luo",
      "Wenchao Yu",
      "Jingchao Ni",
      "Liang Tong",
      "Haifeng Chen",
      "Xiang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14830"
  },
  {
    "id": "arXiv:2210.14831",
    "title": "Streaming Radiance Fields for 3D Video Synthesis",
    "abstract": "We present an explicit-grid based method for efficiently reconstructing\nstreaming radiance fields for novel view synthesis of real world dynamic\nscenes. Instead of training a single model that combines all the frames, we\nformulate the dynamic modeling problem with an incremental learning paradigm in\nwhich per-frame model difference is trained to complement the adaption of a\nbase model on the current frame. By exploiting the simple yet effective tuning\nstrategy with narrow bands, the proposed method realizes a feasible framework\nfor handling video sequences on-the-fly with high training efficiency. The\nstorage overhead induced by using explicit grid representations can be\nsignificantly reduced through the use of model difference based compression. We\nalso introduce an efficient strategy to further accelerate model optimization\nfor each frame. Experiments on challenging video sequences demonstrate that our\napproach is capable of achieving a training speed of 15 seconds per-frame with\ncompetitive rendering quality, which attains $1000 \\times$ speedup over the\nstate-of-the-art implicit methods. Code is available at\nhttps://github.com/AlgoHunt/StreamRF.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Lingzhi Li",
      "Zhen Shen",
      "Zhongshu Wang",
      "Li Shen",
      "Ping Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14831"
  },
  {
    "id": "arXiv:2210.14833",
    "title": "Ballot stuffing and participation privacy in pollsite voting",
    "abstract": "We study the problem of simultaneously addressing both ballot stuffing and\nparticipation privacy for pollsite voting systems. Ballot stuffing is the\nattack where fake ballots (not cast by any eligible voter) are inserted into\nthe system. Participation privacy is about hiding which eligible voters have\nactually cast their vote. So far, the combination of ballot stuffing and\nparticipation privacy has been mostly studied for internet voting, where voters\nare assumed to own trusted computing devices. Such approaches are inapplicable\nto pollsite voting where voters typically vote bare handed. We present an\neligibility audit protocol to detect ballot stuffing in pollsite voting\nprotocols. This is done while protecting participation privacy from a remote\nobserver - one who does not physically observe voters during voting. Our\nprotocol can be instantiated as an additional layer on top of most existing\npollsite E2E-V voting protocols. To achieve our guarantees, we develop an\nefficient zero-knowledge proof (ZKP), that, given a value $v$ and a set $\\Phi$\nof commitments, proves $v$ is committed by some commitment in $\\Phi$, without\nrevealing which one. We call this a ZKP of reverse set membership because of\nits relationship to the popular ZKPs of set membership. This ZKP may be of\nindependent interest.",
    "descriptor": "",
    "authors": [
      "Prashant Agrawal",
      "Abhinav Nakarmi",
      "Mahabir Prasad Jhanwar",
      "Subodh Sharma",
      "Subhashis Banerjee"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14833"
  },
  {
    "id": "arXiv:2210.14837",
    "title": "NeuralSearchX: Serving a Multi-billion-parameter Reranker for  Multilingual Metasearch at a Low Cost",
    "abstract": "The widespread availability of search API's (both free and commercial) brings\nthe promise of increased coverage and quality of search results for metasearch\nengines, while decreasing the maintenance costs of the crawling and indexing\ninfrastructures. However, merging strategies frequently comprise complex\npipelines that require careful tuning, which is often overlooked in the\nliterature. In this work, we describe NeuralSearchX, a metasearch engine based\non a multi-purpose large reranking model to merge results and highlight\nsentences. Due to the homogeneity of our architecture, we could focus our\noptimization efforts on a single component. We compare our system with\nMicrosoft's Biomedical Search and show that our design choices led to a much\ncost-effective system with competitive QPS while having close to\nstate-of-the-art results on a wide range of public benchmarks. Human evaluation\non two domain-specific tasks shows that our retrieval system outperformed\nGoogle API by a large margin in terms of nDCG@10 scores. By describing our\narchitecture and implementation in detail, we hope that the community will\nbuild on our design choices. The system is available at\nhttps://neuralsearchx.nsx.ai.",
    "descriptor": "\nComments: published as a full paper at the DESIRES 2022 Conference. 13 pages\n",
    "authors": [
      "Thales Sales Almeida",
      "Thiago Laitz",
      "Jo\u00e3o Ser\u00f3dio",
      "Luiz Henrique Bonifacio",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14837"
  },
  {
    "id": "arXiv:2210.14842",
    "title": "Continuum Robot State Estimation Using Gaussian Process Regression on  $SE(3)$",
    "abstract": "Continuum robots have the potential to enable new applications in medicine,\ninspection, and countless other areas due to their unique shape, compliance,\nand size. Excellent progess has been made in the mechanical design and dynamic\nmodelling of continuum robots, to the point that there are some canonical\ndesigns, although new concepts continue to be explored. In this paper, we turn\nto the problem of state estimation for continuum robots that can been modelled\nwith the common Cosserat rod model. Sensing for continuum robots might comprise\nexternal camera observations, embedded tracking coils or strain gauges. We\nrepurpose a Gaussian process (GP) regression approach to state estimation,\ninitially developed for continuous-time trajectory estimation in $SE(3)$. In\nour case, the continuous variable is not time but arclength and we show how to\nestimate the continuous shape (and strain) of the robot (along with associated\nuncertainties) given discrete, noisy measurements of both pose and strain along\nthe length. We demonstrate our approach quantitatively through simulations as\nwell as through experiments. Our evaluations show that accurate and continuous\nestimates of a continuum robot's shape can be achieved, resulting in average\nend-effector errors between the estimated and ground truth shape as low as\n3.5mm and 0.016$^\\circ$ in simulation or 3.3mm and 0.035$^\\circ$ for unloaded\nconfigurations and 6.2mm and 0.041$^\\circ$ for loaded ones during experiments,\nwhen using discrete pose measurements.",
    "descriptor": "\nComments: Accepted for publication in International Journal of Robotics Research (IJRR). 26 pages, 18 figures\n",
    "authors": [
      "Sven Lilge",
      "Timothy D. Barfoot",
      "Jessica Burgner-Kahrs"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14842"
  },
  {
    "id": "arXiv:2210.14846",
    "title": "ProVe: A Pipeline for Automated Provenance Verification of Knowledge  Graphs against Textual Sources",
    "abstract": "Knowledge Graphs are repositories of information that gather data from a\nmultitude of domains and sources in the form of semantic triples, serving as a\nsource of structured data for various crucial applications in the modern web\nlandscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve\nas secondary sources of information and depend on well-documented and\nverifiable provenance to ensure their trustworthiness and usability. However,\ntheir ability to systematically assess and assure the quality of this\nprovenance, most crucially whether it properly supports the graph's\ninformation, relies mainly on manual processes that do not scale with size.\nProVe aims at remedying this, consisting of a pipelined approach that\nautomatically verifies whether a Knowledge Graph triple is supported by text\nextracted from its documented provenance. ProVe is intended to assist\ninformation curators and consists of four main steps involving rule-based\nmethods and machine learning models: text extraction, triple verbalisation,\nsentence selection, and claim verification. ProVe is evaluated on a Wikidata\ndataset, achieving promising results overall and excellent performance on the\nbinary classification task of detecting support from provenance, with 87.5%\naccuracy and 82.9% F1-macro on text-rich sources. The evaluation data and\nscripts used in this paper are available on GitHub and Figshare.",
    "descriptor": "",
    "authors": [
      "Gabriel Amaral",
      "Odinaldo Rodrigues",
      "Elena Simperl"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14846"
  },
  {
    "id": "arXiv:2210.14850",
    "title": "Text-to-speech synthesis from dark data with evaluation-in-the-loop data  selection",
    "abstract": "This paper proposes a method for selecting training data for text-to-speech\n(TTS) synthesis from dark data. TTS models are typically trained on\nhigh-quality speech corpora that cost much time and money for data collection,\nwhich makes it very challenging to increase speaker variation. In contrast,\nthere is a large amount of data whose availability is unknown (a.k.a, \"dark\ndata\"), such as YouTube videos. To utilize data other than TTS corpora,\nprevious studies have selected speech data from the corpora on the basis of\nacoustic quality. However, considering that TTS models robust to data noise\nhave been proposed, we should select data on the basis of its importance as\ntraining data to the given TTS model, not the quality of speech itself. Our\nmethod with a loop of training and evaluation selects training data on the\nbasis of the automatically predicted quality of synthetic speech of a given TTS\nmodel. Results of evaluations using YouTube data reveal that our method\noutperforms the conventional acoustic-quality-based method.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Kentaro Seki",
      "Shinnosuke Takamichi",
      "Takaaki Saeki",
      "Hiroshi Saruwatari"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14850"
  },
  {
    "id": "arXiv:2210.14852",
    "title": "Causality Detection using Multiple Annotation Decision",
    "abstract": "The paper describes the work that has been submitted to the 5th workshop on\nChallenges and Applications of Automated Extraction of socio-political events\nfrom text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3\nthat aims to detect causality in protest news corpus. The authors used\ndifferent large language models with customized cross-entropy loss functions\nthat exploit annotation information. The experiments showed that\nbert-based-uncased with refined cross-entropy outperformed the others,\nachieving a F1 score of 0.8501 on the Causal News Corpus dataset.",
    "descriptor": "",
    "authors": [
      "Quynh Anh Nguyen",
      "Arka Mitra"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14852"
  },
  {
    "id": "arXiv:2210.14855",
    "title": "Multi-level Data Representation For Training Deep Helmholtz Machines",
    "abstract": "A vast majority of the current research in the field of Machine Learning is\ndone using algorithms with strong arguments pointing to their biological\nimplausibility such as Backpropagation, deviating the field's focus from\nunderstanding its original organic inspiration to a compulsive search for\noptimal performance. Yet, there have been a few proposed models that respect\nmost of the biological constraints present in the human brain and are valid\ncandidates for mimicking some of its properties and mechanisms. In this paper,\nwe will focus on guiding the learning of a biologically plausible generative\nmodel called the Helmholtz Machine in complex search spaces using a heuristic\nbased on the Human Image Perception mechanism. We hypothesize that this model's\nlearning algorithm is not fit for Deep Networks due to its Hebbian-like local\nupdate rule, rendering it incapable of taking full advantage of the\ncompositional properties that multi-layer networks provide. We propose to\novercome this problem, by providing the network's hidden layers with visual\nqueues at different resolutions using a Multi-level Data representation. The\nresults on several image datasets showed the model was able to not only obtain\nbetter overall quality but also a wider diversity in the generated images,\ncorroborating our intuition that using our proposed heuristic allows the model\nto take more advantage of the network's depth growth. More importantly, they\nshow the unexplored possibilities underlying brain-inspired models and\ntechniques.",
    "descriptor": "",
    "authors": [
      "Jose Miguel Ramos",
      "Luis Sa-Couto",
      "Andreas Wichert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14855"
  },
  {
    "id": "arXiv:2210.14859",
    "title": "Recursive Secondary Controller for Voltage Profile Improvement Based on  the Virtual Admittance Concept",
    "abstract": "This paper proposes a recursive, virtual admittance based, secondary\ncontroller for DG units that improves the voltage profile in distribution\nnetworks. First, the adaptation of the virtual admittance concept for the goal\nof voltage regulation is explained. Then, a recursive secondary controller is\ndeveloped to periodically update the virtual admittance gains. The controller\nis formulated as an optimization problem with current and stability limitations\nas constraints. Measurements across the grid, transmitted through low-bandwidth\ncommunications, are used to simplify the calculations, resulting in a recursive\nalgorithm. Weight vectors are included in the objective function to allow\nparticipation flexibility of each converter. Results show that the primary\nvirtual admittance controller is able to mitigate over- and under-voltages in\nsteady state and under transient conditions. Subsequently, the secondary\ncontroller is shown to further improve the voltage profiles across the grid.\nExperimental results obtained from a laboratory environment, comprising three\nDG units and a grid emulator, validate the functionality of the complete\ncontrol structure.",
    "descriptor": "",
    "authors": [
      "Dionysios Moutevelis",
      "Javier Roldan-Perez",
      "Njegos Jankovic",
      "Milan Prodanovic"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14859"
  },
  {
    "id": "arXiv:2210.14861",
    "title": "The Information Bottleneck Principle in Corporate Hierarchies",
    "abstract": "The hierarchical nature of corporate information processing is a topic of\ngreat interest in economic and management literature. Firms are characterised\nby a need to make complex decisions, often aggregating partial and uncertain\ninformation, which greatly exceeds the attention capacity of constituent\nindividuals. However, the efficient transmission of these signals is still not\nfully understood. Recently, the information bottleneck principle has emerged as\na powerful tool for understanding the transmission of relevant information\nthrough intermediate levels in a hierarchical structure. In this paper we note\nthat the information bottleneck principle may similarly be applied directly to\ncorporate hierarchies. In doing so we provide a bridge between organisation\ntheory and that of rapidly expanding work in deep neural networks (DNNs),\nincluding the use of skip connections as a means of more efficient transmission\nof information in hierarchical organisations.",
    "descriptor": "\nComments: To appear in the Information-Theoretic Principles in Cognitive Systems Workshop at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 7 pages, 2 figures\n",
    "authors": [
      "Cameron Gordon"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2210.14861"
  },
  {
    "id": "arXiv:2210.14862",
    "title": "Visual Semantic Parsing: From Images to Abstract Meaning Representation",
    "abstract": "The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.",
    "descriptor": "\nComments: published in CoNLL 2022\n",
    "authors": [
      "Mohamed Ashraf Abdelsalam",
      "Zhan Shi",
      "Federico Fancellu",
      "Kalliopi Basioti",
      "Dhaivat J. Bhatt",
      "vladimir pavlovic",
      "Afsaneh Fazly"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14862"
  },
  {
    "id": "arXiv:2210.14867",
    "title": "Beyond English-Centric Bitexts for Better Multilingual Language  Representation Learning",
    "abstract": "In this paper, we elaborate upon recipes for building multilingual\nrepresentation models that are not only competitive with existing\nstate-of-the-art models but are also more parameter efficient, thereby\npromoting better adoption in resource-constrained scenarios and practical\napplications. We show that going beyond English-centric bitexts, coupled with a\nnovel sampling strategy aimed at reducing under-utilization of training data,\nsubstantially boosts performance across model sizes for both Electra and MLM\npre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language\nENcodings using Transformers which not only achieves state-of-the-art\nperformance over 5 cross-lingual tasks within all model size bands, is also\ncompetitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and\nexhibits competitive performance with mT5 XXL while being 5x and 6x smaller\nrespectively. We then show that our proposed method helps ameliorate the curse\nof multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and\n98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same\nsize band. We then analyze our models performance on extremely low resource\nlanguages and posit that scaling alone may not be sufficient for improving the\nperformance in this scenario",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Barun Patra",
      "Saksham Singhal",
      "Shaohan Huang",
      "Zewen Chi",
      "Li Dong",
      "Furu Wei",
      "Vishrav Chaudhary",
      "Xia Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14867"
  },
  {
    "id": "arXiv:2210.14868",
    "title": "Multi-lingual Evaluation of Code Generation Models",
    "abstract": "We present MBXP, an execution-based code completion benchmark in 10+\nprogramming languages. This collection of datasets is generated by our\nconversion framework that translates prompts and test cases from the original\nMBPP dataset to the corresponding data in a target language. Based on this\nbenchmark, we are able to evaluate code generation models in a multi-lingual\nfashion, and in particular discover generalization ability of language models\non out-of-domain languages, advantages of large multi-lingual models over\nmono-lingual, benefits of few-shot prompting, and zero-shot translation\nabilities. In addition, we use our code generation model to perform large-scale\nbootstrapping to obtain synthetic canonical solutions in several languages.\nThese solutions can be used for other code-related evaluations such as\ninsertion-based, summarization, or code translation tasks where we demonstrate\nresults and release as part of our benchmark.",
    "descriptor": "\nComments: Code and data release: this https URL\n",
    "authors": [
      "Ben Athiwaratkun",
      "Sanjay Krishna Gouda",
      "Zijian Wang",
      "Xiaopeng Li",
      "Yuchen Tian",
      "Ming Tan",
      "Wasi Uddin Ahmad",
      "Shiqi Wang",
      "Qing Sun",
      "Mingyue Shang",
      "Sujan Kumar Gonugondla",
      "Hantian Ding",
      "Varun Kumar",
      "Nathan Fulton",
      "Arash Farahani",
      "Siddhartha Jain",
      "Robert Giaquinto",
      "Haifeng Qian",
      "Murali Krishna Ramanathan",
      "Ramesh Nallapati",
      "Baishakhi Ray",
      "Parminder Bhatia",
      "Sudipta Sengupta",
      "Dan Roth",
      "Bing Xiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14868"
  },
  {
    "id": "arXiv:2210.14869",
    "title": "An Efficient Dynamic Multi-Sources To Single-Destination (DMS-SD)  Algorithm In Smart City Navigation Using Adjacent Matrix",
    "abstract": "Dijkstra's algorithm is one of the most popular classic path planning\nalgorithms, achieving optimal solutions across a wide range of challenging\ntasks. However, it only calculates the shortest distance from one vertex to\nanother, which is hard to directly apply to the Dynamic Multi-Sources to\nSingle-Destination (DMS-SD) problem. This paper proposes a modified Dijkstra\nalgorithm to address the DMS-SD problem, where the destination can be\ndynamically changed. Our method deploys the concept of Adjacent Matrix from\nFloyd's algorithm and achieves the goal with mathematical calculations. We\nformally show that all-pairs shortest distance information in Floyd's algorithm\nis not required in our algorithm. Extensive experiments verify the scalability\nand optimality of the proposed method.",
    "descriptor": "\nComments: 2022 International Conference On Human-Centered Cognitive Systems (HCCS)\n",
    "authors": [
      "Ziren Xiao",
      "Ruxin Xiao",
      "Chang Liu",
      "Honghao Gao",
      "Xiaolong Xu",
      "Shan Luo",
      "Xinheng Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.14869"
  },
  {
    "id": "arXiv:2210.14871",
    "title": "Impact of the 2022 OSTP Memo: A Bibliometric Analysis of U.S. Federally  Funded Publications, 2017-2021",
    "abstract": "On August 25, 2022, the White House Office of Science and Technology Policy\n(OSTP) released a memo regarding public access to scientific research. Signed\nby Director Alondra Nelson, this updated guidance eliminated the 12-month\nembargo period on publications arising from U.S. federal funding that had been\nallowed from a previous 2013 OSTP memo.\nWhile reactions to this updated federal guidance have been plentiful, to date\nthere has not been a detailed analysis of the publications which would fall\nunder this new framework. The OSTP released a companion report along with the\nmemo, but it only provided a broad estimate of total numbers affected per year.\nTherefore, this study seeks to more deeply investigate the characteristics of\nU.S. federally funded research over a 5-year period from 2017-2021 to better\nunderstand the updated guidance's impact. It uses a manually created custom\nfilter in the Dimensions database to return only publications that arise from\nU.S. federal funding.\nResults show that an average of 265,000 articles were published each year\nthat acknowledge U.S. federal funding agencies, and these research outputs are\nfurther examined by publisher, journal title, institutions, and Open Access\nstatus.\nInteractive versions of the plots are available online at\nhttps://bit.ly/qss-ostp.",
    "descriptor": "\nComments: 27 pages, including Appendix. 11 figures, 6 tables\n",
    "authors": [
      "Eric Schares"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.14871"
  },
  {
    "id": "arXiv:2210.14874",
    "title": "Anisotropic multiresolution analyses for deep fake detection",
    "abstract": "Generative Adversarial Networks (GANs) have paved the path towards entirely\nnew media generation capabilities at the forefront of image, video, and audio\nsynthesis. However, they can also be misused and abused to fabricate elaborate\nlies, capable of stirring up the public debate. The threat posed by GANs has\nsparked the need to discern between genuine content and fabricated one.\nPrevious studies have tackled this task by using classical machine learning\ntechniques, such as k-nearest neighbours and eigenfaces, which unfortunately\ndid not prove very effective. Subsequent methods have focused on leveraging on\nfrequency decompositions, i.e., discrete cosine transform, wavelets, and\nwavelet packets, to preprocess the input features for classifiers. However,\nexisting approaches only rely on isotropic transformations. We argue that,\nsince GANs primarily utilize isotropic convolutions to generate their output,\nthey leave clear traces, their fingerprint, in the coefficient distribution on\nsub-bands extracted by anisotropic transformations. We employ the fully\nseparable wavelet transform and multiwavelets to obtain the anisotropic\nfeatures to feed to standard CNN classifiers. Lastly, we find the fully\nseparable transform capable of improving the state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Wei Huang",
      "Michelangelo Valsecchi",
      "Michael Multerer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14874"
  },
  {
    "id": "arXiv:2210.14878",
    "title": "Duality-Based Stochastic Policy Optimization for Estimation with Unknown  Noise Covariances",
    "abstract": "Duality of control and estimation allows mapping recent advances in\ndata-guided control to the estimation setup. This paper formalizes and utilizes\nsuch a mapping by considering learning the optimal (steady-state) Kalman gain\nwhen process and measurement noise statistics are unknown. Specifically,\nbuilding on the duality between synthesizing optimal control and estimation\ngains, the filter design problem is formalized as direct policy learning;\nsubsequently, a Stochastic Gradient Descent (SGD) approach is adopted to learn\nthe optimal filter gain. In this direction, control and estimation duality is\nalso used to extend existing theoretical results for direct policy updates for\nLinear Quadratic Regulator (LQR) to establish convergence of the proposed\nalgorithm-while addressing subtle differences between the two synthesis\nproblems. The results are illustrated via several numerical examples.",
    "descriptor": "",
    "authors": [
      "Shahriar Talebi",
      "Amirhossein Taghvaei",
      "Mehran Mesbahi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14878"
  },
  {
    "id": "arXiv:2210.14879",
    "title": "A Control-theoretic Model for Bidirectional Molecular Communication  Systems",
    "abstract": "Molecular communication (MC) enables cooperation of spatially dispersed\nmolecular robots through the feedback control mediated by diffusing signal\nmolecules. However, conventional analysis frameworks for the MC channels mostly\nconsiders the dynamics of unidirectional communication, lacking the effect of\nfeedback interactions. In this paper, we propose a general control-theoretic\nmodeling framework for bidirectional MC systems capable of capturing the\ndynamics of feedback control via MC in a systematic manner. The proposed\nframework considers not only the dynamics of molecular diffusion but also the\nboundary dynamics at the molecular robots that captures the lag at the\ninterface between the communication channel and the molecular robots, and thus,\ndynamics of the bidirectional communication systems can be systematically\nanalyzed using methods in control theory. We perform a frequency response\nanalysis based on the proposed framework to show a general design guideline for\nMC systems to transfer signal with desired control bandwidth. Finally, these\nresults are demonstrated by showing the step-by-step design procedure of a\nspecific MC channel satisfying a given specification.",
    "descriptor": "",
    "authors": [
      "Taishi Kotsuka",
      "Yutaka Hori"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.14879"
  },
  {
    "id": "arXiv:2210.14880",
    "title": "Integrated Sensing and Communication in Distributed Antenna Networks",
    "abstract": "In this paper, we investigate the resource allocation design for integrated\nsensing and communication (ISAC) in distributed antenna networks (DANs). In\nparticular, coordinated by a central processor (CP), a set of remote radio\nheads (RRHs) provide communication services to multiple users and sense several\ntarget locations within an ISAC frame. To avoid the severe interference between\nthe information transmission and the radar echo, we propose to divide the ISAC\nframe into a communication phase and a sensing phase. During the communication\nphase, the data signal is generated at the CP and then conveyed to the RRHs via\nfronthaul links. As for the sensing phase, based on pre-determined RRH-target\npairings, each RRH senses a dedicated target location with a synthesized\nhighly-directional beam and then transfers the samples of the received echo to\nthe CP via its fronthaul link for further processing of the sensing\ninformation. Taking into account the limited fronthaul capacity and the\nquality-of-service requirements of both communication and sensing, we jointly\noptimize the durations of the two phases, the information beamforming, and the\ncovariance matrix of the sensing signal for minimization of the total energy\nconsumption over a given finite time horizon. To solve the formulated\nnon-convex design problem, we develop a low-complexity alternating optimization\nalgorithm which converges to a suboptimal solution. Simulation results show\nthat the proposed scheme achieves significant energy savings compared to two\nbaseline schemes. Moreover, our results reveal that for efficient ISAC in\nwireless networks, energy-focused short-duration pulses are favorable for\nsensing while low-power long-duration signals are preferable for communication.",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Dongfang Xu",
      "Ata Khalili",
      "Xianghao Yu",
      "Derrick Wing Kwan Ng",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14880"
  },
  {
    "id": "arXiv:2210.14884",
    "title": "PREPRINT: Do OpenSSF Scorecard Practices Contribute to Fewer  Vulnerabilities?",
    "abstract": "Due to the ever-increasing security breaches, practitioners are motivated to\nproduce more secure software. In the United States, the White House Office\nreleased a memorandum on Executive Order (EO) 14028 that mandates organizations\nprovide self-attestation of the use of secure software development practices.\nThe OpenSSF Scorecard project allows practitioners to measure the use of\nsoftware security practices automatically. However, little research has been\ndone to determine whether the use of security practices improves package\nsecurity, particularly which security practices have the biggest impact on\nsecurity outcomes. The goal of this study is to assist practitioners and\nresearchers making informed decisions on which security practices to adopt\nthrough the development of models between software security practice scores and\nsecurity vulnerability counts.\nTo that end, we developed five supervised machine learning models for npm and\nPyPI packages using the OpenSSF Scorecared security practices scores and\naggregate security scores as predictors and the number of externally-reported\nvulnerabilities as a target variable. Our models found four security practices\n(Maintained, Code Review, Branch Protection, and Security Policy) were the most\nimportant practices influencing vulnerability count. However, we had low R^2\n(ranging from 9% to 12%) when we tested the models to predict vulnerability\ncounts. Additionally, we observed that the number of reported vulnerabilities\nincreased rather than reduced as the aggregate security score of the packages\nincreased. Both findings indicate that additional factors may influence the\npackage vulnerability count. We suggest that vulnerability count and security\nscore data be refined such that these measures may be used to provide\nactionable guidance on security practices.",
    "descriptor": "\nComments: 12 pages, 2 Figures\n",
    "authors": [
      "Nusrat Zahan",
      "Shohanuzzaman Shohan",
      "Dan Harris",
      "Laurie Williams"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14884"
  },
  {
    "id": "arXiv:2210.14888",
    "title": "A Decision Framework for Blockchain Adoption",
    "abstract": "Blockchain and distributed ledger technologies are gaining the interest of\nthe academy, companies, and institutions. Nonetheless, the path toward\nblockchain adoption is not straightforward, as blockchain is a complex\ntechnology that requires revisiting the standard way of addressing problems and\ntackling them from a decentralized perspective. Thus, decision-makers adopt\nblockchain technology for the wrong reasons or prefer it to more suitable ones.\nThis work presents a decision framework for blockchain adoption to help\ndecision-makers decide whether blockchain is applicable, valuable, and\npreferable to other technologies. In particular, The decision framework is\ncomposed of a small set of questions that can be answered from a managerial\nstandpoint and that do not require a deep technical knowledge of\nblockchain-related topics.",
    "descriptor": "\nComments: 10 pages, 1 figure, 1 table\n",
    "authors": [
      "Vittorio Capocasale",
      "Guido Perboli"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14888"
  },
  {
    "id": "arXiv:2210.14889",
    "title": "Perfectly Secure Steganography Using Minimum Entropy Coupling",
    "abstract": "Steganography is the practice of encoding secret information into innocuous\ncontent in such a manner that an adversarial third party would not realize that\nthere is hidden meaning. While this problem has classically been studied in\nsecurity literature, recent advances in generative models have led to a shared\ninterest among security and machine learning researchers in developing scalable\nsteganography techniques. In this work, we show that a steganography procedure\nis perfectly secure under \\citet{cachin_perfect}'s information theoretic-model\nof steganography if and only if it is induced by a coupling. Furthermore, we\nshow that, among perfectly secure procedures, a procedure is maximally\nefficient if and only if it is induced by a minimum entropy coupling. These\ninsights yield what are, to the best of our knowledge, the first steganography\nalgorithms to achieve perfect security guarantees with non-trivial efficiency;\nadditionally, these algorithms are highly scalable. To provide empirical\nvalidation, we compare a minimum entropy coupling-based approach to three\nmodern baselines -- arithmetic coding, Meteor, and adaptive dynamic grouping --\nusing GPT-2 and WaveRNN as communication channels. We find that the minimum\nentropy coupling-based approach yields superior encoding efficiency, despite\nits stronger security constraints. In aggregate, these results suggest that it\nmay be natural to view information-theoretic steganography through the lens of\nminimum entropy coupling.",
    "descriptor": "",
    "authors": [
      "Christian Schroeder de Witt",
      "Samuel Sokota",
      "J. Zico Kolter",
      "Jakob Foerster",
      "Martin Strohmeier"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.14889"
  },
  {
    "id": "arXiv:2210.14891",
    "title": "Broken Neural Scaling Laws",
    "abstract": "We present a smoothly broken power law functional form that accurately models\nand extrapolates the scaling behaviors of deep neural networks (i.e. how the\nevaluation metric of interest varies as the amount of compute used for\ntraining, number of model parameters, or training dataset size varies) for each\ntask within a large and diverse set of upstream and downstream tasks, in\nzero-shot, prompted, and fine-tuned settings. This set includes large-scale\nvision and unsupervised language tasks, diffusion generative modeling of\nimages, arithmetic, and reinforcement learning. When compared to other\nfunctional forms for neural scaling behavior, this functional form yields\nextrapolations of scaling behavior that often are considerably more accurate\n(root mean squared log error of its extrapolations are 0.86 times that of\nprevious state-of-the-art on average) on this set. Moreover, this functional\nform accurately models and extrapolates scaling behavior that other functional\nforms are incapable of expressing such as the non-monotonic transitions present\nin the scaling behavior of phenomena such as double descent and the delayed,\nsharp inflection points present in the scaling behavior of tasks such as\narithmetic. Code is available at\nhttps://github.com/ethancaballero/broken_neural_scaling_laws",
    "descriptor": "",
    "authors": [
      "Ethan Caballero",
      "Kshitij Gupta",
      "Irina Rish",
      "David Krueger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14891"
  },
  {
    "id": "arXiv:2210.14896",
    "title": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image  Generative Models",
    "abstract": "With recent advancements in diffusion models, users can generate high-quality\nimages by writing text prompts in natural language. However, generating images\nwith desired details requires proper prompts, and it is often unclear how a\nmodel reacts to different prompts and what the best prompts are. To help\nresearchers tackle these critical challenges, we introduce DiffusionDB, the\nfirst large-scale text-to-image prompt dataset. DiffusionDB contains 2 million\nimages generated by Stable Diffusion using prompts and hyperparameters\nspecified by real users. We analyze prompts in the dataset and discuss key\nproperties of these prompts. The unprecedented scale and diversity of this\nhuman-actuated dataset provide exciting research opportunities in understanding\nthe interplay between prompts and generative models, detecting deepfakes, and\ndesigning human-AI interaction tools to help users more easily use these\nmodels. DiffusionDB is publicly available at:\nhttps://poloclub.github.io/diffusiondb.",
    "descriptor": "\nComments: 13 pages, 4 figures. The dataset is available at this https URL The code is at this https URL\n",
    "authors": [
      "Zijie J. Wang",
      "Evan Montoya",
      "David Munechika",
      "Haoyang Yang",
      "Benjamin Hoover",
      "Duen Horng Chau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14896"
  },
  {
    "id": "arXiv:2210.14897",
    "title": "Searching Dense Point Correspondences via Permutation Matrix Learning",
    "abstract": "Although 3D point cloud data has received widespread attentions as a general\nform of 3D signal expression, applying point clouds to the task of dense\ncorrespondence estimation between 3D shapes has not been investigated widely.\nFurthermore, even in the few existing 3D point cloud-based methods, an\nimportant and widely acknowledged principle, i.e . one-to-one matching, is\nusually ignored. In response, this paper presents a novel end-to-end\nlearning-based method to estimate the dense correspondence of 3D point clouds,\nin which the problem of point matching is formulated as a zero-one assignment\nproblem to achieve a permutation matching matrix to implement the one-to-one\nprinciple fundamentally. Note that the classical solutions of this assignment\nproblem are always non-differentiable, which is fatal for deep learning\nframeworks. Thus we design a special matching module, which solves a doubly\nstochastic matrix at first and then projects this obtained approximate solution\nto the desired permutation matrix. Moreover, to guarantee end-to-end learning\nand the accuracy of the calculated loss, we calculate the loss from the learned\npermutation matrix but propagate the gradient to the doubly stochastic matrix\ndirectly which bypasses the permutation matrix during the backward propagation.\nOur method can be applied to both non-rigid and rigid 3D point cloud data and\nextensive experiments show that our method achieves state-of-the-art\nperformance for dense correspondence learning.",
    "descriptor": "\nComments: Accepted to IEEE Signal Processing Letters (SPL) 2022\n",
    "authors": [
      "Zhiyuan Zhang",
      "Jiadai Sun",
      "Yuchao Dai",
      "Bin Fan",
      "Qi Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14897"
  },
  {
    "id": "arXiv:2210.14898",
    "title": "CU-Net: LiDAR Depth-Only Completion With Coupled U-Net",
    "abstract": "LiDAR depth-only completion is a challenging task to estimate dense depth\nmaps only from sparse measurement points obtained by LiDAR. Even though the\ndepth-only methods have been widely developed, there is still a significant\nperformance gap with the RGB-guided methods that utilize extra color images. We\nfind that existing depth-only methods can obtain satisfactory results in the\nareas where the measurement points are almost accurate and evenly distributed\n(denoted as normal areas), while the performance is limited in the areas where\nthe foreground and background points are overlapped due to occlusion (denoted\nas overlap areas) and the areas where there are no measurement points around\n(denoted as blank areas) since the methods have no reliable input information\nin these areas. Building upon these observations, we propose an effective\nCoupled U-Net (CU-Net) architecture for depth-only completion. Instead of\ndirectly using a large network for regression, we employ the local U-Net to\nestimate accurate values in the normal areas and provide the global U-Net with\nreliable initial values in the overlap and blank areas. The depth maps\npredicted by the two coupled U-Nets are fused by learned confidence maps to\nobtain final results. In addition, we propose a confidence-based outlier\nremoval module, which removes outliers using simple judgment conditions. Our\nproposed method boosts the final results with fewer parameters and achieves\nstate-of-the-art results on the KITTI benchmark. Moreover, it owns a powerful\ngeneralization ability under various depth densities, varying lighting, and\nweather conditions.",
    "descriptor": "\nComments: Accepted to IEEE Robotics and Automation Letters (RA-L), Code: this https URL\n",
    "authors": [
      "Yufei Wang",
      "Yuchao Dai",
      "Qi Liu",
      "Peng Yang",
      "Jiadai Sun",
      "Bo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14898"
  },
  {
    "id": "arXiv:2210.14899",
    "title": "Learning a Task-specific Descriptor for Robust Matching of 3D Point  Clouds",
    "abstract": "Existing learning-based point feature descriptors are usually task-agnostic,\nwhich pursue describing the individual 3D point clouds as accurate as possible.\nHowever, the matching task aims at describing the corresponding points\nconsistently across different 3D point clouds. Therefore these too accurate\nfeatures may play a counterproductive role due to the inconsistent point\nfeature representations of correspondences caused by the unpredictable noise,\npartiality, deformation, \\etc, in the local geometry. In this paper, we propose\nto learn a robust task-specific feature descriptor to consistently describe the\ncorrect point correspondence under interference. Born with an Encoder and a\nDynamic Fusion module, our method EDFNet develops from two aspects. First, we\naugment the matchability of correspondences by utilizing their repetitive local\nstructure. To this end, a special encoder is designed to exploit two input\npoint clouds jointly for each point descriptor. It not only captures the local\ngeometry of each point in the current point cloud by convolution, but also\nexploits the repetitive structure from paired point cloud by Transformer.\nSecond, we propose a dynamical fusion module to jointly use different scale\nfeatures. There is an inevitable struggle between robustness and\ndiscriminativeness of the single scale feature. Specifically, the small scale\nfeature is robust since little interference exists in this small receptive\nfield. But it is not sufficiently discriminative as there are many repetitive\nlocal structures within a point cloud. Thus the resultant descriptors will lead\nto many incorrect matches. In contrast, the large scale feature is more\ndiscriminative by integrating more neighborhood information. ...",
    "descriptor": "\nComments: Accepted to IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 2022\n",
    "authors": [
      "Zhiyuan Zhang",
      "Yuchao Dai",
      "Bin Fan",
      "Jiadai Sun",
      "Mingyi He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14899"
  },
  {
    "id": "arXiv:2209.08563",
    "title": "Submodularity and pairwise independence",
    "abstract": "In this paper, we provide a characterization of the expected value of\nsubmodular set functions with pairwise independent random input. The set of\npairwise independent (uncorrelated) probability distributions contains the\nmutually independent distribution and is contained within the set of\narbitrarily dependent (correlated) distributions. We study the ratio of the\nmaximum expected value of a function with arbitrary dependence among the random\ninput with given marginal probabilities to the maximum expected value of the\nfunction with pairwise independent random input with the same marginal\nprobabilities. The definition of the ratio is inspired from the correlation gap\nratio of Agrawal et al. (2012) and Calinescu et al. (2007). Our results show\nthat for any monotone submodular set function defined on n variables, the ratio\nis bounded from above by 4/3 in the following cases: (a) for small n\n(specifically n = 2, 3) with general marginal probabilities, and (b) for\ngeneral n with small marginal probabilities. The bound is tight in cases (a)\nand (b). This contrasts with the e/(e-1) bound on the correlation gap ratio for\nmonotone submodular set functions with mutually independent random input which\nis known to be tight in case (b). Our results illustrate a fundamental\ndifference in the behavior of submodular functions with weaker notions of\nindependence. We discuss an application in distributionally robust optimization\nand end the paper with a conjecture.",
    "descriptor": "\nComments: 20 pages, 2 figures, 6 tables\n",
    "authors": [
      "Arjun Ramachandra",
      "Karthik Natarajan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2209.08563"
  },
  {
    "id": "arXiv:2210.11921",
    "title": "Data reconstruction of turbulent flows with Gappy POD, Extended POD and  Generative Adversarial Networks",
    "abstract": "Three methods are used to reconstruct two-dimensional instantaneous velocity\nfields in a turbulent flow under rotation. The first two methods both use the\nlinear proper orthogonal decomposition (POD), which are Gappy POD (GPOD) and\nExtended POD (EPOD), while the third one reconstructs the flow using a fully\nnon-linear Convolutional Neural Network embedded in a Generative Adversarial\nNetwork (GAN). First, we show that there is always an optimal number of modes\nregarding a specific gap for the GPOD with dimension reduction. Moreover,\nadopting a Lasso regularizer for GPOD provides comparable reconstruction\nresults. In order to systematically compare the applicability of the three\ntools, we consider a square gap at changing the size. Results show that\ncompared with POD-based methods, GAN reconstruction not only has a smaller\n$L_2$ error, but also better turbulent statistics of both the velocity module\nand the velocity module gradient. This can be attributed to the ability of\nnonlinearity expression of the network and the presence of adversarial loss\nduring the GAN training. We also investigate effects of the adversarial ratio,\nwhich controls the compromising between the $L_2$ error and the statistical\nproperties. Finally, we assess the reconstruction on random gappiness. All\nmethods perform well for small- and medium-size gaps, while GAN works better\nwhen the gappiness is large.",
    "descriptor": "",
    "authors": [
      "Tianyi Li",
      "Michele Buzzicotti",
      "Luca Biferale",
      "Fabio Bonaccorso",
      "Shiyi Chen",
      "Minping Wan"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Chaotic Dynamics (nlin.CD)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.11921"
  },
  {
    "id": "arXiv:2210.14231",
    "title": "NAS-PRNet: Neural Architecture Search generated Phase Retrieval Net for  Off-axis Quantitative Phase Imaging",
    "abstract": "Single neural networks have achieved simultaneous phase retrieval with\naberration compensation and phase unwrapping in off-axis Quantitative Phase\nImaging (QPI). However, when designing the phase retrieval neural network\narchitecture, the trade-off between computation latency and accuracy has been\nlargely neglected. Here, we propose Neural Architecture Search (NAS) generated\nPhase Retrieval Net (NAS-PRNet), which is an encoder-decoder style neural\nnetwork, automatically found from a large neural network architecture search\nspace. The NAS scheme in NAS-PRNet is modified from SparseMask, in which the\nlearning of skip connections between the encoder and the decoder is formulated\nas a differentiable NAS problem, and the gradient decent is applied to\nefficiently search the optimal skip connections. Using MobileNet-v2 as the\nencoder and a synthesized loss that incorporates phase reconstruction and\nnetwork sparsity losses, NAS-PRNet has realized fast and accurate phase\nretrieval of biological cells. When tested on a cell dataset, NAS-PRNet has\nachieved a Peak Signal-to-Noise Ratio (PSNR) of 36.1 dB, outperforming the\nwidely used U-Net and original SparseMask-generated neural network. Notably,\nthe computation latency of NAS-PRNet is only 31 ms which is 12 times less than\nU-Net. Moreover, the connectivity scheme in NAS-PRNet, identified from one\noff-axis QPI system, can be well fitted to another with different fringe\npatterns.",
    "descriptor": "",
    "authors": [
      "Xin Shu",
      "Mengxuan Niu",
      "Yi Zhang",
      "Renjie Zhou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14231"
  },
  {
    "id": "arXiv:2210.14245",
    "title": "CaloFlow for CaloChallenge Dataset 1",
    "abstract": "CaloFlow is a new and promising approach to fast calorimeter simulation based\non normalizing flows. Applying CaloFlow to the photon and charged pion Geant4\nshowers of Dataset 1 of the Fast Calorimeter Simulation Challenge 2022, we show\nhow it can produce high-fidelity samples with a sampling time that is several\norders of magnitude faster than Geant4. We demonstrate the fidelity of the\nsamples using calorimeter shower images, histograms of high level features, and\naggregate metrics such as a classifier trained to distinguish CaloFlow from\nGeant4 samples.",
    "descriptor": "\nComments: 31 pages, 16 figures\n",
    "authors": [
      "Claudius Krause",
      "Ian Pang",
      "David Shih"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.14245"
  },
  {
    "id": "arXiv:2210.14265",
    "title": "Uncloneable Cryptography",
    "abstract": "The no-cloning theorem asserts that, unlike classical information, quantum\ninformation cannot be copied. This seemingly undesirable phenomenon is\nharnessed in quantum cryptography. Uncloneable cryptography studies settings in\nwhich the impossibility of copying is a desired property, and achieves forms of\nsecurity that are classically unattainable. The first example discovered and\nanalyzed was in the context of cash. On the one hand, we want users to hold the\ncash; on the other hand, the cash should be hard to counterfeit. Quantum money\nuses variants of the no-cloning theorem to make counterfeiting impossible.\nIn the past decade, this field developed in various directions: several\nflavors of quantum money, such as classically verifiable, locally verifiable,\nsemi-quantum, quantum coins, and quantum lightning were constructed. New\nuncloneable primitives were introduced, such as uncloneable signatures, quantum\ncopy protection for classical software, pseudorandom states, and several\nuncloneable forms of encryption. This work is a gentle introduction to these\ntopics.",
    "descriptor": "",
    "authors": [
      "Or Sattath"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14265"
  },
  {
    "id": "arXiv:2210.14298",
    "title": "Wasserstein Archetypal Analysis",
    "abstract": "Archetypal analysis is an unsupervised machine learning method that\nsummarizes data using a convex polytope. In its original formulation, for fixed\nk, the method finds a convex polytope with k vertices, called archetype points,\nsuch that the polytope is contained in the convex hull of the data and the mean\nsquared Euclidean distance between the data and the polytope is minimal.\nIn the present work, we consider an alternative formulation of archetypal\nanalysis based on the Wasserstein metric, which we call Wasserstein archetypal\nanalysis (WAA). In one dimension, there exists a unique solution of WAA and, in\ntwo dimensions, we prove existence of a solution, as long as the data\ndistribution is absolutely continuous with respect to Lebesgue measure. We\ndiscuss obstacles to extending our result to higher dimensions and general data\ndistributions. We then introduce an appropriate regularization of the problem,\nvia a Renyi entropy, which allows us to obtain existence of solutions of the\nregularized problem for general data distributions, in arbitrary dimensions. We\nprove a consistency result for the regularized problem, ensuring that if the\ndata are iid samples from a probability measure, then as the number of samples\nis increased, a subsequence of the archetype points converges to the archetype\npoints for the limiting data distribution, almost surely. Finally, we develop\nand implement a gradient-based computational approach for the two-dimensional\nproblem, based on the semi-discrete formulation of the Wasserstein metric. Our\nanalysis is supported by detailed computational experiments.",
    "descriptor": "",
    "authors": [
      "Katy Craig",
      "Braxton Osting",
      "Dong Wang",
      "Yiming Xu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.14298"
  },
  {
    "id": "arXiv:2210.14321",
    "title": "Artificial ASMR: A Cyber-Psychological Study",
    "abstract": "The popularity of Autonomous Sensory Meridian Response (ASMR) has\nskyrockteted over the past decade, but scientific studies on it are still few\nand immature. With our attention caught by the common acoustic patterns in ASMR\naudios, we investigate the correlation between the time-frequency and cyclic\nfeatures of audio signals and their effectiveness in triggering ASMR effects. A\ncyber-psychological approach that combines signal processing, artificial\nintelligence, and experimental psychology is taken, with which we are able to\nidentify ASMR-related acoustic features, and therewith synthesize random\nartificial ASMR audios.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Zexiu Wu",
      "Bin Han",
      "C. Clark Cao",
      "Hans. D. Schotten"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14321"
  },
  {
    "id": "arXiv:2210.14330",
    "title": "A single-cell gene expression language model",
    "abstract": "Gene regulation is a dynamic process that connects genotype and phenotype.\nGiven the difficulty of physically mapping mammalian gene circuitry, we require\nnew computational methods to learn regulatory rules. Natural language is a\nvaluable analogy to the communication of regulatory control. Machine learning\nsystems model natural language by explicitly learning context dependencies\nbetween words. We propose a similar system applied to single-cell RNA\nexpression profiles to learn context dependencies between genes. Our model,\nExceiver, is trained across a diversity of cell types using a self-supervised\ntask formulated for discrete count data, accounting for feature sparsity. We\nfound agreement between the similarity profiles of latent sample\nrepresentations and learned gene embeddings with respect to biological\nannotations. We evaluated Exceiver on a new dataset and a downstream prediction\ntask and found that pretraining supports transfer learning. Our work provides a\nframework to model gene regulation on a single-cell level and transfer\nknowledge to downstream tasks.",
    "descriptor": "\nComments: 10 pages, 5 figures, Accepted at Learning Meaningful Representations of Life Workshop, 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "William Connell",
      "Umair Khan",
      "Michael J. Keiser"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2210.14330"
  },
  {
    "id": "arXiv:2210.14335",
    "title": "A Noise-aware Transpiler for Optimal Amplitude Amplification",
    "abstract": "Amplitude amplification provides a quadratic speed-up for an array of quantum\nalgorithms when run on a quantum machine perfectly isolated from its\nenvironment. However, the advantage is substantially diminished as the NISQ-era\nquantum machines lack the large number of qubits necessary to provide error\ncorrection. Noise in the computation grows with the number of gate counts in\nthe circuit with each iteration of amplitude amplification. After a certain\nnumber of amplifications, the loss in accuracy from the gate noise starts to\novershadow the gain in accuracy due to amplification, forming an inflection\npoint. Beyond this point, accuracy continues to deteriorate until the machine\nreaches a maximally mixed state where the result is uniformly random. Hence,\nquantum transpilers should take the noise parameters of the underlying quantum\nmachine into consideration such that the circuit can be optimized to attain the\nmaximal accuracy possible for that machine. In this work, we propose an\nextension to the transpiler that predicts the accuracy of the result at every\namplification with high fidelity by applying pure Bayesian analysis to\nindividual gate noise rates. Using this information, it finds the inflection\npoint and optimizes the circuit by halting amplification at that point. The\nprediction is made without needing to execute the circuit either on a quantum\nsimulator or an actual quantum machine.",
    "descriptor": "",
    "authors": [
      "Debashis Ganguly",
      "Wondun Ahn"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.14335"
  },
  {
    "id": "arXiv:2210.14341",
    "title": "Modular Software for Real-Time Quantum Control Systems",
    "abstract": "Real-time control software and hardware is essential for operating quantum\ncomputers. In particular, the software plays a crucial role in bridging the gap\nbetween quantum programs and the quantum system. Unfortunately, current control\nsoftware is often optimized for a specific system at the cost of flexibility\nand portability. We propose a systematic design strategy for modular real-time\nquantum control software and demonstrate that modular control software can\nreduce the execution time overhead of kernels by 63.3% on average while not\nincreasing the binary size. Our analysis shows that modular control software\nfor two distinctly different systems can share between 49.8% and 91.0% of\ncovered code statements. To demonstrate the modularity and portability of our\nsoftware architecture, we run a portable randomized benchmarking experiment on\ntwo different ion-trap quantum systems.",
    "descriptor": "\nComments: 11 pages, 10 figures\n",
    "authors": [
      "Leon Riesebos",
      "Brad Bondurant",
      "Jacob Whitlow",
      "Junki Kim",
      "Mark Kuzyk",
      "Tianyi Chen",
      "Samuel Phiri",
      "Ye Wang",
      "Chao Fang",
      "Andrew Van Horn",
      "Jungsang Kim",
      "Kenneth R. Brown"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14341"
  },
  {
    "id": "arXiv:2210.14347",
    "title": "Interpolating Discriminant Functions in High-Dimensional Gaussian Latent  Mixtures",
    "abstract": "This paper considers binary classification of high-dimensional features under\na postulated model with a low-dimensional latent Gaussian mixture structure and\nnon-vanishing noise. A generalized least squares estimator is used to estimate\nthe direction of the optimal separating hyperplane. The estimated hyperplane is\nshown to interpolate on the training data. While the direction vector can be\nconsistently estimated as could be expected from recent results in linear\nregression, a naive plug-in estimate fails to consistently estimate the\nintercept. A simple correction, that requires an independent hold-out sample,\nrenders the procedure minimax optimal in many scenarios. The interpolation\nproperty of the latter procedure can be retained, but surprisingly depends on\nthe way the labels are encoded.",
    "descriptor": "",
    "authors": [
      "Xin Bing",
      "Marten Wegkamp"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.14347"
  },
  {
    "id": "arXiv:2210.14351",
    "title": "Arc travel time and path choice model estimation subsumed",
    "abstract": "We propose a method for maximum likelihood estimation of path choice model\nparameters and arc travel time using data of different levels of granularity.\nHitherto these two tasks have been tackled separately under strong assumptions.\nUsing a small example, we illustrate that this can lead to biased results.\nResults on both real (New York yellow cab) and simulated data show strong\nperformance of our method compared to existing baselines.",
    "descriptor": "",
    "authors": [
      "Sobhan Mohammadpour",
      "Emma Frejinger"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14351"
  },
  {
    "id": "arXiv:2210.14355",
    "title": "Parameter-free Regret in High Probability with Heavy Tails",
    "abstract": "We present new algorithms for online convex optimization over unbounded\ndomains that obtain parameter-free regret in high-probability given access only\nto potentially heavy-tailed subgradient estimates. Previous work in unbounded\ndomains considers only in-expectation results for sub-exponential subgradients.\nUnlike in the bounded domain case, we cannot rely on straight-forward\nmartingale concentration due to exponentially large iterates produced by the\nalgorithm. We develop new regularization techniques to overcome these problems.\nOverall, with probability at most $\\delta$, for all comparators $\\mathbf{u}$\nour algorithm achieves regret $\\tilde{O}(\\| \\mathbf{u} \\| T^{1/\\mathfrak{p}}\n\\log (1/\\delta))$ for subgradients with bounded $\\mathfrak{p}^{th}$ moments for\nsome $\\mathfrak{p} \\in (1, 2]$.",
    "descriptor": "",
    "authors": [
      "Jiujia Zhang",
      "Ashok Cutkosky"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14355"
  },
  {
    "id": "arXiv:2210.14364",
    "title": "Functional Simulation of Real-Time Quantum Control Software",
    "abstract": "Modern quantum computers rely heavily on real-time control systems for\noperation. Software for these systems is becoming increasingly more complex due\nto the demand for more features and more real-time devices to control.\nUnfortunately, testing real-time control software is often a complex process,\nand existing simulation software is not usable or practical for software\ntesting. For this purpose, we implemented an interactive simulator that\nsimulates signals at the application programming interface level. We show that\nour simulation infrastructure simulates kernels 6.9 times faster on average\ncompared to execution on hardware, while the position of the timeline cursor is\nsimulated with an average accuracy of 97.9% when choosing the appropriate\nconfiguration.",
    "descriptor": "\nComments: 10 pages, 7 figures, IEEE Quantum Week 2022 best paper award\n",
    "authors": [
      "Leon Riesebos",
      "Kenneth R. Brown"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14364"
  },
  {
    "id": "arXiv:2210.14406",
    "title": "RedPen: Region- and Reason-Annotated Dataset of Unnatural Speech",
    "abstract": "Even with recent advances in speech synthesis models, the evaluation of such\nmodels is based purely on human judgement as a single naturalness score, such\nas the Mean Opinion Score (MOS). The score-based metric does not give any\nfurther information about which parts of speech are unnatural or why human\njudges believe they are unnatural. We present a novel speech dataset, RedPen,\nwith human annotations on unnatural speech regions and their corresponding\nreasons. RedPen consists of 180 synthesized speeches with unnatural regions\nannotated by crowd workers; These regions are then reasoned and categorized by\nerror types, such as voice trembling and background noise. We find that our\ndataset shows a better explanation for unnatural speech regions than the\nmodel-driven unnaturalness prediction. Our analysis also shows that each model\nincludes different types of error types. Summing up, our dataset successfully\nshows the possibility that various error regions and types lie under the single\nnaturalness score. We believe that our dataset will shed light on the\nevaluation and development of more interpretable speech models in the future.\nOur dataset will be publicly available upon acceptance.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Kyumin Park",
      "Keon Lee",
      "Daeyoung Kim",
      "Dongyeop Kang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14406"
  },
  {
    "id": "arXiv:2210.14416",
    "title": "RBP-DIP: High-Quality CT Reconstruction Using an Untrained Neural  Network with Residual Back Projection and Deep Image Prior",
    "abstract": "Neural network related methods, due to their unprecedented success in image\nprocessing, have emerged as a new set of tools in CT reconstruction with the\npotential to change the field. However, the lack of high-quality training data\nand theoretical guarantees, together with increasingly complicated network\nstructures, make its implementation impractical. In this paper, we present a\nnew framework (RBP-DIP) based on Deep Image Prior (DIP) and a special residual\nback projection (RBP) connection to tackle these challenges. Comparing to other\npre-trained neural network related algorithms, the proposed framework is closer\nto an iterative reconstruction (IR) algorithm as it requires no training data\nor training process. In that case, the proposed framework can be altered (e.g,\ndifferent hyperparameters and constraints) on demand, adapting to different\nconditions (e.g, different imaged objects, imaging instruments, and noise\nlevels) without retraining. Experiments show that the proposed framework has\nsignificant improvements over other state-of-the-art conventional methods, as\nwell as pre-trained and untrained models with similar network structures,\nespecially under sparse-view, limited-angle, and low-dose conditions.",
    "descriptor": "",
    "authors": [
      "Ziyu Shu",
      "Alireza Entezari"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14416"
  },
  {
    "id": "arXiv:2210.14420",
    "title": "Optimizing Pessimism in Dynamic Treatment Regimes: A Bayesian Learning  Approach",
    "abstract": "In this article, we propose a novel pessimism-based Bayesian learning method\nfor optimal dynamic treatment regimes in the offline setting. When the coverage\ncondition does not hold, which is common for offline data, the existing\nsolutions would produce sub-optimal policies. The pessimism principle addresses\nthis issue by discouraging recommendation of actions that are less explored\nconditioning on the state. However, nearly all pessimism-based methods rely on\na key hyper-parameter that quantifies the degree of pessimism, and the\nperformance of the methods can be highly sensitive to the choice of this\nparameter. We propose to integrate the pessimism principle with Thompson\nsampling and Bayesian machine learning for optimizing the degree of pessimism.\nWe derive a credible set whose boundary uniformly lower bounds the optimal\nQ-function, and thus does not require additional tuning of the degree of\npessimism. We develop a general Bayesian learning method that works with a\nrange of models, from Bayesian linear basis model to Bayesian neural network\nmodel. We develop the computational algorithm based on variational inference,\nwhich is highly efficient and scalable. We establish the theoretical guarantees\nof the proposed method, and show empirically that it outperforms the existing\nstate-of-the-art solutions through both simulations and a real data example.",
    "descriptor": "\nComments: 42 pages, 6 figures\n",
    "authors": [
      "Yunzhe Zhou",
      "Zhengling Qi",
      "Chengchun Shi",
      "Lexin Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14420"
  },
  {
    "id": "arXiv:2210.14459",
    "title": "Policy iteration: for want of recursive feasibility, all is not lost",
    "abstract": "This paper investigates recursive feasibility, recursive robust stability and\nnear-optimality properties of policy iteration (PI). For this purpose, we\nconsider deterministic nonlinear discrete-time systems whose inputs are\ngenerated by PI for undiscounted cost functions. We first assume that PI is\nrecursively feasible, in the sense that the optimization problems solved at\neach iteration admit a solution. In this case, we provide novel conditions to\nestablish recursive robust stability properties for a general attractor,\nmeaning that the policies generated at each iteration ensure a robust\n\\KL-stability property with respect to a general state measure. We then derive\nnovel explicit bounds on the mismatch between the (suboptimal) value function\nreturned by PI at each iteration and the optimal one. Afterwards, motivated by\na counter-example that shows that PI may fail to be recursively feasible, we\nmodify PI so that recursive feasibility is guaranteed a priori under mild\nconditions. This modified algorithm, called PI+, is shown to preserve the\nrecursive robust stability when the attractor is compact. Additionally, PI+\nenjoys the same near-optimality properties as its PI counterpart under the same\nassumptions. Therefore, PI+ is an attractive tool for generating near-optimal\nstabilizing control of deterministic discrete-time nonlinear systems.",
    "descriptor": "\nComments: Submitted for review\n",
    "authors": [
      "Mathieu Granzotto",
      "Olivier Lindamulage De Silva",
      "Romain Postoyan",
      "Dragan Nesic",
      "Zhong-Ping Jiang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14459"
  },
  {
    "id": "arXiv:2210.14476",
    "title": "Sinusoidal Frequency Estimation by Gradient Descent",
    "abstract": "Sinusoidal parameter estimation is a fundamental task in applications from\nspectral analysis to time-series forecasting. Estimating the sinusoidal\nfrequency parameter by gradient descent is, however, often impossible as the\nerror function is non-convex and densely populated with local minima. The\ngrowing family of differentiable signal processing methods has therefore been\nunable to tune the frequency of oscillatory components, preventing their use in\na broad range of applications. This work presents a technique for joint\nsinusoidal frequency and amplitude estimation using the Wirtinger derivatives\nof a complex exponential surrogate and any first order gradient-based\noptimizer, enabling end to-end training of neural network controllers for\nunconstrained sinusoidal models.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Ben Hayes",
      "Charalampos Saitis",
      "Gy\u00f6rgy Fazekas"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14476"
  },
  {
    "id": "arXiv:2210.14484",
    "title": "Imputation of missing values in multi-view data",
    "abstract": "When missing values occur in multi-view data, all features in a view are\nlikely to be missing simultaneously. This leads to very large quantities of\nmissing data which, especially when combined with high-dimensionality, makes\nthe application of conditional imputation methods computationally infeasible.\nWe introduce a new meta-learning imputation method based on stacked penalized\nlogistic regression (StaPLR), which performs imputation in a dimension-reduced\nspace. We evaluate the new imputation method with several imputation algorithms\nusing simulations. The results show that meta-level imputation of missing\nvalues leads to good results at a much lower computational cost, and makes the\nuse of advanced imputation algorithms such as missForest and predictive mean\nmatching possible in settings where they would otherwise be computationally\ninfeasible.",
    "descriptor": "\nComments: 26 pages, 11 figures\n",
    "authors": [
      "Wouter van Loon",
      "Marjolein Fokkema",
      "Mark de Rooij"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.14484"
  },
  {
    "id": "arXiv:2210.14488",
    "title": "History-Based, Bayesian, Closure for Stochastic Parameterization:  Application to Lorenz '96",
    "abstract": "Physical parameterizations are used as representations of unresolved subgrid\nprocesses within weather and global climate models or coarse-scale turbulent\nmodels, whose resolutions are too coarse to resolve small-scale processes.\nThese parameterizations are typically grounded on physically-based, yet\nempirical, representations of the underlying small-scale processes. Machine\nlearning-based parameterizations have recently been proposed as an alternative\nand have shown great promises to reduce uncertainties associated with\nsmall-scale processes. Yet, those approaches still show some important\nmismatches that are often attributed to stochasticity in the considered\nprocess. This stochasticity can be due to noisy data, unresolved variables or\nsimply to the inherent chaotic nature of the process. To address these issues,\nwe develop a new type of parameterization (closure) which is based on a\nBayesian formalism for neural networks, to account for uncertainty\nquantification, and includes memory, to account for the non-instantaneous\nresponse of the closure. To overcome the curse of dimensionality of Bayesian\ntechniques in high-dimensional spaces, the Bayesian strategy is based on a\nHamiltonian Monte Carlo Markov Chain sampling strategy that takes advantage of\nthe likelihood function and kinetic energy's gradients with respect to the\nparameters to accelerate the sampling process. We apply the proposed Bayesian\nhistory-based parameterization to the Lorenz '96 model in the presence of noisy\nand sparse data, similar to satellite observations, and show its capacity to\npredict skillful forecasts of the resolved variables while returning\ntrustworthy uncertainty quantifications for different sources of error. This\napproach paves the way for the use of Bayesian approaches for closure problems.",
    "descriptor": "",
    "authors": [
      "Mohamed Aziz Bhouri",
      "Pierre Gentine"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Chaotic Dynamics (nlin.CD)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14488"
  },
  {
    "id": "arXiv:2210.14501",
    "title": "Effect of different splitting criteria on the performance of speech  emotion recognition",
    "abstract": "Traditional speech emotion recognition (SER) evaluations have been performed\nmerely on a speaker-independent condition; some of them even did not evaluate\ntheir result on this condition. This paper highlights the importance of\nsplitting training and test data for SER by script, known as sentence-open or\ntext-independent criteria. The results show that employing sentence-open\ncriteria degraded the performance of SER. This finding implies the difficulties\nof recognizing emotion from speech in different linguistic information embedded\nin acoustic information. Surprisingly, text-independent criteria consistently\nperformed worse than speaker+text-independent criteria. The full order of\ndifficulties for splitting criteria on SER performances from the most difficult\nto the easiest is text-independent, speaker+text-independent,\nspeaker-independent, and speaker+text-dependent. The gap between\nspeaker+text-independent and text-independent was smaller than other criteria,\nstrengthening the difficulties of recognizing emotion from speech in different\nsentences.",
    "descriptor": "\nComments: Accepted at TENCON 2021\n",
    "authors": [
      "Bagus Tris Atmaja",
      "Akira Sasou"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14501"
  },
  {
    "id": "arXiv:2210.14510",
    "title": "Multi-Environment based Meta-Learning with CSI Fingerprints for Radio  Based Positioning",
    "abstract": "Radio based positioning of a user equipment (UE) based on deep learning (DL)\nmethods using channel state information (CSI) fingerprints have shown promising\nresults. DL models are able to capture complex properties embedded in the CSI\nabout a particular environment and map UE's CSI to the UE's position. However,\nthe CSI fingerprints and the DL models trained on such fingerprints are highly\ndependent on a particular propagation environment, which generally limits the\ntransfer of knowledge of the DL models from one environment to another. In this\npaper, we propose a DL model consisting of two parts: the first part aims to\nlearn environment independent features while the second part combines those\nfeatures depending on the particular environment. To improve transfer learning,\nwe propose a meta learning scheme for training the first part over multiple\nenvironments. We show that for positioning in a new environment, initializing a\nDL model with the meta learned environment independent function achieves higher\nUE positioning accuracy compared to regular transfer learning from one\nenvironment to the new environment, or compared to training the DL model from\nscratch with only fingerprints from the new environment. Our proposed scheme is\nable to create an environment independent function which can embed knowledge\nfrom multiple environments and more effectively learn from a new environment.",
    "descriptor": "\nComments: 6 pages, 9 figures, submitted for publication at IEEE WCNC2023\n",
    "authors": [
      "Anastasios Foliadis",
      "Mario H. Casta\u00f1eda Garcia",
      "Richard A. Stirling-Gallacher",
      "Reiner S. Thom\u00e4"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14510"
  },
  {
    "id": "arXiv:2210.14515",
    "title": "UFO2: A unified pre-training framework for online and offline speech  recognition",
    "abstract": "In this paper, we propose a Unified pre-training Framework for Online and\nOffline (UFO2) Automatic Speech Recognition (ASR), which 1) simplifies the two\nseparate training workflows for online and offline modes into one process, and\n2) improves the Word Error Rate (WER) performance with limited utterance\nannotating. Specifically, we extend the conventional offline-mode\nSelf-Supervised Learning (SSL)-based ASR approach to a unified manner, where\nthe model training is conditioned on both the full-context and dynamic-chunked\ninputs. To enhance the pre-trained representation model, stop-gradient\noperation is applied to decouple the online-mode objectives to the quantizer.\nMoreover, in both the pre-training and the downstream fine-tuning stages, joint\nlosses are proposed to train the unified model with full-weight sharing for the\ntwo modes. Experimental results on the LibriSpeech dataset show that UFO2\noutperforms the SSL-based baseline method by 29.7% and 18.2% relative WER\nreduction in offline and online modes, respectively.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Li Fu",
      "Siqi Li",
      "Qingtao Li",
      "Liping Deng",
      "Fangzhu Li",
      "Lu Fan",
      "Meng Chen",
      "Xiaodong He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14515"
  },
  {
    "id": "arXiv:2210.14522",
    "title": "Simulation-based Modelling of Growth and Pollination of Greenhouse  Strawberry",
    "abstract": "The cultivated strawberry Fragaria ananassa Duch. is widely planted in\ngreenhouses in China. Its production heavily depends on pollination services.\nCompared with artificial pollination, bee pollination can significantly improve\nfruit quality and save considerable labor requirement. Multiple factors such as\nbee foraging behavior, planting pattern and the spatial complexity of the\ngreenhouse environment interacting over time and space are major obstacles to\nunderstanding of bee pollination dynamics. We propose a spatially-explicit\nagent-based simulation model which allows users to explore how various factors\nincluding bee foraging behavior and strawberry phenology conditions as well as\nthe greenhouse environment influence pollination efficiency and fruit quality.\nSimulation experiments allowed us to compare pollination efficiencies in\ndifferent conditions. Especially, the cause of bee pollination advantage,\noptimal bee density and bee hive location were discussed based on sensitivity\nanalysis. In addition, simulation results provide some insights for strawberry\nplanting in a greenhouse. The firmly validated open-source model is a useful\ntool for hypothesis testing and theory development for strawberry pollination\nresearch.",
    "descriptor": "",
    "authors": [
      "Zhihao Cao",
      "Hongchun Qu"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2210.14522"
  },
  {
    "id": "arXiv:2210.14536",
    "title": "Position tracking of a varying number of sound sources with sliding  permutation invariant training",
    "abstract": "Recent data- and learning-based sound source localization (SSL) methods have\nshown strong performance in challenging acoustic scenarios. However, little\nwork has been done on adapting such methods to track consistently multiple\nsources appearing and disappearing, as would occur in reality. In this paper,\nwe present a new training strategy for deep learning SSL models with a\nstraightforward implementation based on the mean squared error of the optimal\nassociation between estimated and reference positions in the preceding time\nframes. It optimizes the desired properties of a tracking system: handling a\ntime-varying number of sources and ordering localization estimates according to\ntheir trajectories, minimizing identity switches (IDSs). Evaluation on\nsimulated data of multiple reverberant moving sources and on two model\narchitectures proves its effectiveness on reducing identity switches without\ncompromising frame-wise localization accuracy.",
    "descriptor": "\nComments: Submitted to the ICASSP 2023\n",
    "authors": [
      "David Diaz-Guerra",
      "Archontis Politis",
      "Tuomas Virtanen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14536"
  },
  {
    "id": "arXiv:2210.14537",
    "title": "Accelerated Weight Histogram Method for Rare Event Simulations",
    "abstract": "We describe an adaptive Markov chain Monte Carlo method suitable for the\nestimation of rare failure probabilities in complex probabilistic models. This\nmethod, the Accelerated Weight Histogram (AWH) method, has its origin in\nstatistical physics (Lidmar, 2012) and has successfully been applied to\nmolecular dynamics simulations in biophysics. Here we introduce it in the\ncontext of structural reliability and demonstrate its usefulness for\ncalculation of failure probabilities in some selected problems of varying\ndegrees of complexity and compare with other established techniques, e.g.,\nsubset simulations.",
    "descriptor": "\nComments: Proceedings of the 13th International Conference on Structural Safety and Reliability (ICOSSAR 2021-2022)\n",
    "authors": [
      "Jack Lidmar",
      "Johan Spross",
      "John Leander"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14537"
  },
  {
    "id": "arXiv:2210.14564",
    "title": "Deep Metric Learning with Adaptive Margin and Adaptive Scale for  Acoustic Word Discrimination",
    "abstract": "Many recent loss functions in deep metric learning are expressed with\nlogarithmic and exponential forms, and they involve margin and scale as\nessential hyper-parameters. Since each data class has an intrinsic\ncharacteristic, several previous works have tried to learn embedding space\nclose to the real distribution by introducing adaptive margins. However, there\nwas no work on adaptive scales at all. We argue that both margin and scale\nshould be adaptively adjustable during the training. In this paper, we propose\na method called Adaptive Margin and Scale (AdaMS), where hyper-parameters of\nmargin and scale are replaced with learnable parameters of adaptive margins and\nadaptive scales for each class. Our method is evaluated on Wall Street Journal\ndataset, and we achieve outperforming results for word discrimination tasks.",
    "descriptor": "\nComments: Submitted to ICASSP2023\n",
    "authors": [
      "Myunghun Jung",
      "Hoirin Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14564"
  },
  {
    "id": "arXiv:2210.14567",
    "title": "Reducing Language confusion for Code-switching Speech Recognition with  Token-level Language Diarization",
    "abstract": "Code-switching (CS) refers to the phenomenon that languages switch within a\nspeech signal and leads to language confusion for automatic speech recognition\n(ASR). This paper aims to address language confusion for improving CS-ASR from\ntwo perspectives: incorporating and disentangling language information. We\nincorporate language information in the CS-ASR model by dynamically biasing the\nmodel with token-level language posteriors which are outputs of a\nsequence-to-sequence auxiliary language diarization module. In contrast, the\ndisentangling process reduces the difference between languages via adversarial\ntraining so as to normalize two languages. We conduct the experiments on the\nSEAME dataset. Compared to the baseline model, both the joint optimization with\nLD and the language posterior bias achieve performance improvement. The\ncomparison of the proposed methods indicates that incorporating language\ninformation is more effective than disentangling for reducing language\nconfusion in CS speech.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Hexin Liu",
      "Haihua Xu",
      "Leibny Paola Garcia",
      "Andy W. H. Khong",
      "Yi He",
      "Sanjeev Khudanpur"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14567"
  },
  {
    "id": "arXiv:2210.14573",
    "title": "Learning Causal Graphs in Manufacturing Domains using Structural  Equation Models",
    "abstract": "Many production processes are characterized by numerous and complex\ncause-and-effect relationships. Since they are only partially known they pose a\nchallenge to effective process control. In this work we present how Structural\nEquation Models can be used for deriving cause-and-effect relationships from\nthe combination of prior knowledge and process data in the manufacturing\ndomain. Compared to existing applications, we do not assume linear\nrelationships leading to more informative results.",
    "descriptor": "\nComments: To be published in the Proceedings of IEEE AI4I 2022\n",
    "authors": [
      "Maximilian Kertel",
      "Stefan Harmeling",
      "Markus Pauly"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14573"
  },
  {
    "id": "arXiv:2210.14581",
    "title": "Deep Learning Based Audio-Visual Multi-Speaker DOA Estimation Using  Permutation-Free Loss Function",
    "abstract": "In this paper, we propose a deep learning based multi-speaker direction of\narrival (DOA) estimation with audio and visual signals by using\npermutation-free loss function. We first collect a data set for multi-modal\nsound source localization (SSL) where both audio and visual signals are\nrecorded in real-life home TV scenarios. Then we propose a novel spatial\nannotation method to produce the ground truth of DOA for each speaker with the\nvideo data by transformation between camera coordinate and pixel coordinate\naccording to the pin-hole camera model. With spatial location information\nserved as another input along with acoustic feature, multi-speaker DOA\nestimation could be solved as a classification task of active speaker\ndetection. Label permutation problem in multi-speaker related tasks will be\naddressed since the locations of each speaker are used as input. Experiments\nconducted on both simulated data and real data show that the proposed\naudio-visual DOA estimation model outperforms audio-only DOA estimation model\nby a large margin.",
    "descriptor": "\nComments: 5 pages, 3 figures, accepted by ISCSLP 2022\n",
    "authors": [
      "Qing Wang",
      "Hang Chen",
      "Ya Jiang",
      "Zhe Wang",
      "Yuyang Wang",
      "Jun Du",
      "Chin-Hui Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.14581"
  },
  {
    "id": "arXiv:2210.14586",
    "title": "Compressed Sensing MRI Reconstruction Regularized by VAEs with  Structured Image Covariance",
    "abstract": "Learned regularization for MRI reconstruction can provide complex data-driven\npriors to inverse problems while still retaining the control and insight of a\nvariational regularization method. Moreover, unsupervised learning, without\npaired training data, allows the learned regularizer to remain flexible to\nchanges in the forward problem such as noise level, sampling pattern or coil\nsensitivities. One such approach uses generative models, trained on\nground-truth images, as priors for inverse problems, penalizing reconstructions\nfar from images the generator can produce. In this work, we utilize variational\nautoencoders (VAEs) that generate not only an image but also a covariance\nuncertainty matrix for each image. The covariance can model changing\nuncertainty dependencies caused by structure in the image, such as edges or\nobjects, and provides a new distance metric from the manifold of learned\nimages. We demonstrate these novel generative regularizers on radially\nsub-sampled MRI knee measurements from the fastMRI dataset and compare them to\nother unlearned, unsupervised and supervised methods. Our results show that the\nproposed method is competitive with other state-of-the-art methods and behaves\nconsistently with changing sampling patterns and noise levels.",
    "descriptor": "",
    "authors": [
      "Margaret Duff",
      "Ivor J. A. Simpson",
      "Matthias J. Ehrhardt",
      "Neill D. F. Campbell"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14586"
  },
  {
    "id": "arXiv:2210.14597",
    "title": "A Stronger Baseline For Automatic Pfirrmann Grading Of Lumbar Spine MRI  Using Deep Learning",
    "abstract": "This paper addresses the challenge of grading visual features in lumbar spine\nMRI using Deep Learning. Such a method is essential for the automatic\nquantification of structural changes in the spine, which is valuable for\nunderstanding low back pain. Multiple recent studies investigated different\narchitecture designs, and the most recent success has been attributed to the\nuse of transformer architectures. In this work, we argue that with a well-tuned\nthree-stage pipeline comprising semantic segmentation, localization, and\nclassification, convolutional networks outperform the state-of-the-art\napproaches. We conducted an ablation study of the existing methods in a\npopulation cohort, and report performance generalization across various\nsubgroups. Our code is publicly available to advance research on disc\ndegeneration and low back pain.",
    "descriptor": "\nComments: 5 pages, under review\n",
    "authors": [
      "Narasimharao Kowlagi",
      "Huy Hoang Nguyen",
      "Terence McSweeney",
      "Simo Saarakkala",
      "Juhani m\u00e4\u00e4tt\u00e4",
      "Jaro Karppinen",
      "Aleksei Tiulpin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14597"
  },
  {
    "id": "arXiv:2210.14598",
    "title": "Exact Manifold Gaussian Variational Bayes",
    "abstract": "We propose an optimization algorithm for Variational Inference (VI) in\ncomplex models. Our approach relies on natural gradient updates where the\nvariational space is a Riemann manifold. We develop an efficient algorithm for\nGaussian Variational Inference that implicitly satisfies the positive definite\nconstraint on the variational covariance matrix. Our Exact manifold Gaussian\nVariational Bayes (EMGVB) provides exact but simple update rules and is\nstraightforward to implement. Due to its black-box nature, EMGVB stands as a\nready-to-use solution for VI in complex models. Over five datasets, we\nempirically validate our feasible approach on different statistical,\neconometric, and deep learning models, discussing its performance with respect\nto baseline methods.",
    "descriptor": "",
    "authors": [
      "Martin Magris",
      "Mostafa Shabani",
      "Alexandros Iosifidis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14598"
  },
  {
    "id": "arXiv:2210.14608",
    "title": "Inapproximability of shortest paths on perfect matching polytopes",
    "abstract": "We consider the computational problem of finding short paths in the skeleton\nof the perfect matching polytope of a bipartite graph. We prove that unless\n$P=NP$, there is no polynomial-time algorithm that computes a path of constant\nlength between two vertices at distance two of the perfect matching polytope of\na bipartite graph. Conditioned on $P\\neq NP$, this disproves a conjecture by\nIto, Kakimura, Kamiyama, Kobayashi and Okamoto [SIAM Journal on Discrete\nMathematics, 36(2), pp. 1102-1123 (2022)]. Assuming the Exponential Time\nHypothesis we prove the stronger result that there exists no polynomial-time\nalgorithm computing a path of length at most\n$\\left(\\frac{1}{4}-o(1)\\right)\\frac{\\log N}{\\log \\log N}$ between two vertices\nat distance two of the perfect matching polytope of an $N$-vertex bipartite\ngraph. These results remain true if the bipartite graph is restricted to be of\nmaximum degree three. The above has the following interesting implication for\nthe performance of pivot rules for the simplex algorithm on simply-structured\ncombinatorial polytopes: If $P\\neq NP$, then for every simplex pivot rule\nexecutable in polynomial time and every constant $k \\in \\mathbb{N}$ there\nexists a linear program on a perfect matching polytope and a starting vertex of\nthe polytope such that the optimal solution can be reached in two monotone\nsteps from the starting vertex, yet the pivot rule will require at least $k$\nsteps to reach the optimal solution. This result remains true in the more\ngeneral setting of pivot rules for so-called circuit-augmentation algorithms.",
    "descriptor": "\nComments: 15 pages, 5 figures\n",
    "authors": [
      "Jean Cardinal",
      "Raphael Steiner"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.14608"
  },
  {
    "id": "arXiv:2210.14610",
    "title": "Stochastic Control of Launch Vehicle Upper Stage with Minimum-Variance  Splash-Down",
    "abstract": "This paper presents a novel synthesis method for designing an optimal and\nrobust guidance law for a non-throttleable upper stage of a launch vehicle,\nusing a convex approach. In the unperturbed scenario, a combination of lossless\nand successive convexification techniques is employed to formulate the guidance\nproblem as a sequence of convex problems that yields the optimal trajectory, to\nbe used as a reference for the design of a feedback controller, with little\ncomputational effort. Then, based on the reference state and control, a\nstochastic optimal control problem is defined to find a closed-loop control law\nthat rejects random in-flight disturbance. The control is parameterized as a\nmultiplicative feedback law; thus, only the control direction is regulated,\nwhile the magnitude corresponds to the nominal one, enabling its use for solid\nrocket motors. The objective of the optimization is to minimize the splash-down\ndispersion to ensure that the spent stage falls as close as possible to the\nnominal point. Thanks to an original convexification strategy, the stochastic\noptimal control problem can be solved in polynomial time since it reduces to a\nsemidefinite programming problem. Numerical results assess the robustness of\nthe stochastic controller and compare its performance with a model predictive\ncontrol algorithm via extensive Monte Carlo campaigns.",
    "descriptor": "",
    "authors": [
      "Boris Benedikter",
      "Alessandro Zavoli",
      "Guido Colasurdo",
      "Simone Pizzurro",
      "Enrico Cavallini"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14610"
  },
  {
    "id": "arXiv:2210.14615",
    "title": "Exploring the Dynamics of Fungal Cellular Automata",
    "abstract": "Cells in a fungal hyphae are separated by internal walls (septa). The septa\nhave tiny pores that allow cytoplasm flowing between cells. Cells can close\ntheir septa blocking the flow if they are injured, preventing fluid loss from\nthe rest of filament. This action is achieved by special organelles called\nWoronin bodies. Using the controllable pores as an inspiration we advance one\nand two-dimensional cellular automata into Elementary fungal cellular automata\n(EFCA) and Majority fungal automata (MFA) by adding a concept of Woronin bodies\nto the cell state transition rules. EFCA is a cellular automaton where the\ncommunications between neighboring cells can be blocked by the activation of\nthe Woronin bodies (Wb), allowing or blocking the flow of information\n(represented by a cytoplasm and chemical elements it carries) between them. We\nexplore a novel version of the fungal automata where the evolution of the\nsystem is only affected by the activation of the Wb. We explore two case\nstudies: the Elementary Fungal Cellular Automata (EFCA), which is a direct\napplication of this variant for elementary cellular automata rules, and the\nMajority Fungal Automata (MFA), which correspond to an application of the Wb to\ntwo dimensional automaton with majority rule with Von Neumann neighborhood. By\nstudying the EFCA model, we analyze how the 256 elementary cellular automata\nrules are affected by the activation of Wb in different modes, increasing the\ncomplexity on applied rule in some cases. Also we explore how a consensus over\nMFA is affected when the continuous flow of information is interrupted due to\nthe activation of Woronin bodies.",
    "descriptor": "\nComments: 31 pages, 30 figures\n",
    "authors": [
      "Carlos S. Sep\u00falveda",
      "Eric Goles",
      "Mart\u00edn R\u00edos-Wilson",
      "Andrew Adamatzky"
    ],
    "subjectives": [
      "Cellular Automata and Lattice Gases (nlin.CG)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.14615"
  },
  {
    "id": "arXiv:2210.14629",
    "title": "Highly unbreakable graph with a fixed excluded minor are almost rigid",
    "abstract": "A set $X \\subseteq V(G)$ in a graph $G$ is $(q,k)$-unbreakable if every\nseparation $(A,B)$ of order at most $k$ in $G$ satisfies $|A \\cap X| \\leq q$ or\n$|B \\cap X| \\leq q$. In this paper, we prove the following result: If a graph\n$G$ excludes a fixed complete graph $K_h$ as a minor and satisfies certain\nunbreakability guarantees, then $G$ is almost rigid in the following sense: the\nvertices of $G$ can be partitioned in an isomorphism-invariant way into a part\ninducing a graph of bounded treewidth and a part that admits a small\nisomorphism-invariant family of labelings. This result is the key ingredient in\nthe fixed-parameter algorithm for Graph Isomorphism parameterized by the\nHadwiger number of the graph, which is presented in a companion paper.",
    "descriptor": "\nComments: Part II of a full version of a paper appearing at STOC 2022\n",
    "authors": [
      "Daniel Lokshtanov",
      "Marcin Pilipczuk",
      "Micha\u0142 Pilipczuk",
      "Saket Saurabh"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.14629"
  },
  {
    "id": "arXiv:2210.14633",
    "title": "Graph Filter Transfer via Probability Density Ratio Weighting",
    "abstract": "The problem of recovering graph signals is one of the main topics in graph\nsignal processing. A representative approach to this problem is the graph\nWiener filter, which utilizes the statistical information of the target signal\ncomputed from historical data to construct an effective estimator. However, we\noften encounter situations where the current graph differs from that of\nhistorical data due to topology changes, leading to performance degradation of\nthe estimator. This paper proposes a graph filter transfer method, which learns\nan effective estimator from historical data under topology changes. The\nproposed method leverages the probability density ratio of the current and\nhistorical observations and constructs an estimator that minimizes the\nreconstruction error in the current graph domain. The experiment on synthetic\ndata demonstrates that the proposed method outperforms other methods.",
    "descriptor": "\nComments: 5 pages, submitted to ICASSP 2023\n",
    "authors": [
      "Koki Yamada"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14633"
  },
  {
    "id": "arXiv:2210.14645",
    "title": "Super-Resolution Based Patch-Free 3D Medical Image Segmentation with  Self-Supervised Guidance",
    "abstract": "High resolution (HR) 3D medical image segmentation plays an important role in\nclinical diagnoses. However, HR images are difficult to be directly processed\nby mainstream graphical cards due to limited video memory. Therefore, most\nexisting 3D medical image segmentation methods use patch-based models, which\nignores global context information that is useful in accurate segmentation and\nhas low inference efficiency. To address these problems, we propose a\nsuper-resolution (SR) guided patch-free 3D medical image segmentation framework\nthat can realize HR segmentation with global information of low-resolution (LR)\ninput. The framework contains two tasks: semantic segmentation (main task) and\nsuper resolution (auxiliary task). To balance the information loss with the LR\ninput, we introduce a Self-Supervised Guidance Module (SGM), which employs a\nselective search method to crop a HR patch from the original image as\nrestoration guidance. Multi-scale convolutional layers are used to mitigate the\nscale-inconsistency between the HR guidance features and the LR features.\nMoreover, we propose a Task-Fusion Module (TFM) to exploit the inter\nconnections between segmentation and SR task. This module can also be used for\nTest Phase Fine-tuning (TPF), leading to a better model generalization ability.\nWhen predicting, only the main segmentation task is needed, while other modules\ncan be removed to accelerate the inference. The experiments results on two\ndifferent datasets show that our framework outperforms current patch-based and\npatch-free models. Our model also has a four times higher inference speed\ncompared to traditional patch-based methods. Our codes are available at:\nhttps://github.com/Dootmaan/PFSeg-Full.",
    "descriptor": "",
    "authors": [
      "Hongyi Wang",
      "Lanfen Lin",
      "Hongjie Hu",
      "Qingqing Chen",
      "Yinhao Li",
      "Yutaro Iwamoto",
      "Xian-Hua Han",
      "Yen-Wei Chen",
      "Ruofeng Tong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14645"
  },
  {
    "id": "arXiv:2210.14648",
    "title": "Masked Modeling Duo: Learning Representations by Encouraging Both  Networks to Model the Input",
    "abstract": "Masked Autoencoders is a simple yet powerful self-supervised learning method.\nHowever, it learns representations indirectly by reconstructing masked input\npatches. Several methods learn representations directly by predicting\nrepresentations of masked patches; however, we think using all patches to\nencode training signal representations is suboptimal. We propose a new method,\nMasked Modeling Duo (M2D), that learns representations directly while obtaining\ntraining signals using only masked patches. In the M2D, the online network\nencodes visible patches and predicts masked patch representations, and the\ntarget network, a momentum encoder, encodes masked patches. To better predict\ntarget representations, the online network should model the input well, while\nthe target network should also model it well to agree with online predictions.\nThen the learned representations should better model the input. We validated\nthe M2D by learning general-purpose audio representations, and M2D set new\nstate-of-the-art performance on tasks such as UrbanSound8K, VoxCeleb1,\nAudioSet20K, GTZAN, and SpeechCommandsV2.",
    "descriptor": "\nComments: 5 pages, 3 figures, and 5 tables. Under review\n",
    "authors": [
      "Daisuke Niizumi",
      "Daiki Takeuchi",
      "Yasunori Ohishi",
      "Noboru Harada",
      "Kunio Kashino"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14648"
  },
  {
    "id": "arXiv:2210.14666",
    "title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on  Generative Adversarial Network",
    "abstract": "XiaoiceSing is a singing voice synthesis (SVS) system that aims at generating\n48kHz singing voices. However, the mel-spectrogram generated by it is\nover-smoothing in middle- and high-frequency areas due to no special design for\nmodeling the details of these parts. In this paper, we propose XiaoiceSing2,\nwhich can generate the details of middle- and high-frequency parts to better\nconstruct the full-band mel-spectrogram. Specifically, in order to alleviate\nthis problem, XiaoiceSing2 adopts a generative adversarial network (GAN), which\nconsists of a FastSpeech-based generator and a multi-band discriminator. We\nimprove the feed-forward Transformer (FFT) block by adding multiple residual\nconvolutional blocks in parallel with the self-attention block to balance the\nlocal and global features. The multi-band discriminator contains three\nsub-discriminators responsible for low-, middle-, and high-frequency parts of\nthe mel-spectrogram, respectively. Each sub-discriminator is composed of\nseveral segment discriminators (SD) and detail discriminators (DD) to\ndistinguish the audio from different aspects. The experiment on our internal\n48kHz singing voice dataset shows XiaoiceSing2 significantly improves the\nquality of the singing voice over XiaoiceSing.",
    "descriptor": "\nComments: submitted to icassp2023\n",
    "authors": [
      "Chunhui Wang",
      "Chang Zeng",
      "Xing He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14666"
  },
  {
    "id": "arXiv:2210.14711",
    "title": "Weighted Pressure Matching Based on Kernel Interpolation For Sound Field  Reproduction",
    "abstract": "A sound field reproduction method called weighted pressure matching is\nproposed. Sound field reproduction is aimed at synthesizing the desired sound\nfield using multiple loudspeakers inside a target region. Optimization-based\nmethods are derived from the minimization of errors between synthesized and\ndesired sound fields, which enable the use of an arbitrary array geometry in\ncontrast with integral-equation-based methods. Pressure matching is widely used\nin the optimization-based sound field reproduction methods because of its\nsimplicity of implementation. Its cost function is defined as the synthesis\nerrors at multiple control points inside the target region; then, the driving\nsignals of the loudspeakers are obtained by solving a least-squares problem.\nHowever, in pressure matching, the region between the control points is not\ntaken into consideration. We define the cost function as the regional\nintegration of the synthesis error over the target region. On the basis of the\nkernel interpolation of the sound field, this cost function is represented as\nthe weighted square error of the synthesized pressures at the control points.\nExperimental results indicate that the proposed weighted pressure matching\noutperforms conventional pressure matching.",
    "descriptor": "\nComments: Presented at 24th International Congress on Acoustics (ICA) 2022\n",
    "authors": [
      "Shoichi Koyama",
      "Kazuyuki Arikawa"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14711"
  },
  {
    "id": "arXiv:2210.14735",
    "title": "Distribution-Free Finite-Sample Guarantees and Split Conformal  Prediction",
    "abstract": "Modern black-box predictive models are often accompanied by weak performance\nguarantees that only hold asymptotically in the size of the dataset or require\nstrong parametric assumptions. In response to this, split conformal prediction\nrepresents a promising avenue to obtain finite-sample guarantees under minimal\ndistribution-free assumptions. Although prediction set validity most often\nconcerns marginal coverage, we explore the related but different guarantee of\ntolerance regions, reformulating known results in the language of nested\nprediction sets and extending on the duality between marginal coverage and\ntolerance regions. Furthermore, we highlight the connection between split\nconformal prediction and classical tolerance predictors developed in the 1940s,\nas well as recent developments in distribution-free risk control. One result\nthat transfers from classical tolerance predictors is that the coverage of a\nprediction set based on order statistics, conditional on the calibration set,\nis a random variable stochastically dominating the Beta distribution. We\ndemonstrate the empirical effectiveness of our findings on synthetic and real\ndatasets using a popular split conformal prediction procedure called\nconformalized quantile regression (CQR).",
    "descriptor": "\nComments: 38 pages, 3 figures\n",
    "authors": [
      "Roel Hulsman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14735"
  },
  {
    "id": "arXiv:2210.14737",
    "title": "Impact of network topology changes on information source localization",
    "abstract": "Well-established methods of locating the source of information in a complex\nnetwork are usually derived with the assumption of complete and exact knowledge\nof network topology. We study the performance of three such algorithms (LPTVA,\nGMLA and Pearson correlation algorithm) in scenarios that do not fulfill this\nassumption by modifying the network prior to localization. This is done by\nadding superfluous new links, hiding existing ones, or reattaching links in\naccordance with the network's structural Hamiltonian. We find that GMLA is\nhighly resilient to the addition of superfluous edges, as its precision falls\nby more than statistical uncertainty only when the number of links is\napproximately doubled. On the other hand, if the edge set is underestimated or\nreattachment has taken place, the performance of GMLA drops significantly. In\nsuch a scenario the Pearson algorithm is preferable, retaining most of its\nperformance when other simulation parameters favor localization (high density\nof observers, highly deterministic propagation). It is also generally more\naccurate than LPTVA, as well as orders of magnitude faster. The aforementioned\ndifferences between localization algorithms can be intuitively explained,\nalthough a need for further theoretical research is noted.",
    "descriptor": "",
    "authors": [
      "Piotr Machura",
      "Robert Paluch"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.14737"
  },
  {
    "id": "arXiv:2210.14755",
    "title": "Multitask Detection of Speaker Changes, Overlapping Speech and Voice  Activity Using wav2vec 2.0",
    "abstract": "Self-supervised learning approaches have lately achieved great success on a\nbroad spectrum of machine learning problems. In the field of speech processing,\none of the most successful recent self-supervised models is wav2vec 2.0. In\nthis paper, we explore the effectiveness of this model on three basic speech\nclassification tasks: speaker change detection, overlapped speech detection,\nand voice activity detection. First, we concentrate on only one task -- speaker\nchange detection -- where our proposed system surpasses the previously reported\nresults on four different corpora, and achieves comparable performance even\nwhen trained on out-of-domain data from an artificially designed dataset. Then\nwe expand our approach to tackle all three tasks in a single multitask system\nwith state-of-the-art performance on the AMI corpus. The implementation of the\nalgorithms in this paper is publicly available at\nhttps://github.com/mkunes/w2v2_audioFrameClassification.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Marie Kune\u0161ov\u00e1",
      "Zbyn\u011bk Zaj\u00edc"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14755"
  },
  {
    "id": "arXiv:2210.14760",
    "title": "A New Task: Deriving Semantic Class Targets for the Physical Sciences",
    "abstract": "We define deriving semantic class targets as a novel multi-modal task. By\ndoing so, we aim to improve classification schemes in the physical sciences\nwhich can be severely abstracted and obfuscating. We address this task for\nupcoming radio astronomy surveys and present the derived semantic radio galaxy\nmorphology class targets.",
    "descriptor": "\nComments: 6 pages, 1 figure, Accepted at Fifth Workshop on Machine Learning and the Physical Sciences (NeurIPS 2022), Neural Information Processing Systems 2022\n",
    "authors": [
      "Micah Bowles",
      "Hongming Tang",
      "Eleni Vardoulaki",
      "Emma L. Alexander",
      "Yan Luo",
      "Lawrence Rudnick",
      "Mike Walmsley",
      "Fiona Porter",
      "Anna M. M. Scaife",
      "Inigo Val Slijepcevic",
      "Gary Segal"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14760"
  },
  {
    "id": "arXiv:2210.14775",
    "title": "Evaluation of Synthetically Generated CT for use in Transcranial Focused  Ultrasound Procedures",
    "abstract": "Transcranial focused ultrasound (tFUS) is a therapeutic ultrasound method\nthat focuses sound through the skull to a small region noninvasively and often\nunder MRI guidance. CT imaging is used to estimate the acoustic properties that\nvary between individual skulls to enable effective focusing during tFUS\nprocedures, exposing patients to potentially harmful radiation. A method to\nestimate acoustic parameters in the skull without the need for CT would be\ndesirable. Here, we synthesized CT images from routinely acquired T1-weighted\nMRI by using a 3D patch-based conditional generative adversarial network (cGAN)\nand evaluated the performance of synthesized CT (sCT) images for treatment\nplanning with tFUS. We compared the performance of sCT to real CT (rCT) images\nfor tFUS planning using Kranion and simulations using the acoustic toolbox,\nk-Wave. Simulations were performed for 3 tFUS scenarios: 1) no aberration\ncorrection, 2) correction with phases calculated from Kranion, and 3) phase\nshifts calculated from time-reversal. From Kranion, skull density ratio, skull\nthickness, and number of active elements between rCT and sCT had Pearson's\nCorrelation Coefficients of 0.94, 0.92, and 0.98, respectively. Among 20\ntargets, differences in simulated peak pressure between rCT and sCT were\nlargest without phase correction (12.4$\\pm$8.1%) and smallest with Kranion\nphases (7.3$\\pm$6.0%). The distance between peak focal locations between rCT\nand sCT was less than 1.3 mm for all simulation cases. Real and synthetically\ngenerated skulls had comparable image similarity, skull measurements, and\nacoustic simulation metrics. Our work demonstrates the feasibility of replacing\nreal CTs with the MR-synthesized CT for tFUS planning. Source code and a docker\nimage with the trained model are available at\nhttps://github.com/han-liu/SynCT_TcMRgFUS",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2202.10136\n",
    "authors": [
      "Han Liu",
      "Michelle K. Sigona",
      "Thomas J. Manuel",
      "Li Min Chen",
      "Benoit M. Dawant",
      "Charles F. Caskey"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14775"
  },
  {
    "id": "arXiv:2210.14794",
    "title": "The Contribution of Human Body Capacitance/Body-Area Electric Field To  Individual and Collaborative Activity Recognition",
    "abstract": "The current dominated wearable body motion sensor is IMU. This work presented\nan alternative wearable motion-sensing approach: human body capacitance (HBC,\nalso commonly defined as body-area electric field). While being less robust in\ntracking the posture and trajectory, HBC has two properties that make it an\nattractive. First, the deployment of the sensing node on the being tracked body\npart is not a requirement for HBC sensing approach. Second, HBC is sensitive to\nthe body's interaction with its surroundings, including both touching and being\nin the immediate proximity of people and objects. We first described the\nsensing principle for HBC, sensor architecture and implementation, and methods\nfor evaluation. We then presented two case studies demonstrating the usefulness\nof HBC as a complement/alternative to IMUs. First, we explored the exercise\nrecognition and repetition counting of seven machine-free leg-only exercises\nand eleven general gym workouts with the signal source of HBC and IMU. The HBC\nsensing shows significant advantages over the IMU signals in\nclassification(0.89 vs 0.78 in F-score) and counting(0.982 vs 0.938 in\naccuracy) of the leg-only exercises. For the general gym workouts, HBC only\nshows recognition improvement for certain workouts like adductor where legs\nalone complete the movement. And it also supplies better results over the IMU\nfor workouts counting(0.800 vs. 0.756 when wearing the sensors on the wrist).\nIn the second case, we tried to recognize actions related to manipulating\nobjects and physical collaboration between users by using a wrist-worn HBC\nsensing unit. We detected collaboration between the users with 0.69 F-score\nwhen receiving data from a single user and 0.78 when receiving data from both\nusers. The capacitive sensor can improve the recognition of collaborative\nactivities with an F-score over a single wrist accelerometer approach by 16\\%.",
    "descriptor": "\nComments: 30 Pages, 35 Figures\n",
    "authors": [
      "Sizhen Bian",
      "Vitor Fortes Rey",
      "Siyu Yuan",
      "Paul Lukowicz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.14794"
  },
  {
    "id": "arXiv:2210.14799",
    "title": "Segmentation of Bruch's Membrane in retinal OCT with AMD using  anatomical priors and uncertainty quantification",
    "abstract": "Bruch's membrane (BM) segmentation on optical coherence tomography (OCT) is a\npivotal step for the diagnosis and follow-up of age-related macular\ndegeneration (AMD), one of the leading causes of blindness in the developed\nworld. Automated BM segmentation methods exist, but they usually do not account\nfor the anatomical coherence of the results, neither provide feedback on the\nconfidence of the prediction. These factors limit the applicability of these\nsystems in real-world scenarios. With this in mind, we propose an end-to-end\ndeep learning method for automated BM segmentation in AMD patients. An\nAttention U-Net is trained to output a probability density function of the BM\nposition, while taking into account the natural curvature of the surface.\nBesides the surface position, the method also estimates an A-scan wise\nuncertainty measure of the segmentation output. Subsequently, the A-scans with\nhigh uncertainty are interpolated using thin plate splines (TPS). We tested our\nmethod with ablation studies on an internal dataset with 138 patients covering\nall three AMD stages, and achieved a mean absolute localization error of 4.10\num. In addition, the proposed segmentation method was compared against the\nstate-of-the-art methods and showed a superior performance on an external\npublicly available dataset from a different patient cohort and OCT device,\ndemonstrating strong generalization ability.",
    "descriptor": "",
    "authors": [
      "Botond Fazekas",
      "Dmitrii Lachinov",
      "Guilherme Aresta",
      "Julia Mai",
      "Ursula Schmidt-Erfurth",
      "Hrvoje Bogunovic"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14799"
  },
  {
    "id": "arXiv:2210.14800",
    "title": "Naturalistic Head Motion Generation from Speech",
    "abstract": "Synthesizing natural head motion to accompany speech for an embodied\nconversational agent is necessary for providing a rich interactive experience.\nMost prior works assess the quality of generated head motion by comparing them\nagainst a single ground-truth using an objective metric. Yet there are many\nplausible head motion sequences to accompany a speech utterance. In this work,\nwe study the variation in the perceptual quality of head motions sampled from a\ngenerative model. We show that, despite providing more diverse head motions,\nthe generative model produces motions with varying degrees of perceptual\nquality. We finally show that objective metrics commonly used in previous\nresearch do not accurately reflect the perceptual quality of generated head\nmotions. These results open an interesting avenue for future work to\ninvestigate better objective metrics that correlate with human perception of\nquality.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Trisha Mittal",
      "Zakaria Aldeneh",
      "Masha Fedzechkina",
      "Anurag Ranjan",
      "Barry-John Theobald"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Human-Computer Interaction (cs.HC)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14800"
  },
  {
    "id": "arXiv:2210.14804",
    "title": "Approximate Quantum Random Access Memory Architectures",
    "abstract": "Quantum supremacy in many applications using well-known quantum algorithms\nrely on availability of data in quantum format. Quantum Random Access Memory\n(QRAM), an equivalent of classical Random Access Memory (RAM), fulfills this\nrequirement. However, the existing QRAM proposals either require qutrit\ntechnology and/or incur access challenges. We propose an approximate Parametric\nQuantum Circuit (PQC) based QRAM which takes address lines as input and gives\nout the corresponding data in these address lines as the output. We present two\napplications of the proposed PQC-based QRAM namely, storage of binary data and\nstorage of machine learning (ML) dataset for classification.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Koustubh Phalak",
      "Junde Li",
      "Swaroop Ghosh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.14804"
  },
  {
    "id": "arXiv:2210.14843",
    "title": "TuneUp: A Training Strategy for Improving Generalization of Graph Neural  Networks",
    "abstract": "Despite many advances in Graph Neural Networks (GNNs), their training\nstrategies simply focus on minimizing a loss over nodes in a graph. However,\nsuch simplistic training strategies may be sub-optimal as they neglect that\ncertain nodes are much harder to make accurate predictions on than others. Here\nwe present TuneUp, a curriculum learning strategy for better training GNNs.\nCrucially, TuneUp trains a GNN in two stages. The first stage aims to produce a\nstrong base GNN. Such base GNNs tend to perform well on head nodes (nodes with\nlarge degrees) but less so on tail nodes (nodes with small degrees). So, the\nsecond stage of TuneUp specifically focuses on improving prediction on tail\nnodes. Concretely, TuneUp synthesizes many additional supervised tail node data\nby dropping edges from head nodes and reusing the supervision on the original\nhead nodes. TuneUp then minimizes the loss over the synthetic tail nodes to\nfinetune the base GNN. TuneUp is a general training strategy that can be used\nwith any GNN architecture and any loss, making TuneUp applicable to a wide\nrange of prediction tasks. Extensive evaluation of TuneUp on two GNN\narchitectures, three types of prediction tasks, and both inductive and\ntransductive settings shows that TuneUp significantly improves the performance\nof the base GNN on tail nodes, while often even improving the performance on\nhead nodes, which together leads up to 58.5% relative improvement in GNN\npredictive performance. Moreover, TuneUp significantly outperforms its variants\nwithout the two-stage curriculum learning, existing graph data augmentation\ntechniques, as well as other specialized methods for tail nodes.",
    "descriptor": "",
    "authors": [
      "Weihua Hu",
      "Kaidi Cao",
      "Kexin Huang",
      "Edward W Huang",
      "Karthik Subbian",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14843"
  },
  {
    "id": "arXiv:2210.14845",
    "title": "Synthetic Tumors Make AI Segment Tumors Better",
    "abstract": "We develop a novel strategy to generate synthetic tumors. Unlike existing\nworks, the tumors generated by our strategy have two intriguing advantages: (1)\nrealistic in shape and texture, which even medical professionals can confuse\nwith real tumors; (2) effective for AI model training, which can perform liver\ntumor segmentation similarly to a model trained on real tumors - this result is\nunprecedented because no existing work, using synthetic tumors only, has thus\nfar reached a similar or even close performance to the model trained on real\ntumors. This result also implies that manual efforts for developing per-voxel\nannotation of tumors (which took years to create) can be considerably reduced\nfor training AI models in the future. Moreover, our synthetic tumors have the\npotential to improve the success rate of small tumor detection by automatically\ngenerating enormous examples of small (or tiny) synthetic tumors.",
    "descriptor": "\nComments: NeurIPS Workshop on Medical Imaging Meets NeurIPS, 2022\n",
    "authors": [
      "Qixin Hu",
      "Junfei Xiao",
      "Yixiong Chen",
      "Shuwen Sun",
      "Jie-Neng Chen",
      "Alan Yuille",
      "Zongwei Zhou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14845"
  },
  {
    "id": "arXiv:2210.14876",
    "title": "Quantum deep recurrent reinforcement learning",
    "abstract": "Recent advances in quantum computing (QC) and machine learning (ML) have\ndrawn significant attention to the development of quantum machine learning\n(QML). Reinforcement learning (RL) is one of the ML paradigms which can be used\nto solve complex sequential decision making problems. Classical RL has been\nshown to be capable to solve various challenging tasks. However, RL algorithms\nin the quantum world are still in their infancy. One of the challenges yet to\nsolve is how to train quantum RL in the partially observable environments. In\nthis paper, we approach this challenge through building QRL agents with quantum\nrecurrent neural networks (QRNN). Specifically, we choose the quantum long\nshort-term memory (QLSTM) to be the core of the QRL agent and train the whole\nmodel with deep $Q$-learning. We demonstrate the results via numerical\nsimulations that the QLSTM-DRQN can solve standard benchmark such as Cart-Pole\nwith more stable and higher average scores than classical DRQN with similar\narchitecture and number of model parameters.",
    "descriptor": "",
    "authors": [
      "Samuel Yen-Chi Chen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)",
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.14876"
  },
  {
    "id": "arXiv:2210.14893",
    "title": "Superstabilizing Control of Discrete-Time ARX Models under Error in  Variables",
    "abstract": "This paper applies a polynomial optimization based framework towards the\nsuperstabilizing control of an Autoregressive with Exogenous Input (ARX) model\ngiven noisy data observations. The recorded input and output values are\ncorrupted with L-infinity bounded noise where the bounds are known. This is an\ninstance of Error in Variables (EIV) in which true internal state of the ARX\nsystem remains unknown. The consistency set of ARX models compatible with noisy\ndata has a bilinearity between unknown plant parameters and unknown noise\nterms. The requirement for a dynamic compensator to superstabilize all\nconsistent plants is expressed using polynomial nonnegativity constraints, and\nsolved using sum-of-squares (SOS) methods in a converging hierarchy of\nsemidefinite programs in increasing size. The computational complexity of this\nmethod may be reduced by applying a Theorem of Alternatives to eliminate the\nnoise terms. Effectiveness of this method is demonstrated on control of example\nARX models.",
    "descriptor": "\nComments: 13 pages, 0 figures, 5 tables\n",
    "authors": [
      "Jared Miller",
      "Tianyu Dai",
      "Mario Sznaier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.14893"
  },
  {
    "id": "arXiv:2210.14894",
    "title": "Learning to predict arbitrary quantum processes",
    "abstract": "We present an efficient machine learning (ML) algorithm for predicting any\nunknown quantum process $\\mathcal{E}$ over $n$ qubits. For a wide range of\ndistributions $\\mathcal{D}$ on arbitrary $n$-qubit states, we show that this ML\nalgorithm can learn to predict any local property of the output from the\nunknown process $\\mathcal{E}$, with a small average error over input states\ndrawn from $\\mathcal{D}$. The ML algorithm is computationally efficient even\nwhen the unknown process is a quantum circuit with exponentially many gates.\nOur algorithm combines efficient procedures for learning properties of an\nunknown state and for learning a low-degree approximation to an unknown\nobservable. The analysis hinges on proving new norm inequalities, including a\nquantum analogue of the classical Bohnenblust-Hille inequality, which we derive\nby giving an improved algorithm for optimizing local Hamiltonians. Overall, our\nresults highlight the potential for ML models to predict the output of complex\nquantum dynamics much faster than the time needed to run the process itself.",
    "descriptor": "\nComments: 10 pages + 37-page appendix\n",
    "authors": [
      "Hsin-Yuan Huang",
      "Sitan Chen",
      "John Preskill"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14894"
  },
  {
    "id": "arXiv:1706.00178",
    "title": "Network Capacity Bound for Personalized PageRank in Multimodal Networks",
    "abstract": "Comments: 21 pages. 2 tables, 30 bibliography positions",
    "descriptor": "\nComments: 21 pages. 2 tables, 30 bibliography positions\n",
    "authors": [
      "M.A. K\u0142opotek",
      "S.T. Wierzcho\u0144",
      "R.A. K\u0142opotek"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Information Retrieval (cs.IR)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/1706.00178"
  },
  {
    "id": "arXiv:1709.05876",
    "title": "Approximations for Generalized Unsplittable Flow on Paths with  Application to Power Systems Optimization",
    "abstract": "Approximations for Generalized Unsplittable Flow on Paths with  Application to Power Systems Optimization",
    "descriptor": "",
    "authors": [
      "Areg Karapetyan",
      "Khaled Elbassioni",
      "Majid Khonji",
      "Chi-Kin Chau"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1709.05876"
  },
  {
    "id": "arXiv:1909.03218",
    "title": "AFR: An Efficient Buffering Algorithm for Cloud Robotic Systems",
    "abstract": "Comments: Accepted by IROS 2022",
    "descriptor": "\nComments: Accepted by IROS 2022\n",
    "authors": [
      "Yu-Ping Wang",
      "Hao-Ning Wang",
      "Zi-Xin Zou",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/1909.03218"
  },
  {
    "id": "arXiv:1910.00413",
    "title": "A Note On k-Means Probabilistic Poverty",
    "abstract": "Comments: 14 pages",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Mieczys\u0142aw A. K\u0142opotek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1910.00413"
  },
  {
    "id": "arXiv:2007.15433",
    "title": "Novel Modelling and Control Strategies for a Steam Boiler under Fast  Load Dynamics",
    "abstract": "Comments: 25 pages, 7 figures. Submitted for publication",
    "descriptor": "\nComments: 25 pages, 7 figures. Submitted for publication\n",
    "authors": [
      "Diego S. Carrasco",
      "Graham C. Goodwin",
      "Robert D. Peirce"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2007.15433"
  },
  {
    "id": "arXiv:2009.03717",
    "title": "Hierarchical Message-Passing Graph Neural Networks",
    "abstract": "Hierarchical Message-Passing Graph Neural Networks",
    "descriptor": "",
    "authors": [
      "Zhiqiang Zhong",
      "Cheng-Te Li",
      "Jun Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.03717"
  },
  {
    "id": "arXiv:2009.08545",
    "title": "Asymptotic Performance Prediction for ADMM-Based Compressed Sensing",
    "abstract": "Comments: accepted to IEEE Transactions on Signal Processing",
    "descriptor": "\nComments: accepted to IEEE Transactions on Signal Processing\n",
    "authors": [
      "Ryo Hayakawa"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2009.08545"
  },
  {
    "id": "arXiv:2009.08716",
    "title": "Federated Learning with Nesterov Accelerated Gradient",
    "abstract": "Comments: publised in TPDS. 18 pages, 6 figures",
    "descriptor": "\nComments: publised in TPDS. 18 pages, 6 figures\n",
    "authors": [
      "Zhengjie Yang",
      "Wei Bao",
      "Dong Yuan",
      "Nguyen H. Tran",
      "Albert Y. Zomaya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2009.08716"
  },
  {
    "id": "arXiv:2009.09435",
    "title": "Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation",
    "abstract": "Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation",
    "descriptor": "",
    "authors": [
      "Francisco Vargas",
      "Ryan Cotterell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2009.09435"
  },
  {
    "id": "arXiv:2010.16295",
    "title": "Sharp threshold for alignment of graph databases with Gaussian weights",
    "abstract": "Comments: 18 pages, 2 figures. Latest version: typos corrected",
    "descriptor": "\nComments: 18 pages, 2 figures. Latest version: typos corrected\n",
    "authors": [
      "Luca Ganassali"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2010.16295"
  },
  {
    "id": "arXiv:2012.05359",
    "title": "Algorithmically-Consistent Deep Learning Frameworks for Structural  Topology Optimization",
    "abstract": "Comments: 29 pages, 28 figures, 9 tables",
    "descriptor": "\nComments: 29 pages, 28 figures, 9 tables\n",
    "authors": [
      "Jaydeep Rade",
      "Aditya Balu",
      "Ethan Herron",
      "Jay Pathak",
      "Rishikesh Ranade",
      "Soumik Sarkar",
      "Adarsh Krishnamurthy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.05359"
  },
  {
    "id": "arXiv:2101.06561",
    "title": "GENIE: A Leaderboard for Human-in-the-Loop Evaluation of Text Generation",
    "abstract": "Comments: Accepted to EMNLP 2022 main conference, visit our project page at: this https URL",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference, visit our project page at: this https URL\n",
    "authors": [
      "Daniel Khashabi",
      "Gabriel Stanovsky",
      "Jonathan Bragg",
      "Nicholas Lourie",
      "Jungo Kasai",
      "Yejin Choi",
      "Noah A. Smith",
      "Daniel S. Weld"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.06561"
  },
  {
    "id": "arXiv:2103.12158",
    "title": "Convergence of Finite Memory Q-Learning for POMDPs and Near Optimality  of Learned Policies under Filter Stability",
    "abstract": "Convergence of Finite Memory Q-Learning for POMDPs and Near Optimality  of Learned Policies under Filter Stability",
    "descriptor": "",
    "authors": [
      "Ali Devran Kara",
      "Serdar Yuksel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2103.12158"
  },
  {
    "id": "arXiv:2104.03425",
    "title": "Maximal and minimal dynamic Petri net slicing",
    "abstract": "Maximal and minimal dynamic Petri net slicing",
    "descriptor": "",
    "authors": [
      "Marisa Llorens",
      "Javier Oliver",
      "Josep Silva",
      "Salvador Tamarit"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2104.03425"
  },
  {
    "id": "arXiv:2104.13866",
    "title": "Reachability in Vector Addition Systems is Ackermann-complete",
    "abstract": "Reachability in Vector Addition Systems is Ackermann-complete",
    "descriptor": "",
    "authors": [
      "Wojciech Czerwi\u0144ski",
      "\u0141ukasz Orlikowski"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.13866"
  },
  {
    "id": "arXiv:2105.07736",
    "title": "A deterministic Kaczmarz algorithm for solving linear systems",
    "abstract": "Comments: 29 pages, more numerical experiments are added",
    "descriptor": "\nComments: 29 pages, more numerical experiments are added\n",
    "authors": [
      "Changpeng Shao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.07736"
  },
  {
    "id": "arXiv:2105.09611",
    "title": "Dependency Parsing with Bottom-up Hierarchical Pointer Networks",
    "abstract": "Comments: Final peer-reviewed manuscript accepted for publication in Information Fusion",
    "descriptor": "\nComments: Final peer-reviewed manuscript accepted for publication in Information Fusion\n",
    "authors": [
      "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
      "Carlos G\u00f3mez-Rodr\u00edguez"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.09611"
  },
  {
    "id": "arXiv:2106.07044",
    "title": "Optimal detection of the feature matching map in presence of noise and  outliers",
    "abstract": "Comments: accepted to ejs",
    "descriptor": "\nComments: accepted to ejs\n",
    "authors": [
      "Tigran Galstyan",
      "Arshak Minasyan",
      "Arnak Dalalyan"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07044"
  },
  {
    "id": "arXiv:2106.09798",
    "title": "Wide stochastic networks: Gaussian limit and PAC-Bayesian training",
    "abstract": "Comments: 20 pages, 2 figures",
    "descriptor": "\nComments: 20 pages, 2 figures\n",
    "authors": [
      "Eugenio Clerico",
      "George Deligiannidis",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.09798"
  },
  {
    "id": "arXiv:2106.11813",
    "title": "Enabling hyper-differential sensitivity analysis for ill-posed inverse  problems",
    "abstract": "Comments: 24 pages",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Joseph Hart",
      "Bart van Bloemen Waanders"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2106.11813"
  },
  {
    "id": "arXiv:2107.04386",
    "title": "Joint Matrix Decomposition for Deep Convolutional Neural Networks  Compression",
    "abstract": "Comments: Code is publicly available at: this https URL",
    "descriptor": "\nComments: Code is publicly available at: this https URL\n",
    "authors": [
      "Shaowu Chen",
      "Jiahao Zhou",
      "Weize Sun",
      "Lei Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.04386"
  },
  {
    "id": "arXiv:2107.12301",
    "title": "Enhanced Bilevel Optimization via Bregman Distance",
    "abstract": "Comments: Published in NeurIPS 2022",
    "descriptor": "\nComments: Published in NeurIPS 2022\n",
    "authors": [
      "Feihu Huang",
      "Junyi Li",
      "Shangqian Gao",
      "Heng Huang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.12301"
  },
  {
    "id": "arXiv:2108.11826",
    "title": "Fast and Flexible Human Pose Estimation with HyperPose",
    "abstract": "Comments: 4 pages, 1 figure. Published in ACM Multimedia",
    "descriptor": "\nComments: 4 pages, 1 figure. Published in ACM Multimedia\n",
    "authors": [
      "Yixiao Guo",
      "Jiawei Liu",
      "Guo Li",
      "Luo Mai",
      "Hao Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.11826"
  },
  {
    "id": "arXiv:2109.11415",
    "title": "A survey of Bayesian Network structure learning",
    "abstract": "A survey of Bayesian Network structure learning",
    "descriptor": "",
    "authors": [
      "Neville K. Kitson",
      "Anthony C. Constantinou",
      "Zhigao Guo",
      "Yang Liu",
      "Kiattikun Chobtham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.11415"
  },
  {
    "id": "arXiv:2110.00637",
    "title": "ML4C: Seeing Causality Through Latent Vicinity",
    "abstract": "Comments: causal discovery, supervised causal learning, vicinity, identifiability, learnability",
    "descriptor": "\nComments: causal discovery, supervised causal learning, vicinity, identifiability, learnability\n",
    "authors": [
      "Haoyue Dai",
      "Rui Ding",
      "Yuanyuan Jiang",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.00637"
  },
  {
    "id": "arXiv:2110.02776",
    "title": "SIRe-Networks: Convolutional Neural Networks Architectural Extension for  Information Preservation via Skip/Residual Connections and Interlaced  Auto-Encoders",
    "abstract": "SIRe-Networks: Convolutional Neural Networks Architectural Extension for  Information Preservation via Skip/Residual Connections and Interlaced  Auto-Encoders",
    "descriptor": "",
    "authors": [
      "Danilo Avola",
      "Luigi Cinque",
      "Alessio Fagioli",
      "Gian Luca Foresti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.02776"
  },
  {
    "id": "arXiv:2110.02800",
    "title": "Unital Qubit Queue-channels: Classical Capacity and Product Decoding",
    "abstract": "Comments: 25 pages with 3 Figures. Merges and supersedes our prior submissions arXiv:2107.13486 and arXiv:2110.02800",
    "descriptor": "\nComments: 25 pages with 3 Figures. Merges and supersedes our prior submissions arXiv:2107.13486 and arXiv:2110.02800\n",
    "authors": [
      "Vikesh Siddhu",
      "Avhishek Chatterjee",
      "Krishna Jagannathan",
      "Prabha Mandayam",
      "Sridhar Tayur"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2110.02800"
  },
  {
    "id": "arXiv:2110.05360",
    "title": "Towards Providing Connectivity When and Where It Counts: An Overview of  Deployable 5G Networks",
    "abstract": "Comments: 8 pages, 6 figures, accepted by IEEE Communications Standards Magazine",
    "descriptor": "\nComments: 8 pages, 6 figures, accepted by IEEE Communications Standards Magazine\n",
    "authors": [
      "Jingya Li",
      "Xingqin Lin",
      "Keerthi Kumar Nagalapur",
      "Zhiqiang Qi",
      "Adri\u00e1n Lahuerta-Lavieja",
      "Thomas Chapman",
      "Sam Agneessens",
      "Henrik Sahlin",
      "Daniel Guldbrand",
      "Joakim \u00c5kesson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2110.05360"
  },
  {
    "id": "arXiv:2111.08795",
    "title": "A Projection Operator-based Newton Method for the Trajectory  Optimization of Closed Quantum Systems",
    "abstract": "Comments: 10 pages",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Jieqiu Shao",
      "Joshua Combes",
      "John Hauser",
      "Marco M. Nicotra"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.08795"
  },
  {
    "id": "arXiv:2111.12295",
    "title": "Animal behavior classification via deep learning on embedded systems",
    "abstract": "Animal behavior classification via deep learning on embedded systems",
    "descriptor": "",
    "authors": [
      "Reza Arablouei",
      "Liang Wang",
      "Lachlan Currie",
      "Jordan Yates",
      "Flavio A. P. Alvarenga",
      "Greg J. Bishop-Hurley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.12295"
  },
  {
    "id": "arXiv:2112.00508",
    "title": "A symmetrized parametric finite element method for anisotropic surface  diffusion of closed curves",
    "abstract": "Comments: 25 pages, 8 figures",
    "descriptor": "\nComments: 25 pages, 8 figures\n",
    "authors": [
      "Weizhu Bao",
      "Wei Jiang",
      "Yifei Li"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2112.00508"
  },
  {
    "id": "arXiv:2112.01285",
    "title": "Adaptive non-intrusive reconstruction of solutions to high-dimensional  parametric PDEs",
    "abstract": "Adaptive non-intrusive reconstruction of solutions to high-dimensional  parametric PDEs",
    "descriptor": "",
    "authors": [
      "Martin Eigel",
      "Nando Farchmin",
      "Sebastian Heidenreich",
      "Philipp Trunschke"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Functional Analysis (math.FA)",
      "Spectral Theory (math.SP)"
    ],
    "url": "https://arxiv.org/abs/2112.01285"
  },
  {
    "id": "arXiv:2112.08321",
    "title": "Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics",
    "abstract": "Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics",
    "descriptor": "",
    "authors": [
      "Hyundong Cho",
      "Chinnadhurai Sankar",
      "Christopher Lin",
      "Kaushik Ram Sadagopan",
      "Shahin Shayandeh",
      "Asli Celikyilmaz",
      "Jonathan May",
      "Ahmad Beirami"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08321"
  },
  {
    "id": "arXiv:2112.10684",
    "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Mikel Artetxe",
      "Shruti Bhosale",
      "Naman Goyal",
      "Todor Mihaylov",
      "Myle Ott",
      "Sam Shleifer",
      "Xi Victoria Lin",
      "Jingfei Du",
      "Srinivasan Iyer",
      "Ramakanth Pasunuru",
      "Giri Anantharaman",
      "Xian Li",
      "Shuohui Chen",
      "Halil Akin",
      "Mandeep Baines",
      "Louis Martin",
      "Xing Zhou",
      "Punit Singh Koura",
      "Brian O'Horo",
      "Jeff Wang",
      "Luke Zettlemoyer",
      "Mona Diab",
      "Zornitsa Kozareva",
      "Ves Stoyanov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.10684"
  },
  {
    "id": "arXiv:2112.12533",
    "title": "PyCIL: A Python Toolbox for Class-Incremental Learning",
    "abstract": "Comments: Accepted to SCIENCE CHINA Information Sciences. Code is available at this https URL",
    "descriptor": "\nComments: Accepted to SCIENCE CHINA Information Sciences. Code is available at this https URL\n",
    "authors": [
      "Da-Wei Zhou",
      "Fu-Yun Wang",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.12533"
  },
  {
    "id": "arXiv:2112.12717",
    "title": "Forward Composition Propagation for Explainable Neural Reasoning",
    "abstract": "Forward Composition Propagation for Explainable Neural Reasoning",
    "descriptor": "",
    "authors": [
      "Isel Grau",
      "Gonzalo N\u00e1poles",
      "Marilyn Bello",
      "Yamisleydi Salgueiro",
      "Agnieszka Jastrzebska"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.12717"
  },
  {
    "id": "arXiv:2201.01340",
    "title": "State-dependent Importance Sampling for Estimating Expectations of  Functionals of Sums of Independent Random Variables",
    "abstract": "State-dependent Importance Sampling for Estimating Expectations of  Functionals of Sums of Independent Random Variables",
    "descriptor": "",
    "authors": [
      "Eya Ben Amar",
      "Nadhir Ben Rached",
      "Abdul-Lateef Haji-Ali",
      "Ra\u00fal Tempone"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2201.01340"
  },
  {
    "id": "arXiv:2201.01537",
    "title": "Few-shot Domain Adaptation for IMU Denoising",
    "abstract": "Few-shot Domain Adaptation for IMU Denoising",
    "descriptor": "",
    "authors": [
      "Feiyu Yao",
      "Zongkai Wu",
      "Zhenyu Wei",
      "Donglin Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.01537"
  },
  {
    "id": "arXiv:2201.01930",
    "title": "Codes from symmetric polynomials",
    "abstract": "Codes from symmetric polynomials",
    "descriptor": "",
    "authors": [
      "Mrinmoy Datta",
      "Trygve Johnsen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2201.01930"
  },
  {
    "id": "arXiv:2201.05787",
    "title": "One-Sided Matching with Permission",
    "abstract": "One-Sided Matching with Permission",
    "descriptor": "",
    "authors": [
      "Tianyi Yang",
      "Yuxiang Zhai",
      "Dengji Zhao",
      "Xinwei Song",
      "Miao Li"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2201.05787"
  },
  {
    "id": "arXiv:2201.09694",
    "title": "Scaling Up Knowledge Graph Creation to Large and Heterogeneous Data  Sources",
    "abstract": "Scaling Up Knowledge Graph Creation to Large and Heterogeneous Data  Sources",
    "descriptor": "",
    "authors": [
      "Enrique Iglesias",
      "Samaneh Jozashoori",
      "Maria-Esther Vidal"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.09694"
  },
  {
    "id": "arXiv:2201.10075",
    "title": "Splatting-based Synthesis for Video Frame Interpolation",
    "abstract": "Comments: WACV 2023, this http URL",
    "descriptor": "\nComments: WACV 2023, this http URL\n",
    "authors": [
      "Simon Niklaus",
      "Ping Hu",
      "Jiawen Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10075"
  },
  {
    "id": "arXiv:2201.10683",
    "title": "Promises and Challenges of Causality for Ethical Machine Learning",
    "abstract": "Promises and Challenges of Causality for Ethical Machine Learning",
    "descriptor": "",
    "authors": [
      "Aida Rahmattalabi",
      "Alice Xiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2201.10683"
  },
  {
    "id": "arXiv:2201.10866",
    "title": "CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search",
    "abstract": "Comments: Accepted to EMNLP 2022 (main conference)",
    "descriptor": "\nComments: Accepted to EMNLP 2022 (main conference)\n",
    "authors": [
      "Xiaonan Li",
      "Yeyun Gong",
      "Yelong Shen",
      "Xipeng Qiu",
      "Hang Zhang",
      "Bolun Yao",
      "Weizhen Qi",
      "Daxin Jiang",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2201.10866"
  },
  {
    "id": "arXiv:2202.02063",
    "title": "Color Image Inpainting via Robust Pure Quaternion Matrix Completion:  Error Bound and Weighted Loss",
    "abstract": "Color Image Inpainting via Robust Pure Quaternion Matrix Completion:  Error Bound and Weighted Loss",
    "descriptor": "",
    "authors": [
      "Junren Chen",
      "Michael K. Ng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2202.02063"
  },
  {
    "id": "arXiv:2202.03555",
    "title": "data2vec: A General Framework for Self-supervised Learning in Speech,  Vision and Language",
    "abstract": "data2vec: A General Framework for Self-supervised Learning in Speech,  Vision and Language",
    "descriptor": "",
    "authors": [
      "Alexei Baevski",
      "Wei-Ning Hsu",
      "Qiantong Xu",
      "Arun Babu",
      "Jiatao Gu",
      "Michael Auli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03555"
  },
  {
    "id": "arXiv:2202.04020",
    "title": "Local Linear Convergence of Gradient Methods for Subspace Optimization  via Strict Complementarity",
    "abstract": "Comments: In Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: In Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Dan Garber",
      "Ron Fisher"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04020"
  },
  {
    "id": "arXiv:2202.04947",
    "title": "OWL (Observe, Watch, Listen): Audiovisual Temporal Context for  Localizing Actions in Egocentric Videos",
    "abstract": "OWL (Observe, Watch, Listen): Audiovisual Temporal Context for  Localizing Actions in Egocentric Videos",
    "descriptor": "",
    "authors": [
      "Merey Ramazanova",
      "Victor Escorcia",
      "Fabian Caba Heilbron",
      "Chen Zhao",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2202.04947"
  },
  {
    "id": "arXiv:2202.06698",
    "title": "Digital Contact Tracing Solutions: Promises, Pitfalls and Challenges",
    "abstract": "Comments: A core part of this paper is to be published in IEEE Transactions on Emerging Topics in Computing, DOI: 10.1109/TETC.2022.3216473",
    "descriptor": "\nComments: A core part of this paper is to be published in IEEE Transactions on Emerging Topics in Computing, DOI: 10.1109/TETC.2022.3216473\n",
    "authors": [
      "Thien Duc Nguyen",
      "Markus Miettinen",
      "Alexandra Dmitrienko",
      "Ahmad-Reza Sadeghi",
      "Ivan Visconti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2202.06698"
  },
  {
    "id": "arXiv:2202.07654",
    "title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question  Answering Evaluation",
    "abstract": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question  Answering Evaluation",
    "descriptor": "",
    "authors": [
      "Jannis Bulian",
      "Christian Buck",
      "Wojciech Gajewski",
      "Benjamin Boerschinger",
      "Tal Schuster"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.07654"
  },
  {
    "id": "arXiv:2202.12459",
    "title": "APEACH: Attacking Pejorative Expressions with Analysis on  Crowd-Generated Hate Speech Evaluation Datasets",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Kichang Yang",
      "Wonjun Jang",
      "Won Ik Cho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.12459"
  },
  {
    "id": "arXiv:2202.13157",
    "title": "High Dimensional Statistical Estimation under Uniformly Dithered One-bit  Quantization",
    "abstract": "Comments: $\\|\\Theta^*\\|_1=O(1)$ in Theorem 9 has been relaxed to the more natural $\\|\\Theta^*\\|=O(1)$",
    "descriptor": "\nComments: $\\|\\Theta^*\\|_1=O(1)$ in Theorem 9 has been relaxed to the more natural $\\|\\Theta^*\\|=O(1)$\n",
    "authors": [
      "Junren Chen",
      "Cheng-Long Wang",
      "Michael K. Ng",
      "Di Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.13157"
  },
  {
    "id": "arXiv:2202.13490",
    "title": "Limitations of Deep Learning for Inverse Problems on Digital Hardware",
    "abstract": "Comments: Simplified proof",
    "descriptor": "\nComments: Simplified proof\n",
    "authors": [
      "Holger Boche",
      "Adalbert Fono",
      "Gitta Kutyniok"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.13490"
  },
  {
    "id": "arXiv:2203.06363",
    "title": "MDT-Net: Multi-domain Transfer by Perceptual Supervision for Unpaired  Images in OCT Scan",
    "abstract": "MDT-Net: Multi-domain Transfer by Perceptual Supervision for Unpaired  Images in OCT Scan",
    "descriptor": "",
    "authors": [
      "Weinan Song",
      "Gaurav Fotedar",
      "Nima Tajbakhsh",
      "Ziheng Zhou",
      "Lei He",
      "Xiaowei Ding"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.06363"
  },
  {
    "id": "arXiv:2203.07079",
    "title": "Strategy Complexity of Point Payoff, Mean Payoff and Total Payoff  Objectives in Countable MDPs",
    "abstract": "Comments: Revised and extended journal version of results presented at the CONCUR 2021 conference. For a special issue in the arxiv overlay journal LMCS (this https URL). This is not a duplicate of arXiv:2107.03287 (the conference version), but the significantly changed journal version for LMCS (which uses arXiv as a backend). Minor revisions in v2 to address LMCS referee comments",
    "descriptor": "\nComments: Revised and extended journal version of results presented at the CONCUR 2021 conference. For a special issue in the arxiv overlay journal LMCS (this https URL). This is not a duplicate of arXiv:2107.03287 (the conference version), but the significantly changed journal version for LMCS (which uses arXiv as a backend). Minor revisions in v2 to address LMCS referee comments\n",
    "authors": [
      "Richard Mayr",
      "Eric Munday"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2203.07079"
  },
  {
    "id": "arXiv:2203.07172",
    "title": "RED-ACE: Robust Error Detection for ASR using Confidence Embeddings",
    "abstract": "Comments: Accepted as a short paper in EMNLP 2022",
    "descriptor": "\nComments: Accepted as a short paper in EMNLP 2022\n",
    "authors": [
      "Zorik Gekhman",
      "Dina Zverinski",
      "Jonathan Mallinson",
      "Genady Beryozkin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.07172"
  },
  {
    "id": "arXiv:2203.08111",
    "title": "Does Corpus Quality Really Matter for Low-Resource Languages?",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Mikel Artetxe",
      "Itziar Aldabe",
      "Rodrigo Agerri",
      "Olatz Perez-de-Vi\u00f1aspre",
      "Aitor Soroa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.08111"
  },
  {
    "id": "arXiv:2203.08568",
    "title": "In-Context Learning for Few-Shot Dialogue State Tracking",
    "abstract": "Comments: To appear in Findings of EMNLP 2022",
    "descriptor": "\nComments: To appear in Findings of EMNLP 2022\n",
    "authors": [
      "Yushi Hu",
      "Chia-Hsuan Lee",
      "Tianbao Xie",
      "Tao Yu",
      "Noah A. Smith",
      "Mari Ostendorf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08568"
  },
  {
    "id": "arXiv:2203.09445",
    "title": "Image Super-Resolution With Deep Variational Autoencoders",
    "abstract": "Comments: ECCV 2022 Workshop on Advances in Image Manipulation",
    "descriptor": "\nComments: ECCV 2022 Workshop on Advances in Image Manipulation\n",
    "authors": [
      "Darius Chira",
      "Ilian Haralampiev",
      "Ole Winther",
      "Andrea Dittadi",
      "Valentin Li\u00e9vin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2203.09445"
  },
  {
    "id": "arXiv:2203.11933",
    "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models  with Adversarial Learning",
    "abstract": "Comments: 17 pages, 4 figures, 7 tables. For code and trained token embeddings, see this https URL; Changed to use ACL layout, added joint training with comparison figure, corrected spelling and formatting errors; This paper is accepted for publication at AACL 2022, the official version of record is in the ACL Anthology",
    "descriptor": "\nComments: 17 pages, 4 figures, 7 tables. For code and trained token embeddings, see this https URL; Changed to use ACL layout, added joint training with comparison figure, corrected spelling and formatting errors; This paper is accepted for publication at AACL 2022, the official version of record is in the ACL Anthology\n",
    "authors": [
      "Hugo Berg",
      "Siobhan Mackenzie Hall",
      "Yash Bhalgat",
      "Wonsuk Yang",
      "Hannah Rose Kirk",
      "Aleksandar Shtedritski",
      "Max Bain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.11933"
  },
  {
    "id": "arXiv:2203.12054",
    "title": "Self-supervision through Random Segments with Autoregressive Coding  (RandSAC)",
    "abstract": "Self-supervision through Random Segments with Autoregressive Coding  (RandSAC)",
    "descriptor": "",
    "authors": [
      "Tianyu Hua",
      "Yonglong Tian",
      "Sucheng Ren",
      "Michalis Raptis",
      "Hang Zhao",
      "Leonid Sigal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.12054"
  },
  {
    "id": "arXiv:2203.12591",
    "title": "Web Page Content Extraction Based on Multi-feature Fusion",
    "abstract": "Comments: Part of the content is for other purposes",
    "descriptor": "\nComments: Part of the content is for other purposes\n",
    "authors": [
      "Bowen Yu",
      "Junping Du",
      "Yingxia Shao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.12591"
  },
  {
    "id": "arXiv:2203.12948",
    "title": "Personalized incentives as feedback design in generalized Nash  equilibrium problems",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2111.03854",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2111.03854\n",
    "authors": [
      "Filippo Fabiani",
      "Andrea Simonetto",
      "Paul J. Goulart"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.12948"
  },
  {
    "id": "arXiv:2203.13068",
    "title": "SIFT and SURF based feature extraction for the anomaly detection",
    "abstract": "Comments: 28th Conference STUDENT EEICT 2022, Brno University of Technology",
    "descriptor": "\nComments: 28th Conference STUDENT EEICT 2022, Brno University of Technology\n",
    "authors": [
      "Simon Bilik",
      "Karel Horak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.13068"
  },
  {
    "id": "arXiv:2203.15952",
    "title": "4-bit Conformer with Native Quantization Aware Training for Speech  Recognition",
    "abstract": "Comments: Published at INTERSPEECH 2022",
    "descriptor": "\nComments: Published at INTERSPEECH 2022\n",
    "authors": [
      "Shaojin Ding",
      "Phoenix Meadowlark",
      "Yanzhang He",
      "Lukasz Lew",
      "Shivani Agrawal",
      "Oleg Rybakov"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.15952"
  },
  {
    "id": "arXiv:2203.17006",
    "title": "Quantum simulation of real-space dynamics",
    "abstract": "Quantum simulation of real-space dynamics",
    "descriptor": "",
    "authors": [
      "Andrew M. Childs",
      "Jiaqi Leng",
      "Tongyang Li",
      "Jin-Peng Liu",
      "Chenyi Zhang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2203.17006"
  },
  {
    "id": "arXiv:2204.00435",
    "title": "Sequent calculi of finite dimension",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1806.06537",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1806.06537\n",
    "authors": [
      "Antonio Bucciarelli",
      "Antonio Ledda",
      "Francesco Paoli",
      "Antonino Salibra"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2204.00435"
  },
  {
    "id": "arXiv:2204.03781",
    "title": "Color My World: Deterministic Tagging for Memory Safety",
    "abstract": "Color My World: Deterministic Tagging for Memory Safety",
    "descriptor": "",
    "authors": [
      "Hans Liljestrand",
      "Carlos Chinea",
      "R\u00e9mi Denis-Courmont",
      "Jan-Erik Ekberg",
      "N. Asokan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.03781"
  },
  {
    "id": "arXiv:2204.04333",
    "title": "A Study of Using Cepstrogram for Countermeasure Against Replay Attacks",
    "abstract": "Comments: Submitted to SLT 2022",
    "descriptor": "\nComments: Submitted to SLT 2022\n",
    "authors": [
      "Shih-Kuang Lee",
      "Yu Tsao",
      "Hsin-Min Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Cryptography and Security (cs.CR)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.04333"
  },
  {
    "id": "arXiv:2204.04802",
    "title": "On the pragmatism of using binary classifiers over data intensive neural  network classifiers for detection of COVID-19 from voice",
    "abstract": "Comments: Submitted to ICASSP 2022",
    "descriptor": "\nComments: Submitted to ICASSP 2022\n",
    "authors": [
      "Ankit Shah",
      "Hira Dhamyal",
      "Yang Gao",
      "Daniel Arancibia",
      "Mario Arancibia",
      "Bhiksha Raj",
      "Rita Singh"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.04802"
  },
  {
    "id": "arXiv:2204.11424",
    "title": "It Takes Two Flints to Make a Fire: Multitask Learning of Neural  Relation and Explanation Classifiers",
    "abstract": "It Takes Two Flints to Make a Fire: Multitask Learning of Neural  Relation and Explanation Classifiers",
    "descriptor": "",
    "authors": [
      "Zheng Tang",
      "Mihai Surdeanu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.11424"
  },
  {
    "id": "arXiv:2204.12115",
    "title": "Fast Successive-Cancellation Decoding of Polar Codes with Sequence Nodes",
    "abstract": "Comments: In the previous version, we ignored the fact that using the proposed decoding algorithm, the performance will be better than the conventional SC decoder since for the considered special node, the performance achieved by the proposed decoding algorithm is near-ML, while the conventional SC is not. Therefore, this version is temporarily withdrawn, we will update this paper as soon as possible",
    "descriptor": "\nComments: In the previous version, we ignored the fact that using the proposed decoding algorithm, the performance will be better than the conventional SC decoder since for the considered special node, the performance achieved by the proposed decoding algorithm is near-ML, while the conventional SC is not. Therefore, this version is temporarily withdrawn, we will update this paper as soon as possible\n",
    "authors": [
      "Yang Lu",
      "Ming-Min Zhao",
      "Ming Lei",
      "Min-Jian Zhao"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2204.12115"
  },
  {
    "id": "arXiv:2204.12670",
    "title": "SVD Perspectives for Augmenting DeepONet Flexibility and  Interpretability",
    "abstract": "SVD Perspectives for Augmenting DeepONet Flexibility and  Interpretability",
    "descriptor": "",
    "authors": [
      "Simone Venturi",
      "Tiernan Casey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Computational Physics (physics.comp-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2204.12670"
  },
  {
    "id": "arXiv:2204.13622",
    "title": "Fast Cross-Correlation for TDoA Estimation on Small Aperture Microphone  Arrays",
    "abstract": "Comments: Submitted to IEEE ICASSP 2023",
    "descriptor": "\nComments: Submitted to IEEE ICASSP 2023\n",
    "authors": [
      "Fran\u00e7ois Grondin",
      "Marc-Antoine Maheux",
      "Jean-Samuel Lauzon",
      "Jonathan Vincent",
      "Fran\u00e7ois Michaud"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.13622"
  },
  {
    "id": "arXiv:2204.14256",
    "title": "Handling and Presenting Harmful Text in NLP Research",
    "abstract": "Comments: in Findings of EMNLP 2022",
    "descriptor": "\nComments: in Findings of EMNLP 2022\n",
    "authors": [
      "Hannah Rose Kirk",
      "Abeba Birhane",
      "Bertie Vidgen",
      "Leon Derczynski"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.14256"
  },
  {
    "id": "arXiv:2205.00287",
    "title": "Assessing Fatigue with Multimodal Wearable Sensors and Machine Learning",
    "abstract": "Comments: 7 pages, 8 figures, 5 tables",
    "descriptor": "\nComments: 7 pages, 8 figures, 5 tables\n",
    "authors": [
      "Ashish Jaiswal",
      "Mohammad Zaki Zadeh",
      "Aref Hebri",
      "Fillia Makedon"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2205.00287"
  },
  {
    "id": "arXiv:2205.01493",
    "title": "On the uncertainty principle of neural networks",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Jun-Jie Zhang",
      "Dong-Xiao Zhang",
      "Jian-Nan Chen",
      "Long-Gang Pang",
      "Deyu Meng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.01493"
  },
  {
    "id": "arXiv:2205.03720",
    "title": "Empowering parameter-efficient transfer learning by recognizing the  kernel structure in self-attention",
    "abstract": "Comments: Accepted in NAACL 2022. Code is available at this https URL",
    "descriptor": "\nComments: Accepted in NAACL 2022. Code is available at this https URL\n",
    "authors": [
      "Yifan Chen",
      "Devamanyu Hazarika",
      "Mahdi Namazifar",
      "Yang Liu",
      "Di Jin",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.03720"
  },
  {
    "id": "arXiv:2205.04382",
    "title": "FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated  Objects",
    "abstract": "Comments: Accepted to Robotics Science and Systems (RSS) 2022, Best Paper Finalist",
    "descriptor": "\nComments: Accepted to Robotics Science and Systems (RSS) 2022, Best Paper Finalist\n",
    "authors": [
      "Ben Eisner",
      "Harry Zhang",
      "David Held"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.04382"
  },
  {
    "id": "arXiv:2205.06892",
    "title": "Lax completeness for gs-monoidal categories",
    "abstract": "Lax completeness for gs-monoidal categories",
    "descriptor": "",
    "authors": [
      "Tobias Fritz",
      "Fabio Gadducci",
      "Davide Trotta",
      "Andrea Corradini"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2205.06892"
  },
  {
    "id": "arXiv:2205.09726",
    "title": "RankGen: Improving Text Generation with Large Ranking Models",
    "abstract": "Comments: EMNLP 2022 camera ready (34 pages), model checkpoints available at this https URL",
    "descriptor": "\nComments: EMNLP 2022 camera ready (34 pages), model checkpoints available at this https URL\n",
    "authors": [
      "Kalpesh Krishna",
      "Yapei Chang",
      "John Wieting",
      "Mohit Iyyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09726"
  },
  {
    "id": "arXiv:2205.10914",
    "title": "Weisfeiler and Leman Go Walking: Random Walk Kernels Revisited",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Nils M. Kriege"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10914"
  },
  {
    "id": "arXiv:2205.11211",
    "title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Yichao Du",
      "Weizhi Wang",
      "Zhirui Zhang",
      "Boxing Chen",
      "Tong Xu",
      "Jun Xie",
      "Enhong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11211"
  },
  {
    "id": "arXiv:2205.11726",
    "title": "On the Role of Bidirectionality in Language Model Pre-Training",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Mikel Artetxe",
      "Jingfei Du",
      "Naman Goyal",
      "Luke Zettlemoyer",
      "Ves Stoyanov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11726"
  },
  {
    "id": "arXiv:2205.12209",
    "title": "EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start",
    "abstract": "Comments: To be published in Findings of EMNLP 2022",
    "descriptor": "\nComments: To be published in Findings of EMNLP 2022\n",
    "authors": [
      "Jonathan Mallinson",
      "Jakub Adamek",
      "Eric Malmi",
      "Aliaksei Severyn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12209"
  },
  {
    "id": "arXiv:2205.12486",
    "title": "Factorizing Content and Budget Decisions in Abstractive Summarization of  Long Documents",
    "abstract": "Comments: EMNLP 2022 camera ready",
    "descriptor": "\nComments: EMNLP 2022 camera ready\n",
    "authors": [
      "Marcio Fonseca",
      "Yftah Ziser",
      "Shay B. Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12486"
  },
  {
    "id": "arXiv:2205.12640",
    "title": "Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating  Spurious Correlations in Entity Typing",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Nan Xu",
      "Fei Wang",
      "Bangzheng Li",
      "Mingtao Dong",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12640"
  },
  {
    "id": "arXiv:2205.12673",
    "title": "InstructDial: Improving Zero and Few-shot Generalization in Dialogue  through Instruction Tuning",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Prakhar Gupta",
      "Cathy Jiao",
      "Yi-Ting Yeh",
      "Shikib Mehri",
      "Maxine Eskenazi",
      "Jeffrey P. Bigham"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12673"
  },
  {
    "id": "arXiv:2205.12697",
    "title": "PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation",
    "abstract": "Comments: EMNLP'22",
    "descriptor": "\nComments: EMNLP'22\n",
    "authors": [
      "Ao Liu",
      "Haoyu Dong",
      "Naoaki Okazaki",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12697"
  },
  {
    "id": "arXiv:2205.14568",
    "title": "Calibrated Predictive Distributions via Diagnostics for Conditional  Coverage",
    "abstract": "Comments: 9 pages, 7 figures. Under review",
    "descriptor": "\nComments: 9 pages, 7 figures. Under review\n",
    "authors": [
      "Biprateep Dey",
      "David Zhao",
      "Jeffrey A. Newman",
      "Brett H. Andrews",
      "Rafael Izbicki",
      "Ann B. Lee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2205.14568"
  },
  {
    "id": "arXiv:2206.00129",
    "title": "Fairness Transferability Subject to Bounded Distribution Shift",
    "abstract": "Fairness Transferability Subject to Bounded Distribution Shift",
    "descriptor": "",
    "authors": [
      "Yatong Chen",
      "Reilly Raab",
      "Jialu Wang",
      "Yang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.00129"
  },
  {
    "id": "arXiv:2206.02535",
    "title": "Certified Robustness in Federated Learning",
    "abstract": "Comments: Accepted at Workshop on Federated Learning: Recent Advances and New Challenges, NeurIPS 2022",
    "descriptor": "\nComments: Accepted at Workshop on Federated Learning: Recent Advances and New Challenges, NeurIPS 2022\n",
    "authors": [
      "Motasem Alfarra",
      "Juan C. P\u00e9rez",
      "Egor Shulgin",
      "Peter Richt\u00e1rik",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02535"
  },
  {
    "id": "arXiv:2206.04405",
    "title": "Conformal Off-Policy Prediction in Contextual Bandits",
    "abstract": "Comments: Proceedings of 36th Conference on Neural Information Processing System (NeurIPS 2022)",
    "descriptor": "\nComments: Proceedings of 36th Conference on Neural Information Processing System (NeurIPS 2022)\n",
    "authors": [
      "Muhammad Faaiz Taufiq",
      "Jean-Francois Ton",
      "Rob Cornish",
      "Yee Whye Teh",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04405"
  },
  {
    "id": "arXiv:2206.04803",
    "title": "Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application  of Machine Learning-based Forensics",
    "abstract": "Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application  of Machine Learning-based Forensics",
    "descriptor": "",
    "authors": [
      "Nadia Pocher",
      "Mirko Zichichi",
      "Fabio Merizzi",
      "Muhammad Zohaib Shafiq",
      "Stefano Ferretti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04803"
  },
  {
    "id": "arXiv:2206.07682",
    "title": "Emergent Abilities of Large Language Models",
    "abstract": "Comments: Transactions on Machine Learning Research (TMLR), 2022",
    "descriptor": "\nComments: Transactions on Machine Learning Research (TMLR), 2022\n",
    "authors": [
      "Jason Wei",
      "Yi Tay",
      "Rishi Bommasani",
      "Colin Raffel",
      "Barret Zoph",
      "Sebastian Borgeaud",
      "Dani Yogatama",
      "Maarten Bosma",
      "Denny Zhou",
      "Donald Metzler",
      "Ed H. Chi",
      "Tatsunori Hashimoto",
      "Oriol Vinyals",
      "Percy Liang",
      "Jeff Dean",
      "William Fedus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.07682"
  },
  {
    "id": "arXiv:2206.08756",
    "title": "Tensor-on-Tensor Regression: Riemannian Optimization,  Over-parameterization, Statistical-computational Gap, and Their Interplay",
    "abstract": "Tensor-on-Tensor Regression: Riemannian Optimization,  Over-parameterization, Statistical-computational Gap, and Their Interplay",
    "descriptor": "",
    "authors": [
      "Yuetian Luo",
      "Anru R. Zhang"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08756"
  },
  {
    "id": "arXiv:2206.08868",
    "title": "A Conditional Gradient-based Method for Simple Bilevel Optimization with  Convex Lower-level Problem",
    "abstract": "Comments: 28 pages, 5 figures; add one new experiment",
    "descriptor": "\nComments: 28 pages, 5 figures; add one new experiment\n",
    "authors": [
      "Ruichen Jiang",
      "Nazanin Abolfazli",
      "Aryan Mokhtari",
      "Erfan Yazdandoost Hamedani"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08868"
  },
  {
    "id": "arXiv:2206.09370",
    "title": "Frank-Wolfe-based Algorithms for Approximating Tyler's M-estimator",
    "abstract": "Comments: In Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: In Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Lior Danon",
      "Dan Garber"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.09370"
  },
  {
    "id": "arXiv:2206.10562",
    "title": "Semantics-Depth-Symbiosis: Deeply Coupled Semi-Supervised Learning of  Semantics and Depth",
    "abstract": "Semantics-Depth-Symbiosis: Deeply Coupled Semi-Supervised Learning of  Semantics and Depth",
    "descriptor": "",
    "authors": [
      "Nitin Bansal",
      "Pan Ji",
      "Junsong Yuan",
      "Yi Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.10562"
  },
  {
    "id": "arXiv:2206.11083",
    "title": "Investigating the Benefits of Free-Form Rationales",
    "abstract": "Comments: EMNLP 2022, Findings",
    "descriptor": "\nComments: EMNLP 2022, Findings\n",
    "authors": [
      "Jiao Sun",
      "Swabha Swayamdipta",
      "Jonathan May",
      "Xuezhe Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.11083"
  },
  {
    "id": "arXiv:2206.11212",
    "title": "VisFIS: Visual Feature Importance Supervision with  Right-for-the-Right-Reason Objectives",
    "abstract": "Comments: NeurIPS 2022 (first two authors contributed equally)",
    "descriptor": "\nComments: NeurIPS 2022 (first two authors contributed equally)\n",
    "authors": [
      "Zhuofan Ying",
      "Peter Hase",
      "Mohit Bansal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.11212"
  },
  {
    "id": "arXiv:2206.11889",
    "title": "Provably Efficient Model-Free Constrained RL with Linear Function  Approximation",
    "abstract": "Comments: Accepted at the 36th Neural Information Processing Systems (NeurIPS'22)",
    "descriptor": "\nComments: Accepted at the 36th Neural Information Processing Systems (NeurIPS'22)\n",
    "authors": [
      "Arnob Ghosh",
      "Xingyu Zhou",
      "Ness Shroff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.11889"
  },
  {
    "id": "arXiv:2206.11973",
    "title": "Liquidity Risks in Lending Protocols (LPs): Evidence from Aave Protocol",
    "abstract": "Liquidity Risks in Lending Protocols (LPs): Evidence from Aave Protocol",
    "descriptor": "",
    "authors": [
      "Xiaotong Sun",
      "Charalampos Stasinakis",
      "Georgios Sermpinis"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Cryptography and Security (cs.CR)",
      "Computational Finance (q-fin.CP)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2206.11973"
  },
  {
    "id": "arXiv:2206.12078",
    "title": "Multimodal sensor data fusion for in-situ classification of animal  behavior using accelerometry and GNSS data",
    "abstract": "Multimodal sensor data fusion for in-situ classification of animal  behavior using accelerometry and GNSS data",
    "descriptor": "",
    "authors": [
      "Reza Arablouei",
      "Ziwei Wang",
      "Greg J. Bishop-Hurley",
      "Jiajun Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.12078"
  },
  {
    "id": "arXiv:2206.12100",
    "title": "zPROBE: Zero Peek Robustness Checks for Federated Learning",
    "abstract": "zPROBE: Zero Peek Robustness Checks for Federated Learning",
    "descriptor": "",
    "authors": [
      "Zahra Ghodsi",
      "Mojan Javaheripi",
      "Nojan Sheybani",
      "Xinqiao Zhang",
      "Ke Huang",
      "Farinaz Koushanfar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.12100"
  },
  {
    "id": "arXiv:2206.14254",
    "title": "No imputation without representation",
    "abstract": "No imputation without representation",
    "descriptor": "",
    "authors": [
      "Oliver Urs Lenz",
      "Daniel Peralta",
      "Chris Cornelis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.14254"
  },
  {
    "id": "arXiv:2207.00160",
    "title": "When Does Differentially Private Learning Not Suffer in High Dimensions?",
    "abstract": "Comments: 26 pages; v3 includes additional experiments and clarification",
    "descriptor": "\nComments: 26 pages; v3 includes additional experiments and clarification\n",
    "authors": [
      "Xuechen Li",
      "Daogao Liu",
      "Tatsunori Hashimoto",
      "Huseyin A. Inan",
      "Janardhan Kulkarni",
      "Yin Tat Lee",
      "Abhradeep Guha Thakurta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.00160"
  },
  {
    "id": "arXiv:2207.01282",
    "title": "Connected Reconfiguration of Polyominoes Amid Obstacles using RRT*",
    "abstract": "Comments: Nine pages, nine figures. (Updated) full version of an extended abstract that is published in the proceedings of the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)",
    "descriptor": "\nComments: Nine pages, nine figures. (Updated) full version of an extended abstract that is published in the proceedings of the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Javier Garcia",
      "Michael Yannuzzi",
      "Peter Kramer",
      "Christian Rieck",
      "Aaron T. Becker"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2207.01282"
  },
  {
    "id": "arXiv:2207.03105",
    "title": "Uncertainty-Aware Self-supervised Neural Network for Liver $T_{1\u03c1}$  Mapping with Relaxation Constraint",
    "abstract": "Comments: Provisionally accepted by Physics in Medicine and Biology",
    "descriptor": "\nComments: Provisionally accepted by Physics in Medicine and Biology\n",
    "authors": [
      "Chaoxing Huang",
      "Yurui Qian",
      "Simon Chun Ho Yu",
      "Jian Hou",
      "Baiyan Jiang",
      "Queenie Chan",
      "Vincent Wai-Sun Wong",
      "Winnie Chiu-Wing Chu",
      "Weitian Chen"
    ],
    "subjectives": [
      "Tissues and Organs (q-bio.TO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2207.03105"
  },
  {
    "id": "arXiv:2207.06921",
    "title": "Automatic Sleep Scoring from Large-scale Multi-channel Pediatric EEG",
    "abstract": "Comments: Learning from Time Series for Health. Workshop at NeurIPS 2022",
    "descriptor": "\nComments: Learning from Time Series for Health. Workshop at NeurIPS 2022\n",
    "authors": [
      "Harlin Lee",
      "Aaqib Saeed"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.06921"
  },
  {
    "id": "arXiv:2207.08445",
    "title": "Automatic universal taxonomies for multi-domain semantic segmentation",
    "abstract": "Comments: BMVC 2022, 8 pages, 5 figures, 3 tables",
    "descriptor": "\nComments: BMVC 2022, 8 pages, 5 figures, 3 tables\n",
    "authors": [
      "Petra Bevandi\u0107",
      "Sini\u0161a \u0160egvi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.08445"
  },
  {
    "id": "arXiv:2207.08547",
    "title": "Few-shot Fine-grained Image Classification via Multi-Frequency  Neighborhood and Double-cross Modulation",
    "abstract": "Comments: 13 pages, 11 figures",
    "descriptor": "\nComments: 13 pages, 11 figures\n",
    "authors": [
      "Hegui Zhu",
      "Zhan Gao",
      "Jiayi Wang",
      "Yange Zhou",
      "Chengqing Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.08547"
  },
  {
    "id": "arXiv:2207.09684",
    "title": "On the Versatile Uses of Partial Distance Correlation in Deep Learning",
    "abstract": "Comments: This paper has been selected as best paper award for ECCV 2022!",
    "descriptor": "\nComments: This paper has been selected as best paper award for ECCV 2022!\n",
    "authors": [
      "Xingjian Zhen",
      "Zihang Meng",
      "Rudrasis Chakraborty",
      "Vikas Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.09684"
  },
  {
    "id": "arXiv:2207.12497",
    "title": "Estimating and Controlling for Fairness via Sensitive Attribute  Predictors",
    "abstract": "Estimating and Controlling for Fairness via Sensitive Attribute  Predictors",
    "descriptor": "",
    "authors": [
      "Beepul Bharti",
      "Paul Yi",
      "Jeremias Sulam"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2207.12497"
  },
  {
    "id": "arXiv:2208.04813",
    "title": "The explosive value of the networks",
    "abstract": "The explosive value of the networks",
    "descriptor": "",
    "authors": [
      "Antonio Scala",
      "Marco Delmastro"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2208.04813"
  },
  {
    "id": "arXiv:2208.05516",
    "title": "Quality Not Quantity: On the Interaction between Dataset Design and  Robustness of CLIP",
    "abstract": "Comments: Updated plots",
    "descriptor": "\nComments: Updated plots\n",
    "authors": [
      "Thao Nguyen",
      "Gabriel Ilharco",
      "Mitchell Wortsman",
      "Sewoong Oh",
      "Ludwig Schmidt"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.05516"
  },
  {
    "id": "arXiv:2208.11776",
    "title": "Comprehensive Dataset of Face Manipulations for Development and  Evaluation of Forensic Tools",
    "abstract": "Comments: Includes distribution statement",
    "descriptor": "\nComments: Includes distribution statement\n",
    "authors": [
      "Brian DeCann",
      "Kirill Trapeznikov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2208.11776"
  },
  {
    "id": "arXiv:2208.11926",
    "title": "Dynamic collaborative filtering Thompson Sampling for cross-domain  advertisements recommendation",
    "abstract": "Comments: Published at ADKDD 2022",
    "descriptor": "\nComments: Published at ADKDD 2022\n",
    "authors": [
      "Shion Ishikawa",
      "Young-joo Chung",
      "Yu Hirate"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.11926"
  },
  {
    "id": "arXiv:2208.12104",
    "title": "Algorithmic Differentiation for Automated Modeling of Machine Learned  Force Fields",
    "abstract": "Comments: 17 pages, 4 figures",
    "descriptor": "\nComments: 17 pages, 4 figures\n",
    "authors": [
      "Niklas Frederik Schmitz",
      "Klaus-Robert M\u00fcller",
      "Stefan Chmiela"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.12104"
  },
  {
    "id": "arXiv:2208.13592",
    "title": "Smooth Monotone Stochastic Variational Inequalities and Saddle Point  Problems -- Survey",
    "abstract": "Comments: 12 pages",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Aleksandr Beznosikov",
      "Boris Polyak",
      "Eduard Gorbunov",
      "Dmitry Kovalev",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.13592"
  },
  {
    "id": "arXiv:2208.14834",
    "title": "Deep Anomaly Detection and Search via Reinforcement Learning",
    "abstract": "Comments: We have changed the experiment in section 4 to achieve better performance",
    "descriptor": "\nComments: We have changed the experiment in section 4 to achieve better performance\n",
    "authors": [
      "Chao Chen",
      "Dawei Wang",
      "Feng Mao",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.14834"
  },
  {
    "id": "arXiv:2209.00110",
    "title": "Complexity of diameter on AT-free graphs is linear",
    "abstract": "Complexity of diameter on AT-free graphs is linear",
    "descriptor": "",
    "authors": [
      "Oleksiy Al-Saadi",
      "Jitender Deogun"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2209.00110"
  },
  {
    "id": "arXiv:2209.00585",
    "title": "Adversarial Stain Transfer to Study the Effect of Color Variation on  Cell Instance Segmentation",
    "abstract": "Adversarial Stain Transfer to Study the Effect of Color Variation on  Cell Instance Segmentation",
    "descriptor": "",
    "authors": [
      "Huaqian Wu",
      "Nicolas Souedet",
      "Camille Mabillon",
      "Caroline Jan",
      "C\u00e9dric Clouchoux",
      "Thierry Delzescaux"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.00585"
  },
  {
    "id": "arXiv:2209.02370",
    "title": "Continual Learning, Fast and Slow",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2110.00175",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2110.00175\n",
    "authors": [
      "Quang Pham",
      "Chenghao Liu",
      "Steven C. H. Hoi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.02370"
  },
  {
    "id": "arXiv:2209.02525",
    "title": "A PAC-Bayes bound for deterministic classifiers",
    "abstract": "A PAC-Bayes bound for deterministic classifiers",
    "descriptor": "",
    "authors": [
      "Eugenio Clerico",
      "George Deligiannidis",
      "Benjamin Guedj",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.02525"
  },
  {
    "id": "arXiv:2209.03834",
    "title": "Pre-Training a Graph Recurrent Network for Language Representation",
    "abstract": "Comments: NeurIPS Efficient Natural Language and Speech Processing (ENLSP) Workshop 2022",
    "descriptor": "\nComments: NeurIPS Efficient Natural Language and Speech Processing (ENLSP) Workshop 2022\n",
    "authors": [
      "Yile Wang",
      "Linyi Yang",
      "Zhiyang Teng",
      "Ming Zhou",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.03834"
  },
  {
    "id": "arXiv:2209.04093",
    "title": "Learning Audio-Visual embedding for Person Verification in the Wild",
    "abstract": "Learning Audio-Visual embedding for Person Verification in the Wild",
    "descriptor": "",
    "authors": [
      "Peiwen Sun",
      "Shanshan Zhang",
      "Zishan Liu",
      "Yougen Yuan",
      "Taotao Zhang",
      "Honggang Zhang",
      "Pengfei Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.04093"
  },
  {
    "id": "arXiv:2209.04198",
    "title": "I'm stuck! How to efficiently debug computational solid mechanics models  so you can enjoy the beauty of simulations",
    "abstract": "Comments: 13 pages, 2 figures, accepted manuscript to be published in VSI:Joy of Mechanics of the European Journal of Mechanics / A Solids. Replacement of 2209.04198. Main changes w.r.t. preprint version are addition of sec 3.1.3; footnote 3 in sec 3.1.4, sec 3.1.6, final pargraph in sec 4.1.1 and sec 4.1.3",
    "descriptor": "\nComments: 13 pages, 2 figures, accepted manuscript to be published in VSI:Joy of Mechanics of the European Journal of Mechanics / A Solids. Replacement of 2209.04198. Main changes w.r.t. preprint version are addition of sec 3.1.3; footnote 3 in sec 3.1.4, sec 3.1.6, final pargraph in sec 4.1.1 and sec 4.1.3\n",
    "authors": [
      "Ester Comellas",
      "Jean-Paul Pelteret",
      "Wolfgang Bangerth"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.04198"
  },
  {
    "id": "arXiv:2209.04892",
    "title": "\"Calibeating\": Beating Forecasters at Their Own Game",
    "abstract": "Comments: this http URL",
    "descriptor": "\nComments: this http URL\n",
    "authors": [
      "Dean P. Foster",
      "Sergiu Hart"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.04892"
  },
  {
    "id": "arXiv:2209.07474",
    "title": "On the Surprising Effectiveness of Transformers in Low-Labeled Video  Recognition",
    "abstract": "Comments: Accepted to NeurIPS 2022 Workshop on Vision Transformers: Theory and applications (VTTA)",
    "descriptor": "\nComments: Accepted to NeurIPS 2022 Workshop on Vision Transformers: Theory and applications (VTTA)\n",
    "authors": [
      "Farrukh Rahman",
      "\u00d6mer Mubarek",
      "Zsolt Kira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.07474"
  },
  {
    "id": "arXiv:2209.07569",
    "title": "FlexER: Flexible Entity Resolution for Multiple Intents",
    "abstract": "FlexER: Flexible Entity Resolution for Multiple Intents",
    "descriptor": "",
    "authors": [
      "Bar Genossar",
      "Roee Shraga",
      "Avigdor Gal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2209.07569"
  },
  {
    "id": "arXiv:2209.07699",
    "title": "Graph Contrastive Learning with Cross-view Reconstruction",
    "abstract": "Graph Contrastive Learning with Cross-view Reconstruction",
    "descriptor": "",
    "authors": [
      "Qianlong Wen",
      "Zhongyu Ouyang",
      "Chunhui Zhang",
      "Yiyue Qian",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.07699"
  },
  {
    "id": "arXiv:2209.08040",
    "title": "Exploring the Whole Rashomon Set of Sparse Decision Trees",
    "abstract": "Comments: NeurIPS 2022 (Oral)",
    "descriptor": "\nComments: NeurIPS 2022 (Oral)\n",
    "authors": [
      "Rui Xin",
      "Chudi Zhong",
      "Zhi Chen",
      "Takuya Takagi",
      "Margo Seltzer",
      "Cynthia Rudin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.08040"
  },
  {
    "id": "arXiv:2209.08842",
    "title": "Rewarding Episodic Visitation Discrepancy for Exploration in  Reinforcement Learning",
    "abstract": "Comments: 16 pages, 9 figures",
    "descriptor": "\nComments: 16 pages, 9 figures\n",
    "authors": [
      "Mingqi Yuan",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.08842"
  },
  {
    "id": "arXiv:2209.11879",
    "title": "Enhancing Data Diversity for Self-training Based Unsupervised  Cross-modality Vestibular Schwannoma and Cochlea Segmentation",
    "abstract": "Comments: Accepted by BrainLes MICCAI proceedings",
    "descriptor": "\nComments: Accepted by BrainLes MICCAI proceedings\n",
    "authors": [
      "Han Liu",
      "Yubo Fan",
      "Ipek Oguz",
      "Benoit M. Dawant"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.11879"
  },
  {
    "id": "arXiv:2209.11904",
    "title": "CryptoGCN: Fast and Scalable Homomorphically Encrypted Graph  Convolutional Network Inference",
    "abstract": "Comments: Accepted in Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Accepted in Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Ran Ran",
      "Nuo Xu",
      "Wei Wang",
      "Gang Quan",
      "Jieming Yin",
      "Wujie Wen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.11904"
  },
  {
    "id": "arXiv:2209.12034",
    "title": "Radio Environment Map and Deep Q-Learning for 5G Dynamic Point Blanking",
    "abstract": "Radio Environment Map and Deep Q-Learning for 5G Dynamic Point Blanking",
    "descriptor": "",
    "authors": [
      "Marcin Hoffmann",
      "Pawe\u0142 Kryszkiewicz"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.12034"
  },
  {
    "id": "arXiv:2209.12309",
    "title": "Feature Encodings for Gradient Boosting with Automunge",
    "abstract": "Comments: 10 pages, 4 figures, preprint",
    "descriptor": "\nComments: 10 pages, 4 figures, preprint\n",
    "authors": [
      "Nicholas J. Teague"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.12309"
  },
  {
    "id": "arXiv:2209.13347",
    "title": "Solving homogeneous linear equations over polynomial semirings",
    "abstract": "Comments: 21 pages including appendix, 1 figure",
    "descriptor": "\nComments: 21 pages including appendix, 1 figure\n",
    "authors": [
      "Ruiwen Dong"
    ],
    "subjectives": [
      "Rings and Algebras (math.RA)",
      "Symbolic Computation (cs.SC)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2209.13347"
  },
  {
    "id": "arXiv:2209.13667",
    "title": "Robust MADER: Decentralized and Asynchronous Multiagent Trajectory  Planner Robust to Communication Delay",
    "abstract": "Comments: 7 pages",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Kota Kondo",
      "Jesus Tordesillas",
      "Reinaldo Figueroa",
      "Juan Rached",
      "Joseph Merkel",
      "Parker C. Lusk",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.13667"
  },
  {
    "id": "arXiv:2209.14338",
    "title": "Who is GPT-3? An Exploration of Personality, Values and Demographics",
    "abstract": "Comments: EMNLP 2022 NLP+CSS workshop",
    "descriptor": "\nComments: EMNLP 2022 NLP+CSS workshop\n",
    "authors": [
      "Maril\u00f9 Miotto",
      "Nicola Rossberg",
      "Bennett Kleinberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.14338"
  },
  {
    "id": "arXiv:2210.00597",
    "title": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "abstract": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "descriptor": "",
    "authors": [
      "Thomas Steinke"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00597"
  },
  {
    "id": "arXiv:2210.00688",
    "title": "On the infinite-depth limit of finite-width neural networks",
    "abstract": "Comments: 71 pages, 21 figures",
    "descriptor": "\nComments: 71 pages, 21 figures\n",
    "authors": [
      "Soufiane Hayou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.00688"
  },
  {
    "id": "arXiv:2210.00943",
    "title": "Simple Pooling Front-ends For Efficient Audio Classification",
    "abstract": "Comments: Submitted to ICASSP 2023",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Xubo Liu",
      "Haohe Liu",
      "Qiuqiang Kong",
      "Xinhao Mei",
      "Mark D. Plumbley",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.00943"
  },
  {
    "id": "arXiv:2210.03515",
    "title": "Spiking neural networks for nonlinear regression",
    "abstract": "Spiking neural networks for nonlinear regression",
    "descriptor": "",
    "authors": [
      "Alexander Henkes",
      "Jason K. Eshraghian",
      "Henning Wessels"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03515"
  },
  {
    "id": "arXiv:2210.03521",
    "title": "STSyn: Speeding Up Local SGD with Straggler-Tolerant Synchronization",
    "abstract": "Comments: 12 pages, 10 figures, submitted for transaction publication",
    "descriptor": "\nComments: 12 pages, 10 figures, submitted for transaction publication\n",
    "authors": [
      "Feng Zhu",
      "Jingjing Zhang",
      "Xin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.03521"
  },
  {
    "id": "arXiv:2210.03591",
    "title": "Modeling Inter-Class and Intra-Class Constraints in Novel Class  Discovery",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Wenbin Li",
      "Zhichen Fan",
      "Jing Huo",
      "Yang Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.03591"
  },
  {
    "id": "arXiv:2210.04216",
    "title": "AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human  Pose Estimation",
    "abstract": "Comments: 7 pages, 4 figures",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Hongxin Lin",
      "Yunwei Chiu",
      "Peiyuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.04216"
  },
  {
    "id": "arXiv:2210.04610",
    "title": "Red-Teaming the Stable Diffusion Safety Filter",
    "abstract": "Comments: ML Safety Workshop NeurIPS 2022",
    "descriptor": "\nComments: ML Safety Workshop NeurIPS 2022\n",
    "authors": [
      "Javier Rando",
      "Daniel Paleka",
      "David Lindner",
      "Lennart Heim",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04610"
  },
  {
    "id": "arXiv:2210.05035",
    "title": "Not All Errors are Equal: Learning Text Generation Metrics using  Stratified Error Synthesis",
    "abstract": "Comments: EMNLP2022",
    "descriptor": "\nComments: EMNLP2022\n",
    "authors": [
      "Wenda Xu",
      "Yilin Tuan",
      "Yujie Lu",
      "Michael Saxon",
      "Lei Li",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05035"
  },
  {
    "id": "arXiv:2210.06742",
    "title": "H2RBox: Horizontal Box Annotation is All You Need for Oriented Object  Detection",
    "abstract": "Comments: 14 pages, 6 figures, 6 tables, the source code is available at this https URL",
    "descriptor": "\nComments: 14 pages, 6 figures, 6 tables, the source code is available at this https URL\n",
    "authors": [
      "Xue Yang",
      "Gefan Zhang",
      "Wentong Li",
      "Xuehui Wang",
      "Yue Zhou",
      "Junchi Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06742"
  },
  {
    "id": "arXiv:2210.06825",
    "title": "Fast Optimization of Weighted Sparse Decision Trees for use in Optimal  Treatment Regimes and Optimal Policy Design",
    "abstract": "Comments: Advances in Interpretable Machine Learning, AIMLAI 2022. arXiv admin note: text overlap with arXiv:2112.00798",
    "descriptor": "\nComments: Advances in Interpretable Machine Learning, AIMLAI 2022. arXiv admin note: text overlap with arXiv:2112.00798\n",
    "authors": [
      "Ali Behrouz",
      "Mathias Lecuyer",
      "Cynthia Rudin",
      "Margo Seltzer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06825"
  },
  {
    "id": "arXiv:2210.07185",
    "title": "On the Utility of Self-supervised Models for Prosody-related Tasks",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Guan-Ting Lin",
      "Chi-Luen Feng",
      "Wei-Ping Huang",
      "Yuan Tseng",
      "Tzu-Han Lin",
      "Chen-An Li",
      "Hung-yi Lee",
      "Nigel G. Ward"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07185"
  },
  {
    "id": "arXiv:2210.08339",
    "title": "Reachable Polyhedral Marching (RPM): An Exact Analysis Tool for  Deep-Learned Control Systems",
    "abstract": "Comments: Submitted to IEEE Transactions on Neural Networks and Learning Systems. arXiv admin note: text overlap with arXiv:2011.11609",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Neural Networks and Learning Systems. arXiv admin note: text overlap with arXiv:2011.11609\n",
    "authors": [
      "Joseph A. Vincent",
      "Mac Schwager"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.08339"
  },
  {
    "id": "arXiv:2210.08486",
    "title": "Streaming PAC-Bayes Gaussian process regression with a performance  guarantee for online decision making",
    "abstract": "Streaming PAC-Bayes Gaussian process regression with a performance  guarantee for online decision making",
    "descriptor": "",
    "authors": [
      "Tianyu Liu",
      "Jie Lu",
      "Zheng Yan",
      "Guangquan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.08486"
  },
  {
    "id": "arXiv:2210.08933",
    "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
    "abstract": "Comments: 18 pages, fix typos",
    "descriptor": "\nComments: 18 pages, fix typos\n",
    "authors": [
      "Shansan Gong",
      "Mukai Li",
      "Jiangtao Feng",
      "Zhiyong Wu",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.08933"
  },
  {
    "id": "arXiv:2210.09403",
    "title": "A Transfer Learning Based Approach for Classification of COVID-19 and  Pneumonia in CT Scan Imaging",
    "abstract": "Comments: 8 pages, 8 figures, under reviewing process",
    "descriptor": "\nComments: 8 pages, 8 figures, under reviewing process\n",
    "authors": [
      "Gargi Desai",
      "Nelly Elsayed",
      "Zag Elsayed",
      "Murat Ozer"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09403"
  },
  {
    "id": "arXiv:2210.09521",
    "title": "A Practical, Progressively-Expressive GNN",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Lingxiao Zhao",
      "Louis H\u00e4rtel",
      "Neil Shah",
      "Leman Akoglu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09521"
  },
  {
    "id": "arXiv:2210.09597",
    "title": "Soft-Labeled Contrastive Pre-training for Function-level Code  Representation",
    "abstract": "Comments: Accepted to EMNLP 2022 (findings)",
    "descriptor": "\nComments: Accepted to EMNLP 2022 (findings)\n",
    "authors": [
      "Xiaonan Li",
      "Daya Guo",
      "Yeyun Gong",
      "Yun Lin",
      "Yelong Shen",
      "Xipeng Qiu",
      "Daxin Jiang",
      "Weizhu Chen",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.09597"
  },
  {
    "id": "arXiv:2210.09634",
    "title": "DPIS: An Enhanced Mechanism for Differentially Private SGD with  Importance Sampling",
    "abstract": "Comments: A short version of this paper will appear in CCS 2022",
    "descriptor": "\nComments: A short version of this paper will appear in CCS 2022\n",
    "authors": [
      "Jianxin Wei",
      "Ergute Bao",
      "Xiaokui Xiao",
      "Yin Yang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09634"
  },
  {
    "id": "arXiv:2210.09709",
    "title": "Importance Weighting Correction of Regularized Least-Squares for  Covariate and Target Shifts",
    "abstract": "Comments: The paper can be significantly improved",
    "descriptor": "\nComments: The paper can be significantly improved\n",
    "authors": [
      "Davit Gogolashvili"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.09709"
  },
  {
    "id": "arXiv:2210.09900",
    "title": "SA-DNet: A on-demand semantic object registration network adapting to  non-rigid deformation",
    "abstract": "Comments: 15 pages, 12 figures",
    "descriptor": "\nComments: 15 pages, 12 figures\n",
    "authors": [
      "Housheng Xie",
      "Junhui Qiu",
      "Yuan Dai",
      "Yang Yang",
      "Changcheng Xiang",
      "Yukuan Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.09900"
  },
  {
    "id": "arXiv:2210.10414",
    "title": "High-Resolution Depth Estimation for 360-degree Panoramas through  Perspective and Panoramic Depth Images Registration",
    "abstract": "Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023, to appear",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023, to appear\n",
    "authors": [
      "Chi-Han Peng",
      "Jiayao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.10414"
  },
  {
    "id": "arXiv:2210.10824",
    "title": "Supervised Contrastive Learning with Tree-Structured Parzen Estimator  Bayesian Optimization for Imbalanced Tabular Data",
    "abstract": "Comments: 28 pages, 6 figures",
    "descriptor": "\nComments: 28 pages, 6 figures\n",
    "authors": [
      "Shuting Tao",
      "Peng Peng",
      "Qi Li",
      "Hongwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.10824"
  },
  {
    "id": "arXiv:2210.10951",
    "title": "Automatic Document Selection for Efficient Encoder Pretraining",
    "abstract": "Automatic Document Selection for Efficient Encoder Pretraining",
    "descriptor": "",
    "authors": [
      "Yukun Feng",
      "Patrick Xia",
      "Benjamin Van Durme",
      "Jo\u00e3o Sedoc"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.10951"
  },
  {
    "id": "arXiv:2210.10983",
    "title": "PSA-Det3D: Pillar Set Abstraction for 3D object Detection",
    "abstract": "PSA-Det3D: Pillar Set Abstraction for 3D object Detection",
    "descriptor": "",
    "authors": [
      "Zhicong Huang",
      "Jingwen Zhao",
      "Zhijie Zheng",
      "Dihu Chena",
      "Haifeng Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.10983"
  },
  {
    "id": "arXiv:2210.11109",
    "title": "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text  Generation",
    "abstract": "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text  Generation",
    "descriptor": "",
    "authors": [
      "Yu Zhao",
      "Jianguo Wei",
      "Zhichao Lin",
      "Yueheng Sun",
      "Meishan Zhang",
      "Min Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11109"
  },
  {
    "id": "arXiv:2210.11715",
    "title": "Empathetic Dialogue Generation via Sensitive Emotion Recognition and  Sensible Knowledge Selection",
    "abstract": "Empathetic Dialogue Generation via Sensitive Emotion Recognition and  Sensible Knowledge Selection",
    "descriptor": "",
    "authors": [
      "Lanrui Wang",
      "Jiangnan Li",
      "Zheng Lin",
      "Fandong Meng",
      "Chenxu Yang",
      "Weiping Wang",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11715"
  },
  {
    "id": "arXiv:2210.11723",
    "title": "Evidence of Vocal Tract Articulation in Self-Supervised Learning of  Speech",
    "abstract": "Evidence of Vocal Tract Articulation in Self-Supervised Learning of  Speech",
    "descriptor": "",
    "authors": [
      "Cheol Jun Cho",
      "Peter Wu",
      "Abdelrahman Mohamed",
      "Gopala K. Anumanchipalli"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.11723"
  },
  {
    "id": "arXiv:2210.11828",
    "title": "Towards Employing Recommender Systems for Supporting Data and Algorithm  Sharing",
    "abstract": "Comments: Accepted to the DataEconomy Workshop at CoNEXT'22",
    "descriptor": "\nComments: Accepted to the DataEconomy Workshop at CoNEXT'22\n",
    "authors": [
      "Peter M\u00fcllner",
      "Stefan Schmerda",
      "Dieter Theiler",
      "Stefanie Lindstaedt",
      "Dominik Kowald"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.11828"
  },
  {
    "id": "arXiv:2210.11997",
    "title": "Extending F1 metric, probabilistic approach",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Mikolaj Sitarz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.11997"
  },
  {
    "id": "arXiv:2210.12034",
    "title": "Proceedings of the Dialogue Robot Competition 2022",
    "abstract": "Comments: Proceedings of the Dialogue Robot Competition 2022",
    "descriptor": "\nComments: Proceedings of the Dialogue Robot Competition 2022\n",
    "authors": [
      "Ryuichiro Higashinaka",
      "Takashi Minato",
      "Hiromitsu Nishizaki",
      "Takayuki Nagai"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12034"
  },
  {
    "id": "arXiv:2210.12409",
    "title": "Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in  Transformer-Based Variational AutoEncoder for Diverse Text Generation",
    "abstract": "Comments: EMNLP 2022 Findings",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Jinyi Hu",
      "Xiaoyuan Yi",
      "Wenhao Li",
      "Maosong Sun",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12409"
  },
  {
    "id": "arXiv:2210.12467",
    "title": "ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long  Earnings Call Transcripts",
    "abstract": "Comments: 14 pages; Accepted as a Long Paper in EMNLP 2022 (Main Conference); Codes: this https URL",
    "descriptor": "\nComments: 14 pages; Accepted as a Long Paper in EMNLP 2022 (Main Conference); Codes: this https URL\n",
    "authors": [
      "Rajdeep Mukherjee",
      "Abhinav Bohra",
      "Akash Banerjee",
      "Soumya Sharma",
      "Manjunath Hegde",
      "Afreen Shaikh",
      "Shivani Shrivastava",
      "Koustuv Dasgupta",
      "Niloy Ganguly",
      "Saptarshi Ghosh",
      "Pawan Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12467"
  },
  {
    "id": "arXiv:2210.12606",
    "title": "Nash Equilibria and Pitfalls of Adversarial Training in Adversarial  Robustness Games",
    "abstract": "Comments: 22 pages",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Maria-Florina Balcan",
      "Rattana Pukdee",
      "Pradeep Ravikumar",
      "Hongyang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.12606"
  },
  {
    "id": "arXiv:2210.12635",
    "title": "Quantitative Evidence on Overlooked Aspects of Enrollment Speaker  Embeddings for Target Speaker Separation",
    "abstract": "Comments: Submitted version to ICASSP 2023",
    "descriptor": "\nComments: Submitted version to ICASSP 2023\n",
    "authors": [
      "Xiaoyu Liu",
      "Xu Li",
      "Joan Serr\u00e0"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12635"
  },
  {
    "id": "arXiv:2210.12692",
    "title": "Focus Is What You Need For Chinese Grammatical Error Correction",
    "abstract": "Comments: Submitted to ICASSP2023 (currently under review)",
    "descriptor": "\nComments: Submitted to ICASSP2023 (currently under review)\n",
    "authors": [
      "Jingheng Ye",
      "Yinghui Li",
      "Shirong Ma",
      "Rui Xie",
      "Wei Wu",
      "Hai-Tao Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12692"
  },
  {
    "id": "arXiv:2210.12740",
    "title": "HiFi-WaveGAN: Generative Adversarial Network with Auxiliary  Spectrogram-Phase Loss for High-Fidelity Singing Voice Generation",
    "abstract": "Comments: submitted to icassp2023",
    "descriptor": "\nComments: submitted to icassp2023\n",
    "authors": [
      "Chunhui Wang",
      "Chang Zeng",
      "Xing He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.12740"
  },
  {
    "id": "arXiv:2210.12746",
    "title": "Principal Component Classification",
    "abstract": "Comments: 5 pages; 5 figures; 1 table",
    "descriptor": "\nComments: 5 pages; 5 figures; 1 table\n",
    "authors": [
      "Rozenn Dahyot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12746"
  },
  {
    "id": "arXiv:2210.12849",
    "title": "Learning to Advise Humans By Leveraging Algorithm Discretion",
    "abstract": "Learning to Advise Humans By Leveraging Algorithm Discretion",
    "descriptor": "",
    "authors": [
      "Nicholas Wolczynski",
      "Maytal Saar-Tsechansky",
      "Tong Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12849"
  },
  {
    "id": "arXiv:2210.12852",
    "title": "1st Place Solution of The Robust Vision Challenge (RVC) 2022 Semantic  Segmentation Track",
    "abstract": "Comments: The Winning Solution to The Robust Vision Challenge 2022 Semantic Segmentation Track",
    "descriptor": "\nComments: The Winning Solution to The Robust Vision Challenge 2022 Semantic Segmentation Track\n",
    "authors": [
      "Junfei Xiao",
      "Zhichao Xu",
      "Shiyi Lan",
      "Zhiding Yu",
      "Alan Yuille",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12852"
  },
  {
    "id": "arXiv:2210.12903",
    "title": "Gallery Filter Network for Person Search",
    "abstract": "Comments: WACV 2023; Code: this https URL",
    "descriptor": "\nComments: WACV 2023; Code: this https URL\n",
    "authors": [
      "Lucas Jaffe",
      "Avideh Zakhor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12903"
  },
  {
    "id": "arXiv:2210.13012",
    "title": "CMU-Net: A Strong ConvMixer-based Medical Ultrasound Image Segmentation  Network",
    "abstract": "Comments: 5 pages, 13 figures, conference",
    "descriptor": "\nComments: 5 pages, 13 figures, conference\n",
    "authors": [
      "Fenghe Tang",
      "Lingtao Wang",
      "Chunping Ning",
      "Min Xian",
      "Jianrui Ding"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13012"
  },
  {
    "id": "arXiv:2210.13109",
    "title": "Domain Adaptive Segmentation of Electron Microscopy with Sparse Point  Annotations",
    "abstract": "Comments: Accepted by BIBM 2022: International Conference on Bioinformatics & Biomedicine",
    "descriptor": "\nComments: Accepted by BIBM 2022: International Conference on Bioinformatics & Biomedicine\n",
    "authors": [
      "Dafei Qiu",
      "Jiajin Yi",
      "Jialin Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13109"
  },
  {
    "id": "arXiv:2210.13209",
    "title": "Response to \"Comment on 'Origin of the Curie--von Schweidler law and the  fractional capacitor from time-varying capacitance [J. Pow. Sources 532  (2022) 231309]' \"",
    "abstract": "Comments: 8 pages",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Vikash Pandey"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13209"
  },
  {
    "id": "arXiv:2210.13325",
    "title": "ICSSIM-A Framework for Building Industrial Control Systems Security  Simulation Testbeds",
    "abstract": "Comments: 36 pages, 10 figures",
    "descriptor": "\nComments: 36 pages, 10 figures\n",
    "authors": [
      "Alireza Dehlaghi-Ghadim",
      "Ali Balador",
      "Hans Hansson",
      "Mauro Conti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13325"
  },
  {
    "id": "arXiv:2210.13373",
    "title": "Local Metric Learning for Off-Policy Evaluation in Contextual Bandits  with Continuous Actions",
    "abstract": "Local Metric Learning for Off-Policy Evaluation in Contextual Bandits  with Continuous Actions",
    "descriptor": "",
    "authors": [
      "Haanvid Lee",
      "Jongmin Lee",
      "Yunseon Choi",
      "Wonseok Jeon",
      "Byung-Jun Lee",
      "Yung-Kyun Noh",
      "Kee-Eung Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13373"
  },
  {
    "id": "arXiv:2210.13387",
    "title": "Towards a Higher-Order Mathematical Operational Semantics",
    "abstract": "Towards a Higher-Order Mathematical Operational Semantics",
    "descriptor": "",
    "authors": [
      "Sergey Goncharov",
      "Stefan Milius",
      "Lutz Schr\u00f6der",
      "Stelios Tsampas",
      "Henning Urbat"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2210.13387"
  },
  {
    "id": "arXiv:2210.13401",
    "title": "Entity-level Sentiment Analysis in Contact Center Telephone  Conversations",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Xue-Yong Fu",
      "Cheng Chen",
      "Md Tahmid Rahman Laskar",
      "Shayna Gardiner",
      "Pooja Hiranandani",
      "Shashi Bhushan TN"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13401"
  },
  {
    "id": "arXiv:2210.13617",
    "title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text",
    "abstract": "Comments: Our code, models, and data (e.g., integration corpus and extended datasets) are available: this https URL",
    "descriptor": "\nComments: Our code, models, and data (e.g., integration corpus and extended datasets) are available: this https URL\n",
    "authors": [
      "Yifan Hou",
      "Wenxiang Jiao",
      "Meizhen Liu",
      "Carl Allen",
      "Zhaopeng Tu",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13617"
  },
  {
    "id": "arXiv:2210.13756",
    "title": "Mixed Emotion Modelling for Emotional Voice Conversion",
    "abstract": "Comments: Submitted to ICASSP 2023. arXiv admin note: text overlap with arXiv:2208.05890",
    "descriptor": "\nComments: Submitted to ICASSP 2023. arXiv admin note: text overlap with arXiv:2208.05890\n",
    "authors": [
      "Kun Zhou",
      "Berrak Sisman",
      "Carlos Busso",
      "Haizhou Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13756"
  },
  {
    "id": "arXiv:2210.13769",
    "title": "GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion  Estimates",
    "abstract": "Comments: Accepted in WACV 2022",
    "descriptor": "\nComments: Accepted in WACV 2022\n",
    "authors": [
      "Jerin Geo James",
      "Devansh Jain",
      "Ajit Rajwade"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13769"
  },
  {
    "id": "arXiv:2210.13801",
    "title": "Deep Boosting Robustness of DNN-based Image Watermarking via DBMark",
    "abstract": "Deep Boosting Robustness of DNN-based Image Watermarking via DBMark",
    "descriptor": "",
    "authors": [
      "Guanhui Ye",
      "Jiashi Gao",
      "Wei Xie",
      "Bo Yin",
      "Xuetao Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13801"
  },
  {
    "id": "arXiv:2210.13838",
    "title": "Multilingual Relation Classification via Efficient and Effective  Prompting",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Yuxuan Chen",
      "David Harbecke",
      "Leonhard Hennig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13838"
  },
  {
    "id": "arXiv:2210.13918",
    "title": "Differentially Private Language Models for Secure Data Sharing",
    "abstract": "Comments: Accepted at EMNLP 2022",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Justus Mattern",
      "Zhijing Jin",
      "Benjamin Weggenmann",
      "Bernhard Schoelkopf",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13918"
  },
  {
    "id": "arXiv:2210.13958",
    "title": "Mitigating Health Data Poverty: Generative Approaches versus Resampling  for Time-series Clinical Data",
    "abstract": "Comments: Accepted at NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research (Neurips 2022 SyntheticData4ML)",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research (Neurips 2022 SyntheticData4ML)\n",
    "authors": [
      "Raffaele Marchesi",
      "Nicolo Micheletti",
      "Giuseppe Jurman",
      "Venet Osmani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13958"
  },
  {
    "id": "arXiv:2210.13983",
    "title": "COEP: Cascade Optimization for Inverse Problems with Entropy-Preserving  Hyperparameter Tuning",
    "abstract": "COEP: Cascade Optimization for Inverse Problems with Entropy-Preserving  Hyperparameter Tuning",
    "descriptor": "",
    "authors": [
      "Tianci Liu",
      "Tong Yang",
      "Quan Zhang",
      "Qi Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13983"
  },
  {
    "id": "arXiv:2210.13994",
    "title": "Minutiae-Guided Fingerprint Embeddings via Vision Transformers",
    "abstract": "Minutiae-Guided Fingerprint Embeddings via Vision Transformers",
    "descriptor": "",
    "authors": [
      "Steven A. Grosz",
      "Joshua J. Engelsma",
      "Rajeev Ranjan",
      "Naveen Ramakrishnan",
      "Manoj Aggarwal",
      "Gerard G. Medioni",
      "Anil K. Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13994"
  },
  {
    "id": "arXiv:2210.14056",
    "title": "Unsupervised Anomaly Detection for Auditing Data and Impact of  Categorical Encodings",
    "abstract": "Comments: This work has been accepted at Proceedings of the Neurips 2022 Workshop on Synthetic Data 4ML",
    "descriptor": "\nComments: This work has been accepted at Proceedings of the Neurips 2022 Workshop on Synthetic Data 4ML\n",
    "authors": [
      "Ajay Chawda",
      "Stefanie Grimm",
      "Marius Kloft"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14056"
  },
  {
    "id": "arXiv:2210.14169",
    "title": "Weakly Supervised Data Augmentation Through Prompting for Dialogue  Understanding",
    "abstract": "Comments: To appear in SyntheticData4ML @ NeurIPS 2022. 16 pages, 10 figures, 3 tables",
    "descriptor": "\nComments: To appear in SyntheticData4ML @ NeurIPS 2022. 16 pages, 10 figures, 3 tables\n",
    "authors": [
      "Maximillian Chen",
      "Alexandros Papangelis",
      "Chenyang Tao",
      "Andy Rosenbaum",
      "Seokhwan Kim",
      "Yang Liu",
      "Zhou Yu",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14169"
  }
]